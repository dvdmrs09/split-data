{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a duration for the specified key", "response": "def add_duration(self, key, value):\n        \"\"\"Add a duration for the specified key\n\n        :param str key: The value name\n        :param float value: The value\n\n        .. versionadded:: 3.19.0\n\n        \"\"\"\n        if key not in self.durations:\n            self.durations[key] = []\n        self.durations[key].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef track_duration(self, key):\n        if key not in self.durations:\n            self.durations[key] = []\n        start_time = time.time()\n        try:\n            yield\n        finally:\n            self.durations[key].append(\n                max(start_time, time.time()) - start_time)", "response": "Context manager that tracks the duration of time that it takes to execute whatever it is wrapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(self, record):\n        if self._exists:\n            return int(getattr(record, 'correlation_id', None) is not None)\n        return int(getattr(record, 'correlation_id', None) is None)", "response": "Returns a boolean indicating whether the specified log record should be logged."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self, msg, kwargs):\n        kwargs['extra'] = {\n            'correlation_id': self.parent.correlation_id,\n            'parent': self.parent.name\n        }\n        return msg, kwargs", "response": "Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef observe_id(self):\n        if self._observe_id is None:\n            hasher = hashlib.sha256()\n            hasher.update(self.viewset_class.__module__.encode('utf8'))\n            hasher.update(self.viewset_class.__name__.encode('utf8'))\n            hasher.update(self.viewset_method.encode('utf8'))\n            # Arguments do not need to be taken into account as they are\n            # derived from the request path, which is already accounted for.\n            for key in sorted(self.GET.keys()):\n                hasher.update(key.encode('utf8'))\n                hasher.update(self.GET[key].encode('utf8'))\n            hasher.update(self.path.encode('utf8'))\n            hasher.update(self.path_info.encode('utf8'))\n            if self._force_auth_user is not None:\n                hasher.update(\n                    (str(self._force_auth_user.id) or 'anonymous').encode('utf8')\n                )\n            else:\n                hasher.update(b'anonymous')\n            self._observe_id = hasher.hexdigest()\n\n        return self._observe_id", "response": "Unique identifier that identifies the observer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle a query observer unsubscription request.", "response": "def post(self, request):\n        \"\"\"Handle a query observer unsubscription request.\"\"\"\n        try:\n            observer_id = request.query_params['observer']\n            session_id = request.query_params['subscriber']\n        except KeyError:\n            return response.Response(status=400)\n\n        observer.remove_subscriber(session_id, observer_id)\n        return response.Response()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransmits ORM table change notification.", "response": "def notify_observers(table, kind, primary_key=None):\n    \"\"\"Transmit ORM table change notification.\n\n    :param table: Name of the table that has changed\n    :param kind: Change type\n    :param primary_key: Primary key of the affected instance\n    \"\"\"\n\n    if IN_MIGRATIONS:\n        return\n\n    # Don't propagate events when there are no observers to receive them.\n    if not Observer.objects.filter(dependencies__table=table).exists():\n        return\n\n    def handler():\n        \"\"\"Send a notification to the given channel.\"\"\"\n        try:\n            async_to_sync(get_channel_layer().send)(\n                CHANNEL_MAIN,\n                {\n                    'type': TYPE_ORM_NOTIFY,\n                    'table': table,\n                    'kind': kind,\n                    'primary_key': str(primary_key),\n                },\n            )\n        except ChannelFull:\n            logger.exception(\"Unable to notify workers.\")\n\n    batcher = PrioritizedBatcher.global_instance()\n    if batcher.is_started:\n        # If a batch is open, queue the send via the batcher.\n        batcher.add(\n            'rest_framework_reactive', handler, group_by=(table, kind, primary_key)\n        )\n    else:\n        # If no batch is open, invoke immediately.\n        handler()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model_post_save(sender, instance, created=False, **kwargs):\n\n    if sender._meta.app_label == 'rest_framework_reactive':\n        # Ignore own events.\n        return\n\n    def notify():\n        table = sender._meta.db_table\n        if created:\n            notify_observers(table, ORM_NOTIFY_KIND_CREATE, instance.pk)\n        else:\n            notify_observers(table, ORM_NOTIFY_KIND_UPDATE, instance.pk)\n\n    transaction.on_commit(notify)", "response": "Signal emitted after any model is saved via Django ORM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef model_post_delete(sender, instance, **kwargs):\n\n    if sender._meta.app_label == 'rest_framework_reactive':\n        # Ignore own events.\n        return\n\n    def notify():\n        table = sender._meta.db_table\n        notify_observers(table, ORM_NOTIFY_KIND_DELETE, instance.pk)\n\n    transaction.on_commit(notify)", "response": "Signal emitted after any model is deleted via Django ORM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsignal emitted after any M2M relation changes via Django ORM.", "response": "def model_m2m_changed(sender, instance, action, **kwargs):\n    \"\"\"\n    Signal emitted after any M2M relation changes via Django ORM.\n\n    :param sender: M2M intermediate model\n    :param instance: The actual instance that was saved\n    :param action: M2M action\n    \"\"\"\n\n    if sender._meta.app_label == 'rest_framework_reactive':\n        # Ignore own events.\n        return\n\n    def notify():\n        table = sender._meta.db_table\n        if action == 'post_add':\n            notify_observers(table, ORM_NOTIFY_KIND_CREATE)\n        elif action in ('post_remove', 'post_clear'):\n            notify_observers(table, ORM_NOTIFY_KIND_DELETE)\n\n    transaction.on_commit(notify)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess notification from ORM.", "response": "async def observer_orm_notify(self, message):\n        \"\"\"Process notification from ORM.\"\"\"\n\n        @database_sync_to_async\n        def get_observers(table):\n            # Find all observers with dependencies on the given table.\n            return list(\n                Observer.objects.filter(\n                    dependencies__table=table, subscribers__isnull=False\n                )\n                .distinct('pk')\n                .values_list('pk', flat=True)\n            )\n\n        observers_ids = await get_observers(message['table'])\n\n        for observer_id in observers_ids:\n            await self.channel_layer.send(\n                CHANNEL_WORKER, {'type': TYPE_EVALUATE, 'observer': observer_id}\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def observer_poll(self, message):\n        # Sleep until we need to notify the observer.\n        await asyncio.sleep(message['interval'])\n\n        # Dispatch task to evaluate the observable.\n        await self.channel_layer.send(\n            CHANNEL_WORKER, {'type': TYPE_EVALUATE, 'observer': message['observer']}\n        )", "response": "Poll observer after a delay."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the observer evaluation on the worker or throttle.", "response": "async def observer_evaluate(self, message):\n        \"\"\"Execute observer evaluation on the worker or throttle.\"\"\"\n        observer_id = message['observer']\n        throttle_rate = get_queryobserver_settings()['throttle_rate']\n        if throttle_rate <= 0:\n            await self._evaluate(observer_id)\n            return\n\n        cache_key = throttle_cache_key(observer_id)\n        try:\n            count = cache.incr(cache_key)\n            # Ignore if delayed observer already scheduled.\n            if count == 2:\n                await self.channel_layer.send(\n                    CHANNEL_MAIN,\n                    {\n                        'type': TYPE_POLL,\n                        'observer': observer_id,\n                        'interval': throttle_rate,\n                    },\n                )\n        except ValueError:\n            count = cache.get_or_set(cache_key, default=1, timeout=throttle_rate)\n            # Ignore if cache was set and increased in another thread.\n            if count == 1:\n                await self._evaluate(observer_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef websocket_connect(self, message):\n        self.session_id = self.scope['url_route']['kwargs']['subscriber_id']\n        super().websocket_connect(message)\n\n        # Create new subscriber object.\n        Subscriber.objects.get_or_create(session_id=self.session_id)", "response": "Called when WebSocket connection is established."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when WebSocket connection is closed.", "response": "def disconnect(self, code):\n        \"\"\"Called when WebSocket connection is closed.\"\"\"\n        Subscriber.objects.filter(session_id=self.session_id).delete()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef observer_update(self, message):\n        # Demultiplex observer update into multiple messages.\n        for action in ('added', 'changed', 'removed'):\n            for item in message[action]:\n                self.send_json(\n                    {\n                        'msg': action,\n                        'observer': message['observer'],\n                        'primary_key': message['primary_key'],\n                        'order': item['order'],\n                        'item': item['data'],\n                    }\n                )", "response": "Called when an update from observer is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_subscriber(session_id, observer_id):\n    models.Observer.subscribers.through.objects.filter(\n        subscriber_id=session_id, observer_id=observer_id\n    ).delete()", "response": "Removes a subscriber from the given observer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_logging_extra(self, duration=None, results=None):\n        return {\n            'duration': duration,\n            'results': results,\n            'observer_id': self.id,\n            'viewset': '{}.{}'.format(\n                self._request.viewset_class.__module__,\n                self._request.viewset_class.__name__,\n            ),\n            'method': self._request.viewset_method,\n            'path': self._request.path,\n            'get': self._request.GET,\n        }", "response": "Returns extra information for logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subscribe(self, session_id, dependencies=None):\n        try:\n            change_detection = self._meta.change_detection\n            if change_detection not in [\n                Options.CHANGE_DETECTION_PUSH,\n                Options.CHANGE_DETECTION_POLL,\n            ]:\n                raise NotImplementedError(\n                    \"Change detection mechanism '{}' not implemented.\".format(\n                        change_detection\n                    )\n                )\n\n            viewset_results = self._viewset_results()\n\n            poll_interval = (\n                self._meta.poll_interval\n                if change_detection == Options.CHANGE_DETECTION_POLL\n                else None\n            )\n\n            # Subscribe to observer in a single query. First, create an\n            # observer, then create a subscriber, and finally subscribe to\n            # the observer. If already subscribed, ignore the conflict.\n            for retry in range(MAX_INTEGRITY_ERROR_RETRIES):\n                is_subscribed = False\n                cursor = connection.cursor()\n                try:\n                    cursor.execute(\n                        \"\"\"\n                        WITH inserted_observer AS (\n                            INSERT into {observer_table} (\"id\", \"request\", \"poll_interval\")\n                            VALUES (%(observer_id)s, %(request)s, %(poll_interval)s)\n                            ON CONFLICT DO NOTHING\n                        ), inserted_subscriber AS (\n                            INSERT into {subscriber_table} (\"session_id\", \"created\")\n                            VALUES (%(subscriber_id)s, NOW())\n                            ON CONFLICT DO NOTHING\n                        )\n                        INSERT INTO {observer_subscribers_table} (\"observer_id\", \"subscriber_id\")\n                        VALUES (%(observer_id)s, %(subscriber_id)s)\n                        \"\"\".format(\n                            observer_table=models.Observer._meta.db_table,\n                            subscriber_table=models.Subscriber._meta.db_table,\n                            observer_subscribers_table=models.Observer.subscribers.through._meta.db_table,\n                        ),\n                        params={\n                            'observer_id': self.id,\n                            'request': pickle.dumps(self._request),\n                            'poll_interval': poll_interval,\n                            'subscriber_id': session_id,\n                        },\n                    )\n                    is_subscribed = True\n                except IntegrityError as err:\n                    msg = str(err)\n                    if (\n                        'Key (observer_id, subscriber_id)' in msg\n                        and 'already exists' in msg\n                    ):\n                        # Subscriber already subscribed, we're good.\n                        is_subscribed = True\n                    elif (\n                        'Key (observer_id)' in msg or 'Key (subscriber_id)' in msg\n                    ) and 'not present in table' in msg:\n                        # Could not subscribe because observer, subscriber or\n                        # both are missing, retry.\n                        if retry == MAX_INTEGRITY_ERROR_RETRIES - 1:\n                            raise\n                    else:\n                        raise\n                finally:\n                    cursor.close()\n\n                if is_subscribed:\n                    break\n\n            # Determine who should notify us based on the configured change\n            # detection mechanism.\n            if change_detection == Options.CHANGE_DETECTION_PUSH:\n                if dependencies:\n                    tables = [model._meta.db_table for model in dependencies]\n                else:\n                    tables = [self._viewset.get_queryset().model._meta.db_table]\n\n                # Register table dependencies for push observables.\n                for table in tables:\n                    try:\n                        models.Dependency.objects.get_or_create(\n                            observer_id=self.id, table=table\n                        )\n                    except models.Observer.DoesNotExist:\n                        # The observer was removed before dependency tables\n                        # were created.\n                        return viewset_results\n\n            elif self._meta.change_detection == Options.CHANGE_DETECTION_POLL:\n                # Register poller.\n                async_to_sync(get_channel_layer().send)(\n                    CHANNEL_MAIN,\n                    {\n                        'type': TYPE_POLL,\n                        'observer': self.id,\n                        'interval': self._meta.poll_interval,\n                    },\n                )\n\n            self._evaluate(viewset_results)\n\n        except Exception:\n            logger.exception(\n                \"Error while subscribing to observer ({})\".format(\n                    self._get_logging_id()\n                ),\n                extra=self._get_logging_extra(),\n            )\n\n        return viewset_results", "response": "Subscribe to an observer and subscribe to a subscriber."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def evaluate(self):\n\n        @database_sync_to_async\n        def remove_subscribers():\n            models.Observer.subscribers.through.objects.filter(\n                observer_id=self.id\n            ).delete()\n\n        @database_sync_to_async\n        def get_subscriber_sessions():\n            return list(\n                models.Observer.subscribers.through.objects.filter(observer_id=self.id)\n                .distinct('subscriber_id')\n                .values_list('subscriber_id', flat=True)\n            )\n\n        try:\n            settings = get_queryobserver_settings()\n\n            start = time.time()\n            # Evaluate the observer\n            added, changed, removed = await database_sync_to_async(self._evaluate)()\n            duration = time.time() - start\n\n            # Log slow observers.\n            if duration > settings['warnings']['max_processing_time']:\n                self._warning(\"Slow observed viewset\", duration=duration)\n\n            # Remove subscribers of really slow observers.\n            if duration > settings['errors']['max_processing_time']:\n                logger.error(\n                    \"Removing subscribers to extremely slow observed viewset ({})\".format(\n                        self._get_logging_id()\n                    ),\n                    extra=self._get_logging_extra(duration=duration),\n                )\n                await remove_subscribers()\n\n            if self._meta.change_detection == Options.CHANGE_DETECTION_POLL:\n                # Register poller.\n                await get_channel_layer().send(\n                    CHANNEL_MAIN,\n                    {\n                        'type': TYPE_POLL,\n                        'observer': self.id,\n                        'interval': self._meta.poll_interval,\n                    },\n                )\n\n            message = {\n                'type': TYPE_ITEM_UPDATE,\n                'observer': self.id,\n                'primary_key': self._meta.primary_key,\n                'added': added,\n                'changed': changed,\n                'removed': removed,\n            }\n\n            # Only generate notifications in case there were any changes.\n            if added or changed or removed:\n                for session_id in await get_subscriber_sessions():\n                    await get_channel_layer().group_send(\n                        GROUP_SESSIONS.format(session_id=session_id), message\n                    )\n\n        except Exception:\n            logger.exception(\n                \"Error while evaluating observer ({})\".format(self._get_logging_id()),\n                extra=self._get_logging_extra(),\n            )", "response": "Evaluate the query observer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing results from the viewset response.", "response": "def _viewset_results(self):\n        \"\"\"Parse results from the viewset response.\"\"\"\n        results = []\n        try:\n            response = self._viewset_method(\n                self._viewset.request, *self._request.args, **self._request.kwargs\n            )\n\n            if response.status_code == 200:\n                results = response.data\n\n                if not isinstance(results, list):\n                    if isinstance(results, dict):\n                        # XXX: This can incidently match if a single\n                        # object has results key\n                        if 'results' in results and isinstance(\n                            results['results'], list\n                        ):\n                            # Support paginated results.\n                            results = results['results']\n                        else:\n                            results.setdefault(self._meta.primary_key, 1)\n                            results = [collections.OrderedDict(results)]\n                    else:\n                        raise ValueError(\n                            \"Observable views must return a dictionary or a list of dictionaries!\"\n                        )\n        except Http404:\n            pass\n        except django_exceptions.ObjectDoesNotExist:\n            # The evaluation may fail when certain dependent objects (like users) are removed\n            # from the database. In this case, the observer is stopped.\n            pass\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _evaluate(self, viewset_results=None):\n        if viewset_results is None:\n            viewset_results = self._viewset_results()\n\n        try:\n            observer = models.Observer.objects.get(id=self.id)\n            # Do not evaluate the observer if there are no subscribers\n            if observer.subscribers.count() == 0:\n                return (None, None, None)\n\n            # Update last evaluation time.\n            models.Observer.objects.filter(id=self.id).update(\n                last_evaluation=timezone.now()\n            )\n\n            # Log viewsets with too much output.\n            max_result = get_queryobserver_settings()['warnings']['max_result_length']\n            if len(viewset_results) > max_result:\n                self._warning(\n                    \"Observed viewset returns too many results\",\n                    results=len(viewset_results),\n                )\n            new_results = collections.OrderedDict()\n            for order, item in enumerate(viewset_results):\n                if not isinstance(item, dict):\n                    raise ValueError(\n                        \"Observable views must return a dictionary or a list of dictionaries!\"\n                    )\n\n                item = {'order': order, 'data': item}\n\n                try:\n                    new_results[str(item['data'][self._meta.primary_key])] = item\n                except KeyError:\n                    raise KeyError(\n                        \"Observable view did not return primary key field '{}'!\".format(\n                            self._meta.primary_key\n                        )\n                    )\n\n            # Process difference between old results and new results.\n            added, changed = [], []\n            new_ids = list(new_results.keys())\n\n            removed_qs = observer.items.exclude(primary_key__in=new_results.keys())\n            removed = list(removed_qs.values('order', 'data'))\n\n            maybe_changed_qs = observer.items.filter(primary_key__in=new_results.keys())\n\n            with transaction.atomic():\n                # Removed items.\n                removed_qs.delete()\n\n                # Defer unique ordering constraint before processing order updates.\n                # NOTE: The name of the constrait is generated by Django ORM.\n                with connection.cursor() as cursor:\n                    cursor.execute(\n                        \"SET CONSTRAINTS rest_framework_reactive_item_observer_id_order_9b8adde6_uniq DEFERRED\"\n                    )\n\n                # Changed items.\n                for item_id, old_order, old_data in maybe_changed_qs.values_list(\n                    'primary_key', 'order', 'data'\n                ):\n                    new_item = new_results[item_id]\n                    new_ids.remove(item_id)\n\n                    if new_item['data'] != old_data:\n                        changed.append(new_item)\n                        observer.items.filter(primary_key=item_id).update(\n                            data=new_item['data'], order=new_item['order']\n                        )\n                    elif new_item['order'] != old_order:\n                        # TODO: If only order has changed, don't transmit\n                        # full data (needs frontend support).\n                        changed.append(new_item)\n                        observer.items.filter(primary_key=item_id).update(\n                            order=new_item['order']\n                        )\n\n                # Added items.\n                for item_id in new_ids:\n                    item = new_results[item_id]\n                    added.append(item)\n                    observer.items.create(\n                        primary_key=item_id, order=item['order'], data=item['data']\n                    )\n\n            return (added, changed, removed)\n\n        except models.Observer.DoesNotExist:\n            # Observer removed, ignore evaluation\n            return (None, None, None)", "response": "Evaluate query observer.\n\n        :param viewset_results: Objects returned by the viewset query"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking ViewSet or ViewSet method observable. Decorating a ViewSet class is the same as decorating its `list` method. If decorated method returns a response containing a list of items, it must use the provided `LimitOffsetPagination` for any pagination. In case a non-list response is returned, the resulting item will be wrapped into a list. When multiple decorators are used, `observable` must be the first one to be applied as it needs access to the method name. :param poll_interval: Configure given observable as a polling observable :param primary_key: Primary key for tracking observable items :param dependencies: List of ORM to register as dependencies for orm_notify. If None the observer will subscribe to notifications from the queryset model.", "response": "def observable(\n    _method_or_viewset=None, poll_interval=None, primary_key=None, dependencies=None\n):\n    \"\"\"Make ViewSet or ViewSet method observable.\n\n    Decorating a ViewSet class is the same as decorating its `list` method.\n\n    If decorated method returns a response containing a list of items, it must\n    use the provided `LimitOffsetPagination` for any pagination. In case a\n    non-list response is returned, the resulting item will be wrapped into a\n    list.\n\n    When multiple decorators are used, `observable` must be the first one to be\n    applied as it needs access to the method name.\n\n    :param poll_interval: Configure given observable as a polling observable\n    :param primary_key: Primary key for tracking observable items\n    :param dependencies: List of ORM to register as dependencies for\n        orm_notify. If None the observer will subscribe to notifications from\n        the queryset model.\n\n    \"\"\"\n    if poll_interval and dependencies:\n        raise ValueError('Only one of poll_interval and dependencies arguments allowed')\n\n    def decorator_observable(method_or_viewset):\n\n        if inspect.isclass(method_or_viewset):\n            list_method = getattr(method_or_viewset, 'list', None)\n            if list_method is not None:\n                method_or_viewset.list = observable(list_method)\n\n            return method_or_viewset\n\n        # Do not decorate an already observable method twice.\n        if getattr(method_or_viewset, 'is_observable', False):\n            return method_or_viewset\n\n        @functools.wraps(method_or_viewset)\n        def wrapper(self, request, *args, **kwargs):\n            if observer_request.OBSERVABLE_QUERY_PARAMETER in request.query_params:\n                # TODO: Validate the session identifier.\n                session_id = request.query_params[\n                    observer_request.OBSERVABLE_QUERY_PARAMETER\n                ]\n\n                # Create request and subscribe the session to given observer.\n                request = observer_request.Request(\n                    self.__class__, method_or_viewset.__name__, request, args, kwargs\n                )\n\n                # Initialize observer and subscribe.\n                instance = observer.QueryObserver(request)\n                data = instance.subscribe(session_id, dependencies)\n\n                return response.Response({'observer': instance.id, 'items': data})\n            else:\n                # Non-reactive API.\n                return method_or_viewset(self, request, *args, **kwargs)\n\n        wrapper.is_observable = True\n\n        if poll_interval is not None:\n            wrapper.observable_change_detection = observer.Options.CHANGE_DETECTION_POLL\n            wrapper.observable_poll_interval = poll_interval\n\n        if primary_key is not None:\n            wrapper.observable_primary_key = primary_key\n\n        return wrapper\n\n    if _method_or_viewset is None:\n        return decorator_observable\n    else:\n        return decorator_observable(_method_or_viewset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all observers and subscribers.", "response": "def handle(self, *args, **options):\n        \"\"\"Command handle.\"\"\"\n        models.Observer.objects.all().delete()\n        models.Subscriber.objects.all().delete()\n\n        for cache_key in cache.keys(search='{}*'.format(THROTTLE_CACHE_PREFIX)):\n            cache.delete(cache_key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck to if this is actually a valid RSS feed", "response": "def set_is_valid_rss(self):\n        \"\"\"Check to if this is actually a valid RSS feed\"\"\"\n        if self.title and self.link and self.description:\n            self.is_valid_rss = True\n        else:\n            self.is_valid_rss = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse and sets non required elements", "response": "def set_extended_elements(self):\n        \"\"\"Parses and sets non required elements\"\"\"\n        self.set_creative_commons()\n        self.set_owner()\n        self.set_subtitle()\n        self.set_summary()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the elements related to itunes", "response": "def set_itunes(self):\n        \"\"\"Sets elements related to itunes\"\"\"\n        self.set_itunes_author_name()\n        self.set_itunes_block()\n        self.set_itunes_complete()\n        self.set_itunes_explicit()\n        self.set_itune_image()\n        self.set_itunes_keywords()\n        self.set_itunes_new_feed_url()\n        self.set_itunes_categories()\n        self.set_items()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_optional_elements(self):\n        self.set_categories()\n        self.set_copyright()\n        self.set_generator()\n        self.set_image()\n        self.set_language()\n        self.set_last_build_date()\n        self.set_managing_editor()\n        self.set_published_date()\n        self.set_pubsubhubbub()\n        self.set_ttl()\n        self.set_web_master()", "response": "Sets the optional elements of the RSS document"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset soup and strips items", "response": "def set_soup(self):\n        \"\"\"Sets soup and strips items\"\"\"\n        self.soup = BeautifulSoup(self.feed_content, \"html.parser\")\n        for item in self.soup.findAll('item'):\n            item.decompose()\n        for image in self.soup.findAll('image'):\n            image.decompose()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and sets feed categories", "response": "def set_categories(self):\n        \"\"\"Parses and set feed categories\"\"\"\n        self.categories = []\n        temp_categories = self.soup.findAll('category')\n        for category in temp_categories:\n            category_text = category.string\n            self.categories.append(category_text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncounting Items in soup and full_soup. For debugging", "response": "def count_items(self):\n        \"\"\"Counts Items in full_soup and soup. For debugging\"\"\"\n        soup_items = self.soup.findAll('item')\n        full_soup_items = self.full_soup.findAll('item')\n        return len(soup_items), len(full_soup_items)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_copyright(self):\n        try:\n            self.copyright = self.soup.find('copyright').string\n        except AttributeError:\n            self.copyright = None", "response": "Parses copyright and set value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing creative commons for item and sets value", "response": "def set_creative_commons(self):\n        \"\"\"Parses creative commons for item and sets value\"\"\"\n        try:\n            self.creative_commons = self.soup.find(\n                'creativecommons:license').string\n        except AttributeError:\n            self.creative_commons = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_description(self):\n        try:\n            self.description = self.soup.find('description').string\n        except AttributeError:\n            self.description = None", "response": "Parses description and sets value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing feed generator and sets value", "response": "def set_generator(self):\n        \"\"\"Parses feed generator and sets value\"\"\"\n        try:\n            self.generator = self.soup.find('generator').string\n        except AttributeError:\n            self.generator = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_image(self):\n        temp_soup = self.full_soup\n        for item in temp_soup.findAll('item'):\n            item.decompose()\n        image = temp_soup.find('image')\n        try:\n            self.image_title = image.find('title').string\n        except AttributeError:\n            self.image_title = None\n        try:\n            self.image_url = image.find('url').string\n        except AttributeError:\n            self.image_url = None\n        try:\n            self.image_link = image.find('link').string\n        except AttributeError:\n            self.image_link = None\n        try:\n            self.image_width = image.find('width').string\n        except AttributeError:\n            self.image_width = None\n        try:\n            self.image_height = image.find('height').string\n        except AttributeError:\n            self.image_height = None", "response": "Parses image element and sets attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_itunes_author_name(self):\n        try:\n            self.itunes_author_name = self.soup.find('itunes:author').string\n        except AttributeError:\n            self.itunes_author_name = None", "response": "Parses author name from itunes tags and sets value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking and see if podcast is blocked from iTunes and sets value", "response": "def set_itunes_block(self):\n        \"\"\"Check and see if podcast is blocked from iTunes and sets value\"\"\"\n        try:\n            block = self.soup.find('itunes:block').string.lower()\n        except AttributeError:\n            block = \"\"\n        if block == \"yes\":\n            self.itunes_block = True\n        else:\n            self.itunes_block = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_itunes_categories(self):\n        self.itunes_categories = []\n        temp_categories = self.soup.findAll('itunes:category')\n        for category in temp_categories:\n            category_text = category.get('text')\n            self.itunes_categories.append(category_text)", "response": "Parses and sets the itunes categories"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_itunes_complete(self):\n        try:\n            self.itunes_complete = self.soup.find('itunes:complete').string\n            self.itunes_complete = self.itunes_complete.lower()\n        except AttributeError:\n            self.itunes_complete = None", "response": "Parses complete from itunes tags and sets value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing explicit from itunes tags and sets value", "response": "def set_itunes_explicit(self):\n        \"\"\"Parses explicit from itunes tags and sets value\"\"\"\n        try:\n            self.itunes_explicit = self.soup.find('itunes:explicit').string\n            self.itunes_explicit = self.itunes_explicit.lower()\n        except AttributeError:\n            self.itunes_explicit = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses itunes images and set url as value", "response": "def set_itune_image(self):\n        \"\"\"Parses itunes images and set url as value\"\"\"\n        try:\n            self.itune_image = self.soup.find('itunes:image').get('href')\n        except AttributeError:\n            self.itune_image = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_itunes_keywords(self):\n        try:\n            keywords = self.soup.find('itunes:keywords').string\n        except AttributeError:\n            keywords = None\n        try:\n            self.itunes_keywords = [keyword.strip()\n                                    for keyword in keywords.split(',')]\n            self.itunes_keywords = list(set(self.itunes_keywords))\n        except AttributeError:\n            self.itunes_keywords = []", "response": "Parses itunes keywords and set value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse new feed url from itunes tags and sets value", "response": "def set_itunes_new_feed_url(self):\n        \"\"\"Parses new feed url from itunes tags and sets value\"\"\"\n        try:\n            self.itunes_new_feed_url = self.soup.find(\n                'itunes:new-feed-url').string\n        except AttributeError:\n            self.itunes_new_feed_url = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse feed language and set value", "response": "def set_language(self):\n        \"\"\"Parses feed language and set value\"\"\"\n        try:\n            self.language = self.soup.find('language').string\n        except AttributeError:\n            self.language = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the last build date and set value", "response": "def set_last_build_date(self):\n        \"\"\"Parses last build date and set value\"\"\"\n        try:\n            self.last_build_date = self.soup.find('lastbuilddate').string\n        except AttributeError:\n            self.last_build_date = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses link to homepage and set value", "response": "def set_link(self):\n        \"\"\"Parses link to homepage and set value\"\"\"\n        try:\n            self.link = self.soup.find('link').string\n        except AttributeError:\n            self.link = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_managing_editor(self):\n        try:\n            self.managing_editor = self.soup.find('managingeditor').string\n        except AttributeError:\n            self.managing_editor = None", "response": "Parses managing editor and set value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_published_date(self):\n        try:\n            self.published_date = self.soup.find('pubdate').string\n        except AttributeError:\n            self.published_date = None", "response": "Parses published date and set value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_pubsubhubbub(self):\n        self.pubsubhubbub = None\n        atom_links = self.soup.findAll('atom:link')\n        for atom_link in atom_links:\n            rel = atom_link.get('rel')\n            if rel == \"hub\":\n                self.pubsubhubbub = atom_link.get('href')", "response": "Parses pubsubhubbub and email then sets value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_owner(self):\n        owner = self.soup.find('itunes:owner')\n        try:\n            self.owner_name = owner.find('itunes:name').string\n        except AttributeError:\n            self.owner_name = None\n        try:\n            self.owner_email = owner.find('itunes:email').string\n        except AttributeError:\n            self.owner_email = None", "response": "Parses owner name and email then sets value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses subtitle and sets value", "response": "def set_subtitle(self):\n        \"\"\"Parses subtitle and sets value\"\"\"\n        try:\n            self.subtitle = self.soup.find('itunes:subtitle').string\n        except AttributeError:\n            self.subtitle = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_summary(self):\n        try:\n            self.summary = self.soup.find('itunes:summary').string\n        except AttributeError:\n            self.summary = None", "response": "Parses summary and set value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing summary and set value", "response": "def set_ttl(self):\n        \"\"\"Parses summary and set value\"\"\"\n        try:\n            self.ttl = self.soup.find('ttl').string\n        except AttributeError:\n            self.ttl = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_web_master(self):\n        try:\n            self.web_master = self.soup.find('webmaster').string\n        except AttributeError:\n            self.web_master = None", "response": "Parses the feed s webmaster and sets value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset each of the basic rss elements.", "response": "def set_rss_element(self):\n        \"\"\"Set each of the basic rss elements.\"\"\"\n        self.set_author()\n        self.set_categories()\n        self.set_comments()\n        self.set_creative_commons()\n        self.set_description()\n        self.set_enclosure()\n        self.set_guid()\n        self.set_link()\n        self.set_published_date()\n        self.set_title()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses author and set value.", "response": "def set_author(self):\n        \"\"\"Parses author and set value.\"\"\"\n        try:\n            self.author = self.soup.find('author').string\n        except AttributeError:\n            self.author = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses comments and set value.", "response": "def set_comments(self):\n        \"\"\"Parses comments and set value.\"\"\"\n        try:\n            self.comments = self.soup.find('comments').string\n        except AttributeError:\n            self.comments = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_enclosure(self):\n        try:\n            self.enclosure_url = self.soup.find('enclosure')['url']\n        except:\n            self.enclosure_url = None\n        try:\n            self.enclosure_type = self.soup.find('enclosure')['type']\n        except:\n            self.enclosure_type = None\n        try:\n            self.enclosure_length = self.soup.find('enclosure')['length']\n            self.enclosure_length = int(self.enclosure_length)\n        except:\n            self.enclosure_length = None", "response": "Parses enclosure_url enclosure_type then set values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses guid and set value", "response": "def set_guid(self):\n        \"\"\"Parses guid and set value\"\"\"\n        try:\n            self.guid = self.soup.find('guid').string\n        except AttributeError:\n            self.guid = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing title and set value.", "response": "def set_title(self):\n        \"\"\"Parses title and set value.\"\"\"\n        try:\n            self.title = self.soup.find('title').string\n        except AttributeError:\n            self.title = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_itunes_element(self):\n        self.set_itunes_author_name()\n        self.set_itunes_block()\n        self.set_itunes_closed_captioned()\n        self.set_itunes_duration()\n        self.set_itunes_explicit()\n        self.set_itune_image()\n        self.set_itunes_order()\n        self.set_itunes_subtitle()\n        self.set_itunes_summary()", "response": "Set each of the itunes elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_itunes_closed_captioned(self):\n        try:\n            self.itunes_closed_captioned = self.soup.find(\n                'itunes:isclosedcaptioned').string\n            self.itunes_closed_captioned = self.itunes_closed_captioned.lower()\n        except AttributeError:\n            self.itunes_closed_captioned = None", "response": "Parses isClosedCaptioned from itunes tags and sets value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing duration from itunes tags and sets value", "response": "def set_itunes_duration(self):\n        \"\"\"Parses duration from itunes tags and sets value\"\"\"\n        try:\n            self.itunes_duration = self.soup.find('itunes:duration').string\n        except AttributeError:\n            self.itunes_duration = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_itunes_order(self):\n        try:\n            self.itunes_order = self.soup.find('itunes:order').string\n            self.itunes_order = self.itunes_order.lower()\n        except AttributeError:\n            self.itunes_order = None", "response": "Parses episode order and set url as value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse subtitle from itunes tags and sets value", "response": "def set_itunes_subtitle(self):\n        \"\"\"Parses subtitle from itunes tags and sets value\"\"\"\n        try:\n            self.itunes_subtitle = self.soup.find('itunes:subtitle').string\n        except AttributeError:\n            self.itunes_subtitle = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses summary from itunes tags and sets value", "response": "def set_itunes_summary(self):\n        \"\"\"Parses summary from itunes tags and sets value\"\"\"\n        try:\n            self.itunes_summary = self.soup.find('itunes:summary').string\n        except AttributeError:\n            self.itunes_summary = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies AdaGrad update to parameters.", "response": "def _apply_updates(self, gradients):\n        \"\"\"Apply AdaGrad update to parameters.\n\n        Parameters\n        ----------\n        gradients\n\n        Returns\n        -------\n\n        \"\"\"\n        if not hasattr(self, 'optimizers'):\n            self.optimizers = \\\n                {obj: AdaGradOptimizer(self.learning_rate)\n                 for obj in ['W', 'C', 'bw', 'bc']}\n        self.W -= self.optimizers['W'].get_step(gradients['W'])\n        self.C -= self.optimizers['C'].get_step(gradients['C'])\n        self.bw -= self.optimizers['bw'].get_step(gradients['bw'])\n        self.bc -= self.optimizers['bc'].get_step(gradients['bc'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_step(self, grad):\n        if self._momentum is None:\n            self._momentum = self.initial_accumulator_value * np.ones_like(grad)\n        self._momentum += grad ** 2\n        return self.learning_rate * grad / np.sqrt(self._momentum)", "response": "Computes the step to take for the next gradient descent update."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap string values in quotes and returns as key = value.", "response": "def _format_param_value(key, value):\n    \"\"\"Wraps string values in quotes, and returns as 'key=value'.\n    \"\"\"\n    if isinstance(value, str):\n        value = \"'{}'\".format(value)\n    return \"{}={}\".format(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an array containing the logs of the nonzero elements of M. Zeros are left alone since log ( 0 is not a nonzero .", "response": "def log_of_array_ignoring_zeros(M):\n    \"\"\"Returns an array containing the logs of the nonzero\n    elements of M. Zeros are left alone since log(0) isn't\n    defined.\n\n    Parameters\n    ----------\n    M : array-like\n\n    Returns\n    -------\n    array-like\n        Shape matches `M`\n\n    \"\"\"\n    log_M = M.copy()\n    mask = log_M > 0\n    log_M[mask] = np.log(log_M[mask])\n    return log_M"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an m x n matrix drawn using the Xavier Glorot method.", "response": "def randmatrix(m, n, random_seed=None):\n    \"\"\"Creates an m x n matrix of random values drawn using\n    the Xavier Glorot method.\"\"\"\n    val = np.sqrt(6.0 / (m + n))\n    np.random.seed(random_seed)\n    return np.random.uniform(-val, val, size=(m, n))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self,\n            X,\n            vocab=None,\n            initial_embedding_dict=None,\n            fixed_initialization=None):\n        \"\"\"Run GloVe and return the new matrix.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_words, n_words]\n            The square count matrix.\n\n        vocab : iterable or None (default: None)\n            Rownames for `X`.\n\n        initial_embedding_dict : dict or None (default: None)\n            Map into representations that we want to use for a \"warm\n            start\" -- e.g., GloVe vectors trained on a massive corpus.\n\n            Learned representations of words in `vocab` are initialized\n            from `initial_embedding_dict` wherever possible; words not\n            found in `initial_embedding_dict` are randomly initialized.\n\n        fixed_initialization : dict or None (default: None)\n            If a dict, this will replace the random initializations\n            of `W`, `C`, `bw` and `bc`. Dict keys must be\n            ['W', 'C', 'bw', 'bc'], and values should be np.arrays\n            of appropriate size ((n_words, n) for `W` and `C`, and\n            (n_words, ) for `bw` and `bc`).\n\n        Returns\n        -------\n        np.array\n            Of shape = [n_words, embedding_dim]. I.e. each row is the\n            embedding of the corresponding element in `vocab`.\n\n        \"\"\"\n        if fixed_initialization is not None:\n            assert self.test_mode, \\\n                \"Fixed initialization parameters can only be provided\" \\\n                \" in test mode. Initialize {} with `test_mode=True`.\". \\\n                     format(self.__class__.split(\".\")[-1])\n        self._check_dimensions(\n            X, vocab, initial_embedding_dict\n        )\n        weights, log_coincidence = self._initialize(X)\n        return self._fit(X, weights, log_coincidence,\n                         vocab=vocab,\n                         initial_embedding_dict=initial_embedding_dict,\n                         fixed_initialization=fixed_initialization)", "response": "Fit GloVe to the matrix X."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a progress bar with current loss.", "response": "def _progressbar(self, msg, iter_num):\n        \"\"\"Display a progress bar with current loss.\n\n        Parameters\n        ----------\n        msg : str\n            Message to print alongside the progress bar\n\n        iter_num : int\n            Iteration number.\n            Progress is only printed if this is a multiple of\n            `self.display_progress`.\n\n        \"\"\"\n        if self.display_progress and \\\n                                (iter_num + 1) % self.display_progress == 0:\n            sys.stderr.write('\\r')\n            sys.stderr.write(\"Iteration {}: {}\".format(iter_num + 1, msg))\n            sys.stderr.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns GloVe and return the new matrix.", "response": "def fit(self, X, fixed_initialization=None):\n        \"\"\"Run GloVe and return the new matrix.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_words, n_words]\n            The square count matrix.\n\n        fixed_initialization : None or dict (default: None)\n            If a dict, this will replace the random initializations\n            of `W`, `C`, `bw` and `bc`. Dict keys must be\n            ['W', 'C', 'bw', 'bc'], and values should be np.arrays\n            of appropriate size ((n_words, n) for `W` and `C`, and\n            (n_words, ) for `bw` and `bc`).\n\n        Returns\n        -------\n        np.array\n            The learned vectors, of dimensionality `(len(X), self.n)`\n            I.e. each row is the embedding of the corresponding element\n            of X.\n        \"\"\"\n        return super(GloVeBase, self).fit(\n            X, fixed_initialization=fixed_initialization)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the computatation graph for the object.", "response": "def _build_graph(self, vocab, initial_embedding_dict):\n        \"\"\"Builds the computatation graph.\n\n        Parameters\n        ------------\n        vocab : Iterable\n        initial_embedding_dict : dict\n        \"\"\"\n        # Constants\n        self.ones = tf.ones([self.n_words, 1])\n\n        # Parameters:\n        if initial_embedding_dict is None:\n            # Ordinary GloVe\n            self.W = self._weight_init(self.n_words, self.n, 'W')\n            self.C = self._weight_init(self.n_words, self.n, 'C')\n        else:\n            # This is the case where we have values to use as a\n            # \"warm start\":\n            self.n = len(next(iter(initial_embedding_dict.values())))\n            W = randmatrix(len(vocab), self.n)\n            C = randmatrix(len(vocab), self.n)\n            self.original_embedding = np.zeros((len(vocab), self.n))\n            self.has_embedding = np.zeros(len(vocab))\n            for i, w in enumerate(vocab):\n                if w in initial_embedding_dict:\n                    self.has_embedding[i] = 1.0\n                    embedding = np.array(initial_embedding_dict[w])\n                    self.original_embedding[i] = embedding\n                    # Divide the original embedding into W and C,\n                    # plus some noise to break the symmetry that would\n                    # otherwise cause both gradient updates to be\n                    # identical.\n                    W[i] = 0.5 * embedding + noise(self.n)\n                    C[i] = 0.5 * embedding + noise(self.n)\n            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n            self.C = tf.Variable(C, name='C', dtype=tf.float32)\n            self.original_embedding = tf.constant(self.original_embedding,\n                                                  dtype=tf.float32)\n            self.has_embedding = tf.constant(self.has_embedding,\n                                             dtype=tf.float32)\n            # This is for testing. It differs from\n            # `self.original_embedding` only in that it includes the\n            # random noise we added above to break the symmetry.\n            self.G_start = W + C\n\n        self.bw = self._weight_init(self.n_words, 1, 'bw')\n        self.bc = self._weight_init(self.n_words, 1, 'bc')\n\n        self.model = tf.tensordot(self.W, tf.transpose(self.C), axes=1) + \\\n                     tf.tensordot(self.bw, tf.transpose(self.ones), axes=1) + \\\n                     tf.tensordot(self.ones, tf.transpose(self.bc), axes=1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_cost_function(self):\n        self.weights = tf.placeholder(\n            tf.float32, shape=[self.n_words, self.n_words])\n        self.log_coincidence = tf.placeholder(\n            tf.float32, shape=[self.n_words, self.n_words])\n        self.diffs = tf.subtract(self.model, self.log_coincidence)\n        cost = tf.reduce_sum(\n            0.5 * tf.multiply(self.weights, tf.square(self.diffs)))\n        if self.mittens > 0:\n            self.mittens = tf.constant(self.mittens, tf.float32)\n            cost += self.mittens * tf.reduce_sum(\n                tf.multiply(\n                    self.has_embedding,\n                    self._tf_squared_euclidean(\n                        tf.add(self.W, self.C),\n                        self.original_embedding)))\n        tf.summary.scalar(\"cost\", cost)\n        return cost", "response": "Compute the cost of the Mittens objective function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsquaring Euclidean distance between the rows of X and Y.", "response": "def _tf_squared_euclidean(X, Y):\n        \"\"\"Squared Euclidean distance between the rows of `X` and `Y`.\n        \"\"\"\n        return tf.reduce_sum(tf.pow(tf.subtract(X, Y), 2), axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_optimizer(self):\n        optim = tf.train.AdagradOptimizer(self.learning_rate)\n        gradients = optim.compute_gradients(self.cost)\n        if self.log_dir:\n            for name, (g, v) in zip(['W', 'C', 'bw', 'bc'], gradients):\n                tf.summary.histogram(\"{}_grad\".format(name), g)\n                tf.summary.histogram(\"{}_vals\".format(name), v)\n        return optim.apply_gradients(gradients)", "response": "Uses Adagrad to optimize the GloVe objective and return the optimizer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the weights for a specific resource.", "response": "def _weight_init(self, m, n, name):\n        \"\"\"\n        Uses the Xavier Glorot method for initializing weights. This is\n        built in to TensorFlow as `tf.contrib.layers.xavier_initializer`,\n        but it's nice to see all the details.\n        \"\"\"\n        x = np.sqrt(6.0/(m+n))\n        with tf.name_scope(name) as scope:\n            return tf.Variable(\n                tf.random_uniform(\n                    [m, n], minval=-x, maxval=x), name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef round_robin(self, x, y, n_keep):\n        vals = y.unique()\n        scores = {}\n        for cls in vals:\n            scores[cls] = self.rank(x, np.equal(cls, y).astype('Int64'))\n            scores[cls].reverse()\n        keepers = set()\n        while len(keepers) < n_keep:\n            for cls in vals:\n                keepers.add(scores[cls].pop()[1])\n        return list(keepers)", "response": "Finds all classes with strong features and returns a list of all the keepers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump a pickle object to a file.", "response": "def dumppickle(obj, fname, protocol=-1):\n    \"\"\"\n    Pickle object `obj` to file `fname`.\n    \"\"\"\n    with open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows\n        pickle.dump(obj, fout, protocol=protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_maxprob(self, x, **kwargs):\n        return self.base_estimator_.predict(x.values, **kwargs)", "response": "Predict the maximum probability of a set of entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pprint(params):\n    params_list = list()\n    line_sep = ','\n    for i, (k, v) in enumerate(sorted(params.iteritems())):\n        if type(v) is float:\n            # use str for representing floating point numbers\n            # this way we get consistent representation across\n            # architectures and versions.\n            this_repr = '%s=%.10f' % (k, v)\n        else:\n            # use repr of the rest\n            this_repr = '%s=%r' % (k, v)\n        params_list.append(this_repr)\n\n    lines = ','.join(params_list)\n    return lines", "response": "prints object state in stable manner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cross_validate(data=None, folds=5, repeat=1, metrics=None,\n                   reporters=None, model_def=None, **kwargs):\n    \"\"\"Shortcut to cross-validate a single configuration.\n\n    ModelDefinition variables are passed in as keyword args, along\n    with the cross-validation parameters.\n    \"\"\"\n    md_kwargs = {}\n    if model_def is None:\n        for arg in ModelDefinition.params:\n            if arg in kwargs:\n                md_kwargs[arg] = kwargs.pop(arg)\n        model_def = ModelDefinition(**md_kwargs)\n    if metrics is None:\n        metrics = []\n    if reporters is None:\n        reporters = []\n    metrics = [MetricReporter(metric) for metric in metrics]\n    results = modeling.cross_validate(model_def, data, folds, repeat=repeat, **kwargs)\n    for r in reporters + metrics:\n        r.process_results(results)\n    return CVResult(results, reporters, metrics)", "response": "Shortcut to cross - validate a single configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef param_search(estimator, param_dict, n_iter=None, seed=None):\n\n    if n_iter is None:\n        param_iter = ParameterGrid(param_dict)\n    else:\n        param_iter = ParameterSampler(param_dict,\n                                      n_iter,\n                                      random_state=seed)\n\n    estimators = []\n    for params in param_iter:\n        new_estimator = sklearn.clone(estimator)\n        new_estimator.set_params(**params)\n        estimators.append(new_estimator)\n    return estimators", "response": "Generator for cloned copies of estimator with parameters\n    as specified by param_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the summary_df for the current cache curves.", "response": "def build_report(self):\n        \"\"\"\n        Calculates the pair of metrics for each threshold for each result.\n        \"\"\"\n        thresholds = self.thresholds\n        lower_quantile = self.config['lower_quantile']\n        upper_quantile = self.config['upper_quantile']\n\n        if self.n_current_results > self.n_cached_curves:\n            # If there are new curves, recompute\n            colnames = ['_'.join([metric, stat])\n                        for metric in [self.metric1.name, self.metric2.name]\n                        for stat in ['Mean', 'Median',\n                                     '%d_Percentile' % (100*lower_quantile),\n                                     '%d_Percentile' % (upper_quantile*100)]]\n            self.ret = pd.DataFrame(columns=colnames, index=thresholds, dtype='float64')\n\n            for threshold in thresholds:\n                m1s = Series([self.metric1.score(result, threshold) for result in self.results])\n                m2s = Series([self.metric2.score(result, threshold) for result in self.results])\n                self.ret.loc[threshold] = (m1s.mean(), m1s.quantile(.5), m1s.quantile(.05), m1s.quantile(.95),\n                                           m2s.mean(), m2s.quantile(.5), m2s.quantile(.05), m2s.quantile(.95))\n        self.build_curves()\n        self.summary_df = self.ret\n        return self.ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef model_definition_factory(base_model_definition, **kwargs):\n    if not kwargs:\n        yield config\n    else:\n        for param in kwargs:\n            if not hasattr(base_model_definition, param):\n                raise ValueError(\"'%s' is not a valid configuration parameter\" % param)\n\n        for raw_params in itertools.product(*kwargs.values()):\n            new_definition = copy.copy(base_model_definition)\n            new_definition.update(dict(zip(kwargs.keys(), raw_params)))\n            yield new_definition", "response": "Generates an iterator over passed - in base model definition values allowing for easy\n    exploration of models."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summary(self):\n        if self.features is not None:\n            feature_count = len(self.features)\n        else:\n            feature_count = 0\n        feature_hash = 'feathash:' + str(hash(tuple(self.features)))\n        return (str(self.estimator), feature_count, feature_hash, self.target)", "response": "Summary of the model definition for labeling."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, dct):\n        d = self.__dict__.copy()\n        d.update(dct)\n        self.set_attrs(**d)", "response": "Update the configuration with new parameters. Must use same\n        kwargs as __init__"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string that can be used to rename the column name of the current object.", "response": "def column_rename(self, existing_name, hsh=None):\n        \"\"\"\n        Like unique_name, but in addition must be unique to each column of this\n        feature. accomplishes this by prepending readable string to existing\n        column name and replacing unique hash at end of column name.\n        \"\"\"\n        try:\n            existing_name = str(existing_name)\n        except UnicodeEncodeError:\n            pass\n        if hsh is None:\n            hsh = self._hash()\n        if self._name:\n            return '%s(%s) [%s]' %(self._name, self._remove_hashes(existing_name),\n                    hsh)\n        return '%s [%s]'%(self._remove_hashes(existing_name),\n                    hsh)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing a signature to a normalized lower - S form.", "response": "def ecdsa_signature_normalize(self, raw_sig, check_only=False):\n        \"\"\"\n        Check and optionally convert a signature to a normalized lower-S form.\n        If check_only is True then the normalized signature is not returned.\n\n        This function always return a tuple containing a boolean (True if\n        not previously normalized or False if signature was already\n        normalized), and the normalized signature. When check_only is True,\n        the normalized signature returned is always None.\n        \"\"\"\n        if check_only:\n            sigout = ffi.NULL\n        else:\n            sigout = ffi.new('secp256k1_ecdsa_signature *')\n\n        result = lib.secp256k1_ecdsa_signature_normalize(\n            self.ctx, sigout, raw_sig)\n\n        return (bool(result), sigout if sigout != ffi.NULL else None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schnorr_partial_combine(self, schnorr_sigs):\n        if not HAS_SCHNORR:\n            raise Exception(\"secp256k1_schnorr not enabled\")\n        assert len(schnorr_sigs) > 0\n\n        sig64 = ffi.new('char [64]')\n        sig64sin = []\n        for sig in schnorr_sigs:\n            if not isinstance(sig, bytes):\n                raise TypeError('expected bytes, got {}'.format(type(sig)))\n            if len(sig) != 64:\n                raise Exception('invalid signature length')\n            sig64sin.append(ffi.new('char []', sig))\n\n        res = lib.secp256k1_schnorr_partial_combine(\n            self.ctx, sig64, sig64sin, len(sig64sin))\n        if res <= 0:\n            raise Exception('failed to combine signatures ({})'.format(res))\n\n        return bytes(ffi.buffer(sig64, 64))", "response": "Combine multiple Schnorr partial signatures."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef combine(self, pubkeys):\n        assert len(pubkeys) > 0\n\n        outpub = ffi.new('secp256k1_pubkey *')\n        for item in pubkeys:\n            assert ffi.typeof(item) is ffi.typeof('secp256k1_pubkey *')\n\n        res = lib.secp256k1_ec_pubkey_combine(\n            self.ctx, outpub, pubkeys, len(pubkeys))\n        if not res:\n            raise Exception('failed to combine public keys')\n\n        self.public_key = outpub\n        return outpub", "response": "Add a number of public keys together."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a nonce pair deterministically for use with with .", "response": "def schnorr_generate_nonce_pair(self, msg, raw=False,\n                                    digest=hashlib.sha256):\n        \"\"\"\n        Generate a nonce pair deterministically for use with\n        schnorr_partial_sign.\n        \"\"\"\n        if not HAS_SCHNORR:\n            raise Exception(\"secp256k1_schnorr not enabled\")\n\n        msg32 = _hash32(msg, raw, digest)\n        pubnonce = ffi.new('secp256k1_pubkey *')\n        privnonce = ffi.new('char [32]')\n\n        valid = lib.secp256k1_schnorr_generate_nonce_pair(\n            self.ctx, pubnonce, privnonce, msg32, self.private_key,\n            ffi.NULL, ffi.NULL)\n        assert valid == 1\n\n        return pubnonce, privnonce"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce a partial Schnorr signature for the message msg.", "response": "def schnorr_partial_sign(self, msg, privnonce, pubnonce_others,\n                             raw=False, digest=hashlib.sha256):\n        \"\"\"\n        Produce a partial Schnorr signature, which can be combined using\n        schnorr_partial_combine to end up with a full signature that is\n        verifiable using PublicKey.schnorr_verify.\n\n        To combine pubnonces, use PublicKey.combine.\n\n        Do not pass the pubnonce produced for the respective privnonce;\n        combine the pubnonces from other signers and pass that instead.\n        \"\"\"\n        if not HAS_SCHNORR:\n            raise Exception(\"secp256k1_schnorr not enabled\")\n\n        msg32 = _hash32(msg, raw, digest)\n        sig64 = ffi.new('char [64]')\n\n        res = lib.secp256k1_schnorr_partial_sign(\n            self.ctx, sig64, msg32, self.private_key,\n            pubnonce_others, privnonce)\n        if res <= 0:\n            raise Exception('failed to partially sign ({})'.format(res))\n\n        return bytes(ffi.buffer(sig64, 64))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_flags(library, type_, path):\n\n    pkg_config_path = [path]\n    if \"PKG_CONFIG_PATH\" in os.environ:\n        pkg_config_path.append(os.environ['PKG_CONFIG_PATH'])\n    if \"LIB_DIR\" in os.environ:\n        pkg_config_path.append(os.environ['LIB_DIR'])\n        pkg_config_path.append(os.path.join(os.environ['LIB_DIR'], \"pkgconfig\"))\n\n    options = [\n        \"--static\",\n        {\n            'I': \"--cflags-only-I\",\n            'L': \"--libs-only-L\",\n            'l': \"--libs-only-l\"\n        }[type_]\n    ]\n\n    return [\n        flag.strip(\"-{}\".format(type_))\n        for flag\n        in subprocess.check_output(\n            [\"pkg-config\"] + options + [library],\n            env=dict(os.environ, PKG_CONFIG_PATH=\":\".join(pkg_config_path))\n        ).decode(\"UTF-8\").split()\n    ]", "response": "Return separated build flags from pkg - config output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the command line arguments and run the command.", "response": "def run(self, default=None):\n        \"\"\"Parse the command line arguments.\n\n        default:\n            Name of default command to run if no arguments are passed.\n\n        \"\"\"\n        parent, *sys_args = sys.argv\n        self.parent = Path(parent).stem\n        cmd_name = default\n        if sys_args:\n            cmd_name, *sys_args = sys_args\n\n        if cmd_name is None or cmd_name.lstrip(\"-\") in HELP_COMMANDS:\n            self.show_help_root()\n            return\n\n        command = self.commands.get(cmd_name)\n        if command is None:\n            self.show_error(f\"command `{cmd_name}` not found\")\n            self.show_help_root()\n            return\n\n        args, opts = parse_args(sys_args)\n        return command.run(*args, **opts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_doc(func):\n    doc = func.__doc__\n    if not doc:\n        return \"\"\n\n    # doc has only one line\n    if \"\\n\" not in doc:\n        return doc\n\n    # Only Python core devs write __doc__ like this\n    if doc.startswith((\"\\n\", \"\\\\\\n\")):\n        return textwrap.dedent(doc)\n\n    # First line is not indented\n    first, rest = doc.split(\"\\n\", 1)\n    return first + \"\\n\" + textwrap.dedent(rest)", "response": "Extract and dedent the __doc__ of a function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_args(cliargs):\n    # Split the \"key=arg\" arguments\n    largs = []\n    for arg in cliargs:\n        if \"=\" in arg:\n            key, arg = arg.split(\"=\")\n            largs.append(key)\n        largs.append(arg)\n\n    args = []\n    flags = []\n    kwargs = {}\n    key = None\n    for sarg in largs:\n        if is_key(sarg):\n            if key is not None:\n                flags.append(key)\n            key = sarg.strip(\"-\")\n            continue\n\n        if not key:\n            args.append(sarg)\n            continue\n\n        value = kwargs.get(key)\n        if value:\n            if isinstance(value, list):\n                value.append(sarg)\n            else:\n                value = [value, sarg]\n            kwargs[key] = value\n        else:\n            kwargs[key] = sarg\n\n    # Get the flags\n    if key:\n        flags.append(key)\n    # An extra key whitout a value is a flag if it hasn\"t been used before.\n    # Otherwise is a typo.\n    for flag in flags:\n        if not kwargs.get(flag):\n            kwargs[flag] = True\n\n    return args, kwargs", "response": "Parse the command line arguments and return a list of the positional arguments and a dictionary with the named ones."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if sarg is a key.", "response": "def is_key(sarg):\n    \"\"\"Check if `sarg` is a key (eg. -foo, --foo) or a negative number (eg. -33).\n    \"\"\"\n    if not sarg.startswith(\"-\"):\n        return False\n    if sarg.startswith(\"--\"):\n        return True\n    return not sarg.lstrip(\"-\").isnumeric()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef param(name, help=\"\"):\n    def decorator(func):\n        params = getattr(func, \"params\", [])\n        _param = Param(name, help)\n        # Insert at the beginning so the apparent order is preserved\n        params.insert(0, _param)\n        func.params = params\n        return func\n\n    return decorator", "response": "Decorator that adds a parameter to the wrapped command or function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef option(name, help=\"\"):\n    def decorator(func):\n        options = getattr(func, \"options\", [])\n        _option = Param(name, help)\n        # Insert at the beginning so the apparent order is preserved\n        options.insert(0, _option)\n        func.options = options\n        return func\n\n    return decorator", "response": "Decorator that adds an option to the wrapped command or function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_nvra(nvra):\n    if nvra.endswith(\".rpm\"):\n        nvra = nvra[:-4]\n    result = RPM_NVRA_RE.match(nvra).groupdict()\n    result[\"epoch\"] = result[\"epoch\"] or 0\n    result[\"epoch\"] = int(result[\"epoch\"])\n    return result", "response": "Parses the N - E V - R. A string to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_version(version):\n    if re.match(\"^[^0-9].*\", version):\n        return [version]\n    return [int(i) for i in version.split(\".\")]", "response": "Split version to a list of integers\n    that can be easily compared."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_major_version(version, remove=None):\n    if remove:\n        warnings.warn(\"remove argument is deprecated\", DeprecationWarning)\n    version_split = version.split(\".\")\n    return version_split[0]", "response": "Return the major version of a version string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the minor version of a version string.", "response": "def get_minor_version(version, remove=None):\n    \"\"\"Return minor version of a provided version string. Minor version is the\n    second component in the dot-separated version string. For non-version-like\n    strings this function returns ``None``.\n\n    The ``remove`` parameter is deprecated since version 1.18 and will be\n    removed in the future.\n\n    :param version: Version string\n    :type version: str\n    :rtype: str\n    \"\"\"\n    if remove:\n        warnings.warn(\"remove argument is deprecated\", DeprecationWarning)\n    version_split = version.split(\".\")\n    try:\n        # Assume MAJOR.MINOR.REST...\n        return version_split[1]\n    except IndexError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating release_id from given parts.", "response": "def create_release_id(short, version, type, bp_short=None, bp_version=None, bp_type=None):\n    \"\"\"\n    Create release_id from given parts.\n\n    :param short: Release short name\n    :type short: str\n    :param version: Release version\n    :type version: str\n    :param version: Release type\n    :type version: str\n    :param bp_short: Base Product short name\n    :type bp_short: str\n    :param bp_version: Base Product version\n    :type bp_version: str\n    :param bp_type: Base Product type\n    :rtype: str\n    \"\"\"\n    if not is_valid_release_short(short):\n        raise ValueError(\"Release short name is not valid: %s\" % short)\n    if not is_valid_release_version(version):\n        raise ValueError(\"Release short version is not valid: %s\" % version)\n    if not is_valid_release_type(type):\n        raise ValueError(\"Release type is not valid: %s\" % type)\n\n    if type == \"ga\":\n        result = \"%s-%s\" % (short, version)\n    else:\n        result = \"%s-%s-%s\" % (short, version, type)\n\n    if bp_short:\n        result += \"@%s\" % create_release_id(bp_short, bp_version, bp_type)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a release_id into a dict of parts.", "response": "def parse_release_id(release_id):\n    \"\"\"\n    Parse release_id to parts:\n    {short, version, type}\n    or\n    {short, version, type, bp_short, bp_version, bp_type}\n\n    :param release_id: Release ID string\n    :type release_id: str\n    :rtype: dict\n    \"\"\"\n    if \"@\" in release_id:\n        release, base_product = release_id.split(\"@\")\n    else:\n        release = release_id\n        base_product = None\n\n    result = _parse_release_id_part(release)\n    if base_product is not None:\n        result.update(_parse_release_id_part(base_product, prefix=\"bp_\"))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _assert_matches_re(self, field, expected_patterns):\n        value = getattr(self, field)\n        for pattern in expected_patterns:\n            try:\n                if pattern.match(value):\n                    return\n            except AttributeError:\n                # It's not a compiled regex, treat it as string.\n                if re.match(pattern, value):\n                    return\n        raise ValueError(\"%s: Field '%s' has invalid value: %s. It does not match any provided REs: %s\"\n                         % (self.__class__.__name__, field, value, expected_patterns))", "response": "Assert that the value of a field matches one of the expected patterns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate attributes by running all self. _validate_* methods.", "response": "def validate(self):\n        \"\"\"\n        Validate attributes by running all self._validate_*() methods.\n\n        :raises TypeError: if an attribute has invalid type\n        :raises ValueError: if an attribute contains invalid value\n        \"\"\"\n        method_names = sorted([i for i in dir(self) if i.startswith(\"_validate\") and callable(getattr(self, i))])\n        for method_name in method_names:\n            method = getattr(self, method_name)\n            method()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, f):\n        with _open_file_obj(f) as f:\n            parser = self.parse_file(f)\n            self.deserialize(parser)", "response": "Load data from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loads(self, s):\n        io = six.StringIO()\n        io.write(s)\n        io.seek(0)\n        self.load(io)\n        self.validate()", "response": "Load data from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps data to a file.", "response": "def dump(self, f):\n        \"\"\"\n        Dump data to a file.\n\n        :param f: file-like object or path to file\n        :type f: file or str\n        \"\"\"\n        self.validate()\n        with _open_file_obj(f, \"w\") as f:\n            parser = self._get_parser()\n            self.serialize(parser)\n            self.build_file(parser, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dumps(self):\n        io = six.StringIO()\n        self.dump(io)\n        io.seek(0)\n        return io.read()", "response": "Dump the current object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self):\n        if self._composeinfo is not None:\n            return self._composeinfo\n\n        paths = [\n            \"metadata/composeinfo.json\",\n        ]\n        self._composeinfo = self._load_metadata(paths, productmd.composeinfo.ComposeInfo)\n        return self._composeinfo", "response": "Returns a dict of ComposeInfo objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef images(self):\n        if self._images is not None:\n            return self._images\n\n        paths = [\n            \"metadata/images.json\",\n            \"metadata/image-manifest.json\",\n        ]\n        self._images = self._load_metadata(paths, productmd.images.Images)\n        return self._images", "response": "Returns a list of images metadata for the current application."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of RPMs metadata objects.", "response": "def rpms(self):\n        \"\"\"(:class:`productmd.rpms.Rpms`) -- Compose RPMs metadata\"\"\"\n        if self._rpms is not None:\n            return self._rpms\n\n        paths = [\n            \"metadata/rpms.json\",\n            \"metadata/rpm-manifest.json\",\n        ]\n        self._rpms = self._load_metadata(paths, productmd.rpms.Rpms)\n        return self._rpms"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modules(self):\n        if self._modules is not None:\n            return self._modules\n\n        paths = [\n            \"metadata/modules.json\",\n        ]\n        self._modules = self._load_metadata(paths, productmd.modules.Modules)\n        return self._modules", "response": "Returns a list of modules metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_version(self):\n        self._assert_type(\"version\", list(six.string_types))\n        self._assert_matches_re(\"version\", [RELEASE_VERSION_RE])", "response": "Validate that the version is a sematic - versioning\n style string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_variants(self, arch=None, types=None, recursive=False):\n        types = types or []\n        result = []\n\n        if \"self\" in types:\n            result.append(self)\n\n        for variant in six.itervalues(self.variants):\n            if types and variant.type not in types:\n                continue\n            if arch and arch not in variant.arches.union([\"src\"]):\n                continue\n            result.append(variant)\n            if recursive:\n                result.extend(variant.get_variants(types=[i for i in types if i != \"self\"], recursive=True))\n\n        result.sort(key=lambda x: x.uid)\n        return result", "response": "Return all variants of given arch and types."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new entry to the map.", "response": "def add(self, variant, arch, nevra, path, sigkey, category, srpm_nevra=None):\n        \"\"\"\n        Map RPM to to variant and arch.\n\n        :param variant: compose variant UID\n        :type  variant: str\n        :param arch:    compose architecture\n        :type  arch:    str\n        :param nevra:   name-epoch:version-release.arch\n        :type  nevra:   str\n        :param sigkey:  sigkey hash\n        :type  sigkey:  str or None\n        :param category:    RPM category, one of binary, debug, source\n        :type  category:    str\n        :param srpm_nevra:  name-epoch:version-release.arch of RPM's SRPM\n        :type  srpm_nevra:  str\n        \"\"\"\n\n        if arch not in productmd.common.RPM_ARCHES:\n            raise ValueError(\"Arch not found in RPM_ARCHES: %s\" % arch)\n\n        if arch in [\"src\", \"nosrc\"]:\n            raise ValueError(\"Source arch is not allowed. Map source files under binary arches.\")\n\n        if category not in SUPPORTED_CATEGORIES:\n            raise ValueError(\"Invalid category value: %s\" % category)\n\n        if path.startswith(\"/\"):\n            raise ValueError(\"Relative path expected: %s\" % path)\n\n        nevra, nevra_dict = self._check_nevra(nevra)\n\n        if category == \"source\" and srpm_nevra is not None:\n            raise ValueError(\"Expected blank srpm_nevra for source package: %s\" % nevra)\n\n        if category != \"source\" and srpm_nevra is None:\n            raise ValueError(\"Missing srpm_nevra for package: %s\" % nevra)\n\n        if (category == \"source\") != (nevra_dict[\"arch\"] in (\"src\", \"nosrc\")):\n            raise ValueError(\"Invalid category/arch combination: %s/%s\" % (category, nevra))\n\n        if sigkey is not None:\n            sigkey = sigkey.lower()\n\n        if srpm_nevra:\n            srpm_nevra, _ = self._check_nevra(srpm_nevra)\n        else:\n            srpm_nevra = nevra\n\n        arches = self.rpms.setdefault(variant, {})\n        srpms = arches.setdefault(arch, {})\n        rpms = srpms.setdefault(srpm_nevra, {})\n        rpms[nevra] = {\"sigkey\": sigkey, \"path\": path, \"category\": category}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproviding a tuple of image s UNIQUE_IMAGE_ATTRIBUTES.", "response": "def identify_image(image):\n    \"\"\"Provides a tuple of image's UNIQUE_IMAGE_ATTRIBUTES. Note:\n    this is not guaranteed to be unique (and will often not be)\n    for pre-1.1 metadata, as subvariant did not exist. Provided as\n    a function so consumers can use it on plain image dicts read from\n    the metadata or PDC.\n    \"\"\"\n    try:\n        # Image instance case\n        attrs = tuple(getattr(image, attr) for attr in UNIQUE_IMAGE_ATTRIBUTES)\n    except AttributeError:\n        # Plain dict case\n        attrs = tuple(image.get(attr, None) for attr in UNIQUE_IMAGE_ATTRIBUTES)\n    ui = UniqueImage(*attrs)\n    # If unified is None (which could happen in the dict case, we want default\n    # value of False instead. Also convert additional_variants to a list.\n    return ui._replace(\n        unified=ui.unified or False, additional_variants=ui.additional_variants or []\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, variant, arch, image):\n\n        if arch not in productmd.common.RPM_ARCHES:\n            raise ValueError(\"Arch not found in RPM_ARCHES: %s\" % arch)\n        if arch in [\"src\", \"nosrc\"]:\n            raise ValueError(\"Source arch is not allowed. Map source files under binary arches.\")\n        if self.header.version_tuple >= (1, 1):\n            # disallow adding a different image with same 'unique'\n            # attributes. can't do this pre-1.1 as we couldn't truly\n            # identify images before subvariant\n            for checkvar in self.images:\n                for checkarch in self.images[checkvar]:\n                    for curimg in self.images[checkvar][checkarch]:\n                        if identify_image(curimg) == identify_image(image) and curimg.checksums != image.checksums:\n                            raise ValueError(\"Image {0} shares all UNIQUE_IMAGE_ATTRIBUTES with \"\n                                             \"image {1}! This is forbidden.\".format(image, curimg))\n        self.images.setdefault(variant, {}).setdefault(arch, set()).add(image)", "response": "Add an image to the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a mopidy album to a mopidy ref.", "response": "def album_to_ref(album):\n    \"\"\"Convert a mopidy album to a mopidy ref.\"\"\"\n    name = ''\n    for artist in album.artists:\n        if len(name) > 0:\n            name += ', '\n        name += artist.name\n    if (len(name)) > 0:\n        name += ' - '\n    if album.name:\n        name += album.name\n    else:\n        name += 'Unknown Album'\n    return Ref.directory(uri=album.uri, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef artist_to_ref(artist):\n    if artist.name:\n        name = artist.name\n    else:\n        name = 'Unknown artist'\n    return Ref.directory(uri=artist.uri, name=name)", "response": "Convert a mopidy artist to a mopidy ref."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef track_to_ref(track, with_track_no=False):\n    if with_track_no and track.track_no > 0:\n        name = '%d - ' % track.track_no\n    else:\n        name = ''\n    for artist in track.artists:\n        if len(name) > 0:\n            name += ', '\n        name += artist.name\n    if (len(name)) > 0:\n        name += ' - '\n    name += track.name\n    return Ref.track(uri=track.uri, name=name)", "response": "Convert a mopidy track to a mopidy ref."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a child HTTPAPI instance for a given user.", "response": "def user(self, user: str) -> \"ChildHTTPAPI\":\n        \"\"\"\n        Get a child HTTPAPI instance.\n\n        Args:\n            user: The Matrix ID of the user whose API to get.\n\n        Returns:\n            A HTTPAPI instance that always uses the given Matrix ID.\n        \"\"\"\n        if self.is_real_user:\n            raise ValueError(\"Can't get child of real user\")\n\n        try:\n            return self.children[user]\n        except KeyError:\n            child = ChildHTTPAPI(user, self)\n            self.children[user] = child\n            return child"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bot_intent(self) -> \"IntentAPI\":\n        if not self._bot_intent:\n            self._bot_intent = IntentAPI(self.bot_mxid, self, state_store=self.state_store,\n                                         log=self.intent_log)\n        return self._bot_intent", "response": "Returns the IntentAPI for the bot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intent(self, user: str = None, token: Optional[str] = None) -> \"IntentAPI\":\n        if self.is_real_user:\n            raise ValueError(\"Can't get child intent of real user\")\n        if token:\n            return IntentAPI(user, self.real_user(user, token), self.bot_intent(), self.state_store,\n                             self.intent_log)\n        return IntentAPI(user, self.user(user), self.bot_intent(), self.state_store,\n                         self.intent_log)", "response": "Get the intent API for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user(self, user: str, token: Optional[str] = None) -> \"IntentAPI\":\n        if not self.bot:\n            return self.client.intent(user, token)\n        else:\n            self.log.warning(\"Called IntentAPI#user() of child intent object.\")\n            return self.bot.client.intent(user, token)", "response": "Get the intent API for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of rooms the user is in. See also : _API reference _", "response": "async def get_joined_rooms(self) -> List[str]:\n        \"\"\"\n        Get the list of rooms the user is in. See also: `API reference`_\n\n        Returns:\n            The list of room IDs the user is in.\n\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#get-matrix-client-r0-joined-rooms\n        \"\"\"\n        await self.ensure_registered()\n        response = await self.client.request(\"GET\", \"/joined_rooms\")\n        return response[\"joined_rooms\"]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def set_display_name(self, name: str):\n        await self.ensure_registered()\n        content = {\"displayname\": name}\n        await self.client.request(\"PUT\", f\"/profile/{self.mxid}/displayname\", content)", "response": "Set the display name of the user. See also : _API reference _\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the online status of the user.", "response": "async def set_presence(self, status: str = \"online\", ignore_cache: bool = False):\n        \"\"\"\n        Set the online status of the user. See also: `API reference`_\n\n        Args:\n            status: The online status of the user. Allowed values: \"online\", \"offline\", \"unavailable\".\n            ignore_cache: Whether or not to set presence even if the cache says the presence is\n                already set to that value.\n\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#put-matrix-client-r0-presence-userid-status\n        \"\"\"\n        await self.ensure_registered()\n        if not ignore_cache and self.state_store.has_presence(self.mxid, status):\n            return\n\n        content = {\n            \"presence\": status\n        }\n        resp = await self.client.request(\"PUT\", f\"/presence/{self.mxid}/status\", content)\n        self.state_store.set_presence(self.mxid, status)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the avatar of the user.", "response": "async def set_avatar(self, url: str):\n        \"\"\"\n        Set the avatar of the user. See also: `API reference`_\n\n        Args:\n            url: The new avatar URL for the user. Must be a MXC URI.\n\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#put-matrix-client-r0-profile-userid-avatar-url\n        \"\"\"\n        await self.ensure_registered()\n        content = {\"avatar_url\": url}\n        await self.client.request(\"PUT\", f\"/profile/{self.mxid}/avatar_url\", content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def upload_file(self, data: bytes, mime_type: Optional[str] = None) -> str:\n        await self.ensure_registered()\n        if magic:\n            mime_type = mime_type or magic.from_buffer(data, mime=True)\n        resp = await self.client.request(\"POST\", \"\", content=data,\n                                         headers={\"Content-Type\": mime_type},\n                                         api_path=\"/_matrix/media/r0/upload\")\n        try:\n            return resp[\"content_uri\"]\n        except KeyError:\n            raise MatrixResponseError(\"Media repo upload response did not contain content_uri.\")", "response": "Uploads a file to the content repository."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload a file from the content repository.", "response": "async def download_file(self, url: str) -> bytes:\n        \"\"\"\n        Download a file from the content repository. See also: `API reference`_\n\n        Args:\n            url: The MXC URI to download.\n\n        Returns:\n            The raw downloaded data.\n\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#get-matrix-media-r0-download-servername-mediaid\n        \"\"\"\n        await self.ensure_registered()\n        url = self.client.get_download_url(url)\n        async with self.client.session.get(url) as response:\n            return await response.read()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new room.", "response": "async def create_room(self, alias: Optional[str] = None, is_public: bool = False,\n                          name: Optional[str] = None, topic: Optional[str] = None,\n                          is_direct: bool = False, invitees: Optional[List[str]] = None,\n                          initial_state: Optional[List[dict]] = None) -> str:\n        \"\"\"\n        Create a new room. See also: `API reference`_\n\n        Args:\n            alias: The desired room alias **local part**. If this is included, a room alias will be\n                created and mapped to the newly created room. The alias will belong on the same\n                homeserver which created the room. For example, if this was set to \"foo\" and sent to\n                the homeserver \"example.com\" the complete room alias would be ``#foo:example.com``.\n            is_public: This flag sets the state event preset to ``public_chat``, which sets\n                ``join_rules`` to ``public``. Defaults to false, which sets ``join_rules`` to\n                ``invite``.\n            name: If this is included, an ``m.room.name`` event will be sent into the room to\n                indicate the name of the room. See `Room Events`_ for more information on\n                ``m.room.name``.\n            topic: If this is included, an ``m.room.topic`` event will be sent into the room to\n                indicate the topic for the room. See `Room Events`_ for more information on\n                ``m.room.topic``.\n            is_direct: This flag makes the server set the ``is_direct`` flag on the\n                ``m.room.member`` events sent to the users in ``invite`` and ``invite_3pid``. See\n                `Direct Messaging`_ for more information.\n            invitees: A list of user IDs to invite to the room. This will tell the server to invite\n                everyone in the list to the newly created room.\n            initial_state: A list of state events to set in the new room. This allows the user to\n                override the default state events set in the new room. The expected format of the\n                state events are an object with type, state_key and content keys set.\n\n                Takes precedence over events set by `is_public`, but gets overriden by ``name`` and\n                ``topic keys``.\n\n        Returns:\n            The ID of the newly created room.\n\n        Raises:\n            MatrixResponseError: If the response does not contain a ``room_id`` field.\n\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#post-matrix-client-r0-createroom\n        .. _Room Events:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#room-events\n        .. _Direct Messaging:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#direct-messaging\n        \"\"\"\n        await self.ensure_registered()\n        content = {\n            \"visibility\": \"private\",\n            \"is_direct\": is_direct,\n            \"preset\": \"public_chat\" if is_public else \"private_chat\",\n        }\n        if alias:\n            content[\"room_alias_name\"] = alias\n        if invitees:\n            content[\"invite\"] = invitees\n        if name:\n            content[\"name\"] = name\n        if topic:\n            content[\"topic\"] = topic\n        if initial_state:\n            content[\"initial_state\"] = initial_state\n\n        resp = await self.client.request(\"POST\", \"/createRoom\", content)\n        try:\n            return resp[\"room_id\"]\n        except KeyError:\n            raise MatrixResponseError(\"Room create response did not contain room_id.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvite a user to a particular room.", "response": "async def invite(self, room_id: str, user_id: str, check_cache: bool = False\n                     ) -> Optional[dict]:\n        \"\"\"\n        Invite a user to participate in a particular room. See also: `API reference`_\n\n        Args:\n            room_id: The room identifier (not alias) to which to invite the user.\n            user_id: The fully qualified user ID of the invitee.\n            check_cache: Whether or not to check the state cache before inviting.\n                If true, the actual invite HTTP request will only be made if the user is not in the\n                room according to local state caches.\n\n        Returns:\n        .. _API reference:\n           https://matrix.org/docs/spec/client_server/r0.3.0.html#post-matrix-client-r0-createroom\n        \"\"\"\n        await self.ensure_joined(room_id)\n        try:\n            ok_states = {\"invite\", \"join\"}\n            do_invite = (not check_cache\n                         or self.state_store.get_membership(room_id, user_id) not in ok_states)\n            if do_invite:\n                response = await self._invite_direct(room_id, user_id)\n                self.state_store.invited(room_id, user_id)\n                return response\n        except MatrixRequestError as e:\n            if e.errcode != \"M_FORBIDDEN\":\n                raise IntentError(f\"Failed to invite {user_id} to {room_id}\", e)\n            if \"is already in the room\" in e.message:\n                self.state_store.joined(room_id, user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_section(self, section):\n        section.rid = 0     \n        plen = 0\n\n        while self._merge and self._sections and plen != len(self._sections):\n            plen = len(self._sections)\n            self._sections = [s for s in self._sections if not section.join(s)]\n        self._sections.append(section)", "response": "Adds a new free section to the free section list. If merge is enabled the free section list is joined with all existing \n        sections until the operation fails."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _split_horizontal(self, section, width, height):\n        # First remove the section we are splitting so it doesn't \n        # interfere when later we try to merge the resulting split\n        # rectangles, with the rest of free sections.\n        #self._sections.remove(section)\n\n        # Creates two new empty sections, and returns the new rectangle.\n        if height < section.height:\n            self._add_section(Rectangle(section.x, section.y+height,\n                section.width, section.height-height))\n\n        if width < section.width:\n            self._add_section(Rectangle(section.x+width, section.y,\n                section.width-width, height))", "response": "Split a horizontal section into two sections and returns the new rectangle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_rect(self, width, height, rid=None):     \n        assert(width > 0 and height >0)\n\n        # Obtain the best section to place the rectangle.\n        section, rotated = self._select_fittest_section(width, height)\n        if not section:\n            return None\n        \n        if rotated:\n            width, height = height, width\n        \n        # Remove section, split and store results\n        self._sections.remove(section)\n        self._split(section, width, height)\n       \n        # Store rectangle in the selected position\n        rect = Rectangle(section.x, section.y, width, height, rid)\n        self.rectangles.append(rect)\n        return rect", "response": "Adds a rectangle to the current rectangle list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fitness(self, width, height):\n        assert(width > 0 and height > 0)\n\n        # Get best fitness section.\n        section, rotated = self._select_fittest_section(width, height)\n        if not section:\n            return None\n        \n        # Return fitness of returned section, with correct dimmensions if the\n        # the rectangle was rotated.\n        if rotated:\n            return self._section_fitness(section, height, width)\n        else:\n            return self._section_fitness(section, width, height)", "response": "Returns the minimum fitness of all \n        in guillotine algorithm case returns the min of the fitness of all \n        free sections for the given dimension both normal and rotated\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _container_candidates(self):\n        if not self._rectangles:\n            return []\n\n        if self._rotation:\n            sides = sorted(side for rect in self._rectangles for side in rect)\n            max_height = sum(max(r[0], r[1]) for r in self._rectangles)\n            min_width = max(min(r[0], r[1]) for r in self._rectangles)\n            max_width = max_height\n        else:\n            sides = sorted(r[0] for r in self._rectangles)\n            max_height = sum(r[1] for r in self._rectangles)\n            min_width = max(r[0] for r in self._rectangles)\n            max_width = sum(sides)\n        \n        if self._max_width and self._max_width < max_width:\n            max_width = self._max_width\n\n        if self._max_height and self._max_height < max_height:\n            max_height = self._max_height\n\n        assert(max_width>min_width)\n \n        # Generate initial container widths\n        candidates = [max_width, min_width]\n\n        width = 0\n        for s in reversed(sides):\n            width += s\n            candidates.append(width)\n        \n        width = 0\n        for s in sides:\n            width += s\n            candidates.append(width)\n\n        candidates.append(max_width)\n        candidates.append(min_width)\n      \n        # Remove duplicates and widths too big or small\n        seen = set()\n        seen_add = seen.add\n        candidates = [x for x in candidates if not(x in seen or seen_add(x))] \n        candidates = [x for x in candidates if not(x>max_width or x<min_width)]\n        \n        # Remove candidates too small to fit all the rectangles\n        min_area = sum(r[0]*r[1] for r in self._rectangles)\n        return [(c, max_height) for c in candidates if c*max_height>=min_area]", "response": "Generate a list of container candidates for a given set of image types."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a candidate size for the given width and height.", "response": "def _refine_candidate(self, width, height):\n        \"\"\"\n        Use bottom-left packing algorithm to find a lower height for the \n        container.\n\n        Arguments:\n            width\n            height\n\n        Returns:\n            tuple (width, height, PackingAlgorithm):\n        \"\"\"\n        packer = newPacker(PackingMode.Offline, PackingBin.BFF, \n                pack_algo=self._pack_algo, sort_algo=SORT_LSIDE,\n                rotation=self._rotation)\n        packer.add_bin(width, height)\n       \n        for r in self._rectangles:\n            packer.add_rect(*r)\n\n        packer.pack()\n\n        # Check all rectangles where packed\n        if len(packer[0]) != len(self._rectangles):\n            return None\n\n        # Find highest rectangle\n        new_height = max(packer[0], key=lambda x: x.top).top\n        return(width, new_height, packer)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the fitness value for a given rectangle.", "response": "def _rect_fitness(self, max_rect, width, height):\n        \"\"\"\n        Arguments:\n            max_rect (Rectangle): Destination max_rect\n            width (int, float): Rectangle width\n            height (int, float): Rectangle height\n\n        Returns:\n            None: Rectangle couldn't be placed into max_rect\n            integer, float: fitness value \n        \"\"\"\n        if width <= max_rect.width and height <= max_rect.height:\n            return 0\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating new maximal rectangles based on the placement of the maximal rectangle.", "response": "def _generate_splits(self, m, r):\n        \"\"\"\n        When a rectangle is placed inside a maximal rectangle, it stops being one\n        and up to 4 new maximal rectangles may appear depending on the placement.\n        _generate_splits calculates them.\n\n        Arguments:\n            m (Rectangle): max_rect rectangle\n            r (Rectangle): rectangle placed\n\n        Returns:\n            list : list containing new maximal rectangles or an empty list\n        \"\"\"\n        new_rects = []\n        \n        if r.left > m.left:\n            new_rects.append(Rectangle(m.left, m.bottom, r.left-m.left, m.height))\n        if r.right < m.right:\n            new_rects.append(Rectangle(r.right, m.bottom, m.right-r.right, m.height))\n        if r.top < m.top:\n            new_rects.append(Rectangle(m.left, r.top, m.width, m.top-r.top))\n        if r.bottom > m.bottom:\n            new_rects.append(Rectangle(m.left, m.bottom, m.width, r.bottom-m.bottom))\n        \n        return new_rects"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits all max_rects intersecting the rectangle rect into up to max_rects.", "response": "def _split(self, rect):\n        \"\"\"\n        Split all max_rects intersecting the rectangle rect into up to\n        4 new max_rects.\n        \n        Arguments:\n            rect (Rectangle): Rectangle\n\n        Returns:\n            split (Rectangle list): List of rectangles resulting from the split\n        \"\"\"\n        max_rects = collections.deque()\n\n        for r in self._max_rects:\n            if r.intersects(rect):\n                max_rects.extend(self._generate_splits(r, rect))\n            else:\n                max_rects.append(r)\n\n        # Add newly generated max_rects\n        self._max_rects = list(max_rects)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _remove_duplicates(self):\n        contained = set()\n        for m1, m2 in itertools.combinations(self._max_rects, 2):\n            if m1.contains(m2):\n                contained.add(m2)\n            elif m2.contains(m1):\n                contained.add(m1)\n        \n        # Remove from max_rects\n        self._max_rects = [m for m in self._max_rects if m not in contained]", "response": "Remove every maximal rectangle contained by another one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fitness(self, width, height): \n        assert(width > 0 and height > 0)\n        \n        rect, max_rect = self._select_position(width, height)\n        if rect is None:\n            return None\n\n        # Return fitness\n        return self._rect_fitness(max_rect, rect.width, rect.height)", "response": "Returns the fitness of the more \n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_rect(self, width, height, rid=None):\n        assert(width > 0 and height >0)\n\n        # Search best position and orientation\n        rect, _ = self._select_position(width, height)\n        if not rect:\n            return None\n        \n        # Subdivide all the max rectangles intersecting with the selected \n        # rectangle.\n        self._split(rect)\n    \n        # Remove any max_rect contained by another \n        self._remove_duplicates()\n\n        # Store and return rectangle position.\n        rect.rid = rid\n        self.rectangles.append(rect)\n        return rect", "response": "Adds a rectangle to the list of available rectangles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the position where the y coordinate of the top of the rectangle is lower.", "response": "def _select_position(self, w, h): \n        \"\"\"\n        Select the position where the y coordinate of the top of the rectangle\n        is lower, if there are severtal pick the one with the smallest x \n        coordinate\n        \"\"\"\n        fitn = ((m.y+h, m.x, w, h, m) for m in self._max_rects \n                if self._rect_fitness(m, w, h) is not None)\n        fitr = ((m.y+w, m.x, h, w, m) for m in self._max_rects \n                if self._rect_fitness(m, h, w) is not None)\n\n        if not self.rot:\n            fitr = []\n\n        fit = itertools.chain(fitn, fitr)\n        \n        try:\n            _, _, w, h, m = min(fit, key=first_item)\n        except ValueError:\n            return None, None\n\n        return Rectangle(m.x, m.y, w, h), m"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fits_surface(self, width, height):\n        assert(width > 0 and height > 0)\n        if self.rot and (width > self.width or height > self.height):\n            width, height = height, width\n\n        if width > self.width or height > self.height:\n            return False\n        else:\n            return True", "response": "Test if a rectangle is big enough to place a rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rect_list(self):\n        rectangle_list = []\n        for r in self:\n            rectangle_list.append((r.x, r.y, r.width, r.height, r.rid))\n\n        return rectangle_list", "response": "Returns a list with all rectangles placed into the surface."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_packing(self):\n        surface = Rectangle(0, 0, self.width, self.height)\n\n        for r in self:\n            if not surface.contains(r):\n                raise Exception(\"Rectangle placed outside surface\")\n\n        \n        rectangles = [r for r in self]\n        if len(rectangles) <= 1:\n            return\n\n        for r1 in range(0, len(rectangles)-2):\n            for r2 in range(r1+1, len(rectangles)-1):\n                if rectangles[r1].intersects(rectangles[r2]):\n                    raise Exception(\"Rectangle collision detected\")", "response": "Check for collisions between rectangles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_waste(self, x, y, width, height):\n        self._add_section(Rectangle(x, y, width, height))", "response": "Add a new waste section to the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distance(self, point):\n        return sqrt((self.x-point.x)**2+(self.y-point.y)**2)", "response": "Calculate distance to another point"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove the rectangle to x y", "response": "def move(self, x, y):\n        \"\"\"\n        Move Rectangle to x,y coordinates\n\n        Arguments:\n            x (int, float): X coordinate\n            y (int, float): Y coordinate\n        \"\"\"\n        self.x = x\n        self.y = y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contains(self, rect):\n        return (rect.y >= self.y and \\\n                rect.x >= self.x and \\\n                rect.y+rect.height <= self.y+self.height and \\\n                rect.x+rect.width  <= self.x+self.width)", "response": "Tests if another rectangle is contained by this one"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the rectangle intersects the current one.", "response": "def intersects(self, rect, edges=False):\n        \"\"\"\n        Detect intersections between this rectangle and rect.\n\n        Args:\n            rect (Rectangle): Rectangle to test for intersections.\n            edges (bool): Accept edge touching rectangles as intersects or not\n\n        Returns:\n            bool: True if the rectangles intersect, False otherwise\n        \"\"\"\n        # Not even touching\n        if (self.bottom > rect.top or \\\n            self.top < rect.bottom or \\\n            self.left > rect.right or \\\n            self.right < rect.left):\n            return False\n      \n        # Discard edge intersects\n        if not edges:\n            if (self.bottom == rect.top or \\\n                self.top == rect.bottom or \\\n                self.left == rect.right or \\\n                self.right == rect.left):\n                return False\n\n        # Discard corner intersects \n        if (self.left == rect.right and self.bottom == rect.top or \\\n            self.left == rect.right and rect.bottom == self.top or \\\n            rect.left == self.right and self.bottom == rect.top or \\\n            rect.left == self.right and rect.bottom == self.top):\n            return False\n    \n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the rectangle resulting of the intersection between this and another Arc Arc.", "response": "def intersection(self, rect, edges=False):\n        \"\"\"\n        Returns the rectangle resulting of the intersection between this and another \n        rectangle. If the rectangles are only touching by their edges, and the \n        argument 'edges' is True the rectangle returned will have an area of 0.\n        Returns None if there is no intersection.\n        \n        Arguments:\n             rect (Rectangle): The other rectangle.\n             edges (bool): If true touching edges are considered an intersection, and\n             a rectangle of 0 height or width will be returned\n\n        Returns:\n            Rectangle: Intersection.\n            None: There was no intersection.\n        \"\"\"\n        if not self.intersects(rect, edges=edges):\n            return None\n        \n        bottom = max(self.bottom, rect.bottom)\n        left = max(self.left, rect.left)\n        top = min(self.top, rect.top)\n        right = min(self.right, rect.right)\n\n        return Rectangle(left, bottom, right-left, top-bottom)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef join(self, other):\n        if self.contains(other):\n            return True\n\n        if other.contains(self):\n            self.x = other.x\n            self.y = other.y\n            self.width = other.width\n            self.height = other.height\n            return True\n\n        if not self.intersects(other, edges=True):\n            return False\n\n        # Other rectangle is Up/Down from this\n        if  self.left == other.left and self.width == other.width:\n            y_min = min(self.bottom, other.bottom)\n            y_max = max(self.top, other.top)  \n            self.y = y_min\n            self.height = y_max-y_min\n            return True\n\n        # Other rectangle is Right/Left from this\n        if  self.bottom == other.bottom and self.height == other.height:\n            x_min = min(self.left, other.left)\n            x_max = max(self.right, other.right)\n            self.x = x_min\n            self.width = x_max-x_min\n            return True\n\n        return False", "response": "Try to join a rectangle to this one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a float to Decimal", "response": "def float2dec(ft, decimal_digits):\n    \"\"\"\n    Convert float (or int) to Decimal (rounding up) with the\n    requested number of decimal digits.\n\n    Arguments:\n        ft (float, int): Number to convert\n        decimal (int): Number of digits after decimal point\n\n    Return:\n        Decimal: Number converted to decima\n    \"\"\"\n    with decimal.localcontext() as ctx:\n        ctx.rounding = decimal.ROUND_UP\n        places = decimal.Decimal(10)**(-decimal_digits)\n        return decimal.Decimal.from_float(float(ft)).quantize(places)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef newPacker(mode=PackingMode.Offline, \n         bin_algo=PackingBin.BBF, \n        pack_algo=MaxRectsBssf,\n        sort_algo=SORT_AREA, \n        rotation=True):\n    \"\"\"\n    Packer factory helper function\n\n    Arguments:\n        mode (PackingMode): Packing mode\n            Online: Rectangles are packed as soon are they are added\n            Offline: Rectangles aren't packed untils pack() is called\n        bin_algo (PackingBin): Bin selection heuristic\n        pack_algo (PackingAlgorithm): Algorithm used\n        rotation (boolean): Enable or disable rectangle rotation. \n\n    Returns:\n        Packer: Initialized packer instance.\n    \"\"\"\n    packer_class = None\n\n    # Online Mode\n    if mode == PackingMode.Online:\n        sort_algo=None\n        if bin_algo == PackingBin.BNF:\n            packer_class = PackerOnlineBNF\n        elif bin_algo == PackingBin.BFF:\n            packer_class = PackerOnlineBFF\n        elif bin_algo == PackingBin.BBF:\n            packer_class = PackerOnlineBBF\n        else:\n            raise AttributeError(\"Unsupported bin selection heuristic\")\n\n    # Offline Mode\n    elif mode == PackingMode.Offline:\n        if bin_algo == PackingBin.BNF:\n            packer_class = PackerBNF\n        elif bin_algo == PackingBin.BFF:\n            packer_class = PackerBFF\n        elif bin_algo == PackingBin.BBF:\n            packer_class = PackerBBF\n        elif bin_algo == PackingBin.Global:\n            packer_class = PackerGlobal\n            sort_algo=None\n        else:\n            raise AttributeError(\"Unsupported bin selection heuristic\")\n\n    else:\n        raise AttributeError(\"Unknown packing mode.\")\n\n    if sort_algo:\n        return packer_class(pack_algo=pack_algo, sort_algo=sort_algo, \n            rotation=rotation)\n    else:\n        return packer_class(pack_algo=pack_algo, rotation=rotation)", "response": "Returns a new packer instance for the given packing mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _new_open_bin(self, width=None, height=None, rid=None):\n        factories_to_delete = set() #\n        new_bin = None\n\n        for key, binfac in self._empty_bins.items():\n\n            # Only return the new bin if the rect fits.\n            # (If width or height is None, caller doesn't know the size.)\n            if not binfac.fits_inside(width, height):\n                continue\n           \n            # Create bin and add to open_bins\n            new_bin = binfac.new_bin()\n            if new_bin is None:\n                continue\n            self._open_bins.append(new_bin)\n\n            # If the factory was depleted mark for deletion\n            if binfac.is_empty():\n                factories_to_delete.add(key)\n       \n            break\n\n        # Delete marked factories\n        for f in factories_to_delete:\n            del self._empty_bins[f]\n\n        return new_bin", "response": "Extract the next empty bin and append it to open_bins."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_best_fit(self, pbin):\n        fit = ((pbin.fitness(r[0], r[1]), k) for k, r in self._sorted_rect.items())\n        fit = (f for f in fit if f[0] is not None)\n        try:\n            _, rect = min(fit, key=self.first_item)\n            return rect\n        except ValueError:\n            return None", "response": "Find the best fitness rectangle from rectangles packing _sorted_rect list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _new_open_bin(self, remaining_rect):\n        factories_to_delete = set() #\n        new_bin = None\n\n        for key, binfac in self._empty_bins.items():\n\n            # Only return the new bin if at least one of the remaining \n            # rectangles fit inside.\n            a_rectangle_fits = False\n            for _, rect in remaining_rect.items():\n                if binfac.fits_inside(rect[0], rect[1]):\n                    a_rectangle_fits = True\n                    break\n\n            if not a_rectangle_fits:\n                factories_to_delete.add(key)\n                continue\n           \n            # Create bin and add to open_bins\n            new_bin = binfac.new_bin()\n            if new_bin is None:\n                continue\n            self._open_bins.append(new_bin)\n\n            # If the factory was depleted mark for deletion\n            if binfac.is_empty():\n                factories_to_delete.add(key)\n       \n            break\n\n        # Delete marked factories\n        for f in factories_to_delete:\n            del self._empty_bins[f]\n\n        return new_bin", "response": "Extract the next bin where at least one of the rectangles fit inside."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _placement_points_generator(self, skyline, width):\n        skyline_r = skyline[-1].right\n        skyline_l = skyline[0].left\n\n        # Placements using skyline segment left point\n        ppointsl = (s.left for s in skyline if s.left+width <= skyline_r)\n\n        # Placements using skyline segment right point\n        ppointsr = (s.right-width for s in skyline if s.right-width >= skyline_l)\n\n        # Merge positions\n        return heapq.merge(ppointsl, ppointsr)", "response": "Returns a generator for the x coordinates of all the placement\n        points on the given rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_placements(self, width, height):\n        skyline = self._skyline\n\n        points = collections.deque()\n\n        left_index = right_index = 0 # Left and right side skyline index\n        support_height = skyline[0].top\n        support_index = 0 \n    \n        placements = self._placement_points_generator(skyline, width)\n        for p in placements:\n\n            # If Rectangle's right side changed segment, find new support\n            if p+width > skyline[right_index].right:\n                for right_index in range(right_index+1, len(skyline)):\n                    if skyline[right_index].top >= support_height:\n                        support_index = right_index\n                        support_height = skyline[right_index].top\n                    if p+width <= skyline[right_index].right:\n                        break\n                \n            # If left side changed segment.\n            if p >= skyline[left_index].right:\n                left_index +=1\n           \n            # Find new support if the previous one was shifted out.\n            if support_index < left_index:\n                support_index = left_index\n                support_height = skyline[left_index].top\n                for i in range(left_index, right_index+1):\n                    if skyline[i].top >= support_height:\n                        support_index = i\n                        support_height = skyline[i].top\n\n            # Add point if there is enought room at the top\n            if support_height+height <= self.height:\n                points.append((Rectangle(p, support_height, width, height),\\\n                    left_index, right_index))\n\n        return points", "response": "Generates a list of placements for a given size and height."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_skyline(self, skylineq, segment):\n        if len(skylineq) == 0:\n            skylineq.append(segment)\n            return\n\n        if skylineq[-1].top == segment.top:\n            s = skylineq[-1]\n            skylineq[-1] = HSegment(s.start, s.length+segment.length)\n        else:\n            skylineq.append(segment)", "response": "Merge a segment into a skyline."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_skyline(self, rect):\n        skylineq = collections.deque([]) # Skyline after adding new one\n        \n        for sky in self._skyline:\n            if sky.right <= rect.left or sky.left >= rect.right:\n                self._merge_skyline(skylineq, sky)\n                continue\n\n            if sky.left < rect.left and sky.right > rect.left:\n                # Skyline section partially under segment left\n                self._merge_skyline(skylineq, \n                        HSegment(sky.start, rect.left-sky.left))\n                sky = HSegment(P(rect.left, sky.top), sky.right-rect.left)\n            \n            if sky.left < rect.right:\n                if sky.left == rect.left:\n                    self._merge_skyline(skylineq, \n                        HSegment(P(rect.left, rect.top), rect.width))\n                # Skyline section partially under segment right\n                if sky.right > rect.right:\n                    self._merge_skyline(skylineq, \n                        HSegment(P(rect.right, sky.top), sky.right-rect.right))\n                    sky = HSegment(sky.start, rect.right-sky.left)\n            \n            if sky.left >= rect.left and sky.right <= rect.right:\n                # Skyline section fully under segment, account for wasted space\n                if self._waste_management and sky.top < rect.bottom:\n                    self._waste.add_waste(sky.left, sky.top, \n                        sky.length, rect.bottom - sky.top)\n            else:\n                # Segment\n                self._merge_skyline(skylineq, sky)\n\n        # Aaaaand ..... Done\n        self._skyline = list(skylineq)", "response": "Add a new one - element segment to the list of skyline sections."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _select_position(self, width, height):\n        positions = self._generate_placements(width, height)\n        if self.rot and width != height:\n            positions += self._generate_placements(height, width)\n        if not positions:\n            return None, None\n        return min(((p[0], self._rect_fitness(*p))for p in positions), \n                key=operator.itemgetter(1))", "response": "Select the best fittest position for the rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fitness(self, width, height):\n        assert(width > 0 and height >0)\n        if width > max(self.width, self.height) or\\\n            height > max(self.height, self.width):\n            return None\n\n        # If there is room in wasted space, FREE PACKING!!\n        if self._waste_management:\n            if self._waste.fitness(width, height) is not None:\n                return 0\n\n        # Get best fitness segment, for normal rectangle, and for\n        # rotated rectangle if rotation is enabled.\n        rect, fitness = self._select_position(width, height)\n        return fitness", "response": "Search for the best fitness for a given rectangle and height."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_rect(self, width, height, rid=None):\n        assert(width > 0 and height > 0)\n        if width > max(self.width, self.height) or\\\n            height > max(self.height, self.width):\n            return None\n\n        rect = None\n        # If Waste managment is enabled, first try to place the rectangle there\n        if self._waste_management:\n            rect = self._waste.add_rect(width, height, rid)\n\n        # Get best possible rectangle position\n        if not rect:\n            rect, _ = self._select_position(width, height)\n            if rect:\n                self._add_skyline(rect)\n\n        if rect is None:\n            return None\n        \n        # Store rectangle, and recalculate skyline\n        rect.rid = rid\n        self.rectangles.append(rect)\n        return rect", "response": "Add a new rectangle to the current one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef abspath(path, ref=None):\n    if ref:\n        path = os.path.join(ref, path)\n    if not os.path.isabs(path):\n        raise ValueError(\"expected an absolute path but got '%s'\" %  path)\n    return path", "response": "Create an absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a path into its components.", "response": "def split_path(path, ref=None):\n    \"\"\"\n    Split a path into its components.\n\n    Parameters\n    ----------\n    path : str\n        absolute or relative path with respect to `ref`\n    ref : str or None\n        reference path if `path` is relative\n\n    Returns\n    -------\n    list : str\n        components of the path\n    \"\"\"\n    path = abspath(path, ref)\n    return path.strip(os.path.sep).split(os.path.sep)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the value from instance at the given path.", "response": "def get_value(instance, path, ref=None):\n    \"\"\"\n    Get the value from `instance` at the given `path`.\n\n    Parameters\n    ----------\n    instance : dict or list\n        instance from which to retrieve a value\n    path : str\n        path to retrieve a value from\n    ref : str or None\n        reference path if `path` is relative\n\n    Returns\n    -------\n    value :\n        value at `path` in `instance`\n\n    Raises\n    ------\n    KeyError\n        if `path` is not valid\n    TypeError\n        if a value along the `path` is not a list or dictionary\n    \"\"\"\n    for part in split_path(path, ref):\n        if isinstance(instance, list):\n            part = int(part)\n        elif not isinstance(instance, dict):\n            raise TypeError(\"expected `list` or `dict` but got `%s`\" % instance)\n        try:\n            instance = instance[part]\n        except KeyError:\n            raise KeyError(abspath(path, ref))\n    return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pop_value(instance, path, ref=None):\n    head, tail = os.path.split(abspath(path, ref))\n    instance = get_value(instance, head)\n    if isinstance(instance, list):\n        tail = int(tail)\n    return instance.pop(tail)", "response": "Pop the value from instance at the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets value on instance at the given path.", "response": "def set_value(instance, path, value, ref=None):\n    \"\"\"\n    Set `value` on `instance` at the given `path` and create missing intermediate objects.\n\n    Parameters\n    ----------\n    instance : dict or list\n        instance from which to retrieve a value\n    path : str\n        path to retrieve a value from\n    value :\n        value to set\n    ref : str or None\n        reference path if `path` is relative\n    \"\"\"\n    *head, tail = split_path(path, ref)\n    for part in head:\n        instance = instance.setdefault(part, {})\n    instance[tail] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(x, y):\n    keys_x = set(x)\n    keys_y = set(y)\n\n    for key in keys_y - keys_x:\n        x[key] = y[key]\n\n    for key in keys_x & keys_y:\n        value_x = x[key]\n        value_y = y[key]\n\n        if isinstance(value_x, dict) and isinstance(value_y, dict):\n            x[key] = merge(value_x, value_y)\n        else:\n            if value_x != value_y:\n                raise ValueError\n\n    return x", "response": "Merge two dictionaries and raise an error for inconsistencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_default_from_schema(instance, schema):\n    for name, property_ in schema.get('properties', {}).items():\n        # Set the defaults at this level of the schema\n        if 'default' in property_:\n            instance.setdefault(name, property_['default'])\n        # Descend one level if the property is an object\n        if 'properties' in property_:\n            set_default_from_schema(instance.setdefault(name, {}), property_)\n    return instance", "response": "Populate default values on an instance given a schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply(instance, func, path=None):\n    path = path or os.path.sep\n    if isinstance(instance, list):\n        return [apply(item, func, os.path.join(path, str(i))) for i, item in enumerate(instance)]\n    elif isinstance(instance, dict):\n        return {key: apply(value, func, os.path.join(path, key)) for key, value in instance.items()}\n    return func(instance, path)", "response": "Apply func to all fundamental types of instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a free port.", "response": "def get_free_port(ports=None):\n    \"\"\"\n    Get a free port.\n\n    Parameters\n    ----------\n    ports : iterable\n        ports to check (obtain a random port by default)\n\n    Returns\n    -------\n    port : int\n        a free port\n    \"\"\"\n    if ports is None:\n        with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as _socket:\n            _socket.bind(('', 0))\n            _, port = _socket.getsockname()\n            return port\n\n    # Get ports from the specified list\n    for port in ports:\n        with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as _socket:\n            try:\n                _socket.bind(('', port))\n                return port\n            except socket.error as ex:\n                if ex.errno not in (48, 98):\n                    raise\n\n    raise RuntimeError(\"could not find a free port\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct command line arguments for one or more parameters.", "response": "def build_parameter_parts(configuration, *parameters):\n    \"\"\"\n    Construct command parts for one or more parameters.\n\n    Parameters\n    ----------\n    configuration : dict\n        configuration\n    parameters : list\n        list of parameters to create command line arguments for\n\n    Yields\n    ------\n    argument : str\n        command line argument\n    \"\"\"\n    for parameter in parameters:\n        values = configuration.pop(parameter, [])\n        if values:\n            if not isinstance(values, list):\n                values = [values]\n            for value in values:\n                yield '--%s=%s' % (parameter, value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_dict_parameter_parts(configuration, *parameters, **defaults):\n    for parameter in parameters:\n        for key, value in configuration.pop(parameter, {}).items():\n            yield '--%s=%s=%s' % (parameter, key, value)", "response": "Construct command line arguments for one or more dict parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntranslate a declarative docker configuration to a docker run command.", "response": "def build_docker_run_command(configuration):\n    \"\"\"\n    Translate a declarative docker `configuration` to a `docker run` command.\n\n    Parameters\n    ----------\n    configuration : dict\n        configuration\n\n    Returns\n    -------\n    args : list\n        sequence of command line arguments to run a command in a container\n    \"\"\"\n    parts = configuration.pop('docker').split()\n    parts.append('run')\n\n    run = configuration.pop('run')\n\n    # Ensure all env-files have proper paths\n    if 'env-file' in run:\n        run['env-file'] = [os.path.join(configuration['workspace'], env_file)\n                           for env_file in run['env-file']]\n\n    parts.extend(build_parameter_parts(\n        run, 'user', 'workdir', 'rm', 'interactive', 'tty', 'env-file', 'cpu-shares', 'name',\n        'network', 'label', 'memory', 'entrypoint', 'runtime', 'privileged', 'group-add'\n    ))\n\n    # Add the mounts\n    # The following code requires docker >= 17.06\n    '''for mount in run.pop('mount', []):\n        if mount['type'] == 'bind':\n            mount['source'] = os.path.join(\n                configuration['workspace'], mount['source'])\n        parts.extend(['--mount', \",\".join([\"%s=%s\" % item for item in mount.items()])])'''\n\n    # Add the mounts\n    for mount in run.pop('mount', []):\n        if mount['type'] == 'tmpfs':\n            raise RuntimeError('tmpfs-mounts are currently not supported via the mount ' +\n                               'directive in docker_interface. Consider using the tmpfs ' +\n                               'directive instead.')\n        if mount['type'] == 'bind':\n           mount['source'] = os.path.abspath(\n               os.path.join(configuration['workspace'], mount['source']))\n        vol_config = '--volume=%s:%s' % (mount['source'], mount['destination'])\n        if 'readonly' in mount and mount['readonly']:\n            vol_config += ':ro'\n        parts.append(vol_config)\n\n    # Set or forward environment variables\n    for key, value in run.pop('env', {}).items():\n        if value is None:\n            parts.append('--env=%s' % key)\n        else:\n            parts.append('--env=%s=%s' % (key, value))\n    parts.append('--env=DOCKER_INTERFACE=true')\n\n    # Forward ports\n    for publish in run.pop('publish', []):\n        parts.append('--publish=%s:%s:%s' % tuple([\n            publish.get(key, '') for key in \"ip host container\".split()]))\n\n    # Add temporary file systems\n    for tmpfs in run.pop('tmpfs', []):\n        destination = tmpfs['destination']\n        options = tmpfs.pop('options', [])\n        for key in ['mode', 'size']:\n            if key in tmpfs:\n                options.append('%s=%s' % (key, tmpfs[key]))\n        if options:\n            destination = \"%s:%s\" % (destination, \",\".join(options))\n        parts.extend(['--tmpfs', destination])\n\n    parts.append(run.pop('image'))\n    parts.extend(run.pop('cmd', []))\n\n    return parts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_docker_build_command(configuration):\n    parts = configuration.pop('docker', 'docker').split()\n    parts.append('build')\n\n    build = configuration.pop('build')\n\n    build['path'] = os.path.join(configuration['workspace'], build['path'])\n    build['file'] = os.path.join(build['path'], build['file'])\n\n    parts.extend(build_parameter_parts(\n        build, 'tag', 'file', 'no-cache', 'quiet', 'cpu-shares', 'memory'))\n\n    parts.extend(build_dict_parameter_parts(build, 'build-arg'))\n    parts.append(build.pop('path'))\n\n    return parts", "response": "Translate a declarative docker configuration to a docker build command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an argument to the parser based on a schema definition.", "response": "def add_argument(self, parser, path, name=None, schema=None, **kwargs):\n        \"\"\"\n        Add an argument to the `parser` based on a schema definition.\n\n        Parameters\n        ----------\n        parser : argparse.ArgumentParser\n            parser to add an argument to\n        path : str\n            path in the configuration document to add an argument for\n        name : str or None\n            name of the command line parameter (defaults to the name in the schema)\n        schema : dict\n            JSON schema definition (defaults to the schema of the plugin)\n\n        Returns\n        -------\n        arg :\n            command line argument definition\n        \"\"\"\n        schema = schema or self.SCHEMA\n        name = name or ('--%s' % os.path.basename(path))\n        self.arguments[name.strip('-')] = path\n        # Build a path to the help in the schema\n        path = util.split_path(path)\n        path = os.path.sep.join(\n            it.chain([os.path.sep], *zip(it.repeat(\"properties\"), path)))\n        property_ = util.get_value(schema, path)\n        defaults = {\n            'choices': property_.get('enum'),\n            'help': property_.get('description')\n        }\n        if 'type' in property_:\n            defaults['type'] = util.TYPES[property_['type']]\n        defaults.update(kwargs)\n        return parser.add_argument(name, **defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the plugin to the configuration.", "response": "def apply(self, configuration, schema, args):\n        \"\"\"\n        Apply the plugin to the configuration.\n\n        Inheriting plugins should implement this method to add additional functionality.\n\n        Parameters\n        ----------\n        configuration : dict\n            configuration\n        schema : dict\n            JSON schema\n        args : argparse.NameSpace\n            parsed command line arguments\n\n        Returns\n        -------\n        configuration : dict\n            updated configuration after applying the plugin\n        \"\"\"\n        # Set values from the command line\n        for name, path in self.arguments.items():\n            value = getattr(args, name.replace('-', '_'))\n            if value is not None:\n                util.set_value(configuration, path, value)\n\n        return configuration"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_plugins():\n        plugin_cls = {}\n        for entry_point in pkg_resources.iter_entry_points('docker_interface.plugins'):\n            cls = entry_point.load()\n            assert cls.COMMANDS is not None, \\\n                \"plugin '%s' does not define its commands\" % entry_point.name\n            assert cls.ORDER is not None, \\\n                \"plugin '%s' does not define its priority\" % entry_point.name\n            plugin_cls[entry_point.name] = cls\n        return plugin_cls", "response": "Load all availabe plugins."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a command and return the exit code.", "response": "def execute_command(self, parts, dry_run):\n        \"\"\"\n        Execute a command.\n\n        Parameters\n        ----------\n        parts : list\n            Sequence of strings constituting a command.\n        dry_run : bool\n            Whether to just log the command instead of executing it.\n\n        Returns\n        -------\n        status : int\n            Status code of the executed command or 0 if `dry_run` is `True`.\n        \"\"\"\n        if dry_run:\n            self.logger.info(\"dry-run command '%s'\", \" \".join(map(str, parts)))\n            return 0\n        else:  # pragma: no cover\n            self.logger.debug(\"executing command '%s'\", \" \".join(map(str, parts)))\n            status_code = os.spawnvpe(os.P_WAIT, parts[0], parts, os.environ)\n            if status_code:\n                self.logger.warning(\"command '%s' returned status code %d\",\n                                    \" \".join(map(str, parts)), status_code)\n            return status_code"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef substitute_variables(cls, configuration, value, ref):\n        if isinstance(value, str):\n            # Substitute all intra-document references\n            while True:\n                match = cls.REF_PATTERN.search(value)\n                if match is None:\n                    break\n                path = os.path.join(os.path.dirname(ref), match.group('path'))\n                try:\n                    value = value.replace(\n                        match.group(0), str(util.get_value(configuration, path)))\n                except KeyError:\n                    raise KeyError(path)\n\n            # Substitute all variable references\n            while True:\n                match = cls.VAR_PATTERN.search(value)\n                if match is None:\n                    break\n                value = value.replace(\n                    match.group(0),\n                    str(util.get_value(cls.VARIABLES, match.group('path'), '/')))\n        return value", "response": "Substitute variables in value from configuration where any path reference is relative to ref."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the user and group information.", "response": "def get_user_group(self, user=None, group=None):\n        \"\"\"\n        Get the user and group information.\n\n        Parameters\n        ----------\n        user : str\n            User name or user id (default is the `os.getuid()`).\n        group : str\n            Group name or group id (default is the group of `user`).\n\n        Returns\n        -------\n        user : pwd.struct_passwd\n            User object.\n        group : grp.struct_group\n            Group object.\n        \"\"\"\n        user = user or os.getuid()\n        # Convert the information we have obtained to a user object\n        try:\n            try:\n                user = pwd.getpwuid(int(user))\n            except ValueError:\n                user = pwd.getpwnam(user)\n        except KeyError as ex:  # pragma: no cover\n            self.logger.fatal(\"could not resolve user: %s\", ex)\n            raise\n\n        # Get the group\n        group = group or user.pw_gid\n        try:\n            try:\n                group = grp.getgrgid(int(group))\n            except ValueError:\n                group = grp.getgrnam(group)\n        except KeyError as ex:  # pragma: no cover\n            self.logger.fatal(\"could not resolve group:%s\", ex)\n            raise\n\n        return user, group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_for_statement(self):\n        cppobj = CppLoop(self.scope, self.parent, \"for\")\n        cppobj.file = self.file\n        cppobj.line = self.line\n        cppobj.column = self.column\n        builders = []\n        children = list(self.cursor.get_children())\n        assert len(children) >= 1\n        # body always comes last\n        builders.append(CppStatementBuilder(children[-1], self.scope, cppobj,\n                                            insert = cppobj._set_body))\n        if len(children) == 1:\n    # ----- just body -------------------------------------\n            cppobj.condition = True\n        elif len(children) == 2:\n    # ----- condition + body ------------------------------\n            builders.append(CppExpressionBuilder(children[0], self.scope,\n                            cppobj, insert = cppobj._set_condition))\n        elif len(children) >= 4:\n    # ----- var + condition + increment + body ------------\n            builders.append(CppStatementBuilder(children[0], cppobj,\n                            cppobj, insert = cppobj._set_declarations))\n            builders.append(CppExpressionBuilder(children[1], self.scope,\n                            cppobj, insert = cppobj._set_condition))\n            builders.append(CppStatementBuilder(children[2], cppobj,\n                            cppobj, insert = cppobj._set_increment))\n        elif children[0].kind == clang.CursorKind.DECL_STMT:\n    # ----- var + condition + body ------------------------\n            builders.append(CppStatementBuilder(children[0], cppobj,\n                            cppobj, insert = cppobj._set_declarations))\n            builders.append(CppExpressionBuilder(children[1], self.scope,\n                            cppobj, insert = cppobj._set_condition))\n        else:\n    # ----- condition + increment + body ------------------\n            builders.append(CppExpressionBuilder(children[0], self.scope,\n                            cppobj, insert = cppobj._set_condition))\n            builders.append(CppStatementBuilder(children[1], cppobj,\n                            cppobj, insert = cppobj._set_increment))\n        return (cppobj, builders)", "response": "Builds a for statement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a human - readable string representation of an object.", "response": "def pretty_str(something, indent=0):\n    \"\"\"Return a human-readable string representation of an object.\n\n        Uses `pretty_str` if the given value is an instance of\n        `CodeEntity` and `repr` otherwise.\n\n    Args:\n        something: Some value to convert.\n\n    Kwargs:\n        indent (int): The amount of spaces to use as indentation.\n    \"\"\"\n    if isinstance(something, CodeEntity):\n        return something.pretty_str(indent=indent)\n    else:\n        return (' ' * indent) + repr(something)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates the program tree starting from this object going down.", "response": "def walk_preorder(self):\n        \"\"\"Iterates the program tree starting from this object, going down.\"\"\"\n        yield self\n        for child in self._children():\n            for descendant in child.walk_preorder():\n                yield descendant"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all descendants that are instances of a given class.", "response": "def filter(self, cls, recursive=False):\n        \"\"\"Retrieves all descendants (including self) that are instances\n            of a given class.\n\n        Args:\n            cls (class): The class to use as a filter.\n\n        Kwargs:\n            recursive (bool): Whether to descend recursively down the tree.\n        \"\"\"\n        source = self.walk_preorder if recursive else self._children\n        return [\n            codeobj\n            for codeobj in source()\n            if isinstance(codeobj, cls)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lookup_parent(self, cls):\n        codeobj = self.parent\n        while codeobj is not None and not isinstance(codeobj, cls):\n            codeobj = codeobj.parent\n        return codeobj", "response": "Lookup a transitive parent object that is an instance\n            of a given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ast_str(self, indent=0):\n        line = self.line or 0\n        col = self.column or 0\n        name = type(self).__name__\n        spell = getattr(self, 'name', '[no spelling]')\n        result = ' ({})'.format(self.result) if hasattr(self, 'result') else ''\n        prefix = indent * '| '\n        return '{}[{}:{}] {}{}: {}'.format(prefix, line, col,\n                                           name, result, spell)", "response": "Return a minimal string to print a tree - like structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_local(self):\n        return (isinstance(self.scope, CodeStatement)\n                or (isinstance(self.scope, CodeFunction)\n                    and self not in self.scope.parameters))", "response": "Whether this variable is local."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_parameter(self):\n        return (isinstance(self.scope, CodeFunction)\n                and self in self.scope.parameters)", "response": "Whether this is a function parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add(self, codeobj):\n        assert isinstance(codeobj, CodeExpression.TYPES)\n        self.value = codeobj", "response": "Add a child to this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_str(self, indent=0):\n        return '{}{} {} = {}'.format(' ' * indent, self.result, self.name,\n                                     pretty_str(self.value))", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a child statement to this object.", "response": "def _add(self, codeobj):\n        \"\"\"Add a child (statement) to this object.\"\"\"\n        assert isinstance(codeobj, (CodeStatement, CodeExpression))\n        self.body._add(codeobj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _children(self):\n        for codeobj in self.parameters:\n            yield codeobj\n        for codeobj in self.body._children():\n            yield codeobj", "response": "Yield all direct children of this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns a function - local index to each child object and register a write operation to variables.", "response": "def _afterpass(self):\n        \"\"\"Assign a function-local index to each child object and register\n            write operations to variables.\n\n            This should only be called after the object is fully built.\n        \"\"\"\n        if hasattr(self, '_fi'):\n            return\n        fi = 0\n        for codeobj in self.walk_preorder():\n            codeobj._fi = fi\n            fi += 1\n            if isinstance(codeobj, CodeOperator) and codeobj.is_assignment:\n                if codeobj.arguments and isinstance(codeobj.arguments[0],\n                                                    CodeReference):\n                    var = codeobj.arguments[0].reference\n                    if isinstance(var, CodeVariable):\n                        var.writes.append(codeobj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        params = ', '.join(map(lambda p: p.result + ' ' + p.name,\n                           self.parameters))\n        if self.is_constructor:\n            pretty = '{}{}({}):\\n'.format(spaces, self.name, params)\n        else:\n            pretty = '{}{} {}({}):\\n'.format(spaces, self.result,\n                                             self.name, params)\n        if self._definition is not self:\n            pretty += spaces + '  [declaration]'\n        else:\n            pretty += self.body.pretty_str(indent + 2)\n        return pretty", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add(self, codeobj):\n        assert isinstance(codeobj, (CodeFunction, CodeVariable, CodeClass))\n        self.members.append(codeobj)\n        codeobj.member_of = self", "response": "Add a child to this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassign the member_of of child members and call their _afterpass method.", "response": "def _afterpass(self):\n        \"\"\"Assign the `member_of` of child members and call\n            their `_afterpass()`.\n\n            This should only be called after the object is fully built.\n        \"\"\"\n        for codeobj in self.members:\n            if not codeobj.is_definition:\n                if not codeobj._definition is None:\n                    codeobj._definition.member_of = self\n            codeobj._afterpass()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        pretty = spaces + 'class ' + self.name\n        if self.superclasses:\n            superclasses = ', '.join(self.superclasses)\n            pretty += '(' + superclasses + ')'\n        pretty += ':\\n'\n        if self.members:\n            pretty += '\\n\\n'.join(\n                    c.pretty_str(indent + 2)\n                    for c in self.members\n            )\n        else:\n            pretty += spaces + '  [declaration]'\n        return pretty", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add(self, codeobj):\n        assert isinstance(codeobj, (CodeNamespace, CodeClass,\n                                    CodeFunction, CodeVariable))\n        self.children.append(codeobj)", "response": "Add a child to this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        spaces = ' ' * indent\n        pretty = '{}namespace {}:\\n'.format(spaces, self.name)\n        pretty += '\\n\\n'.join(c.pretty_str(indent + 2) for c in self.children)\n        return pretty"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pretty_str(self, indent=0):\n        return '\\n\\n'.join(\n                codeobj.pretty_str(indent=indent)\n                for codeobj in self.children\n        )", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pretty_str(self, indent=0):\n        if self.parenthesis:\n            return (' ' * indent) + '(' + self.name + ')'\n        return (' ' * indent) + self.name", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_str(self, indent=0):\n        if self.parenthesis:\n            return '{}({})'.format(' ' * indent, pretty_str(self.value))\n        return pretty_str(self.value, indent=indent)", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        indent = ' ' * indent\n        values = '{{{}}}'.format(', '.join(map(pretty_str, self.value)))\n\n        if self.parenthesis:\n            return '{}({})'.format(indent, values)\n        return '{}{}'.format(indent, values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        pretty = '{}({})' if self.parenthesis else '{}{}'\n        name = ('{}.{}'.format(self.field_of.pretty_str(), self.name)\n                if self.field_of else self.name)\n        return pretty.format(spaces, name)", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a child ( argument ) to this object.", "response": "def _add(self, codeobj):\n        \"\"\"Add a child (argument) to this object.\"\"\"\n        assert isinstance(codeobj, CodeExpression.TYPES)\n        self.arguments = self.arguments + (codeobj,)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _children(self):\n        if self.method_of:\n            yield self.method_of\n        for codeobj in self.arguments:\n            if isinstance(codeobj, CodeExpression):\n                yield codeobj", "response": "Yield all direct children of this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        indent = ' ' * indent\n        pretty = '{}({})' if self.parenthesis else '{}{}'\n        args = ', '.join(map(pretty_str, self.arguments))\n        if self.method_of:\n            call = '{}.{}({})'.format(self.method_of.pretty_str(),\n                                      self.name, args)\n        elif self.is_constructor:\n            call = 'new {}({})'.format(self.name, args)\n        else:\n            call = '{}({})'.format(self.name, args)\n        return pretty.format(indent, call)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pretty_str(self, indent=0):\n        indent = ' ' * indent\n        if self.value is not None:\n            return '{}{} {}'.format(indent, self.name, pretty_str(self.value))\n        return indent + self.name", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a child statement to this object.", "response": "def _add(self, codeobj):\n        \"\"\"Add a child (statement) to this object.\"\"\"\n        assert isinstance(codeobj, CodeStatement)\n        codeobj._si = len(self.body)\n        self.body.append(codeobj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        if self.body:\n            return '\\n'.join(stmt.pretty_str(indent) for stmt in self.body)\n        else:\n            return (' ' * indent) + '[empty]'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a child ( variable to this object.", "response": "def _add(self, codeobj):\n        \"\"\"Add a child (variable) to this object.\"\"\"\n        assert isinstance(codeobj, CodeVariable)\n        self.variables.append(codeobj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        return spaces + ', '.join(v.pretty_str() for v in self.variables)", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_condition(self, condition):\n        assert isinstance(condition, CodeExpression.TYPES)\n        self.condition = condition", "response": "Set the condition for this control flow structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_body(self, body):\n        assert isinstance(body, CodeStatement)\n        if isinstance(body, CodeBlock):\n            self.body = body\n        else:\n            self.body._add(body)", "response": "Set the main body for this control flow structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef statement(self, i):\n    # ----- This code is just to avoid creating a new list and\n    #       returning a custom exception message.\n        o = len(self.body)\n        n = o + len(self.else_body)\n        if i >= 0 and i < n:\n            if i < o:\n                return self.body.statement(i)\n            return self.else_body.statement(i - o)\n        elif i < 0 and i >= -n:\n            if i >= o - n:\n                return self.else_body.statement(i)\n            return self.body.statement(i - o + n)\n        raise IndexError('statement index out of range')", "response": "Return the statement of the ith entry in the block."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef statement_after(self, i):\n        k = i + 1\n        o = len(self.body)\n        n = o + len(self.else_body)\n        if k > 0:\n            if k < o:\n                return self.body.statement(k)\n            if k > o and k < n:\n                return self.else_body.statement(k)\n        if k < 0:\n            if k < o - n and k > -n:\n                return self.body.statement(k)\n            if k > o - n:\n                return self.else_body.statement(k)\n        return None", "response": "Return the statement after the i* - th one or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_branches(self):\n        if self.else_branch:\n            return [self.then_branch, self.else_branch]\n        return [self.then_branch]", "response": "Return a list with the conditional branch and the default branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_default_branch(self, body):\n        assert isinstance(body, CodeStatement)\n        if isinstance(body, CodeBlock):\n            self.else_body = body\n        else:\n            self.else_body._add(body)", "response": "Add a default body for this conditional."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield all direct children of this object.", "response": "def _children(self):\n        \"\"\"Yield all direct children of this object.\"\"\"\n        if isinstance(self.condition, CodeExpression):\n            yield self.condition\n        for codeobj in self.body._children():\n            yield codeobj\n        for codeobj in self.else_body._children():\n            yield codeobj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        condition = pretty_str(self.condition)\n        pretty = '{}if ({}):\\n'.format(spaces, condition)\n        pretty += self.body.pretty_str(indent=indent + 2)\n        if self.else_body:\n            pretty += '\\n{}else:\\n'.format(spaces)\n            pretty += self.else_body.pretty_str(indent=indent + 2)\n        return pretty", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset declarations local to this loop.", "response": "def _set_declarations(self, declarations):\n        \"\"\"Set declarations local to this loop (e.g. `for` variables).\"\"\"\n        assert isinstance(declarations, CodeStatement)\n        self.declarations = declarations\n        declarations.scope = self.body"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_increment(self, statement):\n        assert isinstance(statement, CodeStatement)\n        self.increment = statement\n        statement.scope = self.body", "response": "Set the increment statement for this loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding all direct children of this object.", "response": "def _children(self):\n        \"\"\"Yield all direct children of this object.\"\"\"\n        if self.declarations:\n            yield self.declarations\n        if isinstance(self.condition, CodeExpression):\n            yield self.condition\n        if self.increment:\n            yield self.increment\n        for codeobj in self.body._children():\n            yield codeobj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_str(self, indent=0):\n        spaces = ' ' * indent\n        condition = pretty_str(self.condition)\n        v = self.declarations.pretty_str() if self.declarations else ''\n        i = self.increment.pretty_str(indent=1) if self.increment else ''\n        pretty = '{}for ({}; {}; {}):\\n'.format(spaces, v, condition, i)\n        pretty += self.body.pretty_str(indent=indent + 2)\n        return pretty", "response": "Return a human - readable string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        spaces = ' ' * indent\n        condition = pretty_str(self.condition)\n        pretty = '{}switch ({}):\\n'.format(spaces, condition)\n        pretty += self.body.pretty_str(indent=indent + 2)\n        return pretty"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a catch block to this try block structure.", "response": "def _add_catch(self, catch_block):\n        \"\"\"Add a catch block (exception variable declaration and block)\n            to this try block structure.\n        \"\"\"\n        assert isinstance(catch_block, self.CodeCatchBlock)\n        self.catches.append(catch_block)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield all direct children of this object.", "response": "def _children(self):\n        \"\"\"Yield all direct children of this object.\"\"\"\n        for codeobj in self.body._children():\n            yield codeobj\n        for catch_block in self.catches:\n            for codeobj in catch_block._children():\n                yield codeobj\n        for codeobj in self.finally_body._children():\n            yield codeobj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a human - readable string representation of this object.", "response": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n        spaces = ' ' * indent\n        pretty = spaces + 'try:\\n'\n        pretty += self.body.pretty_str(indent=indent + 2)\n        for block in self.catches:\n            pretty += '\\n' + block.pretty_str(indent)\n        if len(self.finally_body) > 0:\n            pretty += '\\n{}finally:\\n'.format(spaces)\n            pretty += self.finally_body.pretty_str(indent=indent + 2)\n        return pretty"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef current_git_dir():\n    path = os.path.abspath(os.curdir)\n    while path != '/':\n        if os.path.isdir(os.path.join(path, '.git')):\n            return os.path.join(path, '.git')\n        path = os.path.dirname(path)\n    return None", "response": "Locate the. git directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_files(path):\n    files = []\n    if os.path.isdir(path):\n        for stats in os.walk(path):\n            for f in stats[2]:\n                files.append(os.path.join(stats[0], f))\n    elif os.path.isfile(path):\n        files = [path]\n    return files", "response": "Recursively collects a list of files at a path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(self):\n        text = ''\n\n        for result in self.objects:\n            if result.is_failure or result.is_error:\n                text += '\\n#{red}#{bright}'\n                text += '{}\\n'.format(''.ljust(79, '='))\n\n                status = 'FAILED' if result.is_failure else 'ERROR'\n                text += '{}: {}\\n'.format(status, result.process)\n                text += '{}\\n#{{reset_all}}'.format(''.ljust(79, '='))\n\n                if result.output:\n                    text += result.output\n\n                if result.error:\n                    if result.output:\n                        text += '\\n{}\\n'.format(''.ljust(79, '-'))\n                        text += 'Additional error output:\\n'\n                        text += '{}\\n'.format(''.ljust(79, '-'))\n                    text += result.error\n\n                if not text.endswith('\\n'):\n                    text += '\\n'\n\n        if self.has_modified_files:\n            text += '\\n#{{yellow}}#{{bright}}{}\\n'.format(''.ljust(79, '-'))\n            text += 'Modified files:\\n'\n            text += '{}\\n'.format(''.ljust(79, '-'))\n            for path, modified_by in self.modified_files:\n                text += '#{{reset_all}}{} #{{cyan}}<- {}\\n'.format(path, ', '.join(modified_by))\n\n        return text", "response": "Returns the results in string format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string containing XML mapped to the JUnit schema.", "response": "def dump_junit(self):\n        \"\"\"Returns a string containing XML mapped to the JUnit schema.\"\"\"\n        testsuites = ElementTree.Element('testsuites', name='therapist', time=str(round(self.execution_time, 2)),\n                                         tests=str(self.count()), failures=str(self.count(status=Result.FAILURE)),\n                                         errors=str(self.count(status=Result.ERROR)))\n\n        for result in self.objects:\n            failures = '1' if result.is_failure else '0'\n            errors = '1' if result.is_error else '0'\n\n            testsuite = ElementTree.SubElement(testsuites, 'testsuite', id=result.process.name,\n                                               name=str(result.process), time=str(round(result.execution_time, 2)),\n                                               tests='1', failures=failures, errors=errors)\n\n            testcase = ElementTree.SubElement(testsuite, 'testcase', time=str(round(result.execution_time, 2)))\n            testcase.attrib['name'] = result.process.name\n\n            if result.is_failure or result.is_error:\n                if result.is_failure:\n                    element = ElementTree.SubElement(testcase, 'failure', type='failure')\n                else:\n                    element = ElementTree.SubElement(testcase, 'error', type='error')\n\n                if result.error:\n                    element.text = result.error\n                else:\n                    element.text = result.output if result.output else ''\n\n        xmlstr = ElementTree.tostring(testsuites, encoding='utf-8').decode('utf-8')\n        return '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n{}'.format(xmlstr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install(**kwargs):\n    force = kwargs.get('force')\n    preserve_legacy = kwargs.get('preserve_legacy')\n\n    colorama.init(strip=kwargs.get('no_color'))\n\n    stdout = subprocess.check_output('which therapist', shell=True)\n    therapist_bin = stdout.decode('utf-8').split()[0]\n\n    git_dir = current_git_dir()\n\n    if git_dir is None:\n        output(NOT_GIT_REPO_MSG)\n        exit(1)\n\n    hook_options = {\n        'fix': '--fix' if kwargs.get('fix') else '',\n        'stage_modified_files': '' if kwargs.get('no_stage_modified_files') else '--stage-modified-files',\n        'therapist_bin': therapist_bin,\n    }\n\n    srchook_path = os.path.join(BASE_DIR, 'hooks', 'pre-commit-template')\n    with open(srchook_path, 'r') as f:\n        srchook = f.read()\n    srchook_hash = hash_hook(srchook_path, hook_options)\n\n    dsthook_path = os.path.join(git_dir, 'hooks', 'pre-commit')\n\n    if os.path.isfile(dsthook_path):\n        dsthook_hash = identify_hook(dsthook_path)\n        if dsthook_hash:\n            if dsthook_hash == srchook_hash:\n                output(HOOK_ALREADY_INSTALLED_MSG)\n                exit(0)\n        else:\n            if not force and not preserve_legacy:\n                print(EXISTING_HOOK_MSG)\n                preserve_legacy = click.confirm(CONFIRM_PRESERVE_LEGACY_HOOK_MSG, default=True)\n\n            if preserve_legacy:\n                output(COPYING_HOOK_MSG, end='')\n                shutil.copy2(dsthook_path, '{}.legacy'.format(dsthook_path))\n                output(DONE_COPYING_HOOK_MSG)\n            elif not force:\n                if not click.confirm(CONFIRM_REPLACE_HOOK_MSG, default=False):\n                    output(INSTALL_ABORTED_MSG)\n                    exit(1)\n\n    output(INSTALLING_HOOK_MSG, end='')\n    with open(dsthook_path, 'w+') as f:\n        srchook = srchook.replace('%hash%', srchook_hash)\n        for k, v in iteritems(hook_options):\n            srchook = srchook.replace('%{}%'.format(k), v)\n        f.write(srchook)\n    os.chmod(dsthook_path, 0o775)\n    output(DONE_INSTALLING_HOOK_MSG)", "response": "Install the pre - commit hook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uninstall(**kwargs):\n    force = kwargs.get('force')\n    restore_legacy = kwargs.get('restore_legacy')\n\n    colorama.init(strip=kwargs.get('no_color'))\n\n    git_dir = current_git_dir()\n\n    if git_dir is None:\n        output(NOT_GIT_REPO_MSG)\n        exit(1)\n\n    hook_path = os.path.join(git_dir, 'hooks', 'pre-commit')\n\n    if not os.path.isfile(hook_path):\n        output(NO_HOOK_INSTALLED_MSG)\n        exit(0)\n\n    hook_hash = identify_hook(hook_path)\n\n    if hook_hash:\n        if not force:\n            if not click.confirm(CONFIRM_UNINSTALL_HOOK_MSG, default=False):\n                output(UNINSTALL_ABORTED_MSG)\n                exit(1)\n    else:\n        output(CURRENT_HOOK_NOT_THERAPIST_MSG)\n        exit(1)\n\n    legacy_hook_path = os.path.join(git_dir, 'hooks', 'pre-commit.legacy')\n\n    if os.path.isfile(legacy_hook_path):\n        if not force and not restore_legacy:\n            output(LEGACY_HOOK_EXISTS_MSG)\n            restore_legacy = click.confirm(CONFIRM_RESTORE_LEGACY_HOOK_MSG, default=True)\n        if restore_legacy:\n            output(COPYING_LEGACY_HOOK_MSG, end='')\n            shutil.copy2(legacy_hook_path, hook_path)\n            os.remove(legacy_hook_path)\n            output(DONE_COPYING_LEGACY_HOOK_MSG)\n            exit(0)\n        else:\n            if force or click.confirm('Would you like to remove the legacy hook?', default=False):\n                output(REMOVING_LEGACY_HOOK_MSG, end='')\n                os.remove(legacy_hook_path)\n                output(DONE_REMOVING_LEGACY_HOOK_MSG)\n\n    output(UNINSTALLING_HOOK_MSG, end='')\n    os.remove(hook_path)\n    output(DONE_UNINSTALLING_HOOK_MSG)", "response": "Uninstall the current pre - commit hook."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the Therapist suite.", "response": "def run(**kwargs):\n    \"\"\"Run the Therapist suite.\"\"\"\n    paths = kwargs.pop('paths', ())\n    action = kwargs.pop('action')\n    plugin = kwargs.pop('plugin')\n    junit_xml = kwargs.pop('junit_xml')\n    use_tracked_files = kwargs.pop('use_tracked_files')\n    quiet = kwargs.pop('quiet')\n\n    colorama.init(strip=kwargs.pop('no_color'))\n\n    git_dir = current_git_dir()\n\n    if git_dir is None:\n        output(NOT_GIT_REPO_MSG)\n        exit(1)\n\n    repo_root = os.path.dirname(git_dir)\n\n    files = []\n    if paths:\n        # We want to look at files in their current state if paths are passed through\n        kwargs['include_unstaged_changes'] = True\n\n        # If paths were provided get all the files for each path\n        for path in paths:\n            for f in list_files(path):\n                f = os.path.relpath(f, repo_root)\n                if not f.startswith('..'):  # Don't include files outside the repo root.\n                    files.append(f)\n    elif use_tracked_files:\n        # If the use tracked files flag was passed, get a list of all the tracked files\n        out, err, code = git.ls_files()\n        files = out.splitlines()\n\n        if kwargs.get('include_untracked'):\n            out, err, code = git.ls_files(o=True, exclude_standard=True)\n            files += out.splitlines()\n\n    if files or paths:\n        kwargs['files'] = files\n\n    config = get_config(repo_root)\n    runner = Runner(config.cwd, **kwargs)\n    results = ResultCollection()\n\n    if runner.unstaged_changes and not quiet:\n        output(UNSTAGED_CHANGES_MSG, end='\\n\\n')\n\n    processes = list(config.actions) + list(config.plugins)\n    processes.sort(key=lambda x: x.name)  # Sort the list of processes for consistent results\n\n    if plugin:\n        try:\n            processes = [config.plugins.get(plugin)]\n        except config.plugins.DoesNotExist as e:\n            output('{}\\nAvailable plugins:'.format(e.message))\n\n            for p in config.plugins:\n                output(p.name)\n            exit(1)\n\n    if action:\n        try:\n            processes = [config.actions.get(action)]\n        except config.actions.DoesNotExist as e:\n            output('{}\\nAvailable actions:'.format(e.message))\n\n            for a in config.actions:\n                output(a.name)\n            exit(1)\n\n    for process in processes:\n        result, message = runner.run_process(process)\n        results.append(result)\n\n        if not quiet:\n            output(message)\n\n    if junit_xml:\n        with open(junit_xml, 'w+') as f:\n            f.write('{}'.format(results.dump_junit()))\n\n    if not quiet:\n        output(results.dump())\n        output('#{{bright}}{}\\nCompleted in: {}s'.format(''.ljust(79, '-'), round(results.execution_time, 2)))\n\n    if results.has_error:\n        exit(1)\n    elif results.has_failure:\n        exit(2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef use(ctx, shortcut):\n    git_dir = current_git_dir()\n\n    if git_dir is None:\n        output(NOT_GIT_REPO_MSG)\n        exit(1)\n\n    repo_root = os.path.dirname(git_dir)\n\n    config = get_config(repo_root)\n\n    try:\n        use_shortcut = config.shortcuts.get(shortcut)\n\n        while use_shortcut.extends is not None:\n            base = config.shortcuts.get(use_shortcut.extends)\n            use_shortcut = base.extend(use_shortcut)\n    except config.shortcuts.DoesNotExist as err:\n        output('{}\\nAvailable shortcuts:'.format(err.message))\n\n        for s in config.shortcuts:\n            output(s.name)\n        exit(1)\n    else:\n        options = use_shortcut.options\n        for flag in use_shortcut.flags:\n            options[flag.replace('-', '_')] = True\n\n        options_string = ''\n        for k, v in sorted(iteritems(options)):\n            options_string += ' --{}'.format(k.replace('_', '-'))\n            if v is not True:\n                options_string += ' {}'.format(v)\n\n        output('#{{dim}}$ therapist run{}\\n'.format(options_string))\n        ctx.invoke(run, **options)", "response": "Use a single shortcut."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef identify_hook(path):\n    with open(path, 'r') as f:\n        f.readline()  # Discard the shebang line\n        version_line = f.readline()\n        if version_line.startswith('# THERAPIST'):\n            return version_line.split()[2]", "response": "Verify that the file at path is the therapist hook and return the hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhashes a hook file", "response": "def hash_hook(path, options):\n    \"\"\"Hash a hook file\"\"\"\n    with open(path, 'r') as f:\n        data = f.read()\n        for key in sorted(iterkeys(options)):\n            data += '\\n#{}={}'.format(key, options.get(key))\n        return hashlib.md5(data.encode()).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a single action.", "response": "def run_process(self, process):\n        \"\"\"Runs a single action.\"\"\"\n        message = u'#{bright}'\n        message += u'{} '.format(str(process)[:68]).ljust(69, '.')\n\n        stashed = False\n        if self.unstaged_changes and not self.include_unstaged_changes:\n            out, err, code = self.git.stash(keep_index=True, quiet=True)\n            stashed = code == 0\n\n        try:\n            result = process(files=self.files, cwd=self.cwd, fix=self.fix)\n\n            # Check for modified files\n            out, err, code = self.git.status(porcelain=True, untracked_files='no')\n            for line in out.splitlines():\n                file_status = Status(line)\n\n                # Make sure the file is one of the files that was processed\n                if file_status.path in self.files and file_status.is_modified:\n                    mtime = os.path.getmtime(file_status.path) if os.path.exists(file_status.path) else 0\n                    if mtime > self.file_mtimes.get(file_status.path, 0):\n                        self.file_mtimes[file_status.path] = mtime\n                        result.add_modified_file(file_status.path)\n                        if self.stage_modified_files:\n                            self.git.add(file_status.path)\n\n        except:  # noqa: E722\n            raise\n        finally:\n            if stashed:\n                self.git.reset(hard=True, quiet=True)\n                self.git.stash.pop(index=True, quiet=True)\n\n        if result.is_success:\n            message += u' #{green}[SUCCESS]'\n        elif result.is_failure:\n            message += u' #{red}[FAILURE]'\n        elif result.is_skip:\n            message += u' #{cyan}[SKIPPED]'\n        elif result.is_error:\n            message += u' #{red}[ERROR!!]'\n\n        return result, message"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_time(val):\n    dt = parse_date(val)\n    if dt.tzinfo is None:\n        return dt.time()\n    return datetime.time(dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo)", "response": "Parse a time to a datetime. time value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef marshal_value(datatype, value):\n    for types, func in MARSHAL_FUNCTIONS:\n        if datatype in types:\n            return True, func(value)\n    return False, value", "response": "Marshal a given string into a relevant Python type given the uPnP datatype."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the error code and error description from an error response.", "response": "def _extract_upnperror(self, err_xml):\n        \"\"\"\n        Extract the error code and error description from an error returned by the device.\n        \"\"\"\n        nsmap = {'s': list(err_xml.nsmap.values())[0]}\n        fault_str = err_xml.findtext(\n            's:Body/s:Fault/faultstring', namespaces=nsmap)\n        try:\n            err = err_xml.xpath(\n                's:Body/s:Fault/detail/*[name()=\"%s\"]' % fault_str, namespaces=nsmap)[0]\n        except IndexError:\n            msg = 'Tag with name of %r was not found in the error response.' % fault_str\n            self._log.debug(\n                msg + '\\n' + etree.tostring(err_xml, pretty_print=True).decode('utf8'))\n            raise SOAPProtocolError(msg)\n\n        err_code = err.findtext('errorCode', namespaces=err.nsmap)\n        err_desc = err.findtext('errorDescription', namespaces=err.nsmap)\n\n        if err_code is None or err_desc is None:\n            msg = 'Tags errorCode or errorDescription were not found in the error response.'\n            self._log.debug(\n                msg + '\\n' + etree.tostring(err_xml, pretty_print=True).decode('utf8'))\n            raise SOAPProtocolError(msg)\n        return int(err_code), err_desc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_extraneous_xml_declarations(xml_str):\n        xml_declaration = ''\n        if xml_str.startswith('<?xml'):\n            xml_declaration, xml_str = xml_str.split('?>', maxsplit=1)\n            xml_declaration += '?>'\n        xml_str = re.sub(r'<\\?xml.*?\\?>', '', xml_str, flags=re.I)\n        return xml_declaration + xml_str", "response": "Removes any extra XML declarations from the given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a call to the device.", "response": "def call(self, action_name, arg_in=None, http_auth=None, http_headers=None):\n        \"\"\"\n        Construct the XML and make the call to the device. Parse the response values into a dict.\n        \"\"\"\n        if arg_in is None:\n            arg_in = {}\n\n        soap_env = '{%s}' % NS_SOAP_ENV\n        m = '{%s}' % self.service_type\n\n        root = etree.Element(soap_env+'Envelope', nsmap={'SOAP-ENV': NS_SOAP_ENV})\n        root.attrib[soap_env+'encodingStyle'] = ENCODING_STYLE\n        body = etree.SubElement(root, soap_env+'Body')\n        action = etree.SubElement(body, m+action_name, nsmap={'m': self.service_type})\n        for key, value in arg_in.items():\n            etree.SubElement(action, key).text = str(value)\n        body = etree.tostring(root, encoding=ENCODING, xml_declaration=True)\n        headers = {\n            'SOAPAction': '\"%s#%s\"' % (self.service_type, action_name),\n            'Host': self._host,\n            'Content-Type': 'text/xml',\n            'Content-Length': str(len(body)),\n        }\n        headers.update(http_headers or {})\n\n        try:\n            resp = requests.post(\n                self.url,\n                body,\n                headers=headers,\n                timeout=SOAP_TIMEOUT,\n                auth=http_auth\n            )\n            resp.raise_for_status()\n        except requests.exceptions.HTTPError as exc:\n            # If the body of the error response contains XML then it should be a UPnP error,\n            # otherwise reraise the HTTPError.\n            try:\n                err_xml = etree.fromstring(exc.response.content)\n            except etree.XMLSyntaxError:\n                raise exc\n            raise SOAPError(*self._extract_upnperror(err_xml))\n\n        xml_str = resp.content.strip()\n        try:\n            xml = etree.fromstring(xml_str)\n        except etree.XMLSyntaxError:\n            # Try removing any extra XML declarations in case there are more than one.\n            # This sometimes happens when a device sends its own XML config files.\n            xml = etree.fromstring(self._remove_extraneous_xml_declarations(xml_str))\n        except ValueError:\n            # This can occur when requests returns a `str` (unicode) but there's also an XML\n            # declaration, which lxml doesn't like.\n            xml = etree.fromstring(xml_str.encode('utf8'))\n\n        response = xml.find(\".//{%s}%sResponse\" % (self.service_type, action_name))\n        if response is None:\n            msg = ('Returned XML did not include an element which matches namespace %r and tag name'\n                   ' \\'%sResponse\\'.' % (self.service_type, action_name))\n            self._log.debug(msg + '\\n' + etree.tostring(xml, pretty_print=True).decode('utf8'))\n            raise SOAPProtocolError(msg)\n\n        # Sometimes devices return XML strings as their argument values without escaping them with\n        # CDATA. This checks to see if the argument has been parsed as XML and un-parses it if so.\n        ret = {}\n        for arg in response.getchildren():\n            children = arg.getchildren()\n            if children:\n                ret[arg.tag] = b\"\\n\".join(etree.tostring(x) for x in children)\n            else:\n                ret[arg.tag] = arg.text\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the control XML file and populates self. services with Service instances.", "response": "def _read_services(self):\n        \"\"\"\n        Read the control XML file and populate self.services with a list of\n        services in the form of Service class instances.\n        \"\"\"\n        # The double slash in the XPath is deliberate, as services can be\n        # listed in two places (Section 2.3 of uPNP device architecture v1.1)\n        for node in self._findall('device//serviceList/service'):\n            findtext = partial(node.findtext, namespaces=self._root_xml.nsmap)\n            svc = Service(\n                self,\n                self._url_base,\n                findtext('serviceType'),\n                findtext('serviceId'),\n                findtext('controlURL'),\n                findtext('SCPDURL'),\n                findtext('eventSubURL')\n            )\n            self._log.debug(\n                '%s: Service %r at %r', self.device_name, svc.service_type, svc.scpd_url)\n            self.services.append(svc)\n            self.service_map[svc.name] = svc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_action(self, action_name):\n        for service in self.services:\n            action = service.find_action(action_name)\n            if action is not None:\n                return action", "response": "Find an action by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subscribe(self, callback_url, timeout=None):\n        url = urljoin(self._url_base, self._event_sub_url)\n        headers = dict(\n            HOST=urlparse(url).netloc,\n            CALLBACK='<%s>' % callback_url,\n            NT='upnp:event'\n        )\n        if timeout is not None:\n            headers['TIMEOUT'] = 'Second-%s' % timeout\n        resp = requests.request('SUBSCRIBE', url, headers=headers, auth=self.device.http_auth)\n        resp.raise_for_status()\n        return Service.validate_subscription_response(resp)", "response": "Subscribe to the events offered by this service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrenew a previously configured subscription.", "response": "def renew_subscription(self, sid, timeout=None):\n        \"\"\"\n        Renews a previously configured subscription.\n        \"\"\"\n        url = urljoin(self._url_base, self._event_sub_url)\n        headers = dict(\n            HOST=urlparse(url).netloc,\n            SID=sid\n        )\n        if timeout is not None:\n            headers['TIMEOUT'] = 'Second-%s' % timeout\n        resp = requests.request('SUBSCRIBE', url, headers=headers, auth=self.device.http_auth)\n        resp.raise_for_status()\n        return Service.validate_subscription_renewal_response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cancel_subscription(self, sid):\n        url = urljoin(self._url_base, self._event_sub_url)\n        headers = dict(\n            HOST=urlparse(url).netloc,\n            SID=sid\n        )\n        resp = requests.request('UNSUBSCRIBE', url, headers=headers, auth=self.device.http_auth)\n        resp.raise_for_status()", "response": "Unsubscribes from a previously configured subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_arg(arg, argdef):\n        datatype = argdef['datatype']\n        reasons = set()\n        ranges = {\n            'ui1': (int, 0, 255),\n            'ui2': (int, 0, 65535),\n            'ui4': (int, 0, 4294967295),\n            'i1': (int, -128, 127),\n            'i2': (int, -32768, 32767),\n            'i4': (int, -2147483648, 2147483647),\n            'r4': (Decimal, Decimal('3.40282347E+38'), Decimal('1.17549435E-38'))\n        }\n        try:\n            if datatype in set(ranges.keys()):\n                v_type, v_min, v_max = ranges[datatype]\n                if not v_min <= v_type(arg) <= v_max:\n                    reasons.add('%r datatype must be a number in the range %s to %s' % (\n                        datatype, v_min, v_max))\n\n            elif datatype in {'r8', 'number', 'float', 'fixed.14.4'}:\n                v = Decimal(arg)\n                if v < 0:\n                    assert Decimal('-1.79769313486232E308') <= v <= Decimal('4.94065645841247E-324')\n                else:\n                    assert Decimal('4.94065645841247E-324') <= v <= Decimal('1.79769313486232E308')\n\n            elif datatype == 'char':\n                v = arg.decode('utf8') if six.PY2 or isinstance(arg, bytes) else arg\n                assert len(v) == 1\n\n            elif datatype == 'string':\n                v = arg.decode(\"utf8\") if six.PY2 or isinstance(arg, bytes) else arg\n                if argdef['allowed_values'] and v not in argdef['allowed_values']:\n                    reasons.add('Value %r not in allowed values list' % arg)\n\n            elif datatype == 'date':\n                v = parse_date(arg)\n                if any((v.hour, v.minute, v.second)):\n                    reasons.add(\"'date' datatype must not contain a time\")\n\n            elif datatype in ('dateTime', 'dateTime.tz'):\n                v = parse_date(arg)\n                if datatype == 'dateTime' and v.tzinfo is not None:\n                    reasons.add(\"'dateTime' datatype must not contain a timezone\")\n\n            elif datatype in ('time', 'time.tz'):\n                now = datetime.datetime.utcnow()\n                v = parse_date(arg, default=now)\n                if v.tzinfo is not None:\n                    now += v.utcoffset()\n                if not all((\n                        v.day == now.day,\n                        v.month == now.month,\n                        v.year == now.year)):\n                    reasons.add('%r datatype must not contain a date' % datatype)\n                if datatype == 'time' and v.tzinfo is not None:\n                    reasons.add('%r datatype must not have timezone information' % datatype)\n\n            elif datatype == 'boolean':\n                valid = {'true', 'yes', '1', 'false', 'no', '0'}\n                if arg.lower() not in valid:\n                    reasons.add('%r datatype must be one of %s' % (datatype, ','.join(valid)))\n\n            elif datatype == 'bin.base64':\n                b64decode(arg)\n\n            elif datatype == 'bin.hex':\n                unhexlify(arg)\n\n            elif datatype == 'uri':\n                urlparse(arg)\n\n            elif datatype == 'uuid':\n                if not re.match(\n                        r'^[0-9a-f]{8}\\-[0-9a-f]{4}\\-[0-9a-f]{4}\\-[0-9a-f]{4}\\-[0-9a-f]{12}$',\n                        arg, re.I):\n                    reasons.add('%r datatype must contain a valid UUID')\n\n            else:\n                reasons.add(\"%r datatype is unrecognised.\" % datatype)\n\n        except ValueError as exc:\n            reasons.add(str(exc))\n\n        return not bool(len(reasons)), reasons", "response": "Validate an incoming unicode string argument according the UPnP spec. Raises UPNPError if the argument is invalid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of UPnP devices on the network.", "response": "def discover(timeout=5):\n    \"\"\"\n    Convenience method to discover UPnP devices on the network. Returns a\n    list of `upnp.Device` instances. Any invalid servers are silently\n    ignored.\n    \"\"\"\n    devices = {}\n    for entry in scan(timeout):\n        if entry.location in devices:\n            continue\n        try:\n            devices[entry.location] = Device(entry.location)\n        except Exception as exc:\n            log = _getLogger(\"ssdp\")\n            log.error('Error \\'%s\\' for %s', exc, entry.location)\n    return list(devices.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_transitions(initial_state, accepting_states, transition_function):\n        # type: (State, Set[State], TransitionFunction) -> DFA\n        \"\"\"\n        Initialize a DFA without explicitly specifying the set of states and the alphabet.\n\n        :param initial_state: the initial state.\n        :param accepting_states: the accepting state.\n        :param transition_function: the transition function.\n        :return: the DFA.\n        \"\"\"\n        states, alphabet = _extract_states_from_transition_function(transition_function)\n\n        return DFA(states, alphabet, initial_state, accepting_states, transition_function)", "response": "Initialize a new DFA with the given initial state and accepting states and transition function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dot(self, path: str, title: Optional[str] = None):\n        g = graphviz.Digraph(format='svg')\n        g.node('fake', style='invisible')\n        for state in self._states:\n            if state == self._initial_state:\n                if state in self._accepting_states:\n                    g.node(str(state), root='true',\n                           shape='doublecircle')\n                else:\n                    g.node(str(state), root='true')\n            elif state in self._accepting_states:\n                g.node(str(state), shape='doublecircle')\n            else:\n                g.node(str(state))\n\n        g.edge('fake', str(self._initial_state), style='bold')\n        for start in self._transition_function:\n            for symbol, end in self._transition_function[start].items():\n                g.edge(str(start),\n                       str(end),\n                       label=str(symbol))\n\n        if title:\n            g.attr(label=title)\n            g.attr(fontsize='20')\n\n        g.render(filename=path)\n        return", "response": "Print the automaton to a dot file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict from states to level i. e. the number of steps to reach any accepting state.", "response": "def levels_to_accepting_states(self) -> dict:\n        \"\"\"Return a dict from states to level, i.e. the number of steps to reach any accepting state.\n        level = -1 if the state cannot reach any accepting state\"\"\"\n\n        res = {accepting_state: 0 for accepting_state in self._accepting_states}\n        level = 0\n\n        # least fixpoint\n        z_current, z_next = set(), set()\n        z_next = set(self._accepting_states)\n\n        while z_current != z_next:\n            level += 1\n            z_current = z_next\n            z_next = copy(z_current)\n            for state in self._transition_function:\n                for action in self._transition_function[state]:\n                    if state in z_current:\n                        continue\n                    next_state = self._transition_function[state][action]\n                    if next_state in z_current:\n                        z_next.add(state)\n                        res[state] = level\n                        break\n\n        z_current = z_next\n        for failure_state in filter(lambda x: x not in z_current, self._states):\n            res[failure_state] = -1\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef powerset(iterable):\n    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n    s = list(set(iterable))\n    combs = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n    res = set(frozenset(x) for x in combs)\n    # res = map(frozenset, combs)\n    return res", "response": "powerset - returns a set of all combinations of the given iterable"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef determinize(self) -> DFA:\n\n        nfa = self\n\n        new_states = {macro_state for macro_state in powerset(nfa._states)}\n        initial_state = frozenset([nfa._initial_state])\n        final_states = {q for q in new_states if len(q.intersection(nfa._accepting_states)) != 0}\n        transition_function = {}\n\n        for state_set in new_states:\n            for action in nfa._alphabet:\n\n                next_macrostate = set()\n                for s in state_set:\n                    for next_state in nfa._transition_function.get(s, {}).get(action, set()):\n                        next_macrostate.add(next_state)\n\n                next_macrostate = frozenset(next_macrostate)\n                transition_function.setdefault(state_set, {})[action] = next_macrostate\n\n        return DFA(new_states, set(nfa._alphabet), initial_state, set(final_states), transition_function)", "response": "Determinize the NFA\n\n        :return: the DFA equivalent to the DFA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_transitions(cls, initial_state, accepting_states, transition_function):\n        # type: (State, Set[State], NondeterministicTransitionFunction) -> NFA\n        \"\"\"\n        Initialize a DFA without explicitly specifying the set of states and the alphabet.\n\n        :param initial_state: the initial state.\n        :param accepting_states: the accepting state.\n        :param transition_function: the (nondeterministic) transition function.\n        :return: the NFA.\n        \"\"\"\n        states, alphabet = _extract_states_from_nondeterministic_transition_function(transition_function)\n\n        return NFA(states, alphabet, initial_state, accepting_states, transition_function)", "response": "Initialize a new NFA with the given initial state and accepting states and transition function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the config file for username and password", "response": "def userpass(self, dir=\"ppcoin\"):\n        \"\"\"Reads config file for username/password\"\"\"\n\n        source = os.path.expanduser(\"~/.{0}/{0}.conf\").format(dir)\n        dest = open(source, \"r\")\n        with dest as conf:\n            for line in conf:\n                if line.startswith(\"rpcuser\"):\n                    username = line.split(\"=\")[1].strip()\n                if line.startswith(\"rpcpassword\"):\n                    password = line.split(\"=\")[1].strip()\n\n        return username, password"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends request to ppcoind", "response": "def req(self, method, params=()):\n        \"\"\"send request to ppcoind\"\"\"\n\n        response = self.session.post(\n            self.url,\n            data=json.dumps({\"method\": method, \"params\": params, \"jsonrpc\": \"1.1\"}),\n        ).json()\n\n        if response[\"error\"] is not None:\n            return response[\"error\"]\n        else:\n            return response[\"result\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a batch request using jsonrpc 2. 0", "response": "def batch(self, reqs):\n        \"\"\" send batch request using jsonrpc 2.0 \"\"\"\n\n        batch_data = []\n\n        for req_id, req in enumerate(reqs):\n            batch_data.append(\n                {\"method\": req[0], \"params\": req[1], \"jsonrpc\": \"2.0\", \"id\": req_id}\n            )\n\n        data = json.dumps(batch_data)\n        response = self.session.post(self.url, data=data).json()\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse to unlock wallet for minting", "response": "def walletpassphrase(self, passphrase, timeout=99999999, mint_only=True):\n        \"\"\"used to unlock wallet for minting\"\"\"\n        return self.req(\"walletpassphrase\", [passphrase, timeout, mint_only])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning detail block info", "response": "def getblock(self, blockhash, decode=False):\n        \"\"\"returns detail block info.\"\"\"\n\n        if not decode:\n            decode = \"false\"\n\n            return self.req(\"getblock\", [blockhash, decode])\n\n        else:\n            return self.req(\"getblock\", [blockhash])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getbalance(self, account=None, minconf=6):\n        if account:\n            return self.req(\"getbalance\", [account, minconf])\n        else:\n            return self.req(\"getbalance\")", "response": "retrieve balance in account"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a amount of data to a specific address. Returns txid.", "response": "def sendtoaddress(self, recv_addr, amount, comment=\"\"):\n        \"\"\"send ammount to address, with optional comment. Returns txid.\n        sendtoaddress(ADDRESS, AMMOUNT, COMMENT)\"\"\"\n        return self.req(\"sendtoaddress\", [recv_addr, amount, comment])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendfrom(self, account, address, amount):\n        return self.req(\"sendfrom\", [account, address, amount])", "response": "send outgoing tx from a given account to a given address"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sendmany(self, recv_dict, account=\"\", comment=\"\"):\n        # {\"addr1\":#coin,\"addr2\":#coin,\"addr3\":#coin...}\n        return self.req(\"sendmany\", [account, recv_dict, comment])", "response": "send outgoing tx to many addresses input is dict of addr : coins returns txid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listtransactions(self, account=\"\", many=999, since=0):\n        return self.req(\"listtransactions\", [account, many, since])", "response": "list all transactions associated with this wallet"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new transaction.", "response": "def createrawtransaction(self, inputs, outputs):\n        \"\"\"[{\"txid\":input_txid,\"vout\":0}, ...], {recv_addr: amount, change: amount, ...}\"\"\"\n        if not isinstance(outputs, dict):\n            raise TypeError(\"outputs variable must be a dictionary\")\n        if not isinstance(inputs, list):\n            raise TypeError(\"inputs variable must be a list\")\n        return self.req(\"createrawtransaction\", [inputs, outputs])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef signrawtransaction(self, rawtxhash, parent_tx_outputs=None, private_key=None):\n\n        if not parent_tx_outputs and not private_key:\n            return self.req(\"signrawtransaction\", [rawtxhash])\n        else:\n            return self.req(\n                \"signrawtransaction\", [rawtxhash, parent_tx_outputs, private_key]\n            )", "response": "signrawtransaction returns status and rawtxhash"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verifymessage(self, address, signature, message):\n        return self.req(\"verifymessage\", [address, signature, message])", "response": "Verify a signed message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef coroutine(f):\n    @wraps(f)\n    def g(*args, **kwargs):\n        sink = f(*args, **kwargs)\n        sink.send(None)\n        return sink\n\n    return g", "response": "A coroutine function that returns a new object that can be used to yield the next item in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maybe(func):\n\n    name = object_name(func)\n\n    @wraps(func)\n    def maybe_wrapped(*args, **kwargs):\n        \"\"\"@maybe wrapped version of ``func``.\"\"\"\n        fails = [\n            (name, k, v)\n            for k, v in chain(enumerate(args), kwargs.items())\n            if isinstance(v, Fail)]\n\n        if fails:\n            return Fail(func, fails=fails)\n\n        try:\n            result = func(*args, **kwargs)\n\n        except Exception as exc:\n            return Fail(func, exception=exc)\n\n        else:\n            if isinstance(result, Fail):\n                result.add_call(func)\n\n            return result\n\n    return maybe_wrapped", "response": "A function decorator that returns a Fail object if any of the arguments to the function f fails in any way."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_call(self, func):\n        self.trace.append(\"{} ({}:{})\".format(\n            object_name(func),\n            inspect.getsourcefile(func),\n            inspect.getsourcelines(func)[1]))\n\n        return self", "response": "Add a call to the trace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all(pred: Callable, xs: Iterable):\n    for x in xs:\n        if not pred(x):\n            return False\n\n    return True", "response": "Check whether all the elements of the iterable xs have a predicate pred."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if at least one element of the iterable xs fullfills predicate pred.", "response": "def any(pred: Callable, xs: Iterable):\n    \"\"\"\n    Check if at least one element of the iterable `xs`\n    fullfills predicate `pred`.\n\n    :param pred:\n       predicate function.\n    :param xs:\n       iterable object.\n    :returns: boolean\n    \"\"\"\n    b = find_first(pred, xs)\n\n    return True if b is not None else False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(pred: Callable, xs: Iterable):\n    generator = (x for x in xs if pred(x))\n\n    return gather(*generator)", "response": "Applies a predicate to a list returning a PromisedObject containing the values satisfying the predicate."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntraverses an iterable object while performing stateful computations with the elements.", "response": "def fold(\n        fun: Callable, state: Any, xs: Iterable):\n    \"\"\"\n    Traverse an iterable object while performing stateful computations\n    with the elements. It returns a :py:class:`PromisedObject` containing\n    the result of the stateful computations.\n\n    For a general definition of folding see:\n    https://en.wikipedia.org/wiki/Fold_(higher-order_function)\n\n    :param fun: stateful function.\n    :param state: initial state.\n    :param xs: iterable object.\n    :returns: :py:class:`PromisedObject`\n    \"\"\"\n    def generator(state):\n        for x in xs:\n            state, r = unpack(fun(state, x), 2)\n            yield r\n\n    return gather(*generator(state))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(fun: Callable, xs: Iterable):\n    generator = (fun(x) for x in xs)\n\n    return gather(*generator)", "response": "Traverse an iterable object applying function fun to each element and then creats a workflow from it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfuse two Iterable object using the function fun.", "response": "def zip_with(fun: Callable, xs: Iterable, ys: Iterable):\n    \"\"\"\n    Fuse two Iterable object using the function `fun`.\n    Notice that if the two objects have different len,\n    the shortest object gives the result's shape.\n\n    :param fun:\n       function taking two argument use to process\n       element x from `xs` and y from `ys`.\n\n    :param xs:\n       first iterable.\n\n    :param ys:\n       second iterable.\n\n    returns::py:class:`PromisedObject`\n    \"\"\"\n    generator = (fun(*rs) for rs in zip(xs, ys))\n\n    return gather(*generator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts argument to a string.", "response": "def _arg_to_str(arg):\n    \"\"\"Convert argument to a string.\"\"\"\n    if isinstance(arg, str):\n        return _sugar(repr(arg))\n    elif arg is Empty:\n        return '\\u2014'\n    else:\n        return _sugar(str(arg))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the node is ready to be used in the tree.", "response": "def is_node_ready(node):\n    \"\"\"Returns True if none of the argument holders contain any `Empty` object.\n    \"\"\"\n    return all(ref_argument(node.bound_args, a) is not Empty\n               for a in serialize_arguments(node.bound_args))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data(self):\n        return NodeData(\n            self.foo, get_arguments(self.bound_args), self.hints)", "response": "Convert to a : py : class : NodeData for subsequent serial."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(self, job):\n        with self.lock:\n            self.cur.execute(\n                'insert into \"jobs\" (\"name\", \"session\", \"status\") '\n                'values (?, ?, ?)', (job.name, self.session, Status.INACTIVE))\n            self.jobs[self.cur.lastrowid] = job\n            return JobMessage(self.cur.lastrowid, job.node)", "response": "Takes a job and adds it to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_job_to_db(self, key, job):\n        job_msg = self.registry.deep_encode(job)\n        prov = prov_key(job_msg)\n\n        def set_link(duplicate_id):\n            self.cur.execute(\n                'update \"jobs\" set \"link\" = ?, \"status\" = ? where \"id\" = ?',\n                (duplicate_id, Status.DUPLICATE, key))\n\n        with self.lock:\n            self.cur.execute(\n                'select * from \"jobs\" where \"prov\" = ? '\n                'and ((\"result\" is not null) or '\n                '(\"session\" = ? and \"link\" is null))',\n                (prov, self.session))\n            rec = self.cur.fetchone()\n            rec = JobEntry(*rec) if rec is not None else None\n\n            self.cur.execute(\n                'update \"jobs\" set \"prov\" = ?, \"version\" = ?, \"function\" = ?, '\n                '\"arguments\" = ?, \"status\" = ? where \"id\" = ?',\n                (prov, job_msg['data']['hints'].get('version'),\n                 json.dumps(job_msg['data']['function']),\n                 json.dumps(job_msg['data']['arguments']),\n                 Status.WAITING,\n                 key))\n\n            if not rec:\n                # no duplicate found, go on\n                return 'initialized', None\n\n            set_link(rec.id)\n\n            if rec.result is not None and rec.status == Status.WORKFLOW:\n                # the found duplicate returned a workflow\n                if rec.link is not None:\n                    # link is set, so result is fully realized\n                    self.cur.execute(\n                        'select * from \"jobs\" where \"id\" = ?',\n                        (rec.link,))\n                    rec = self.cur.fetchone()\n                    assert rec is not None, \"database integrity violation\"\n                    rec = JobEntry(*rec)\n\n                else:\n                    # link is not set, the result is still waited upon\n                    assert rec.session == self.session, \\\n                        \"database integrity violation\"\n                    self.attached[rec.id].append(key)\n                    return 'attached', None\n\n            if rec.result is not None:\n                # result is found! return it\n                result_value = self.registry.from_json(rec.result, deref=True)\n                result = ResultMessage(\n                    key, 'retrieved', result_value, None)\n                return 'retrieved', result\n\n            if rec.session == self.session:\n                # still waiting for result, attach\n                self.attached[rec.id].append(key)\n                return 'attached', None", "response": "Add a job to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef job_exists(self, prov):\n        with self.lock:\n            self.cur.execute('select * from \"jobs\" where \"prov\" = ?;', (prov,))\n            rec = self.cur.fetchone()\n            return rec is not None", "response": "Check if a job exists in the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_result_in_db(self, result, always_cache=True):\n        job = self[result.key]\n\n        def extend_dependent_links():\n            with self.lock:\n                new_workflow_id = id(get_workflow(result.value))\n                self.links[new_workflow_id].extend(\n                    self.links[id(job.workflow)])\n                del self.links[id(job.workflow)]\n\n        def store_result(status):\n            result_value_msg = self.registry.to_json(result.value)\n            with self.lock:\n                self.cur.execute(\n                    'update \"jobs\" set \"result\" = ?, '\n                    '\"status\" = ? where \"id\" = ?;',\n                    (result_value_msg, status, result.key))\n\n        def acquire_links():\n            with self.lock:\n                linked_keys = tuple(self.links[id(job.workflow)])\n                del self.links[id(job.workflow)]\n\n                # update links for jobs up in the call-stack (parent workflows)\n                n_questions = ','.join('?' * len(linked_keys))\n                self.cur.execute(\n                    'update \"jobs\" set \"link\" = ? where \"id\" in ({});'\n                    .format(n_questions),\n                    (result.key,) + linked_keys)\n\n                # jobs that were attached to the parent workflow(s) will not\n                # receive the current result automatically, so we need to force\n                # feed them to the scheduler.\n                attached_keys = ()\n                for k in linked_keys:\n                    attached_keys += tuple(self.attached[k])\n                    del self.attached[k]\n\n            return attached_keys\n\n        # if the returned job is not set to be stored, but a parent job\n        # is, we still need to store the result.\n        if 'store' not in job.hints and not always_cache:\n            if job.is_root_node and id(job.workflow) in self.links:\n                if is_workflow(result.value):\n                    extend_dependent_links()\n                    return ()\n                else:\n                    store_result(Status.LINKEE)\n                    return acquire_links()\n            else:\n                return ()\n\n        # if the return value is a workflow, store the workflow, and add\n        # links to this job, to be updated when the resolved result comes in\n        if is_workflow(result.value):\n            store_result(Status.WORKFLOW)\n            with self.lock:\n                self.links[id(get_workflow(result.value))].append(result.key)\n            if job.is_root_node:\n                extend_dependent_links()\n            return ()\n\n        store_result(Status.STORED)\n        with self.lock:\n            attached_keys = tuple(self.attached[result.key])\n            del self.attached[result.key]\n\n        if job.is_root_node:\n            attached_keys += acquire_links()\n\n        return attached_keys", "response": "Store a result in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a timestamp to the database.", "response": "def add_time_stamp(self, db_id, name):\n        \"\"\"Add a timestamp to the database.\"\"\"\n        with self.lock:\n            self.cur.execute(\n                'insert into \"timestamps\" (\"job\", \"what\")'\n                'values (?, ?);', (db_id, name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef static_sum(values, limit_n=1000):\n    if len(values) < limit_n:\n        return sum(values)\n\n    else:\n        half = len(values) // 2\n        return add(\n                static_sum(values[:half], limit_n),\n                static_sum(values[half:], limit_n))", "response": "Example of static sum routine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nerasing queue and set end - of - queue message.", "response": "def flush(self):\n        \"\"\"Erases queue and set `end-of-queue` message.\"\"\"\n        while not self._queue.empty():\n            self._queue.get()\n            self._queue.task_done()\n        self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding links to a node.", "response": "def find_links_to(links, node):\n    \"\"\"Find links to a node.\n\n    :param links:\n        forward links of a workflow\n    :type links: Mapping[NodeId, Set[(NodeId, ArgumentType, [int|str]])]\n\n    :param node:\n        index to a node\n    :type node: int\n\n    :returns:\n        dictionary of sources for each argument\n    :rtype: Mapping[(ArgumentType, [int|str]), NodeId]\n    \"\"\"\n    return {address: src\n            for src, (tgt, address) in _all_valid(links)\n            if tgt == node}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over all links and forgetting emtpy registers.", "response": "def _all_valid(links):\n    \"\"\"Iterates over all links, forgetting emtpy registers.\"\"\"\n    for k, v in links.items():\n        for i in v:\n            yield k, i"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_call(foo, args, kwargs, hints, call_by_value=True):\n    # create the bound_args object\n    bound_args = signature(foo).bind(*args, **kwargs)\n    bound_args.apply_defaults()\n\n    # get the name of the variadic argument if there is one\n    variadic = next((x.name\n                     for x in bound_args.signature.parameters.values()\n                     if x.kind == Parameter.VAR_POSITIONAL), None)\n\n    # *HACK*\n    # the BoundArguments class uses a tuple to store the\n    # variadic arguments. Since we need to modify them,\n    # we have to replace the tuple with a list. This works, for now...\n    if variadic:\n        if variadic not in bound_args.arguments:\n            bound_args.arguments[variadic] = []\n        else:\n            bound_args.arguments[variadic] = \\\n                list(bound_args.arguments[variadic])\n\n    # create the node and initialise hash key\n    node = FunctionNode(foo, bound_args, hints)\n\n    # setup the new workflow\n    root = id(node)\n    nodes = {root: node}\n    links = {root: set()}\n\n    # walk the arguments to the function call\n    for address in serialize_arguments(node.bound_args):\n        arg = ref_argument(node.bound_args, address)\n\n        # the argument may still become a workflow if it\n        # has the __deepcopy__ operator overloaded to return a workflow\n        call_by_ref = 'call_by_ref' in hints and \\\n            (hints['call_by_ref'] is True or\n             address.name in hints['call_by_ref'])\n\n        if not is_workflow(arg) and call_by_value and not call_by_ref:\n            arg = deepcopy(arg)\n\n        # if still not a workflow, we have a plain value!\n        if not is_workflow(arg):\n            set_argument(node.bound_args, address, arg)\n            continue\n\n        # merge the argument workflow into the new workflow\n        workflow = get_workflow(arg)\n        set_argument(node.bound_args, address, Empty)\n        for n in workflow.nodes:\n            if n not in nodes:\n                nodes[n] = workflow.nodes[n]\n                links[n] = set()\n\n            links[n].update(workflow.links[n])\n\n        links[workflow.root].add((root, address))\n\n    return Workflow(root, nodes, links)", "response": "Takes a function and a set of arguments and returns a new workflow representing the promised value of the function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing a global provenance key for the entire workflow before evaluation. This key can be used to store and retrieve the results in a database.", "response": "def set_global_provenance(wf: Workflow, registry: Registry):\n    \"\"\"Compute a global provenance key for the entire workflow\n    before evaluation. This key can be used to store and retrieve\n    results in a database. The key computed in this stage is different\n    from the (local) provenance key that can be computed for a node\n    if all its arguments are known.\n\n    In cases where a result derives from other results that were\n    computed in child workflows, we can prevent the workflow system\n    from reevaluating the results at each step to find that we already\n    had the end-result somewhere. This is where the global prov-key\n    comes in.\n\n    Each node is assigned a `prov` attribute. If all arguments for this\n    node are known, this key will be the same as the local prov-key.\n    If some of the arguments are still empty, we add the global prov-keys\n    of the dependent nodes to the hash.\n\n    In this algorithm we traverse from the bottom of the DAG to the top\n    and back using a stack. This allows us to compute the keys for each\n    node without modifying the node other than setting the `prov` attribute\n    with the resulting key.\"\"\"\n    stack = [wf.root]\n\n    while stack:\n        i = stack.pop()\n        n = wf.nodes[i]\n\n        if n.prov:\n            continue\n\n        if is_node_ready(n):\n            job_msg = registry.deep_encode(n)\n            n.prov = prov_key(job_msg)\n            continue\n\n        deps = wf.inverse_links[i]\n        todo = [j for j in deps if not wf.nodes[j].prov]\n\n        if not todo:\n            link_dict = dict(links(wf, i, deps))\n            link_prov = registry.deep_encode(\n                [link_dict[arg] for arg in empty_args(n)])\n            job_msg = registry.deep_encode(n)\n            n.prov = prov_key(job_msg, link_prov)\n            continue\n\n        stack.append(i)\n        stack.extend(deps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a logger component.", "response": "def make_logger(name, stream_type, jobs):\n    \"\"\"Create a logger component.\n\n    :param name: name of logger child, i.e. logger will be named\n        `noodles.<name>`.\n    :type name: str\n    :param stream_type: type of the stream that this logger will\n        be inserted into, should be |pull_map| or |push_map|.\n    :type stream_type: function\n    :param jobs: job-keeper instance.\n    :type jobs: dict, |JobKeeper| or |JobDB|.\n\n    :return: a stream.\n\n    The resulting stream receives messages and sends them on after\n    sending an INFO message to the logger. In the case of a |JobMessage|\n    or |ResultMessage| a meaningful message is composed otherwise the\n    string representation of the object is passed.\"\"\"\n    logger = logging.getLogger('noodles').getChild(name)\n    # logger.setLevel(logging.DEBUG)\n\n    @stream_type\n    def log_message(message):\n        if message is EndOfQueue:\n            logger.info(\"-end-of-queue-\")\n\n        elif isinstance(message, JobMessage):\n            logger.info(\n                \"job    %10s: %s\", message.key, message.node)\n\n        elif isinstance(message, ResultMessage):\n            job = jobs[message.key]\n            if is_workflow(message.value):\n                logger.info(\n                    \"result %10s [%s]: %s -> workflow %x\", message.key,\n                    job.node, message.status, id(message.value))\n            else:\n                value_string = repr(message.value)\n                logger.info(\n                    \"result %10s [%s]: %s -> %s\", message.key, job.node,\n                    message.status, _sugar(value_string))\n\n        else:\n            logger.info(\n                \"unknown message: %s\", message)\n\n        return message\n\n    return log_message"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_worker(registry, verbose=False, jobdirs=False,\n                   init=None, finish=None, status=True):\n    \"\"\"Process worker\"\"\"\n    name = \"process-\" + str(uuid.uuid4())\n\n    cmd = [sys.prefix + \"/bin/python\", \"-m\", \"noodles.pilot_job\",\n           \"-name\", name, \"-registry\", object_name(registry)]\n    if verbose:\n        cmd.append(\"-verbose\")\n    if jobdirs:\n        cmd.append(\"-jobdirs\")\n    if not status:\n        cmd.append(\"-nostatus\")\n    if init:\n        cmd.extend([\"-init\", object_name(init)])\n    if finish:\n        cmd.extend([\"-finish\", object_name(finish)])\n\n    remote = Popen(\n        cmd,\n        stdin=PIPE, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n\n    def read_stderr():\n        \"\"\"Read stderr of remote process and sends lines to logger.\"\"\"\n        for line in remote.stderr:\n            print(name + \": \" + line.rstrip())\n\n    stderr_reader_thread = threading.Thread(target=read_stderr, daemon=True)\n    stderr_reader_thread.start()\n\n    @push\n    def send_job():\n        \"\"\"Coroutine, sends jobs to remote worker over standard input.\"\"\"\n        reg = registry()\n\n        sink = JSONObjectWriter(reg, remote.stdin)\n\n        while True:\n            msg = yield\n            if msg is EndOfQueue:\n                try:\n                    sink.send(EndOfWork)\n                except StopIteration:\n                    pass\n\n                remote.wait()\n                stderr_reader_thread.join()\n\n                return\n\n            if msg is FlushQueue:\n                continue\n\n            sink.send(msg)\n\n    @pull\n    def get_result():\n        \"\"\"Generator, reading results from process standard output.\"\"\"\n        reg = registry()\n        yield from JSONObjectReader(reg, remote.stdout)\n\n    return Connection(get_result, send_job)", "response": "Process a single process and return a connection object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_process(workflow, *, n_processes, registry,\n                verbose=False, jobdirs=False,\n                init=None, finish=None, deref=False):\n    \"\"\"Run the workflow using a number of new python processes. Use this\n    runner to test the workflow in a situation where data serial\n    is needed.\n\n    :param workflow:\n        The workflow.\n    :type workflow: `Workflow` or `PromisedObject`\n\n    :param n_processes:\n        Number of processes to start.\n\n    :param registry:\n        The serial registry.\n\n    :param verbose:\n        Request verbose output on worker side\n\n    :param jobdirs:\n        Create a new directory for each job to prevent filename collision.(NYI)\n\n    :param init:\n        An init function that needs to be run in each process before other jobs\n        can be run. This should be a scheduled function returning True on\n        success.\n\n    :param finish:\n        A function that wraps up when the worker closes down.\n\n    :param deref:\n        Set this to True to pass the result through one more encoding and\n        decoding step with object derefencing turned on.\n    :type deref: bool\n\n    :returns: the result of evaluating the workflow\n    :rtype: any\n    \"\"\"\n    workers = {}\n    for i in range(n_processes):\n        new_worker = process_worker(registry, verbose, jobdirs, init, finish)\n        workers['worker {0:2}'.format(i)] = new_worker\n\n    worker_names = list(workers.keys())\n\n    def random_selector(_):\n        \"\"\"Selects a worker to send a job to at random.\"\"\"\n        return random.choice(worker_names)\n\n    master_worker = hybrid_threaded_worker(random_selector, workers)\n    result = Scheduler().run(master_worker, get_workflow(workflow))\n\n    for worker in workers.values():\n        try:\n            worker.sink().send(EndOfQueue)\n        except StopIteration:\n            pass\n\n#        w.aux.join()\n\n    if deref:\n        return registry().dereference(result, host='localhost')\n    else:\n        return result", "response": "Run the workflow using a number of new python processes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse Xenon to run a single remote interactive worker.", "response": "def xenon_interactive_worker(\n        machine, worker_config, input_queue=None, stderr_sink=None):\n    \"\"\"Uses Xenon to run a single remote interactive worker.\n\n    Jobs are read from stdin, and results written to stdout.\n\n    :param machine:\n        Specification of the machine on which to run.\n    :type machine: noodles.run.xenon.Machine\n\n    :param worker_config:\n        Job configuration. Specifies the command to be run remotely.\n    :type worker_config: noodles.run.xenon.XenonJobConfig\n    \"\"\"\n    if input_queue is None:\n        input_queue = Queue()\n\n    registry = worker_config.registry()\n\n    @pull_map\n    def serialise(obj):\n        \"\"\"Serialise incoming objects, yielding strings.\"\"\"\n        if isinstance(obj, JobMessage):\n            print('serializing:', str(obj.node), file=sys.stderr)\n        return (registry.to_json(obj, host='scheduler') + '\\n').encode()\n\n    @pull_map\n    def echo(line):\n        print('{} input: {}'.format(worker_config.name, line), file=sys.stderr)\n        return line\n\n    def do_iterate(source):\n        for x in source():\n            if x is EndOfQueue:\n                yield EndOfWork\n                return\n            yield x\n\n    job, output_stream = machine.scheduler.submit_interactive_job(\n        worker_config.xenon_job_description,\n        echo(lambda: serialise(lambda: do_iterate(input_queue.source))))\n\n    @sink_map\n    def echo_stderr(text):\n        \"\"\"Print lines.\"\"\"\n        for line in text.split('\\n'):\n            print(\"{}: {}\".format(worker_config.name, line), file=sys.stderr)\n\n    if stderr_sink is None:\n        stderr_sink = echo_stderr()\n\n    @pull\n    def read_output(source):\n        \"\"\"Handle output from job, sending stderr data to given\n        `stderr_sink`, passing on lines from stdout.\"\"\"\n        line_buffer = \"\"\n        try:\n            for chunk in source():\n                if chunk.stdout:\n                    lines = chunk.stdout.decode().splitlines(keepends=True)\n\n                    if not lines:\n                        continue\n\n                    if lines[0][-1] == '\\n':\n                        yield line_buffer + lines[0]\n                        line_buffer = \"\"\n                    else:\n                        line_buffer += lines[0]\n\n                    if len(lines) == 1:\n                        continue\n\n                    yield from lines[1:-1]\n\n                    if lines[-1][-1] == '\\n':\n                        yield lines[-1]\n                    else:\n                        line_buffer = lines[-1]\n\n                if chunk.stderr:\n                    for line in chunk.stderr.decode().split('\\n'):\n                        stripped_line = line.strip()\n                        if stripped_line != '':\n                            stderr_sink.send(stripped_line)\n\n        except grpc.RpcError as e:\n            return\n\n    @pull_map\n    def deserialise(line):\n        result = registry.from_json(line, deref=False)\n        return result\n\n    return Connection(\n        lambda: deserialise(lambda: read_output(lambda: output_stream)),\n        input_queue.sink, aux=job)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits until the remote worker is running.", "response": "def wait_until_running(self, callback=None):\n        \"\"\"Waits until the remote worker is running, then calls the callback.\n        Usually, this method is passed to a different thread; the callback\n        is then a function patching results through to the result queue.\"\"\"\n        status = self.machine.scheduler.wait_until_running(\n                self.job, self.worker_config.time_out)\n\n        if status.running:\n            self.online = True\n            if callback:\n                callback(self)\n        else:\n            raise TimeoutError(\"Timeout while waiting for worker to run: \" +\n                               self.worker_config.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a worker to the pool.", "response": "def add_xenon_worker(self, worker_config):\n        \"\"\"Adds a worker to the pool; sets gears in motion.\"\"\"\n        c = XenonInteractiveWorker(self.machine, worker_config)\n        w = RemoteWorker(\n            worker_config.name, threading.Lock(),\n            worker_config.n_threads, [], *c.setup())\n\n        with self.wlock:\n            self.workers[worker_config.name] = w\n\n        def populate(job_source):\n            \"\"\"Populate the worker with jobs, if jobs are available.\"\"\"\n            with w.lock, self.plock:  # Worker lock ~~~~~~~~~~~~~~~~~~\n                while len(w.jobs) < w.max and not self.job_queue.empty():\n                    msg = next(job_source)\n                    w.sink.send(msg)\n\n                    if msg is not EndOfQueue:\n                        key, job = msg\n                        w.jobs.append(key)\n                    else:\n                        break\n            # lock end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n        def activate(_):\n            \"\"\"Activate the worker.\"\"\"\n            job_source = self.job_queue.source()\n            populate(job_source)\n\n            sink = self.result_queue.sink()\n\n            for result in w.source:\n                sink.send(result)\n\n                # do bookkeeping and submit a new job to the worker\n                with w.lock:  # Worker lock ~~~~~~~~~~~~~~~~~~~~~\n                    w.jobs.remove(result.key)\n                populate(job_source)\n                # lock end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n            for key in w.jobs:\n                sink.send(ResultMessage(\n                    key, 'aborted', None, 'connection to remote worker lost.'))\n\n        # Start the `activate` function when the worker goes online.\n        t = threading.Thread(\n            target=self.count(c.wait_until_running), args=(activate,),\n            daemon=True)\n        t.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_first(pred, lst):\n    if lst:\n        return s_find_first(pred, lst[0], [quote(l) for l in lst[1:]])\n    else:\n        return None", "response": "Find the first result of a list of promises lst that satisfies a predicate pred."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef s_find_first(pred, first, lst):\n    if pred(first):\n        return first\n    elif lst:\n        return s_find_first(pred, unquote(lst[0]), lst[1:])\n    else:\n        return None", "response": "Evaluate first ; otherwise recur on the first element of lst."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_online_mode(args):\n    print(\"\\033[47;30m Netherlands\\033[48;2;0;174;239;37m\u258c\"\n          \"\\033[38;2;255;255;255me\\u20d2Science\\u20d2\\033[37m\u2590\"\n          \"\\033[47;30mcenter \\033[m Noodles worker\", file=sys.stderr)\n\n    if args.n == 1:\n        registry = look_up(args.registry)()\n        finish = None\n\n        input_stream = JSONObjectReader(\n            registry, sys.stdin, deref=True)\n        output_stream = JSONObjectWriter(\n            registry, sys.stdout, host=args.name)\n        sys.stdout.flush()\n\n        # run the init function if it is given\n        if args.init:\n            with redirect_stdout(sys.stderr):\n                look_up(args.init)()\n\n        if args.finish:\n            finish = look_up(args.finish)\n\n        for msg in input_stream:\n            if isinstance(msg, JobMessage):\n                key, job = msg\n            elif msg is EndOfWork:\n                print(\"received EndOfWork, bye\", file=sys.stderr)\n                sys.exit(0)\n            elif isinstance(msg, tuple):\n                key, job = msg\n            elif msg is None:\n                continue\n            else:\n                raise RuntimeError(\"Unknown message received: {}\".format(msg))\n\n            if args.jobdirs:\n                # make a directory\n                os.mkdir(\"noodles-{0}\".format(key.hex))\n                # enter it\n                os.chdir(\"noodles-{0}\".format(key.hex))\n\n            if args.verbose:\n                print(\"worker: \",\n                      job.foo.__name__,\n                      job.bound_args.args,\n                      job.bound_args.kwargs,\n                      file=sys.stderr, flush=True)\n\n            with redirect_stdout(sys.stderr):\n                result = run_job(key, job)\n\n            if args.verbose:\n                print(\"result: \", result.value, file=sys.stderr, flush=True)\n\n            if args.jobdirs:\n                # parent directory\n                os.chdir(\"..\")\n\n            output_stream.send(result)\n\n        if finish:\n            finish()\n\n        time.sleep(0.1)\n        sys.stdout.flush()\n        sys.stderr.flush()", "response": "Runs jobs in online mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat a list of arguments for pretty printing.", "response": "def _format_arg_list(args, variadic=False):\n    \"\"\"Format a list of arguments for pretty printing.\n\n    :param a: list of arguments.\n    :type a: list\n\n    :param v: tell if the function accepts variadic arguments\n    :type v: bool\n    \"\"\"\n    def sugar(s):\n        \"\"\"Shorten strings that are too long for decency.\"\"\"\n        s = s.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        if len(s) > 50:\n            return s[:20] + \" ... \" + s[-20:]\n        else:\n            return s\n\n    def arg_to_str(arg):\n        \"\"\"Convert argument to a string.\"\"\"\n        if isinstance(arg, str):\n            return sugar(repr(arg))\n        elif arg is Parameter.empty:\n            return '\\u2014'\n        else:\n            return sugar(str(arg))\n\n    if not args:\n        if variadic:\n            return \"(\\u2026)\"\n        else:\n            return \"()\"\n\n    return \"(\" + \", \".join(map(arg_to_str, args)) + \")\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_workflow_graph(promise):\n    workflow = get_workflow(promise)\n\n    dot = Digraph()\n    for i, n in workflow.nodes.items():\n        dot.node(str(i), label=\"{0} \\n {1}\".format(\n            n.foo.__name__,\n            _format_arg_list(n.bound_args.args)))\n\n    for i in workflow.links:\n        for j in workflow.links[i]:\n            dot.edge(str(i), str(j[0]), label=str(j[1].name))\n\n    return dot", "response": "Get a graph of a promise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display_workflows(prefix, **kwargs):\n    from IPython.display import display, Markdown\n\n    def create_svg(name, workflow):\n        \"\"\"Create an SVG file with rendered graph from a workflow.\"\"\"\n        name = name.replace('_', '-')\n        filename = '{}-{}.svg'.format(prefix, name)\n        dot = get_workflow_graph(workflow)\n        dot.attr('graph', bgcolor='transparent')\n        with open(filename, 'bw') as file:\n            file.write(dot.pipe(format='svg'))\n        return name, filename\n\n    svg_files = {name: svg for name, svg in\n                 (create_svg(name, workflow)\n                  for name, workflow in kwargs.items())}\n\n    markdown_table = \\\n        '| ' + ' | '.join(svg_files.keys()) + ' |\\n' + \\\n        '| ' + ' | '.join('---' for _ in svg_files) + ' |\\n' + \\\n        '| ' + ' | '.join('![workflow {}]({})'.format(name, svg)\n                          for name, svg in svg_files.items()) + ' |'\n    display(Markdown(markdown_table))", "response": "Display workflows in a Jupyter notebook."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef snip_line(line, max_width, split_at):\n    if len(line) < max_width:\n        return line\n    return line[:split_at] + \" \u2026 \" \\\n        + line[-(max_width - split_at - 3):]", "response": "Shorten a line to a maximum length."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays a single - line text in a Jupyter notebook.", "response": "def display_text(text, highlight=None, max_width=80, split_at=None):\n    \"\"\"In a Jupyter notebook, takes a multi-line string, displays these lines\n    in '<pre></pre>' block HTML. A list of lines may be provided that should\n    be highlighted in the result. A highlight gives a gray background and bold\n    font.\n\n    :param text: string\n    :param lines: list of lines to be highlighted.\n    \"\"\"\n    from IPython.display import display, HTML\n\n    if not isinstance(text, str):\n        text = str(text)\n\n    highlight = highlight or []\n    split_at = split_at or max_width - 10\n    style = [\n        'font-size: 9pt; margin: 0pt',\n        'background: #eeeeee; color: black;'\n        'font-weight: bold; font-size: 9pt; margin: 0pt']\n    display(HTML('\\n'.join(\n        '<pre style=\"{}\">{}</pre>'.format(\n            style[i in highlight],\n            html.escape(snip_line(line, max_width, split_at)))\n        for i, line in enumerate(text.splitlines()))))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_and_print_log(workflow, highlight=None):\n    from noodles.run.threading.sqlite3 import run_parallel\n    from noodles import serial\n\n    import io\n    import logging\n\n    log = io.StringIO()\n    log_handler = logging.StreamHandler(log)\n    formatter = logging.Formatter('%(asctime)s - %(message)s')\n    log_handler.setFormatter(formatter)\n\n    logger = logging.getLogger('noodles')\n    logger.setLevel(logging.INFO)\n    logger.handlers = [log_handler]\n\n    result = run_parallel(\n        workflow, n_threads=4, registry=serial.base, db_file='tutorial.db',\n        always_cache=True, echo_log=False)\n    display_text(log.getvalue(), highlight or [], split_at=40)\n    return result", "response": "Run a workflow on multi - threaded worker and print the log."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a display to the single thread.", "response": "def run_single_with_display(wf, display):\n    \"\"\"Adds a display to the single runner. Everything still runs in a single\n    thread. Every time a job is pulled by the worker, a message goes to the\n    display routine; when the job is finished the result is sent to the display\n    routine.\"\"\"\n    S = Scheduler(error_handler=display.error_handler)\n    W = Queue() \\\n        >> branch(log_job_start.to(sink_map(display))) \\\n        >> worker \\\n        >> branch(sink_map(display))\n\n    return S.run(W, get_workflow(wf))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a display to the parallel runner.", "response": "def run_parallel_with_display(wf, n_threads, display):\n    \"\"\"Adds a display to the parallel runner. Because messages come in\n    asynchronously now, we start an extra thread just for the display\n    routine.\"\"\"\n    LogQ = Queue()\n\n    S = Scheduler(error_handler=display.error_handler)\n\n    threading.Thread(\n        target=patch,\n        args=(LogQ.source, sink_map(display)),\n        daemon=True).start()\n\n    W = Queue() \\\n        >> branch(log_job_start >> LogQ.sink) \\\n        >> thread_pool(*repeat(worker, n_threads)) \\\n        >> branch(LogQ.sink)\n\n    result = S.run(W, get_workflow(wf))\n    LogQ.wait()\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decorator(f):\n    @functools.wraps(f)\n    def decoratored_function(*args, **kwargs):\n        if args and len(args) == 1:\n            return f(*args, **kwargs)\n\n        if args:\n            raise TypeError(\n                \"This decorator only accepts extra keyword arguments.\")\n\n        return lambda g: f(g, **kwargs)\n\n    return decoratored_function", "response": "Creates a paramatric decorator from a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a workflow. :param connection: A connection giving a sink to the job-queue and a source yielding results. :type connection: Connection :param master: The workflow. :type master: Workflow", "response": "def run(self, connection: Connection, master: Workflow):\n        \"\"\"Run a workflow.\n\n        :param connection:\n            A connection giving a sink to the job-queue and a source yielding\n            results.\n        :type connection: Connection\n\n        :param master:\n            The workflow.\n        :type master: Workflow\n        \"\"\"\n        # initiate worker slave army and take up reins ...\n        source, sink = connection.setup()\n\n        # schedule work\n        self.add_workflow(master, master, master.root, sink)\n        graceful_exit = False\n        errors = []\n\n        # process results\n        for job_key, status, result, err_msg in source:\n            wf, n = self.jobs[job_key]\n            if status == 'error':\n                graceful_exit = True\n                errors.append(err_msg)\n\n                try:\n                    sink.send(FlushQueue)\n                except StopIteration:\n                    pass\n\n                print(\"Uncaught error running job: {}, {}\".format(n, err_msg),\n                      file=sys.stderr)\n                print(\"Flushing queue and waiting for threads to close.\",\n                      file=sys.stderr, flush=True)\n\n            if status == 'aborted':\n                print(\"Job {} got aborted: {}\".format(n, err_msg),\n                      file=sys.stderr)\n                print(\"Flushing queue and waiting for threads to close.\",\n                      file=sys.stderr, flush=True)\n                graceful_exit = True\n                errors.append(err_msg)\n                try:\n                    sink.send(FlushQueue)\n                except StopIteration:\n                    pass\n\n            if self.verbose:\n                print(\"sched result [{0}]: \".format(self.key_map[job_key]),\n                      result,\n                      file=sys.stderr, flush=True)\n\n            del self.jobs[job_key]\n            if len(self.jobs) == 0 and graceful_exit:\n                for error in errors:\n                    print(\"Exception of type\", type(error), \":\")\n                    print(error)\n                raise errors[0]\n\n            # if this result is the root of a workflow, pop to parent\n            # we do this before scheduling a child workflow, as to\n            # achieve tail-call elimination.\n            while n == wf.root and wf is not master:\n                child = id(wf)\n                _, wf, n = self.dynamic_links[child]\n                del self.dynamic_links[child]\n\n            # if we retrieve a workflow, push a child\n            if is_workflow(result) and not graceful_exit:\n                child_wf = get_workflow(result)\n                self.add_workflow(child_wf, wf, n, sink)\n                continue\n\n            # insert the result in the nodes that need it\n            wf.nodes[n].result = result\n            for (tgt, address) in wf.links[n]:\n                insert_result(wf.nodes[tgt], address, result)\n                if is_node_ready(wf.nodes[tgt]) and not graceful_exit:\n                    self.schedule(Job(workflow=wf, node_id=tgt), sink)\n\n            # see if we're done\n            if wf == master and n == master.root:\n                try:\n                    sink.send(EndOfQueue)\n                except StopIteration:\n                    pass\n\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serialize_arguments(bound_args):\n    for p in bound_args.signature.parameters.values():\n        if p.kind == Parameter.VAR_POSITIONAL:\n            for i, _ in enumerate(bound_args.arguments[p.name]):\n                yield ArgumentAddress(ArgumentKind.variadic, p.name, i)\n            continue\n\n        if p.kind == Parameter.VAR_KEYWORD:\n            for k in bound_args.arguments[p.name].keys():\n                yield ArgumentAddress(ArgumentKind.keyword, p.name, k)\n            continue\n\n        yield ArgumentAddress(ArgumentKind.regular, p.name, None)", "response": "Generates an iterator over all the arguments that are defined in the signature. bind method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ref_argument(bound_args, address):\n    if address.kind == ArgumentKind.regular:\n        return bound_args.arguments[address.name]\n\n    return bound_args.arguments[address.name][address.key]", "response": "Takes a bound_args object and an ArgumentAddress retrieves the data\n    currently stored in bound_args for this particular address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_argument(bound_args, address, value):\n    if address.kind == ArgumentKind.regular:\n        bound_args.arguments[address.name] = value\n        return\n\n    if address.kind == ArgumentKind.variadic:\n        if address.name not in bound_args.arguments:\n            bound_args.arguments[address.name] = []\n\n        n_args = len(bound_args.arguments[address.name])\n        if address.key >= n_args:\n            bound_args.arguments[address.name].extend(\n                repeat(Empty, address.key - n_args + 1))\n\n    if address.kind == ArgumentKind.keyword:\n        if address.name not in bound_args.arguments:\n            bound_args.arguments[address.name] = {}\n\n    bound_args.arguments[address.name][address.key] = value", "response": "Sets the value pointed to by the address to value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat an ArgumentAddress for human reading.", "response": "def format_address(address):\n    \"\"\"Formats an ArgumentAddress for human reading.\"\"\"\n    if address.kind == ArgumentKind.regular:\n        return address.name\n\n    return \"{0}[{1}]\".format(address.name, address.key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_parallel(workflow, n_threads):\n    scheduler = Scheduler()\n    threaded_worker = Queue() >> thread_pool(\n        *repeat(worker, n_threads))\n\n    return scheduler.run(threaded_worker, get_workflow(workflow))", "response": "Run a workflow in parallel threads."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef thread_counter(finalize):\n    n_threads = 0\n    lock = threading.Lock()\n\n    def target_modifier(target):\n        @functools.wraps(target)\n        def modified_target(*args, **kwargs):\n            nonlocal n_threads, lock\n\n            with lock:\n                n_threads += 1\n\n            return_value = target(*args, **kwargs)\n\n            with lock:\n                n_threads -= 1\n                if n_threads == 0:\n                    finalize()\n\n            return return_value\n\n        return modified_target\n\n    return target_modifier", "response": "Returns a thread target function that increments the number of active threads and then returns the value of the finalizer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef thread_pool(*workers, results=None, end_of_queue=EndOfQueue):\n    if results is None:\n        results = Queue(end_of_queue=end_of_queue)\n\n    count = thread_counter(results.close)\n\n    @pull\n    def thread_pool_results(source):\n        for worker in workers:\n            t = threading.Thread(\n                target=count(patch),\n                args=(pull(source) >> worker, results.sink),\n                daemon=True)\n            t.start()\n\n        yield from results.source()\n\n    return thread_pool_results", "response": "Returns a |pull| object that runs each worker in parallel and then returns a |pull| object that yields from the source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dynamic_exclusion_worker(display, n_threads):\n\n    LogQ = Queue()\n\n    threading.Thread(\n        target=patch,\n        args=(LogQ.source, sink_map(display)),\n        daemon=True).start()\n\n    W = Queue() \\\n        >> branch(log_job_start >> LogQ.sink) \\\n        >> thread_pool(*repeat(worker, n_threads)) \\\n        >> branch(LogQ.sink)\n\n    result_source, job_sink = W.setup()\n\n    jobs = {}\n    key_task = {}\n\n    @push\n    def pass_job():\n        \"\"\"The scheduler sends jobs to this coroutine. If the 'exclude' key is\n        found in the hints, it is run in exclusive mode. We keep an internal\n        record of these jobs, and whether they are 'waiting', 'running' or\n        'done'.  \"\"\"\n        while True:\n            key, job = yield\n\n            if job.hints and 'exclude' in job.hints:\n                j = Job(task=job.hints['task'],\n                        exclude=job.hints['exclude'],\n                        state='waiting',\n                        job=job,\n                        key=key)\n                jobs[j.task] = j\n                key_task[key] = j.task\n                try_to_start(j.task)\n\n            else:\n                job_sink.send((key, job))\n\n    def is_not_running(task):\n        \"\"\"Checks if a task is not running.\"\"\"\n        return not (task in jobs and jobs[task].state == 'running')\n\n    def try_to_start(task):\n        \"\"\"Try to start a task. This only succeeds if the task hasn't already\n        run, and no jobs are currently running that is excluded by the task.\"\"\"\n        if jobs[task].state != 'waiting':\n            return\n\n        if all(is_not_running(i) for i in jobs[task].exclude):\n            jobs[task].state = 'running'\n            key, job = jobs[task].key, jobs[task].job\n            job_sink.send((key, job))\n\n    def finish(key):\n        \"\"\"Finish a job. This function is called when we recieve a result.\"\"\"\n        task = key_task[key]\n        jobs[task].state = 'done'\n        for i in jobs[task].exclude:\n            try_to_start(i)\n\n    def pass_result():\n        \"\"\"Recieve a result; finish the task in the register and send the result\n        back to the scheduler.\"\"\"\n        for key, status, result, err in result_source:\n            if key in key_task:\n                finish(key)\n\n            yield (key, status, result, err)\n\n    return Connection(pass_result, pass_job)", "response": "This worker allows mutualy exclusive jobs to start safely."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(wf, *, display, n_threads=1):\n    worker = dynamic_exclusion_worker(display, n_threads)\n    return noodles.Scheduler(error_handler=display.error_handler)\\\n        .run(worker, get_workflow(wf))", "response": "Run the workflow using the dynamic - exclusion worker."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npromises an object of class cls with content members.", "response": "def create_object(cls, members):\n    \"\"\"Promise an object of class `cls` with content `members`.\"\"\"\n    obj = cls.__new__(cls)\n    obj.__dict__ = members\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lift(obj, memo=None):\n    if memo is None:\n        memo = {}\n\n    if isinstance(obj, (PromisedObject, str, int, float, complex)):\n        return obj\n\n    if id(obj) in memo:\n        return memo[id(obj)]\n\n    if hasattr(obj, '__lift__'):\n        rv = obj.__lift__(memo)\n        memo[id(obj)] = rv\n        return rv\n\n    actions = {\n        list:  (lambda x: x, make_list),\n        dict:  (lambda x: list(x.items()), make_dict),\n        tuple: (lambda x: x, make_tuple),\n        set:   (lambda x: x, make_set)\n    }\n\n    if obj.__class__ in actions:\n        items, construct = actions[obj.__class__]\n        tmp = [lift(a, memo) for a in items(obj)]\n        if any(isinstance(a, PromisedObject) for a in tmp):\n            rv = construct(*tmp)\n            memo[id(obj)] = rv\n            return rv\n        else:\n            memo[id(obj)] = obj\n            return obj\n\n    subclass = next(filter(\n        lambda x: issubclass(obj.__class__, x),\n        actions.keys()), None)\n\n    if subclass:\n        members = lift(obj.__dict__, memo)\n        internal = lift(subclass(obj), memo)\n\n        if isinstance(internal, PromisedObject):\n            internal = construct_object(obj.__class__, internal)\n            rv = set_dict(internal, members)\n        elif isinstance(members, PromisedObject):\n            rv = set_dict(obj.__class__(internal), members)\n        else:\n            rv = obj\n\n        memo[id(obj)] = rv\n        return rv\n\n    try:\n        members = lift(obj.__dict__, memo)\n        if isinstance(members, PromisedObject):\n            rv = create_object(obj.__class__, members)\n        else:\n            rv = obj\n    except AttributeError:\n        memo[id(obj)] = obj\n        return obj\n\n    memo[id(obj)] = rv\n    return rv", "response": "Returns a promise out of obj where obj may contain promises\n    internally."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef branch(*sinks_):\n    @pull\n    def junction(source):\n        sinks = [s() for s in sinks_]\n\n        for msg in source():\n            for s in sinks:\n                s.send(msg)\n            yield msg\n\n    return junction", "response": "A single - object decorator that pulls from a single\n    and sends to all the sinks given."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef broadcast(*sinks_):\n    @push\n    def bc():\n        sinks = [s() for s in sinks_]\n        while True:\n            msg = yield\n            for s in sinks:\n                s.send(msg)\n\n    return bc", "response": "A |broadcast| decorator creates a |push| object that receives a\n            message by yield and then sends it on to all the given sinks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push_from(iterable):\n    def p(sink):\n        sink = sink()\n        for x in iterable:\n            sink.send(x)\n\n    return push(p, dont_wrap=True)", "response": "Creates a |push| object from an iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a direct link between a source and a sink.", "response": "def patch(source, sink):\n    \"\"\"Create a direct link between a source and a sink.\n\n    Implementation::\n\n        sink = sink()\n        for value in source():\n            sink.send(value)\n\n    .. |patch| replace:: :py:func:`patch`\n    \"\"\"\n    sink = sink()\n    for v in source():\n        try:\n            sink.send(v)\n        except StopIteration:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncontrolling statement to follow a branch in workflow. Equivalent to the `if` statement in standard Python. The quote function delay the evaluation of the branches until the boolean is evaluated. :param b: promised boolean value. :param branch_true: statement to execute in case of a true predicate. :param branch_false: default operation to execute in case of a false predicate. :returns: :py:class:`PromisedObject`", "response": "def conditional(\n        b: bool,\n        branch_true: Any,\n        branch_false: Any=None) -> Any:\n    \"\"\"\n    Control statement to follow a branch\n    in workflow. Equivalent to the `if` statement\n    in standard Python.\n\n    The quote function delay the evaluation of the branches\n    until the boolean is evaluated.\n\n    :param b:\n       promised boolean value.\n    :param branch_true:\n       statement to execute in case of a true predicate.\n    :param branch_false:\n       default operation to execute in case of a false predicate.\n    :returns: :py:class:`PromisedObject`\n    \"\"\"\n    return schedule_branches(b, quote(branch_true), quote(branch_false))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef array_sha256(a):\n    dtype = str(a.dtype).encode()\n    shape = numpy.array(a.shape)\n    sha = hashlib.sha256()\n    sha.update(dtype)\n    sha.update(shape)\n    sha.update(a.tobytes())\n    return sha.hexdigest()", "response": "Create a SHA256 hash from a Numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a serialisation registry for serialising NumPy data and UFuncs as well as any UFuncs that have no normal way of serialising NumPy data and UFuncs qualified names.", "response": "def arrays_to_file(file_prefix=None):\n    \"\"\"Returns a serialisation registry for serialising NumPy data and\n    as well as any UFuncs that have no normal way of retrieving\n    qualified names.\"\"\"\n    return Registry(\n        types={\n            numpy.ndarray: SerNumpyArrayToFile(file_prefix)\n        },\n        hooks={\n            '<ufunc>': SerUFunc()\n        },\n        hook_fn=_numpy_hook\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arrays_to_string(file_prefix=None):\n    return Registry(\n        types={\n            numpy.ndarray: SerNumpyArray(),\n            numpy.floating: SerNumpyScalar()\n        },\n        hooks={\n            '<ufunc>': SerUFunc()\n        },\n        hook_fn=_numpy_hook\n    )", "response": "Returns a Registry for serialising arrays as a Base64 string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a registry for serialising arrays to a HDF5 reference.", "response": "def arrays_to_hdf5(filename=\"cache.hdf5\"):\n    \"\"\"Returns registry for serialising arrays to a HDF5 reference.\"\"\"\n    return Registry(\n        types={\n            numpy.ndarray: SerNumpyArrayToHDF5(filename, \"cache.lock\")\n        },\n        hooks={\n            '<ufunc>': SerUFunc()\n        },\n        hook_fn=_numpy_hook\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a workflow using a single Xenon remote worker.", "response": "def run_xenon_simple(workflow, machine, worker_config):\n    \"\"\"Run a workflow using a single Xenon remote worker.\n\n    :param workflow: |Workflow| or |PromisedObject| to evaluate.\n    :param machine: |Machine| instance.\n    :param worker_config: Configuration for the pilot job.\"\"\"\n    scheduler = Scheduler()\n\n    return scheduler.run(\n        xenon_interactive_worker(machine, worker_config),\n        get_workflow(workflow)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_xenon(\n        workflow, *, machine, worker_config, n_processes, deref=False,\n        verbose=False):\n    \"\"\"Run the workflow using a number of online Xenon workers.\n\n    :param workflow: |Workflow| or |PromisedObject| to evaluate.\n    :param machine: The |Machine| instance.\n    :param worker_config: Configuration of the pilot job\n    :param n_processes: Number of pilot jobs to start.\n    :param deref: Set this to True to pass the result through one more\n        encoding and decoding step with object dereferencing turned on.\n    :returns: the result of evaluating the workflow\n    \"\"\"\n\n    dynamic_pool = DynamicPool(machine)\n\n    for i in range(n_processes):\n        cfg = copy(worker_config)\n        cfg.name = 'xenon-{0:02}'.format(i)\n        dynamic_pool.add_xenon_worker(cfg)\n\n    job_keeper = JobKeeper()\n    S = Scheduler(job_keeper=job_keeper, verbose=verbose)\n\n    result = S.run(\n        dynamic_pool, get_workflow(workflow)\n    )\n\n    dynamic_pool.close_all()\n\n    if deref:\n        return worker_config.registry().dereference(result, host='scheduler')\n    else:\n        return result", "response": "Runs the workflow using a number of online Xenon workers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a pull stream that receives jobs and passes them on to the result_queue.", "response": "def pass_job(db: JobDB, result_queue: Queue, always_cache=False):\n    \"\"\"Create a pull stream that receives jobs and passes them on to the\n    database. If the job already has a result, that result is pushed onto\n    the `result_queue`.\n    \"\"\"\n    @pull\n    def pass_job_stream(job_source):\n        \"\"\"Pull stream instance created by `pass_job`.\"\"\"\n        result_sink = result_queue.sink()\n\n        for message in job_source():\n            if message is EndOfQueue:\n                return\n\n            key, job = message\n            if always_cache or ('store' in job.hints):\n                status, retrieved_result = db.add_job_to_db(key, job)\n\n                if status == 'retrieved':\n                    result_sink.send(retrieved_result)\n                    continue\n\n                elif status == 'attached':\n                    continue\n\n            yield message\n\n    return pass_job_stream"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a pull stream that receives results from the scheduler and stores them in the database.", "response": "def pass_result(db: JobDB, always_cache=False):\n    \"\"\"Creates a pull stream receiving results, storing them in the database,\n    then sending them on. At this stage, the database may return a list of\n    attached jobs which also need to be sent on to the scheduler.\"\"\"\n    @pull\n    def pass_result_stream(worker_source):\n        \"\"\"Pull stream instance created by `pass_result`.\"\"\"\n        for result in worker_source():\n            if result is EndOfQueue:\n                return\n\n            attached = db.store_result_in_db(\n                result, always_cache=always_cache)\n\n            yield result\n            yield from (ResultMessage(key, 'attached', result.value, None)\n                        for key in attached)\n\n    return pass_result_stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a workflow in parallel threads storing results in a Sqlite3 database.", "response": "def run_parallel(\n        workflow, *, n_threads, registry, db_file, echo_log=True,\n        always_cache=False):\n    \"\"\"Run a workflow in parallel threads, storing results in a Sqlite3\n    database.\n\n    :param workflow: Workflow or PromisedObject to evaluate.\n    :param n_threads: number of threads to use (in addition to the scheduler).\n    :param registry: serialization Registry function.\n    :param db_file: filename of Sqlite3 database, give `':memory:'` to\n        keep the database in memory only.\n    :param echo_log: set log-level high enough\n    :param always_cache: enable caching by schedule hint.\n    :return: Evaluated result.\n    \"\"\"\n    if echo_log:\n        logging.getLogger('noodles').setLevel(logging.DEBUG)\n        logging.debug(\"--- start log ---\")\n\n    with JobDB(db_file, registry) as db:\n        job_queue = Queue()\n        result_queue = Queue()\n\n        job_logger = make_logger(\"worker\", push_map, db)\n        result_logger = make_logger(\"worker\", pull_map, db)\n\n        worker_pool = job_queue.source \\\n            >> pass_job(db, result_queue, always_cache) \\\n            >> thread_pool(*repeat(worker, n_threads), results=result_queue)\n        job_front_end = job_logger >> job_queue.sink\n        result_front_end = worker_pool \\\n            >> pass_result(db, always_cache) \\\n            >> result_logger\n\n        scheduler = Scheduler(job_keeper=db)\n        parallel_sqlite_worker = Connection(result_front_end, job_front_end)\n\n        result = scheduler.run(parallel_sqlite_worker, get_workflow(workflow))\n\n    return registry().dereference(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef look_up(name):\n    path = name.split('.')\n    module = import_module('.'.join(path[:-1]))\n    return getattr(module, path[-1])", "response": "Obtain an object from a qualified name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if an object can be serialised as a qualified name.", "response": "def importable(obj):\n    \"\"\"Check if an object can be serialised as a qualified name. This is done\n    by checking that a ``look_up(object_name(obj))`` gives back the same\n    object.\n\n    .. |importable| replace:: :py:func:`importable`\"\"\"\n    try:\n        return look_up(object_name(obj)) is obj\n    except (AttributeError, TypeError, ImportError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_unwrapped(f):\n    try:\n        g = look_up(object_name(f))\n        return g != f and unwrap(g) == f\n\n    except (AttributeError, TypeError, ImportError):\n        return False", "response": "Returns True if f was imported and then unwrapped."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deep_map(f, root):\n    result = f(root)\n\n    if isinstance(result, dict):\n        return {k: deep_map(f, v) for k, v in result.items()}\n\n    if isinstance(result, list) or isinstance(result, tuple):\n        return [deep_map(f, v) for v in result]\n\n    return result", "response": "Sibling to |deep_map|. As : py : func| maps over a structure of nested dicts and lists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inverse_deep_map(f, root):\n    if isinstance(root, dict):\n        r = {k: inverse_deep_map(f, v) for k, v in root.items()}\n    elif isinstance(root, list):\n        r = [inverse_deep_map(f, v) for v in root]\n    else:\n        r = root\n\n    return f(r)", "response": "Sibling to |deep_map|. Recursively maps objects in a nested structure of\n    list and dict objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(self, obj, host=None):\n        if obj is None:\n            return None\n\n        if type(obj) in [dict, list, str, int, float, bool]:\n            return obj\n\n        if isinstance(obj, RefObject):\n            return obj.rec\n\n        hook = self._hook(obj) if self._hook else None\n        typename = hook if hook else '<object>'\n        classname = object_name(type(obj))\n\n        def make_rec(data, ref=None, files=None):\n            rec = {'_noodles': noodles.__version__,\n                   'type': typename,\n                   'class': classname,\n                   'data': data}\n\n            if ref is not None:\n                rec['ref'] = ref\n\n            if files:\n                rec['host'] = host\n                rec['files'] = files\n\n            return rec\n\n        if hook:\n            return self._sers[hook].encode(obj, make_rec)\n\n        enc = self[type(obj)]\n        result = enc.encode(obj, make_rec)\n        return result", "response": "Encode an object using the serialisers available\n        in this registry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode(self, rec, deref=False):\n        if not isinstance(rec, dict):\n            return rec\n\n        if '_noodles' not in rec:\n            return rec\n\n        # if not deref:\n        if rec.get('ref', False) and not deref:\n            return RefObject(rec)\n\n        typename = rec['type']\n        classname = rec['class']\n        try:\n            cls = look_up(classname) if classname else None\n        except (AttributeError, ImportError):\n            cls = None\n\n        if typename == '<object>':\n            assert cls is not None, \\\n                \"could not lookup class '{}', decoding '{}'\".format(\n                    classname, rec)\n            return self[cls].decode(cls, rec['data'])\n        else:\n            return self._sers[typename].decode(cls, rec['data'])", "response": "Decode a record to return an object that could be considered\n            equivalent to the original."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self, obj, host=None, indent=None):\n        if indent:\n            return json.dumps(deep_map(lambda o: self.encode(o, host), obj),\n                              indent=indent)\n        else:\n            return json.dumps(deep_map(lambda o: self.encode(o, host), obj))", "response": "Recursively encode obj and convert it to a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes the string from JSON to return the original object.", "response": "def from_json(self, data, deref=False):\n        \"\"\"Decode the string from JSON to return the original object (if\n        `deref` is true. Uses the `json.loads` function with `self.decode`\n        as object_hook.\n\n        :param data:\n            JSON encoded string.\n        :type data: str\n\n        :param deref:\n            Whether to decode records that gave `ref=True` at encoding.\n        :type deref: bool\"\"\"\n        return self.deep_decode(json.loads(data), deref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dereference(self, data, host=None):\n        return self.deep_decode(self.deep_encode(data, host), deref=True)", "response": "Dereferences RefObjects stuck in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns workflow in a single thread storing results in a Sqlite3 database.", "response": "def run_single(workflow, *, registry, db_file, always_cache=True):\n    \"\"\"\"Run workflow in a single thread, storing results in a Sqlite3\n    database.\n\n    :param workflow: Workflow or PromisedObject to be evaluated.\n    :param registry: serialization Registry function.\n    :param db_file: filename of Sqlite3 database, give `':memory:'` to\n        keep the database in memory only.\n    :param always_cache: Currently ignored. always_cache is true.\n    :return: Evaluated result.\n    \"\"\"\n    with JobDB(db_file, registry) as db:\n        job_logger = make_logger(\"worker\", push_map, db)\n        result_logger = make_logger(\"worker\", pull_map, db)\n\n        @pull\n        def pass_job(source):\n            \"\"\"Receives jobs from source, passes back results.\"\"\"\n            for msg in source():\n                key, job = msg\n                status, retrieved_result = db.add_job_to_db(key, job)\n\n                if status == 'retrieved':\n                    yield retrieved_result\n                    continue\n\n                elif status == 'attached':\n                    continue\n\n                result = run_job(key, job)\n                attached = db.store_result_in_db(result, always_cache=True)\n\n                yield result\n                yield from (ResultMessage(key, 'attached', result.value, None)\n                            for key in attached)\n\n        scheduler = Scheduler(job_keeper=db)\n        queue = Queue()\n        job_front_end = job_logger >> queue.sink\n        result_front_end = queue.source >> pass_job >> result_logger\n        single_worker = Connection(result_front_end, job_front_end)\n\n        return scheduler.run(single_worker, get_workflow(workflow))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scheduler(self):\n        if self._scheduler is None:\n            self._scheduler = xenon.Scheduler.create(**self.scheduler_args)\n\n        return self._scheduler", "response": "Returns the scheduler object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file_system(self):\n        if self._file_system is None:\n            self._file_system = self.scheduler.get_file_system()\n\n        return self._file_system", "response": "Gets the filesystem corresponding to the open scheduler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef registry():\n    return Registry(\n        types={\n            dict: SerDict(),\n            tuple: SerSequence(tuple),\n            set: SerSequence(set),\n            bytes: SerBytes(),\n            slice: SerSlice(),\n            complex: SerByMembers(complex, ['real', 'imag']),\n            Reasonable: SerReasonableObject(Reasonable),\n            ArgumentKind: SerEnum(ArgumentKind),\n            FunctionNode: SerNode(),\n            ArgumentAddress: SerNamedTuple(ArgumentAddress),\n            Workflow: SerWorkflow(),\n            PromisedObject: SerPromisedObject(),\n            Quote: SerReasonableObject(Quote),\n            Path: SerPath(),\n            Fail: SerReasonableObject(Fail)\n        },\n        hooks={\n            '<method>': SerMethod(),\n            '<boundmethod>': SerBoundMethod(),\n            '<importable>': SerImportable(),\n            '<automagic>': SerAuto(),\n            '<unwrapped>': SerUnwrapped()\n        },\n        hook_fn=_noodles_hook,\n        default=SerUnknown(),\n    )", "response": "Returns the Noodles base serialisation registry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the hints on the root - node of a promotion object.", "response": "def update_hints(obj, data):\n    \"\"\"Update the hints on the root-node of a workflow. Usually, schedule\n    hints are fixed per function. Sometimes a user may want to set hints\n    manually on a specific promised object. :func:`update_hints` uses the\n    `update` method on the hints dictionary with `data` as its argument.\n\n    :param obj: a :py:class:`PromisedObject`.\n    :param data: a :py:class:`dict` containing additional hints.\n\n    The hints are modified, in place, on the node. All workflows that contain\n    the node are affected.\"\"\"\n    root = obj._workflow.root\n    obj._workflow.nodes[root].hints.update(data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef worker(job):\n    if job is EndOfQueue:\n        return\n\n    if not isinstance(job, JobMessage):\n        print(\"Warning: Job should be communicated using `JobMessage`.\",\n              file=sys.stderr)\n\n    key, node = job\n    return run_job(key, node)", "response": "Primary |worker| coroutine. This is a primary |pull| object that pulls jobs from\n    and yields evaluated results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_job(key, node):\n    try:\n        result = node.apply()\n        return ResultMessage(key, 'done', result, None)\n\n    except Exception as exc:\n        return ResultMessage(key, 'error', None, exc)", "response": "Run a job. This applies the function node, and returns a |ResultMessage|\n    when complete. If an exception is raised in the job, the |ResultMessage|\n    will have ``'error'`` status.\n\n    .. |run_job| replace:: :py:func:`run_job`"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hybrid_coroutine_worker(selector, workers):\n    jobs = Queue()\n\n    worker_source = {}\n    worker_sink = {}\n\n    for k, w in workers.items():\n        worker_source[k], worker_sink[k] = w.setup()\n\n    def get_result():\n        source = jobs.source()\n\n        for msg in source:\n            key, job = msg\n            worker = selector(job)\n            if worker is None:\n                yield run_job(key, job)\n            else:\n                # send the worker a job and wait for it to return\n                worker_sink[worker].send(msg)\n                result = next(worker_source[worker])\n                yield result\n\n    return Connection(get_result, jobs.sink)", "response": "Runs a set of workers and returns a connection to the main thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hybrid_threaded_worker(selector, workers):\n    result_queue = Queue()\n\n    job_sink = {k: w.sink() for k, w in workers.items()}\n\n    @push\n    def dispatch_job():\n        default_sink = result_queue.sink()\n\n        while True:\n            msg = yield\n\n            if msg is EndOfQueue:\n                for k in workers.keys():\n                    try:\n                        job_sink[k].send(EndOfQueue)\n                    except StopIteration:\n                        pass\n                return\n\n            if msg is FlushQueue:\n                for k in workers.keys():\n                    try:\n                        job_sink[k].send(FlushQueue)\n                    except StopIteration:\n                        pass\n                return\n\n            worker = selector(msg.node)\n            if worker:\n                job_sink[worker].send(msg)\n            else:\n                default_sink.send(run_job(*msg))\n\n    for key, worker in workers.items():\n        t = threading.Thread(\n            target=patch,\n            args=(worker.source, result_queue.sink))\n        t.daemon = True\n        t.start()\n\n    return Connection(result_queue.source, dispatch_job)", "response": "Runs a set of workers each in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun through several worker functions in as many threads as possible.", "response": "def run_hybrid(wf, selector, workers):\n    \"\"\"\n    Returns the result of evaluating the workflow; runs through several\n    supplied workers in as many threads.\n\n    :param wf:\n        Workflow to compute\n    :type wf: :py:class:`Workflow` or :py:class:`PromisedObject`\n\n    :param selector:\n        A function selecting the worker that should be run, given a hint.\n    :param workers:\n        A dictionary of workers\n\n    :returns:\n        result of running the workflow\n    \"\"\"\n    worker = hybrid_threaded_worker(selector, workers)\n    return Scheduler().run(worker, get_workflow(wf))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a MD5 hash of the function call.", "response": "def prov_key(job_msg, extra=None):\n    \"\"\"Retrieves a MD5 sum from a function call. This takes into account the\n    name of the function, the arguments and possibly a version number of the\n    function, if that is given in the hints.\n    This version can also be auto-generated by generating an MD5 hash from the\n    function source. However, the source-code may not always be reachable, or\n    the result may depend on an external process which has its own\n    versioning.\"\"\"\n    m = hashlib.md5()\n    update_object_hash(m, job_msg['data']['function'])\n    update_object_hash(m, job_msg['data']['arguments'])\n\n    if 'version' in job_msg['data']['hints']:\n        update_object_hash(m, job_msg['data']['hints']['version'])\n\n    if extra is not None:\n        update_object_hash(m, extra)\n\n    return m.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstreams objects from a JSON file.", "response": "def JSONObjectReader(registry, fi, deref=False):\n    \"\"\"Stream objects from a JSON file.\n\n    :param registry: serialisation registry.\n    :param fi: input file\n    :param deref: flag, if True, objects will be dereferenced on decoding,\n        otherwise we are lazy about decoding a JSON string.\n    \"\"\"\n    for line in fi:\n        yield registry.from_json(line, deref=deref)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsink ; writes object as JSON to a file.", "response": "def JSONObjectWriter(registry, fo, host=None):\n    \"\"\"Sink; writes object as JSON to a file.\n\n    :param registry: serialisation registry.\n    :param fo: output file.\n    :param host: name of the host that encodes the JSON. This is relevant if\n        the encoded data refers to external files for mass storage.\n\n    In normal use, it may occur that the pipe to which we write is broken,\n    for instance when the remote process shuts down. In that case, this\n    coroutine exits.\n    \"\"\"\n    while True:\n        obj = yield\n        try:\n            print(registry.to_json(obj, host=host), file=fo, flush=True)\n        except BrokenPipeError:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matrix(mat):\n    import ROOT\n    if isinstance(mat, (ROOT.TMatrixD, ROOT.TMatrixDSym)):\n        return _librootnumpy.matrix_d(ROOT.AsCObject(mat))\n    elif isinstance(mat, (ROOT.TMatrixF, ROOT.TMatrixFSym)):\n        return _librootnumpy.matrix_f(ROOT.AsCObject(mat))\n    raise TypeError(\n        \"unable to convert object of type {0} \"\n        \"into a numpy matrix\".format(type(mat)))", "response": "Convert a ROOT TMatrix into a NumPy matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_graph(graph, array):\n    import ROOT\n    array = np.asarray(array, dtype=np.double)\n    if isinstance(graph, ROOT.TGraph):\n        if array.ndim != 2:\n            raise ValueError(\"array must be 2-dimensional\")\n        if array.shape[1] != 2:\n            raise ValueError(\n                \"length of the second dimension must equal \"\n                \"the dimension of the graph\")\n        return _librootnumpy.fill_g1(\n            ROOT.AsCObject(graph), array)\n    elif isinstance(graph, ROOT.TGraph2D):\n        if array.ndim != 2:\n            raise ValueError(\"array must be 2-dimensional\")\n        if array.shape[1] != 3:\n            raise ValueError(\n                \"length of the second dimension must equal \"\n                \"the dimension of the graph\")\n        return _librootnumpy.fill_g2(\n            ROOT.AsCObject(graph), array)\n    else:\n        raise TypeError(\n            \"hist must be an instance of ROOT.TGraph or ROOT.TGraph2D\")", "response": "Fill a ROOT graph with a NumPy array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_classification_events(obj, events, labels, signal_label=None,\n                              weights=None, test=False):\n    \"\"\"Add classification events to a TMVA::Factory or TMVA::DataLoader from NumPy arrays.\n\n    Parameters\n    ----------\n    obj : TMVA::Factory or TMVA::DataLoader\n        A TMVA::Factory or TMVA::DataLoader (TMVA's interface as of ROOT\n        6.07/04) instance with variables already booked in exactly the same\n        order as the columns in ``events``.\n    events : numpy array of shape [n_events, n_variables]\n        A two-dimensional NumPy array containing the rows of events and columns\n        of variables. The order of the columns must match the order in which\n        you called ``AddVariable()`` for each variable.\n    labels : numpy array of shape [n_events]\n        The class labels (signal or background) corresponding to each event in\n        ``events``.\n    signal_label : float or int, optional (default=None)\n        The value in ``labels`` for signal events, if ``labels`` contains only\n        two classes. If None, the highest value in ``labels`` is used.\n    weights : numpy array of shape [n_events], optional\n        Event weights.\n    test : bool, optional (default=False)\n        If True, then the events will be added as test events, otherwise they\n        are added as training events by default.\n\n    Notes\n    -----\n    * A TMVA::Factory or TMVA::DataLoader requires you to add both training and\n      test events even if you don't intend to call ``TestAllMethods()``.\n\n    * When using MethodCuts, the first event added must be a signal event,\n      otherwise TMVA will fail with ``<FATAL> Interval : maximum lower than\n      minimum``. To place a signal event first::\n\n        # Get index of first signal event\n        first_signal = np.nonzero(labels == signal_label)[0][0]\n        # Swap this with first event\n        events[0], events[first_signal] = events[first_signal].copy(), events[0].copy()\n        labels[0], labels[first_signal] = labels[first_signal], labels[0]\n        weights[0], weights[first_signal] = weights[first_signal], weights[0]\n\n    \"\"\"\n    if NEW_TMVA_API:  # pragma: no cover\n        if not isinstance(obj, TMVA.DataLoader):\n            raise TypeError(\n                \"obj must be a TMVA.DataLoader \"\n                \"instance for ROOT >= 6.07/04\")\n    else:  # pragma: no cover\n        if not isinstance(obj, TMVA.Factory):\n            raise TypeError(\n                \"obj must be a TMVA.Factory instance\")\n    events = np.ascontiguousarray(events, dtype=np.float64)\n    if events.ndim == 1:\n        # convert to 2D\n        events = events[:, np.newaxis]\n    elif events.ndim != 2:\n        raise ValueError(\n            \"events must be a two-dimensional array \"\n            \"with one event per row\")\n    class_labels, class_idx = np.unique(labels, return_inverse=True)\n    if class_idx.shape[0] != events.shape[0]:\n        raise ValueError(\"numbers of events and labels do not match\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=np.float64)\n        if weights.shape[0] != events.shape[0]:\n            raise ValueError(\"numbers of events and weights do not match\")\n        if weights.ndim != 1:\n            raise ValueError(\"weights must be one-dimensional\")\n    n_classes = class_labels.shape[0]\n    if n_classes > 2:\n        # multiclass classification\n        _libtmvanumpy.add_events_multiclass(\n            ROOT.AsCObject(obj), events, class_idx,\n            weights, test)\n    elif n_classes == 2:\n        # binary classification\n        if signal_label is None:\n            signal_label = class_labels[1]\n        signal_label = np.where(class_labels == signal_label)[0][0]\n        _libtmvanumpy.add_events_twoclass(\n            ROOT.AsCObject(obj), events, class_idx,\n            signal_label, weights, test)\n    else:\n        raise ValueError(\"labels must contain at least two classes\")", "response": "Adds classification events to a TMVA object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding regression events to a TMVA object.", "response": "def add_regression_events(obj, events, targets, weights=None, test=False):\n    \"\"\"Add regression events to a TMVA::Factory or TMVA::DataLoader from NumPy arrays.\n\n    Parameters\n    ----------\n    obj : TMVA::Factory or TMVA::DataLoader\n        A TMVA::Factory or TMVA::DataLoader (TMVA's interface as of ROOT\n        6.07/04) instance with variables already\n        booked in exactly the same order as the columns in ``events``.\n    events : numpy array of shape [n_events, n_variables]\n        A two-dimensional NumPy array containing the rows of events and columns\n        of variables. The order of the columns must match the order in which\n        you called ``AddVariable()`` for each variable.\n    targets : numpy array of shape [n_events] or [n_events, n_targets]\n        The target value(s) for each event in ``events``. For multiple target\n        values, ``targets`` must be a two-dimensional array with a column for\n        each target in the same order in which you called ``AddTarget()``.\n    weights : numpy array of shape [n_events], optional\n        Event weights.\n    test : bool, optional (default=False)\n        If True, then the events will be added as test events, otherwise they\n        are added as training events by default.\n\n    Notes\n    -----\n    A TMVA::Factory or TMVA::DataLoader requires you to add both training and\n    test events even if you don't intend to call ``TestAllMethods()``.\n\n    \"\"\"\n    if NEW_TMVA_API:  # pragma: no cover\n        if not isinstance(obj, TMVA.DataLoader):\n            raise TypeError(\n                \"obj must be a TMVA.DataLoader \"\n                \"instance for ROOT >= 6.07/04\")\n    else:  # pragma: no cover\n        if not isinstance(obj, TMVA.Factory):\n            raise TypeError(\n                \"obj must be a TMVA.Factory instance\")\n    events = np.ascontiguousarray(events, dtype=np.float64)\n    if events.ndim == 1:\n        # convert to 2D\n        events = events[:, np.newaxis]\n    elif events.ndim != 2:\n        raise ValueError(\n            \"events must be a two-dimensional array \"\n            \"with one event per row\")\n    targets = np.asarray(targets, dtype=np.float64)\n    if targets.shape[0] != events.shape[0]:\n        raise ValueError(\"the lengths of events and targets do not match\")\n    if targets.ndim == 1:\n        # convert to 2D\n        targets = targets[:, np.newaxis]\n    elif targets.ndim > 2:\n        raise ValueError(\"targets can not have more than two dimensions\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=np.float64)\n        if weights.shape[0] != events.shape[0]:\n            raise ValueError(\"numbers of events and weights do not match\")\n        if weights.ndim != 1:\n            raise ValueError(\"weights must be one-dimensional\")\n    _libtmvanumpy.add_events_regression(\n        ROOT.AsCObject(obj), events, targets, weights, test)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(obj, array):\n    import ROOT\n    array = np.asarray(array, dtype=np.double)\n    if isinstance(obj, ROOT.TH1):\n        if isinstance(obj, ROOT.TH3):\n            if array.ndim != 2:\n                raise ValueError(\"array must be 2-dimensional\")\n            if array.shape[1] != 3:\n                raise ValueError(\n                    \"length of the second dimension must equal \"\n                    \"the dimension of the histogram\")\n            return _librootnumpy.evaluate_h3(\n                ROOT.AsCObject(obj), array)\n        elif isinstance(obj, ROOT.TH2):\n            if array.ndim != 2:\n                raise ValueError(\"array must be 2-dimensional\")\n            if array.shape[1] != 2:\n                raise ValueError(\n                    \"length of the second dimension must equal \"\n                    \"the dimension of the histogram\")\n            return _librootnumpy.evaluate_h2(\n                ROOT.AsCObject(obj), array)\n        if array.ndim != 1:\n            raise ValueError(\"array must be 1-dimensional\")\n        return _librootnumpy.evaluate_h1(\n            ROOT.AsCObject(obj), array)\n    elif isinstance(obj, ROOT.TF1):\n        if isinstance(obj, ROOT.TF3):\n            if array.ndim != 2:\n                raise ValueError(\"array must be 2-dimensional\")\n            if array.shape[1] != 3:\n                raise ValueError(\n                    \"length of the second dimension must equal \"\n                    \"the dimension of the function\")\n            return _librootnumpy.evaluate_f3(\n                ROOT.AsCObject(obj), array)\n        elif isinstance(obj, ROOT.TF2):\n            if array.ndim != 2:\n                raise ValueError(\"array must be 2-dimensional\")\n            if array.shape[1] != 2:\n                raise ValueError(\n                    \"length of the second dimension must equal \"\n                    \"the dimension of the function\")\n            return _librootnumpy.evaluate_f2(\n                ROOT.AsCObject(obj), array)\n        if array.ndim != 1:\n            raise ValueError(\"array must be 1-dimensional\")\n        return _librootnumpy.evaluate_f1(\n            ROOT.AsCObject(obj), array)\n    elif isinstance(obj, (string_types, ROOT.TFormula)):\n        if isinstance(obj, string_types):\n            # attempt to create a formula\n            obj = ROOT.TFormula(uuid.uuid4().hex, obj)\n        ndim = obj.GetNdim()\n        if ndim == 0:\n            raise ValueError(\"invalid formula expression\")\n        if ndim == 1:\n            if array.ndim != 1:\n                raise ValueError(\"array must be 1-dimensional\")\n            return _librootnumpy.evaluate_formula_1d(\n                ROOT.AsCObject(obj), array)\n        if array.ndim != 2:\n            raise ValueError(\"array must be 2-dimensional\")\n        if array.shape[1] != ndim:\n            raise ValueError(\n                \"length of the second dimension must equal \"\n                \"the dimension of the function\")\n        if ndim == 2:\n            return _librootnumpy.evaluate_formula_2d(\n                ROOT.AsCObject(obj), array)\n        elif ndim == 3:\n            return _librootnumpy.evaluate_formula_3d(\n                ROOT.AsCObject(obj), array)\n        # 4d\n        return _librootnumpy.evaluate_formula_4d(\n            ROOT.AsCObject(obj), array)\n    elif isinstance(obj, ROOT.TGraph):\n        if array.ndim != 1:\n            raise ValueError(\"array must be 1-dimensional\")\n        return _librootnumpy.evaluate_graph(\n            ROOT.AsCObject(obj), array)\n    elif isinstance(obj, ROOT.TSpline):\n        if array.ndim != 1:\n            raise ValueError(\"array must be 1-dimensional\")\n        return _librootnumpy.evaluate_spline(\n            ROOT.AsCObject(obj), array)\n    raise TypeError(\n        \"obj is not a ROOT histogram, function, formula, \"\n        \"graph, spline or string\")", "response": "Evaluate a ROOT object at a given array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_sample(obj, n_samples, seed=None):\n    import ROOT\n    if n_samples <= 0:\n        raise ValueError(\"n_samples must be greater than 0\")\n    if seed is not None:\n        if seed < 0:\n            raise ValueError(\"seed must be positive or 0\")\n        ROOT.gRandom.SetSeed(seed)\n    # functions\n    if isinstance(obj, ROOT.TF1):\n        if isinstance(obj, ROOT.TF3):\n            return _librootnumpy.sample_f3(\n                ROOT.AsCObject(obj), n_samples)\n        elif isinstance(obj, ROOT.TF2):\n            return _librootnumpy.sample_f2(\n                ROOT.AsCObject(obj), n_samples)\n        return _librootnumpy.sample_f1(ROOT.AsCObject(obj), n_samples)\n    # histograms\n    elif isinstance(obj, ROOT.TH1):\n        if isinstance(obj, ROOT.TH3):\n            return _librootnumpy.sample_h3(\n                ROOT.AsCObject(obj), n_samples)\n        elif isinstance(obj, ROOT.TH2):\n            return _librootnumpy.sample_h2(\n                ROOT.AsCObject(obj), n_samples)\n        return _librootnumpy.sample_h1(ROOT.AsCObject(obj), n_samples)\n    raise TypeError(\n        \"obj must be a ROOT function or histogram\")", "response": "Create a random array by sampling a ROOT function or histogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _glob(filenames):\n    if isinstance(filenames, string_types):\n        filenames = [filenames]\n    matches = []\n    for name in filenames:\n        matched_names = glob(name)\n        if not matched_names:\n            # use the original string\n            matches.append(name)\n        else:\n            matches.extend(matched_names)\n    return matches", "response": "Glob a filename or list of filenames but always return the original\n    string if the glob didn t match anything so URLs for remote file access\n    are not clobbered."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_structures(filename, treename=None):\n    warnings.warn(\"list_structures is deprecated and will be \"\n                  \"removed in 5.0.0.\", DeprecationWarning)\n    return _librootnumpy.list_structures(filename, treename)", "response": "Returns a dictionary mapping branch names to leaf structures."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a ROOT file into a numpy structured array.", "response": "def root2array(filenames,\n               treename=None,\n               branches=None,\n               selection=None,\n               object_selection=None,\n               start=None,\n               stop=None,\n               step=None,\n               include_weight=False,\n               weight_name='weight',\n               cache_size=-1,\n               warn_missing_tree=False):\n    \"\"\"Convert trees in ROOT files into a numpy structured array.\n\n    Refer to the documentation of :func:`tree2array`.\n\n    Parameters\n    ----------\n    filenames : str or list\n        ROOT file name pattern or list of patterns. Wildcarding is supported by\n        Python globbing.\n    treename : str, optional (default=None)\n        Name of the tree to convert (optional if each file contains exactly one\n        tree).\n    branches : list of strings and tuples or a string or tuple, optional (default=None)\n        List of branches and expressions to include as columns of the array or\n        a single branch or expression in which case a nonstructured array is\n        returned. If None then include all branches that can be converted.\n        Branches or expressions that result in variable-length subarrays can be\n        truncated at a fixed length by using the tuple ``(branch_or_expression,\n        fill_value, length)`` or converted into a single value with\n        ``(branch_or_expression, fill_value)`` where ``length==1`` is implied.\n        ``fill_value`` is used when the original array is shorter than\n        ``length``. This truncation is after any object selection performed\n        with the ``object_selection`` argument.\n    selection : str, optional (default=None)\n        Only include entries fulfilling this condition. If the condition\n        evaluates to multiple values per tree entry (e.g. conditions on array\n        branches) then an entry will be included if the condition evaluates to\n        true for at least one array element.\n    object_selection : dict, optional (default=None)\n        A dictionary mapping selection strings to branch names or lists of\n        branch names. Only array elements passing the selection strings will be\n        included in the output array per entry in the tree. The branches\n        specified must be variable-length array-type branches and the length of\n        the selection and branches it acts on must match for each tree entry.\n        For example ``object_selection={'a > 0': ['a', 'b']}`` will include all\n        elements of 'a' and corresponding elements of 'b' where 'a > 0' for\n        each tree entry. 'a' and 'b' must have the same length in every tree\n        entry.\n    start, stop, step: int, optional (default=None)\n        The meaning of the ``start``, ``stop`` and ``step`` parameters is the\n        same as for Python slices. If a range is supplied (by setting some of\n        the ``start``, ``stop`` or ``step`` parameters), only the entries in\n        that range and fulfilling the ``selection`` condition (if defined) are\n        used.\n    include_weight : bool, optional (default=False)\n        Include a column containing the tree weight ``TTree::GetWeight()``.\n        Note that this will be the same value for all entries unless the tree\n        is actually a TChain containing multiple trees with different weights.\n    weight_name : str, optional (default='weight')\n        The field name for the weight column if ``include_weight=True``.\n    cache_size : int, optional (default=-1)\n        Set the size (in bytes) of the TTreeCache used while reading a TTree. A\n        value of -1 uses ROOT's default cache size. A value of 0 disables the\n        cache.\n    warn_missing_tree : bool, optional (default=False)\n        If True, then warn when a tree is missing from an input file instead of\n        raising an IOError.\n\n    Notes\n    -----\n    * Refer to the :ref:`type conversion table <conversion_table>`.\n\n    See Also\n    --------\n    tree2array\n    array2tree\n    array2root\n\n    \"\"\"\n    filenames = _glob(filenames)\n\n    if not filenames:\n        raise ValueError(\"specify at least one filename\")\n\n    if treename is None:\n        trees = list_trees(filenames[0])\n        if len(trees) > 1:\n            raise ValueError(\n                \"treename must be specified if the file \"\n                \"contains more than one tree\")\n        elif not trees:\n            raise IOError(\n                \"no trees present in {0}\".format(filenames[0]))\n        treename = trees[0]\n\n    if isinstance(branches, string_types):\n        # single branch selected\n        flatten = branches\n        branches = [branches]\n    elif isinstance(branches, tuple):\n        if len(branches) not in (2, 3):\n            raise ValueError(\n                \"invalid branch tuple: {0}. \"\n                \"A branch tuple must contain two elements \"\n                \"(branch_name, fill_value) or three elements \"\n                \"(branch_name, fill_value, length) \"\n                \"to yield a single value or truncate, respectively\".format(branches))\n        flatten = branches[0]\n        branches = [branches]\n    else:\n        flatten = False\n\n    arr = _librootnumpy.root2array_fromfile(\n        filenames, treename, branches,\n        selection, object_selection,\n        start, stop, step,\n        include_weight,\n        weight_name,\n        cache_size,\n        warn_missing_tree)\n\n    if flatten:\n        # select single column\n        return arr[flatten]\n    return arr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a list of files into a record array.", "response": "def root2rec(filenames,\n             treename=None,\n             branches=None,\n             selection=None,\n             object_selection=None,\n             start=None,\n             stop=None,\n             step=None,\n             include_weight=False,\n             weight_name='weight',\n             cache_size=-1,\n             warn_missing_tree=False):  # pragma: no cover\n    \"\"\"View the result of :func:`root2array` as a record array.\n\n    .. warning:: ``root2rec`` is deprecated and will be removed in\n       release 5.0.0. Instead use ``root2array(...).view(np.recarray)``.\n\n    Notes\n    -----\n    * This is equivalent to::\n\n        root2array(filenames, treename, branches).view(np.recarray)\n\n    * Refer to the :ref:`type conversion table <conversion_table>`.\n\n    See Also\n    --------\n    root2array\n\n    \"\"\"\n    warnings.warn(\"root2rec is deprecated and will be removed in 5.0.0. \"\n                  \"Instead use root2array(...).view(np.recarray)\",\n                  DeprecationWarning)\n    return root2array(filenames, treename,\n                      branches, selection, object_selection,\n                      start, stop, step,\n                      include_weight,\n                      weight_name,\n                      cache_size,\n                      warn_missing_tree).view(np.recarray)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tree2array(tree,\n               branches=None,\n               selection=None,\n               object_selection=None,\n               start=None,\n               stop=None,\n               step=None,\n               include_weight=False,\n               weight_name='weight',\n               cache_size=-1):\n    \"\"\"Convert a tree into a numpy structured array.\n\n    Convert branches of strings and basic types such as bool, int, float,\n    double, etc. as well as variable-length and fixed-length multidimensional\n    arrays and 1D or 2D vectors of basic types and strings. ``tree2array`` can\n    also create columns in the output array that are expressions involving the\n    TTree branches (i.e. ``'vect.Pt() / 1000'``) similar to ``TTree::Draw()``.\n    See the notes below for important details.\n\n    Parameters\n    ----------\n    tree : ROOT TTree instance\n        The ROOT TTree to convert into an array.\n    branches : list of strings and tuples or a string or tuple, optional (default=None)\n        List of branches and expressions to include as columns of the array or\n        a single branch or expression in which case a nonstructured array is\n        returned. If None then include all branches that can be converted.\n        Branches or expressions that result in variable-length subarrays can be\n        truncated at a fixed length by using the tuple ``(branch_or_expression,\n        fill_value, length)`` or converted into a single value with\n        ``(branch_or_expression, fill_value)`` where ``length==1`` is implied.\n        ``fill_value`` is used when the original array is shorter than\n        ``length``. This truncation is after any object selection performed\n        with the ``object_selection`` argument.\n    selection : str, optional (default=None)\n        Only include entries fulfilling this condition. If the condition\n        evaluates to multiple values per tree entry (e.g. conditions on array\n        branches) then an entry will be included if the condition evaluates to\n        true for at least one array element.\n    object_selection : dict, optional (default=None)\n        A dictionary mapping selection strings to branch names or lists of\n        branch names. Only array elements passing the selection strings will be\n        included in the output array per entry in the tree. The branches\n        specified must be variable-length array-type branches and the length of\n        the selection and branches it acts on must match for each tree entry.\n        For example ``object_selection={'a > 0': ['a', 'b']}`` will include all\n        elements of 'a' and corresponding elements of 'b' where 'a > 0' for\n        each tree entry. 'a' and 'b' must have the same length in every tree\n        entry.\n    start, stop, step: int, optional (default=None)\n        The meaning of the ``start``, ``stop`` and ``step`` parameters is the\n        same as for Python slices. If a range is supplied (by setting some of\n        the ``start``, ``stop`` or ``step`` parameters), only the entries in\n        that range and fulfilling the ``selection`` condition (if defined) are\n        used.\n    include_weight : bool, optional (default=False)\n        Include a column containing the tree weight ``TTree::GetWeight()``.\n        Note that this will be the same value for all entries unless the tree\n        is actually a TChain containing multiple trees with different weights.\n    weight_name : str, optional (default='weight')\n        The field name for the weight column if ``include_weight=True``.\n    cache_size : int, optional (default=-1)\n        Set the size (in bytes) of the TTreeCache used while reading a TTree. A\n        value of -1 uses ROOT's default cache size. A value of 0 disables the\n        cache.\n\n    Notes\n    -----\n    Types are converted according to the following table:\n\n    .. _conversion_table:\n\n    ========================  ===============================\n    ROOT                      NumPy\n    ========================  ===============================\n    ``Bool_t``                ``np.bool``\n    ``Char_t``                ``np.int8``\n    ``UChar_t``               ``np.uint8``\n    ``Short_t``               ``np.int16``\n    ``UShort_t``              ``np.uint16``\n    ``Int_t``                 ``np.int32``\n    ``UInt_t``                ``np.uint32``\n    ``Float_t``               ``np.float32``\n    ``Double_t``              ``np.float64``\n    ``Long64_t``              ``np.int64``\n    ``ULong64_t``             ``np.uint64``\n    ``<type>[2][3]...``       ``(<nptype>, (2, 3, ...))``\n    ``<type>[nx][2]...``      ``np.object``\n    ``string``                ``np.object``\n    ``vector<t>``             ``np.object``\n    ``vector<vector<t> >``    ``np.object``\n    ========================  ===============================\n\n    * Variable-length arrays (such as ``x[nx][2]``) and vectors (such as\n      ``vector<int>``) are converted to NumPy arrays of the corresponding\n      types.\n\n    * Fixed-length arrays are converted to fixed-length NumPy array fields.\n\n    **Branches with different lengths:**\n\n    Note that when converting trees that have branches of different lengths\n    into numpy arrays, the shorter branches will be extended to match the\n    length of the longest branch by repeating their last values. If all\n    requested branches are shorter than the longest branch in the tree, this\n    will result in a \"read failure\" since beyond the end of the longest\n    requested branch no additional bytes will be read from the file and\n    root_numpy is unable to distinguish this from other ROOT errors that result\n    in no bytes being read. In this case, explicitly set the ``stop`` argument\n    to the length of the longest requested branch.\n\n\n    See Also\n    --------\n    root2array\n    array2root\n    array2tree\n\n    \"\"\"\n    import ROOT\n    if not isinstance(tree, ROOT.TTree):\n        raise TypeError(\"tree must be a ROOT.TTree\")\n    cobj = ROOT.AsCObject(tree)\n\n    if isinstance(branches, string_types):\n        # single branch selected\n        flatten = branches\n        branches = [branches]\n    elif isinstance(branches, tuple):\n        if len(branches) not in (2, 3):\n            raise ValueError(\n                \"invalid branch tuple: {0}. \"\n                \"A branch tuple must contain two elements \"\n                \"(branch_name, fill_value) or three elements \"\n                \"(branch_name, fill_value, length) \"\n                \"to yield a single value or truncate, respectively\".format(branches))\n        flatten = branches[0]\n        branches = [branches]\n    else:\n        flatten = False\n\n    arr = _librootnumpy.root2array_fromtree(\n        cobj, branches, selection, object_selection,\n        start, stop, step,\n        include_weight,\n        weight_name,\n        cache_size)\n\n    if flatten:\n        # select single column\n        return arr[flatten]\n    return arr", "response": "Convert a ROOT TTree into a numpy structured array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tree2rec(tree,\n             branches=None,\n             selection=None,\n             object_selection=None,\n             start=None,\n             stop=None,\n             step=None,\n             include_weight=False,\n             weight_name='weight',\n             cache_size=-1):  # pragma: no cover\n    \"\"\"View the result of :func:`tree2array` as a record array.\n\n    .. warning:: ``tree2rec`` is deprecated and will be removed in\n       release 5.0.0. Instead use ``tree2array(...).view(np.recarray)``.\n\n    Notes\n    -----\n    * This is equivalent to::\n\n        tree2array(treename, branches).view(np.recarray)\n\n    * Refer to the :ref:`type conversion table <conversion_table>`.\n\n    See Also\n    --------\n    tree2array\n\n    \"\"\"\n    warnings.warn(\"tree2rec is deprecated and will be removed in 5.0.0. \"\n                  \"Instead use tree2array(...).view(np.recarray)\",\n                  DeprecationWarning)\n    return tree2array(tree,\n                      branches=branches,\n                      selection=selection,\n                      object_selection=object_selection,\n                      start=start,\n                      stop=stop,\n                      step=step,\n                      include_weight=include_weight,\n                      weight_name=weight_name,\n                      cache_size=cache_size).view(np.recarray)", "response": "Convert a tree into a record array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a numpy array into a ROOT TTree.", "response": "def array2tree(arr, name='tree', tree=None):\n    \"\"\"Convert a numpy structured array into a ROOT TTree.\n\n    Fields of basic types, strings, and fixed-size subarrays of basic types are\n    supported. ``np.object`` and ``np.float16`` are currently not supported.\n\n    Parameters\n    ----------\n    arr : array\n        A numpy structured array\n    name : str (optional, default='tree')\n        Name of the created ROOT TTree if ``tree`` is None.\n    tree : ROOT TTree (optional, default=None)\n        An existing ROOT TTree to be extended by the numpy array. Any branch\n        with the same name as a field in the numpy array will be extended as\n        long as the types are compatible, otherwise a TypeError is raised. New\n        branches will be created and filled for all new fields.\n\n    Returns\n    -------\n    root_tree : a ROOT TTree\n\n    Notes\n    -----\n    When using the ``tree`` argument to extend and/or add new branches to an\n    existing tree, note that it is possible to create branches of different\n    lengths. This will result in a warning from ROOT when root_numpy calls the\n    tree's ``SetEntries()`` method. Beyond that, the tree should still be\n    usable. While it might not be generally recommended to create branches with\n    differing lengths, this behaviour could be required in certain situations.\n    root_numpy makes no attempt to prevent such behaviour as this would be more\n    strict than ROOT itself. Also see the note about converting trees that have\n    branches of different lengths into numpy arrays in the documentation of\n    :func:`tree2array`.\n\n    See Also\n    --------\n    array2root\n    root2array\n    tree2array\n\n    Examples\n    --------\n\n    Convert a numpy array into a tree:\n\n    >>> from root_numpy import array2tree\n    >>> import numpy as np\n    >>>\n    >>> a = np.array([(1, 2.5, 3.4),\n    ...               (4, 5, 6.8)],\n    ...              dtype=[('a', np.int32),\n    ...                     ('b', np.float32),\n    ...                     ('c', np.float64)])\n    >>> tree = array2tree(a)\n    >>> tree.Scan()\n    ************************************************\n    *    Row   *         a *         b *         c *\n    ************************************************\n    *        0 *         1 *       2.5 *       3.4 *\n    *        1 *         4 *         5 *       6.8 *\n    ************************************************\n\n    Add new branches to an existing tree (continuing from the example above):\n\n    >>> b = np.array([(4, 10),\n    ...               (3, 5)],\n    ...              dtype=[('d', np.int32),\n    ...                     ('e', np.int32)])\n    >>> array2tree(b, tree=tree)\n    <ROOT.TTree object (\"tree\") at 0x1449970>\n    >>> tree.Scan()\n    ************************************************************************\n    *    Row   *         a *         b *         c *         d *         e *\n    ************************************************************************\n    *        0 *         1 *       2.5 *       3.4 *         4 *        10 *\n    *        1 *         4 *         5 *       6.8 *         3 *         5 *\n    ************************************************************************\n\n    \"\"\"\n    import ROOT\n    if tree is not None:\n        if not isinstance(tree, ROOT.TTree):\n            raise TypeError(\"tree must be a ROOT.TTree\")\n        incobj = ROOT.AsCObject(tree)\n    else:\n        incobj = None\n    cobj = _librootnumpy.array2tree_toCObj(arr, name=name, tree=incobj)\n    return ROOT.BindObject(cobj, 'TTree')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array2root(arr, filename, treename='tree', mode='update'):\n    _librootnumpy.array2root(arr, filename, treename, mode)", "response": "Convert a numpy array into a ROOT TTree and save it in a ROOT TFile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_hist(hist, array, weights=None, return_indices=False):\n    import ROOT\n    array = np.asarray(array, dtype=np.double)\n    if weights is not None:\n        weights = np.asarray(weights, dtype=np.double)\n        if weights.shape[0] != array.shape[0]:\n            raise ValueError(\"array and weights must have the same length\")\n        if weights.ndim != 1:\n            raise ValueError(\"weight must be 1-dimensional\")\n    if isinstance(hist, ROOT.TH3):\n        if array.ndim != 2:\n            raise ValueError(\"array must be 2-dimensional\")\n        if array.shape[1] != 3:\n            raise ValueError(\n                \"length of the second dimension must equal \"\n                \"the dimension of the histogram\")\n        return _librootnumpy.fill_h3(\n            ROOT.AsCObject(hist), array, weights, return_indices)\n    elif isinstance(hist, ROOT.TH2):\n        if array.ndim != 2:\n            raise ValueError(\"array must be 2-dimensional\")\n        if array.shape[1] != 2:\n            raise ValueError(\n                \"length of the second dimension must equal \"\n                \"the dimension of the histogram\")\n        return _librootnumpy.fill_h2(\n            ROOT.AsCObject(hist), array, weights, return_indices)\n    elif isinstance(hist, ROOT.TH1):\n        if array.ndim != 1:\n            raise ValueError(\"array must be 1-dimensional\")\n        return _librootnumpy.fill_h1(\n            ROOT.AsCObject(hist), array, weights, return_indices)\n    raise TypeError(\n        \"hist must be an instance of ROOT.TH1, ROOT.TH2, or ROOT.TH3\")", "response": "Fill a ROOT array with a NumPy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills a ROOT profile with a NumPy array.", "response": "def fill_profile(profile, array, weights=None, return_indices=False):\n    \"\"\"Fill a ROOT profile with a NumPy array.\n\n    Parameters\n    ----------\n    profile : ROOT TProfile, TProfile2D, or TProfile3D\n        The ROOT profile to fill.\n    array : numpy array of shape [n_samples, n_dimensions]\n        The values to fill the histogram with. There must be one more column\n        than the dimensionality of the profile.\n    weights : numpy array\n        A flat numpy array of weights for each sample in ``array``.\n    return_indices : bool, optional (default=False)\n        If True then return an array of the bin indices filled for each element\n        in ``array``.\n\n    Returns\n    -------\n    indices : numpy array or None\n        If ``return_indices`` is True, then return an array of the bin indices\n        filled for each element in ``array`` otherwise return None.\n\n    \"\"\"\n    import ROOT\n    array = np.asarray(array, dtype=np.double)\n    if array.ndim != 2:\n        raise ValueError(\"array must be 2-dimensional\")\n    if array.shape[1] != profile.GetDimension() + 1:\n        raise ValueError(\n            \"there must be one more column than the \"\n            \"dimensionality of the profile\")\n    if weights is not None:\n        weights = np.asarray(weights, dtype=np.double)\n        if weights.shape[0] != array.shape[0]:\n            raise ValueError(\"array and weights must have the same length\")\n        if weights.ndim != 1:\n            raise ValueError(\"weight must be 1-dimensional\")\n    if isinstance(profile, ROOT.TProfile3D):\n        return _librootnumpy.fill_p3(\n            ROOT.AsCObject(profile), array, weights, return_indices)\n    elif isinstance(profile, ROOT.TProfile2D):\n        return _librootnumpy.fill_p2(\n            ROOT.AsCObject(profile), array, weights, return_indices)\n    elif isinstance(profile, ROOT.TProfile):\n        return _librootnumpy.fill_p1(\n            ROOT.AsCObject(profile), array, weights, return_indices)\n    raise TypeError(\n        \"profile must be an instance of \"\n        \"ROOT.TProfile, ROOT.TProfile2D, or ROOT.TProfile3D\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a ROOT histogram into a NumPy array.", "response": "def hist2array(hist, include_overflow=False, copy=True, return_edges=False):\n    \"\"\"Convert a ROOT histogram into a NumPy array\n\n    Parameters\n    ----------\n    hist : ROOT TH1, TH2, TH3, THn, or THnSparse\n        The ROOT histogram to convert into an array\n    include_overflow : bool, optional (default=False)\n        If True, the over- and underflow bins will be included in the\n        output numpy array. These bins are excluded by default.\n    copy : bool, optional (default=True)\n        If True (the default) then copy the underlying array, otherwise the\n        NumPy array will view (and not own) the same memory as the ROOT\n        histogram's array.\n    return_edges : bool, optional (default=False)\n        If True, also return the bin edges along each axis.\n\n    Returns\n    -------\n    array : numpy array\n        A NumPy array containing the histogram bin values\n    edges : list of numpy arrays\n        A list of numpy arrays where each array contains the bin edges along\n        the corresponding axis of ``hist``. Overflow and underflow bins are not\n        included.\n\n    Raises\n    ------\n    TypeError\n        If hist is not a ROOT histogram.\n\n    See Also\n    --------\n    array2hist\n\n    \"\"\"\n    import ROOT\n    # Determine dimensionality and shape\n    simple_hist = True\n    if isinstance(hist, ROOT.TH3):\n        shape = (hist.GetNbinsZ() + 2,\n                 hist.GetNbinsY() + 2,\n                 hist.GetNbinsX() + 2)\n    elif isinstance(hist, ROOT.TH2):\n        shape = (hist.GetNbinsY() + 2, hist.GetNbinsX() + 2)\n    elif isinstance(hist, ROOT.TH1):\n        shape = (hist.GetNbinsX() + 2,)\n    elif isinstance(hist, ROOT.THnBase):\n        shape = tuple([hist.GetAxis(i).GetNbins() + 2\n                       for i in range(hist.GetNdimensions())])\n        simple_hist = False\n    else:\n        raise TypeError(\n            \"hist must be an instance of ROOT.TH1, \"\n            \"ROOT.TH2, ROOT.TH3, or ROOT.THnBase\")\n\n    # Determine the corresponding numpy dtype\n    if simple_hist:\n        for hist_type in 'DFISC':\n            if isinstance(hist, getattr(ROOT, 'TArray{0}'.format(hist_type))):\n                break\n        else:\n            raise AssertionError(\n                \"hist is somehow an instance of TH[1|2|3] \"\n                \"but not TArray[D|F|I|S|C]\")\n    else:  # THn, THnSparse\n        if isinstance(hist, ROOT.THnSparse):\n            cls_string = 'THnSparse{0}'\n        else:\n            cls_string = 'THn{0}'\n        for hist_type in 'CSILFD':\n            if isinstance(hist, getattr(ROOT, cls_string.format(hist_type))):\n                break\n        else:\n            raise AssertionError(\n                \"unsupported THn or THnSparse bin type\")\n\n    if simple_hist:\n        # Constuct a NumPy array viewing the underlying histogram array\n        if hist_type == 'C':\n            array_func = getattr(_librootnumpy,\n                                 'array_h{0}c'.format(len(shape)))\n            array = array_func(ROOT.AsCObject(hist))\n            array.shape = shape\n        else:\n            dtype = np.dtype(DTYPE_ROOT2NUMPY[hist_type])\n            array = np.ndarray(shape=shape, dtype=dtype,\n                               buffer=hist.GetArray())\n    else:  # THn THnSparse\n        dtype = np.dtype(DTYPE_ROOT2NUMPY[hist_type])\n        if isinstance(hist, ROOT.THnSparse):\n            array = _librootnumpy.thnsparse2array(ROOT.AsCObject(hist),\n                                                  shape, dtype)\n        else:\n            array = _librootnumpy.thn2array(ROOT.AsCObject(hist),\n                                            shape, dtype)\n\n    if return_edges:\n        if simple_hist:\n            ndims = hist.GetDimension()\n            axis_getters = ['GetXaxis', 'GetYaxis', 'GetZaxis'][:ndims]\n        else:\n            ndims = hist.GetNdimensions()\n            axis_getters = ['GetAxis'] * ndims\n\n        edges = []\n        for idim, axis_getter in zip(range(ndims), axis_getters):\n            # GetXaxis expects 0 parameters while we need the axis in GetAxis\n            ax = getattr(hist, axis_getter)(*(() if simple_hist else (idim,)))\n            # `edges` is Nbins + 1 in order to have the last bin's upper edge as well\n            edges.append(np.empty(ax.GetNbins() + 1, dtype=np.double))\n            # load the lower edges into `edges`\n            ax.GetLowEdge(edges[-1])\n            # Get the upper edge of the last bin\n            edges[-1][-1] = ax.GetBinUpEdge(ax.GetNbins())\n\n    if not include_overflow:\n        # Remove overflow and underflow bins\n        array = array[tuple([slice(1, -1) for idim in range(array.ndim)])]\n\n    if simple_hist:\n        # Preserve x, y, z -> axis 0, 1, 2 order\n        array = np.transpose(array)\n        if copy:\n            array = np.copy(array)\n\n    if return_edges:\n        return array, edges\n    return array"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef array2hist(array, hist, errors=None):\n    import ROOT\n    if isinstance(hist, ROOT.TH3):\n        shape = (hist.GetNbinsX() + 2,\n                 hist.GetNbinsY() + 2,\n                 hist.GetNbinsZ() + 2)\n    elif isinstance(hist, ROOT.TH2):\n        shape = (hist.GetNbinsX() + 2, hist.GetNbinsY() + 2)\n    elif isinstance(hist, ROOT.TH1):\n        shape = (hist.GetNbinsX() + 2,)\n    else:\n        raise TypeError(\n            \"hist must be an instance of ROOT.TH1, ROOT.TH2, or ROOT.TH3\")\n\n    # Determine the corresponding numpy dtype\n    for hist_type in 'DFISC':\n        if isinstance(hist, getattr(ROOT, 'TArray{0}'.format(hist_type))):\n            break\n    else:\n        raise AssertionError(\n            \"hist is somehow an instance of TH[1|2|3] \"\n            \"but not TArray[D|F|I|S|C]\")\n\n    # Constuct a NumPy array viewing the underlying histogram array\n    dtype = np.dtype(DTYPE_ROOT2NUMPY[hist_type])\n    # No copy is made if the dtype is the same as input\n    _array = np.ascontiguousarray(array, dtype=dtype)\n    if errors is not None:\n        if errors.shape != array.shape:\n            raise ValueError(\"Contents and errors are not compatible\")\n        # errors are specified as doubles in SetError function\n        _errors = np.ascontiguousarray(errors, dtype=np.float64)\n    else:\n        _errors = None\n\n    if _array.ndim != len(shape):\n        raise ValueError(\n            \"array and histogram do not have \"\n            \"the same number of dimensions\")\n    if _array.shape != shape:\n        # Check for overflow along each axis\n        slices = []\n        for axis, bins in enumerate(shape):\n            if _array.shape[axis] == bins - 2:\n                slices.append(slice(1, -1))\n            elif _array.shape[axis] == bins:\n                slices.append(slice(None))\n            else:\n                raise ValueError(\n                    \"array and histogram are not compatible along \"\n                    \"the {0}-axis\".format(\"xyz\"[axis]))\n        array_overflow = np.zeros(shape, dtype=dtype)\n        array_overflow[tuple(slices)] = _array\n        _array = array_overflow\n\n        if _errors is not None:\n            errors_overflow = np.zeros(shape, dtype=np.float64)\n            errors_overflow[tuple(slices)] = _errors\n            _errors = errors_overflow\n\n    ARRAY_NUMPY2ROOT[len(shape)][hist_type](\n        ROOT.AsCObject(hist), np.ravel(np.transpose(_array)))\n    # Set the number of entries to the number of array elements\n    hist.SetEntries(_array.size)\n    if _errors is not None:\n        hist.SetError(np.ravel(_errors.T))\n    return hist", "response": "Convert a NumPy array into a ROOT histogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_docstring(filename):\n    lines = file(filename).readlines()\n    start_row = 0\n    if lines[0].startswith('#!'):\n        lines.pop(0)\n        start_row = 1\n\n    docstring = ''\n    first_par = ''\n    tokens = tokenize.generate_tokens(iter(lines).next)\n    for tok_type, tok_content, _, (erow, _), _ in tokens:\n        tok_type = token.tok_name[tok_type]\n        if tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT'):\n            continue\n        elif tok_type == 'STRING':\n            docstring = eval(tok_content)\n            # If the docstring is formatted with several paragraphs, extract\n            # the first one:\n            paragraphs = '\\n'.join(line.rstrip()\n                              for line in docstring.split('\\n')).split('\\n\\n')\n            if len(paragraphs) > 0:\n                first_par = paragraphs[0]\n        break\n    end_row = erow + 1 + start_row\n    if lines and lines[end_row - 2] == 'print(__doc__)\\n':\n        end_row += 1\n    return docstring, first_par, end_row", "response": "Extract a module - level docstring from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_example_rst(app):\n    root_dir = os.path.join(app.builder.srcdir, 'auto_examples')\n    example_dir = os.path.abspath(app.builder.srcdir + '/../' + 'examples')\n    try:\n        plot_gallery = eval(app.builder.config.plot_gallery)\n    except TypeError:\n        plot_gallery = bool(app.builder.config.plot_gallery)\n    if not os.path.exists(example_dir):\n        os.makedirs(example_dir)\n    if not os.path.exists(root_dir):\n        os.makedirs(root_dir)\n\n    # we create an index.rst with all examples\n    fhindex = file(os.path.join(root_dir, 'index.rst'), 'w')\n    #Note: The sidebar button has been removed from the examples page for now\n    #      due to how it messes up the layout. Will be fixed at a later point\n    fhindex.write(\"\"\"\\\n\n.. raw:: html\n\n\n    <style type=\"text/css\">\n\n    div#sidebarbutton {\n        display: none;\n    }\n\n    .figure {\n        float: left;\n        margin: 10px;\n        width: auto;\n        height: 200px;\n        width: 180px;\n    }\n\n    .figure img {\n        display: inline;\n        }\n\n    .figure .caption {\n        width: 170px;\n        text-align: center !important;\n    }\n    </style>\n\n.. _examples-index:\n\nExamples\n========\n\n\"\"\")\n    # Here we don't use an os.walk, but we recurse only twice: flat is\n    # better than nested.\n    generate_dir_rst('.', fhindex, example_dir, root_dir, plot_gallery)\n    for dir in sorted(os.listdir(example_dir)):\n        if os.path.isdir(os.path.join(example_dir, dir)):\n            generate_dir_rst(dir, fhindex, example_dir, root_dir, plot_gallery)\n    fhindex.flush()", "response": "Generate the list of examples as well as the contents of the examples page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the rst file for a directory.", "response": "def generate_dir_rst(dir, fhindex, example_dir, root_dir, plot_gallery):\n    \"\"\" Generate the rst file for an example directory.\n    \"\"\"\n    if not dir == '.':\n        target_dir = os.path.join(root_dir, dir)\n        src_dir = os.path.join(example_dir, dir)\n    else:\n        target_dir = root_dir\n        src_dir = example_dir\n    if not os.path.exists(os.path.join(src_dir, 'README.txt')):\n        print('Example directory %s does not have a README.txt file' % src_dir)\n        print('Skipping this directory')\n        print(80 * '_')\n        return\n    fhindex.write(\"\"\"\n\n\n%s\n\n\n\"\"\" % file(os.path.join(src_dir, 'README.txt')).read())\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    def sort_key(a):\n        # put last elements without a plot\n        if not a.startswith('plot') and a.endswith('.py'):\n            return 'zz' + a\n        return a\n\n    fhindex.write(\"\"\"\n\n.. toctree::\n\n\"\"\")\n    for fname in sorted(os.listdir(src_dir), key=sort_key):\n        if fname.endswith('py'):\n            generate_file_rst(fname, target_dir, src_dir, plot_gallery)\n            #thumb = os.path.join(dir, 'images', 'thumb', fname[:-3] + '.png')\n            link_name = os.path.join(dir, fname).replace(os.path.sep, '_')\n            #fhindex.write('.. figure:: %s\\n' % thumb)\n            #if link_name.startswith('._'):\n            #    link_name = link_name[2:]\n            #if dir != '.':\n            #    fhindex.write('   :target: ./%s/%s.html\\n\\n' % (dir,\n            #                                                   fname[:-3]))\n            #else:\n            #    fhindex.write('   :target: ./%s.html\\n\\n' % link_name[:-3])\n            fhindex.write(\"\"\"\n   %s/%s\n\"\"\" % (dir, fname[:-3]))\n    fhindex.write(\"\"\"\n\n.. raw:: html\n\n    <div style=\"clear: both\"></div>\n    \"\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_file_rst(fname, target_dir, src_dir, plot_gallery):\n    base_image_name = os.path.splitext(fname)[0]\n    image_fname = '%s_%%s.png' % base_image_name\n    root_image_fname = 'root_%s_%%s.png' % base_image_name\n    root_fig_num = 1\n\n    this_template = rst_template\n    last_dir = os.path.split(src_dir)[-1]\n    # to avoid leading . in file names, and wrong names in links\n    if last_dir == '.' or last_dir == 'examples':\n        last_dir = ''\n    else:\n        last_dir += '_'\n    short_fname = last_dir + fname\n    src_file = os.path.join(src_dir, fname)\n    example_file = os.path.join(target_dir, fname)\n    shutil.copyfile(src_file, example_file)\n\n    # The following is a list containing all the figure names\n    figure_list = []\n\n    image_dir = os.path.join(target_dir, 'images')\n    thumb_dir = os.path.join(image_dir, 'thumb')\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n    if not os.path.exists(thumb_dir):\n        os.makedirs(thumb_dir)\n    image_path = os.path.join(image_dir, image_fname)\n    root_image_path = os.path.join(image_dir, root_image_fname)\n\n    stdout_path = os.path.join(image_dir,\n                               'stdout_%s.txt' % base_image_name)\n    time_path = os.path.join(image_dir,\n                               'time_%s.txt' % base_image_name)\n    thumb_file = os.path.join(thumb_dir, fname[:-3] + '.png')\n    time_elapsed = 0\n    if plot_gallery and fname.startswith('plot'):\n        # generate the plot as png image if file name\n        # starts with plot and if it is more recent than an\n        # existing image.\n        first_image_file = image_path % 1\n        first_root_image_file = root_image_path % 1\n        if os.path.exists(stdout_path):\n            stdout = open(stdout_path).read()\n        else:\n            stdout = ''\n        if os.path.exists(time_path):\n            time_elapsed = float(open(time_path).read())\n\n        if (not os.path.exists(first_image_file) or\n            not os.path.exists(first_root_image_file) or\n                os.stat(first_image_file).st_mtime <=\n                                    os.stat(src_file).st_mtime):\n            # We need to execute the code\n            print('plotting %s' % fname)\n            t0 = time()\n            import matplotlib.pyplot as plt\n            plt.close('all')\n            cwd = os.getcwd()\n            try:\n                # First CD in the original example dir, so that any file\n                # created by the example get created in this directory\n                orig_stdout = sys.stdout\n                os.chdir(os.path.dirname(src_file))\n                my_buffer = StringIO()\n                my_stdout = Tee(sys.stdout, my_buffer)\n                sys.stdout = my_stdout\n                my_globals = {'pl': plt}\n                execfile(os.path.basename(src_file), my_globals)\n                time_elapsed = time() - t0\n                sys.stdout = orig_stdout\n                my_stdout = my_buffer.getvalue()\n                if '__doc__' in my_globals:\n                    # The __doc__ is often printed in the example, we\n                    # don't with to echo it\n                    my_stdout = my_stdout.replace(\n                                            my_globals['__doc__'],\n                                            '')\n                my_stdout = my_stdout.strip()\n                if my_stdout:\n                    stdout = '**Script output**::\\n\\n  %s\\n\\n' % (\n                        '\\n  '.join(my_stdout.split('\\n')))\n                open(stdout_path, 'w').write(stdout)\n                open(time_path, 'w').write('%f' % time_elapsed)\n                os.chdir(cwd)\n\n                # In order to save every figure we have two solutions :\n                # * iterate from 1 to infinity and call plt.fignum_exists(n)\n                #   (this requires the figures to be numbered\n                #    incrementally: 1, 2, 3 and not 1, 2, 5)\n                # * iterate over [fig_mngr.num for fig_mngr in\n                #   matplotlib._pylab_helpers.Gcf.get_all_fig_managers()]\n                for fig_num in (fig_mngr.num for fig_mngr in\n                        matplotlib._pylab_helpers.Gcf.get_all_fig_managers()):\n                    # Set the fig_num figure as the current figure as we can't\n                    # save a figure that's not the current figure.\n                    plt.figure(fig_num)\n                    plt.savefig(image_path % fig_num)\n                    figure_list.append(image_fname % fig_num)\n                for canvas in ROOT.gROOT.GetListOfCanvases():\n                    maybe_root_filename = os.path.join(os.path.dirname(src_file), canvas.name)\n                    if os.path.isfile(maybe_root_filename):\n                        os.rename(maybe_root_filename, os.path.join(image_dir, canvas.name))\n                        figure_list.append(canvas.name)\n                        canvas.Close()\n                    else:\n                        canvas.SaveAs(root_image_path % root_fig_num)\n                        canvas.Close()\n                        figure_list.append(root_image_fname % root_fig_num)\n                        root_fig_num += 1\n            except:\n                print(80 * '_')\n                print('%s is not compiling:' % fname)\n                traceback.print_exc()\n                print(80 * '_')\n            finally:\n                os.chdir(cwd)\n                sys.stdout = orig_stdout\n\n            print(\" - time elapsed : %.2g sec\" % time_elapsed)\n        else:\n            figure_list = [f[len(image_dir):]\n                            for f in glob.glob(image_path % '[1-9]')]\n                            #for f in glob.glob(image_path % '*')]\n\n        # generate thumb file\n        this_template = plot_rst_template\n        from matplotlib import image\n        if os.path.exists(first_image_file):\n            image.thumbnail(first_image_file, thumb_file, 0.2)\n        elif os.path.exists(first_root_image_file):\n            image.thumbnail(first_root_image_file, thumb_file, 0.2)\n\n    if not os.path.exists(thumb_file):\n        # create something not to replace the thumbnail\n        shutil.copy('images/blank_image.png', thumb_file)\n\n    docstring, short_desc, end_row = extract_docstring(example_file)\n\n    # Depending on whether we have one or more figures, we're using a\n    # horizontal list or a single rst call to 'image'.\n    if len(figure_list) == 1:\n        figure_name = figure_list[0]\n        image_list = SINGLE_IMAGE % figure_name.lstrip('/')\n    else:\n        image_list = HLIST_HEADER\n        for figure_name in figure_list:\n            image_list += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')\n\n    f = open(os.path.join(target_dir, fname[:-2] + 'rst'), 'w')\n    f.write(this_template % locals())\n    f.flush()", "response": "Generate the rst file for a given example."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef array(arr, copy=True):\n    import ROOT\n    if isinstance(arr, ROOT.TArrayD):\n        arr = _librootnumpy.array_d(ROOT.AsCObject(arr))\n    elif isinstance(arr, ROOT.TArrayF):\n        arr = _librootnumpy.array_f(ROOT.AsCObject(arr))\n    elif isinstance(arr, ROOT.TArrayL):\n        arr = _librootnumpy.array_l(ROOT.AsCObject(arr))\n    elif isinstance(arr, ROOT.TArrayI):\n        arr = _librootnumpy.array_i(ROOT.AsCObject(arr))\n    elif isinstance(arr, ROOT.TArrayS):\n        arr = _librootnumpy.array_s(ROOT.AsCObject(arr))\n    elif isinstance(arr, ROOT.TArrayC):\n        arr = _librootnumpy.array_c(ROOT.AsCObject(arr))\n    else:\n        raise TypeError(\n            \"unable to convert object of type {0} \"\n            \"into a numpy array\".format(type(arr)))\n    if copy:\n        return np.copy(arr)\n    return arr", "response": "Convert a ROOT TArray into a NumPy array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rec2array(rec, fields=None):\n    simplify = False\n    if fields is None:\n        fields = rec.dtype.names\n    elif isinstance(fields, string_types):\n        fields = [fields]\n        simplify = True\n    # Creates a copy and casts all data to the same type\n    arr = np.dstack([rec[field] for field in fields])\n    # Check for array-type fields. If none, then remove outer dimension.\n    # Only need to check first field since np.dstack will anyway raise an\n    # exception if the shapes don't match\n    # np.dstack will also fail if fields is an empty list\n    if not rec.dtype[fields[0]].shape:\n        arr = arr[0]\n    if simplify:\n        # remove last dimension (will be of size 1)\n        arr = arr.reshape(arr.shape[:-1])\n    return arr", "response": "Convert a record to an ndarray with homogenous data types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstack common fields in multiple record arrays.", "response": "def stack(recs, fields=None):\n    \"\"\"Stack common fields in multiple record arrays (concatenate them).\n\n    Parameters\n    ----------\n    recs : list\n        List of NumPy record arrays\n    fields : list of strings, optional (default=None)\n        The list of fields to include in the stacked array. If None, then\n        include the fields in common to all the record arrays.\n\n    Returns\n    -------\n    rec : NumPy record array\n        The stacked array.\n\n    \"\"\"\n    if fields is None:\n        fields = list(set.intersection(\n            *[set(rec.dtype.names) for rec in recs]))\n        # preserve order of fields wrt first record array\n        if set(fields) == set(recs[0].dtype.names):\n            fields = list(recs[0].dtype.names)\n    return np.hstack([rec[fields] for rec in recs])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stretch(arr, fields=None, return_indices=False):\n    dtype = []\n    len_array = None\n\n    flatten = False\n    if fields is None:\n        fields = arr.dtype.names\n    elif isinstance(fields, string_types):\n        fields = [fields]\n        flatten = True\n\n    # Construct dtype and check consistency\n    for field in fields:\n        dt = arr.dtype[field]\n        if dt == 'O' or len(dt.shape):\n            if dt == 'O':\n                # Variable-length array field\n                lengths = VLEN(arr[field])\n            else:\n                # Fixed-length array field\n                lengths = np.repeat(dt.shape[0], arr.shape[0])\n            if len_array is None:\n                len_array = lengths\n            elif not np.array_equal(lengths, len_array):\n                raise ValueError(\n                    \"inconsistent lengths of array columns in input\")\n            if dt == 'O':\n                dtype.append((field, arr[field][0].dtype))\n            else:\n                dtype.append((field, arr[field].dtype, dt.shape[1:]))\n        else:\n            # Scalar field\n            dtype.append((field, dt))\n\n    if len_array is None:\n        raise RuntimeError(\"no array column in input\")\n\n    # Build stretched output\n    ret = np.empty(np.sum(len_array), dtype=dtype)\n    for field in fields:\n        dt = arr.dtype[field]\n        if dt == 'O' or len(dt.shape) == 1:\n            # Variable-length or 1D fixed-length array field\n            ret[field] = np.hstack(arr[field])\n        elif len(dt.shape):\n            # Multidimensional fixed-length array field\n            ret[field] = np.vstack(arr[field])\n        else:\n            # Scalar field\n            ret[field] = np.repeat(arr[field], len_array)\n\n    if flatten:\n        ret = ret[fields[0]]\n\n    if return_indices:\n        idx = np.concatenate(list(map(np.arange, len_array)))\n        return ret, idx\n\n    return ret", "response": "Stretch an array by stacked array fields while keeping column names and record array structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the indices of all duplicated array elements.", "response": "def dup_idx(arr):\n    \"\"\"Return the indices of all duplicated array elements.\n\n    Parameters\n    ----------\n    arr : array-like object\n        An array-like object\n\n    Returns\n    -------\n    idx : NumPy array\n        An array containing the indices of the duplicated elements\n\n    Examples\n    --------\n    >>> from root_numpy import dup_idx\n    >>> dup_idx([1, 2, 3, 4, 5])\n    array([], dtype=int64)\n    >>> dup_idx([1, 2, 3, 4, 5, 5])\n    array([4, 5])\n    >>> dup_idx([1, 2, 3, 4, 5, 5, 1])\n    array([0, 4, 5, 6])\n\n    \"\"\"\n    _, b = np.unique(arr, return_inverse=True)\n    return np.nonzero(np.logical_or.reduce(\n        b[:, np.newaxis] == np.nonzero(np.bincount(b) > 1),\n        axis=1))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blockwise_inner_join(data, left, foreign_key, right,\n                         force_repeat=None,\n                         foreign_key_name=None):\n    \"\"\"Perform a blockwise inner join.\n\n    Perform a blockwise inner join from names specified in ``left`` to\n    ``right`` via ``foreign_key``: left->foreign_key->right.\n\n    Parameters\n    ----------\n    data : array\n        A structured NumPy array.\n    left : array\n        Array of left side column names.\n    foreign_key : array or string\n        NumPy array or string ``foreign_key`` column name. This column can be\n        either an integer or an array of ints. If ``foreign_key`` is an array\n        of int column, left column will be treated according to left column\n        type:\n\n        * Scalar columns or columns in ``force_repeat`` will be repeated\n        * Array columns not in ``force_repeat`` will be assumed to the\n          same length as ``foreign_key`` and will be stretched by index\n    right : array\n        Array of right side column names. These are array columns that each\n        index ``foreign_key`` points to. These columns are assumed to have the\n        same length.\n    force_repeat : array, optional (default=None)\n        Array of left column names that will be forced to stretch even if it's\n        an array (useful when you want to emulate a multiple join).\n    foreign_key_name : str, optional (default=None)\n        The name of foreign key column in the output array.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from root_numpy import blockwise_inner_join\n    >>> test_data = np.array([\n    (1.0, np.array([11, 12, 13]), np.array([1, 0, 1]), 0, np.array([1, 2, 3])),\n    (2.0, np.array([21, 22, 23]), np.array([-1, 2, -1]), 1, np.array([31, 32, 33]))],\n    dtype=[('sl', np.float), ('al', 'O'), ('fk', 'O'), ('s_fk', np.int), ('ar', 'O')])\n\n    >>> blockwise_inner_join(test_data, ['sl', 'al'], test_data['fk'], ['ar'])\n    array([(1.0, 11, 2, 1), (1.0, 12, 1, 0), (1.0, 13, 2, 1), (2.0, 22, 33, 2)],\n    dtype=[('sl', '<f8'), ('al', '<i8'), ('ar', '<i8'), ('fk', '<i8')])\n\n    >>> blockwise_inner_join(test_data, ['sl', 'al'], test_data['fk'], ['ar'], force_repeat=['al'])\n    array([(1.0, [11, 12, 13], 2, 1), (1.0, [11, 12, 13], 1, 0),\n    (1.0, [11, 12, 13], 2, 1), (2.0, [21, 22, 23], 33, 2)],\n    dtype=[('sl', '<f8'), ('al', '|O8'), ('ar', '<i8'), ('fk', '<i8')])\n\n    \"\"\"\n    if isinstance(foreign_key, string_types):\n        foreign_key = data[foreign_key]\n    return _blockwise_inner_join(data, left, foreign_key, right,\n                                 force_repeat, foreign_key_name)", "response": "Perform a blockwise inner join from left to right via foreign_key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate a TMVA. Reader over a NumPy array.", "response": "def evaluate_reader(reader, name, events, aux=0.):\n    \"\"\"Evaluate a TMVA::Reader over a NumPy array.\n\n    Parameters\n    ----------\n    reader : TMVA::Reader\n        A TMVA::Factory instance with variables booked in exactly the same\n        order as the columns in ``events``.\n    name : string\n        The method name.\n    events : numpy array of shape [n_events, n_variables]\n        A two-dimensional NumPy array containing the rows of events and columns\n        of variables. The order of the columns must match the order in which\n        you called ``AddVariable()`` for each variable.\n    aux : float, optional (default=0.)\n        Auxiliary value used by MethodCuts to set the desired signal\n        efficiency.\n\n    Returns\n    -------\n    output : numpy array of shape [n_events]\n        The method output value for each event\n\n    See Also\n    --------\n    evaluate_method\n\n    \"\"\"\n    if not isinstance(reader, TMVA.Reader):\n        raise TypeError(\"reader must be a TMVA.Reader instance\")\n    events = np.ascontiguousarray(events, dtype=np.float64)\n    if events.ndim == 1:\n        # convert to 2D\n        events = events[:, np.newaxis]\n    elif events.ndim != 2:\n        raise ValueError(\n            \"events must be a two-dimensional array \"\n            \"with one event per row\")\n    return _libtmvanumpy.evaluate_reader(\n        ROOT.AsCObject(reader), name, events, aux)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate a TMVA method over a NumPy array.", "response": "def evaluate_method(method, events, aux=0.):\n    \"\"\"Evaluate a TMVA::MethodBase over a NumPy array.\n\n    .. warning:: TMVA::Reader has known problems with thread safety in versions\n       of ROOT earlier than 6.03. There will potentially be a crash if you call\n       ``method = reader.FindMVA(name)`` in Python and then pass this\n       ``method`` here. Consider using ``evaluate_reader`` instead if you are\n       affected by this crash.\n\n    Parameters\n    ----------\n    method : TMVA::MethodBase\n        A TMVA::MethodBase instance with variables booked in exactly the same\n        order as the columns in ``events``.\n    events : numpy array of shape [n_events, n_variables]\n        A two-dimensional NumPy array containing the rows of events and columns\n        of variables. The order of the columns must match the order in which\n        you called ``AddVariable()`` for each variable.\n    aux : float, optional (default=0.)\n        Auxiliary value used by MethodCuts to set the desired signal\n        efficiency.\n\n    Returns\n    -------\n    output : numpy array of shape [n_events]\n        The method output value for each event\n\n    See Also\n    --------\n    evaluate_reader\n\n    \"\"\"\n    if not isinstance(method, TMVA.MethodBase):\n        raise TypeError(\"reader must be a TMVA.MethodBase instance\")\n    events = np.ascontiguousarray(events, dtype=np.float64)\n    if events.ndim == 1:\n        # convert to 2D\n        events = events[:, np.newaxis]\n    elif events.ndim != 2:\n        raise ValueError(\n            \"events must be a two-dimensional array \"\n            \"with one event per row\")\n    return _libtmvanumpy.evaluate_method(ROOT.AsCObject(method), events, aux)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, query, args=None):\n        '''Execute a query'''\n        conn = self._get_db()\n\n        while (yield self.nextset()):\n            pass\n\n        if PY2:  # Use bytes on Python 2 always\n            encoding = conn.encoding\n\n            def ensure_bytes(x):\n                if isinstance(x, unicode):\n                    x = x.encode(encoding)\n                return x\n\n            query = ensure_bytes(query)\n\n            if args is not None:\n                if isinstance(args, (tuple, list)):\n                    args = tuple(map(ensure_bytes, args))\n                elif isinstance(args, dict):\n                    args = dict((ensure_bytes(key), ensure_bytes(val)) for (key, val) in args.items())\n                else:\n                    args = ensure_bytes(args)\n\n        if args is not None:\n            query = query % self._escape_args(args, conn)\n\n        yield self._query(query)\n        self._executed = query\n        raise gen.Return(self.rowcount)", "response": "Execute a query and return the number of rows"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef executemany(self, query, args):\n        if not args:\n            return\n\n        m = RE_INSERT_VALUES.match(query)\n        if m:\n            q_prefix = m.group(1)\n            q_values = m.group(2).rstrip()\n            q_postfix = m.group(3) or ''\n            assert q_values[0] == '(' and q_values[-1] == ')'\n            yield self._do_execute_many(q_prefix, q_values, q_postfix, args,\n                                        self.max_stmt_length,\n                                        self._get_db().encoding)\n        else:\n            rows = 0\n            for arg in args:\n                yield self.execute(query, arg)\n                rows += self.rowcount\n            self.rowcount = rows\n        raise gen.Return(self.rowcount)", "response": "Execute a query against one query."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the next row from the result stream.", "response": "def read_next(self):\n        \"\"\" Read next row \"\"\"\n        row = yield self._result._read_rowdata_packet_unbuffered()\n        row = self._conv_row(row)\n        raise gen.Return(row)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching next row from the table.", "response": "def fetchone(self):\n        \"\"\" Fetch next row \"\"\"\n        self._check_executed()\n        row = yield self.read_next()\n        if row is None:\n            raise gen.Return(None)\n        self.rownumber += 1\n        raise gen.Return(row)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch all entries from the table as per MySQLdb.", "response": "def fetchall(self):\n        \"\"\"\n        Fetch all, as per MySQLdb. Pretty useless for large queries, as\n        it is buffered. See fetchall_unbuffered(), if you want an unbuffered\n        generator version of this method.\n        \"\"\"\n        rows = []\n        while True:\n            row = yield self.fetchone()\n            if row is None:\n                break\n            rows.append(row)\n        raise gen.Return(rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Binary(x):\n    if isinstance(x, text_type) and not (JYTHON or IRONPYTHON):\n        return x.encode()\n    return bytes(x)", "response": "Return x as a binary type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(*args, **kwargs):\n    from .connections import Connection\n    conn = Connection(*args, **kwargs)\n    yield conn.connect()\n    raise gen.Return(conn)", "response": "Connect to a single node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a string representing a datetime or TIMESTAMP column value into a datetime object.", "response": "def convert_datetime(obj):\n    \"\"\"Returns a DATETIME or TIMESTAMP column value as a datetime object:\n\n      >>> datetime_or_None('2007-02-25 23:06:20')\n      datetime.datetime(2007, 2, 25, 23, 6, 20)\n      >>> datetime_or_None('2007-02-25T23:06:20')\n      datetime.datetime(2007, 2, 25, 23, 6, 20)\n\n    Illegal values are returned as None:\n\n      >>> datetime_or_None('2007-02-31T23:06:20') is None\n      True\n      >>> datetime_or_None('0000-00-00 00:00:00') is None\n      True\n\n    \"\"\"\n    if ' ' in obj:\n        sep = ' '\n    elif 'T' in obj:\n        sep = 'T'\n    else:\n        return convert_date(obj)\n\n    try:\n        ymd, hms = obj.split(sep, 1)\n        usecs = '0'\n        if '.' in hms:\n            hms, usecs = hms.split('.')\n        usecs = float('0.' + usecs) * 1e6\n        return datetime.datetime(*[ int(x) for x in ymd.split('-')+hms.split(':')+[usecs] ])\n    except ValueError:\n        return convert_date(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a string representing a TIME column into a timedelta object.", "response": "def convert_timedelta(obj):\n    \"\"\"Returns a TIME column as a timedelta object:\n\n      >>> timedelta_or_None('25:06:17')\n      datetime.timedelta(1, 3977)\n      >>> timedelta_or_None('-25:06:17')\n      datetime.timedelta(-2, 83177)\n\n    Illegal values are returned as None:\n\n      >>> timedelta_or_None('random crap') is None\n      True\n\n    Note that MySQL always returns TIME columns as (+|-)HH:MM:SS, but\n    can accept values as (+|-)DD HH:MM:SS. The latter format will not\n    be parsed correctly by this function.\n    \"\"\"\n    try:\n        microseconds = 0\n        if \".\" in obj:\n            (obj, tail) = obj.split('.')\n            microseconds = float('0.' + tail) * 1e6\n        hours, minutes, seconds = obj.split(':')\n        negate = 1\n        if hours.startswith(\"-\"):\n            hours = hours[1:]\n            negate = -1\n        tdelta = datetime.timedelta(\n            hours = int(hours),\n            minutes = int(minutes),\n            seconds = int(seconds),\n            microseconds = int(microseconds)\n            ) * negate\n        return tdelta\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a string representation of a TIME column into a time object.", "response": "def convert_time(obj):\n    \"\"\"Returns a TIME column as a time object:\n\n      >>> time_or_None('15:06:17')\n      datetime.time(15, 6, 17)\n\n    Illegal values are returned as None:\n\n      >>> time_or_None('-25:06:17') is None\n      True\n      >>> time_or_None('random crap') is None\n      True\n\n    Note that MySQL always returns TIME columns as (+|-)HH:MM:SS, but\n    can accept values as (+|-)DD HH:MM:SS. The latter format will not\n    be parsed correctly by this function.\n\n    Also note that MySQL's TIME column corresponds more closely to\n    Python's timedelta and not time. However if you want TIME columns\n    to be treated as time-of-day and not a time offset, then you can\n    use set this function as the converter for FIELD_TYPE.TIME.\n    \"\"\"\n    try:\n        microseconds = 0\n        if \".\" in obj:\n            (obj, tail) = obj.split('.')\n            microseconds = float('0.' + tail) * 1e6\n        hours, minutes, seconds = obj.split(':')\n        return datetime.time(hour=int(hours), minute=int(minutes),\n                             second=int(seconds), microsecond=int(microseconds))\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_date(obj):\n    try:\n        return datetime.date(*[ int(x) for x in obj.split('-', 2) ])\n    except ValueError:\n        return None", "response": "Converts a string to a DATE column."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a MySQL TIMESTAMP to a Timestamp object.", "response": "def convert_mysql_timestamp(timestamp):\n    \"\"\"Convert a MySQL TIMESTAMP to a Timestamp object.\n\n    MySQL >= 4.1 returns TIMESTAMP in the same format as DATETIME:\n\n      >>> mysql_timestamp_converter('2007-02-25 22:32:17')\n      datetime.datetime(2007, 2, 25, 22, 32, 17)\n\n    MySQL < 4.1 uses a big string of numbers:\n\n      >>> mysql_timestamp_converter('20070225223217')\n      datetime.datetime(2007, 2, 25, 22, 32, 17)\n\n    Illegal values are returned as None:\n\n      >>> mysql_timestamp_converter('2007-02-31 22:32:17') is None\n      True\n      >>> mysql_timestamp_converter('00000000000000') is None\n      True\n\n    \"\"\"\n    if timestamp[4] == '-':\n        return convert_datetime(timestamp)\n    timestamp += \"0\"*(14-len(timestamp)) # padding\n    year, month, day, hour, minute, second = \\\n        int(timestamp[:4]), int(timestamp[4:6]), int(timestamp[6:8]), \\\n        int(timestamp[8:10]), int(timestamp[10:12]), int(timestamp[12:14])\n    try:\n        return datetime.datetime(year, month, day, hour, minute, second)\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the socket without sending quit message.", "response": "def close(self):\n        \"\"\"Close the socket without sending quit message.\"\"\"\n        stream = self._stream\n        if stream is None:\n            return\n        self._stream = None\n        stream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend the quit message and close the socket", "response": "def close_async(self):\n        \"\"\"Send the quit message and close the socket\"\"\"\n        if self._stream is None or self._stream.closed():\n            self._stream = None\n            return\n        send_data = struct.pack('<i', 1) + int2byte(COMMAND.COM_QUIT)\n        yield self._stream.write(send_data)\n        self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_warnings(self):\n        yield self._execute_command(COMMAND.COM_QUERY, \"SHOW WARNINGS\")\n        result = MySQLResult(self)\n        yield result.read()\n        raise gen.Return(result.rows)", "response": "Show the warning messages for all the available tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nescape whatever value you pass to it", "response": "def escape(self, obj):\n        ''' Escape whatever value you pass to it  '''\n        if isinstance(obj, str_type):\n            return \"'\" + self.escape_string(obj) + \"'\"\n        return escape_item(obj, self.charset)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_packet(self, packet_type=MysqlPacket):\n        buff = b''\n        try:\n            while True:\n                packet_header = yield self._stream.read_bytes(4)\n                if DEBUG: dump_packet(packet_header)\n\n                btrl, btrh, packet_number = struct.unpack('<HBB', packet_header)\n                bytes_to_read = btrl + (btrh << 16)\n                #TODO: check sequence id\n                recv_data = yield self._stream.read_bytes(bytes_to_read)\n                if DEBUG: dump_packet(recv_data)\n                buff += recv_data\n                if bytes_to_read < MAX_PACKET_LEN:\n                    break\n        except iostream.StreamClosedError as e:\n            raise OperationalError(2006, \"MySQL server has gone away (%s)\" % (e,))\n        packet = packet_type(buff, self.encoding)\n        packet.check_error()\n        raise gen.Return(packet)", "response": "Read an entire mysql packet in its entirety and return a MysqlPacket type that represents the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple of opened connections free connections and waiters.", "response": "def stat(self):\n        \"\"\"Returns (opened connections, free connections, waiters)\"\"\"\n        return (self._opened_conns, len(self._free_conn), len(self._waitings))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting a query in pool. Returns future yielding closed cursor.", "response": "def execute(self, query, params=None, cursor=None):\n        \"\"\"Execute query in pool.\n\n        Returns future yielding closed cursor.\n        You can get rows, lastrowid, etc from the cursor.\n        :param cursor: cursor class(Cursor, DictCursor. etc.)\n\n        :return: Future of cursor\n        :rtype: Future\n        \"\"\"\n        conn = yield self._get_conn()\n        try:\n            cur = conn.cursor(cursor)\n            yield cur.execute(query, params)\n            yield cur.close()\n        except:\n            self._close_conn(conn)\n            raise\n        else:\n            self._put_conn(conn)\n        raise Return(cur)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef begin(self):\n        conn = yield self._get_conn()\n        try:\n            yield conn.begin()\n        except:\n            self._close_conn(conn)\n            raise\n        trx = Transaction(self, conn)\n        raise Return(trx)", "response": "Start a transaction. Wait to get connection and returns Transaction object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a query and return a cursor.", "response": "def execute(self, query, args=None):\n        \"\"\"\n        :return: Future[Cursor]\n        :rtype: Future\n        \"\"\"\n        self._ensure_conn()\n        cur = self._conn.cursor()\n        yield cur.execute(query, args)\n        raise Return(cur)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_to_response(self, context, **response_kwargs):\n        serializer = GeoJSONSerializer()\n        response = self.response_class(**response_kwargs)\n        queryset = self.get_queryset()\n\n        options = dict(properties=self.properties,\n                       precision=self.precision,\n                       simplify=self.simplify,\n                       srid=self.srid,\n                       geometry_field=self.geometry_field,\n                       force2d=self.force2d,\n                       bbox=self.bbox,\n                       bbox_auto=self.bbox_auto,\n                       use_natural_keys=self.use_natural_keys,\n                       with_modelname=self.with_modelname)\n        serializer.serialize(queryset, stream=response, ensure_ascii=False,\n                             **options)\n        return response", "response": "Returns a JSON response for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_queryset(self):\n        self.z, self.x, self.y = self._parse_args()\n        nw = self.tile_coord(self.x, self.y, self.z)\n        se = self.tile_coord(self.x + 1, self.y + 1, self.z)\n        bbox = Polygon((nw, (se[0], nw[1]),\n                       se, (nw[0], se[1]), nw))\n        qs = super(TiledGeoJSONLayerView, self).get_queryset()\n        qs = qs.filter(**{\n            '%s__intersects' % self.geometry_field: bbox\n        })\n        self.bbox = bbox.extent\n\n        # Simplification dict by zoom level\n        simplifications = self.simplifications or {}\n        z = self.z\n        self.simplify = simplifications.get(z)\n        while self.simplify is None and z < 32:\n            z += 1\n            self.simplify = simplifications.get(z)\n\n        # Won't trim point geometries to a boundary\n        model_field = qs.model._meta.get_field(self.geometry_field)\n        self.trim_to_boundary = (self.trim_to_boundary and\n                                 not isinstance(model_field, PointField) and\n                                 Intersection is not None)\n        if self.trim_to_boundary:\n            if django.VERSION < (1, 9):\n                qs = qs.intersection(bbox)\n            else:\n                qs = qs.annotate(intersection=Intersection(self.geometry_field, bbox))\n            self.geometry_field = 'intersection'\n\n        return qs", "response": "Returns a queryset of all the TiledGeoJSONLayer objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_encoder_with_precision(precision, JSONEncoderClass):\n    needs_class_hack = not hasattr(json.encoder, 'FLOAT_REPR')\n    try:\n        if precision is not None:\n            def float_repr(o):\n                return format(o, '.%sf' % precision)\n            if not needs_class_hack:\n                # python is not 3.5\n                original_float_repr = json.encoder.FLOAT_REPR\n                json.encoder.FLOAT_REPR = float_repr\n            else:\n                # HACK to allow usage of float precision in python 3.5\n                # Needed because python 3.5 removes the global FLOAT_REPR hack\n\n                class JSONEncoderClass(JSONEncoderClass):\n                    FLOAT_REPR = float.__repr__\n\n                    # Follows code copied from cPython Lib/json/encoder.py\n                    # The only change is that in floatstr _repr=float.__repr__ is replaced\n\n                    def iterencode(self, o, _one_shot=False):\n                        \"\"\"Encode the given object and yield each string\n                        representation as available.\n\n                        For example::\n\n                            for chunk in JSONEncoder().iterencode(bigobject):\n                                mysocket.write(chunk)\n\n                        \"\"\"\n                        if self.check_circular:\n                            markers = {}\n                        else:\n                            markers = None\n                        if self.ensure_ascii:\n                            _encoder = json.encoder.encode_basestring_ascii\n                        else:\n                            _encoder = json.encoder.encode_basestring\n\n                        def floatstr(o, allow_nan=self.allow_nan, _repr=float_repr, _inf=json.encoder.INFINITY, _neginf=-json.encoder.INFINITY):\n                            # Check for specials.  Note that this type of test is processor\n                            # and/or platform-specific, so do tests which don't depend on the\n                            # internals.\n\n                            if o != o:\n                                text = 'NaN'\n                            elif o == _inf:\n                                text = 'Infinity'\n                            elif o == _neginf:\n                                text = '-Infinity'\n                            else:\n                                return _repr(o)\n\n                            if not allow_nan:\n                                raise ValueError(\n                                    \"Out of range float values are not JSON compliant: \" +\n                                    repr(o))\n\n                            return text\n\n                        if (_one_shot and json.encoder.c_make_encoder is not None and self.indent is None):\n                            _iterencode = json.encoder.c_make_encoder(\n                                markers, self.default, _encoder, self.indent,\n                                self.key_separator, self.item_separator, self.sort_keys,\n                                self.skipkeys, self.allow_nan)\n                        else:\n                            _iterencode = json.encoder._make_iterencode(\n                                markers, self.default, _encoder, self.indent, floatstr,\n                                self.key_separator, self.item_separator, self.sort_keys,\n                                self.skipkeys, _one_shot)\n                        return _iterencode(o, 0)\n        yield JSONEncoderClass\n    finally:\n        if precision is not None:\n            if not needs_class_hack:\n                # restore FLOAT_REPR\n                json.encoder.FLOAT_REPR = original_float_repr", "response": "A JSON encoder class that can encode float values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Deserializer(stream_or_string, **options):\n\n    geometry_field = options.get(\"geometry_field\", \"geom\")\n\n    def FeatureToPython(dictobj):\n        properties = dictobj['properties']\n        model_name = options.get(\"model_name\") or properties.pop('model')\n        # Deserialize concrete fields only (bypass dynamic properties)\n        model = _get_model(model_name)\n        field_names = [f.name for f in model._meta.fields]\n        fields = {}\n        for k, v in iteritems(properties):\n            if k in field_names:\n                fields[k] = v\n        obj = {\n            \"model\": model_name,\n            \"pk\": dictobj.get('id') or properties.get('id'),\n            \"fields\": fields\n        }\n        if isinstance(model._meta.get_field(geometry_field), GeoJSONField):\n            obj['fields'][geometry_field] = dictobj['geometry']\n        else:\n            shape = GEOSGeometry(json.dumps(dictobj['geometry']))\n            obj['fields'][geometry_field] = shape.wkt\n        return obj\n\n    if isinstance(stream_or_string, string_types):\n        stream = StringIO(stream_or_string)\n    else:\n        stream = stream_or_string\n    try:\n        collection = json.load(stream)\n        objects = [FeatureToPython(f) for f in collection['features']]\n        for obj in PythonDeserializer(objects, **options):\n            yield obj\n    except GeneratorExit:\n        raise\n    except Exception as e:\n        # Map to deserializer error\n        raise DeserializationError(repr(e))", "response": "Deserializes a stream or string of JSON data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all related objects except many - to - many ones", "response": "def get_all_related_objects(opts):\n    \"\"\"\n    Django 1.8 changed meta api, see\n    https://docs.djangoproject.com/en/1.8/ref/models/meta/#migrating-old-meta-api\n    https://code.djangoproject.com/ticket/12663\n    https://github.com/django/django/pull/3848\n    Initially from Django REST Framework:\n    https://github.com/tomchristie/django-rest-framework/blob/3.3.2/rest_framework/compat.py\n    :param opts: Options instance\n    :return: list of relations except many-to-many ones\n    \"\"\"\n    if django.VERSION < (1, 8):\n        return opts.get_all_related_objects()\n    else:\n        return [r for r in opts.related_objects if not r.field.many_to_many]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the geometry field.", "response": "def _handle_geom(self, value):\n        \"\"\" Geometry processing (in place), depending on options \"\"\"\n        if value is None:\n            geometry = None\n        elif isinstance(value, dict) and 'type' in value:\n            geometry = value\n        else:\n            if isinstance(value, GEOSGeometry):\n                geometry = value\n            else:\n                try:\n                    # this will handle string representations (e.g. ewkt, bwkt)\n                    geometry = GEOSGeometry(value)\n                except ValueError:\n                    # if the geometry couldn't be parsed.\n                    # we can't generate valid geojson\n                    error_msg = 'The field [\"%s\", \"%s\"] could not be parsed as a valid geometry' % (\n                        self.geometry_field, value\n                    )\n                    raise SerializationError(error_msg)\n\n            # Optional force 2D\n            if self.options.get('force2d'):\n                wkb_w = WKBWriter()\n                wkb_w.outdim = 2\n                geometry = GEOSGeometry(wkb_w.write(geometry), srid=geometry.srid)\n            # Optional geometry simplification\n            simplify = self.options.get('simplify')\n            if simplify is not None:\n                geometry = geometry.simplify(tolerance=simplify, preserve_topology=True)\n            # Optional geometry reprojection\n            if geometry.srid and geometry.srid != self.srid:\n                geometry.transform(self.srid)\n            # Optional bbox\n            if self.options.get('bbox_auto'):\n                self._current['bbox'] = geometry.extent\n\n        self._current['geometry'] = geometry"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef geojsonfeature(source, params=''):\n    parse = re.search(r'(?P<properties>((\\w+)(,\\w+)*)?)(:(?P<field>(\\w+)?))?(:(?P<srid>(\\d+)?))?', params)\n    if parse:\n        parse = parse.groupdict()\n    else:\n        parse = {}\n\n    geometry_field = parse.get('field') or 'geom'\n    properties = parse.get('properties', '').split(',')\n    srid = parse.get('srid') or GEOJSON_DEFAULT_SRID\n\n    if source is None or isinstance(source, string_types):\n        return 'null'\n\n    if isinstance(source, (GEOSGeometry, GeometryField)):\n        encoder = DjangoGeoJSONEncoder()\n        if source.srid != srid:\n            source.transform(srid)\n        feature = {\"type\": \"Feature\", \"properties\": {}}\n        feature['geometry'] = encoder.default(source)\n        return json.dumps(feature)\n\n    serializer = Serializer()\n\n    if not hasattr(source, '__iter__'):\n        source = [source]\n\n    return serializer.serialize(source, properties=properties,\n                                geometry_field=geometry_field, srid=srid)", "response": "Returns a GeoJSON feature from a GeoJSON geometry or list of GeoJSON geometries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datetime_to_utc(ts):\n    if not isinstance(ts, datetime.datetime):\n        msg = '<%s> object' % type(ts)\n        raise InvalidDateError(date=msg)\n\n    if not ts.tzinfo:\n        ts = ts.replace(tzinfo=dateutil.tz.tzutc())\n\n    try:\n        ts = ts.astimezone(dateutil.tz.tzutc())\n    except ValueError:\n        logger.warning(\"Date %s str does not have a valid timezone\", ts)\n        logger.warning(\"Date converted to UTC removing timezone info\")\n        ts = ts.replace(tzinfo=dateutil.tz.tzutc()).astimezone(dateutil.tz.tzutc())\n\n    return ts", "response": "Convert a timestamp to UTC + 0 timezone."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_to_datetime(ts):\n    def parse_datetime(ts):\n        dt = dateutil.parser.parse(ts)\n        if not dt.tzinfo:\n            dt = dt.replace(tzinfo=dateutil.tz.tzutc())\n        return dt\n\n    if not ts:\n        raise InvalidDateError(date=str(ts))\n\n    try:\n        # Try to remove additional information after\n        # timezone section because it cannot be parsed,\n        # like in 'Wed, 26 Oct 2005 15:20:32 -0100 (GMT+1)'\n        # or in 'Thu, 14 Aug 2008 02:07:59 +0200 CEST'.\n        m = re.search(r\"^.+?\\s+[\\+\\-\\d]\\d{4}(\\s+.+)$\", ts)\n        if m:\n            ts = ts[:m.start(1)]\n\n        try:\n            dt = parse_datetime(ts)\n        except ValueError as e:\n            # Try to remove the timezone, usually it causes\n            # problems.\n            m = re.search(r\"^(.+?)\\s+[\\+\\-\\d]\\d{4}.*$\", ts)\n\n            if m:\n                dt = parse_datetime(m.group(1))\n                logger.warning(\"Date %s str does not have a valid timezone\", ts)\n                logger.warning(\"Date converted removing timezone info\")\n                return dt\n\n            raise e\n\n        return dt\n\n    except ValueError as e:\n        raise InvalidDateError(date=str(ts))", "response": "Format a string to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unixtime_to_datetime(ut):\n    try:\n        dt = datetime.datetime.utcfromtimestamp(ut)\n        dt = dt.replace(tzinfo=dateutil.tz.tzutc())\n        return dt\n    except Exception:\n        raise InvalidDateError(date=str(ut))", "response": "Convert a unixtime timestamp to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the parameters of a callable.", "response": "def inspect_signature_parameters(callable_, excluded=None):\n    \"\"\"Get the parameters of a callable.\n\n    Returns a list with the signature parameters of `callable_`.\n    Parameters contained in `excluded` tuple will not be included\n    in the result.\n\n    :param callable_: callable object\n    :param excluded: tuple with default parameters to exclude\n\n    :result: list of parameters\n    \"\"\"\n    if not excluded:\n        excluded = ()\n\n    signature = inspect.signature(callable_)\n    params = [\n        v for p, v in signature.parameters.items()\n        if p not in excluded\n    ]\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind on a set of candidates the parameters needed to execute a callable.", "response": "def find_signature_parameters(callable_, candidates,\n                              excluded=('self', 'cls')):\n    \"\"\"Find on a set of candidates the parameters needed to execute a callable.\n\n    Returns a dictionary with the `candidates` found on `callable_`.\n    When any of the required parameters of a callable is not found,\n    it raises a `AttributeError` exception. A signature parameter\n    whitout a default value is considered as required.\n\n    :param callable_: callable object\n    :param candidates: dict with the possible parameters to use\n        with the callable\n    :param excluded: tuple with default parameters to exclude\n\n    :result: dict of parameters ready to use with the callable\n\n    :raises AttributeError: when any of the required parameters for\n        executing a callable is not found in `candidates`\n    \"\"\"\n    signature_params = inspect_signature_parameters(callable_,\n                                                    excluded=excluded)\n    exec_params = {}\n\n    add_all = False\n    for param in signature_params:\n        name = param.name\n\n        if str(param).startswith('*'):\n            add_all = True\n        elif name in candidates:\n            exec_params[name] = candidates[name]\n        elif param.default == inspect.Parameter.empty:\n            msg = \"required argument %s not found\" % name\n            raise AttributeError(msg, name)\n        else:\n            continue\n\n    if add_all:\n        exec_params = candidates\n\n    return exec_params"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_class_properties(cls):\n    candidates = inspect.getmembers(cls, inspect.isdatadescriptor)\n    result = [\n        (name, value) for name, value in candidates\n        if isinstance(value, property)\n    ]\n    return result", "response": "Find property members in a class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef coroutine(func):\n    def __start(*args, **kwargs):\n        \"\"\"Automatically calls next() on the internal generator function.\"\"\"\n        __cr = func(*args, **kwargs)\n        next(__cr)\n        return __cr\n    return __start", "response": "Decorator to implement the coroutine pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether or not a given line appears to be Python code.", "response": "def _checkIfCode(self, inCodeBlockObj):\n        \"\"\"Checks whether or not a given line appears to be Python code.\"\"\"\n        while True:\n            line, lines, lineNum = (yield)\n            testLineNum = 1\n            currentLineNum = 0\n            testLine = line.strip()\n            lineOfCode = None\n            while lineOfCode is None:\n                match = AstWalker.__errorLineRE.match(testLine)\n                if not testLine or testLine == '...' or match:\n                    # These are ambiguous.\n                    line, lines, lineNum = (yield)\n                    testLine = line.strip()\n                    #testLineNum = 1\n                elif testLine.startswith('>>>'):\n                    # This is definitely code.\n                    lineOfCode = True\n                else:\n                    try:\n                        compLine = compile_command(testLine)\n                        if compLine and lines[currentLineNum].strip().startswith('#'):\n                            lineOfCode = True\n                        else:\n                            line, lines, lineNum = (yield)\n                            line = line.strip()\n                            if line.startswith('>>>'):\n                                # Definitely code, don't compile further.\n                                lineOfCode = True\n                            else:\n                                testLine += linesep + line\n                                testLine = testLine.strip()\n                                testLineNum += 1\n                    except (SyntaxError, RuntimeError):\n                        # This is definitely not code.\n                        lineOfCode = False\n                    except Exception:\n                        # Other errors are ambiguous.\n                        line, lines, lineNum = (yield)\n                        testLine = line.strip()\n                        #testLineNum = 1\n                currentLineNum = lineNum - testLineNum\n            if not inCodeBlockObj[0] and lineOfCode:\n                inCodeBlockObj[0] = True\n                lines[currentLineNum] = '{0}{1}# @code{1}'.format(\n                    lines[currentLineNum],\n                    linesep\n                )\n            elif inCodeBlockObj[0] and lineOfCode is False:\n                # None is ambiguous, so strict checking\n                # against False is necessary.\n                inCodeBlockObj[0] = False\n                lines[currentLineNum] = '{0}{1}# @endcode{1}'.format(\n                    lines[currentLineNum],\n                    linesep\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __alterDocstring(self, tail='', writer=None):\n        assert isinstance(tail, str) and isinstance(writer, GeneratorType)\n\n        lines = []\n        timeToSend = False\n        inCodeBlock = False\n        inCodeBlockObj = [False]\n        inSection = False\n        prefix = ''\n        firstLineNum = -1\n        sectionHeadingIndent = 0\n        codeChecker = self._checkIfCode(inCodeBlockObj)\n        while True:\n            lineNum, line = (yield)\n            if firstLineNum < 0:\n                firstLineNum = lineNum\n            # Don't bother doing extra work if it's a sentinel.\n            if line is not None:\n                # Also limit work if we're not parsing the docstring.\n                if self.options.autobrief:\n                    for doxyTag, tagRE in AstWalker.__singleLineREs.items():\n                        match = tagRE.search(line)\n                        if match:\n                            # We've got a simple one-line Doxygen command\n                            lines[-1], inCodeBlock = self._endCodeIfNeeded(\n                                lines[-1], inCodeBlock)\n                            inCodeBlockObj[0] = inCodeBlock\n                            writer.send((firstLineNum, lineNum - 1, lines))\n                            lines = []\n                            firstLineNum = lineNum\n                            line = line.replace(match.group(1), doxyTag)\n                            timeToSend = True\n\n                    if inSection:\n                        # The last line belonged to a section.\n                        # Does this one too? (Ignoring empty lines.)\n                        match = AstWalker.__blanklineRE.match(line)\n                        if not match:\n                            indent = len(line.expandtabs(self.options.tablength)) - \\\n                                len(line.expandtabs(self.options.tablength).lstrip())\n                            if indent <= sectionHeadingIndent:\n                                inSection = False\n                            else:\n                                if lines[-1] == '#':\n                                    # If the last line was empty, but we're still in a section\n                                    # then we need to start a new paragraph.\n                                    lines[-1] = '# @par'\n\n                    match = AstWalker.__returnsStartRE.match(line)\n                    if match:\n                        # We've got a \"returns\" section\n                        lines[-1], inCodeBlock = self._endCodeIfNeeded(\n                            lines[-1], inCodeBlock)\n                        inCodeBlockObj[0] = inCodeBlock\n                        line = line.replace(match.group(0), ' @return\\t').rstrip()\n                        prefix = '@return\\t'\n                    else:\n                        match = AstWalker.__argsStartRE.match(line)\n                        if match:\n                            # We've got an \"arguments\" section\n                            line = line.replace(match.group(0), '').rstrip()\n                            if 'attr' in match.group(0).lower():\n                                prefix = '@property\\t'\n                            else:\n                                prefix = '@param\\t'\n                            lines[-1], inCodeBlock = self._endCodeIfNeeded(\n                                lines[-1], inCodeBlock)\n                            inCodeBlockObj[0] = inCodeBlock\n                            lines.append('#' + line)\n                            continue\n                        else:\n                            match = AstWalker.__argsRE.match(line)\n                            if match and not inCodeBlock:\n                                # We've got something that looks like an item /\n                                # description pair.\n                                if 'property' in prefix:\n                                    line = '# {0}\\t{1[name]}{2}# {1[desc]}'.format(\n                                        prefix, match.groupdict(), linesep)\n                                else:\n                                    line = ' {0}\\t{1[name]}\\t{1[desc]}'.format(\n                                        prefix, match.groupdict())\n                            else:\n                                match = AstWalker.__raisesStartRE.match(line)\n                                if match:\n                                    line = line.replace(match.group(0), '').rstrip()\n                                    if 'see' in match.group(1).lower():\n                                        # We've got a \"see also\" section\n                                        prefix = '@sa\\t'\n                                    else:\n                                        # We've got an \"exceptions\" section\n                                        prefix = '@exception\\t'\n                                    lines[-1], inCodeBlock = self._endCodeIfNeeded(\n                                        lines[-1], inCodeBlock)\n                                    inCodeBlockObj[0] = inCodeBlock\n                                    lines.append('#' + line)\n                                    continue\n                                else:\n                                    match = AstWalker.__listRE.match(line)\n                                    if match and not inCodeBlock:\n                                        # We've got a list of something or another\n                                        itemList = []\n                                        for itemMatch in AstWalker.__listItemRE.findall(self._stripOutAnds(\n                                                                                        match.group(0))):\n                                            itemList.append('# {0}\\t{1}{2}'.format(\n                                                prefix, itemMatch, linesep))\n                                        line = ''.join(itemList)[1:]\n                                    else:\n                                        match = AstWalker.__examplesStartRE.match(line)\n                                        if match and lines[-1].strip() == '#' \\\n                                           and self.options.autocode:\n                                            # We've got an \"example\" section\n                                            inCodeBlock = True\n                                            inCodeBlockObj[0] = True\n                                            line = line.replace(match.group(0),\n                                                                ' @b Examples{0}# @code'.format(linesep))\n                                        else:\n                                            match = AstWalker.__sectionStartRE.match(line)\n                                            if match:\n                                                # We've got an arbitrary section\n                                                prefix = ''\n                                                inSection = True\n                                                # What's the indentation of the section heading?\n                                                sectionHeadingIndent = len(line.expandtabs(self.options.tablength)) \\\n                                                    - len(line.expandtabs(self.options.tablength).lstrip())\n                                                line = line.replace(\n                                                    match.group(0),\n                                                    ' @par {0}'.format(match.group(1))\n                                                )\n                                                if lines[-1] == '# @par':\n                                                    lines[-1] = '#'\n                                                lines[-1], inCodeBlock = self._endCodeIfNeeded(\n                                                    lines[-1], inCodeBlock)\n                                                inCodeBlockObj[0] = inCodeBlock\n                                                lines.append('#' + line)\n                                                continue\n                                            elif prefix:\n                                                match = AstWalker.__singleListItemRE.match(line)\n                                                if match and not inCodeBlock:\n                                                    # Probably a single list item\n                                                    line = ' {0}\\t{1}'.format(\n                                                        prefix, match.group(0))\n                                                elif self.options.autocode:\n                                                    codeChecker.send(\n                                                        (\n                                                            line, lines,\n                                                            lineNum - firstLineNum\n                                                        )\n                                                    )\n                                                    inCodeBlock = inCodeBlockObj[0]\n                                            else:\n                                                if self.options.autocode:\n                                                    codeChecker.send(\n                                                        (\n                                                            line, lines,\n                                                            lineNum - firstLineNum\n                                                        )\n                                                    )\n                                                    inCodeBlock = inCodeBlockObj[0]\n\n                # If we were passed a tail, append it to the docstring.\n                # Note that this means that we need a docstring for this\n                # item to get documented.\n                if tail and lineNum == len(self.docLines) - 1:\n                    line = '{0}{1}# {2}'.format(line.rstrip(), linesep, tail)\n\n                # Add comment marker for every line.\n                line = '#{0}'.format(line.rstrip())\n                # Ensure the first line has the Doxygen double comment.\n                if lineNum == 0:\n                    line = '#' + line\n\n                lines.append(line.replace(' ' + linesep, linesep))\n            else:\n                # If we get our sentinel value, send out what we've got.\n                timeToSend = True\n\n            if timeToSend:\n                lines[-1], inCodeBlock = self._endCodeIfNeeded(lines[-1],\n                                                               inCodeBlock)\n                inCodeBlockObj[0] = inCodeBlock\n                writer.send((firstLineNum, lineNum, lines))\n                lines = []\n                firstLineNum = -1\n                timeToSend = False", "response": "Runs eternally processing the docstring lines and passes them along in batches for writing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __writeDocstring(self):\n        while True:\n            firstLineNum, lastLineNum, lines = (yield)\n            newDocstringLen = lastLineNum - firstLineNum + 1\n            while len(lines) < newDocstringLen:\n                lines.append('')\n            # Substitute the new block of lines for the original block of lines.\n            self.docLines[firstLineNum: lastLineNum + 1] = lines", "response": "Writes out the docstring lines for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the docstring of a module or function.", "response": "def _processDocstring(self, node, tail='', **kwargs):\n        \"\"\"\n        Handles a docstring for functions, classes, and modules.\n\n        Basically just figures out the bounds of the docstring and sends it\n        off to the parser to do the actual work.\n        \"\"\"\n        typeName = type(node).__name__\n        # Modules don't have lineno defined, but it's always 0 for them.\n        curLineNum = startLineNum = 0\n        if typeName != 'Module':\n            startLineNum = curLineNum = node.lineno - 1\n        # Figure out where both our enclosing object and our docstring start.\n        line = ''\n        while curLineNum < len(self.lines):\n            line = self.lines[curLineNum]\n            match = AstWalker.__docstrMarkerRE.match(line)\n            if match:\n                break\n            curLineNum += 1\n        docstringStart = curLineNum\n        # Figure out where our docstring ends.\n        if not AstWalker.__docstrOneLineRE.match(line):\n            # Skip for the special case of a single-line docstring.\n            curLineNum += 1\n            while curLineNum < len(self.lines):\n                line = self.lines[curLineNum]\n                if line.find(match.group(2)) >= 0:\n                    break\n                curLineNum += 1\n        endLineNum = curLineNum + 1\n\n        # Isolate our enclosing object's declaration.\n        defLines = self.lines[startLineNum: docstringStart]\n        # Isolate our docstring.\n        self.docLines = self.lines[docstringStart: endLineNum]\n\n        # If we have a docstring, extract information from it.\n        if self.docLines:\n            # Get rid of the docstring delineators.\n            self.docLines[0] = AstWalker.__docstrMarkerRE.sub('',\n                                                              self.docLines[0])\n            self.docLines[-1] = AstWalker.__docstrMarkerRE.sub('',\n                                                               self.docLines[-1])\n            # Handle special strings within the docstring.\n            docstringConverter = self.__alterDocstring(\n                tail, self.__writeDocstring())\n            for lineInfo in enumerate(self.docLines):\n                docstringConverter.send(lineInfo)\n            docstringConverter.send((len(self.docLines) - 1, None))\n\n        # Add a Doxygen @brief tag to any single-line description.\n        if self.options.autobrief:\n            safetyCounter = 0\n            while len(self.docLines) > 0 and self.docLines[0].lstrip('#').strip() == '':\n                del self.docLines[0]\n                self.docLines.append('')\n                safetyCounter += 1\n                if safetyCounter >= len(self.docLines):\n                    # Escape the effectively empty docstring.\n                    break\n            if len(self.docLines) == 1 or (len(self.docLines) >= 2 and (\n                self.docLines[1].strip(whitespace + '#') == '' or\n                    self.docLines[1].strip(whitespace + '#').startswith('@'))):\n                self.docLines[0] = \"## @brief {0}\".format(self.docLines[0].lstrip('#'))\n                if len(self.docLines) > 1 and self.docLines[1] == '# @par':\n                    self.docLines[1] = '#'\n\n        if defLines:\n            match = AstWalker.__indentRE.match(defLines[0])\n            indentStr = match and match.group(1) or ''\n            self.docLines = [AstWalker.__newlineRE.sub(indentStr + '#', docLine)\n                             for docLine in self.docLines]\n\n        # Taking away a docstring from an interface method definition sometimes\n        # leaves broken code as the docstring may be the only code in it.\n        # Here we manually insert a pass statement to rectify this problem.\n        if typeName != 'Module':\n            if docstringStart < len(self.lines):\n                match = AstWalker.__indentRE.match(self.lines[docstringStart])\n                indentStr = match and match.group(1) or ''\n            else:\n                indentStr = ''\n            containingNodes = kwargs.get('containingNodes', []) or []\n            fullPathNamespace = self._getFullPathName(containingNodes)\n            parentType = fullPathNamespace[-2][1]\n            if parentType == 'interface' and typeName == 'FunctionDef' \\\n               or fullPathNamespace[-1][1] == 'interface':\n                defLines[-1] = '{0}{1}{2}pass'.format(defLines[-1],\n                                                      linesep, indentStr)\n            elif self.options.autobrief and typeName == 'ClassDef':\n                # If we're parsing docstrings separate out class attribute\n                # definitions to get better Doxygen output.\n                for firstVarLineNum, firstVarLine in enumerate(self.docLines):\n                    if '@property\\t' in firstVarLine:\n                        break\n                lastVarLineNum = len(self.docLines)\n                if lastVarLineNum > 0 and '@property\\t' in firstVarLine:\n                    while lastVarLineNum > firstVarLineNum:\n                        lastVarLineNum -= 1\n                        if '@property\\t' in self.docLines[lastVarLineNum]:\n                            break\n                    lastVarLineNum += 1\n                    if firstVarLineNum < len(self.docLines):\n                        indentLineNum = endLineNum\n                        indentStr = ''\n                        while not indentStr and indentLineNum < len(self.lines):\n                            match = AstWalker.__indentRE.match(self.lines[indentLineNum])\n                            indentStr = match and match.group(1) or ''\n                            indentLineNum += 1\n                        varLines = ['{0}{1}'.format(linesep, docLine).replace(\n                                    linesep, linesep + indentStr)\n                                    for docLine in self.docLines[\n                                        firstVarLineNum: lastVarLineNum]]\n                        defLines.extend(varLines)\n                        self.docLines[firstVarLineNum: lastVarLineNum] = []\n                        # After the property shuffling we will need to relocate\n                        # any existing namespace information.\n                        namespaceLoc = defLines[-1].find('\\n# @namespace')\n                        if namespaceLoc >= 0:\n                            self.docLines[-1] += defLines[-1][namespaceLoc:]\n                            defLines[-1] = defLines[-1][:namespaceLoc]\n\n        # For classes and functions, apply our changes and reverse the\n        # order of the declaration and docstring, and for modules just\n        # apply our changes.\n        if typeName != 'Module':\n            self.lines[startLineNum: endLineNum] = self.docLines + defLines\n        else:\n            self.lines[startLineNum: endLineNum] = defLines + self.docLines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a member name indicates that it should be private or protected.", "response": "def _checkMemberName(name):\n        \"\"\"\n        See if a member name indicates that it should be private.\n\n        Private variables in Python (starting with a double underscore but\n        not ending in a double underscore) and bed lumps (variables that\n        are not really private but are by common convention treated as\n        protected because they begin with a single underscore) get Doxygen\n        tags labeling them appropriately.\n        \"\"\"\n        assert isinstance(name, str)\n        restrictionLevel = None\n        if not name.endswith('__'):\n            if name.startswith('__'):\n                restrictionLevel = 'private'\n            elif name.startswith('_'):\n                restrictionLevel = 'protected'\n        return restrictionLevel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _processMembers(self, node, contextTag):\n        restrictionLevel = self._checkMemberName(node.name)\n        if restrictionLevel:\n            workTag = '{0}{1}# @{2}'.format(contextTag,\n                                            linesep,\n                                            restrictionLevel)\n        else:\n            workTag = contextTag\n        return workTag", "response": "Process members and return the appropriate Doxygen tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit(self, node, **kwargs):\n        containingNodes = kwargs.get('containingNodes', [])\n        method = 'visit_' + node.__class__.__name__\n        visitor = getattr(self, method, self.generic_visit)\n        return visitor(node, containingNodes=containingNodes)", "response": "Visit a node and extract useful information from it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the full path name for the given module name.", "response": "def _getFullPathName(self, containingNodes):\n        \"\"\"\n        Returns the full node hierarchy rooted at module name.\n\n        The list representing the full path through containing nodes\n        (starting with the module itself) is returned.\n        \"\"\"\n        assert isinstance(containingNodes, list)\n        return [(self.options.fullPathNamespace, 'module')] + containingNodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the module - level docstring and create appropriate Doxygen tags.", "response": "def visit_Module(self, node, **kwargs):\n        \"\"\"\n        Handles the module-level docstring.\n\n        Process the module-level docstring and create appropriate Doxygen tags\n        if autobrief option is set.\n        \"\"\"\n        containingNodes=kwargs.get('containingNodes', [])\n        if self.options.debug:\n            stderr.write(\"# Module {0}{1}\".format(self.options.fullPathNamespace,\n                                                  linesep))\n        if get_docstring(node):\n            if self.options.topLevelNamespace:\n                fullPathNamespace = self._getFullPathName(containingNodes)\n                contextTag = '.'.join(pathTuple[0] for pathTuple in fullPathNamespace)\n                tail = '@namespace {0}'.format(contextTag)\n            else:\n                tail = ''\n            self._processDocstring(node, tail)\n        # Visit any contained nodes (in this case pretty much everything).\n        self.generic_visit(node, containingNodes=containingNodes)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_Assign(self, node, **kwargs):\n        lineNum = node.lineno - 1\n        # Assignments have one Doxygen-significant special case:\n        # interface attributes.\n        match = AstWalker.__attributeRE.match(self.lines[lineNum])\n        if match:\n            self.lines[lineNum] = '{0}## @property {1}{2}{0}# {3}{2}' \\\n                '{0}# @hideinitializer{2}{4}{2}'.format(\n                    match.group(1),\n                    match.group(2),\n                    linesep,\n                    match.group(3),\n                    self.lines[lineNum].rstrip()\n                )\n            if self.options.debug:\n                stderr.write(\"# Attribute {0.id}{1}\".format(node.targets[0],\n                                                            linesep))\n        if isinstance(node.targets[0], Name):\n            match = AstWalker.__indentRE.match(self.lines[lineNum])\n            indentStr = match and match.group(1) or ''\n            restrictionLevel = self._checkMemberName(node.targets[0].id)\n            if restrictionLevel:\n                self.lines[lineNum] = '{0}## @var {1}{2}{0}' \\\n                    '# @hideinitializer{2}{0}# @{3}{2}{4}{2}'.format(\n                        indentStr,\n                        node.targets[0].id,\n                        linesep,\n                        restrictionLevel,\n                        self.lines[lineNum].rstrip()\n                    )\n        # Visit any contained nodes.\n        self.generic_visit(node, containingNodes=kwargs['containingNodes'])", "response": "Handles assignment nodes within code."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles function calls within code.", "response": "def visit_Call(self, node, **kwargs):\n        \"\"\"\n        Handles function calls within code.\n\n        Function calls in Python are used to represent interface implementations\n        in addition to their normal use.  If a call appears to mark an\n        implementation, it gets labeled as such for Doxygen.\n        \"\"\"\n        lineNum = node.lineno - 1\n        # Function calls have one Doxygen-significant special case:  interface\n        # implementations.\n        match = AstWalker.__implementsRE.match(self.lines[lineNum])\n        if match:\n            self.lines[lineNum] = '{0}## @implements {1}{2}{0}{3}{2}'.format(\n                match.group(1), match.group(2), linesep,\n                self.lines[lineNum].rstrip())\n            if self.options.debug:\n                stderr.write(\"# Implements {0}{1}\".format(match.group(1),\n                                                          linesep))\n        # Visit any contained nodes.\n        self.generic_visit(node, containingNodes=kwargs['containingNodes'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles function definitions within code.", "response": "def visit_FunctionDef(self, node, **kwargs):\n        \"\"\"\n        Handles function definitions within code.\n\n        Process a function's docstring, keeping well aware of the function's\n        context and whether or not it's part of an interface definition.\n        \"\"\"\n        if self.options.debug:\n            stderr.write(\"# Function {0.name}{1}\".format(node, linesep))\n        # Push either 'interface' or 'class' onto our containing nodes\n        # hierarchy so we can keep track of context.  This will let us tell\n        # if a function is nested within another function or even if a class\n        # is nested within a function.\n        containingNodes = kwargs.get('containingNodes') or []\n        containingNodes.append((node.name, 'function'))\n        if self.options.topLevelNamespace:\n            fullPathNamespace = self._getFullPathName(containingNodes)\n            contextTag = '.'.join(pathTuple[0] for pathTuple in fullPathNamespace)\n            modifiedContextTag = self._processMembers(node, contextTag)\n            tail = '@namespace {0}'.format(modifiedContextTag)\n        else:\n            tail = self._processMembers(node, '')\n        if get_docstring(node):\n            self._processDocstring(node, tail,\n                                   containingNodes=containingNodes)\n        # Visit any contained nodes.\n        self.generic_visit(node, containingNodes=containingNodes)\n        # Remove the item we pushed onto the containing nodes hierarchy.\n        containingNodes.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_ClassDef(self, node, **kwargs):\n        lineNum = node.lineno - 1\n        # Push either 'interface' or 'class' onto our containing nodes\n        # hierarchy so we can keep track of context.  This will let us tell\n        # if a function is a method or an interface method definition or if\n        # a class is fully contained within another class.\n        containingNodes = kwargs.get('containingNodes') or []\n\n        if not self.options.object_respect:\n            # Remove object class of the inherited class list to avoid that all\n            # new-style class inherits from object in the hierarchy class\n            line = self.lines[lineNum]\n            match = AstWalker.__classRE.match(line)\n            if match:\n                if match.group(2) == 'object':\n                    self.lines[lineNum] = line[:match.start(2)] + line[match.end(2):]\n\n        match = AstWalker.__interfaceRE.match(self.lines[lineNum])\n        if match:\n            if self.options.debug:\n                stderr.write(\"# Interface {0.name}{1}\".format(node, linesep))\n            containingNodes.append((node.name, 'interface'))\n        else:\n            if self.options.debug:\n                stderr.write(\"# Class {0.name}{1}\".format(node, linesep))\n            containingNodes.append((node.name, 'class'))\n        if self.options.topLevelNamespace:\n            fullPathNamespace = self._getFullPathName(containingNodes)\n            contextTag = '.'.join(pathTuple[0] for pathTuple in fullPathNamespace)\n            tail = '@namespace {0}'.format(contextTag)\n        else:\n            tail = ''\n        # Class definitions have one Doxygen-significant special case:\n        # interface definitions.\n        if match:\n            contextTag = '{0}{1}# @interface {2}'.format(tail,\n                                                         linesep,\n                                                         match.group(1))\n        else:\n            contextTag = tail\n        contextTag = self._processMembers(node, contextTag)\n        if get_docstring(node):\n            self._processDocstring(node, contextTag,\n                                   containingNodes=containingNodes)\n        # Visit any contained nodes.\n        self.generic_visit(node, containingNodes=containingNodes)\n        # Remove the item we pushed onto the containing nodes hierarchy.\n        containingNodes.pop()", "response": "Handles class definitions within code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforming an AST for the code and produce a new version of the source.", "response": "def parseLines(self):\n        \"\"\"Form an AST for the code and produce a new version of the source.\"\"\"\n        inAst = parse(''.join(self.lines), self.inFilename)\n        # Visit all the nodes in our tree and apply Doxygen tags to the source.\n        self.visit(inAst)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and upload the Lambda zip and deploy the API Gateway.", "response": "def handle(self, *args, **options):  # NoQA\n        \"\"\"\n        Execute the command.\n\n        \"\"\"\n\n        # Load the settings\n        self.require_settings(args, options)\n\n        # Load your AWS credentials from ~/.aws/credentials\n        self.load_credentials()\n\n        #Get the Django settings file\n        self.get_django_settings_file()\n\n        # Make sure the necessary IAM execution roles are available\n        self.zappa.create_iam_roles()\n\n        # Create the Lambda Zip\n        self.create_package()\n\n        # Upload it to S3\n        try:\n            zip_arn = self.zappa.upload_to_s3(\n                self.zip_path, self.s3_bucket_name)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n\n        # Register the Lambda function with that zip as the source\n        # You'll also need to define the path to your lambda_handler code.\n        lambda_arn = self.zappa.create_lambda_function(bucket=self.s3_bucket_name,\n                                                       s3_key=self.zip_path,\n                                                       function_name=self.lambda_name,\n                                                       handler='handler.lambda_handler',\n                                                       vpc_config=self.vpc_config,\n                                                       memory_size=self.memory_size,\n                                                       timeout=self.timeout)\n\n        # Create and configure the API Gateway\n        api_id = self.zappa.create_api_gateway_routes(\n            lambda_arn, self.lambda_name)\n\n        # Deploy the API!\n        endpoint_url = self.zappa.deploy_api_gateway(api_id, self.api_stage)\n\n        # Finally, delete the local copy our zip package\n        if self.zappa_settings[self.api_stage].get('delete_zip', True):\n            os.remove(self.zip_path)\n\n        #Remove the local settings\n        self.remove_s3_local_settings()\n        # Remove the uploaded zip from S3, because it is now registered..\n        self.zappa.remove_from_s3(self.zip_path, self.s3_bucket_name)\n\n        if self.zappa_settings[self.api_stage].get('touch', True):\n            requests.get(endpoint_url)\n\n        print(\"Your Zappa deployment is live!: \" + endpoint_url)\n\n        events = self.zappa_settings[self.api_stage].get('events')\n\n        if options['unschedule'] and events:\n            self.zappa.unschedule_events(lambda_arn, self.lambda_name, events)\n        elif options['unschedule'] and not events:\n            print(\"No Events to Unschedule\")\n\n        if options['schedule'] and events:\n            self.zappa.schedule_events(lambda_arn, self.lambda_name, events)\n        elif options['schedule'] and not events:\n            print(\"No Events to Schedule\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lambda_handler(event, context=None, settings_name=\"zappa_settings\"):  # NoQA\n    time_start = datetime.datetime.now()\n\n    # If in DEBUG mode, log all raw incoming events.\n    if settings.DEBUG:\n        logger.info('Zappa Event: {}'.format(event))\n\n    # This is a normal HTTP request\n    if event.get('method', None):\n\n        # Create the environment for WSGI and handle the request\n        environ = create_wsgi_request(event, script_name=settings.SCRIPT_NAME)\n\n        # We are always on https on Lambda, so tell our wsgi app that.\n        environ['HTTPS'] = 'on'\n        environ['wsgi.url_scheme'] = 'https'\n\n        wrap_me = get_wsgi_application()\n        app = ZappaWSGIMiddleware(wrap_me)\n\n        # Execute the application\n        response = Response.from_app(app, environ)\n        response.content = response.data\n\n        # Prepare the special dictionary which will be returned to the API GW.\n        returnme = {'Content': response.data}\n\n        # Pack the WSGI response into our special dictionary.\n        for (header_name, header_value) in response.headers:\n            returnme[header_name] = header_value\n        returnme['Status'] = response.status_code\n\n        # To ensure correct status codes, we need to\n        # pack the response as a deterministic B64 string and raise it\n        # as an error to match our APIGW regex.\n        # The DOCTYPE ensures that the page still renders in the browser.\n        exception = None\n        if response.status_code in ERROR_CODES:\n            content = u\"<!DOCTYPE html>\" + unicode(response.status_code) + unicode('<meta charset=\"utf-8\" />') + response.data.encode('utf-8')\n            b64_content = base64.b64encode(content)\n            exception = (b64_content)\n        # Internal are changed to become relative redirects\n        # so they still work for apps on raw APIGW and on a domain.\n        elif 300 <= response.status_code < 400 and response.has_header('Location'):\n            location = returnme['Location']\n            location = '/' + location.replace(\"http://zappa/\", \"\")\n            exception = location\n\n        # Calculate the total response time,\n        # and log it in the Common Log format.\n        time_end = datetime.datetime.now()\n        delta = time_end - time_start\n        response_time_ms = delta.total_seconds() * 1000\n        common_log(environ, response, response_time=response_time_ms)\n\n        # Finally, return the response to API Gateway.\n        if exception:\n            raise Exception(exception)\n        else:\n            return returnme\n\n    # This is a management command invocation.\n    elif event.get('command', None):\n        from django.core import management\n\n        # Couldn't figure out how to get the value into stdout with StringIO..\n        # Read the log for now. :[]\n        management.call_command(*event['command'].split(' '))\n        return {}\n    elif event.get('detail'):\n        module, function = event['detail'].rsplit('.', 1)\n\n        app_module = importlib.import_module(module)\n        app_function = getattr(app_module, function)\n\n        # Execute the function!\n        app_function()\n        return\n    else:\n        logger.error('Unhandled event: {}'.format(json.dumps(event)))", "response": "This function is called by the AWS Lambda function which parses the event and processes the Django response and returns that response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef require_settings(self, args, options):\n\n        if not options.has_key('environment'):\n            print(\n                \"You must call deploy with an environment name. \\n python manage.py deploy <environment>\")\n            raise ImproperlyConfigured\n\n        from django.conf import settings\n        if not 'ZAPPA_SETTINGS' in dir(settings):\n            print(\n                \"Please define your ZAPPA_SETTINGS in your settings file before deploying.\")\n            raise ImproperlyConfigured\n\n        self.zappa_settings = settings.ZAPPA_SETTINGS\n\n        # Set your configuration\n        if type(options['environment']) == list:\n            self.api_stage = options['environment'][0]\n        else:\n            self.api_stage = options['environment']\n        if self.zappa_settings[self.api_stage].get('project_name'):\n            self.project_name = self.zappa_settings[self.api_stage]['project_name']\n        else:\n            self.project_name = os.path.abspath(settings.BASE_DIR).split(os.sep)[-1]\n        self.lambda_name = slugify(self.project_name + '-' + self.api_stage).replace(\"_\",\"-\")\n        if self.api_stage not in self.zappa_settings.keys():\n            print(\"Please make sure that the environment '\" + self.api_stage +\n                  \"' is defined in your ZAPPA_SETTINGS in your settings file before deploying.\")\n            raise ImproperlyConfigured\n\n        # Load environment-specific settings\n        self.s3_bucket_name = self.zappa_settings[self.api_stage]['s3_bucket']\n        self.vpc_config = self.zappa_settings[\n            self.api_stage].get('vpc_config', {})\n        self.memory_size = self.zappa_settings[\n            self.api_stage].get('memory_size', 512)\n        self.timeout = self.zappa_settings[\n            self.api_stage].get('timeout', 30)\n\n\n        custom_settings = [\n            'http_methods',\n            'parameter_depth',\n            'integration_response_codes',\n            'method_response_codes',\n            'role_name',\n            'aws_region'\n        ]\n        for setting in custom_settings:\n            if self.zappa_settings[self.api_stage].has_key(setting):\n                setattr(self.zappa, setting, self.zappa_settings[\n                        self.api_stage][setting])", "response": "Load the ZAPPA_SETTINGS as we expect it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_from_s3(self,bucket_name,s3_key,\n                     output_filename='temp_zappa_settings.py'):\n        \"\"\"\n        Download a file from S3\n        :param bucket_name: Name of the S3 bucket (string)\n        :param s3_key: Name of the file hosted on S3 (string)\n        :param output_filename: Name of the file the download operation\n        will create (string)\n        :return: False or the value of output_filename\n        \"\"\"\n        s3 = boto3.resource('s3')\n        bucket = s3.Bucket(bucket_name)\n        try:\n            s3.meta.client.head_object(Bucket=bucket_name,Key=s3_key)\n        except botocore.exceptions.ClientError:\n            return False\n        print(u'Downloading the settings file ({0}) from S3'.format(s3_key))\n        new_file = bucket.download_file(s3_key,output_filename)\n        return output_filename", "response": "Download a file from S3"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the Lambda package and add the settings file to the Zappa package.", "response": "def create_package(self):\n        \"\"\"\n        Ensure that the package can be properly configured,\n        and then create it.\n\n        \"\"\"\n\n        # Create the Lambda zip package (includes project and virtualenvironment)\n        # Also define the path the handler file so it can be copied to the zip\n        # root for Lambda.\n        current_file = os.path.dirname(os.path.abspath(\n            inspect.getfile(inspect.currentframe())))\n        handler_file = os.sep.join(current_file.split(os.sep)[\n                                   0:-2]) + os.sep + 'handler.py'\n        exclude = self.zappa_settings[self.api_stage].get('exclude', []) + ['static', 'media']\n        self.zip_path = self.zappa.create_lambda_zip(\n                self.lambda_name,\n                handler_file=handler_file,\n                use_precompiled_packages=self.zappa_settings[self.api_stage].get('use_precompiled_packages', True),\n                exclude=exclude\n            )\n\n        # Add this environment's Django settings to that zipfile\n        with open(self.settings_file, 'r') as f:\n            contents = f.read()\n            all_contents = contents\n            if not self.zappa_settings[self.api_stage].has_key('domain'):\n                script_name = self.api_stage\n            else:\n                script_name = ''\n\n            all_contents = all_contents + \\\n                '\\n# Automatically added by Zappa:\\nSCRIPT_NAME=\\'/' + script_name + '\\'\\n'\n            f.close()\n\n        with open('zappa_settings.py', 'w') as f:\n            f.write(all_contents)\n\n        with zipfile.ZipFile(self.zip_path, 'a') as lambda_zip:\n            lambda_zip.write('zappa_settings.py', 'zappa_settings.py')\n            lambda_zip.close()\n\n        os.unlink('zappa_settings.py')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves our local zip file.", "response": "def remove_local_zip(self):\n        \"\"\"\n        Remove our local zip file.\n        \"\"\"\n\n        if self.zappa_settings[self.api_stage].get('delete_zip', True):\n            os.remove(self.zip_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the uploaded zip file from S3 and local copy.", "response": "def remove_uploaded_zip(self):\n        \"\"\"\n        Remove the local and S3 zip file after uploading and updating.\n        \"\"\"\n\n        # Remove the uploaded zip from S3, because it is now registered..\n        self.zappa.remove_from_s3(self.zip_path, self.s3_bucket_name)\n\n        # Finally, delete the local copy our zip package\n        self.remove_local_zip()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring this is an income account", "response": "def clean_account(self):\n        \"\"\"Ensure this is an income account\"\"\"\n        account = self.cleaned_data['account']\n        if not account:\n            return\n\n        if account.type != Account.TYPES.income:\n            raise ValidationError('Account must be an income account')\n\n        try:\n            account.housemate\n        except Housemate.DoesNotExist:\n            pass\n        else:\n            raise ValidationError('Account already has a housemate')\n\n        return account"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the amount due on the given billing cycle COOKIE", "response": "def get_amount_normal(self, billing_cycle):\n        \"\"\"Get the amount due on the given billing cycle\n\n        For regular recurring costs this is simply `fixed_amount`. For\n        one-off costs this is the portion of `fixed_amount` for the given\n        billing_cycle.\n        \"\"\"\n        if self.is_one_off():\n            billing_cycle_number = self._get_billing_cycle_number(billing_cycle)\n\n            if billing_cycle_number > self.total_billing_cycles:\n                # A future billing cycle after this one has ended\n                return Decimal('0')\n            else:\n                # This is a current cycle. Split the amount into\n                # equal parts then return the part for this cycle\n                splits = ratio_split(\n                    amount=self.fixed_amount,\n                    ratios=[Decimal('1')] * self.total_billing_cycles,\n                )\n                return splits[billing_cycle_number - 1]\n        else:\n            # This is a none-one-off recurring cost, so the logic is simple\n            return self.fixed_amount"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the balance of to_account at the end of billing_cycle", "response": "def get_amount_arrears_balance(self, billing_cycle):\n        \"\"\"Get the balance of to_account at the end of billing_cycle\"\"\"\n        return self.to_account.balance(\n            transaction__date__lt=billing_cycle.date_range.lower,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the sum of all transaction legs in to_account during given billing cycle", "response": "def get_amount_arrears_transactions(self, billing_cycle):\n        \"\"\"Get the sum of all transaction legs in to_account during given billing cycle\"\"\"\n        previous_billing_cycle = billing_cycle.get_previous()\n        if not previous_billing_cycle:\n            return Decimal(0)\n        return self.to_account.balance(\n            transaction__date__lt=previous_billing_cycle.date_range.upper,\n            transaction__date__gte=previous_billing_cycle.date_range.lower,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enact(self, billing_cycle, disable_if_done=True):\n        as_of = billing_cycle.date_range.lower\n        if not self.is_enactable(as_of):\n            raise CannotEnactUnenactableRecurringCostError(\n                \"RecurringCost {} is unenactable.\".format(self.uuid)\n            )\n\n        if self.has_enacted(billing_cycle):\n            raise RecurringCostAlreadyEnactedForBillingCycle(\n                'RecurringCost cost {} already enacted for {}'.format(self, billing_cycle)\n            )\n\n        with db_transaction.atomic():\n            recurred_cost = RecurredCost(\n                recurring_cost=self,\n                billing_cycle=billing_cycle,\n            )\n            recurred_cost.make_transaction()\n            recurred_cost.save()\n\n        if disable_if_done:\n            self.disable_if_done(billing_cycle)", "response": "Enact this RecurringCost for the given billing cycle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disable_if_done(self, commit=True):\n        if self._is_billing_complete() and not self.disabled:\n            self.disabled = True\n\n            if commit:\n                self.save()", "response": "Disable the related item if we have billed all we need to"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_enactable(self, as_of):\n        return \\\n            not self.disabled and \\\n            not self.archived and \\\n            not self._is_finished(as_of) and \\\n            self._is_ready(as_of) and \\\n            not self._is_billing_complete()", "response": "Is this RecurringCost enactable?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if this recurring cost already enacted transactions for given billing cycle.", "response": "def has_enacted(self, billing_cycle):\n        \"\"\"Has this recurring cost already enacted transactions for given billing cycle?\"\"\"\n        return RecurredCost.objects.filter(\n            recurring_cost=self,\n            billing_cycle=billing_cycle,\n        ).exists()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the RecurringCost is ready to be enacted as of the date as_of.", "response": "def _is_ready(self, as_of):\n        \"\"\"Is the RecurringCost ready to be enacted as of the date `as_of`\n\n        This determines if `as_of` precedes the start of `initial_billing_cycle`. If so,\n        we should not be enacting this RecurringCost yet.\n\n        Args:\n            as_of (Date):\n        \"\"\"\n        if self.is_one_off():\n            return self.initial_billing_cycle.date_range.lower <= as_of\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the billing cycle is finished.", "response": "def _is_finished(self, as_of):\n        \"\"\"Have the specified number of billing cycles been completed?\n\n        If so, we should not be enacting this RecurringCost.\n        \"\"\"\n        if self.is_one_off():\n            last_billing_cycle = self.get_billing_cycles()[self.total_billing_cycles - 1]\n            return last_billing_cycle.date_range.upper <= as_of\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the billing is complete.", "response": "def _is_billing_complete(self):\n        \"\"\"Has the specified `fixed_amount` been billed?\n\n        If so, we should not be enacting this RecurringCost.\n        \"\"\"\n        if self.is_one_off():\n            return self.get_billed_amount() >= Balance(self.fixed_amount, self.currency)\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_billing_cycle_number(self, billing_cycle):\n        begins_before_initial_date = billing_cycle.date_range.lower < self.initial_billing_cycle.date_range.lower\n        if begins_before_initial_date:\n            raise ProvidedBillingCycleBeginsBeforeInitialBillingCycle(\n                '{} precedes initial cycle {}'.format(billing_cycle, self.initial_billing_cycle)\n            )\n\n        billing_cycle_number = BillingCycle.objects.filter(\n            date_range__contained_by=DateRange(\n                self.initial_billing_cycle.date_range.lower,\n                billing_cycle.date_range.upper,\n                bounds='[]',\n            ),\n        ).count()\n\n        return billing_cycle_number", "response": "Gets the 1 - indexed number of the billing cycle relative to the provided billing cycle"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the value given by amount according to the RecurringCostSplit s portions attribute.", "response": "def split(self, amount):\n        \"\"\"Split the value given by amount according to the RecurringCostSplit's portions\n\n        Args:\n            amount (Decimal):\n\n        Returns:\n            list[(RecurringCostSplit, Decimal)]: A list with elements in the form (RecurringCostSplit, Decimal)\n        \"\"\"\n        split_objs = list(self.all())\n        if not split_objs:\n            raise NoSplitsFoundForRecurringCost()\n\n        portions = [split_obj.portion for split_obj in split_objs]\n\n        split_amounts = ratio_split(amount, portions)\n        return [\n            (split_objs[i], split_amount)\n            for i, split_amount\n            in enumerate(split_amounts)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new transaction for this RecurredCost.", "response": "def make_transaction(self):\n        \"\"\"Create the transaction for this RecurredCost\n\n        May only be used to create the RecurredCost's initial transaction.\n\n        Returns:\n            Transaction: The created transaction, also assigned to self.transaction. None if the amount is zero.\n        \"\"\"\n        if self.pk:\n            raise CannotRecreateTransactionOnRecurredCost(\n                'The transaction for this recurred cost has already been created. You cannot create it again.'\n            )\n\n        amount = self.recurring_cost.get_amount(self.billing_cycle)\n\n        # It is quite possible that there will be nothing to bill, in which\n        # case we cannot create a transaction with no legs, nor can we create\n        # legs with zero values. Therefore we don't create any transaction.\n        if not amount:\n            return None\n\n        self.transaction = Transaction.objects.create(\n            description='Created by recurring cost',\n            date=self.billing_cycle.date_range.lower\n        )\n\n        # Use the SplitManager's custom queryset's split() method to get the\n        # amount to be billed for each split\n        splits = self.recurring_cost.splits.all().split(amount)\n\n        # Create the transaction leg for the outbound funds\n        # (normally to an expense account)\n        self.transaction.legs.add(Leg.objects.create(\n            transaction=self.transaction,\n            amount=Money(amount, self.recurring_cost.currency),\n            account=self.recurring_cost.to_account,\n        ))\n\n        for split, split_amount in splits:\n            # Create the transaction legs for the inbound funds\n            # (from housemate accounts)\n            if split_amount:\n                self.transaction.legs.add(Leg.objects.create(\n                    transaction=self.transaction,\n                    amount=Money(split_amount * -1, self.recurring_cost.currency),\n                    account=split.from_account,\n                ))\n\n        return self.transaction"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef populate(cls, as_of=None):\n        return cls._populate(as_of=as_of or date.today(), delete=True)", "response": "Populate the cache with the next X years of billing cycles."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the table with billing cycles starting from as_of.", "response": "def _populate(cls, as_of=None, delete=False):\n        \"\"\"Populate the table with billing cycles starting from `as_of`\n\n        Args:\n            as_of (date): The date at which to begin the populating\n            delete (bool): Should future billing cycles be deleted?\n\n\n        \"\"\"\n        billing_cycle_helper = get_billing_cycle()\n        billing_cycles_exist = BillingCycle.objects.exists()\n\n        try:\n            current_billing_cycle = BillingCycle.objects.as_of(date=as_of)\n        except BillingCycle.DoesNotExist:\n            current_billing_cycle = None\n\n        # If no cycles exist then disable the deletion logic\n        if not billing_cycles_exist:\n            delete = False\n\n        # Cycles exist, but a date has been specified outside of them\n        if billing_cycles_exist and not current_billing_cycle:\n            raise CannotPopulateForDateOutsideExistingCycles()\n\n        # Omit the current billing cycle if we are deleting (as\n        # deleting the current billing cycle will be a Bad Idea)\n        omit_current = (current_billing_cycle and delete)\n\n        stop_date = as_of + relativedelta(years=settings.SWIFTWIND_BILLING_CYCLE_YEARS)\n        date_ranges = billing_cycle_helper.generate_date_ranges(as_of, stop_date=stop_date, omit_current=omit_current)\n        date_ranges = list(date_ranges)\n\n        beginning_date = date_ranges[0][0]\n\n        with db_transaction.atomic():\n\n            if delete:\n                # Delete all the future unused transactions\n                cls.objects.filter(start_date__gte=beginning_date).delete()\n\n            for start_date, end_date in date_ranges:\n                exists = BillingCycle.objects.filter(date_range=(start_date, end_date)).exists()\n                if exists:\n                    if delete:\n                        raise Exception(\n                            'It should not be possible to get here as future billing cycles have just been deleted'\n                        )\n                    else:\n                        # We're updating, so we can just ignore cycles that already exist\n                        pass\n                else:\n                    BillingCycle.objects.create(\n                        date_range=(start_date, end_date),\n                    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the next billing cycle. May return None", "response": "def get_next(self):\n        \"\"\"Get the billing cycle after this one. May return None\"\"\"\n        return BillingCycle.objects.filter(date_range__gt=self.date_range).order_by('date_range').first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_previous(self):\n        return BillingCycle.objects.filter(date_range__lt=self.date_range).order_by('date_range').last()", "response": "Get the billing cycle prior to this one. May return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_reconciled(self):\n        from hordak.models import StatementImport, StatementLine\n        since = datetime(\n            self.date_range.lower.year,\n            self.date_range.lower.month,\n            self.date_range.lower.day,\n            tzinfo=UTC\n        )\n        if not StatementImport.objects.filter(timestamp__gte=since).exists():\n            # No import done since the end of the above billing cycle, and reconciliation\n            # requires an import. Therefore reconciliation can not have been done\n            return False\n\n        if StatementLine.objects.filter(\n            transaction__isnull=True,\n            date__gte=self.date_range.lower,\n            date__lt=self.date_range.upper\n        ).exists():\n            # There are statement lines for this period which have not been reconciled\n            return False\n\n        return True", "response": "Returns True if transactions have been imported and reconciled for this billing cycle False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ignore_comments(lines_enum):\n    for line_number, line in lines_enum:\n        line = COMMENT_RE.sub('', line)\n        line = line.strip()\n        if line:\n            yield line_number, line", "response": "Strips comments and filter empty lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hash(filename, algorithm='sha256'):\n    if incompatible:\n        raise Incompatible\n\n    if algorithm not in ['sha256', 'sha384', 'sha512']:\n        raise InvalidArguments('Algorithm {} not supported'.format(algorithm))\n\n    result = call('hash', '--algorithm', algorithm, filename)\n\n    # result is of the form:\n    # <filename>:\\n--hash=<algorithm>:<hash>\\n\n    return result.strip().split(':')[-1]", "response": "Hash the given filename. Unavailable in pip<8. 0. 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef partition(list_, columns=2):\n\n    iter_ = iter(list_)\n    columns = int(columns)\n    rows = []\n\n    while True:\n        row = []\n        for column_number in range(1, columns + 1):\n            try:\n                value = six.next(iter_)\n            except StopIteration:\n                pass\n            else:\n                row.append(value)\n\n        if not row:\n            return rows\n        rows.append(row)", "response": "Break a list into columns number of columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the high level balances", "response": "def get_balance_context(self):\n        \"\"\"Get the high level balances\"\"\"\n        bank_account = Account.objects.get(name='Bank')\n\n        return dict(\n            bank=bank_account,\n            retained_earnings_accounts=Account.objects.filter(parent__name='Retained Earnings'),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_accounts_context(self):\n        income_parent = Account.objects.get(name='Income')\n        housemate_parent = Account.objects.get(name='Housemate Income')\n        expense_parent = Account.objects.get(name='Expenses')\n        current_liabilities_parent = Account.objects.get(name='Current Liabilities')\n        long_term_liabilities_parent = Account.objects.get(name='Long Term Liabilities')\n\n        return dict(\n            housemate_accounts=Account.objects.filter(parent=housemate_parent),\n            expense_accounts=expense_parent.get_descendants(),\n            current_liability_accounts=Account.objects.filter(parent=current_liabilities_parent),\n            long_term_liability_accounts=Account.objects.filter(parent=long_term_liabilities_parent),\n            other_income_accounts=Account.objects.filter(~Q(pk=housemate_parent.pk), parent=income_parent)\n        )", "response": "Get the accounts we may want to display"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a Pinyin syllable re pattern.", "response": "def _build_syl(vowels, tone_numbers=False):\n    \"\"\"Builds a Pinyin syllable re pattern.\n\n    Syllables can be preceded by a middle dot (tone mark). Syllables that end\n    in a consonant are only valid if they aren't followed directly by a vowel\n    with no apostrophe in between.\n\n    The rough approach used to validate a Pinyin syllable is:\n        1. Get the longest valid syllable.\n        2. If it ends in a consonant make sure it's not followed directly by a\n            vowel (hyphens and apostrophes don't count).\n        3. If the above didn't match, repeat for the next longest valid match.\n\n    Lookahead assertions are used to ensure that hyphens and apostrophes are\n    only considered valid if used correctly. This helps to weed out non-Pinyin\n    strings.\n\n    \"\"\"\n    # This is the end-of-syllable-consonant lookahead assertion.\n    consonant_end = '(?![{a}{e}{i}{o}{u}{v}]|u:)'.format(\n        a=_a, e=_e, i=_i, o=_o, u=_u, v=_v\n    )\n    _vowels = vowels.copy()\n    for v, s in _vowels.items():\n        if len(s) > 1:\n            _vowels[v] = '[{}]'.format(s)\n    return (\n        '(?:\\u00B7|\\u2027)?'\n        '(?:'\n        '(?:(?:[zcs]h|[gkh])u%(a)sng%(consonant_end)s)|'\n        '(?:[jqx]i%(o)sng%(consonant_end)s)|'\n        '(?:[nljqx]i%(a)sng%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[dtnlgkhrjqxy])u%(a)sn%(consonant_end)s)|'\n        '(?:(?:[zcs]h|[gkh])u%(a)si)|'\n        '(?:(?:[zc]h?|[rdtnlgkhsy])%(o)sng%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[rbpmfdtnlgkhw])?%(e)sng%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[rbpmfdtnlgkhwy])?%(a)sng%(consonant_end)s)|'\n        '(?:[bpmdtnljqxy]%(i)sng%(consonant_end)s)|'\n        '(?:[bpmdtnljqx]i%(a)sn%(consonant_end)s)|'\n        '(?:[bpmdtnljqx]i%(a)so)|'\n        '(?:[nl](?:v|u:|\\u00FC)%(e)s)|'\n        '(?:[nl](?:%(v)s|u:))|'\n        '(?:[jqxy]u%(e)s)|'\n        '(?:[bpmnljqxy]%(i)sn%(consonant_end)s)|'\n        '(?:[mdnljqx]i%(u)s)|'\n        '(?:[bpmdtnljqx]i%(e)s)|'\n        '(?:[dljqx]i%(a)s)|'\n        '(?:(?:[zcs]h?|[rdtnlgkhxqjy])%(u)sn%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[rdtgkh])u%(i)s)|'\n        '(?:(?:[zcs]h?|[rdtnlgkh])u%(o)s)|'\n        '(?:(?:[zcs]h|[rgkh])u%(a)s)|'\n        '(?:(?:[zcs]h?|[rbpmfdngkhw])?%(e)sn%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[rbpmfdtnlgkhwy])?%(a)sn%(consonant_end)s)|'\n        '(?:(?:[zcs]h?|[rpmfdtnlgkhy])?%(o)su)|'\n        '(?:(?:[zcs]h?|[rbpmdtnlgkhy])?%(a)so)|'\n        '(?:(?:[zs]h|[bpmfdtnlgkhwz])?%(e)si)|'\n        '(?:(?:[zcs]h?|[bpmdtnlgkhw])?%(a)si)|'\n        '(?:(?:[zcs]h?|[rjqxybpmdtnl])%(i)s)|'\n        '(?:(?:[zcs]h?|[rwbpmfdtnlgkhjqxwy])%(u)s)|'\n        '(?:%(e)s(?:r%(consonant_end)s)?)|'\n        '(?:(?:[zcs]h?|[rmdtnlgkhy])%(e)s)|'\n        '(?:[bpmfwyl]?%(o)s)|'\n        '(?:(?:[zcs]h|[bpmfdtnlgkhzcswy])?%(a)s)|'\n        '(?:r%(consonant_end)s)'\n        ')' + ('[0-5]?' if tone_numbers else '')\n    ) % {\n        'consonant_end': consonant_end, 'a': _vowels['a'], 'e': _vowels['e'],\n        'i': _vowels['i'], 'o': _vowels['o'], 'u': _vowels['u'],\n        'v': _vowels['v']\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a Pinyin word re pattern from a Pinyin syllable.", "response": "def _build_word(syl, vowels):\n    \"\"\"Builds a Pinyin word re pattern from a Pinyin syllable re pattern.\n\n    A word is defined as a series of consecutive valid Pinyin syllables\n    with optional hyphens and apostrophes interspersed. Hyphens must be\n    followed immediately by another valid Pinyin syllable. Apostrophes must be\n    followed by another valid Pinyin syllable that starts with an 'a', 'e', or\n    'o'.\n\n    \"\"\"\n    return \"(?:{syl}(?:-(?={syl})|'(?=[{a}{e}{o}])(?={syl}))?)+\".format(\n        syl=syl, a=vowels['a'], e=vowels['e'], o=vowels['o'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_sentence(word):\n    return (\n        \"(?:{word}|[{non_stops}]|(?<![{stops} ]) )+\"\n        \"[{stops}]['\\\"\\]\\}}\\)]*\"\n    ).format(word=word, non_stops=non_stops.replace('-', '\\-'),\n             stops=stops)", "response": "Builds a Pinyin sentence re pattern from a Pinyin word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the contents of the specified file and returns a list of lines.", "response": "def read_file(filename):\n   \"\"\"Read contents of the specified file.\n\n   Parameters:\n   -----------\n   filename : str\n      The name of the file to be read\n\n   Returns:\n   lines : list of str\n      The contents of the file, split by line\n\n   \"\"\"\n\n   infile = open(filename, 'r')\n   lines = infile.readlines()\n   infile.close()\n\n   return lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getFreeEnergyDifferences(self, compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n\n        # Compute free energy differences.\n        f_i = np.matrix(self.f_k)\n        Deltaf_ij = f_i - f_i.transpose()\n\n        # zero out numerical error for thermodynamically identical states\n        self._zerosamestates(Deltaf_ij)\n\n        returns = []\n        returns.append(np.array(Deltaf_ij))\n\n        if compute_uncertainty or return_theta:\n            # Compute asymptotic covariance matrix.\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                np.exp(self.Log_W_nk), self.N_k, method=uncertainty_method)\n\n        if compute_uncertainty:\n            # compute the covariance component without doing the double loop.\n            # d2DeltaF = Theta_ij[i,i] + Theta_ij[j,j] - 2.0 * Theta_ij[i,j]\n\n            diag = Theta_ij.diagonal()\n            d2DeltaF = diag + diag.transpose() - 2 * Theta_ij\n\n            # zero out numerical error for thermodynamically identical states\n            self._zerosamestates(d2DeltaF)\n\n            # check for any numbers below zero.\n            if (np.any(d2DeltaF < 0.0)):\n                if(np.any(d2DeltaF) < warning_cutoff):\n                    # Hmm.  Will this print correctly?\n                    print(\"A squared uncertainty is negative.  d2DeltaF = %e\" % d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)])\n                else:\n                    d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)] = 0.0\n\n            # take the square root of the entries of the matrix\n            dDeltaf_ij = np.sqrt(d2DeltaF)\n\n            # Return matrix of free energy differences and uncertainties.\n            returns.append(np.array(dDeltaf_ij))\n\n        if (return_theta):\n            returns.append(np.array(Theta_ij))\n\n        return returns", "response": "Returns the dimensionless free energy differences and uncertainties among all thermodynamic states."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeGeneralExpectations(self, A_in, u_ln, state_list, compute_uncertainty=True,\n                                   uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n        \"\"\"Compute the expectations of multiple observables of phase space functions on multiple states.\n\n        Compute the expectations of multiple observables of phase\n        space functions.  [A_0(x),A_1(x),...,A_i(x)] along with the\n        covariances of their estimates at multiple states.\n\n        We calculate all observables at all states that are specified by the choice of state list.\n        Generally, it will be called in specific conditions.\n\n        Parameters\n        ----------\n        A_in : np.ndarray, float, shape=(I, N)\n            A_in[i,n] = A_i(x_n), the value of phase observable i for configuration n\n        u_ln : np.ndarray, float, shape=(L, N)\n            u_n[l,n] is the reduced potential of configuration n at state l\n            if u_ln = None, we use self.u_kn\n\n        state_list : np.ndarray, int, shape (NS,2), where NS is the\n                     total number of states of interest.  it will be\n                     of the form [[0,0],[1,1],[2,1]] which would\n                     indicate we want to output the properties of\n                     three observables total: the first property A[0]\n                     at the 0th state, the 2nd property A[1] at the\n                     1th state, and the 2nd property A[1] at the 2nd\n                     state.  This allows us to tailor this to a number of different situations.\n\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n        return_theta : bool, optional\n            Whether or not to return the theta matrix.  Can be useful for complicated differences.\n\n        Returns\n        -------\n\n        A_i : np.ndarray, float, shape = (I)\n            A_i[i] is the estimate for the expectation of A_state_list[i](x) at the state specified by u_n[state_list[i],:]\n\n        d2A_ik : np.ndarray, float, shape = (I, J)\n            d2A_ij[i,j] is the COVARIANCE in the estimates of observables A_i and A_j (as determined by the state list)\n            (* not the square root *)\n\n        General cases this will be used for.\n            single observable, multiple states (replacement for computeExpectations)\n            multiple observables, single state (replacement for computeMultipleExpectations)\n            diagonal cases of multiple states, single states.\n\n        Examples\n        --------\n\n        update this example to be more general\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> A_in = np.array([x_n,x_n**2,x_n**3])\n        >>> u_n = u_kn[:2,:]\n        >>> state_list = np.array([[0,0],[1,0],[2,0],[2,1]],int)\n        >>> [A_i, d2A_ij] = mbar.computeGeneralExpectations(A_in, u_n, state_list)\n\n        \"\"\"\n\n        # Retrieve N and K for convenience.\n        S = len(state_list) # number of computed expectations examined\n        K = self.K\n\n        N = self.N  # N is total number of samples\n\n        # make observables all positive\n        A_list = np.unique(state_list[:,0])\n        I = len(A_list) # number of observables used\n        A_min = np.zeros([I], dtype=np.float64)\n\n        for i in A_list:  # only need to zero the ones we will use.  May be some repetition here.\n            A_min[i] = np.min(A_in[i, :]) #find the minimum\n            A_in[i, :] = A_in[i,:] - (A_min[i] - 1)  #all values now positive so that we can work in logarithmic scale\n\n        # Augment W_nk, N_k, and c_k for q_A(x) for the observables, with one\n        # row for the specified state and I rows for the observable at that\n        # state.\n        # log weight matrix\n        sizea = K + 2*S # augmented size\n        Log_W_nk = np.zeros([N, sizea], np.float64) # log weight matrix\n        N_k = np.zeros([sizea], np.int32)  # counts\n        f_k = np.zeros([sizea], np.float64)  # free energies\n\n        # Fill in first section of matrix with existing q_k(x) from states.\n        Log_W_nk[:, 0:K] = self.Log_W_nk\n        N_k[0:K] = self.N_k\n        f_k[0:K] = self.f_k\n\n        # Compute row of W matrix for the extra states corresponding to u_ln according to the state list\n        for s in range(S):\n            l = state_list[s,1]\n            la = K+s  #l augmented\n            Log_W_nk[:, la] = self._computeUnnormalizedLogWeights(u_ln[l,:])\n            f_k[la] = -_logsum(Log_W_nk[:, la])\n            Log_W_nk[:, la] += f_k[l]\n\n        # Compute the remaining rows/columns of W_nk and c_k for the\n        # observables.\n        for s in range(S):\n            sa = K+S+s  # augmented s\n            i = state_list[s,0]\n            l = state_list[s,1]\n            Log_W_nk[:, sa] = np.log(A_in[i, :]) + Log_W_nk[:, K+l]\n            f_k[sa] = -_logsum(Log_W_nk[:, sa])\n            Log_W_nk[:, sa] += f_k[sa]    # normalize this row\n\n        # Compute estimates.\n        A_i = np.zeros([S], np.float64)\n        for s in range(S):\n            A_i[s] = np.exp(-f_k[K + S + s])\n\n        if compute_uncertainty or return_theta:\n            # Compute augmented asymptotic covariance matrix.\n            W_nk = np.exp(Log_W_nk)\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                W_nk, N_k, method=uncertainty_method)\n\n        if compute_uncertainty:\n            # Compute estimates of statistical covariance\n            # these variances will be the same whether or not we subtract a different constant from each A_i\n            # todo: vectorize\n            # compute the covariance component without doing the double loop\n            d2A_ij = np.zeros([S, S], np.float64)\n            for i in range(S):\n                si = K+S+i\n                li = K+state_list[i,1]\n                for j in range(S):\n                    sj = K+S+j\n                    lj = K+state_list[j,1]\n                    d2A_ij[i, j] = A_i[i] * A_i[j] * (\n                        Theta_ij[si, sj] - Theta_ij[si, li] - Theta_ij[lj, sj] + Theta_ij[li, lj])\n\n        # Now that covariances are computed, add the constants back to A_i that\n        # were required to enforce positivity\n        for s in range(S):\n            A_i[s] += (A_min[state_list[s,0]] - 1)\n\n        # these values may be used outside the routine, so copy back.\n        for i in A_list:\n            A_in[i, :] = A_in[i,:] + (A_min[i] - 1)\n\n        returns = []\n        returns.append(A_i)\n\n        if compute_uncertainty:\n            returns.append(d2A_ij)\n\n        if return_theta:\n            returns.append(Theta_ij)\n\n        # Return expectations and uncertainties.\n        return returns", "response": "This method computes the expectations of multiple observables of phase space functions on multiple states."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeExpectations(self, A_n, output='averages', compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False, useGeneral = False, state_dependent = False):\n\n        dims = len(np.shape(A_n))\n\n        # Retrieve N and K for convenience.\n        N = self.N\n        K = self.K\n\n        if dims == 3:\n            print(\"expecting dim=1 or dim=2\")\n            return None\n\n        if (useGeneral):\n\n            state_list = np.zeros([K,2],int)\n            if (state_dependent):\n                for k in range(K):\n                    state_list[k,0] = k\n                    state_list[k,1] = k\n                A_in = A_n\n            else:\n                A_in = np.zeros([1,N], dtype=np.float64)\n                if dims == 2:\n                    A_n = kn_to_n(A_n, N_k=self.N_k)\n                A_in[0,:] = A_n\n\n                for k in range(K):\n                    state_list[k,0] = 0\n                    state_list[k,1] = k\n\n            general_results = self.computeGeneralExpectations(A_in, self.u_kn, state_list,\n                                                              compute_uncertainty=compute_uncertainty,\n                                                              uncertainty_method=uncertainty_method,\n                                                              warning_cutoff=warning_cutoff,\n                                                              return_theta=return_theta)\n            returns = []\n\n            if output == 'averages':\n                # Return expectations and uncertainties.\n                returns.append(general_results[0])\n\n                if compute_uncertainty:\n                    indices = np.eye(K,dtype=bool)\n                    returns.append(np.sqrt(general_results[1][indices]))\n\n            if output == 'differences':\n                A_im = np.matrix(general_results[0])\n                A_ij = A_im - A_im.transpose()\n\n                returns.append(np.array(A_ij))\n                if compute_uncertainty:\n                    return np.sqrt(general_results[1])\n\n            if return_theta:\n                returns.append(general_results[2])\n\n        else:  # current style\n\n            if dims == 2: #convert to 1xN shape\n                A_n = kn_to_n(A_n, N_k=self.N_k)\n                # Convert to np array.\n                A_n = np.array(A_n, np.float64)\n\n            # Augment W_nk, N_k, and c_k for q_A(x) for the observable, with one\n            # extra row/column for each state (Eq. 13 of [1]).\n            # log of weight matrix\n            Log_W_nk = np.zeros([N, K * 2], np.float64)\n            N_k = np.zeros([K * 2], np.int32)  # counts\n            # \"free energies\" of the new states\n            f_k = np.zeros([K], np.float64)\n\n            # Fill in first half of matrix with existing q_k(x) from states.\n            Log_W_nk[:, 0:K] = self.Log_W_nk\n            N_k[0:K] = self.N_k\n\n            # Make A_n all positive so we can operate logarithmically for\n            # robustness\n            A_i = np.zeros([K], np.float64)\n            A_min = np.min(A_n)\n            A_n = A_n - (A_min - 1)\n\n            # Compute the remaining rows/columns of W_nk and the rows c_k for the\n            # observables.\n\n            for l in range(K):\n\n                # this works because all A_n are now positive;\n                Log_W_nk[:, K + l] = np.log(A_n) + self.Log_W_nk[:, l]\n                # we took the min at the beginning.\n                f_k[l] = -_logsum(Log_W_nk[:, K + l])\n                Log_W_nk[:, K + l] += f_k[l]              # normalize the row\n                A_i[l] = np.exp(-f_k[l])\n\n            if compute_uncertainty or return_theta:\n                # Compute augmented asymptotic covariance matrix.\n                Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                    np.exp(Log_W_nk), N_k, method=uncertainty_method)\n\n            returns = []\n\n            if output == 'averages':\n\n                if compute_uncertainty:\n                    # Compute uncertainties.\n                    dA_i = np.zeros([K], np.float64)\n                # just the diagonals\n                    for k in range(0, K):\n                        dA_i[k] = np.abs(A_i[k]) * np.sqrt(\n                            Theta_ij[K + k, K + k] + Theta_ij[k, k] - 2.0 * Theta_ij[k, K + k])\n\n                # add back minima now now that uncertainties are computed.\n                    A_i += (A_min - 1)\n\n                # Return expectations and uncertainties.\n                returns.append(np.array(A_i))\n\n                if compute_uncertainty:\n                    returns.append(np.array(dA_i))\n\n            if output == 'differences':\n                # Return differences of expectations and uncertainties.\n\n                # compute expectation differences\n                A_im = np.matrix(A_i)\n                A_ij = A_im - A_im.transpose()\n\n                returns.append(np.array(A_ij))\n\n                # todo - vectorize the differences!  Faster and less likely to give errors.\n\n                if compute_uncertainty:\n                    dA_ij = np.zeros([K, K], dtype=np.float64)\n\n                    for i in range(0, K):\n                        for j in range(0, K):\n                            try:\n                                dA_ij[i, j] = np.sqrt(\n                                    + A_i[i] * Theta_ij[i, i] * A_i[i]\n                                      - A_i[i] * Theta_ij[i, j] * A_i[j]\n                                      - A_i[i] * Theta_ij[i, K + i] * A_i[i]\n                                      + A_i[i] * Theta_ij[i, K + j] * A_i[j]\n                                      - A_i[j] * Theta_ij[j, i] * A_i[i]\n                                      + A_i[j] * Theta_ij[j, j] * A_i[j]\n                                      + A_i[j] * Theta_ij[j, K + i] * A_i[i]\n                                      - A_i[j] * Theta_ij[j, K + j] * A_i[j]\n                                      - A_i[i] * Theta_ij[K + i, i] * A_i[i]\n                                      + A_i[i] * Theta_ij[K + i, j] * A_i[j]\n                                      + A_i[i] * Theta_ij[K + i, K + i] * A_i[i]\n                                      - A_i[i] * Theta_ij[K + i, K + j] * A_i[j]\n                                      + A_i[j] * Theta_ij[K + j, i] * A_i[i]\n                                      - A_i[j] * Theta_ij[K + j, j] * A_i[j]\n                                      - A_i[j] * Theta_ij[K + j, K + i] * A_i[i]\n                                      + A_i[j] * Theta_ij[K + j, K + j] * A_i[j]\n                                      )\n                            except:\n                                dA_ij[i, j] = 0.0\n\n                    returns.append(dA_ij)\n\n                if return_theta:\n                    returns.append(Theta_ij)\n\n        return returns", "response": "Compute the expectation of an observable of a phase space function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef computeMultipleExpectations(self, A_in, u_n, compute_uncertainty=True,\n                                    uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n        \"\"\"Compute the expectations of multiple observables of phase space functions.\n\n        Compute the expectations of multiple observables of phase space functions.\n        [A_0(x),A_1(x),...,A_i(x)] along with the covariances of their estimates.  The state is specified by\n        the choice of u_n, which is the energy of the n samples evaluated at a the chosen state.\n\n        Parameters\n        ----------\n        A_in : np.ndarray, float, shape=(I, k, N)\n            A_in[i,n] = A_i(x_n), the value of phase observable i for configuration n at state of interest\n        u_n : np.ndarray, float, shape=(N)\n            u_n[n] is the reduced potential of configuration n at the state of interest\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n        return_theta : bool, optional\n            Whether or not to return the theta matrix.  Can be useful for complicated differences.\n\n        Returns\n        -------\n\n        A_i : np.ndarray, float, shape=(I)\n            A_i[i] is the estimate for the expectation of A_i(x) at the state specified by u_kn\n        d2A_ij : np.ndarray, float, shape=(I, I)\n            d2A_ij[i,j] is the COVARIANCE in the estimates of A_i[i] and A_i[j],\n            not the square root of the covariance\n\n        Notes\n        -----\n        Not fully tested!\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> A_in = np.array([x_n,x_n**2,x_n**3])\n        >>> u_n = u_kn[0,:]\n        >>> [A_i, d2A_ij] = mbar.computeMultipleExpectations(A_in, u_kn)\n\n        \"\"\"\n\n        # Retrieve N and K for convenience.\n        I = A_in.shape[0]  # number of observables\n        K = self.K\n        N = self.N  # N is total number of samples\n\n        if len(np.shape(A_in)) == 3:\n            A_in_old = A_in.copy()\n            A_in = np.zeros([I, N], np.float64)\n            for i in range(I):\n                A_in[i,:] = kn_to_n(A_in_old[i, :, :], N_k=self.N_k)\n\n        A_min = np.zeros([I], dtype=np.float64)\n        for i in range(I):\n            A_min[i] = np.min(A_in[i, :]) #find the minimum\n            A_in[i, :] -= (A_min[i]-1)  #all now values will be positive so that we can work in logarithmic scale\n\n        if len(np.shape(u_n)) == 2:\n            u_n = kn_to_n(u_n, N_k=self.N_k)\n\n        # Augment W_nk, N_k, and c_k for q_A(x) for the observables, with one\n        # row for the specified state and I rows for the observable at that\n        # state.\n        # log weight matrix\n        Log_W_nk = np.zeros([N, K + 1 + I], np.float64)\n        W_nk = np.zeros([N, K + 1 + I], np.float64)  # weight matrix\n        N_k = np.zeros([K + 1 + I], np.int32)  # counts\n        f_k = np.zeros([K + 1 + I], np.float64)  # free energies\n\n        # Fill in first section of matrix with existing q_k(x) from states.\n        Log_W_nk[:, 0:K] = self.Log_W_nk\n        W_nk[:, 0:K] = np.exp(self.Log_W_nk)\n        N_k[0:K] = self.N_k\n        f_k[0:K] = self.f_k\n\n        # Compute row of W matrix for the extra state corresponding to u_kn.\n        Log_W_nk[:, K]  = self._computeUnnormalizedLogWeights(u_n)\n        f_k[K] = -_logsum(Log_W_nk[:, K])\n        Log_W_nk[:, K] += f_k[K]\n\n        # Compute the remaining rows/columns of W_nk and c_k for the\n        # observables.\n        for i in range(I):\n            Log_W_nk[:, K+1+i] = np.log(A_in[i, :]) + Log_W_nk[:, K]\n            f_k[K + 1 + i] = -_logsum(Log_W_nk[:, K + 1 + i])\n            Log_W_nk[:, K + 1 + i] += f_k[K + 1 + i]    # normalize this row\n\n        # Compute estimates.\n        A_i = np.zeros([I], np.float64)\n        for i in range(I):\n            A_i[i] = np.exp(-f_k[K + 1 + i])\n\n        if compute_uncertainty or return_theta:\n            # Compute augmented asymptotic covariance matrix.\n            W_nk = np.exp(Log_W_nk)\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                W_nk, N_k, method=uncertainty_method)\n\n        if compute_uncertainty:\n            # Compute estimates of statistical covariance\n            # these variances will be the same whether or not we subtract a different constant from each A_i\n            # todo: vectorize\n            # compute the covariance component without doing the double loop\n            d2A_ij = np.zeros([I, I], np.float64)\n            for i in range(I):\n                for j in range(I):\n                    d2A_ij[i, j] = A_i[i] * A_i[j] * (Theta_ij[K + 1 + i, K + 1 + j] - Theta_ij[\n                                                      K + 1 + i, K] - Theta_ij[K, K + 1 + j] + Theta_ij[K, K])\n\n        # Now that covariances are computed, add the constants back to A_i that\n        # were required to enforce positivity\n        A_i = A_i + (A_min - 1)\n\n        returns = []\n        returns.append(A_i)\n\n        if compute_uncertainty:\n            returns.append(d2A_ij)\n\n        if return_theta:\n            returns.append(Theta_ij)\n\n        # Return expectations and uncertainties.\n        return returns", "response": "Compute the expectations of multiple observables of phase space functions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef computeOverlap(self, output='scalar'):\n\n        W = np.matrix(self.getWeights(), np.float64)\n        O = np.multiply(self.N_k, W.T * W)\n        (eigenval, eigevec) = linalg.eig(O)\n        # sort in descending order\n        eigenval = np.sort(eigenval)[::-1]\n        overlap_scalar = 1 - eigenval[1]\n        if (output == 'scalar'):\n            return overlap_scalar\n        elif (output == 'eigenvalues'):\n            return eigenval\n        elif (output == 'matrix'):\n            return O\n        elif (output == 'all'):\n            return overlap_scalar, eigenval, O", "response": "Compute the estimate of overlap between the states."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef computePerturbedExpectation(self, u_n, A_n, compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n\n        if len(np.shape(u_n)) == 2:\n            u_n = kn_to_n(u_n, N_k=self.N_k)\n\n        if len(np.shape(A_n)) == 2:\n            A_n = kn_to_n(A_n, N_k=self.N_k)\n\n        # Convert to np matrix.\n        A_n = np.array(A_n, dtype=np.float64)\n\n        # Retrieve N and K for convenience.\n        N = self.N\n        K = self.K\n\n        # Make A_k all positive so we can operate logarithmically for\n        # robustness\n        A_min = np.min(A_n)\n        A_n = A_n - (A_min - 1)\n\n        # Augment W_nk, N_k, and c_k for q_A(x) for the observable, with one\n        # extra row/column for the specified state (Eq. 13 of [1]).\n        # weight matrix\n        Log_W_nk = np.zeros([N, K + 2], dtype=np.float64)\n        N_k = np.zeros([K + 2], dtype=np.int32)  # counts\n        f_k = np.zeros([K + 2], dtype=np.float64)  # free energies\n\n        # Fill in first K states with existing q_k(x) from states.\n        Log_W_nk[:, 0:K] = self.Log_W_nk\n        N_k[0:K] = self.N_k\n\n        # compute the free energy of the additional state\n        log_w_n = self._computeUnnormalizedLogWeights(u_n)\n        # Compute free energies\n        f_k[K] = -_logsum(log_w_n)\n        Log_W_nk[:, K] = log_w_n + f_k[K]\n\n        # compute the observable at this state\n        Log_W_nk[:, K + 1] = np.log(A_n) + Log_W_nk[:, K]\n        f_k[K + 1] = -_logsum(Log_W_nk[:, K + 1])\n        Log_W_nk[:, K + 1] += f_k[K + 1]              # normalize the row\n        A = np.exp(-f_k[K + 1])\n\n        if (compute_uncertainty or return_theta):\n            # Compute augmented asymptotic covariance matrix.\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                np.exp(Log_W_nk), N_k, method=uncertainty_method)\n\n        if (compute_uncertainty):\n            dA = np.abs(A) * np.sqrt(\n                Theta_ij[K + 1, K + 1] + Theta_ij[K, K] - 2.0 * Theta_ij[K, K + 1])  # Eq. 16 of [1]\n\n        # shift answers back with the offset now that variances are computed\n        A += (A_min - 1)\n\n        returns = []\n        returns.append(A)\n\n        if (compute_uncertainty):\n            returns.append(dA)\n\n        if (return_theta):\n            returns.append(Theta_ij)\n\n        # Return expectations and uncertainties.\n        return returns", "response": "Compute the expectation of an observable of phase space function A for a single new state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the free energies for a new set of states. This is a simple method that computes the free energies for a new set of states.", "response": "def computePerturbedFreeEnergies(self, u_ln, compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n        \"\"\"Compute the free energies for a new set of states.\n\n        Here, we desire the free energy differences among a set of new states, as well as the uncertainty estimates in these differences.\n\n        Parameters\n        ----------\n        u_ln : np.ndarray, float, shape=(L, Nmax)\n            u_ln[l,n] is the reduced potential energy of uncorrelated\n            configuration n evaluated at new state k.  Can be completely indepednent of the original number of states.\n        compute_uncertainty : bool, optional\n            If False, the uncertainties will not be computed (default: True)\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        Returns\n        -------\n        Deltaf_ij : np.ndarray, float, shape=(L, L)\n            Deltaf_ij[i,j] = f_j - f_i, the dimensionless free energy difference between new states i and j\n        dDeltaf_ij : np.ndarray, float, shape=(L, L)\n            dDeltaf_ij[i,j] is the estimated statistical uncertainty in Deltaf_ij[i,j]\n\n        Examples\n        --------\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> [Deltaf_ij, dDeltaf_ij] = mbar.computePerturbedFreeEnergies(u_kn)\n        \"\"\"\n\n        # Convert to np matrix.\n        u_ln = np.array(u_ln, dtype=np.float64)\n\n        # Get the dimensions of the matrix of reduced potential energies, and convert if necessary\n        if len(np.shape(u_ln)) == 3:\n            u_ln = kln_to_kn(u_ln, N_k=self.N_k)\n\n        [L, N] = u_ln.shape\n\n        # Check dimensions.\n        if (N < self.N):\n            raise DataError(\"There seems to be too few samples in u_kn. You must evaluate at the new potential with all of the samples used originally.\")\n\n        # Retrieve N and K for convenience.\n        N = self.N\n        K = self.K\n\n        # Augment W_nk, N_k, and c_k for the new states.\n        W_nk = np.zeros([N, K + L], dtype=np.float64)  # weight matrix\n        N_k = np.zeros([K + L], dtype=np.int32)  # counts\n        f_k = np.zeros([K + L], dtype=np.float64)  # free energies\n\n        # Fill in first half of matrix with existing q_k(x) from states.\n        W_nk[:, 0:K] = np.exp(self.Log_W_nk)\n        N_k[0:K] = self.N_k\n        f_k[0:K] = self.f_k\n\n        # Compute normalized weights.\n        for l in range(0, L):\n            # Compute unnormalized log weights.\n            log_w_n = self._computeUnnormalizedLogWeights(u_ln[l, :])\n            # Compute free energies\n            f_k[K + l] = - _logsum(log_w_n)\n            # Store normalized weights.  Keep in exponential not log form\n            # because we will not store W_nk\n            W_nk[:, K + l] = np.exp(log_w_n + f_k[K + l])\n\n        if (compute_uncertainty or return_theta):\n            # Compute augmented asymptotic covariance matrix.\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                W_nk, N_k, method = uncertainty_method)\n\n        # Compute matrix of free energy differences between states and\n        # associated uncertainties.\n        # makes matrix operations easier to recast\n        f_k = np.matrix(f_k[K:K + L])\n\n        Deltaf_ij = f_k - f_k.transpose()\n\n        returns = []\n        returns.append(Deltaf_ij)\n\n        if (compute_uncertainty):\n            diag = Theta_ij.diagonal()\n            dii = diag[0, K:K + L]\n            d2DeltaF = dii + dii.transpose() - 2 * Theta_ij[K:K + L, K:K + L]\n\n            # check for any numbers below zero.\n            if (np.any(d2DeltaF < 0.0)):\n                if(np.any(d2DeltaF) < warning_cutoff):\n                    print(\"A squared uncertainty is negative.  d2DeltaF = %e\" % d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)])\n                else:\n                    d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)] = 0.0\n\n            # take the square root of entries of the matrix\n            dDeltaf_ij = np.sqrt(d2DeltaF)\n\n            returns.append(dDeltaf_ij)\n\n        if (return_theta):\n            returns.append(Theta_ij)\n\n        # Return matrix of free energy differences and uncertainties.\n        return returns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecompose free energy differences into enthalpy and entropy differences. Compute the decomposition of the free energy difference between states 1 and N into reduced free energy differences, reduced potential (enthalpy) differences, and reduced entropy (S/k) differences. Parameters ---------- uncertainty_method : string , optional Choice of method used to compute asymptotic covariance method, or None to use default See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None) warning_cutoff : float, optional Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10) Returns ------- Delta_f_ij : np.ndarray, float, shape=(K, K) Delta_f_ij[i,j] is the dimensionless free energy difference f_j - f_i dDelta_f_ij : np.ndarray, float, shape=(K, K) uncertainty in Delta_f_ij Delta_u_ij : np.ndarray, float, shape=(K, K) Delta_u_ij[i,j] is the reduced potential energy difference u_j - u_i dDelta_u_ij : np.ndarray, float, shape=(K, K) uncertainty in Delta_f_ij Delta_s_ij : np.ndarray, float, shape=(K, K) Delta_s_ij[i,j] is the reduced entropy difference S/k between states i and j (s_j - s_i) dDelta_s_ij : np.ndarray, float, shape=(K, K) uncertainty in Delta_s_ij Notes ----- This method is EXPERIMENTAL and should be used at your own risk. Examples -------- >>> from pymbar import testsystems >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn') >>> mbar = MBAR(u_kn, N_k) >>> [Delta_f_ij, dDelta_f_ij, Delta_u_ij, dDelta_u_ij, Delta_s_ij, dDelta_s_ij] = mbar.computeEntropyAndEnthalpy()", "response": "def computeEntropyAndEnthalpy(self, uncertainty_method=None, verbose=False, warning_cutoff=1.0e-10):\n        \"\"\"Decompose free energy differences into enthalpy and entropy differences.\n\n        Compute the decomposition of the free energy difference between\n        states 1 and N into reduced free energy differences, reduced potential\n        (enthalpy) differences, and reduced entropy (S/k) differences.\n\n        Parameters\n        ----------\n        uncertainty_method : string , optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        Returns\n        -------\n        Delta_f_ij : np.ndarray, float, shape=(K, K)\n            Delta_f_ij[i,j] is the dimensionless free energy difference f_j - f_i\n        dDelta_f_ij : np.ndarray, float, shape=(K, K)\n            uncertainty in Delta_f_ij\n        Delta_u_ij : np.ndarray, float, shape=(K, K)\n            Delta_u_ij[i,j] is the reduced potential energy difference u_j - u_i\n        dDelta_u_ij : np.ndarray, float, shape=(K, K)\n            uncertainty in Delta_f_ij\n        Delta_s_ij : np.ndarray, float, shape=(K, K)\n            Delta_s_ij[i,j] is the reduced entropy difference S/k between states i and j (s_j - s_i)\n        dDelta_s_ij : np.ndarray, float, shape=(K, K)\n            uncertainty in Delta_s_ij\n\n        Notes\n        -----\n        This method is EXPERIMENTAL and should be used at your own risk.\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> [Delta_f_ij, dDelta_f_ij, Delta_u_ij, dDelta_u_ij, Delta_s_ij, dDelta_s_ij] = mbar.computeEntropyAndEnthalpy()\n\n        \"\"\"\n\n        if verbose:\n            print(\"Computing average energy and entropy by MBAR.\")\n\n        # Retrieve N and K for convenience.\n        N = self.N\n        K = self.K\n\n        # Augment W_nk, N_k, and c_k for q_A(x) for the potential energies,\n        # with one extra row/column for each state.\n        # weight matrix\n        Log_W_nk = np.zeros([N, K * 2], dtype=np.float64)\n        N_k = np.zeros([K * 2], dtype=np.int32)  # counts\n        # \"free energies\" of average states\n        f_k = np.zeros(K, dtype=np.float64)\n\n        # Fill in first half of matrix with existing q_k(x) from states.\n        Log_W_nk[:, 0:K] = self.Log_W_nk\n        N_k[0:K] = self.N_k\n\n        # Compute the remaining rows/columns of W_nk and c_k for the potential\n        # energy observable.\n\n        u_min = self.u_kn.min()\n        u_i = np.zeros([K], dtype=np.float64)\n        for l in range(0, K):\n            u_kn = self.u_kn[l, :] - (u_min-1)  # all positive now!  Subtracting off arbitrary constants doesn't affect results\n            # since they are all differences.\n            # Compute unnormalized weights.\n            # A(x_n) exp[f_{k} - q_{k}(x_n)] / \\sum_{k'=1}^K N_{k'} exp[f_{k'} - q_{k'}(x_n)]\n            # harden for over/underflow with logarithms\n\n            Log_W_nk[:, K + l] = np.log(u_kn) + self.Log_W_nk[:, l]\n\n            f_k[l] = -_logsum(Log_W_nk[:, K + l])\n            Log_W_nk[:, K + l] += f_k[l]              # normalize the row\n            u_i[l] = np.exp(-f_k[l])\n\n            # print \"MBAR u_i[%d]: %10.5f,%10.5f\" % (l,u_i[l]+u_min, u_i[l])\n\n        # Compute augmented asymptotic covariance matrix.\n        W_nk = np.exp(Log_W_nk)\n        Theta_ij = self._computeAsymptoticCovarianceMatrix(\n            W_nk, N_k, method=uncertainty_method)\n\n        # Compute estimators and uncertainties.\n        dDelta_f_ij = np.zeros([K, K], dtype=np.float64)\n        dDelta_u_ij = np.zeros([K, K], dtype=np.float64)\n        dDelta_s_ij = np.zeros([K, K], dtype=np.float64)\n\n        # Compute reduced free energy difference.\n        f_k = np.matrix(self.f_k)\n        Delta_f_ij = f_k - f_k.transpose()\n\n        # Compute reduced enthalpy difference.\n        u_k = np.matrix(u_i)\n        Delta_u_ij = u_k - u_k.transpose()\n\n        # Compute reduced entropy difference\n        s_k = u_k - f_k\n        Delta_s_ij = s_k - s_k.transpose()\n\n        # compute uncertainty matrix in free energies:\n        # d2DeltaF = Theta_ij[i,i] + Theta_ij[j,j] - 2.0 * Theta_ij[i,j]\n\n        diag = Theta_ij.diagonal()\n        dii = diag[0:K, 0:K]\n        d2DeltaF = dii + dii.transpose() - 2 * Theta_ij[0:K, 0:K]\n\n        # check for any numbers below zero.\n        if (np.any(d2DeltaF < 0.0)):\n            if(np.any(d2DeltaF) < warning_cutoff):\n                # Hmm.  Will this print correctly?\n                print(\"A squared uncertainty is negative.  d2DeltaF = %e\" % d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)])\n            else:\n                d2DeltaF[(np.any(d2DeltaF) < warning_cutoff)] = 0.0\n\n        # take the square root of the entries of matrix\n        dDelta_f_ij = np.sqrt(d2DeltaF)\n        # TODO -- vectorize this calculation for entropy and enthalpy!\n\n        for i in range(0, K):\n            for j in range(0, K):\n                try:\n                    dDelta_u_ij[i, j] = math.sqrt(\n                        + u_i[i] * Theta_ij[i, i] * u_i[i] - u_i[i] * Theta_ij[i, j] * u_i[j] - u_i[\n                            i] * Theta_ij[i, K + i] * u_i[i] + u_i[i] * Theta_ij[i, K + j] * u_i[j]\n                        - u_i[j] * Theta_ij[j, i] * u_i[i] + u_i[j] * Theta_ij[j, j] * u_i[j] + u_i[\n                            j] * Theta_ij[j, K + i] * u_i[i] - u_i[j] * Theta_ij[j, K + j] * u_i[j]\n                        - u_i[i] * Theta_ij[K + i, i] * u_i[i] + u_i[i] * Theta_ij[K + i, j] * u_i[\n                            j] + u_i[i] * Theta_ij[K + i, K + i] * u_i[i] - u_i[i] * Theta_ij[K + i, K + j] * u_i[j]\n                        + u_i[j] * Theta_ij[K + j, i] * u_i[i] - u_i[j] * Theta_ij[K + j, j] * u_i[\n                            j] - u_i[j] * Theta_ij[K + j, K + i] * u_i[i] + u_i[j] * Theta_ij[K + j, K + j] * u_i[j]\n                    )\n                except:\n                    dDelta_u_ij[i, j] = 0.0\n\n                # Compute reduced entropy difference.\n                try:\n                    dDelta_s_ij[i, j] = math.sqrt(\n                        + (u_i[i] - 1) * Theta_ij[i, i] * (u_i[i] - 1) + (u_i[i] - 1) * Theta_ij[i, j] * (-u_i[j] + 1) + (\n                            u_i[i] - 1) * Theta_ij[i, K + i] * (-u_i[i]) + (u_i[i] - 1) * Theta_ij[i, K + j] * u_i[j]\n                        + (-u_i[j] + 1) * Theta_ij[j, i] * (u_i[i] - 1) + (-u_i[j] + 1) * Theta_ij[j, j] * (-u_i[j] + 1) +\n                        (-u_i[j] + 1) * Theta_ij[j, K + i] * (-u_i[i]) +\n                        (-u_i[j] + 1) * Theta_ij[j, K + j] * u_i[j]\n                        + (-u_i[i]) * Theta_ij[K + i, i] * (u_i[i] - 1) + (-u_i[i]) * Theta_ij[K + i, j] * (-u_i[j] + 1) +\n                        (-u_i[i]) * Theta_ij[K + i, K + i] * (-u_i[i]) +\n                        (-u_i[i]) * Theta_ij[K + i, K + j] * u_i[j]\n                        + u_i[j] * Theta_ij[K + j, i] * (u_i[i] - 1) + u_i[j] * Theta_ij[K + j, j] * (-u_i[j] + 1) + u_i[\n                            j] * Theta_ij[K + j, K + i] * (-u_i[i]) + u_i[j] * Theta_ij[K + j, K + j] * u_i[j]\n                    )\n                except:\n                    dDelta_s_ij[i, j] = 0.0\n\n        # Return expectations and uncertainties.\n        return (Delta_f_ij, dDelta_f_ij, Delta_u_ij, dDelta_u_ij, Delta_s_ij, dDelta_s_ij)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the free energy of occupying a number of bins. This implementation defines each bin as a separate thermodynamic state. Parameters ---------- u_n : np.ndarray, float, shape=(K, N) u_n[n] is the reduced potential energy of snapshot n for which the PMF is to be computed. bin_n : np.ndarray, int, shape=(N) bin_n[n] is the bin index of snapshot n. bin_n can assume a value in range(0,nbins) nbins : int The number of bins fmax : float, optional The maximum value of the free energy, used for an empty bin (default: 1000) Returns ------- f_i : np.ndarray, float, shape=(K) f_i[i] is the dimensionless free energy of state i, relative to the state of lowest free energy d2f_ij : np.ndarray, float, shape=(K) d2f_ij[i,j] is the uncertainty in the difference of (f_i - f_j) Notes ----- All bins must have some samples in them from at least one of the states -- this will not work if bin_n.sum(0) == 0. Empty bins should be removed before calling computePMF(). This method works by computing the free energy of localizing the system to each bin for the given potential by aggregating the log weights for the given potential. To estimate uncertainties, the NxK weight matrix W_nk is augmented to be Nx(K+nbins) in order to accomodate the normalized weights of states where the potential is given by u_kn within each bin and infinite potential outside the bin. The uncertainties with respect to the bin of lowest free energy are then computed in the standard way. WARNING! This method is EXPERIMENTAL and should be used at your own risk.", "response": "def computePMF_states(self, u_n, bin_n, nbins):\n        \"\"\"Compute the free energy of occupying a number of bins.\n\n        This implementation defines each bin as a separate thermodynamic state.\n\n        Parameters\n        ----------\n        u_n : np.ndarray, float, shape=(K, N)\n            u_n[n] is the reduced potential energy of snapshot n for which the PMF is to be computed.\n        bin_n : np.ndarray, int, shape=(N)\n            bin_n[n] is the bin index of snapshot n.  bin_n can assume a value in range(0,nbins)\n        nbins : int\n            The number of bins\n        fmax : float, optional\n            The maximum value of the free energy, used for an empty bin (default: 1000)\n\n        Returns\n        -------\n        f_i : np.ndarray, float, shape=(K)\n            f_i[i] is the dimensionless free energy of state i, relative to the state of lowest free energy\n        d2f_ij : np.ndarray, float, shape=(K)\n            d2f_ij[i,j] is the uncertainty in the difference of (f_i - f_j)\n\n        Notes\n        -----\n        All bins must have some samples in them from at least one of the states -- this will not work if bin_n.sum(0) == 0. Empty bins should be removed before calling computePMF().\n        This method works by computing the free energy of localizing the system to each bin for the given potential by aggregating the log weights for the given potential.\n        To estimate uncertainties, the NxK weight matrix W_nk is augmented to be Nx(K+nbins) in order to accomodate the normalized weights of states where\n        the potential is given by u_kn within each bin and infinite potential outside the bin.  The uncertainties with respect to the bin of lowest free energy\n        are then computed in the standard way.\n\n        WARNING!\n        This method is EXPERIMENTAL and should be used at your own risk.\n\n        \"\"\"\n\n        # Verify that no PMF bins are empty -- we can't deal with empty bins,\n        # because the free energy is infinite.\n        for i in range(nbins):\n            if np.sum(bin_n == i) == 0:\n                raise ParameterError(\n                    \"At least one bin in provided bin_n argument has no samples.  All bins must have samples for free energies to be finite.  Adjust bin sizes or eliminate empty bins to ensure at least one sample per bin.\")\n\n        K = self.K\n\n        # Unroll to n-indices\n        log_w_n = self._computeUnnormalizedLogWeights(u_n)\n\n        # Compute the free energies for these states.\n        f_i = np.zeros([nbins], np.float64)\n        for i in range(nbins):\n            # Get linear n-indices of samples that fall in this bin.\n            indices = np.where(bin_n[self.indices] == i)[0]\n\n            # Sanity check.\n            if (len(indices) == 0):\n                raise DataError(\"WARNING: bin %d has no samples -- all bins must have at least one sample.\" % i)\n\n            # Compute dimensionless free energy of occupying state i.\n            f_i[i] = - _logsum(log_w_n[indices])\n\n        # Shift so that f_i.min() = 0\n        f_i_min = f_i.min()\n        f_i -= f_i.min()\n\n        if self.verbose:\n            print(\"bins f_i = \")\n            print(f_i)\n\n        # Compute uncertainties by forming matrix of W_nk.\n        if self.verbose:\n            print(\"Forming W_nk matrix...\")\n        N_k = np.zeros([self.K + nbins], np.int32)\n        N_k[0:K] = self.N_k\n        W_nk = np.zeros([self.N, self.K + nbins], np.float64)\n        W_nk[:, 0:K] = np.exp(self.Log_W_nk)\n        for i in range(nbins):\n            # Get indices of samples that fall in this bin.\n            indices = np.where(bin_n[self.indices] == i)[0]\n\n            if self.verbose:\n                print(\"bin %5d count = %10d\" % (i, len(indices)))\n\n            # Compute normalized weights for this state.\n            W_nk[indices, K + i] = np.exp(\n                log_w_n[indices] + f_i[i] + f_i_min)\n\n        # Compute asymptotic covariance matrix using specified method.\n        Theta_ij = self._computeAsymptoticCovarianceMatrix(W_nk, N_k)\n\n        # Compute uncertainties with respect to difference in free energy from\n        # this state j.\n        diag = Theta_ij.diagonal()\n        dii = diag[0, K:K + nbins]\n        d2f_ij = dii + dii.transpose() - 2 * Theta_ij[K:K + nbins, K:K + nbins]\n\n        # Return dimensionless free energy and uncertainty.\n        return (f_i, d2f_ij)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the normalized weights corresponding to samples for the given reduced potential.", "response": "def _computeWeights(self, logform=False, include_nonzero=False, recalc_denom=True, return_f_k=False):\n        \"\"\"Compute the normalized weights corresponding to samples for the given reduced potential.\n\n        Compute the normalized weights corresponding to samples for the given reduced potential.\n        Also stores the all_log_denom array for reuse.\n\n        Parameters\n        ----------\n        logform : bool, optional\n            Whether the output is in logarithmic form, which is better for stability, though sometimes\n            the exponential form is requires.\n        include_nonzero : bool, optional\n            whether to compute weights for states with nonzero states.  Not necessary\n            when performing self-consistent iteration.\n        recalc_denom : bool, optional\n            recalculate the denominator, must be done if the free energies change.\n            default is to do it, so that errors are not made.  But can be turned\n            off if it is known the free energies have not changed.\n        return_f_k : bool, optional\n            return the self-consistent f_k values\n\n        Returns\n        -------\n\n        if logform==True:\n          Log_W_nk (double) - Log_W_nk[n,k] is the normalized log weight of sample n from state k.\n        else:\n          W_nk (double) - W_nk[n,k] is the log weight of sample n from state k.\n        if return_f_k==True:\n          optionally return the self-consistent free energy from these weights.\n\n       \"\"\"\n        if (include_nonzero):\n            f_k = self.f_k\n            K = self.K\n        else:\n            f_k = self.f_k[self.states_with_samples]\n            K = len(self.states_with_samples)\n\n        # array of either weights or normalized log weights\n        Warray_nk = np.zeros([self.N, K], dtype=np.float64)\n        if (return_f_k):\n            f_k_out = np.zeros([K], dtype=np.float64)\n\n        if (recalc_denom):\n            self.log_weight_denom = self._computeUnnormalizedLogWeights(\n                np.zeros([self.N], dtype=np.float64))\n\n        for k in range(K):\n            if (include_nonzero):\n                index = k\n            else:\n                index = self.states_with_samples[k]\n            log_w_n = -self.u_kn[index, :] + self.log_weight_denom + f_k[k]\n\n            if (return_f_k):\n                f_k_out[k] = f_k[k] - _logsum(log_w_n)\n                if (include_nonzero):\n                    # renormalize the weights, needed for nonzero states.\n                    log_w_n += (f_k_out[k] - f_k[k])\n\n            if (logform):\n                Warray_nk[:, k] = log_w_n\n            else:\n                Warray_nk[:, k] = np.exp(log_w_n)\n\n        # Return weights (or log weights)\n        if (return_f_k):\n            f_k_out[:] = f_k_out[:] - f_k_out[0]\n            return Warray_nk, f_k_out\n        else:\n            return Warray_nk"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pseudoinverse(self, A, tol=1.0e-10):\n\n        # DEBUG\n        # TODO: Should we use pinv, or _pseudoinverse?\n        # return np.linalg.pinv(A)\n\n        # Get size\n        [M, N] = A.shape\n        if N != M:\n            raise DataError(\"pseudoinverse can only be computed for square matrices: dimensions were %d x %d\" % (\n                M, N))\n\n        # Make sure A contains no nan.\n        if(np.any(np.isnan(A))):\n            print(\"attempted to compute pseudoinverse of A =\")\n            print(A)\n            raise ParameterError(\"A contains nan.\")\n\n        # DEBUG\n        diagonal_loading = False\n        if diagonal_loading:\n            # Modify matrix by diagonal loading.\n            eigs = linalg.eigvalsh(A)\n            most_negative_eigenvalue = eigs.min()\n            if (most_negative_eigenvalue < 0.0):\n                print(\"most negative eigenvalue = %e\" % most_negative_eigenvalue)\n                # Choose loading value.\n                gamma = -most_negative_eigenvalue * 1.05\n                # Modify Theta by diagonal loading\n                A += gamma * np.eye(A.shape[0])\n\n        # Compute SVD of A.\n        [U, S, Vt] = linalg.svd(A)\n\n        # Compute pseudoinverse by taking square root of nonzero singular\n        # values.\n        Ainv = np.matrix(np.zeros([M, M], dtype=np.float64))\n        for k in range(M):\n            if (abs(S[k]) > tol * abs(S[0])):\n                Ainv += (1.0/S[k]) * np.outer(U[:, k], Vt[k, :]).T\n\n        return Ainv", "response": "Compute the Moore - Penrose pseudoinverse of a matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nzero out states that should be identical to the given matrix", "response": "def _zerosamestates(self, A):\n        \"\"\"\n        zeros out states that should be identical\n\n        REQUIRED ARGUMENTS\n\n        A: the matrix whose entries are to be zeroed.\n\n        \"\"\"\n\n        for pair in self.samestates:\n            A[pair[0], pair[1]] = 0\n            A[pair[1], pair[0]] = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _computeAsymptoticCovarianceMatrix(self, W, N_k, method=None):\n\n        # Set 'svd-ew' as default if uncertainty method specified as None.\n        if method == None:\n            method = 'svd-ew'\n\n        # Get dimensions of weight matrix.\n        [N, K] = W.shape\n\n        # Check dimensions\n        if(K != N_k.size):\n            raise ParameterError(\n                'W must be NxK, where N_k is a K-dimensional array.')\n        if(np.sum(N_k) != N):\n            raise ParameterError('W must be NxK, where N = sum_k N_k.')\n\n        # Check to make sure the weight matrix W is properly normalized.\n        tolerance = 1.0e-4  # tolerance for checking equality of sums\n\n        column_sums = np.sum(W, axis=0)\n        badcolumns = (np.abs(column_sums - 1) > tolerance)\n        if np.any(badcolumns):\n            which_badcolumns = np.arange(K)[badcolumns]\n            firstbad = which_badcolumns[0]\n            raise ParameterError(\n                'Warning: Should have \\sum_n W_nk = 1.  Actual column sum for state %d was %f. %d other columns have similar problems' %\n                (firstbad, column_sums[firstbad], np.sum(badcolumns)))\n\n        row_sums = np.sum(W * N_k, axis=1)\n        badrows = (np.abs(row_sums - 1) > tolerance)\n        if np.any(badrows):\n            which_badrows = np.arange(N)[badrows]\n            firstbad = which_badrows[0]\n            raise ParameterError(\n                'Warning: Should have \\sum_k N_k W_nk = 1.  Actual row sum for sample %d was %f. %d other rows have similar problems' %\n                (firstbad, row_sums[firstbad], np.sum(badrows)))\n\n        # Compute estimate of asymptotic covariance matrix using specified\n        # method.\n        if method == 'generalized-inverse':\n            # Use generalized inverse (Eq. 8 of [1]) -- most general\n            # Theta = W' (I - W N W')^+ W\n\n            # Construct matrices\n            # Diagonal N_k matrix.\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            I = np.identity(N, dtype=np.float64)\n\n            # Compute covariance\n            Theta = W.T * self._pseudoinverse(I - W * Ndiag * W.T) * W\n\n        elif method == 'inverse':\n            # Use standard inverse method (Eq. D8 of [1]) -- only applicable if all K states are different\n            # Theta = [(W'W)^-1 - N + 1 1'/N]^-1\n\n            # Construct matrices\n            # Diagonal N_k matrix.\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            # I = np.identity(N, dtype=np.float64)  # Don't unnecessarily allocate O(N^2) memory.  See Issue #106\n            # matrix of ones, times 1/N\n            O = np.ones([K, K], dtype=np.float64) / float(N)\n\n            # Make sure W is nonsingular.\n            if (abs(linalg.det(W.T * W)) < tolerance):\n                print(\"Warning: W'W appears to be singular, yet 'inverse' method of uncertainty estimation requires W contain no duplicate states.\")\n\n            # Compute covariance\n            Theta = ((W.T * W).I - Ndiag + O).I\n\n        elif method == 'approximate':\n            # Use fast approximate expression from Kong et al. -- this underestimates the true covariance, but may be a good approximation in some cases and requires no matrix inversions\n            # Theta = P'P\n\n            # Construct matrices\n            W = np.matrix(W, dtype=np.float64)\n\n            # Compute covariance\n            Theta = W.T * W\n\n        elif method == 'svd':\n            # Use singular value decomposition based approach given in supplementary material to efficiently compute uncertainty\n            # See Appendix D.1, Eq. D4 in [1].\n\n            # Construct matrices\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            I = np.identity(K, dtype=np.float64)\n\n            # Compute SVD of W\n            [U, S, Vt] = linalg.svd(W)\n            Sigma = np.matrix(np.diag(S))\n            V = np.matrix(Vt).T\n\n            # Compute covariance\n            Theta = V * Sigma * self._pseudoinverse(\n                I - Sigma * V.T * Ndiag * V * Sigma) * Sigma * V.T\n\n        elif method == 'svd-ew':\n            # Use singular value decomposition based approach given in supplementary material to efficiently compute uncertainty\n            # The eigenvalue decomposition of W'W is used to forego computing the SVD.\n            # See Appendix D.1, Eqs. D4 and D5 of [1].\n\n            # Construct matrices\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            I = np.identity(K, dtype=np.float64)\n\n            # Compute singular values and right singular vectors of W without using SVD\n            # Instead, we compute eigenvalues and eigenvectors of W'W.\n            # Note W'W = (U S V')'(U S V') = V S' U' U S V' = V (S'S) V'\n            [S2, V] = linalg.eigh(W.T * W)\n            # Set any slightly negative eigenvalues to zero.\n            S2[np.where(S2 < 0.0)] = 0.0\n            # Form matrix of singular values Sigma, and V.\n            Sigma = np.matrix(np.diag(np.sqrt(S2)))\n            V = np.matrix(V)\n\n            # Compute covariance\n            Theta = V * Sigma * self._pseudoinverse(\n                I - Sigma * V.T * Ndiag * V * Sigma) * Sigma * V.T\n\n        elif method == 'tan-HGH':\n            # Use method suggested by Zhiqiang Tan without further simplification.\n            # TODO: There may be a problem here -- double-check this.\n\n            [N, K] = W.shape\n\n            # Estimate O matrix from W'W.\n            W = np.matrix(W, dtype=np.float64)\n            O = W.T * W\n\n            # Assemble the Lambda matrix.\n            Lambda = np.matrix(np.diag(N_k), dtype=np.float64)\n\n            # Identity matrix.\n            I = np.matrix(np.eye(K), dtype=np.float64)\n\n            # Compute H and G matrices.\n            H = O * Lambda - I\n            G = O - O * Lambda * O\n\n            # Compute pseudoinverse of H\n            Hinv = self._pseudoinverse(H)\n\n            # Compute estimate of asymptotic covariance.\n            Theta = Hinv * G * Hinv.T\n\n        elif method == 'tan':\n            # Use method suggested by Zhiqiang Tan.\n\n            # Estimate O matrix from W'W.\n            W = np.matrix(W, dtype=np.float64)\n            O = W.T * W\n\n            # Assemble the Lambda matrix.\n            Lambda = np.matrix(np.diag(N_k), dtype=np.float64)\n\n            # Compute covariance.\n            Oinv = self._pseudoinverse(O)\n            Theta = self._pseudoinverse(Oinv - Lambda)\n\n        else:\n            # Raise an exception.\n            raise ParameterError('Method ' + method + ' unrecognized.')\n\n        return Theta", "response": "This is the function that computes the asymptotic covariance matrix for the current state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initializeFreeEnergies(self, verbose=False, method='zeros'):\n\n        if (method == 'zeros'):\n            # Use zeros for initial free energies.\n            if verbose:\n                print(\"Initializing free energies to zero.\")\n            self.f_k[:] = 0.0\n        elif (method == 'mean-reduced-potential'):\n            # Compute initial guess at free energies from the mean reduced\n            # potential from each state\n            if verbose:\n                print(\"Initializing free energies with mean reduced potential for each state.\")\n            means = np.zeros([self.K], float)\n            for k in self.states_with_samples:\n                means[k] = self.u_kn[k, 0:self.N_k[k]].mean()\n            if (np.max(np.abs(means)) < 0.000001):\n                print(\"Warning: All mean reduced potentials are close to zero. If you are using energy differences in the u_kln matrix, then the mean reduced potentials will be zero, and this is expected behavoir.\")\n            self.f_k = means\n        elif (method == 'BAR'):\n            # For now, make a simple list of those states with samples.\n            initialization_order = np.where(self.N_k > 0)[0]\n            # Initialize all f_k to zero.\n            self.f_k[:] = 0.0\n            # Initialize the rest\n            for index in range(0, np.size(initialization_order) - 1):\n                k = initialization_order[index]\n                l = initialization_order[index + 1]\n                # forward work\n                # here, we actually need to distinguish which states are which\n                w_F = (\n                    self.u_kn[l,self.x_kindices==k] - self.u_kn[k,self.x_kindices==k])\n                    #self.u_kln[k, l, 0:self.N_k[k]] - self.u_kln[k, k, 0:self.N_k[k]])\n                    # reverse work\n                w_R = (\n                    self.u_kn[k,self.x_kindices==l] - self.u_kn[l,self.x_kindices==l])\n                    #self.u_kln[l, k, 0:self.N_k[l]] - self.u_kln[l, l, 0:self.N_k[l]])\n\n                if (len(w_F) > 0 and len(w_R) > 0):\n                    # BAR solution doesn't need to be incredibly accurate to\n                    # kickstart NR.\n                    import pymbar.bar\n                    self.f_k[l] = self.f_k[k] + pymbar.bar.BAR(\n                        w_F, w_R, relative_tolerance=0.000001, verbose=False, compute_uncertainty=False)\n                else:\n                    # no states observed, so we don't need to initialize this free energy anyway, as\n                    # the solution is noniterative.\n                    self.f_k[l] = 0\n\n        else:\n            # The specified method is not implemented.\n            raise ParameterError('Method ' + method + ' unrecognized.')\n\n        # Shift all free energies such that f_0 = 0.\n        self.f_k[:] = self.f_k[:] - self.f_k[0]\n\n        return", "response": "Initializes the initial guess at the relative free energies."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _computeUnnormalizedLogWeights(self, u_n):\n\n        if (self.use_embedded_helper_code):\n            # Use embedded C++ optimizations.\n            import _pymbar\n            # necessary for helper code to interpret type of u_kn\n            u_n = np.array(u_n, dtype=np.float64)\n            log_w_n = _pymbar.computeUnnormalizedLogWeightsCpp(\n                self.K, self.N, self.K_nonzero, self.states_with_samples, self.N_k, self.f_k, self.u_kn, u_n)\n        else:\n            try:\n                from scipy import weave\n                # Allocate storage for return values.\n                log_w_n = np.zeros([self.N], dtype=np.float64)\n                # Copy useful class members to local variables.\n                K = self.K\n                f_k = self.f_k\n                N = self.N\n                N_k = self.N_k\n                u_kn = self.u_kn\n                # Weave inline C++ code.\n                code = \"\"\"\n        double log_terms[%(K)d]; // temporary storage for log terms\n        for (int n = 0; n < N; n++) {\n            double max_log_term = 0.0;\n            bool first_nonzero = true;\n            for (int k = 0; k < K; k++) {\n              // skip empty states\n              if (N_K1(k) == 0) continue;\n              double log_term = log(N_K1(k)) + F_K1(k) - U_KN2(k,n) + U_N1(n);\n              log_terms[k] = log_term;\n              if (first_nonzero || (log_term > max_log_term)) {\n                max_log_term = log_term;\n                first_nonzero = false;\n              }\n            }\n\n            double term_sum = 0.0;\n            for (int k = 0; k < K; k++) {\n              // skip empty states\n              if (N_K1(k) == 0) continue;\n              term_sum += exp(log_terms[k] - max_log_term);\n            }\n            double log_term_sum = log(term_sum) + max_log_term;\n            LOG_W_N1(n) = - log_term_sum;\n        }\n        \"\"\" % vars()\n                # Execute inline C code with weave.\n                info = weave.inline(\n                    code, ['K', 'N', 'N_k', 'u_n', 'u_kn', 'f_k', 'log_w_n'], headers=['<math.h>', '<stdlib.h>'], verbose=2)\n            except:\n                # Compute unnormalized log weights in pure Python.\n                log_w_n = np.zeros([self.N], dtype=np.float64)\n                for n in range(0, self.N):\n                    log_w_n[n] = - _logsum(np.log(self.N_k[self.states_with_samples]) +\n                                           self.f_k[self.states_with_samples] -\n                                           (self.u_kn[self.states_with_samples, n] - u_n[n]))\n\n        return log_w_n", "response": "This function computes the unnormalized log weights of the unnormalized states at a single state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _selfConsistentIteration(self, relative_tolerance=1.0e-6, maximum_iterations=1000, verbose=True, print_warning=False):\n\n        # Iteratively update dimensionless free energies until convergence to\n        # specified tolerance, or maximum allowed number of iterations has been\n        # exceeded.\n        if verbose:\n            print(\"MBAR: Computing dimensionless free energies by iteration.  This may take from seconds to minutes, depending on the quantity of data...\")\n        for iteration in range(0, maximum_iterations):\n\n            if verbose:\n                print('Self-consistent iteration %d' % iteration)\n\n            # compute the free energies by self consistent iteration (which\n            # also involves calculating the weights)\n            (W_nk, f_k_new) = self._computeWeights(\n                logform=True, return_f_k=True)\n\n            if self._amIdoneIterating(\n                                f_k_new, relative_tolerance, iteration,\n                                maximum_iterations, print_warning, verbose):\n                break\n\n        return", "response": "This function is used to compute the free energies by self - consistent iteration of the MBAR equations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _adaptive(self, gamma=1.0, relative_tolerance=1.0e-8, maximum_iterations=1000, verbose=True, print_warning=True):\n\n        if verbose:\n            print(\"Determining dimensionless free energies by Newton-Raphson iteration.\")\n\n        # keep track of Newton-Raphson and self-consistent iterations\n        nr_iter = 0\n        sci_iter = 0\n\n        N_k = self.N_k[self.states_with_samples]\n        K = len(N_k)\n\n        f_k_sci = np.zeros([K], dtype=np.float64)\n        f_k_new = np.zeros([K], dtype=np.float64)\n\n        # Perform Newton-Raphson iterations (with sci computed on the way)\n        for iteration in range(0, maximum_iterations):\n\n            # Store for new estimate of dimensionless relative free energies.\n            f_k = self.f_k[self.states_with_samples].copy()\n\n            # compute weights for gradients: the denominators and free energies are from the previous\n            # iteration in most cases.\n            (W_nk, f_k_sci) = self._computeWeights(\n                recalc_denom=(iteration == 0), return_f_k = True)\n\n            # Compute gradient and Hessian of last (K-1) states.\n            #\n            # gradient (defined by Eq. C6 of [1])\n            # g_i(theta) = N_i - \\sum_n N_i W_ni\n            #\n            # Hessian (defined by Eq. C9 of [1])\n            # H_ii(theta) = - \\sum_n N_i W_ni (1 - N_i W_ni)\n            # H_ij(theta) = \\sum_n N_i W_ni N_j W_nj\n            #\n\n            \"\"\"\n      g = np.matrix(np.zeros([K-1,1], dtype=np.float64)) # gradient\n      H = np.matrix(np.zeros([K-1,K-1], dtype=np.float64)) # Hessian\n      for i in range(1,K):\n        g[i-1] = N_k[i] - N_k[i] * W_nk[:,i].sum()\n        H[i-1,i-1] = - (N_k[i] * W_nk[:,i] * (1.0 - N_k[i] * W_nk[:,i])).sum() \n        for j in range(1,i):\n          H[i-1,j-1] = (N_k[i] * W_nk[:,i] * N_k[j] * W_nk[:,j]).sum()\n          H[j-1,i-1] = H[i-1,j-1]\n\n      # Update the free energy estimate (Eq. C11 of [1]).\n      Hinvg = linalg.lstsq(H,g)[0]      #\n      # Hinvg = linalg.solve(H,g)       # This might be faster if we can guarantee full rank.\n      for k in range(0,K-1):\n        f_k_new[k+1] = f_k[k+1] - gamma*Hinvg[k]\n\n      \"\"\"\n            g = N_k - N_k * W_nk.sum(axis=0)\n            NW = N_k * W_nk\n            H = np.dot(NW.T, NW)\n            H += (g.T - N_k) * np.eye(K)\n            # Update the free energy estimate (Eq. C11 of [1]).\n            # will always have lower rank the way it is set up\n            Hinvg = linalg.lstsq(H, g)[0]\n            Hinvg -= Hinvg[0]\n            f_k_new = f_k - gamma * Hinvg\n\n            # self-consistent iteration gradient norm and saved log sums.\n            g_sci = self._gradientF(f_k_sci)\n            gnorm_sci = np.dot(g_sci, g_sci)\n            # save this so we can switch it back in if g_sci is lower.\n            log_weight_denom = self.log_weight_denom.copy()\n\n            # newton raphson gradient norm and saved log sums.\n            g_nr = self._gradientF(f_k_new)\n            gnorm_nr = np.dot(g_nr, g_nr)\n\n            # we could save the gradient, too, but it's not too expensive to\n            # compute since we are doing the Hessian anyway.\n\n            if verbose:\n                print(\"self consistent iteration gradient norm is %10.5g, Newton-Raphson gradient norm is %10.5g\" % (gnorm_sci, gnorm_nr))\n            # decide which directon to go depending on size of gradient norm\n            if (gnorm_sci < gnorm_nr or sci_iter < 2):\n                sci_iter += 1\n                self.log_weight_denom = log_weight_denom.copy()\n                if verbose:\n                    if sci_iter < 2:\n                        print(\"Choosing self-consistent iteration on iteration %d\" % iteration)\n                    else:\n                        print(\"Choosing self-consistent iteration for lower gradient on iteration %d\" % iteration)\n\n                f_k_new = f_k_sci.copy()\n            else:\n                nr_iter += 1\n                if verbose:\n                    print(\"Newton-Raphson used on iteration %d\" % iteration)\n\n            # get rid of big matrices that are not used.\n            del(log_weight_denom, NW, W_nk)\n\n            # have to set the free energies back in self, since the gradient\n            # routine changes them.\n            self.f_k[self.states_with_samples] = f_k\n            if (self._amIdoneIterating(f_k_new, relative_tolerance, iteration, maximum_iterations, print_warning, verbose)):\n                if verbose:\n                    print('Of %d iterations, %d were Newton-Raphson iterations and %d were self-consistent iterations' % (iteration + 1, nr_iter, sci_iter))\n                break\n\n        return", "response": "This method is used to compute the root - finding of the new likelihood function for a given set of states."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeEffectiveSampleNumber(self, verbose = False):\n\n        N_eff = np.zeros(self.K)\n        for k in range(self.K):\n            w = np.exp(self.Log_W_nk[:,k])\n            N_eff[k] = 1/np.sum(w**2)\n            if verbose:\n                print(\"Effective number of sample in state %d is %10.3f\" % (k,N_eff[k]))\n                print(\"Efficiency for state %d is %d/%d = %10.4f\" % (k,N_eff[k],len(w),N_eff[k]/len(w)))\n\n        return N_eff", "response": "This function computes the effective sample number of each state in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef computeOverlap(self):\n\n        W = np.matrix(self.getWeights(), np.float64)\n        O = np.multiply(self.N_k, W.T * W)\n        (eigenvals, eigevec) = linalg.eig(O)\n        # sort in descending order\n        eigenvals = np.sort(eigenvals)[::-1]\n        overlap_scalar = 1 - eigenvals[1] # 1 minus the second largest eigenvalue\n\n        results_vals = dict()\n        results_vals['scalar'] = overlap_scalar\n        results_vals['eigenvalues'] = eigenvals\n        results_vals['matrix'] = O\n\n        return results_vals", "response": "Compute the estimate of overlap between states."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getFreeEnergyDifferences(self, compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n        Deltaf_ij, dDeltaf_ij, Theta_ij = None, None, None  # By default, returns None for dDelta and Theta\n\n        # Compute free energy differences.\n        f_i = np.matrix(self.f_k)\n        Deltaf_ij = f_i - f_i.transpose()\n\n        # zero out numerical error for thermodynamically identical states\n        self._zerosamestates(Deltaf_ij)\n\n        Deltaf_ij = np.array(Deltaf_ij)  # Convert from np.matrix to np.array\n\n        result_vals = dict()\n\n        result_vals['Delta_f'] = Deltaf_ij\n\n        if compute_uncertainty or return_theta:\n            # Compute asymptotic covariance matrix.\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                np.exp(self.Log_W_nk), self.N_k, method=uncertainty_method)\n\n        if compute_uncertainty:\n            dDeltaf_ij = self._ErrorOfDifferences(Theta_ij, warning_cutoff=warning_cutoff)\n            # zero out numerical error for thermodynamically identical states\n            self._zerosamestates(dDeltaf_ij)\n            # Return matrix of free energy differences and uncertainties.\n            dDeltaf_ij = np.array(dDeltaf_ij)\n            result_vals['dDelta_f'] = dDeltaf_ij\n\n        if return_theta:\n            result_vals['Theta'] = Theta_ij\n\n        return result_vals", "response": "Returns the dimensionless free energy differences and uncertainties among all thermodynamic states."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef computeExpectationsInner(self, A_n, u_ln, state_map,\n                                 uncertainty_method=None,\n                                 warning_cutoff=1.0e-10,\n                                 return_theta=False):\n        \"\"\"\n        Compute the expectations of multiple observables of phase space functions in multiple states.\n\n        Compute the expectations of multiple observables of phase\n        space functions [A_0(x),A_1(x),...,A_i(x)] along with the\n        covariances of their estimates at multiple states.\n\n        intended as an internal function to keep all the optimized and\n        robust expectation code in one place, but will leave it\n        open to allow for later modifications\n\n        It calculates all input observables at all states which are\n        specified by the list of states in the state list.\n\n        Parameters\n        ----------\n        A_n : np.ndarray, float, shape=(I, N)\n            A_in[i,n] = A_i(x_n), the value of phase observable i for configuration n\n        u_ln : np.ndarray, float, shape=(L, N)\n            u_ln[l,n] is the reduced potential of configuration n at state l\n            if u_ln = None, we use self.u_kn\n        state_map : np.ndarray, int, shape (2,NS) or shape(1,NS)\n            If state_map has only one dimension where NS is the total number of states we want to simulate things\n            a.  The list will be of the form ``[[0,1,2],[0,1,1]]``. This particular example indicates we want to output\n            the properties of three observables total: the first property A[0] at the 0th state, the 2nd property\n            A[1] at the 1th state, and the 2nd property A[1] at the 2nd state. This allows us to tailor our output to a\n            large number of different situations.\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n        return_theta : bool, optional\n            Whether or not to return the theta matrix.  Can be useful for complicated differences of observables.\n\n        Returns\n        -------\n        result_vals : dictionary\n\n        Possible keys in the result_vals dictionary:\n\n        'observables': np.ndarray, float, shape = (S)\n            results_vals['observables'][i] is the estimate for the expectation of A_state_map[i](x) at the state specified by u_n[state_map[i],:]\n\n        'Theta' : np.ndarray, float, shape = (K+len(state_list), K+len(state_list)) the covariance matrix of log weights.\n\n        'Amin' : np.ndarray, float, shape = (S), needed for reconstructing the covariance one level up. \n\n        'f' : np.ndarray, float, shape = (K+len(state_list)), 'free energies' of the new states (i.e. ln (<A>-Amin+1)) as the log form is easier to work with.\n\n        Notes\n        -----\n\n        Situations this will be used for :\n\n        * Multiple observables, single state (called though computeMultipleExpectations)\n        * Single observable, multiple states (called through computeExpectations)\n\n            This has two cases: observables that don't change with state, and observables that\n            do change with state.\n            For example, the set of energies at state k consist in energy function of state\n            1 evaluated at state 1, energies of state 2 evaluated at\n            state 2, and so forth.\n\n        * Computing only free energies at new states.\n\n        * Would require additional work to work with potentials of mean force, because we need to ignore the functions that are zero when integrating.\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> A_n = np.array([x_n,x_n**2,x_n**3])\n        >>> u_n = u_kn[:2,:]\n        >>> state_map = np.array([[0,0],[1,0],[2,0],[2,1]],int)\n        >>> results = mbar.computeExpectationsInner(A_n, u_n, state_map)\n\n        \"\"\"\n\n        # Retrieve N and K for convenience.\n        mapshape = np.shape(state_map) # number of computed expectations we desire\n                                               # need to convert to matrix to be able to pick up D=1\n        if len(mapshape) < 2:\n            # if 1D, it's just a list of states\n            state_list = state_map.copy()\n            state_map = np.zeros([0,0],np.float64)\n            S = 0\n        else:  # if 2D, then it's a list of observables and corresponding states\n            state_list = state_map[0,:]\n            S = mapshape[1]\n\n        # reshape arrays explicitly into 2d (even if only one state) to make it easy to manipulate\n        shapeu = np.shape(u_ln)\n        if len(shapeu) == 1:\n            u_ln = np.reshape(u_ln,[1,shapeu[0]])\n\n        shapeA = np.shape(A_n)\n        if len(shapeA) == 1:\n            A_n = np.reshape(A_n,[1,shapeA[0]])\n\n        K = self.K\n        N = self.N  # N is total number of samples\n        result_vals = dict() # dictionary we will store uncertainties in\n\n        # make observables all positive, allowing us to take the logarithm, which is\n        # required to prevent overflow in some examples.\n        # WARNING: one issue to watch for is if one of the energies is extremely\n        # low (-10^10 or lower), but most of the energies of interest are much higher.\n        # This could lead to roundoff problems (check with Levi N.)\n\n        L_list = np.unique(state_list)\n        NL = len(L_list) # number of states we need to examine\n        if S > 0:\n            A_list = np.unique(state_map[1,:])  # what are the unique observables\n            A_min = np.zeros([len(A_list)], dtype=np.float64)\n        else:\n            A_list = np.zeros(0,dtype=int)\n\n        for i in A_list:\n            A_min[i] = np.min(A_n[i, :]) #find the minimum\n            A_n[i, :] = A_n[i,:] - (A_min[i] - 1)  #all values now positive so that we can work in logarithmic scale\n\n        # Augment W_nk, N_k, and c_k for q_A(x) for the observables, with one\n        # row for the specified state and I rows for the observable at that\n        # state.\n        # log weight matrix\n        msize = K + NL + S # augmented size; all of the states needed to calculate\n                           # the observables, and the observables themselves.\n        Log_W_nk = np.zeros([N, msize], np.float64) # log weight matrix\n        N_k = np.zeros([msize], np.int64)  # counts\n        f_k = np.zeros([msize], np.float64)  # free energies\n\n        # <A> = A(x_n) exp[f_{k} - q_{k}(x_n)] / \\sum_{k'=1}^K N_{k'} exp[f_{k'} - q_{k'}(x_n)]\n        # Fill in first section of matrix with existing q_k(x) from states.\n        Log_W_nk[:, 0:K] = self.Log_W_nk\n        N_k[0:K] = self.N_k\n        f_k[0:K] = self.f_k\n\n        # Pre-calculate the log denominator: Eqns 13, 14 in MBAR paper\n        states_with_samples = (self.N_k > 0)\n        log_denominator_n = logsumexp(self.f_k[states_with_samples] - self.u_kn[states_with_samples].T, b=self.N_k[states_with_samples], axis=1)\n        # Compute row of W_nk matrix for the extra states corresponding to u_ln\n        # that the state list specifies\n        for l in L_list:\n            la = K+l  #l, augmented\n            # Calculate log normalizing constants and log weights via Eqns 13, 14\n            log_C_a = -logsumexp(-u_ln[l] - log_denominator_n)\n            Log_W_nk[:, la] = log_C_a - u_ln[l] - log_denominator_n\n            f_k[la] = log_C_a\n\n        # Compute the remaining rows/columns of W_nk, and calculate\n        # their normalizing constants c_k\n        for s in range(S):\n            sa = K+NL+s  # augmented s\n            l = K + state_map[0,s]\n            i = state_map[1,s]\n            Log_W_nk[:, sa] = np.log(A_n[i, :]) + Log_W_nk[:, l]\n            f_k[sa] = -logsumexp(Log_W_nk[:, sa])\n            Log_W_nk[:, sa] += f_k[sa]    # normalize this row\n\n        # Compute estimates of A_i[s]\n        A_i = np.zeros([S], np.float64)\n        for s in range(S):\n            A_i[s] = np.exp(-f_k[K + NL + s])\n\n        # Now that covariances are computed, add the constants back to A_i that\n        # were required to enforce positivity\n        for s in range(S):\n            A_i[s] += (A_min[state_map[1,s]] - 1)\n\n        # these values may be used outside the routine, so copy back.\n        for i in A_list:\n            A_n[i, :] = A_n[i,:] + (A_min[i] - 1)\n\n        # expectations of the observables at these states\n        if S > 0:\n            result_vals['observables'] = A_i\n\n        if return_theta:\n            Theta_ij = self._computeAsymptoticCovarianceMatrix(\n                np.exp(Log_W_nk), N_k, method=uncertainty_method)\n\n            # Note: these variances will be the same whether or not we\n            # subtract a different constant from each A_i\n            # for efficency, output theta in block form\n            #          K*K   K*S  K*NL\n            # Theta =  K*S   S*S  NL*S\n            #          K*NL  NL*S NL*NL\n\n            # first the observables (S of them), then the free energies (also S of them)\n            if S>0:\n                si = K+NL+np.arange(S)\n            else:\n                si = np.zeros(0,dtype=int)\n            li = K+state_list\n            i = np.concatenate((si,li))\n            Theta = Theta_ij[np.ix_(i, i)]\n            result_vals['Theta'] = Theta\n            if S > 0:\n                # we need to return the minimum A as well\n                result_vals['Amin'] = (A_min[state_map[1,np.arange(S)]] - 1)\n\n        # free energies at these new states\n        result_vals['f'] =  f_k[K+state_list]\n\n        # Return expectations and uncertainties.\n        return result_vals", "response": "This function computes the expectations of the two phase space functions A_n and u_ln. This function is used to compute the expectation of the two phase space functions A_n and u_ln."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeCovarianceOfSums(self, d_ij, K, a):\n\n        \"\"\"\n        We wish to calculate the variance of a weighted sum of free energy differences.\n        for example ``var(\\sum a_i df_i)``.\n\n        We explicitly lay out the calculations for four variables (where each variable\n        is a logarithm of a partition function), then generalize.\n\n        The uncertainty in the sum of two weighted differences is\n\n        .. code-block:: none\n\n            var(a1(f_i1 - f_j1) + a2(f_i2 - f_j2)) =\n                a1^2 var(f_i1 - f_j1) +\n                a2^2 var(f_i2 - f_j2) +\n                2 a1 a2 cov(f_i1 - f_j1, f_i2 - f_j2)\n            cov(f_i1 - f_j1, f_i2 - f_j2) =\n                cov(f_i1,f_i2) -\n                cov(f_i1,f_j2) -\n                cov(f_j1,f_i2) +\n                cov(f_j1,f_j2)\n\n        call:\n\n        .. code-block:: none\n\n            f_i1 = a\n            f_j1 = b\n            f_i2 = c\n            f_j2 = d\n            a1^2 var(a-b) + a2^2 var(c-d) + 2a1a2 cov(a-b,c-d)\n\n        we want ``2cov(a-b,c-d) = 2cov(a,c)-2cov(a,d)-2cov(b,c)+2cov(b,d)``,\n        since ``var(x-y) = var(x) + var(y) - 2cov(x,y)``,\n        then, ``2cov(x,y) = -var(x-y) + var(x) + var(y)``. So, we get\n\n        .. code-block:: none\n\n            2cov(a,c) = -var(a-c) + var(a) + var(c)\n            -2cov(a,d) = +var(a-d) - var(a) - var(d)\n            -2cov(b,c) = +var(b-c) - var(b) - var(c)\n            2cov(b,d) = -var(b-d) + var(b) + var(d)\n\n        adding up, finally :\n\n        .. code-block:: none\n\n            2cov(a-b,c-d) = 2cov(a,c)-2cov(a,d)-2cov(b,c)+2cov(b,d) =\n                - var(a-c) + var(a-d) + var(b-c) - var(b-d)\n\n            a1^2 var(a-b)+a2^2 var(c-d)+2a1a2cov(a-b,c-d) =\n                a1^2 var(a-b)+a2^2 var(c-d)+a1a2 [-var(a-c)+var(a-d)+var(b-c)-var(b-d)]\n\n            var(a1(f_i1 - f_j1) + a2(f_i2 - f_j2)) =\n                a1^2 var(f_i1 - f_j1) + a2^2 var(f_i2 - f_j2) + 2a1 a2 cov(f_i1 - f_j1, f_i2 - f_j2)\n            = a1^2 var(f_i1 - f_j1) + a2^2 var(f_i2 - f_j2) + a1 a2 [-var(f_i1 - f_i2) + var(f_i1 - f_j2) + var(f_j1-f_i2) - var(f_j1 - f_j2)]\n\n        assume two arrays of free energy differences, and and array of constant vectors a.\n        we want the variance ``var(\\sum_k a_k (f_i,k - f_j,k))`` Each set is separated from the other by an offset K\n        same process applies with the sum, with the single var terms and the pair terms\n\n        Parameters\n        ----------\n        d_ij : a matrix of standard deviations of the quantities f_i - f_j\n        K : The number of states in each 'chunk', has to be constant\n        outputs : KxK variance matrix for the sums or differences ``\\sum a_i df_i``\n        \"\"\"\n\n        # todo: vectorize this.\n        var_ij = np.square(d_ij)\n        d2 = np.zeros([K,K],float)\n        n = len(a)\n        for i in range(K):\n            for j in range(K):\n                for k in range(n):\n                    d2[i,j] +=  a[k]**2 * var_ij[i+k*K,j+k*K]\n                    for l in range(n):\n                        d2[i,j] +=  a[k] * a[l] * (-var_ij[i+k*K,i+l*K] + var_ij[i+k*K,j+l*K] + var_ij[j+k*K,i+l*K] - var_ij[j+k*K,j+l*K])\n\n        return np.sqrt(d2)", "response": "This method calculates the variance of a weighted sum of free energy differences."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the expectation of an observable of a single phase space function.", "response": "def computeExpectations(self, A_n, u_kn=None, output='averages', state_dependent=False,\n                            compute_uncertainty=True, uncertainty_method=None,\n                            warning_cutoff=1.0e-10, return_theta=False):\n        \"\"\"Compute the expectation of an observable of a phase space function.\n\n        Compute the expectation of an observable of a single phase space\n        function A(x) at all states where potentials are generated.\n\n        Parameters\n        ----------\n        A_n : np.ndarray, float\n            A_n (N_max np float64 array) - A_n[n] = A(x_n)\n\n        u_kn : np.ndarray\n            u_kn (energies of state of interest length N)\n            default is self.u_kn\n\n        output : string, optional\n            'averages' outputs expectations of observables and 'differences' outputs\n            a matrix of differences in the observables.\n\n        compute_uncertainty : bool, optional\n            If False, the uncertainties will not be computed (default: True)\n\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method,\n            or None to use default See help for _computeAsymptoticCovarianceMatrix()\n            for more information on various methods. (default: None)\n\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        state_dependent: bool, whether the expectations are state-dependent.\n\n        Returns\n        -------\n        result_vals : dictionary\n\n        Possible keys in the result_vals dictionary:\n\n        'mu' : np.ndarray, float\n            if output is 'averages'\n            A_i  (K np float64 array) -  A_i[i] is the estimate for the expectation of A(x) for state i.\n            if output is 'differences'\n        'sigma' : np.ndarray, float\n            dA_i  (K np float64 array) - dA_i[i] is uncertainty estimate (one standard deviation) for A_i[i]\n            or\n            dA_ij (K np float64 array) - dA_ij[i,j] is uncertainty estimate (one standard deviation) for the difference in A beteen i and j\n            or None, if compute_uncertainty is False.\n        'Theta' ((KxK np float64 array): Covariance matrix of log weights\n\n        References\n        ----------\n\n        See Section IV of [1].\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> A_n = x_n\n        >>> results = mbar.computeExpectations(A_n)\n        >>> A_n = u_kn[0,:]\n        >>> results = mbar.computeExpectations(A_n, output='differences')\n        \"\"\"\n\n        dims = len(np.shape(A_n))\n\n        if dims > 2:\n            print(\"Warning: dim=3 for (state_dependent==True) matrices for observables and dim=2 for (state_dependent==False) observables are deprecated; we suggest you convert to NxK form instead of NxKxK form.\")\n\n        if not state_dependent:\n            if dims==2:\n                A_n = kn_to_n(A_n, N_k=self.N_k)\n                if u_kn is not None:\n                    if len(np.shape(u_kn)) == 3:\n                        u_kn = kln_to_kn(u_kn, N_k=self.N_k)\n                    elif len(np.shape(u_kn)) == 2:\n                        u_kn = kn_to_n(u_kn, N_k=self.N_k)\n        else:\n            if dims==3:\n                A_n = kln_to_kn(A_n, N_k=self.N_k)\n                if u_kn is not None:\n                    if len(np.shape(u_kn)) == 3:\n                        u_kn = kln_to_kn(u_kn, N_k=self.N_k)\n                    elif len(np.shape(u_kn)) == 2:\n                        u_kn = kn_to_n(u_kn, N_k=self.N_k)\n\n        if u_kn is None:\n            u_kn = self.u_kn\n\n        # Retrieve N and K for convenience.\n        N = self.N\n        ushape = np.shape(u_kn)\n        if len(ushape) == 1:\n            K = 1\n        else:\n            K = np.shape(u_kn)[0] # number of potentials provided.\n\n        state_map = np.zeros([2,K],int)\n        if state_dependent:\n            for k in range(K):\n                # first property at the first state, 2nd property at the 2nd state\n                state_map[0,k] = k\n                state_map[1,k] = k\n        else:\n            # only one property, evaluate at K different states.\n            for k in range(K):\n                state_map[0,k] = k\n                state_map[1,k] = 0\n\n        inner_results = self.computeExpectationsInner(A_n,u_kn,state_map,\n                                                      return_theta=compute_uncertainty,\n                                                      uncertainty_method=uncertainty_method,\n                                                      warning_cutoff=warning_cutoff)\n\n        result_vals = dict()\n        if compute_uncertainty or return_theta:\n            # we want the theta matrix for the exponentials of the\n            # observables, which means we need to make the\n            # transformation.\n            Adiag = np.zeros([2*K,2*K],dtype=np.float64)\n            diag = np.ones(2*K,dtype=np.float64)\n            diag[0:K] = diag[K:2*K] = inner_results['observables']-inner_results['Amin']\n            np.fill_diagonal(Adiag,diag)\n            Theta = Adiag*inner_results['Theta']*Adiag\n            covA_ij = np.array(Theta[0:K,0:K]+Theta[K:2*K,K:2*K]-Theta[0:K,K:2*K]-Theta[K:2*K,0:K])\n\n        if output == 'averages':\n            result_vals['mu'] = inner_results['observables']\n            if compute_uncertainty:\n                result_vals['sigma'] = np.sqrt(covA_ij[0:K,0:K].diagonal())\n\n        if output == 'differences':\n            A_im = np.matrix(inner_results['observables'])\n            A_ij = A_im - A_im.transpose()\n\n            result_vals['mu'] = np.array(A_ij)\n            if compute_uncertainty:\n                result_vals['sigma'] = self._ErrorOfDifferences(covA_ij,warning_cutoff=warning_cutoff)\n\n        if return_theta:\n            result_vals['Theta'] = Theta\n\n        return result_vals"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the expectations of multiple observables of phase space functions. Compute the expectations of multiple observables of phase space functions [A_0(x),A_1(x),...,A_i(x)] at a single state, along with the error in the estimates and the uncertainty in the estimates. The state is specified by the choice of u_n, which is the energy of the n samples evaluated at a the chosen state. Parameters: ------------- A_in : np.ndarray, float, shape=(I, k, N) A_in[i,n] = A_i(x_n), the value of phase observable i for configuration n at state of interest u_n : np.ndarray, float, shape=(N) u_n[n] is the reduced potential of configuration n at the state of interest compute_uncertainty : bool, optional, default=True If True, calculate the uncertainty compute_covariance : bool, optional, default=False If True, calculate the covariance uncertainty_method : string, optional Choice of method used to compute asymptotic covariance method, or None to use default See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None) warning_cutoff : float, optional Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10) Returns ------------- result_vals : dictionary Possible keys in the result_vals dictionary: 'mu' : np.ndarray, float, shape=(I) result_vals['mu'] is the estimate for the expectation of A_i(x) at the state specified by u_kn 'sigma' : np.ndarray, float, shape = (I) result_vals['sigma'] is the uncertainty in the expectation of A_state_map[i](x) at the state specified by u_n[state_map[i],:] or None if compute_uncertainty is False 'covariances' : np.ndarray, float, shape=(I, I) result_vals['covariances'] is the COVARIANCE in the estimates of A_i[i] and A_i[j]: we can't actually take a square root or None if compute_covariance is False 'Theta': np.ndarray, float, shape=(I, I), covariances of the log weights, useful for some additional calculations. Examples -------- >>> from pymbar import testsystems >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn') >>> mbar = MBAR(u_kn, N_k) >>> A_in = np.array([x_n,x_n**2,x_n**3]) >>> u_n = u_kn[0,:] >>> results = mbar.computeMultipleExpectations(A_in, u_kn)", "response": "def computeMultipleExpectations(self, A_in, u_n, compute_uncertainty=True, compute_covariance=False,\n                                    uncertainty_method=None, warning_cutoff=1.0e-10, return_theta=False):\n        \"\"\"Compute the expectations of multiple observables of phase space functions.\n\n        Compute the expectations of multiple observables of phase\n        space functions [A_0(x),A_1(x),...,A_i(x)] at a single state,\n        along with the error in the estimates and the uncertainty in\n        the estimates.  The state is specified by the choice of u_n,\n        which is the energy of the n samples evaluated at a the chosen\n        state.\n\n        \n        Parameters:\n        -------------\n\n        A_in : np.ndarray, float, shape=(I, k, N)\n            A_in[i,n] = A_i(x_n), the value of phase observable i for configuration n at state of interest\n        u_n : np.ndarray, float, shape=(N)\n            u_n[n] is the reduced potential of configuration n at the state of interest\n        compute_uncertainty : bool, optional, default=True\n            If True, calculate the uncertainty\n        compute_covariance : bool, optional, default=False\n            If True, calculate the covariance\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        Returns\n        -------------\n        result_vals : dictionary\n\n        Possible keys in the result_vals dictionary:\n\n        'mu' : np.ndarray, float, shape=(I)\n            result_vals['mu'] is the estimate for the expectation of A_i(x) at the state specified by u_kn\n        'sigma' : np.ndarray, float, shape = (I)\n            result_vals['sigma'] is the uncertainty in the expectation of A_state_map[i](x) at the state specified by u_n[state_map[i],:]\n            or None if compute_uncertainty is False\n        'covariances' : np.ndarray, float, shape=(I, I)\n            result_vals['covariances'] is the COVARIANCE in the estimates of A_i[i] and A_i[j]: we can't actually take a square root\n            or None if compute_covariance is False\n        'Theta': np.ndarray, float, shape=(I, I), covariances of the log weights, useful for some additional calculations.\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> A_in = np.array([x_n,x_n**2,x_n**3])\n        >>> u_n = u_kn[0,:]\n        >>> results = mbar.computeMultipleExpectations(A_in, u_kn)\n\n        \"\"\"\n\n        # Retrieve N and K for convenience.\n        I = A_in.shape[0]  # number of observables\n        K = self.K\n        N = self.N  # N is total number of samples\n\n        if len(np.shape(A_in)) == 3:\n            A_in_old = A_in.copy()  # convert to k by n format\n            A_in = np.zeros([I, N], np.float64)\n            for i in range(I):\n                A_in[i,:] = kn_to_n(A_in_old[i, :, :], N_k=self.N_k)\n\n        if len(np.shape(u_n)) == 2:\n            u_n = kn_to_n(u_n, N_k = self.N_k)\n\n        state_map = np.zeros([2,I],int)\n        state_map[1,:] = np.arange(I)  # same (first) state for all variables.\n\n        inner_results = self.computeExpectationsInner(A_in,u_n,state_map,\n                                                      return_theta=(compute_uncertainty or compute_covariance),\n                                                      uncertainty_method=uncertainty_method,\n                                                      warning_cutoff=warning_cutoff)\n        result_vals = dict()\n        expectations, uncertainties, covariances = None, None, None\n        result_vals['mu'] = inner_results['observables']\n\n        if compute_uncertainty or compute_covariance or return_theta:\n            Adiag = np.zeros([2*I,2*I],dtype=np.float64)\n            diag = np.ones(2*I,dtype=np.float64)\n            diag[0:I] = diag[I:2*I] = inner_results['observables']-inner_results['Amin']\n            np.fill_diagonal(Adiag,diag)\n            Theta = Adiag*inner_results['Theta']*Adiag\n            if return_theta:\n                result_vals['Theta'] = Theta\n\n            if compute_uncertainty:\n                covA_ij = np.array(Theta[0:I,0:I]+Theta[I:2*I,I:2*I]-Theta[0:I,I:2*I]-Theta[I:2*I,0:I])\n                result_vals['sigma'] = np.sqrt(covA_ij[0:I,0:I].diagonal())\n\n            if compute_covariance:\n                # compute estimate of statistical covariance of the observables\n                result_vals['covariances'] = inner_results['Theta'][0:I,0:I]\n\n        return result_vals"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the perturbed free energy differences among a set of new states.", "response": "def computePerturbedFreeEnergies(self, u_ln, compute_uncertainty=True, uncertainty_method=None, warning_cutoff=1.0e-10):\n        \"\"\"Compute the free energies for a new set of states.\n\n        Here, we desire the free energy differences among a set of new states, as well as the uncertainty estimates in these differences.\n\n        Parameters\n        ----------\n        u_ln : np.ndarray, float, shape=(L, Nmax)\n            u_ln[l,n] is the reduced potential energy of uncorrelated\n            configuration n evaluated at new state k.  Can be completely indepednent of the original number of states.\n        compute_uncertainty : bool, optional, default=True\n            If False, the uncertainties will not be computed (default: True)\n        uncertainty_method : string, optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        Returns\n        -------\n        result_vals : dictionary\n\n        Possible keys in the result_vals dictionary:\n\n        'Delta_f' : np.ndarray, float, shape=(L, L)\n            result_vals['Delta_f'] = f_j - f_i, the dimensionless free energy difference between new states i and j\n        'dDelta_f' : np.ndarray, float, shape=(L, L)\n            result_vals['dDelta_f'] is the estimated statistical uncertainty in result_vals['Delta_f']\n            or not included if `compute_uncertainty` is False\n\n        Examples\n        --------\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> results = mbar.computePerturbedFreeEnergies(u_kn)\n        \"\"\"\n\n        # Convert to np matrix.\n        u_ln = np.array(u_ln, dtype=np.float64)\n\n        # Get the dimensions of the matrix of reduced potential energies, and convert if necessary\n        if len(np.shape(u_ln)) == 3:\n            u_ln = kln_to_kn(u_ln, N_k=self.N_k)\n\n        [L, N] = u_ln.shape\n\n        # Check dimensions.\n        if (N < self.N):\n            raise DataError(\"There seems to be too few samples in u_kn. You must evaluate at the new potential with all of the samples used originally.\")\n\n        state_list = np.arange(L)   # need to get it into the correct shape\n        A_in = np.array([0])\n        inner_results = self.computeExpectationsInner(A_in, u_ln, state_list,\n                                                      return_theta=compute_uncertainty,\n                                                      uncertainty_method=uncertainty_method,\n                                                      warning_cutoff=warning_cutoff)\n\n        Deltaf_ij, dDeltaf_ij = None, None\n\n        f_k = np.matrix(inner_results['f'])\n\n        result_vals = dict()\n        result_vals['Delta_f'] = np.array(f_k - f_k.transpose())\n\n        if (compute_uncertainty):\n            result_vals['dDelta_f'] = self._ErrorOfDifferences(inner_results['Theta'],warning_cutoff=warning_cutoff)\n\n        # Return matrix of free energy differences and uncertainties.\n        return result_vals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecompose free energy differences into enthalpy and entropy differences. Compute the decomposition of the free energy difference between states 1 and N into reduced free energy differences, reduced potential (enthalpy) differences, and reduced entropy (S/k) differences. Parameters ---------- u_kn : float, NxK array The energies of the state that are being used. uncertainty_method : string , optional Choice of method used to compute asymptotic covariance method, or None to use default See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None) warning_cutoff : float, optional Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10) Returns ------- result_vals : dictionary Keys in the result_vals dictionary: 'Delta_f' : np.ndarray, float, shape=(K, K) results['Delta_f'] is the dimensionless free energy difference f_j - f_i 'dDelta_f' : np.ndarray, float, shape=(K, K) uncertainty in results['Delta_f'] 'Delta_u' : np.ndarray, float, shape=(K, K) results['Delta_u'] is the reduced potential energy difference u_j - u_i 'dDelta_u' : np.ndarray, float, shape=(K, K) uncertainty in results['Delta_u'] 'Delta_s' : np.ndarray, float, shape=(K, K) results['Delta_s'] is the reduced entropy difference S/k between states i and j (s_j - s_i) 'dDelta_s' : np.ndarray, float, shape=(K, K) uncertainty in results['Delta_s'] Examples -------- >>> from pymbar import testsystems >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn') >>> mbar = MBAR(u_kn, N_k) >>> results = mbar.computeEntropyAndEnthalpy()", "response": "def computeEntropyAndEnthalpy(self, u_kn=None, uncertainty_method=None, verbose=False, warning_cutoff=1.0e-10):\n        \"\"\"Decompose free energy differences into enthalpy and entropy differences.\n\n        Compute the decomposition of the free energy difference between\n        states 1 and N into reduced free energy differences, reduced potential\n        (enthalpy) differences, and reduced entropy (S/k) differences.\n\n        Parameters\n        ----------\n        u_kn : float, NxK array\n            The energies of the state that are being used.\n        uncertainty_method : string , optional\n            Choice of method used to compute asymptotic covariance method, or None to use default\n            See help for computeAsymptoticCovarianceMatrix() for more information on various methods. (default: None)\n        warning_cutoff : float, optional\n            Warn if squared-uncertainty is negative and larger in magnitude than this number (default: 1.0e-10)\n\n        Returns\n        -------\n        result_vals : dictionary\n\n        Keys in the result_vals dictionary:\n\n        'Delta_f' : np.ndarray, float, shape=(K, K)\n            results['Delta_f'] is the dimensionless free energy difference f_j - f_i\n        'dDelta_f' : np.ndarray, float, shape=(K, K)\n            uncertainty in results['Delta_f']\n        'Delta_u' : np.ndarray, float, shape=(K, K)\n            results['Delta_u'] is the reduced potential energy difference u_j - u_i\n        'dDelta_u' : np.ndarray, float, shape=(K, K)\n            uncertainty in results['Delta_u']\n        'Delta_s' : np.ndarray, float, shape=(K, K)\n            results['Delta_s'] is the reduced entropy difference S/k between states i and j (s_j - s_i)\n        'dDelta_s' : np.ndarray, float, shape=(K, K)\n            uncertainty in results['Delta_s']\n\n        Examples\n        --------\n\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> results = mbar.computeEntropyAndEnthalpy()\n\n        \"\"\"\n        if verbose:\n            print(\"Computing average energy and entropy by MBAR.\")\n\n        dims = len(np.shape(u_kn))\n        if dims==3:\n            u_kn = kln_to_kn(u_kn, N_k=self.N_k)\n\n        if u_kn is None:\n            u_kn = self.u_kn\n\n        # Retrieve N and K for convenience.\n        [K,N] = np.shape(u_kn)\n        A_in = u_kn.copy()\n        state_map = np.zeros([2,K],int)\n        for k in range(K):\n            state_map[0,k] = k\n            state_map[1,k] = k\n\n        inner_results = self.computeExpectationsInner(A_in, u_kn, state_map,\n                                                      return_theta=True,\n                                                      uncertainty_method=uncertainty_method,\n                                                      warning_cutoff=warning_cutoff)\n\n        # construct the covariance matrix of exp(ln c_Ua - ln c_a) - ln c_ca\n\n        Theta = np.zeros([3*K,3*K],dtype=np.float64)\n        Theta[0:2*K,0:2*K] = inner_results['Theta']\n        Theta[2*K:3*K,:] = Theta[K:2*K,:]\n        Theta[:,2*K:3*K] = Theta[:,K:2*K]\n        diag = np.ones(3*K,dtype=np.float64)\n        diag[0:K] = diag[K:2*K] = inner_results['observables']-inner_results['Amin']\n        Adiag = np.matrix(np.zeros([3*K,3*K],dtype=np.float64))\n        np.fill_diagonal(Adiag,diag)\n        Theta = Adiag*Theta*Adiag\n\n        # Compute reduced free energy difference.\n        f_k = np.matrix(inner_results['f'])\n        Delta_f_ij = np.array(f_k - f_k.transpose())\n        # compute uncertainty matrix in free energies:\n        covf = Theta[2*K:3*K,2*K:3*K]\n        dDelta_f_ij = self._ErrorOfDifferences(covf,warning_cutoff=warning_cutoff)\n\n        # Compute reduced enthalpy difference.\n        u_k = np.matrix(inner_results['observables'])\n        Delta_u_ij = np.array(u_k - u_k.transpose())\n        # compute uncertainty matrix in energies:\n        covu = Theta[0:K,0:K]+Theta[K:2*K,K:2*K]-Theta[0:K,K:2*K]-Theta[K:2*K,0:K]\n        dDelta_u_ij = self._ErrorOfDifferences(covu,warning_cutoff=warning_cutoff)\n\n        # Compute reduced entropy difference\n        s_k = u_k - f_k\n        Delta_s_ij = np.array(s_k - s_k.transpose())\n        # compute uncertainty matrix in entropies\n        #s_i = u_i - f_i\n        #cov(s_i) =   cov(u_i - f_i)\n        #         =   cov(exp(ln C_a - ln c_a) + ln c_a)\n        #         =   cov(exp(ln C_a - ln c_a), exp(ln C_a - ln c_a)) + cov(ln c_a, ln c_a)\n        #           + cov(exp(ln C_a - ln c_a), ln c_a) + cov(ln c_a, exp(ln C_a - ln c_a))\n        #         = cov(u,u) + cov(f,f)\n        #             + A cov(ln C_a - ln c_a, ln c_a) + A cov(ln c_a, ln C_a - ln c_a)\n        #         = cov(u,u) + cov(f,f)\n        #             + A cov(ln C_a, ln c_a) - A cov(ln c_a, ln c_a) + A cov(ln c_a, ln C_a) - A cov(ln c_a, ln c_a)\n        #         = cov(u,u) + cov(f,f) + A cov(ln C_a,ln c_a) + A cov(ln c_a, ln C_a) - 2A cov(ln_ca,ln_ca)\n        #\n        covs = covu + covf + Theta[0:K,2*K:3*K] + Theta[2*K:3*K,0:K] - Theta[K:2*K,2*K:3*K] - Theta[2*K:3*K,K:2*K]\n        # note: not clear that Theta[K:2*K,2*K:3*K] and Theta[K:2*K,2*K:3*K] are symmetric?\n        dDelta_s_ij = self._ErrorOfDifferences(covs,warning_cutoff=warning_cutoff)\n\n        result_vals = dict()\n        result_vals['Delta_f'] = Delta_f_ij\n        result_vals['dDelta_f'] = dDelta_f_ij\n        result_vals['Delta_u'] = Delta_u_ij\n        result_vals['dDelta_u'] = dDelta_u_ij\n        result_vals['Delta_s'] = Delta_s_ij\n        result_vals['dDelta_s'] = dDelta_s_ij\n\n        return result_vals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the free energy of occupying a number of bins. This implementation computes the expectation of an indicator-function observable for each bin. Parameters ---------- u_n : np.ndarray, float, shape=(N) u_n[n] is the reduced potential energy of snapshot n of state k for which the PMF is to be computed. bin_n : np.ndarray, float, shape=(N) bin_n[n] is the bin index of snapshot n of state k. bin_n can assume a value in range(0,nbins) nbins : int The number of bins uncertainties : string, optional Method for reporting uncertainties (default: 'from-lowest') * 'from-lowest' - the uncertainties in the free energy difference with lowest point on PMF are reported * 'from-specified' - same as from lowest, but from a user specified point * 'from-normalization' - the normalization \\sum_i p_i = 1 is used to determine uncertainties spread out through the PMF * 'all-differences' - the nbins x nbins matrix df_ij of uncertainties in free energy differences is returned instead of df_i pmf_reference : int, optional the reference state that is zeroed when uncertainty = 'from-specified' Returns ------- result_vals : dictionary Possible keys in the result_vals dictionary: 'f_i' : np.ndarray, float, shape=(K) result_vals['f_i'][i] is the dimensionless free energy of state i, relative to the state of lowest free energy 'df_i' : np.ndarray, float, shape=(K) result_vals['df_i'][i] is the uncertainty in the difference of f_i with respect to the state of lowest free energy Notes ----- * All bins must have some samples in them from at least one of the states -- this will not work if bin_n.sum(0) == 0. Empty bins should be removed before calling computePMF(). * This method works by computing the free energy of localizing the system to each bin for the given potential by aggregating the log weights for the given potential. * To estimate uncertainties, the NxK weight matrix W_nk is augmented to be Nx(K+nbins) in order to accomodate the normalized weights of states where * the potential is given by u_kn within each bin and infinite potential outside the bin. The uncertainties with respect to the bin of lowest free energy are then computed in the standard way. Examples -------- >>> # Generate some test data >>> from pymbar import testsystems >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn') >>> # Initialize MBAR on data. >>> mbar = MBAR(u_kn, N_k) >>> # Select the potential we want to compute the PMF for (here, condition 0). >>> u_n = u_kn[0, :] >>> # Sort into nbins equally-populated bins >>> nbins = 10 # number of equally-populated bins to use >>> import numpy as np >>> N_tot = N_k.sum() >>> x_n_sorted = np.sort(x_n) # unroll to n-indices >>> bins = np.append(x_n_sorted[0::int(N_tot/nbins)], x_n_sorted.max()+0.1) >>> bin_widths = bins[1:] - bins[0:-1] >>> bin_n = np.zeros(x_n.shape, np.int64) >>> bin_n = np.digitize(x_n, bins) - 1 >>> # Compute PMF for these unequally-sized bins. >>> results = mbar.computePMF(u_n, bin_n, nbins) >>> # If we want to correct for unequally-spaced bins to get a PMF on uniform measure >>> f_i_corrected = results['f_i'] - np.log(bin_widths)", "response": "def computePMF(self, u_n, bin_n, nbins, uncertainties='from-lowest', pmf_reference=None):\n        \"\"\"\n        Compute the free energy of occupying a number of bins.\n\n        This implementation computes the expectation of an indicator-function observable for each bin.\n\n        Parameters\n        ----------\n        u_n : np.ndarray, float, shape=(N)\n            u_n[n] is the reduced potential energy of snapshot n of state k for which the PMF is to be computed.\n        bin_n : np.ndarray, float, shape=(N)\n            bin_n[n] is the bin index of snapshot n of state k.  bin_n can assume a value in range(0,nbins)\n        nbins : int\n            The number of bins\n        uncertainties : string, optional\n            Method for reporting uncertainties (default: 'from-lowest')\n\n            * 'from-lowest' - the uncertainties in the free energy difference with lowest point on PMF are reported\n            * 'from-specified' - same as from lowest, but from a user specified point\n            * 'from-normalization' - the normalization \\sum_i p_i = 1 is used to determine uncertainties spread out through the PMF\n            * 'all-differences' - the nbins x nbins matrix df_ij of uncertainties in free energy differences is returned instead of df_i\n\n        pmf_reference : int, optional\n            the reference state that is zeroed when uncertainty = 'from-specified'\n\n        Returns\n        -------\n        result_vals : dictionary\n\n        Possible keys in the result_vals dictionary:\n\n        'f_i' : np.ndarray, float, shape=(K)\n            result_vals['f_i'][i] is the dimensionless free energy of state i, relative to the state of lowest free energy\n        'df_i' : np.ndarray, float, shape=(K)\n            result_vals['df_i'][i] is the uncertainty in the difference of f_i with respect to the state of lowest free energy\n\n        Notes\n        -----\n        * All bins must have some samples in them from at least one of the states -- this will not work if bin_n.sum(0) == 0. Empty bins should be removed before calling computePMF().\n        * This method works by computing the free energy of localizing the system to each bin for the given potential by aggregating the log weights for the given potential.\n        * To estimate uncertainties, the NxK weight matrix W_nk is augmented to be Nx(K+nbins) in order to accomodate the normalized weights of states where\n        * the potential is given by u_kn within each bin and infinite potential outside the bin.  The uncertainties with respect to the bin of lowest free energy are then computed in the standard way.\n\n        Examples\n        --------\n\n        >>> # Generate some test data\n        >>> from pymbar import testsystems\n        >>> (x_n, u_kn, N_k, s_n) = testsystems.HarmonicOscillatorsTestCase().sample(mode='u_kn')\n        >>> # Initialize MBAR on data.\n        >>> mbar = MBAR(u_kn, N_k)\n        >>> # Select the potential we want to compute the PMF for (here, condition 0).\n        >>> u_n = u_kn[0, :]\n        >>> # Sort into nbins equally-populated bins\n        >>> nbins = 10 # number of equally-populated bins to use\n        >>> import numpy as np\n        >>> N_tot = N_k.sum()\n        >>> x_n_sorted = np.sort(x_n) # unroll to n-indices\n        >>> bins = np.append(x_n_sorted[0::int(N_tot/nbins)], x_n_sorted.max()+0.1)\n        >>> bin_widths = bins[1:] - bins[0:-1]\n        >>> bin_n = np.zeros(x_n.shape, np.int64)\n        >>> bin_n = np.digitize(x_n, bins) - 1\n        >>> # Compute PMF for these unequally-sized bins.\n        >>> results = mbar.computePMF(u_n, bin_n, nbins)\n        >>> # If we want to correct for unequally-spaced bins to get a PMF on uniform measure\n        >>> f_i_corrected = results['f_i'] - np.log(bin_widths)\n\n        \"\"\"\n\n        # Verify that no PMF bins are empty -- we can't deal with empty bins,\n        # because the free energy is infinite.\n        for i in range(nbins):\n            if np.sum(bin_n == i) == 0:\n                raise ParameterError(\n                    \"At least one bin in provided bin_n argument has no samples.  All bins must have samples for free energies to be finite.  Adjust bin sizes or eliminate empty bins to ensure at least one sample per bin.\")\n        K = self.K\n\n        if len(np.shape(u_n)) == 2:\n            u_n = kn_to_n(u_n, N_k = self.N_k)\n\n        if len(np.shape(bin_n)) == 2:\n            bin_n = kn_to_n(bin_n, N_k = self.N_k)\n\n        # Compute unnormalized log weights for the given reduced potential\n        # u_n.\n        log_w_n = self._computeUnnormalizedLogWeights(u_n)\n\n        # Compute the free energies for these states.\n        f_i = np.zeros([nbins], np.float64)\n        df_i = np.zeros([nbins], np.float64)\n        for i in range(nbins):\n            # Get linear n-indices of samples that fall in this bin.\n            indices = np.where(bin_n == i)\n\n            # Sanity check.\n            if (len(indices) == 0):\n                raise DataError(\"WARNING: bin %d has no samples -- all bins must have at least one sample.\" % i)\n\n            # Compute dimensionless free energy of occupying state i.\n            f_i[i] = - logsumexp(log_w_n[indices])\n\n        # Compute uncertainties by forming matrix of W_nk.\n        N_k = np.zeros([self.K + nbins], np.int64)\n        N_k[0:K] = self.N_k\n        W_nk = np.zeros([self.N, self.K + nbins], np.float64)\n        W_nk[:, 0:K] = np.exp(self.Log_W_nk)\n        for i in range(nbins):\n            # Get indices of samples that fall in this bin.\n            indices = np.where(bin_n == i)\n\n            # Compute normalized weights for this state.\n            W_nk[indices, K + i] = np.exp(log_w_n[indices] + f_i[i])\n\n        # Compute asymptotic covariance matrix using specified method.\n        Theta_ij = self._computeAsymptoticCovarianceMatrix(W_nk, N_k)\n\n        # create dictionary to return results\n        result_vals = dict()\n\n        if (uncertainties == 'from-lowest') or (uncertainties == 'from-specified'):\n            # Report uncertainties in free energy difference from a given point\n            # on PMF.\n\n            if (uncertainties == 'from-lowest'):\n                # Determine bin index with lowest free energy.\n                j = f_i.argmin()\n            elif (uncertainties == 'from-specified'):\n                if pmf_reference == None:\n                    raise ParameterError(\n                        \"no reference state specified for PMF using uncertainties = from-specified\")\n                else:\n                    j = pmf_reference\n            # Compute uncertainties with respect to difference in free energy\n            # from this state j.\n            for i in range(nbins):\n                df_i[i] = math.sqrt(\n                    Theta_ij[K + i, K + i] + Theta_ij[K + j, K + j] - 2.0 * Theta_ij[K + i, K + j])\n\n            # Shift free energies so that state j has zero free energy.\n            f_i -= f_i[j]\n\n            # Return dimensionless free energy and uncertainty.\n            result_vals['f_i'] = f_i\n            result_vals['df_i'] = df_i\n\n            return result_vals\n\n        elif (uncertainties == 'all-differences'):\n            # Report uncertainties in all free energy differences.\n\n            diag = Theta_ij.diagonal()\n            dii = diag[K, K + nbins]\n            d2f_ij = dii + dii.transpose() - 2 * Theta_ij[K:K + nbins, K:K + nbins]\n\n            # unsquare uncertainties\n            df_ij = np.sqrt(d2f_ij)\n\n            # Return dimensionless free energy and uncertainty.\n            result_vals['f_i'] = f_i\n            result_vals['df_ij'] = df_ij\n\n            return result_vals\n\n        elif (uncertainties == 'from-normalization'):\n            # Determine uncertainties from normalization that \\sum_i p_i = 1.\n\n            # Compute bin probabilities p_i\n            p_i = np.exp(-f_i - logsumexp(-f_i))\n\n            # todo -- eliminate triple loop over nbins!\n            # Compute uncertainties in bin probabilities.\n            d2p_i = np.zeros([nbins], np.float64)\n            for k in range(nbins):\n                for i in range(nbins):\n                    for j in range(nbins):\n                        delta_ik = 1.0 * (i == k)\n                        delta_jk = 1.0 * (j == k)\n                        d2p_i[k] += p_i[k] * (p_i[i] - delta_ik) * p_i[\n                            k] * (p_i[j] - delta_jk) * Theta_ij[K + i, K + j]\n\n            # Transform from d2p_i to df_i\n            d2f_i = d2p_i / p_i ** 2\n            df_i = np.sqrt(d2f_i)\n\n            # return free energy and uncertainty\n            # Return dimensionless free energy and uncertainty.\n            result_vals['f_i'] = f_i\n            result_vals['df_i'] = df_i\n\n        else:\n            raise ParameterError(\"Uncertainty method '%s' not recognized.\" % uncertainties)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the statistical error matrix of the A_i and A_j of the current A_i and A_j of the current A_i.", "response": "def _ErrorOfDifferences(self, cov, warning_cutoff=1.0e-10):\n        \"\"\"\n        inputs:\n        cov is the covariance matrix of A\n\n        returns the statistical error matrix of A_i - A_j\n        \"\"\"\n\n        diag = np.matrix(cov.diagonal())\n        d2 = diag + diag.transpose() - 2 * cov\n\n        # Cast warning_cutoff to compare a negative number\n        cutoff = -abs(warning_cutoff)\n\n        # check for any numbers below zero.\n        if np.any(d2 < 0.0):\n            if np.any(d2 < cutoff):\n                print(\"A squared uncertainty is negative. Largest Magnitude = {0:f}\".format(\n                    abs(np.min(d2[d2 < cutoff]))))\n            else:\n                d2[np.logical_and(0 > d2, d2 > cutoff)] = 0.0\n        return np.sqrt(np.array(d2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the Moore - Penrose pseudoinverse", "response": "def _pseudoinverse(self, A, tol=1.0e-10):\n        \"\"\"Compute the Moore-Penrose pseudoinverse, wraps np.linalg.pinv\n\n        REQUIRED ARGUMENTS\n          A (np KxK matrix) - the square matrix whose pseudoinverse is to be computed\n\n        RETURN VALUES\n          Ainv (np KxK matrix) - the pseudoinverse\n\n        OPTIONAL VALUES\n          tol - the tolerance (relative to largest magnitude singlular value) below which singular values are to not be include in forming pseudoinverse (default: 1.0e-10)\n\n        NOTES\n          In previous versions of pymbar / Numpy, we wrote our own pseudoinverse\n          because of a bug in Numpy.\n\n        \"\"\"\n\n        return np.linalg.pinv(A, rcond=tol)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the asymptotic covariance matrix for the current state.", "response": "def _computeAsymptoticCovarianceMatrix(self, W, N_k, method=None):\n        \"\"\"Compute estimate of the asymptotic covariance matrix.\n\n        Parameters\n        ----------\n        W : np.ndarray, shape=(N, K), dtype='float'\n            The normalized weight matrix for snapshots and states.\n            W[n, k] is the weight of snapshot n in state k.\n        N_k : np.ndarray, shape=(K), dtype='int'\n            N_k[k] is the number of samples from state k.\n        method : string, optional, default=None\n            Method used to compute the asymptotic covariance matrix.\n            Must be either \"approximate\", \"svd\", or \"svd-ew\".  If None,\n            defaults to \"svd-ew\".\n\n        Returns\n        -------\n        Theta: np.ndarray, shape=(K, K), dtype='float'\n            Asymptotic covariance matrix\n\n        Notes\n        -----\n        The computational costs of the various 'method' arguments varies:\n          'svd' computes the generalized inverse using the singular value decomposition -- this should be efficient yet accurate (faster)\n          'svd-ew' is the same as 'svd', but uses the eigenvalue decomposition of W'W to bypass the need to perform an SVD (fastest)\n          'approximate' only requires multiplication of KxN and NxK matrices, but is an approximate underestimate of the uncertainty.\n\n        svd and svd-ew are described in appendix D of Shirts, 2007 JCP, while\n        \"approximate\" in Section 4 of Kong, 2003. J. R. Statist. Soc. B.\n\n        We currently recommend 'svd-ew'.\n        \"\"\"\n\n        # Set 'svd-ew' as default if uncertainty method specified as None.\n        if method == None:\n            method = 'svd-ew'\n\n        # Get dimensions of weight matrix.\n        [N, K] = W.shape\n\n        # Check dimensions\n        if(K != N_k.size):\n            raise ParameterError(\n                'W must be NxK, where N_k is a K-dimensional array.')\n        if(np.sum(N_k) != N):\n            raise ParameterError('W must be NxK, where N = sum_k N_k.')\n\n        check_w_normalized(W, N_k)\n\n        # Compute estimate of asymptotic covariance matrix using specified method.\n        if method == 'approximate':\n            # Use fast approximate expression from Kong et al. -- this underestimates the true covariance, but may be a good approximation in some cases and requires no matrix inversions\n            # Theta = P'P\n\n            # Construct matrices\n            W = np.matrix(W, dtype=np.float64)\n\n            # Compute covariance\n            Theta = W.T * W\n\n        elif method == 'svd':\n            # Use singular value decomposition based approach given in supplementary material to efficiently compute uncertainty\n            # See Appendix D.1, Eq. D4 in [1].\n\n            # Construct matrices\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            I = np.identity(K, dtype=np.float64)\n\n            # Compute SVD of W\n            [U, S, Vt] = linalg.svd(W, full_matrices=False)  # False Avoids O(N^2) memory allocation by only calculting the active subspace of U.\n            Sigma = np.matrix(np.diag(S))\n            V = np.matrix(Vt).T\n\n            # Compute covariance\n            Theta = V * Sigma * self._pseudoinverse(\n                I - Sigma * V.T * Ndiag * V * Sigma) * Sigma * V.T\n\n        elif method == 'svd-ew':\n            # Use singular value decomposition based approach given in supplementary material to efficiently compute uncertainty\n            # The eigenvalue decomposition of W'W is used to forego computing the SVD.\n            # See Appendix D.1, Eqs. D4 and D5 of [1].\n\n            # Construct matrices\n            Ndiag = np.matrix(np.diag(N_k), dtype=np.float64)\n            W = np.matrix(W, dtype=np.float64)\n            I = np.identity(K, dtype=np.float64)\n\n            # Compute singular values and right singular vectors of W without using SVD\n            # Instead, we compute eigenvalues and eigenvectors of W'W.\n            # Note W'W = (U S V')'(U S V') = V S' U' U S V' = V (S'S) V'\n            [S2, V] = linalg.eigh(W.T * W)\n            # Set any slightly negative eigenvalues to zero.\n            S2[np.where(S2 < 0.0)] = 0.0\n            # Form matrix of singular values Sigma, and V.\n            Sigma = np.matrix(np.diag(np.sqrt(S2)))\n            V = np.matrix(V)\n\n            # Compute covariance\n            Theta = V * Sigma * self._pseudoinverse(\n                I - Sigma * V.T * Ndiag * V * Sigma) * Sigma * V.T\n\n        else:\n            # Raise an exception.\n            raise ParameterError('Method ' + method + ' unrecognized.')\n\n        return Theta"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the unnormalized log weights of the set of states at a given set of unnormalized potential energies at a single state.", "response": "def _computeUnnormalizedLogWeights(self, u_n):\n        \"\"\"\n        Return unnormalized log weights.\n\n        REQUIRED ARGUMENTS\n          u_n (N np float64 array) - reduced potential energies at single state\n\n        OPTIONAL ARGUMENTS\n\n        RETURN VALUES\n          log_w_n (N array) - unnormalized log weights of each of a number of states\n\n        REFERENCE\n          'log weights' here refers to \\log [ \\sum_{k=1}^K N_k exp[f_k - (u_k(x_n) - u(x_n)] ]\n        \"\"\"\n        return -1. * logsumexp(self.f_k + u_n[:, np.newaxis] - self.u_kn.T, b=self.N_k, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking types and return inputs for MBAR calculations.", "response": "def validate_inputs(u_kn, N_k, f_k):\n    \"\"\"Check types and return inputs for MBAR calculations.\n\n    Parameters\n    ----------\n    u_kn or q_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies or unnormalized probabilities\n    N_k : np.ndarray, shape=(n_states), dtype='int'\n        The number of samples in each state\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies of each state\n\n    Returns\n    -------\n    u_kn or q_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies or unnormalized probabilities\n    N_k : np.ndarray, shape=(n_states), dtype='float'\n        The number of samples in each state.  Converted to float because this cast is required when log is calculated.\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies of each state\n    \"\"\"\n    n_states, n_samples = u_kn.shape\n\n    u_kn = ensure_type(u_kn, 'float', 2, \"u_kn or Q_kn\", shape=(n_states, n_samples))\n    N_k = ensure_type(N_k, 'float', 1, \"N_k\", shape=(n_states,), warn_on_cast=False)  # Autocast to float because will be eventually used in float calculations.\n    f_k = ensure_type(f_k, 'float', 1, \"f_k\", shape=(n_states,))\n\n    return u_kn, N_k, f_k"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an improved guess for the dimensionless free energies Parameters ---------- u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float' The reduced potential energies, i.e. -log unnormalized probabilities N_k : np.ndarray, shape=(n_states), dtype='int' The number of samples in each state f_k : np.ndarray, shape=(n_states), dtype='float' The reduced free energies of each state Returns ------- f_k : np.ndarray, shape=(n_states), dtype='float' Updated estimate of f_k Notes ----- Equation C3 in MBAR JCP paper.", "response": "def self_consistent_update(u_kn, N_k, f_k):\n    \"\"\"Return an improved guess for the dimensionless free energies\n\n    Parameters\n    ----------\n    u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies, i.e. -log unnormalized probabilities\n    N_k : np.ndarray, shape=(n_states), dtype='int'\n        The number of samples in each state\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies of each state\n\n    Returns\n    -------\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        Updated estimate of f_k\n\n    Notes\n    -----\n    Equation C3 in MBAR JCP paper.\n    \"\"\"\n\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n    \n    states_with_samples = (N_k > 0)\n\n    # Only the states with samples can contribute to the denominator term.\n    log_denominator_n = logsumexp(f_k[states_with_samples] - u_kn[states_with_samples].T, b=N_k[states_with_samples], axis=1)\n    \n    # All states can contribute to the numerator term.\n    return -1. * logsumexp(-log_denominator_n - u_kn, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mbar_gradient(u_kn, N_k, f_k):\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n\n    log_denominator_n = logsumexp(f_k - u_kn.T, b=N_k, axis=1)\n    log_numerator_k = logsumexp(-log_denominator_n - u_kn, axis=1)\n    return -1 * N_k * (1.0 - np.exp(f_k + log_numerator_k))", "response": "Gradient of MBAR objective function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate both objective function and gradient for MBAR.", "response": "def mbar_objective_and_gradient(u_kn, N_k, f_k):\n    \"\"\"Calculates both objective function and gradient for MBAR.\n\n    Parameters\n    ----------\n    u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies, i.e. -log unnormalized probabilities\n    N_k : np.ndarray, shape=(n_states), dtype='int'\n        The number of samples in each state\n    f_k : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies of each state\n\n\n    Returns\n    -------\n    obj : float\n        Objective function\n    grad : np.ndarray, dtype=float, shape=(n_states)\n        Gradient of objective function\n\n    Notes\n    -----\n    This objective function is essentially a doubly-summed partition function and is\n    quite sensitive to precision loss from both overflow and underflow. For optimal\n    results, u_kn can be preconditioned by subtracting out a `n` dependent\n    vector.\n\n    More optimal precision, the objective function uses math.fsum for the\n    outermost sum and logsumexp for the inner sum.\n    \n    The gradient is equation C6 in the JCP MBAR paper; the objective\n    function is its integral.\n    \"\"\"\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n\n    log_denominator_n = logsumexp(f_k - u_kn.T, b=N_k, axis=1)\n    log_numerator_k = logsumexp(-log_denominator_n - u_kn, axis=1)\n    grad = -1 * N_k * (1.0 - np.exp(f_k + log_numerator_k))\n\n    obj = math.fsum(log_denominator_n) - N_k.dot(f_k)\n\n    return obj, grad"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mbar_hessian(u_kn, N_k, f_k):\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n\n    W = mbar_W_nk(u_kn, N_k, f_k)\n\n    H = W.T.dot(W)\n    H *= N_k\n    H *= N_k[:, np.newaxis]\n    H -= np.diag(W.sum(0) * N_k)\n\n    return -1.0 * H", "response": "Hessian of MBAR objective function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mbar_log_W_nk(u_kn, N_k, f_k):\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n\n    log_denominator_n = logsumexp(f_k - u_kn.T, b=N_k, axis=1)\n    logW = f_k - u_kn.T - log_denominator_n[:, np.newaxis]\n    return logW", "response": "Calculate the log weight matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mbar_W_nk(u_kn, N_k, f_k):\n    return np.exp(mbar_log_W_nk(u_kn, N_k, f_k))", "response": "Calculate the weight matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef precondition_u_kn(u_kn, N_k, f_k):\n    u_kn, N_k, f_k = validate_inputs(u_kn, N_k, f_k)\n    u_kn = u_kn - u_kn.min(0)\n    u_kn += (logsumexp(f_k - u_kn.T, b=N_k, axis=1)) - N_k.dot(f_k) / float(N_k.sum())\n    return u_kn", "response": "Subtraction of a sample - dependent constant from u_kn to improve precision in MBAR objective function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef solve_mbar_once(u_kn_nonzero, N_k_nonzero, f_k_nonzero, method=\"hybr\", tol=1E-12, options=None):\n    u_kn_nonzero, N_k_nonzero, f_k_nonzero = validate_inputs(u_kn_nonzero, N_k_nonzero, f_k_nonzero)\n    f_k_nonzero = f_k_nonzero - f_k_nonzero[0]  # Work with reduced dimensions with f_k[0] := 0\n    u_kn_nonzero = precondition_u_kn(u_kn_nonzero, N_k_nonzero, f_k_nonzero)\n\n    pad = lambda x: np.pad(x, (1, 0), mode='constant')  # Helper function inserts zero before first element\n    unpad_second_arg = lambda obj, grad: (obj, grad[1:])  # Helper function drops first element of gradient\n\n    # Create objective functions / nonlinear equations to send to scipy.optimize, fixing f_0 = 0\n    grad = lambda x: mbar_gradient(u_kn_nonzero, N_k_nonzero, pad(x))[1:]  # Objective function gradient\n    grad_and_obj = lambda x: unpad_second_arg(*mbar_objective_and_gradient(u_kn_nonzero, N_k_nonzero, pad(x)))  # Objective function gradient and objective function\n    hess = lambda x: mbar_hessian(u_kn_nonzero, N_k_nonzero, pad(x))[1:][:, 1:]  # Hessian of objective function\n\n    with warnings.catch_warnings(record=True) as w:\n        if method in [\"L-BFGS-B\", \"dogleg\", \"CG\", \"BFGS\", \"Newton-CG\", \"TNC\", \"trust-ncg\", \"SLSQP\"]:\n            if method in [\"L-BFGS-B\", \"CG\"]:\n                hess = None  # To suppress warning from passing a hessian function.\n            results = scipy.optimize.minimize(grad_and_obj, f_k_nonzero[1:], jac=True, hess=hess, method=method, tol=tol, options=options)\n            f_k_nonzero = pad(results[\"x\"])\n        elif method == 'adaptive':\n            results = adaptive(u_kn_nonzero, N_k_nonzero, f_k_nonzero, tol=tol, options=options)\n            f_k_nonzero = results # they are the same for adaptive, until we decide to return more.\n        else:\n            results = scipy.optimize.root(grad, f_k_nonzero[1:], jac=hess, method=method, tol=tol, options=options)\n            f_k_nonzero = pad(results[\"x\"])\n\n    # If there were runtime warnings, show the messages\n    if len(w) > 0:\n        can_ignore = True\n        for warn_msg in w:\n            if \"Unknown solver options\" in str(warn_msg.message):\n                continue\n            warnings.showwarning(warn_msg.message, warn_msg.category,\n                                 warn_msg.filename, warn_msg.lineno, warn_msg.file, \"\")\n            can_ignore = False  # If any warning is not just unknown options, can ]not skip check\n        if not can_ignore:\n            # Ensure MBAR solved correctly\n            w_nk_check = mbar_W_nk(u_kn_nonzero, N_k_nonzero, f_k_nonzero)\n            check_w_normalized(w_nk_check, N_k_nonzero)\n            print(\"MBAR weights converged within tolerance, despite the SciPy Warnings. Please validate your results.\")\n\n    return f_k_nonzero, results", "response": "Solve MBAR self - consistent equations using some form of equation solver."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve MBAR self - consistent equations using some sequence of equations solvers.", "response": "def solve_mbar(u_kn_nonzero, N_k_nonzero, f_k_nonzero, solver_protocol=None):\n    \"\"\"Solve MBAR self-consistent equations using some sequence of equation solvers.\n\n    Parameters\n    ----------\n    u_kn_nonzero : np.ndarray, shape=(n_states, n_samples), dtype='float'\n        The reduced potential energies, i.e. -log unnormalized probabilities\n        for the nonempty states\n    N_k_nonzero : np.ndarray, shape=(n_states), dtype='int'\n        The number of samples in each state for the nonempty states\n    f_k_nonzero : np.ndarray, shape=(n_states), dtype='float'\n        The reduced free energies for the nonempty states\n    solver_protocol: tuple(dict()), optional, default=None\n        Optional list of dictionaries of steps in solver protocol.\n        If None, a default protocol will be used.\n\n    Returns\n    -------\n    f_k : np.ndarray\n        The converged reduced free energies.\n    all_results : list(dict())\n        List of results from each step of solver_protocol.  Each element in\n        list contains the results dictionary from solve_mbar_once()\n        for the corresponding step.\n\n    Notes\n    -----\n    This function requires that N_k_nonzero > 0--that is, you should have\n    already dropped all the states for which you have no samples.\n    Internally, this function works in a reduced coordinate system defined\n    by subtracting off the first component of f_k and fixing that component\n    to be zero.\n\n    This function calls `solve_mbar_once()` multiple times to achieve\n    converged results.  Generally, a single call to solve_mbar_once()\n    will not give fully converged answers because of limited numerical precision.\n    Each call to `solve_mbar_once()` re-conditions the nonlinear\n    equations using the current guess.\n    \"\"\"\n    if solver_protocol is None:\n        solver_protocol = DEFAULT_SOLVER_PROTOCOL\n    for protocol in solver_protocol:\n        if protocol['method'] is None:\n            protocol['method'] = DEFAULT_SOLVER_METHOD\n\n    all_results = []\n    for k, options in enumerate(solver_protocol):\n        f_k_nonzero, results = solve_mbar_once(u_kn_nonzero, N_k_nonzero, f_k_nonzero, **options)\n        all_results.append(results)\n        all_results.append((\"Final gradient norm: %.3g\" % np.linalg.norm(mbar_gradient(u_kn_nonzero, N_k_nonzero, f_k_nonzero))))\n    return f_k_nonzero, all_results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef solve_mbar_for_all_states(u_kn, N_k, f_k, solver_protocol):\n    states_with_samples = np.where(N_k > 0)[0]\n\n    if len(states_with_samples) == 1:\n        f_k_nonzero = np.array([0.0])\n    else:\n        f_k_nonzero, all_results = solve_mbar(u_kn[states_with_samples], N_k[states_with_samples],\n                                              f_k[states_with_samples], solver_protocol=solver_protocol)\n\n    f_k[states_with_samples] = f_k_nonzero\n\n    # Update all free energies because those from states with zero samples are not correctly computed by solvers.\n    f_k = self_consistent_update(u_kn, N_k, f_k)\n    # This is necessary because state 0 might have had zero samples,\n    # but we still want that state to be the reference with free energy 0.\n    f_k -= f_k[0]\n\n    return f_k", "response": "Solve for free energies of states with samples then calculate for all states."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a non - uniform histogram using bins of equal size.", "response": "def construct_nonuniform_bins(x_n, nbins):\n    \"\"\"Construct histogram using bins of unequal size to ensure approximately equal population in each bin.\n\n    ARGUMENTS\n      x_n (1D numpy array of floats) - x_n[n] is data point n\n\n    RETURN VALUES\n      bin_left_boundary_i (1D numpy array of floats) - data in bin i will satisfy bin_left_boundary_i[i] <= x < bin_left_boundary_i[i+1]\n      bin_center_i (1D numpy array of floats) - bin_center_i[i] is the center of bin i\n      bin_width_i (1D numpy array of floats) - bin_width_i[i] is the width of bin i\n      bin_n (1D numpy array of int32) - bin_n[n] is the bin index (in range(nbins)) of x_n[n]\n    \"\"\"\n\n    # Determine number of samples.\n    N = x_n.size\n\n    # Get indices of elements of x_n sorted in order.\n    sorted_indices = x_n.argsort()\n\n    # Allocate storage for results.\n    bin_left_boundary_i = zeros([nbins+1], float64) \n    bin_right_boundary_i = zeros([nbins+1], float64)\n    bin_center_i = zeros([nbins], float64)\n    bin_width_i = zeros([nbins], float64)\n    bin_n = zeros([N], int32)\n    \n    # Determine sampled range, adding a little bit to the rightmost range to ensure no samples escape the range.\n    x_min = x_n.min()\n    x_max = x_n.max()\n    x_max += (x_max - x_min) * 1.0e-5\n\n    # Determine bin boundaries and bin assignments.\n    for bin_index in range(nbins):\n        # indices of first and last data points in this span\n        first_index = int(float(N) / float(nbins) * float(bin_index))\n        last_index = int(float(N) / float(nbins) * float(bin_index+1))\n\n        # store left bin boundary\n        bin_left_boundary_i[bin_index] = x_n[sorted_indices[first_index]]\n\n        # store assignments\n        bin_n[sorted_indices[first_index:last_index]] = bin_index                   \n\n    # set rightmost boundary\n    bin_left_boundary_i[nbins] = x_max\n    \n    # Determine bin centers and widths\n    for bin_index in range(nbins):\n        bin_center_i[bin_index] = (bin_left_boundary_i[bin_index] + bin_left_boundary_i[bin_index+1]) / 2.0\n        bin_width_i[bin_index] = (bin_left_boundary_i[bin_index+1] - bin_left_boundary_i[bin_index])\n\n    # DEBUG\n#    outfile = open('states.out', 'w')\n#    for n in range(N):\n#        outfile.write('%8f %8d\\n' % (x_n[n], bin_n[n]))\n#    outfile.close()\n        \n    return (bin_left_boundary_i, bin_center_i, bin_width_i, bin_n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef statisticalInefficiency(A_n, B_n=None, fast=False, mintime=3, fft=False):\n\n    # Create numpy copies of input arguments.\n    A_n = np.array(A_n)\n\n    if fft and B_n is None:\n        return statisticalInefficiency_fft(A_n, mintime=mintime)    \n\n    if B_n is not None:\n        B_n = np.array(B_n)\n    else:\n        B_n = np.array(A_n)\n\n    # Get the length of the timeseries.\n    N = A_n.size\n\n    # Be sure A_n and B_n have the same dimensions.\n    if(A_n.shape != B_n.shape):\n        raise ParameterError('A_n and B_n must have same dimensions.')\n\n    # Initialize statistical inefficiency estimate with uncorrelated value.\n    g = 1.0\n\n    # Compute mean of each timeseries.\n    mu_A = A_n.mean()\n    mu_B = B_n.mean()\n\n    # Make temporary copies of fluctuation from mean.\n    dA_n = A_n.astype(np.float64) - mu_A\n    dB_n = B_n.astype(np.float64) - mu_B\n\n    # Compute estimator of covariance of (A,B) using estimator that will ensure C(0) = 1.\n    sigma2_AB = (dA_n * dB_n).mean()  # standard estimator to ensure C(0) = 1\n\n    # Trap the case where this covariance is zero, and we cannot proceed.\n    if(sigma2_AB == 0):\n        raise ParameterError('Sample covariance sigma_AB^2 = 0 -- cannot compute statistical inefficiency')\n\n    # Accumulate the integrated correlation time by computing the normalized correlation time at\n    # increasing values of t.  Stop accumulating if the correlation function goes negative, since\n    # this is unlikely to occur unless the correlation function has decayed to the point where it\n    # is dominated by noise and indistinguishable from zero.\n    t = 1\n    increment = 1\n    while (t < N - 1):\n\n        # compute normalized fluctuation correlation function at time t\n        C = np.sum(dA_n[0:(N - t)] * dB_n[t:N] + dB_n[0:(N - t)] * dA_n[t:N]) / (2.0 * float(N - t) * sigma2_AB)\n        # Terminate if the correlation function has crossed zero and we've computed the correlation\n        # function at least out to 'mintime'.\n        if (C <= 0.0) and (t > mintime):\n            break\n\n        # Accumulate contribution to the statistical inefficiency.\n        g += 2.0 * C * (1.0 - float(t) / float(N)) * float(increment)\n\n        # Increment t and the amount by which we increment t.\n        t += increment\n\n        # Increase the interval if \"fast mode\" is on.\n        if fast:\n            increment += 1\n\n    # g must be at least unity\n    if (g < 1.0):\n        g = 1.0\n\n    # Return the computed statistical inefficiency.\n    return g", "response": "Compute the statistical inefficiency of two timeseries."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates the statistical inefficiency from multiple stationary time series.", "response": "def statisticalInefficiencyMultiple(A_kn, fast=False, return_correlation_function=False):\n    \"\"\"Estimate the statistical inefficiency from multiple stationary timeseries (of potentially differing lengths).\n\n    Parameters\n    ----------\n    A_kn : list of np.ndarrays\n        A_kn[k] is the kth timeseries, and A_kn[k][n] is nth value of timeseries k.  Length is deduced from arrays.\n\n    fast : bool, optional, default=False\n        f True, will use faster (but less accurate) method to estimate correlation\n        time, described in Ref. [1] (default: False)\n    return_correlation_function : bool, optional, default=False\n        if True, will also return estimates of normalized fluctuation correlation function that were computed (default: False)\n\n    Returns\n    -------\n    g : np.ndarray,\n        g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n        We enforce g >= 1.0.\n    Ct : list (of tuples)\n        Ct[n] = (t, C) with time t and normalized correlation function estimate C is returned as well if return_correlation_function is set to True\n\n    Notes\n    -----\n    The autocorrelation of the timeseries is used to compute the statistical inefficiency.\n    The normalized fluctuation autocorrelation function is computed by averaging the unnormalized raw correlation functions.\n    The fast method described in Ref [1] is used to compute g.\n\n    References\n    ----------\n    [1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n        histogram analysis method for the analysis of simulated and parallel tempering simulations.\n        JCTC 3(1):26-41, 2007.\n\n    Examples\n    --------\n\n    Estimate statistical efficiency from multiple timeseries of different lengths.\n\n    >>> from pymbar import testsystems\n    >>> N_k = [1000, 2000, 3000, 4000, 5000]\n    >>> tau = 5.0 # exponential relaxation time\n    >>> A_kn = [ testsystems.correlated_timeseries_example(N=N, tau=tau) for N in N_k ]\n    >>> g = statisticalInefficiencyMultiple(A_kn)\n\n    Also return the values of the normalized fluctuation autocorrelation function that were computed.\n\n    >>> [g, Ct] = statisticalInefficiencyMultiple(A_kn, return_correlation_function=True)\n\n    \"\"\"\n\n    # Convert A_kn into a list of arrays if it is not in this form already.\n    if (type(A_kn) == np.ndarray):\n        A_kn_list = list()\n        if A_kn.ndim == 1:\n            A_kn_list.append(A_kn.copy())\n        else:\n            [K, N] = A_kn.shape\n            for k in range(K):\n                A_kn_list.append(A_kn[k, :].copy())\n        A_kn = A_kn_list\n\n    # Determine number of timeseries.\n    K = len(A_kn)\n\n    # Get the length of each timeseries.\n    N_k = np.zeros([K], np.int32)\n    for k in range(K):\n        N_k[k] = A_kn[k].size\n\n    # Compute average timeseries length.\n    Navg = np.array(N_k, np.float64).mean()\n\n    # Determine total number of samples.\n    N = np.sum(N_k)\n\n    # Initialize statistical inefficiency estimate with uncorrelated value.\n    g = 1.0\n\n    # Compute sample mean.\n    mu = 0.0\n    for k in range(K):\n        mu += np.sum(A_kn[k])\n    mu /= float(N)\n\n    # Construct and store fluctuation timeseries.\n    dA_kn = list()\n    for k in range(K):\n        dA_n = A_kn[k] - mu\n        dA_kn.append(dA_n.copy())\n\n    # Compute sample variance from mean of squared fluctuations, to ensure that C(0) = 1.\n    sigma2 = 0.0\n    for k in range(K):\n        sigma2 += np.sum(dA_kn[k] ** 2)\n    sigma2 /= float(N)\n\n    # Initialize statistical inefficiency estimate with uncorrelated value.\n    g = 1.0\n\n    # Initialize storage for correlation function.\n    Ct = list()  # Ct[n] is a tuple (t, C) of the time lag t and estimate of normalized fluctuation correlation function C\n\n    # Accumulate the integrated correlation time by computing the normalized correlation time at\n    # increasing values of t.  Stop accumulating if the correlation function goes negative, since\n    # this is unlikely to occur unless the correlation function has decayed to the point where it\n    # is dominated by noise and indistinguishable from zero.\n    t = 1\n    increment = 1\n    while (t < N_k.max() - 1):\n        # compute unnormalized correlation function\n        numerator = 0.0\n        denominator = 0.0\n        for k in range(K):\n            if (t >= N_k[k]):\n                continue  # skip trajectory if lag time t is greater than its length\n            dA_n = dA_kn[k]  # retrieve trajectory\n            x = dA_n[0:(N_k[k] - t)] * dA_n[t:N_k[k]]\n            numerator += np.sum(x)  # accumulate contribution from trajectory k\n            denominator += float(x.size)  # count how many overlapping time segments we've included\n\n        C = numerator / denominator\n\n        # compute normalized fluctuation correlation function at time t\n        C = C / sigma2\n        # print \"C[%5d] = %16f (%16f / %16f)\" % (t, C, numerator, denominator)\n\n        # Store estimate of correlation function.\n        Ct.append((t, C))\n\n        # Terminate if the correlation function has crossed zero.\n        # Note that we've added a hack (t > 10) condition to avoid terminating too early in correlation functions that have a strong negative peak at\n        if (C <= 0.0) and (t > 10):\n            break\n\n        # Accumulate contribution to the statistical inefficiency.\n        g += 2.0 * C * (1.0 - float(t) / Navg) * float(increment)\n\n        # Increment t and the amount by which we increment t.\n        t += increment\n\n        # Increase the interval if \"fast mode\" is on.\n        if fast:\n            increment += 1\n\n    # g must be at least unity\n    if (g < 1.0):\n        g = 1.0\n\n    # Return statistical inefficency and correlation function estimate, if requested.\n    if return_correlation_function:\n        return (g, Ct)\n\n    # Return the computed statistical inefficiency.\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the integrated autocorrelation time.", "response": "def integratedAutocorrelationTime(A_n, B_n=None, fast=False, mintime=3):\n    \"\"\"Estimate the integrated autocorrelation time.\n\n    See Also\n    --------\n    statisticalInefficiency\n\n    \"\"\"\n\n    g = statisticalInefficiency(A_n, B_n, fast, mintime)\n    tau = (g - 1.0) / 2.0\n    return tau"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nestimating the integrated autocorrelation time from multiple timeseries.", "response": "def integratedAutocorrelationTimeMultiple(A_kn, fast=False):\n    \"\"\"Estimate the integrated autocorrelation time from multiple timeseries.\n\n    See Also\n    --------\n    statisticalInefficiencyMultiple\n\n    \"\"\"\n\n    g = statisticalInefficiencyMultiple(A_kn, fast, False)\n    tau = (g - 1.0) / 2.0\n    return tau"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the normalized fluctuation auto - or cross - correlation function for two stationary time series.", "response": "def normalizedFluctuationCorrelationFunction(A_n, B_n=None, N_max=None, norm=True):\n    \"\"\"Compute the normalized fluctuation (cross) correlation function of (two) stationary timeseries.\n\n    C(t) = (<A(t) B(t)> - <A><B>) / (<AB> - <A><B>)\n\n    This may be useful in diagnosing odd time-correlations in timeseries data.\n\n    Parameters\n    ----------\n    A_n : np.ndarray\n        A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n    B_n : np.ndarray\n        B_n[n] is nth value of timeseries B.  Length is deduced from vector.\n    N_max : int, default=None\n        if specified, will only compute correlation function out to time lag of N_max\n    norm: bool, optional, default=True\n        if False will return the unnormalized correlation function D(t) = <A(t) B(t)>\n\n    Returns\n    -------\n    C_n : np.ndarray\n        C_n[n] is the normalized fluctuation auto- or cross-correlation function for timeseries A(t) and B(t).\n\n    Notes\n    -----\n    The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n    This procedure may be slow.\n    The statistical error in C_n[n] will grow with increasing n.  No effort is made here to estimate the uncertainty.\n\n    References\n    ----------\n    [1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n    histogram analysis method for the analysis of simulated and parallel tempering simulations.\n    JCTC 3(1):26-41, 2007.\n\n    Examples\n    --------\n\n    Estimate normalized fluctuation correlation function.\n\n    >>> from pymbar import testsystems\n    >>> A_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0)\n    >>> C_t = normalizedFluctuationCorrelationFunction(A_t, N_max=25)\n\n    \"\"\"\n\n    # If B_n is not specified, set it to be identical to A_n.\n    if B_n is None:\n        B_n = A_n\n\n    # Create np copies of input arguments.\n    A_n = np.array(A_n)\n    B_n = np.array(B_n)\n\n    # Get the length of the timeseries.\n    N = A_n.size\n\n    # Set maximum time to compute correlation functon for.\n    if (not N_max) or (N_max > N - 1):\n        N_max = N - 1\n\n    # Be sure A_n and B_n have the same dimensions.\n    if(A_n.shape != B_n.shape):\n        raise ParameterError('A_n and B_n must have same dimensions.')\n\n    # Initialize statistical inefficiency estimate with uncorrelated value.\n    g = 1.0\n\n    # Compute means and variance.\n    mu_A = A_n.mean()\n    mu_B = B_n.mean()\n\n    # Make temporary copies at high precision with means subtracted off.\n    dA_n = A_n.astype(np.float64) - mu_A\n    dB_n = B_n.astype(np.float64) - mu_B\n\n    # sigma2_AB = sum((A_n-mu_A) * (B_n-mu_B)) / (float(N)-1.0) # unbiased estimator\n    sigma2_AB = (dA_n * dB_n).mean()  # standard estimator to ensure C(0) = 1\n    if(sigma2_AB == 0):\n        raise ParameterError('Sample covariance sigma_AB^2 = 0 -- cannot compute statistical inefficiency')\n\n    # allocate storage for normalized fluctuation correlation function\n    C_n = np.zeros([N_max + 1], np.float64)\n\n    # Compute normalized correlation function.\n    t = 0\n    for t in range(0, N_max + 1):\n        # compute normalized fluctuation correlation function at time t\n        C_n[t] = np.sum(dA_n[0:(N - t)] * dB_n[t:N] + dB_n[0:(N - t)] * dA_n[t:N]) / (2.0 * float(N - t) * sigma2_AB)\n\n    # Return the computed correlation function\n    if norm:\n        return C_n\n    else:\n        return C_n*sigma2_AB + mu_A*mu_B"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the normalized fluctuation auto - or cross - correlation function of two timeseries samples.", "response": "def normalizedFluctuationCorrelationFunctionMultiple(A_kn, B_kn=None, N_max=None, norm=True, truncate=False):\n    \"\"\"Compute the normalized fluctuation (cross) correlation function of (two) timeseries from multiple timeseries samples.\n\n    C(t) = (<A(t) B(t)> - <A><B>) / (<AB> - <A><B>)\n    This may be useful in diagnosing odd time-correlations in timeseries data.\n\n    Parameters\n    ----------\n    A_kn : Python list of numpy arrays\n        A_kn[k] is the kth timeseries, and A_kn[k][n] is nth value of timeseries k.  Length is deduced from arrays.\n    B_kn : Python list of numpy arrays\n        B_kn[k] is the kth timeseries, and B_kn[k][n] is nth value of timeseries k.  B_kn[k] must have same length as A_kn[k]\n    N_max : int, optional, default=None\n        if specified, will only compute correlation function out to time lag of N_max\n    norm: bool, optional, default=True\n        if False, will return unnormalized D(t) = <A(t) B(t)>\n    truncate: bool, optional, default=False\n        if True, will stop calculating the correlation function when it goes below 0\n\n    Returns\n    -------\n    C_n[n] : np.ndarray\n        The normalized fluctuation auto- or cross-correlation function for timeseries A(t) and B(t).\n\n    Notes\n    -----\n    The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n    This procedure may be slow.\n    The statistical error in C_n[n] will grow with increasing n.  No effort is made here to estimate the uncertainty.\n\n    References\n    ----------\n    [1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n    histogram analysis method for the analysis of simulated and parallel tempering simulations.\n    JCTC 3(1):26-41, 2007.\n\n    Examples\n    --------\n\n    Estimate a portion of the normalized fluctuation autocorrelation function from multiple timeseries of different length.\n\n    >>> from pymbar import testsystems\n    >>> N_k = [1000, 2000, 3000, 4000, 5000]\n    >>> tau = 5.0 # exponential relaxation time\n    >>> A_kn = [ testsystems.correlated_timeseries_example(N=N, tau=tau) for N in N_k ]\n    >>> C_n = normalizedFluctuationCorrelationFunctionMultiple(A_kn, N_max=25)\n\n    \"\"\"\n\n    # If B_kn is not specified, define it to be identical with A_kn.\n    if B_kn is None:\n        B_kn = A_kn\n\n    # TODO: Change this to support other iterable types, like sets.\n    # Make sure A_kn and B_kn are both lists\n    if (type(A_kn) is not list) or (type(B_kn) is not list):\n        raise ParameterError(\"A_kn and B_kn must each be a list of numpy arrays.\")\n\n    # Ensure the same number of timeseries are stored in A_kn and B_kn.\n    if (len(A_kn) != len(B_kn)):\n        raise ParameterError(\"A_kn and B_kn must contain corresponding timeseries -- different numbers of timeseries detected in each.\")\n\n    # Determine number of timeseries stored.\n    K = len(A_kn)\n\n    # Ensure both observable trajectories in each timeseries are of the same length.\n    for k in range(K):\n        A_n = A_kn[k]\n        B_n = B_kn[k]\n        if A_n.size != B_n.size:\n            raise ParameterError(\"A_kn and B_kn must contain corresponding timeseries -- lack of correspondence in timeseries lenghts detected.\")\n\n    # Get the length of each timeseries.\n    N_k = np.zeros([K], np.int32)\n    for k in range(K):\n        N_k[k] = A_kn[k].size\n\n    # Determine total number of samples.\n    N = np.sum(N_k)\n\n    # Set maximum time to compute correlation functon for.\n    if (not N_max) or (N_max > max(N_k) - 1):\n        N_max = max(N_k) - 1\n\n    # Compute means.\n    mu_A = 0.0\n    mu_B = 0.0\n    for k in range(K):\n        mu_A += np.sum(A_kn[k])\n        mu_B += np.sum(B_kn[k])\n    mu_A /= float(N)\n    mu_B /= float(N)\n\n    # Compute fluctuation timeseries.\n    dA_kn = list()\n    dB_kn = list()\n    for k in range(K):\n        dA_n = A_kn[k] - mu_A\n        dB_n = B_kn[k] - mu_B\n        dA_kn.append(dA_n)\n        dB_kn.append(dB_n)\n\n    # Compute covariance.\n    sigma2_AB = 0.0\n    for k in range(K):\n        sigma2_AB += np.sum(dA_kn[k] * dB_kn[k])\n    sigma2_AB /= float(N)\n\n    # allocate storage for normalized fluctuation correlation function\n    C_n = np.zeros([N_max + 1], np.float64)\n\n    # Accumulate the integrated correlation time by computing the normalized correlation time at\n    # increasing values of t.  Stop accumulating if the correlation function goes negative, since\n    # this is unlikely to occur unless the correlation function has decayed to the point where it\n    # is dominated by noise and indistinguishable from zero.\n    t = 0\n    negative = False\n    for t in range(0, N_max + 1):\n        # compute unnormalized correlation function\n        numerator = 0.0\n        denominator = 0.0\n        for k in range(K):\n            if (t >= N_k[k]):\n                continue  # skip this trajectory if t is longer than the timeseries\n            numerator += np.sum(dA_kn[k][0:(N_k[k] - t)] * dB_kn[k][t:N_k[k]])\n            denominator += float(N_k[k] - t)\n            if truncate and numerator < 0:\n                negative = True\n        C = numerator / denominator\n\n        # compute normalized fluctuation correlation function at time t\n        C /= sigma2_AB\n\n        # Store correlation function.\n        C_n[t] = C\n\n        if negative:\n            break\n\n    # Return the computed fluctuation correlation function.\n    if norm:\n        return C_n[:t]\n    else:\n        return C_n[:t]*sigma2_AB + mu_A*mu_B"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the indices of an uncorrelated subsample of the data. Parameters ---------- A_t : np.ndarray A_t[t] is the t-th value of timeseries A(t). Length is deduced from vector. g : float, optional if provided, the statistical inefficiency g is used to subsample the timeseries -- otherwise it will be computed (default: None) fast : bool, optional, default=False fast can be set to True to give a less accurate but very quick estimate (default: False) conservative : bool, optional, default=False if set to True, uniformly-spaced indices are chosen with interval ceil(g), where g is the statistical inefficiency. Otherwise, indices are chosen non-uniformly with interval of approximately g in order to end up with approximately T/g total indices verbose : bool, optional, default=False if True, some output is printed Returns ------- indices : list of int the indices of an uncorrelated subsample of the data Notes ----- The statistical inefficiency is computed with the function computeStatisticalInefficiency(). ToDo ---- Instead of using regular stride, use irregular stride so more data can be fit in when g is non-integral. Examples -------- Subsample a correlated timeseries to extract an effectively uncorrelated dataset. >>> from pymbar import testsystems >>> A_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0) # generate a test correlated timeseries >>> indices = subsampleCorrelatedData(A_t) # compute indices of uncorrelated timeseries >>> A_n = A_t[indices] # extract uncorrelated samples Extract uncorrelated samples from multiple timeseries data from the same process. >>> # Generate multiple correlated timeseries data of different lengths. >>> T_k = [1000, 2000, 3000, 4000, 5000] >>> K = len(T_k) # number of timeseries >>> tau = 5.0 # exponential relaxation time >>> A_kt = [ testsystems.correlated_timeseries_example(N=T, tau=tau) for T in T_k ] # A_kt[k] is correlated timeseries k >>> # Estimate statistical inefficiency from all timeseries data. >>> g = statisticalInefficiencyMultiple(A_kt) >>> # Count number of uncorrelated samples in each timeseries. >>> N_k = np.array([ len(subsampleCorrelatedData(A_t, g=g)) for A_t in A_kt ]) # N_k[k] is the number of uncorrelated samples in timeseries k >>> N = N_k.sum() # total number of uncorrelated samples >>> # Subsample all trajectories to produce uncorrelated samples >>> A_kn = [ A_t[subsampleCorrelatedData(A_t, g=g)] for A_t in A_kt ] # A_kn[k] is uncorrelated subset of trajectory A_kt[t] >>> # Concatenate data into one timeseries. >>> A_n = np.zeros([N], np.float32) # A_n[n] is nth sample in concatenated set of uncorrelated samples >>> A_n[0:N_k[0]] = A_kn[0] >>> for k in range(1,K): A_n[N_k[0:k].sum():N_k[0:k+1].sum()] = A_kn[k]", "response": "def subsampleCorrelatedData(A_t, g=None, fast=False, conservative=False, verbose=False):\n    \"\"\"Determine the indices of an uncorrelated subsample of the data.\n\n    Parameters\n    ----------\n    A_t : np.ndarray\n        A_t[t] is the t-th value of timeseries A(t).  Length is deduced from vector.\n    g : float, optional\n        if provided, the statistical inefficiency g is used to subsample the timeseries -- otherwise it will be computed (default: None)\n    fast : bool, optional, default=False\n        fast can be set to True to give a less accurate but very quick estimate (default: False)\n    conservative : bool, optional, default=False\n        if set to True, uniformly-spaced indices are chosen with interval ceil(g), where\n        g is the statistical inefficiency.  Otherwise, indices are chosen non-uniformly with interval of\n        approximately g in order to end up with approximately T/g total indices\n    verbose : bool, optional, default=False\n        if True, some output is printed\n\n    Returns\n    -------\n    indices : list of int\n        the indices of an uncorrelated subsample of the data\n\n    Notes\n    -----\n    The statistical inefficiency is computed with the function computeStatisticalInefficiency().\n\n    ToDo\n    ----\n    Instead of using regular stride, use irregular stride so more data can be fit in when g is non-integral.\n\n    Examples\n    --------\n\n    Subsample a correlated timeseries to extract an effectively uncorrelated dataset.\n\n    >>> from pymbar import testsystems\n    >>> A_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0) # generate a test correlated timeseries\n    >>> indices = subsampleCorrelatedData(A_t) # compute indices of uncorrelated timeseries\n    >>> A_n = A_t[indices] # extract uncorrelated samples\n\n    Extract uncorrelated samples from multiple timeseries data from the same process.\n\n    >>> # Generate multiple correlated timeseries data of different lengths.\n    >>> T_k = [1000, 2000, 3000, 4000, 5000]\n    >>> K = len(T_k) # number of timeseries\n    >>> tau = 5.0 # exponential relaxation time\n    >>> A_kt = [ testsystems.correlated_timeseries_example(N=T, tau=tau) for T in T_k ] # A_kt[k] is correlated timeseries k\n    >>> # Estimate statistical inefficiency from all timeseries data.\n    >>> g = statisticalInefficiencyMultiple(A_kt)\n    >>> # Count number of uncorrelated samples in each timeseries.\n    >>> N_k = np.array([ len(subsampleCorrelatedData(A_t, g=g)) for A_t in A_kt ]) # N_k[k] is the number of uncorrelated samples in timeseries k\n    >>> N = N_k.sum() # total number of uncorrelated samples\n    >>> # Subsample all trajectories to produce uncorrelated samples\n    >>> A_kn = [ A_t[subsampleCorrelatedData(A_t, g=g)] for A_t in A_kt ] # A_kn[k] is uncorrelated subset of trajectory A_kt[t]\n    >>> # Concatenate data into one timeseries.\n    >>> A_n = np.zeros([N], np.float32) # A_n[n] is nth sample in concatenated set of uncorrelated samples\n    >>> A_n[0:N_k[0]] = A_kn[0]\n    >>> for k in range(1,K): A_n[N_k[0:k].sum():N_k[0:k+1].sum()] = A_kn[k]\n\n    \"\"\"\n\n    # Create np copy of arrays.\n    A_t = np.array(A_t)\n\n    # Get the length of the timeseries.\n    T = A_t.size\n\n    # Compute the statistical inefficiency for the timeseries.\n    if not g:\n        if verbose:\n            print(\"Computing statistical inefficiency...\")\n        g = statisticalInefficiency(A_t, A_t, fast=fast)\n        if verbose:\n            print(\"g = %f\" % g)\n\n    if conservative:\n        # Round g up to determine the stride we can use to pick out regularly-spaced uncorrelated samples.\n        stride = int(math.ceil(g))\n        if verbose:\n            print(\"conservative subsampling: using stride of %d\" % stride)\n\n        # Assemble list of indices of uncorrelated snapshots.\n        indices = range(0, T, stride)\n    else:\n        # Choose indices as floor(n*g), with n = 0,1,2,..., until we run out of data.\n        indices = []\n        n = 0\n        while int(round(n * g)) < T:\n            t = int(round(n * g))\n            # ensure we don't sample the same point twice\n            if (n == 0) or (t != indices[n - 1]):\n                indices.append(t)\n            n += 1\n        if verbose:\n            print(\"standard subsampling: using average stride of %f\" % g)\n\n    # Number of samples in subsampled timeseries.\n    N = len(indices)\n\n    if verbose:\n        print(\"The resulting subsampled set has %d samples (original timeseries had %d).\" % (N, T))\n\n    # Return the list of indices of uncorrelated snapshots.\n    return indices"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detectEquilibration(A_t, fast=True, nskip=1):\n    T = A_t.size\n\n    # Special case if timeseries is constant.\n    if A_t.std() == 0.0:\n        return (0, 1, 1)  # Changed from Neff=N to Neff=1 after issue #122\n\n    g_t = np.ones([T - 1], np.float32)\n    Neff_t = np.ones([T - 1], np.float32)\n    for t in range(0, T - 1, nskip):\n        try:\n            g_t[t] = statisticalInefficiency(A_t[t:T], fast=fast)\n        except ParameterError:  # Fix for issue https://github.com/choderalab/pymbar/issues/122\n            g_t[t] = (T - t + 1)\n        Neff_t[t] = (T - t + 1) / g_t[t]\n    Neff_max = Neff_t.max()\n    t = Neff_t.argmax()\n    g = g_t[t]\n\n    return (t, g, Neff_max)", "response": "Automatically detect equilibrium data for a dataset using a binary search that maximizes number of effectively uncorrelated samples."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the statistical inefficiency of two timeseries.", "response": "def statisticalInefficiency_fft(A_n, mintime=3, memsafe=None):\n    \"\"\"Compute the (cross) statistical inefficiency of (two) timeseries.\n\n    Parameters\n    ----------\n    A_n : np.ndarray, float\n        A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n    mintime : int, optional, default=3\n        minimum amount of correlation function to compute (default: 3)\n        The algorithm terminates after computing the correlation time out to mintime when the\n        correlation function first goes negative.  Note that this time may need to be increased\n        if there is a strong initial negative peak in the correlation function.\n    memsafe: bool, optional, default=None (in depreciation)\n        If this function is used several times on arrays of comparable size then one might benefit \n        from setting this option to False. If set to True then clear np.fft cache to avoid a fast \n        increase in memory consumption when this function is called on many arrays of different sizes.\n\n    Returns\n    -------\n    g : np.ndarray,\n        g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n        We enforce g >= 1.0.\n\n    Notes\n    -----\n    The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n    The fast method described in Ref [1] is used to compute g.\n\n    References\n    ----------\n    [1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n        histogram analysis method for the analysis of simulated and parallel tempering simulations.\n        JCTC 3(1):26-41, 2007.\n\n    \"\"\"\n    try:\n        import statsmodels.api as sm\n    except ImportError as err:\n        err.args = (err.args[0] + \"\\n You need to install statsmodels to use the FFT based correlation function.\",)\n        raise\n\n    # Create np copies of input arguments.\n    A_n = np.array(A_n)\n\n    # Get the length of the timeseries.\n    N = A_n.size\n\n    C_t = sm.tsa.stattools.acf(A_n, fft=True, unbiased=True, nlags=N)\n    t_grid = np.arange(N).astype('float')\n    g_t = 2.0 * C_t * (1.0 - t_grid / float(N))\n\n    \"\"\"\n    LNN Note:\n    Numpy 1.12 fft.fftpack swiched from using a Python dict for the cache to a LRU cache which auto prunes.\n    This makes the .clear() method undefined since the LRU does not have such a method.\n\n    Since the LRU *should* remove the problems which this code resolved (GH issue #168), I am removing this code from\n    execution. However, there may still be a problem with the LRU cache so I am leaving the code in case we need to\n    further test it\n    \"\"\"\n    #make function memory safe by clearing np.fft cache\n    #this assumes that statsmodels uses np.fft\n    #if memsafe:\n    #    np.fft.fftpack._fft_cache.clear()\n    #    np.fft.fftpack._real_fft_cache.clear()\n    if memsafe is not None:\n        warnings.warn(\"NumPy's FFT pack now uses an LRU cache to fix the very problem that the memsafe keyword \"\n                      \"was protecting. This argument no longer changes the code and will be removed in a future \"\n                      \"version.\", FutureWarning)\n    \n    try:\n        ind = np.where((C_t <= 0) & (t_grid > mintime))[0][0]\n    except IndexError:\n        ind = N\n\n    g = 1.0 + g_t[1:ind].sum()\n    g = max(1.0, g)\n\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the free energy difference using Bennett acceptance ratio.", "response": "def BAR(w_F, w_R, DeltaF=0.0, compute_uncertainty=True, uncertainty_method='BAR',maximum_iterations=500, relative_tolerance=1.0e-12, verbose=False, method='false-position', iterated_solution=True):\n    \"\"\"Compute free energy difference using the Bennett acceptance ratio (BAR) method.\n\n    Parameters\n    ----------\n    w_F : np.ndarray\n        w_F[t] is the forward work value from snapshot t.\n        t = 0...(T_F-1)  Length T_F is deduced from vector.\n    w_R : np.ndarray\n        w_R[t] is the reverse work value from snapshot t.\n        t = 0...(T_R-1)  Length T_R is deduced from vector.\n    DeltaF : float, optional, default=0.0\n        DeltaF can be set to initialize the free energy difference with a guess\n    compute_uncertainty : bool, optional, default=True\n        if False, only the free energy is returned\n    uncertainty_method: string, optional, default=BAR\n        There are two possible uncertainty estimates for BAR.  One agrees with MBAR for two states exactly;\n        The other only agrees with MBAR in the limit of good overlap. See below.\n    maximum_iterations : int, optional, default=500\n        can be set to limit the maximum number of iterations performed\n    relative_tolerance : float, optional, default=1E-11\n        can be set to determine the relative tolerance convergence criteria (defailt 1.0e-11)\n    verbose : bool\n        should be set to True if verbse debug output is desired (default False)\n    method : str, optional, defualt='false-position'\n        choice of method to solve BAR nonlinear equations, one of 'self-consistent-iteration' or 'false-position' (default: 'false-position')\n    iterated_solution : bool, optional, default=True\n        whether to fully solve the optimized BAR equation to consistency, or to stop after one step, to be \n        equivalent to transition matrix sampling.\n\n    Returns\n    -------\n    result_vals : dictionary\n    \n    Possible keys in the result_vals dictionary\n    'Delta_f' : float\n        Free energy difference\n    'dDelta_f': float\n        Estimated standard deviation of free energy difference\n\n    References\n    ----------\n\n    [1] Shirts MR, Bair E, Hooker G, and Pande VS. Equilibrium free energies from nonequilibrium\n    measurements using maximum-likelihood methods. PRL 91(14):140601, 2003.\n\n    Notes\n    -----\n    The false position method is used to solve the implicit equation.\n\n    Examples\n    --------\n    Compute free energy difference between two specified samples of work values.\n\n    >>> from pymbar import testsystems\n    >>> [w_F, w_R] = testsystems.gaussian_work_example(mu_F=None, DeltaF=1.0, seed=0)\n    >>> results = BAR(w_F, w_R)\n    >>> print('Free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n    Free energy difference is 1.088 +- 0.050 kT\n\n    Test completion of various other schemes.\n\n    >>> results = BAR(w_F, w_R, method='self-consistent-iteration')\n    >>> results = BAR(w_F, w_R, method='false-position')\n    >>> results = BAR(w_F, w_R, method='bisection')\n\n    \"\"\"\n\n    result_vals = dict()\n    # if computing nonoptimized, one step value, we set the max-iterations\n    # to 1, and the method to 'self-consistent-iteration'\n\n    if not iterated_solution:\n        maximum_iterations = 1\n        method = 'self-consistent-iteration'\n        DeltaF_initial = DeltaF\n\n    if method == 'self-consistent-iteration':\n        nfunc = 0\n\n    if method == 'bisection' or method == 'false-position':\n        UpperB = EXP(w_F)['Delta_f']\n        LowerB = -EXP(w_R)['Delta_f']\n\n        FUpperB = BARzero(w_F, w_R, UpperB)\n        FLowerB = BARzero(w_F, w_R, LowerB)\n        nfunc = 2\n\n        if (np.isnan(FUpperB) or np.isnan(FLowerB)):\n            # this data set is returning NAN -- will likely not work.  Return 0, print a warning:\n            # consider returning more information about failure\n            print(\"Warning: BAR is likely to be inaccurate because of poor overlap. Improve the sampling, or decrease the spacing betweeen states.  For now, guessing that the free energy difference is 0 with no uncertainty.\")\n            if compute_uncertainty:\n                result_vals['Delta_f'] = 0.0 \n                result_vals['dDelta_f'] = 0.0\n                return result_vals\n            else:\n                result_vals['Delta_f'] = 0.0 \n                return result_vals\n\n        while FUpperB * FLowerB > 0:\n            # if they have the same sign, they do not bracket.  Widen the bracket until they have opposite signs.\n            # There may be a better way to do this, and the above bracket should rarely fail.\n            if verbose:\n                print('Initial brackets did not actually bracket, widening them')\n            FAve = (UpperB + LowerB) / 2\n            UpperB = UpperB - max(abs(UpperB - FAve), 0.1)\n            LowerB = LowerB + max(abs(LowerB - FAve), 0.1)\n            FUpperB = BARzero(w_F, w_R, UpperB)\n            FLowerB = BARzero(w_F, w_R, LowerB)\n            nfunc += 2\n\n    # Iterate to convergence or until maximum number of iterations has been exceeded.\n\n    for iteration in range(maximum_iterations):\n\n        DeltaF_old = DeltaF\n\n        if method == 'false-position':\n            # Predict the new value\n            if (LowerB == 0.0) and (UpperB == 0.0):\n                DeltaF = 0.0\n                FNew = 0.0\n            else:\n                DeltaF = UpperB - FUpperB * (UpperB - LowerB) / (FUpperB - FLowerB)\n                FNew = BARzero(w_F, w_R, DeltaF)\n            nfunc += 1\n\n            if FNew == 0:\n                # Convergence is achieved.\n                if verbose:\n                    print('Convergence achieved.')\n                relative_change = 10 ** (-15)\n                break\n\n        if method == 'bisection':\n            # Predict the new value\n            DeltaF = (UpperB + LowerB) / 2\n            FNew = BARzero(w_F, w_R, DeltaF)\n            nfunc += 1\n\n        if method == 'self-consistent-iteration':\n            DeltaF = -BARzero(w_F, w_R, DeltaF) + DeltaF\n            nfunc += 1\n\n        # Check for convergence.\n        if (DeltaF == 0.0):\n            # The free energy difference appears to be zero -- return.\n            if verbose:\n                print('The free energy difference appears to be zero.')\n            break\n\n        if iterated_solution:\n            relative_change = abs((DeltaF - DeltaF_old) / DeltaF)\n            if verbose:\n                print(\"relative_change = {:12.3f}\".format(relative_change))\n\n            if ((iteration > 0) and (relative_change < relative_tolerance)):\n                # Convergence is achieved.\n                if verbose:\n                    print(\"Convergence achieved.\")\n                break\n\n        if method == 'false-position' or method == 'bisection':\n            if FUpperB * FNew < 0:\n                # these two now bracket the root\n                LowerB = DeltaF\n                FLowerB = FNew\n            elif FLowerB * FNew <= 0:\n                # these two now bracket the root\n                UpperB = DeltaF\n                FUpperB = FNew\n            else:\n                message = 'WARNING: Cannot determine bound on free energy'\n                raise BoundsError(message)\n\n        if verbose:\n            print(\"iteration {:5d}: DeltaF = {:16.3f}\".format(iteration, DeltaF))\n\n    # Report convergence, or warn user if not achieved.\n    if iterated_solution:\n        if iteration < maximum_iterations:\n            if verbose:\n                print('Converged to tolerance of {:e} in {:d} iterations ({:d} function evaluations)'.format(relative_change, iteration, nfunc))\n        else:\n            message = 'WARNING: Did not converge to within specified tolerance. max_delta = {:f}, TOLERANCE = {:f}, MAX_ITS = %d'.format(relative_change, relative_tolerance, maximum_iterations)\n            raise ConvergenceError(message)\n\n    if compute_uncertainty:\n\n        '''\n\n        Compute asymptotic variance estimate using Eq. 10a of Bennett,\n        1976 (except with n_1<f>_1^2 in the second denominator, it is\n        an error in the original NOTE: The 'BAR' and 'MBAR' estimators\n        do not agree for poor overlap. This is not because of\n        numerical precision, but because they are fundamentally\n        different estimators. For poor overlap, 'MBAR' diverges high,\n        and 'BAR' diverges by being too low. In situations they are\n        noticeably from each other, they are also pretty different\n        from the true answer (obtained by calculating the standard\n        deviation over lots of realizations).\n\n        First, we examine the 'BAR' equation. Rederive from Bennett, substituting (8) into (7)\n\n        (8)    -> W = [q0/n0 exp(-U1) + q1/n1 exp(-U0)]^-1\n                    <(W exp(-U1))^2 >_0         <(W exp(-U0))^2 >_1\n        (7)    -> -----------------------  +   -----------------------   - 1/n0 - 1/n1\n                   n_0 [<(W exp(-U1)>_0]^2      n_1 [<(W exp(-U0)>_1]^2\n\n            Const cancels out of top and bottom.   Wexp(-U0) = [q0/n0 exp(-(U1-U0)) + q1/n1]^-1\n                                                             =  n1/q1 [n1/n0 q0/q1 exp(-(U1-U0)) + 1]^-1\n                                                             =  n1/q1 [exp (M+(F1-F0)-(U1-U0)+1)^-1]\n                                                             =  n1/q1 f(x)\n                                                   Wexp(-U1) = [q0/n0 + q1/n1 exp(-(U0-U1))]^-1\n                                                             =  n0/q0 [1 + n0/n1 q1/q0 exp(-(U0-U1))]^-1\n                                                             =  n0/q0 [1 + exp(-M+[F0-F1)-(U0-U1))]^-1\n                                                             =  n0/q0 f(-x)\n\n\n                  <(W exp(-U1))^2 >_0          <(W exp(-U0))^2 >_1\n         (7) -> -----------------------   +  -----------------------   - 1/n0 - 1/n1\n                n_0 [<(W exp(-U1)>_0]^2      n_1 [<(W exp(-U0)>_1]^2\n\n                   <[n0/q0 f(-x)]^2>_0        <[n1/q1 f(x)]^2>_1\n                -----------------------  +  ------------------------   -1/n0 -1/n1\n                  n_0 <n0/q0 f(-x)>_0^2      n_1 <n1/q1 f(x)>_1^2\n\n               1      <[f(-x)]^2>_0                 1        <[f(x)]^2>_1\n               -  [-----------------------  - 1]  + -  [------------------------  - 1]\n               n0      <f(-x)>_0^2                  n1      n_1<f(x)>_1^2\n\n        where f = the fermi function, 1/(1+exp(-x))\n\n        This formula the 'BAR' equation works for works for free\n        energies (F0-F1) that don't satisfy the BAR equation.  The\n        'MBAR' equation, detailed below, only works for free energies\n        that satisfy the equation.\n\n\n        Now, let's look at the MBAR version of the uncertainty.  This\n        is written (from Shirts and Chodera, JPC, 129, 124105, Equation E9) as\n\n              [ n0<f(x)f(-x)>_0 + n1<f(x)f(-x)_1 ]^-1 - n0^-1 - n1^-1\n\n              we note the f(-x) + f(x)  = 1, and change this to:\n\n              [ n0<(1-f(-x)f(-x)>_0 + n1<f(x)(1-f(x))_1 ]^-1 - n0^-1 - n1^-1\n\n              [ n0<f(-x)-f(-x)^2)>_0 + n1<f(x)-f(x)^2)_1 ]^-1 - n0^-1 - n1^-1\n\n                                                1                                         1     1\n              --------------------------------------------------------------------    -  --- - ---\n                 n0 <f(-x)>_0 - n0 <[f(-x)]^2>_0 + n1 <f(x)>_1 + n1 <[f(x)]^2>_1          n0    n1\n\n\n        Removing the factor of - (T_F + T_R)/(T_F*T_R)) from both, we compare:\n\n                  <[f(-x)]^2>_0          <[f(x)]^2>_1\n              [------------------]  + [---------------]\n                 n0 <f(-x)>_0^2          n1 <f(x)>_1^2\n\n                                                1\n              --------------------------------------------------------------------\n                 n0 <f(-x)>_0 - n0 <[f(-x)]^2>_0 + n1 <f(x)>_1 + n1 <[f(x)]^2>_1\n\n        denote: <f(-x)>_0 = afF\n                <f(-x)^2>_0 = afF2\n                <f(x)>_1 = afR\n                <f(x)^2>_1 = afF2\n\n        Then we can look at both of these as:\n\n        variance_BAR = (afF2/afF**2)/T_F + (afR2/afR**2)/T_R\n        variance_MBAR = 1/(afF*T_F - afF2*T_F + afR*T_R - afR2*T_R)\n\n        Rearranging:\n\n        variance_BAR = (afF2/afF**2)/T_F + (afR2/afR**2)/T_R\n        variance_MBAR = 1/(afF*T_F + afR*T_R - (afF2*T_F +  afR2*T_R))\n\n        # check the steps below?  Not quite sure.\n        variance_BAR = (afF2/afF**2) + (afR2/afR**2)  = (afF2 + afR2)/afR**2\n        variance_MBAR = 1/(afF + afR - (afF2 +  afR2)) = 1/(2*afR-(afF2+afR2))\n\n        Definitely not the same.  Now, the reason that they both work\n        for high overlap is still not clear. We will determine the\n        difference at some point.\n\n        see https://github.com/choderalab/pymbar/issues/281 for more information.\n\n        Now implement the two computations.\n        '''\n\n        # Determine number of forward and reverse work values provided.\n        T_F = float(w_F.size)  # number of forward work values\n        T_R = float(w_R.size)  # number of reverse work values\n\n        # Compute log ratio of forward and reverse counts.\n        M = np.log(T_F / T_R)\n\n        if iterated_solution:\n            C = M - DeltaF\n        else:\n            C = M - DeltaF_initial\n\n        # In theory, overflow handling should not be needed now, because we use numlogexp or a custom routine?\n\n        # fF = 1 / (1 + np.exp(w_F + C)), but we need to handle overflows\n        exp_arg_F = (w_F + C)\n        max_arg_F  = np.max(exp_arg_F)\n        log_fF = - np.log(np.exp(-max_arg_F) + np.exp(exp_arg_F - max_arg_F))\n        afF  = np.exp(logsumexp(log_fF)-max_arg_F)/T_F\n\n        # fR = 1 / (1 + np.exp(w_R - C)), but we need to handle overflows\n        exp_arg_R = (w_R - C)\n        max_arg_R  = np.max(exp_arg_R)\n        log_fR = - np.log(np.exp(-max_arg_R) + np.exp(exp_arg_R - max_arg_R))\n        afR = np.exp(logsumexp(log_fR)-max_arg_R)/T_R\n\n        afF2 = np.exp(logsumexp(2*log_fF)-2*max_arg_F)/T_F\n        afR2 = np.exp(logsumexp(2*log_fR)-2*max_arg_R)/T_R\n\n        nrat = (T_F + T_R)/(T_F * T_R)   # same for both methods\n\n        if uncertainty_method == 'BAR':\n            variance = (afF2/afF**2)/T_F + (afR2/afR**2)/T_R - nrat\n            dDeltaF = np.sqrt(variance)\n        elif uncertainty_method == 'MBAR':\n            # OR equivalently\n            vartemp = ((afF - afF2)*T_F + (afR - afR2)*T_R)\n            dDeltaF = np.sqrt(1.0/vartemp - nrat)\n        else:\n            message = 'ERROR: BAR uncertainty method {:s} is not defined'.format(uncertainty_method)\n            raise ParameterError(message)\n\n        if verbose:\n            print(\"DeltaF = {:8.3f} +- {:8.3f}\".format(DeltaF, dDeltaF))\n        result_vals['Delta_f'] = DeltaF\n        result_vals['dDelta_f'] = dDeltaF\n    else:\n        if verbose:\n            print(\"DeltaF = {:8.3f}\".format(DeltaF))\n        result_vals['Delta_f'] = DeltaF\n\n    return result_vals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert KxN_max array to KxN_max array", "response": "def kln_to_kn(kln, N_k = None, cleanup = False):\n\n    \"\"\" Convert KxKxN_max array to KxN max array\n\n    if self.N is not initialized, it will be here.\n\n    Parameters\n    ----------\n    u_kln : np.ndarray, float, shape=(KxLxN_max)\n    N_k (optional) : np.array\n        the N_k matrix from the previous formatting form\n    cleanup (optional) : bool\n        optional command to clean up, since u_kln can get very large\n\n    Outputs\n    -------\n    u_kn: np.ndarray, float, shape=(LxN)\n    \"\"\"\n\n    #print \"warning: KxLxN_max arrays deprecated; convering into new preferred KxN shape\"\n\n    # rewrite into kn shape\n    [K, L, N_max] = np.shape(kln)\n\n    if N_k is None:\n        # We assume that all N_k are N_max.\n        # Not really an easier way to do this without being given the answer.\n        N_k = N_max * np.ones([L], dtype=np.int64)\n    N = np.sum(N_k)\n\n    kn = np.zeros([L, N], dtype=np.float64)\n    i = 0\n    for k in range(K):  # loop through the old K; some might be zero\n        for ik in range(N_k[k]):\n            kn[:, i] = kln[k, :, ik]\n            i += 1\n    if cleanup:\n        del(kln)  # very big, let's explicitly delete\n\n    return kn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kn_to_n(kn, N_k = None, cleanup = False):\n\n    \"\"\" Convert KxN_max array to N array\n\n    Parameters\n    ----------\n    u_kn: np.ndarray, float, shape=(KxN_max)\n    N_k (optional) : np.array\n        the N_k matrix from the previous formatting form\n    cleanup (optional) : bool\n        optional command to clean up, since u_kln can get very large\n\n    Outputs\n    -------\n    u_n: np.ndarray, float, shape=(N)\n    \"\"\"\n\n    #print \"warning: KxN arrays deprecated; convering into new preferred N shape\"\n    # rewrite into kn shape\n\n    # rewrite into kn shape\n    [K, N_max] = np.shape(kn)\n\n    if N_k is None:\n        # We assume that all N_k are N_max.\n        # Not really an easier way to do this without being given the answer.\n        N_k = N_max*np.ones([K], dtype=np.int64)\n    N = np.sum(N_k)\n\n    n = np.zeros([N], dtype=np.float64)\n    i = 0\n    for k in range(K):  # loop through the old K; some might be zero\n        for ik in range(N_k[k]):\n            n[i] = kn[k, ik]\n            i += 1\n    if cleanup:\n        del(kn)  # very big, let's explicitly delete\n    return n", "response": "Convert KxN_max array to N array"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_type(val, dtype, ndim, name, length=None, can_be_none=False, shape=None,\n                warn_on_cast=True, add_newaxis_on_deficient_ndim=False):\n    \"\"\"Typecheck the size, shape and dtype of a numpy array, with optional\n    casting.\n\n    Parameters\n    ----------\n    val : {np.ndaraay, None}\n        The array to check\n    dtype : {nd.dtype, str}\n        The dtype you'd like the array to have\n    ndim : int\n        The number of dimensions you'd like the array to have\n    name : str\n        name of the array. This is used when throwing exceptions, so that\n        we can describe to the user which array is messed up.\n    length : int, optional\n        How long should the array be?\n    can_be_none : bool\n        Is ``val == None`` acceptable?\n    shape : tuple, optional\n        What should be shape of the array be? If the provided tuple has\n        Nones in it, those will be semantically interpreted as matching\n        any length in that dimension. So, for example, using the shape\n        spec ``(None, None, 3)`` will ensure that the last dimension is of\n        length three without constraining the first two dimensions\n    warn_on_cast : bool, default=True\n        Raise a warning when the dtypes don't match and a cast is done.\n    add_newaxis_on_deficient_ndim : bool, default=True\n        Add a new axis to the beginining of the array if the number of\n        dimensions is deficient by one compared to your specification. For\n        instance, if you're trying to get out an array of ``ndim == 3``,\n        but the user provides an array of ``shape == (10, 10)``, a new axis will\n        be created with length 1 in front, so that the return value is of\n        shape ``(1, 10, 10)``.\n\n    Notes\n    -----\n    The returned value will always be C-contiguous.\n\n    Returns\n    -------\n    typechecked_val : np.ndarray, None\n        If `val=None` and `can_be_none=True`, then this will return None.\n        Otherwise, it will return val (or a copy of val). If the dtype wasn't right,\n        it'll be casted to the right shape. If the array was not C-contiguous, it'll\n        be copied as well.\n\n    \"\"\"\n    if can_be_none and val is None:\n        return None\n\n    if not isinstance(val, np.ndarray):\n        # special case: if the user is looking for a 1d array, and\n        # they request newaxis upconversion, and provided a scalar\n        # then we should reshape the scalar to be a 1d length-1 array\n        if add_newaxis_on_deficient_ndim and ndim == 1 and np.isscalar(val):\n            val = np.array([val])\n        else:\n            raise TypeError((\"%s must be numpy array. \"\n                             \" You supplied type %s\" % (name, type(val))))\n\n    if warn_on_cast and val.dtype != dtype:\n        warnings.warn(\"Casting %s dtype=%s to %s \" % (name, val.dtype, dtype),\n                      TypeCastPerformanceWarning)\n\n    if not val.ndim == ndim:\n        if add_newaxis_on_deficient_ndim and val.ndim + 1 == ndim:\n            val = val[np.newaxis, ...]\n        else:\n            raise ValueError((\"%s must be ndim %s. \"\n                              \"You supplied %s\" % (name, ndim, val.ndim)))\n\n    val = np.ascontiguousarray(val, dtype=dtype)\n\n    if length is not None and len(val) != length:\n        raise ValueError((\"%s must be length %s. \"\n                          \"You supplied %s\" % (name, length, len(val))))\n\n    if shape is not None:\n        # the shape specified given by the user can look like (None, None 3)\n        # which indicates that ANY length is accepted in dimension 0 or\n        # dimension 1\n        sentenel = object()\n        error = ValueError((\"%s must be shape %s. You supplied  \"\n                            \"%s\" % (name, str(shape).replace('None', 'Any'), val.shape)))\n        for a, b in zip_longest(val.shape, shape, fillvalue=sentenel):\n            if a is sentenel or b is sentenel:\n                # if the sentenel was reached, it means that the ndim didn't\n                # match or something. this really shouldn't happen\n                raise error\n            if b is None:\n                # if the user's shape spec has a None in it, it matches anything\n                continue\n            if a != b:\n                # check for equality\n                raise error\n\n    return val", "response": "Ensure that the array value is of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _logsum(a_n):\n\n    # Compute the maximum argument.\n    max_log_term = np.max(a_n)\n\n    # Compute the reduced terms.\n    terms = np.exp(a_n - max_log_term)\n\n    # Compute the log sum.\n    log_sum = np.log(np.sum(terms)) + max_log_term\n\n    return log_sum", "response": "Compute the log of a sum of exponentiated terms exp ( a_n ) in a numerically - stable manner."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the log of the sum of exponentials of input array.", "response": "def logsumexp(a, axis=None, b=None, use_numexpr=True):\n    \"\"\"Compute the log of the sum of exponentials of input elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : None or int, optional, default=None\n        Axis or axes over which the sum is taken. By default `axis` is None,\n        and all elements are summed.\n    b : array-like, optional\n        Scaling factor for exp(`a`) must be of the same shape as `a` or\n        broadcastable to `a`.\n    use_numexpr : bool, optional, default=True\n        If True, use the numexpr library to speed up the calculation, which\n        can give a 2-4X speedup when working with large arrays.\n\n    Returns\n    -------\n    res : ndarray\n        The result, ``log(sum(exp(a)))`` calculated in a numerically\n        more stable way. If `b` is given then ``log(sum(b*exp(a)))``\n        is returned.\n\n    See Also\n    --------\n    numpy.logaddexp, numpy.logaddexp2, scipy.misc.logsumexp (soon to be replaced with  scipy.special.logsumexp)\n\n    Notes\n    -----\n    This is based on scipy.misc.logsumexp but with optional numexpr\n    support for improved performance.\n    \"\"\"\n\n    a = np.asarray(a)\n\n    a_max = np.amax(a, axis=axis, keepdims=True)\n\n    if a_max.ndim > 0:\n        a_max[~np.isfinite(a_max)] = 0\n    elif not np.isfinite(a_max):\n        a_max = 0\n\n    if b is not None:\n        b = np.asarray(b)\n        if use_numexpr and HAVE_NUMEXPR:\n            out = np.log(numexpr.evaluate(\"b * exp(a - a_max)\").sum(axis))\n        else:\n            out = np.log(np.sum(b * np.exp(a - a_max), axis=axis))\n    else:\n        if use_numexpr and HAVE_NUMEXPR:\n            out = np.log(numexpr.evaluate(\"exp(a - a_max)\").sum(axis))\n        else:\n            out = np.log(np.sum(np.exp(a - a_max), axis=axis))\n\n    a_max = np.squeeze(a_max, axis=axis)\n    out += a_max\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the weight matrix W is properly normalized and returns a new object if the test passes.", "response": "def check_w_normalized(W, N_k, tolerance = 1.0e-4):\n    \"\"\"Check the weight matrix W is properly normalized. The sum over N should be 1, and the sum over k by N_k should aslo be 1\n\n    Parameters\n    ----------\n    W : np.ndarray, shape=(N, K), dtype='float'\n        The normalized weight matrix for snapshots and states.\n        W[n, k] is the weight of snapshot n in state k.\n    N_k : np.ndarray, shape=(K), dtype='int'\n        N_k[k] is the number of samples from state k.\n    tolerance : float, optional, default=1.0e-4\n        Tolerance for checking equality of sums\n\n    Returns\n    -------\n    None : NoneType\n        Returns a None object if test passes, otherwise raises a ParameterError with appropriate message if W is not normalized within tolerance.\n    \"\"\"\n\n    [N, K] = W.shape\n\n    column_sums = np.sum(W, axis=0)\n    badcolumns = (np.abs(column_sums - 1) > tolerance)\n    if np.any(badcolumns):\n        which_badcolumns = np.arange(K)[badcolumns]\n        firstbad = which_badcolumns[0]\n        raise ParameterError(\n            'Warning: Should have \\sum_n W_nk = 1.  Actual column sum for state %d was %f. %d other columns have similar problems' %\n            (firstbad, column_sums[firstbad], np.sum(badcolumns)))\n\n    row_sums = np.sum(W * N_k, axis=1)\n    badrows = (np.abs(row_sums - 1) > tolerance)\n    if np.any(badrows):\n        which_badrows = np.arange(N)[badrows]\n        firstbad = which_badrows[0]\n        raise ParameterError(\n            'Warning: Should have \\sum_k N_k W_nk = 1.  Actual row sum for sample %d was %f. %d other rows have similar problems' %\n            (firstbad, row_sums[firstbad], np.sum(badrows)))\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads in the TEMP#. output file and parses it into an array of energies", "response": "def read_total_energies(pathname,colnum):\n    \"\"\"Reads in the TEMP#/ener_box#.output file and parses it, returning an array of energies\n\n    ARGUMENTS\n        filename (string) - the path to the folder of the simulation\n\tcolnum (integer) column the energy is found in\n    \"\"\"\n\n    print(\"--Reading total energies from %s/...\" % pathname)\n\n    # Initialize Return variables\n    E_kn = numpy.zeros([NumTemps, NumIterations], numpy.float64)\n    \n    #Read files\n    for k in range(NumTemps):\n        #Construct each TEMP#/ener_box#.output name and read in the file\n        filename = os.path.join(pathname,'TEMP' + str(k), 'ener_box'+ str(k) + '.output')\n        infile = open(filename, 'r')\n        lines = infile.readlines()\n        infile.close()\n        numLines = len(lines)\n\n        #Initialize arrays for E\n        E_from_file = numpy.zeros(NumIterations, numpy.float64)\n\n        #Parse lines in each file\n        for n in range(NumIterations):\n            m = numLines - 2 - n #Count down (the 2 is for index purposes(1) and to not use the double-counted last line (1))\n            elements = lines[m].split()\n            E_from_file[n] = float(elements[colnum])\n\n        #Add in the E's for each timestep (n) at this temperature (k)\n        E_kn[k] = E_from_file;\n    return E_kn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_simulation_temps(pathname,NumTemps):\n\n    print(\"--Reading temperatures from %s/...\" % pathname)\n\n    # Initialize return variable\n    temps_from_file = numpy.zeros(NumTemps, numpy.float64)\n\n    for k in range(NumTemps):\n        infile = open(os.path.join(pathname,'TEMP'+ str(k), 'simul'+str(k)+'.output'), 'r')\n        lines = infile.readlines()\n        infile.close()\n\n        for line in lines:\n            if (line[0:11] == 'Temperature'):\n                vals = line.split(':')\n                break\n        temps_from_file[k] = float(vals[1])\n\n    return temps_from_file", "response": "Reads in the various temperatures from each TEMP# and simul. output file by knowing\n        beforehand the total number of temperatures"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef EXP(w_F, compute_uncertainty=True, is_timeseries=False):\n\n    result_vals = dict()\n\n    # Get number of work measurements.\n    T = float(np.size(w_F))  # number of work measurements\n\n    # Estimate free energy difference by exponential averaging using DeltaF = - log < exp(-w_F) >\n    DeltaF = - (logsumexp(- w_F) - np.log(T))\n\n    if compute_uncertainty:\n        # Compute x_i = np.exp(-w_F_i - max_arg)\n        max_arg = np.max(-w_F)  # maximum argument\n        x = np.exp(-w_F - max_arg)\n\n        # Compute E[x] = <x> and dx\n        Ex = x.mean()\n\n        # Compute effective number of uncorrelated samples.\n        g = 1.0  # statistical inefficiency\n        if is_timeseries:\n            # Estimate statistical inefficiency of x timeseries.\n            import timeseries\n            g = timeseries.statisticalInefficiency(x, x)\n\n        # Estimate standard error of E[x].\n        dx = np.std(x) / np.sqrt(T / g)\n\n        # dDeltaF = <x>^-1 dx\n        dDeltaF = (dx / Ex)\n\n        # Return estimate of free energy difference and uncertainty.\n        result_vals['Delta_f'] = DeltaF\n        result_vals['dDelta_f'] = dDeltaF \n    else:\n        result_vals['Delta_f'] = DeltaF\n    return result_vals", "response": "Estimate the free energy difference using one - sided exponential averaging."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef EXPGauss(w_F, compute_uncertainty=True, is_timeseries=False):\n\n    # Get number of work measurements.\n    T = float(np.size(w_F))  # number of work measurements\n\n    var = np.var(w_F)\n    # Estimate free energy difference by Gaussian approximation, dG = <U> - 0.5*var(U)\n    DeltaF = np.average(w_F) - 0.5 * var\n\n    result_vals = dict()\n    if compute_uncertainty:\n        # Compute effective number of uncorrelated samples.\n        g = 1.0  # statistical inefficiency\n        T_eff = T\n        if is_timeseries:\n            # Estimate statistical inefficiency of x timeseries.\n            import timeseries\n            g = timeseries.statisticalInefficiency(w_F, w_F)\n\n            T_eff = T / g\n        # Estimate standard error of E[x].\n        dx2 = var / T_eff + 0.5 * var * var / (T_eff - 1)\n        dDeltaF = np.sqrt(dx2)\n\n        # Return estimate of free energy difference and uncertainty.\n        result_vals['Delta_f'] = DeltaF\n        result_vals['dDelta_f'] = dDeltaF \n    else:\n        result_vals['Delta_f'] = DeltaF\n    return result_vals", "response": "Estimate the free energy difference using one - sided exponential averaging."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_input(prompt=\"\"):\n\t\n\tsys.stderr.flush()\n\t\n\ttty = STDIN.is_a_TTY() and STDOUT.is_a_TTY()\n\t\n\tif RETURN_UNICODE:\n\t\tif tty:\n\t\t\tline_bytes = readline(prompt)\n\t\t\tline = stdin_decode(line_bytes)\n\t\telse:\n\t\t\tline = stdio_readline(prompt)\n\t\t\n\telse:\n\t\tif tty:\n\t\t\tline = readline(prompt)\n\t\telse:\n\t\t\tline_unicode = stdio_readline(prompt)\n\t\t\tline = stdin_encode(line_unicode)\n\t\n\tif line:\n\t\treturn line[:-1] # strip strailing \"\\n\"\n\telse:\n\t\traise EOFError", "response": "raw_input - Reads a string from standard input."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef input(prompt=\"\"):\n\t\n\tstring = stdin_decode(raw_input(prompt))\n\t\n\tcaller_frame = sys._getframe(1)\n\tglobals = caller_frame.f_globals\n\tlocals = caller_frame.f_locals\n\t\n\treturn eval(string, globals, locals)", "response": "input([prompt = \"\" ] -> value\nEquivalent to eval ( raw_input"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(filename):\n\tbuffer = _builtin_open(filename, 'rb')\n\ttry:\n\t\tencoding, lines = detect_encoding(buffer.readline)\n\t\tbuffer.seek(0)\n\t\ttext = TextIOWrapper(buffer, encoding, line_buffering=True)\n\t\ttext.mode = 'r'\n\t\treturn text\n\texcept:\n\t\tbuffer.close()\n\t\traise", "response": "Open a file in read only mode using the encoding detected by\n\tdetect_encoding().\n\t return a TextIOWrapper object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configuration_callback(cmd_name, option_name, config_file_name,\n                           saved_callback, provider, implicit, ctx,\n                           param, value):\n    \"\"\"\n    Callback for reading the config file.\n\n    Also takes care of calling user specified custom callback afterwards.\n\n    cmd_name : str\n        The command name. This is used to determine the configuration directory.\n    option_name : str\n        The name of the option. This is used for error messages.\n    config_file_name : str\n        The name of the configuration file.\n    saved_callback: callable\n        User-specified callback to be called later.\n    provider : callable\n        A callable that parses the configuration file and returns a dictionary\n        of the configuration parameters. Will be called as\n        `provider(file_path, cmd_name)`. Default: `configobj_provider()`\n    implicit : bool\n        Whether a implicit value should be applied if no configuration option\n        value was provided.\n        Default: `False`\n    ctx : object\n        Click context.\n    \"\"\"\n    ctx.default_map = ctx.default_map or {}\n    cmd_name = cmd_name or ctx.info_name\n\n    if implicit:\n        default_value = os.path.join(\n            click.get_app_dir(cmd_name), config_file_name)\n        param.default = default_value\n        value = value or default_value\n\n    if value:\n        try:\n            config = provider(value, cmd_name)\n        except Exception as e:\n            raise click.BadOptionUsage(option_name,\n                \"Error reading configuration file: {}\".format(e), ctx)\n        ctx.default_map.update(config)\n\n    return saved_callback(ctx, param, value) if saved_callback else value", "response": "A callback for reading the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef configuration_option(*param_decls, **attrs):\n    param_decls = param_decls or ('--config', )\n    option_name = param_decls[0]\n\n    def decorator(f):\n\n        attrs.setdefault('is_eager', True)\n        attrs.setdefault('help', 'Read configuration from FILE.')\n        attrs.setdefault('expose_value', False)\n        implicit = attrs.pop('implicit', True)\n        cmd_name = attrs.pop('cmd_name', None)\n        config_file_name = attrs.pop('config_file_name', 'config')\n        provider = attrs.pop('provider', configobj_provider())\n        path_default_params = {\n            'exists': False,\n            'file_okay': True,\n            'dir_okay': False,\n            'writable': False,\n            'readable': True,\n        'resolve_path': False\n        }\n        path_params = {\n            k: attrs.pop(k, v)\n            for k, v in path_default_params.items()\n        }\n        attrs['type'] = click.Path(**path_params)\n        saved_callback = attrs.pop('callback', None)\n        partial_callback = functools.partial(\n            configuration_callback, cmd_name, option_name, config_file_name, saved_callback, provider, implicit)\n        attrs['callback'] = partial_callback\n        return click.option(*param_decls, **attrs)(f)\n\n    return decorator", "response": "This is a click. option that adds configuration file support to a click application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndemonstrating use of EncryptedTable to transparently encrypt an item.", "response": "def encrypt_item(table_name, aws_cmk_id, meta_table_name, material_name):\n    \"\"\"Demonstrate use of EncryptedTable to transparently encrypt an item.\"\"\"\n    index_key = {\"partition_attribute\": \"is this\", \"sort_attribute\": 55}\n    plaintext_item = {\n        \"example\": \"data\",\n        \"some numbers\": 99,\n        \"and some binary\": Binary(b\"\\x00\\x01\\x02\"),\n        \"leave me\": \"alone\",  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal table resource for the meta store.\n    meta_table = boto3.resource(\"dynamodb\").Table(meta_table_name)\n    # Create a crypto materials provider for the meta store using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n    # Create a meta store using the AWS KMS crypto materials provider.\n    meta_store = MetaStore(table=meta_table, materials_provider=aws_kms_cmp)\n    # Create a most recent provider using the meta store.\n    most_recent_cmp = MostRecentProvider(\n        provider_store=meta_store,\n        material_name=material_name,\n        version_ttl=600.0,  # Check for a new material version every five minutes.\n    )\n    # Create a normal table resource.\n    table = boto3.resource(\"dynamodb\").Table(table_name)\n    # Create attribute actions that tells the encrypted table to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted table resource.\n    encrypted_table = EncryptedTable(table=table, materials_provider=most_recent_cmp, attribute_actions=actions)\n\n    # Put the item to the table, using the encrypted table resource to transparently encrypt it.\n    encrypted_table.put_item(Item=plaintext_item)\n\n    # Get the encrypted item using the standard table resource.\n    encrypted_item = table.get_item(Key=index_key)[\"Item\"]\n\n    # Get the item using the encrypted table resource, transparently decyrpting it.\n    decrypted_item = encrypted_table.get_item(Key=index_key)[\"Item\"]\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_table.delete_item(Key=index_key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking an input str or bytes object and returns an equivalent str object.", "response": "def to_str(data):\n    \"\"\"Takes an input str or bytes object and returns an equivalent str object.\n\n    :param data: Input data\n    :type data: str or bytes\n    :returns: Data normalized to str\n    :rtype: str\n    \"\"\"\n    if isinstance(data, bytes):\n        return codecs.decode(data, TEXT_ENCODING)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking an input str or bytes object and returns an equivalent bytes object.", "response": "def to_bytes(data):\n    \"\"\"Takes an input str or bytes object and returns an equivalent bytes object.\n\n    :param data: Input data\n    :type data: str or bytes\n    :returns: Data normalized to bytes\n    :rtype: bytes\n    \"\"\"\n    if isinstance(data, six.string_types) and not isinstance(data, bytes):\n        return codecs.encode(data, TEXT_ENCODING)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes the contents of a value from a serialized stream.", "response": "def decode_value(stream):\n    \"\"\"Decode the contents of a value from a serialized stream.\n\n    :param stream: Source data stream\n    :type stream: io.BytesIO\n    :returns: Decoded value\n    :rtype: bytes\n    \"\"\"\n    length = decode_length(stream)\n    (value,) = unpack_value(\">{:d}s\".format(length), stream)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_tag(stream):\n    (reserved, tag) = unpack_value(\">cc\", stream)\n\n    if reserved != b\"\\x00\":\n        raise DeserializationError(\"Invalid tag: reserved byte is not null\")\n\n    return tag", "response": "Decode a tag value from a serialized stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading complete file contents.", "response": "def read(*args):\n    \"\"\"Reads complete file contents.\"\"\"\n    return io.open(os.path.join(HERE, *args), encoding=\"utf-8\").read()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the signature DynamoDB atttribute.", "response": "def sign_item(encrypted_item, signing_key, crypto_config):\n    # type: (dynamodb_types.ITEM, DelegatedKey, CryptoConfig) -> dynamodb_types.BINARY_ATTRIBUTE\n    \"\"\"Generate the signature DynamoDB atttribute.\n\n    :param dict encrypted_item: Encrypted DynamoDB item\n    :param DelegatedKey signing_key: DelegatedKey to use to calculate the signature\n    :param CryptoConfig crypto_config: Cryptographic configuration\n    :returns: Item signature DynamoDB attribute value\n    :rtype: dict\n    \"\"\"\n    signature = signing_key.sign(\n        algorithm=signing_key.algorithm,\n        data=_string_to_sign(\n            item=encrypted_item,\n            table_name=crypto_config.encryption_context.table_name,\n            attribute_actions=crypto_config.attribute_actions,\n        ),\n    )\n    return {Tag.BINARY.dynamodb_tag: signature}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_item_signature(signature_attribute, encrypted_item, verification_key, crypto_config):\n    # type: (dynamodb_types.BINARY_ATTRIBUTE, dynamodb_types.ITEM, DelegatedKey, CryptoConfig) -> None\n    \"\"\"Verify the item signature.\n\n    :param dict signature_attribute: Item signature DynamoDB attribute value\n    :param dict encrypted_item: Encrypted DynamoDB item\n    :param DelegatedKey verification_key: DelegatedKey to use to calculate the signature\n    :param CryptoConfig crypto_config: Cryptographic configuration\n    \"\"\"\n    signature = signature_attribute[Tag.BINARY.dynamodb_tag]\n    verification_key.verify(\n        algorithm=verification_key.algorithm,\n        signature=signature,\n        data=_string_to_sign(\n            item=encrypted_item,\n            table_name=crypto_config.encryption_context.table_name,\n            attribute_actions=crypto_config.attribute_actions,\n        ),\n    )", "response": "Verifies the signature of an item."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the string to sign from an encrypted DynamoDB item and configuration.", "response": "def _string_to_sign(item, table_name, attribute_actions):\n    # type: (dynamodb_types.ITEM, Text, AttributeActions) -> bytes\n    \"\"\"Generate the string to sign from an encrypted item and configuration.\n\n    :param dict item: Encrypted DynamoDB item\n    :param str table_name: Table name to use when generating the string to sign\n    :param AttributeActions attribute_actions: Actions to take for item\n    \"\"\"\n    hasher = hashes.Hash(hashes.SHA256(), backend=default_backend())\n    data_to_sign = bytearray()\n    data_to_sign.extend(_hash_data(hasher=hasher, data=\"TABLE>{}<TABLE\".format(table_name).encode(TEXT_ENCODING)))\n    for key in sorted(item.keys()):\n        action = attribute_actions.action(key)\n        if action is CryptoAction.DO_NOTHING:\n            continue\n\n        data_to_sign.extend(_hash_data(hasher=hasher, data=key.encode(TEXT_ENCODING)))\n\n        if action is CryptoAction.SIGN_ONLY:\n            data_to_sign.extend(SignatureValues.PLAINTEXT.sha256)\n        else:\n            data_to_sign.extend(SignatureValues.ENCRYPTED.sha256)\n\n        data_to_sign.extend(_hash_data(hasher=hasher, data=serialize_attribute(item[key])))\n    return bytes(data_to_sign)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _hash_data(hasher, data):\n    _hasher = hasher.copy()\n    _hasher.update(data)\n    return _hasher.finalize()", "response": "Generate hash of data using provided hash type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize(material_description):\n    # type: (Dict[Text, Text]) -> dynamodb_types.BINARY_ATTRIBUTE\n    \"\"\"Serialize a material description dictionary into a DynamodDB attribute.\n\n    :param dict material_description: Material description dictionary\n    :returns: Serialized material description as a DynamoDB binary attribute value\n    :rtype: dict\n    :raises InvalidMaterialDescriptionError: if invalid name or value found in material description\n    \"\"\"\n    material_description_bytes = bytearray(_MATERIAL_DESCRIPTION_VERSION)\n\n    # TODO: verify Java sorting order\n    for name, value in sorted(material_description.items(), key=lambda x: x[0]):\n        try:\n            material_description_bytes.extend(encode_value(to_bytes(name)))\n            material_description_bytes.extend(encode_value(to_bytes(value)))\n        except (TypeError, struct.error):\n            raise InvalidMaterialDescriptionError(\n                'Invalid name or value in material description: \"{name}\"=\"{value}\"'.format(name=name, value=value)\n            )\n\n    return {Tag.BINARY.dynamodb_tag: bytes(material_description_bytes)}", "response": "Serializes a dictionary into a Dynamodb binary attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deserialize(serialized_material_description):\n    # type: (dynamodb_types.BINARY_ATTRIBUTE) -> Dict[Text, Text]\n    \"\"\"Deserialize a serialized material description attribute into a material description dictionary.\n\n    :param dict serialized_material_description: DynamoDB attribute value containing serialized material description.\n    :returns: Material description dictionary\n    :rtype: dict\n    :raises InvalidMaterialDescriptionError: if malformed version\n    :raises InvalidMaterialDescriptionVersionError: if unknown version is found\n    \"\"\"\n    try:\n        _raw_material_description = serialized_material_description[Tag.BINARY.dynamodb_tag]\n\n        material_description_bytes = io.BytesIO(_raw_material_description)\n        total_bytes = len(_raw_material_description)\n    except (TypeError, KeyError):\n        message = \"Invalid material description\"\n        _LOGGER.exception(message)\n        raise InvalidMaterialDescriptionError(message)\n    # We don't currently do anything with the version, but do check to make sure it is the one we know about.\n    _read_version(material_description_bytes)\n\n    material_description = {}\n    try:\n        while material_description_bytes.tell() < total_bytes:\n            name = to_str(decode_value(material_description_bytes))\n            value = to_str(decode_value(material_description_bytes))\n            material_description[name] = value\n    except struct.error:\n        message = \"Invalid material description\"\n        _LOGGER.exception(message)\n        raise InvalidMaterialDescriptionError(message)\n    return material_description", "response": "Deserialize a serialized material description attribute into a material description dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the version from the serialized material description and raises an error if it is unknown.", "response": "def _read_version(material_description_bytes):\n    # type: (io.BytesIO) -> None\n    \"\"\"Read the version from the serialized material description and raise an error if it is unknown.\n\n    :param material_description_bytes: serializezd material description\n    :type material_description_bytes: io.BytesIO\n    :raises InvalidMaterialDescriptionError: if malformed version\n    :raises InvalidMaterialDescriptionVersionError: if unknown version is found\n    \"\"\"\n    try:\n        (version,) = unpack_value(\">4s\", material_description_bytes)\n    except struct.error:\n        message = \"Malformed material description version\"\n        _LOGGER.exception(message)\n        raise InvalidMaterialDescriptionError(message)\n    if version != _MATERIAL_DESCRIPTION_VERSION:\n        raise InvalidMaterialDescriptionVersionError(\"Invalid material description version: {}\".format(repr(version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencrypts a DynamoDB item.", "response": "def encrypt_dynamodb_item(item, crypto_config):\n    # type: (dynamodb_types.ITEM, CryptoConfig) -> dynamodb_types.ITEM\n    \"\"\"Encrypt a DynamoDB item.\n\n    >>> from dynamodb_encryption_sdk.encrypted.item import encrypt_dynamodb_item\n    >>> plaintext_item = {\n    ...     'some': {'S': 'data'},\n    ...     'more': {'N': '5'}\n    ... }\n    >>> encrypted_item = encrypt_dynamodb_item(\n    ...     item=plaintext_item,\n    ...     crypto_config=my_crypto_config\n    ... )\n\n    .. note::\n\n        This handles DynamoDB-formatted items and is for use with the boto3 DynamoDB client.\n\n    :param dict item: Plaintext DynamoDB item\n    :param CryptoConfig crypto_config: Cryptographic configuration\n    :returns: Encrypted and signed DynamoDB item\n    :rtype: dict\n    \"\"\"\n    if crypto_config.attribute_actions.take_no_actions:\n        # If we explicitly have been told not to do anything to this item, just copy it.\n        return item.copy()\n\n    for reserved_name in ReservedAttributes:\n        if reserved_name.value in item:\n            raise EncryptionError(\n                'Reserved attribute name \"{}\" is not allowed in plaintext item.'.format(reserved_name.value)\n            )\n\n    encryption_materials = crypto_config.encryption_materials()\n\n    inner_material_description = encryption_materials.material_description.copy()\n    try:\n        encryption_materials.encryption_key\n    except AttributeError:\n        if crypto_config.attribute_actions.contains_action(CryptoAction.ENCRYPT_AND_SIGN):\n            raise EncryptionError(\n                \"Attribute actions ask for some attributes to be encrypted but no encryption key is available\"\n            )\n\n        encrypted_item = item.copy()\n    else:\n        # Add the attribute encryption mode to the inner material description\n        encryption_mode = MaterialDescriptionValues.CBC_PKCS5_ATTRIBUTE_ENCRYPTION.value\n        inner_material_description[MaterialDescriptionKeys.ATTRIBUTE_ENCRYPTION_MODE.value] = encryption_mode\n\n        algorithm_descriptor = encryption_materials.encryption_key.algorithm + encryption_mode\n\n        encrypted_item = {}\n        for name, attribute in item.items():\n            if crypto_config.attribute_actions.action(name) is CryptoAction.ENCRYPT_AND_SIGN:\n                encrypted_item[name] = encrypt_attribute(\n                    attribute_name=name,\n                    attribute=attribute,\n                    encryption_key=encryption_materials.encryption_key,\n                    algorithm=algorithm_descriptor,\n                )\n            else:\n                encrypted_item[name] = attribute.copy()\n\n    signature_attribute = sign_item(encrypted_item, encryption_materials.signing_key, crypto_config)\n    encrypted_item[ReservedAttributes.SIGNATURE.value] = signature_attribute\n\n    try:\n        # Add the signing key algorithm identifier to the inner material description if provided\n        inner_material_description[\n            MaterialDescriptionKeys.SIGNING_KEY_ALGORITHM.value\n        ] = encryption_materials.signing_key.signing_algorithm()\n    except NotImplementedError:\n        # Not all signing keys will provide this value\n        pass\n\n    material_description_attribute = serialize_material_description(inner_material_description)\n    encrypted_item[ReservedAttributes.MATERIAL_DESCRIPTION.value] = material_description_attribute\n\n    return encrypted_item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encrypt_python_item(item, crypto_config):\n    # type: (dynamodb_types.ITEM, CryptoConfig) -> dynamodb_types.ITEM\n    \"\"\"Encrypt a dictionary for DynamoDB.\n\n    >>> from dynamodb_encryption_sdk.encrypted.item import encrypt_python_item\n    >>> plaintext_item = {\n    ...     'some': 'data',\n    ...     'more': 5\n    ... }\n    >>> encrypted_item = encrypt_python_item(\n    ...     item=plaintext_item,\n    ...     crypto_config=my_crypto_config\n    ... )\n\n    .. note::\n\n        This handles human-friendly dictionaries and is for use with the boto3 DynamoDB service or table resource.\n\n    :param dict item: Plaintext dictionary\n    :param CryptoConfig crypto_config: Cryptographic configuration\n    :returns: Encrypted and signed dictionary\n    :rtype: dict\n    \"\"\"\n    ddb_item = dict_to_ddb(item)\n    encrypted_ddb_item = encrypt_dynamodb_item(ddb_item, crypto_config)\n    return ddb_to_dict(encrypted_ddb_item)", "response": "Encrypt a dictionary for DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrypt_dynamodb_item(item, crypto_config):\n    # type: (dynamodb_types.ITEM, CryptoConfig) -> dynamodb_types.ITEM\n    \"\"\"Decrypt a DynamoDB item.\n\n    >>> from dynamodb_encryption_sdk.encrypted.item import decrypt_python_item\n    >>> encrypted_item = {\n    ...     'some': {'B': b'ENCRYPTED_DATA'},\n    ...     'more': {'B': b'ENCRYPTED_DATA'}\n    ... }\n    >>> decrypted_item = decrypt_python_item(\n    ...     item=encrypted_item,\n    ...     crypto_config=my_crypto_config\n    ... )\n\n    .. note::\n\n        This handles DynamoDB-formatted items and is for use with the boto3 DynamoDB client.\n\n    :param dict item: Encrypted and signed DynamoDB item\n    :param CryptoConfig crypto_config: Cryptographic configuration\n    :returns: Plaintext DynamoDB item\n    :rtype: dict\n    \"\"\"\n    unique_actions = set([crypto_config.attribute_actions.default_action.name])\n    unique_actions.update(set([action.name for action in crypto_config.attribute_actions.attribute_actions.values()]))\n\n    if crypto_config.attribute_actions.take_no_actions:\n        # If we explicitly have been told not to do anything to this item, just copy it.\n        return item.copy()\n\n    try:\n        signature_attribute = item.pop(ReservedAttributes.SIGNATURE.value)\n    except KeyError:\n        # The signature is always written, so if no signature is found then the item was not\n        # encrypted or signed.\n        raise DecryptionError(\"No signature attribute found in item\")\n\n    inner_crypto_config = crypto_config.copy()\n    # Retrieve the material description from the item if found.\n    try:\n        material_description_attribute = item.pop(ReservedAttributes.MATERIAL_DESCRIPTION.value)\n    except KeyError:\n        # If no material description is found, we use inner_crypto_config as-is.\n        pass\n    else:\n        # If material description is found, override the material description in inner_crypto_config.\n        material_description = deserialize_material_description(material_description_attribute)\n        inner_crypto_config.encryption_context.material_description = material_description\n\n    decryption_materials = inner_crypto_config.decryption_materials()\n\n    verify_item_signature(signature_attribute, item, decryption_materials.verification_key, inner_crypto_config)\n\n    try:\n        decryption_key = decryption_materials.decryption_key\n    except AttributeError:\n        if inner_crypto_config.attribute_actions.contains_action(CryptoAction.ENCRYPT_AND_SIGN):\n            raise DecryptionError(\n                \"Attribute actions ask for some attributes to be decrypted but no decryption key is available\"\n            )\n\n        return item.copy()\n\n    decryption_mode = inner_crypto_config.encryption_context.material_description.get(\n        MaterialDescriptionKeys.ATTRIBUTE_ENCRYPTION_MODE.value\n    )\n    algorithm_descriptor = decryption_key.algorithm + decryption_mode\n\n    # Once the signature has been verified, actually decrypt the item attributes.\n    decrypted_item = {}\n    for name, attribute in item.items():\n        if inner_crypto_config.attribute_actions.action(name) is CryptoAction.ENCRYPT_AND_SIGN:\n            decrypted_item[name] = decrypt_attribute(\n                attribute_name=name, attribute=attribute, decryption_key=decryption_key, algorithm=algorithm_descriptor\n            )\n        else:\n            decrypted_item[name] = attribute.copy()\n\n    return decrypted_item", "response": "Decrypt a DynamoDB item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_to_ddb(item):\n    # type: (Dict[str, Any]) -> Dict[str, Any]\n    # TODO: narrow these types down\n    \"\"\"Converts a native Python dictionary to a raw DynamoDB item.\n\n    :param dict item: Native item\n    :returns: DynamoDB item\n    :rtype: dict\n    \"\"\"\n    serializer = TypeSerializer()\n    return {key: serializer.serialize(value) for key, value in item.items()}", "response": "Converts a native Python dictionary to a DynamoDB item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a raw DynamoDB item to a native Python dictionary.", "response": "def ddb_to_dict(item):\n    # type: (Dict[str, Any]) -> Dict[str, Any]\n    # TODO: narrow these types down\n    \"\"\"Converts a raw DynamoDB item to a native Python dictionary.\n\n    :param dict item: DynamoDB item\n    :returns: Native item\n    :rtype: dict\n    \"\"\"\n    deserializer = TypeDeserializer()\n    return {key: deserializer.deserialize(value) for key, value in item.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iterable_validator(iterable_type, member_type):\n\n    def _validate_tuple(instance, attribute, value):\n        # pylint: disable=unused-argument\n        \"\"\"Validate that a dictionary is structured as expected.\n\n        :raises TypeError: if ``value`` is not of ``iterable_type`` type\n        :raises TypeError: if ``value`` members are not all of ``member_type`` type\n        \"\"\"\n        if not isinstance(value, iterable_type):\n            raise TypeError('\"{name}\" must be a {type}'.format(name=attribute.name, type=iterable_type))\n\n        for member in value:\n            if not isinstance(member, member_type):\n                raise TypeError(\n                    '\"{name}\" members must all be of type \"{type}\"'.format(name=attribute.name, type=member_type)\n                )\n\n    return _validate_tuple", "response": "Validator for attrs that performs deep type checking of iterables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that an attribute value is callable.", "response": "def callable_validator(instance, attribute, value):\n    # pylint: disable=unused-argument\n    \"\"\"Validate that an attribute value is callable.\n\n    :raises TypeError: if ``value`` is not callable\n    \"\"\"\n    if not callable(value):\n        raise TypeError('\"{name}\" value \"{value}\" must be callable'.format(name=attribute.name, value=value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencrypts a single DynamoDB attribute.", "response": "def encrypt_attribute(attribute_name, attribute, encryption_key, algorithm):\n    # type: (Text, dynamodb_types.RAW_ATTRIBUTE, DelegatedKey, Text) -> dynamodb_types.BINARY_ATTRIBUTE\n    \"\"\"Encrypt a single DynamoDB attribute.\n\n    :param str attribute_name: DynamoDB attribute name\n    :param dict attribute: Plaintext DynamoDB attribute\n    :param DelegatedKey encryption_key: DelegatedKey to use to encrypt the attribute\n    :param str algorithm: Encryption algorithm descriptor (passed to encryption_key as algorithm)\n    :returns: Encrypted DynamoDB binary attribute\n    :rtype: dict\n    \"\"\"\n    serialized_attribute = serialize_attribute(attribute)\n    encrypted_attribute = encryption_key.encrypt(\n        algorithm=algorithm, name=attribute_name, plaintext=serialized_attribute\n    )\n    return {Tag.BINARY.dynamodb_tag: encrypted_attribute}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_rsa_key(key_length):\n    private_key = rsa.generate_private_key(public_exponent=65537, key_size=key_length, backend=default_backend())\n    key_bytes = private_key.private_bytes(\n        encoding=serialization.Encoding.DER,\n        format=serialization.PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption(),\n    )\n    return key_bytes, EncryptionKeyType.PRIVATE, KeyEncodingType.DER", "response": "Generate a new RSA private key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_version():\n    _release = get_release()\n    split_version = _release.split(\".\")\n    if len(split_version) == 3:\n        return \".\".join(split_version[:2])\n    return _release", "response": "Reads the version from this module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encrypt_item(table_name, rsa_wrapping_private_key_bytes, rsa_signing_private_key_bytes):\n    index_key = {\"partition_attribute\": \"is this\", \"sort_attribute\": 55}\n    plaintext_item = {\n        \"example\": \"data\",\n        \"some numbers\": 99,\n        \"and some binary\": Binary(b\"\\x00\\x01\\x02\"),\n        \"leave me\": \"alone\",  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal table resource.\n    table = boto3.resource(\"dynamodb\").Table(table_name)\n    # Create a crypto materials provider using the provided wrapping and signing keys.\n    # We show private keys used here, but public keys could be used as well, allowing\n    # only wrapping or signature verification.\n    wrapping_key = JceNameLocalDelegatedKey(\n        key=rsa_wrapping_private_key_bytes,\n        algorithm=\"RSA\",\n        key_type=EncryptionKeyType.PRIVATE,\n        key_encoding=KeyEncodingType.DER,\n    )\n    signing_key = JceNameLocalDelegatedKey(\n        key=rsa_signing_private_key_bytes,\n        algorithm=\"SHA512withRSA\",\n        key_type=EncryptionKeyType.PRIVATE,\n        key_encoding=KeyEncodingType.DER,\n    )\n    wrapped_cmp = WrappedCryptographicMaterialsProvider(\n        wrapping_key=wrapping_key, unwrapping_key=wrapping_key, signing_key=signing_key\n    )\n    # Create attribute actions that tells the encrypted table to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted table resource.\n    encrypted_table = EncryptedTable(table=table, materials_provider=wrapped_cmp, attribute_actions=actions)\n\n    # Put the item to the table, using the encrypted table resource to transparently encrypt it.\n    encrypted_table.put_item(Item=plaintext_item)\n\n    # Get the encrypted item using the standard table resource.\n    encrypted_item = table.get_item(Key=index_key)[\"Item\"]\n\n    # Get the item using the encrypted table resource, transparently decyrpting it.\n    decrypted_item = encrypted_table.get_item(Key=index_key)[\"Item\"]\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_table.delete_item(Key=index_key)", "response": "Demonstrate use of EncryptedTable to transparently encrypt an item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deserialize_attribute(serialized_attribute):  # noqa: C901 pylint: disable=too-many-locals\n    # type: (bytes) -> dynamodb_types.RAW_ATTRIBUTE\n    \"\"\"Deserializes serialized attributes for decryption.\n\n    :param bytes serialized_attribute: Serialized attribute bytes\n    :returns: Deserialized attribute\n    :rtype: dict\n    \"\"\"\n\n    def _transform_binary_value(value):\n        # (bytes) -> bytes\n        \"\"\"Transforms a serialized binary value.\n\n        :param bytes value: Raw deserialized value\n        :rtype: bytes\n        \"\"\"\n        if isinstance(value, Binary):\n            return value.value\n        return value\n\n    def _deserialize_binary(stream):\n        # type: (io.BytesIO) -> Dict[Text, bytes]\n        \"\"\"Deserializes a binary object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        value = decode_value(stream)\n        return {Tag.BINARY.dynamodb_tag: _transform_binary_value(value)}\n\n    def _transform_string_value(value):\n        # (bytes) -> dynamodb_types.STRING\n        \"\"\"Transforms a serialized string value.\n\n        :param bytes value: Raw deserialized value\n        :rtype: dynamodb_encryption_sdk.internal.dynamodb_types.STRING\n        \"\"\"\n        return codecs.decode(value, TEXT_ENCODING)\n\n    def _deserialize_string(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.STRING]\n        \"\"\"Deserializes a string object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        value = decode_value(stream)\n        return {Tag.STRING.dynamodb_tag: _transform_string_value(value)}\n\n    def _transform_number_value(value):\n        # (bytes) -> dynamodb_types.STRING\n        \"\"\"Transforms a serialized number value.\n\n        :param bytes value: Raw deserialized value\n        :rtype: dynamodb_encryption_sdk.internal.dynamodb_types.STRING\n        \"\"\"\n        raw_value = codecs.decode(value, TEXT_ENCODING)\n        decimal_value = Decimal(to_str(raw_value)).normalize()\n        return \"{0:f}\".format(decimal_value)\n\n    def _deserialize_number(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.STRING]\n        \"\"\"Deserializes a number object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        value = decode_value(stream)\n        return {Tag.NUMBER.dynamodb_tag: _transform_number_value(value)}\n\n    _boolean_map = {TagValues.FALSE.value: False, TagValues.TRUE.value: True}\n\n    def _deserialize_boolean(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.BOOLEAN]\n        \"\"\"Deserializes a boolean object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        value = decode_byte(stream)\n        return {Tag.BOOLEAN.dynamodb_tag: _boolean_map[value]}\n\n    def _deserialize_null(stream):  # we want a consistent API but don't use stream, so pylint: disable=unused-argument\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.BOOLEAN]\n        \"\"\"Deserializes a null object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        return {Tag.NULL.dynamodb_tag: True}\n\n    def _deserialize_set(stream, member_transform):\n        # type: (io.BytesIO, Callable) -> List[Union[dynamodb_types.BINARY, dynamodb_types.STRING]]\n        \"\"\"Deserializes contents of serialized set.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: list\n        \"\"\"\n        member_count = decode_length(stream)\n        return sorted([member_transform(decode_value(stream)) for _ in range(member_count)])\n\n    def _deserialize_binary_set(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.SET[dynamodb_types.BINARY]]\n        \"\"\"Deserializes a binary set object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        return {Tag.BINARY_SET.dynamodb_tag: _deserialize_set(stream, _transform_binary_value)}\n\n    def _deserialize_string_set(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.SET[dynamodb_types.STRING]]\n        \"\"\"Deserializes a string set object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        return {Tag.STRING_SET.dynamodb_tag: _deserialize_set(stream, _transform_string_value)}\n\n    def _deserialize_number_set(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.SET[dynamodb_types.STRING]]\n        \"\"\"Deserializes a number set object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        return {Tag.NUMBER_SET.dynamodb_tag: _deserialize_set(stream, _transform_number_value)}\n\n    def _deserialize_list(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.LIST]\n        \"\"\"Deserializes a list object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        member_count = decode_length(stream)\n        return {Tag.LIST.dynamodb_tag: [_deserialize(stream) for _ in range(member_count)]}\n\n    def _deserialize_map(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.MAP]\n        \"\"\"Deserializes a map object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        member_count = decode_length(stream)\n        members = {}  # type: dynamodb_types.MAP\n        for _ in range(member_count):\n            key = _deserialize(stream)\n            if Tag.STRING.dynamodb_tag not in key:\n                raise DeserializationError(\n                    'Malformed serialized map: found \"{}\" as map key.'.format(list(key.keys())[0])\n                )\n\n            value = _deserialize(stream)\n            members[key[Tag.STRING.dynamodb_tag]] = value\n\n        return {Tag.MAP.dynamodb_tag: members}\n\n    def _deserialize_function(tag):\n        # type: (bytes) -> Callable\n        \"\"\"Identifies the correct deserialization function based on the provided tag.\n\n        :param tag: Identifying tag, read from start of serialized object\n        :type tag: dynamodb_encryption_sdk.internal.identifiers.Tag\n        :rtype: callable\n        \"\"\"\n        deserialize_functions = {\n            Tag.BINARY.tag: _deserialize_binary,\n            Tag.BINARY_SET.tag: _deserialize_binary_set,\n            Tag.NUMBER.tag: _deserialize_number,\n            Tag.NUMBER_SET.tag: _deserialize_number_set,\n            Tag.STRING.tag: _deserialize_string,\n            Tag.STRING_SET.tag: _deserialize_string_set,\n            Tag.BOOLEAN.tag: _deserialize_boolean,\n            Tag.NULL.tag: _deserialize_null,\n            Tag.LIST.tag: _deserialize_list,\n            Tag.MAP.tag: _deserialize_map,\n        }\n        try:\n            return deserialize_functions[tag]\n        except KeyError:\n            raise DeserializationError('Unsupported tag: \"{}\"'.format(tag))\n\n    def _deserialize(stream):\n        # type: (io.BytesIO) -> Dict[Text, dynamodb_types.RAW_ATTRIBUTE]\n        \"\"\"Deserializes a serialized object.\n\n        :param stream: Stream containing serialized object\n        :type stream: io.BytesIO\n        :rtype: dict\n        \"\"\"\n        try:\n            tag = decode_tag(stream)\n            return _deserialize_function(tag)(stream)\n        except struct.error:\n            raise DeserializationError(\"Malformed serialized data\")\n\n    if not serialized_attribute:\n        raise DeserializationError(\"Empty serialized attribute data\")\n\n    stream = io.BytesIO(serialized_attribute)\n    return _deserialize(stream)", "response": "Deserializes the given serialized attribute bytes into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_value(value):\n    # type: (bytes) -> bytes\n    \"\"\"Encodes the value in Length-Value format.\n\n    :param value: Value to encode\n    :type value: six.string_types or :class:`boto3.dynamodb_encryption_sdk.types.Binary`\n    :returns: Length-Value encoded value\n    :rtype: bytes\n    \"\"\"\n    return struct.pack(\">I{attr_len:d}s\".format(attr_len=len(value)), len(value), value)", "response": "Encodes the value in Length - Value format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encrypt_item(table_name, aws_cmk_id):\n    index_key = {\"partition_attribute\": \"is this\", \"sort_attribute\": 55}\n    plaintext_item = {\n        \"example\": \"data\",\n        \"some numbers\": 99,\n        \"and some binary\": Binary(b\"\\x00\\x01\\x02\"),\n        \"leave me\": \"alone\",  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal table resource.\n    table = boto3.resource(\"dynamodb\").Table(table_name)\n\n    # Use the TableInfo helper to collect information about the indexes.\n    table_info = TableInfo(name=table_name)\n    table_info.refresh_indexed_attributes(table.meta.client)\n\n    # Create a crypto materials provider using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n\n    encryption_context = EncryptionContext(\n        table_name=table_name,\n        partition_key_name=table_info.primary_index.partition,\n        sort_key_name=table_info.primary_index.sort,\n        # The only attributes that are used by the AWS KMS cryptographic materials providers\n        # are the primary index attributes.\n        # These attributes need to be in the form of a DynamoDB JSON structure, so first\n        # convert the standard dictionary.\n        attributes=dict_to_ddb(index_key),\n    )\n\n    # Create attribute actions that tells the encrypted table to encrypt all attributes,\n    # only sign the primary index attributes, and ignore the one identified attribute to\n    # ignore.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    actions.set_index_keys(*table_info.protected_index_keys())\n\n    # Build the crypto config to use for this item.\n    # When using the higher-level helpers, this is handled for you.\n    crypto_config = CryptoConfig(\n        materials_provider=aws_kms_cmp, encryption_context=encryption_context, attribute_actions=actions\n    )\n\n    # Encrypt the plaintext item directly\n    encrypted_item = encrypt_python_item(plaintext_item, crypto_config)\n\n    # You could now put the encrypted item to DynamoDB just as you would any other item.\n    # table.put_item(Item=encrypted_item)\n    # We will skip this for the purposes of this example.\n\n    # Decrypt the encrypted item directly\n    decrypted_item = decrypt_python_item(encrypted_item, crypto_config)\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]", "response": "Demonstrate use of EncryptedTable to transparently encrypt an item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndemonstrating use of EncryptedClient to transparently encrypt an item.", "response": "def encrypt_item(table_name, aws_cmk_id):\n    \"\"\"Demonstrate use of EncryptedClient to transparently encrypt an item.\"\"\"\n    index_key = {\"partition_attribute\": {\"S\": \"is this\"}, \"sort_attribute\": {\"N\": \"55\"}}\n    plaintext_item = {\n        \"example\": {\"S\": \"data\"},\n        \"some numbers\": {\"N\": \"99\"},\n        \"and some binary\": {\"B\": b\"\\x00\\x01\\x02\"},\n        \"leave me\": {\"S\": \"alone\"},  # We want to ignore this attribute\n    }\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_item.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_key.keys())\n    unencrypted_attributes.add(\"leave me\")\n    # Add the index pairs to the item.\n    plaintext_item.update(index_key)\n\n    # Create a normal client.\n    client = boto3.client(\"dynamodb\")\n    # Create a crypto materials provider using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n    # Create attribute actions that tells the encrypted client to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted client.\n    encrypted_client = EncryptedClient(client=client, materials_provider=aws_kms_cmp, attribute_actions=actions)\n\n    # Put the item to the table, using the encrypted client to transparently encrypt it.\n    encrypted_client.put_item(TableName=table_name, Item=plaintext_item)\n\n    # Get the encrypted item using the standard client.\n    encrypted_item = client.get_item(TableName=table_name, Key=index_key)[\"Item\"]\n\n    # Get the item using the encrypted client, transparently decyrpting it.\n    decrypted_item = encrypted_client.get_item(TableName=table_name, Key=index_key)[\"Item\"]\n\n    # Verify that all of the attributes are different in the encrypted item\n    for name in encrypted_attributes:\n        assert encrypted_item[name] != plaintext_item[name]\n        assert decrypted_item[name] == plaintext_item[name]\n\n    # Verify that all of the attributes that should not be encrypted were not.\n    for name in unencrypted_attributes:\n        assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_client.delete_item(TableName=table_name, Key=index_key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndemonstrate use of EncryptedClient to transparently encrypt multiple items in a batch request.", "response": "def encrypt_batch_items(table_name, aws_cmk_id):\n    \"\"\"Demonstrate use of EncryptedClient to transparently encrypt multiple items in a batch request.\"\"\"\n    index_keys = [\n        {\"partition_attribute\": {\"S\": \"is this\"}, \"sort_attribute\": {\"N\": \"55\"}},\n        {\"partition_attribute\": {\"S\": \"is this\"}, \"sort_attribute\": {\"N\": \"56\"}},\n        {\"partition_attribute\": {\"S\": \"is this\"}, \"sort_attribute\": {\"N\": \"57\"}},\n        {\"partition_attribute\": {\"S\": \"another\"}, \"sort_attribute\": {\"N\": \"55\"}},\n    ]\n    plaintext_additional_attributes = {\n        \"example\": {\"S\": \"data\"},\n        \"some numbers\": {\"N\": \"99\"},\n        \"and some binary\": {\"B\": b\"\\x00\\x01\\x02\"},\n        \"leave me\": {\"S\": \"alone\"},  # We want to ignore this attribute\n    }\n    plaintext_items = []\n    for key in index_keys:\n        _attributes = key.copy()\n        _attributes.update(plaintext_additional_attributes)\n        plaintext_items.append(_attributes)\n\n    # Collect all of the attributes that will be encrypted (used later).\n    encrypted_attributes = set(plaintext_additional_attributes.keys())\n    encrypted_attributes.remove(\"leave me\")\n    # Collect all of the attributes that will not be encrypted (used later).\n    unencrypted_attributes = set(index_keys[0].keys())\n    unencrypted_attributes.add(\"leave me\")\n\n    # Create a normal client.\n    client = boto3.client(\"dynamodb\")\n    # Create a crypto materials provider using the specified AWS KMS key.\n    aws_kms_cmp = AwsKmsCryptographicMaterialsProvider(key_id=aws_cmk_id)\n    # Create attribute actions that tells the encrypted client to encrypt all attributes except one.\n    actions = AttributeActions(\n        default_action=CryptoAction.ENCRYPT_AND_SIGN, attribute_actions={\"leave me\": CryptoAction.DO_NOTHING}\n    )\n    # Use these objects to create an encrypted client.\n    encrypted_client = EncryptedClient(client=client, materials_provider=aws_kms_cmp, attribute_actions=actions)\n\n    # Put the items to the table, using the encrypted client to transparently encrypt them.\n    encrypted_client.batch_write_item(\n        RequestItems={table_name: [{\"PutRequest\": {\"Item\": item}} for item in plaintext_items]}\n    )\n\n    # Get the encrypted item using the standard client.\n    encrypted_items = client.batch_get_item(RequestItems={table_name: {\"Keys\": index_keys}})[\"Responses\"][table_name]\n\n    # Get the item using the encrypted client, transparently decyrpting it.\n    decrypted_items = encrypted_client.batch_get_item(RequestItems={table_name: {\"Keys\": index_keys}})[\"Responses\"][\n        table_name\n    ]\n\n    def _select_index_from_item(item):\n        \"\"\"Find the index keys that match this item.\"\"\"\n        for index in index_keys:\n            if all([item[key] == value for key, value in index.items()]):\n                return index\n\n        raise Exception(\"Index key not found in item.\")\n\n    def _select_item_from_index(index, all_items):\n        \"\"\"Find the item that matches these index keys.\"\"\"\n        for item in all_items:\n            if all([item[key] == value for key, value in index.items()]):\n                return item\n\n        raise Exception(\"Index key not found in item.\")\n\n    for encrypted_item in encrypted_items:\n        key = _select_index_from_item(encrypted_item)\n        plaintext_item = _select_item_from_index(key, plaintext_items)\n        decrypted_item = _select_item_from_index(key, decrypted_items)\n\n        # Verify that all of the attributes are different in the encrypted item\n        for name in encrypted_attributes:\n            assert encrypted_item[name] != plaintext_item[name]\n            assert decrypted_item[name] == plaintext_item[name]\n\n        # Verify that all of the attributes that should not be encrypted were not.\n        for name in unencrypted_attributes:\n            assert decrypted_item[name] == encrypted_item[name] == plaintext_item[name]\n\n    # Clean up the item\n    encrypted_client.batch_write_item(\n        RequestItems={table_name: [{\"DeleteRequest\": {\"Key\": key}} for key in index_keys]}\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload an RSA key from the provided raw key bytes.", "response": "def load_rsa_key(key, key_type, key_encoding):\n    # (bytes, EncryptionKeyType, KeyEncodingType) -> Any\n    # TODO: narrow down the output type\n    \"\"\"Load an RSA key object from the provided raw key bytes.\n\n    :param bytes key: Raw key bytes to load\n    :param EncryptionKeyType key_type: Type of key to load\n    :param KeyEncodingType key_encoding: Encoding used to serialize ``key``\n    :returns: Loaded key\n    :rtype: TODO:\n    :raises ValueError: if ``key_type`` and ``key_encoding`` are not a valid pairing\n    \"\"\"\n    try:\n        loader = _RSA_KEY_LOADING[key_type][key_encoding]\n    except KeyError:\n        raise ValueError(\"Invalid key type and encoding: {} and {}\".format(key_type, key_encoding))\n\n    kwargs = dict(data=key, backend=default_backend())\n    if key_type is EncryptionKeyType.PRIVATE:\n        kwargs[\"password\"] = None\n\n    loaded_key = loader(**kwargs)\n\n    if loaded_key.key_size < MinimumKeySizes.RSA.value:\n        _LOGGER.warning(\"RSA keys smaller than %d bits are unsafe\" % MinimumKeySizes.RSA.value)\n\n    return loaded_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable encryption methods for ciphers that support them.", "response": "def _disable_encryption(self):\n        # () -> None\n        \"\"\"Enable encryption methods for ciphers that support them.\"\"\"\n        self.encrypt = self._disabled_encrypt\n        self.decrypt = self._disabled_decrypt"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a key from bytes.", "response": "def load_key(self, key, key_type, key_encoding):\n        \"\"\"Load a key from bytes.\n\n        :param bytes key: Key bytes\n        :param EncryptionKeyType key_type: Type of key\n        :param KeyEncodingType key_encoding: Encoding used to serialize key\n        :returns: Loaded key\n        \"\"\"\n        if key_type is not EncryptionKeyType.SYMMETRIC:\n            raise ValueError(\n                'Invalid key type \"{key_type}\" for cipher \"{cipher}\"'.format(key_type=key_type, cipher=self.java_name)\n            )\n\n        if key_encoding is not KeyEncodingType.RAW:\n            raise ValueError(\n                'Invalid key encoding \"{key_encoding}\" for cipher \"{cipher}\"'.format(\n                    key_encoding=key_encoding, cipher=self.java_name\n                )\n            )\n\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap a key using AES keywrap.", "response": "def wrap(self, wrapping_key, key_to_wrap):\n        # type: (bytes, bytes) -> bytes\n        \"\"\"Wrap key using AES keywrap.\n\n        :param bytes wrapping_key: Loaded key with which to wrap\n        :param bytes key_to_wrap: Raw key to wrap\n        :returns: Wrapped key\n        :rtype: bytes\n        \"\"\"\n        if self.java_name not in (\"AES\", \"AESWrap\"):\n            raise NotImplementedError('\"wrap\" is not supported by the \"{}\" cipher'.format(self.java_name))\n\n        try:\n            return keywrap.aes_key_wrap(wrapping_key=wrapping_key, key_to_wrap=key_to_wrap, backend=default_backend())\n        except Exception:\n            error_message = \"Key wrap failed\"\n            _LOGGER.exception(error_message)\n            raise WrappingError(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nunwrap a key using AES keywrap.", "response": "def unwrap(self, wrapping_key, wrapped_key):\n        # type: (bytes, bytes) -> bytes\n        \"\"\"Unwrap key using AES keywrap.\n\n        :param bytes wrapping_key: Loaded key with which to unwrap\n        :param bytes wrapped_key: Wrapped key to unwrap\n        :returns: Unwrapped key\n        :rtype: bytes\n        \"\"\"\n        if self.java_name not in (\"AES\", \"AESWrap\"):\n            raise NotImplementedError('\"unwrap\" is not supported by this cipher')\n\n        try:\n            return keywrap.aes_key_unwrap(wrapping_key=wrapping_key, wrapped_key=wrapped_key, backend=default_backend())\n        except Exception:\n            error_message = \"Key unwrap failed\"\n            _LOGGER.exception(error_message)\n            raise UnwrappingError(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencrypts data using the supplied key.", "response": "def encrypt(self, key, data, mode, padding):\n        # this can be disabled by _disable_encryption, so pylint: disable=method-hidden\n        \"\"\"Encrypt data using the supplied values.\n\n        :param bytes key: Loaded encryption key\n        :param bytes data: Data to encrypt\n        :param JavaMode mode: Encryption mode to use\n        :param JavaPadding padding: Padding mode to use\n        :returns: IV prepended to encrypted data\n        :rtype: bytes\n        \"\"\"\n        try:\n            block_size = self.cipher.block_size\n            iv_len = block_size // 8\n            iv = os.urandom(iv_len)\n\n            encryptor = Cipher(self.cipher(key), mode.build(iv), backend=default_backend()).encryptor()\n            padder = padding.build(block_size).padder()\n\n            padded_data = padder.update(data) + padder.finalize()\n            return iv + encryptor.update(padded_data) + encryptor.finalize()\n        except Exception:\n            error_message = \"Encryption failed\"\n            _LOGGER.exception(error_message)\n            raise EncryptionError(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_key(self, key, key_type, key_encoding):\n        if key_type not in (EncryptionKeyType.PRIVATE, EncryptionKeyType.PUBLIC):\n            raise ValueError(\n                'Invalid key type \"{key_type}\" for cipher \"{cipher}\"'.format(key_type=key_type, cipher=self.java_name)\n            )\n\n        if key_encoding not in (KeyEncodingType.DER, KeyEncodingType.PEM):\n            raise ValueError(\n                'Invalid key encoding \"{key_encoding}\" for cipher \"{cipher}\"'.format(\n                    key_encoding=key_encoding, cipher=self.java_name\n                )\n            )\n\n        return _KEY_LOADERS[self.cipher](key, key_type, key_encoding)", "response": "Load a key from bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encrypt(self, key, data, mode, padding):\n        # pylint: disable=unused-argument,no-self-use\n        \"\"\"Encrypt data using the supplied values.\n\n        :param bytes key: Loaded encryption key\n        :param bytes data: Data to encrypt\n        :param JavaMode mode: Encryption mode to use (not used by :class:`JavaAsymmetricEncryptionAlgorithm`)\n        :param JavaPadding padding: Padding mode to use\n        :returns: Encrypted data\n        :rtype: bytes\n        \"\"\"\n        if hasattr(key, \"private_bytes\"):\n            _key = key.public_key()\n        else:\n            _key = key\n        try:\n            return _key.encrypt(data, padding.build())\n        except Exception:\n            error_message = \"Encryption failed\"\n            _LOGGER.exception(error_message)\n            raise EncryptionError(error_message)", "response": "Encrypt data using the supplied key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that the values in value are DynamoDB items.", "response": "def _validate_attribute_values_are_ddb_items(instance, attribute, value):  # pylint: disable=unused-argument\n    \"\"\"Validate that dictionary values in ``value`` match the structure of DynamoDB JSON\n    items.\n\n    .. note::\n\n        We are not trying to validate the full structure of the item with this validator.\n        This is just meant to verify that the values roughly match the correct format.\n    \"\"\"\n    for data in value.values():\n        if len(list(data.values())) != 1:\n            raise TypeError('\"{}\" values do not look like DynamoDB items'.format(attribute.name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_get_arguments(kwargs):\n    # type: (Dict[Text, Any]) -> None\n    \"\"\"Verify that attribute filtering parameters are not found in the request.\n\n    :raises InvalidArgumentError: if banned parameters are found\n    \"\"\"\n    for arg in (\"AttributesToGet\", \"ProjectionExpression\"):\n        if arg in kwargs:\n            raise InvalidArgumentError('\"{}\" is not supported for this operation'.format(arg))\n\n    if kwargs.get(\"Select\", None) in (\"SPECIFIC_ATTRIBUTES\", \"ALL_PROJECTED_ATTRIBUTES\"):\n        raise InvalidArgumentError('Scan \"Select\" value of \"{}\" is not supported'.format(kwargs[\"Select\"]))", "response": "Verify that the requested attributes to get are not found in the request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npull all encryption - specific parameters from the request and use them to build a crypto config.", "response": "def crypto_config_from_kwargs(fallback, **kwargs):\n    \"\"\"Pull all encryption-specific parameters from the request and use them to build a crypto config.\n\n    :returns: crypto config and updated kwargs\n    :rtype: dynamodb_encryption_sdk.encrypted.CryptoConfig and dict\n    \"\"\"\n    try:\n        crypto_config = kwargs.pop(\"crypto_config\")\n    except KeyError:\n        try:\n            fallback_kwargs = {\"table_name\": kwargs[\"TableName\"]}\n        except KeyError:\n            fallback_kwargs = {}\n        crypto_config = fallback(**fallback_kwargs)\n    return crypto_config, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_config_from_table_info(materials_provider, attribute_actions, table_info):\n    ec_kwargs = table_info.encryption_context_values\n    if table_info.primary_index is not None:\n        ec_kwargs.update(\n            {\"partition_key_name\": table_info.primary_index.partition, \"sort_key_name\": table_info.primary_index.sort}\n        )\n\n    return CryptoConfig(\n        materials_provider=materials_provider,\n        encryption_context=EncryptionContext(**ec_kwargs),\n        attribute_actions=attribute_actions,\n    )", "response": "Builds a CryptoConfig from the provided values and table info."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a crypto config from the provided values loading the table info from the cache.", "response": "def crypto_config_from_cache(materials_provider, attribute_actions, table_info_cache, table_name):\n    \"\"\"Build a crypto config from the provided values, loading the table info from the provided cache.\n\n    :returns: crypto config and updated kwargs\n    :rtype: tuple(CryptoConfig, dict)\n    \"\"\"\n    table_info = table_info_cache.table_info(table_name)\n\n    attribute_actions = attribute_actions.copy()\n    attribute_actions.set_index_keys(*table_info.protected_index_keys())\n\n    return crypto_config_from_table_info(materials_provider, attribute_actions, table_info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decrypt_get_item(decrypt_method, crypto_config_method, read_method, **kwargs):\n    # type: (Callable, Callable, Callable, **Any) -> Dict\n    # TODO: narrow this down\n    \"\"\"Transparently decrypt an item after getting it from the table.\n\n    :param callable decrypt_method: Method to use to decrypt item\n    :param callable crypto_config_method: Method that accepts ``kwargs`` and provides a :class:`CryptoConfig`\n    :param callable read_method: Method that reads from the table\n    :param **kwargs: Keyword arguments to pass to ``read_method``\n    :return: DynamoDB response\n    :rtype: dict\n    \"\"\"\n    validate_get_arguments(kwargs)\n    crypto_config, ddb_kwargs = crypto_config_method(**kwargs)\n    response = read_method(**ddb_kwargs)\n    if \"Item\" in response:\n        response[\"Item\"] = decrypt_method(\n            item=response[\"Item\"],\n            crypto_config=crypto_config.with_item(_item_transformer(decrypt_method)(response[\"Item\"])),\n        )\n    return response", "response": "Transparently decrypt an item from the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt_batch_get_item(decrypt_method, crypto_config_method, read_method, **kwargs):\n    # type: (Callable, Callable, Callable, **Any) -> Dict\n    # TODO: narrow this down\n    \"\"\"Transparently decrypt multiple items after getting them in a batch request.\n\n    :param callable decrypt_method: Method to use to decrypt items\n    :param callable crypto_config_method: Method that accepts ``kwargs`` and provides a :class:`CryptoConfig`\n    :param callable read_method: Method that reads from the table\n    :param **kwargs: Keyword arguments to pass to ``read_method``\n    :return: DynamoDB response\n    :rtype: dict\n    \"\"\"\n    request_crypto_config = kwargs.pop(\"crypto_config\", None)\n\n    for _table_name, table_kwargs in kwargs[\"RequestItems\"].items():\n        validate_get_arguments(table_kwargs)\n\n    response = read_method(**kwargs)\n    for table_name, items in response[\"Responses\"].items():\n        if request_crypto_config is not None:\n            crypto_config = request_crypto_config\n        else:\n            crypto_config = crypto_config_method(table_name=table_name)\n\n        for pos, value in enumerate(items):\n            items[pos] = decrypt_method(\n                item=value, crypto_config=crypto_config.with_item(_item_transformer(decrypt_method)(items[pos]))\n            )\n    return response", "response": "Transparently decrypt multiple items in a batch request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encrypt_batch_write_item(encrypt_method, crypto_config_method, write_method, **kwargs):\n    # type: (Callable, Callable, Callable, **Any) -> Dict\n    # TODO: narrow this down\n    \"\"\"Transparently encrypt multiple items before putting them in a batch request.\n\n    :param callable encrypt_method: Method to use to encrypt items\n    :param callable crypto_config_method: Method that accepts a table name string and provides a :class:`CryptoConfig`\n    :param callable write_method: Method that writes to the table\n    :param **kwargs: Keyword arguments to pass to ``write_method``\n    :return: DynamoDB response\n    :rtype: dict\n    \"\"\"\n    request_crypto_config = kwargs.pop(\"crypto_config\", None)\n    table_crypto_configs = {}\n    plaintext_items = copy.deepcopy(kwargs[\"RequestItems\"])\n\n    for table_name, items in kwargs[\"RequestItems\"].items():\n        if request_crypto_config is not None:\n            crypto_config = request_crypto_config\n        else:\n            crypto_config = crypto_config_method(table_name=table_name)\n        table_crypto_configs[table_name] = crypto_config\n\n        for pos, value in enumerate(items):\n            for request_type, item in value.items():\n                # We don't encrypt primary indexes, so we can ignore DeleteItem requests\n                if request_type == \"PutRequest\":\n                    items[pos][request_type][\"Item\"] = encrypt_method(\n                        item=item[\"Item\"],\n                        crypto_config=crypto_config.with_item(_item_transformer(encrypt_method)(item[\"Item\"])),\n                    )\n\n    response = write_method(**kwargs)\n    return _process_batch_write_response(plaintext_items, response, table_crypto_configs)", "response": "Transparently encrypt multiple items in a batch request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the response from a transparently encrypted write.", "response": "def _process_batch_write_response(request, response, table_crypto_config):\n    # type: (Dict, Dict, Dict[Text, CryptoConfig]) -> Dict\n    \"\"\"Handle unprocessed items in the response from a transparently encrypted write.\n\n    :param dict request: The DynamoDB plaintext request dictionary\n    :param dict response: The DynamoDB response from the batch operation\n    :param Dict[Text, CryptoConfig] table_crypto_config: table level CryptoConfig used in encrypting the request items\n    :return: DynamoDB response, with any unprocessed items reverted back to the original plaintext values\n    :rtype: dict\n    \"\"\"\n    try:\n        unprocessed_items = response[\"UnprocessedItems\"]\n    except KeyError:\n        return response\n\n    # Unprocessed items need to be returned in their original state\n    for table_name, unprocessed in unprocessed_items.items():\n        original_items = request[table_name]\n        crypto_config = table_crypto_config[table_name]\n\n        if crypto_config.encryption_context.partition_key_name:\n            items_match = partial(_item_keys_match, crypto_config)\n        else:\n            items_match = partial(_item_attributes_match, crypto_config)\n\n        for pos, operation in enumerate(unprocessed):\n            for request_type, item in operation.items():\n                if request_type != \"PutRequest\":\n                    continue\n\n                for plaintext_item in original_items:\n                    if plaintext_item.get(request_type) and items_match(\n                        plaintext_item[request_type][\"Item\"], item[\"Item\"]\n                    ):\n                        unprocessed[pos] = plaintext_item.copy()\n                        break\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if the values in the primary and sort keys ( if they exist ) are the same", "response": "def _item_keys_match(crypto_config, item1, item2):\n    # type: (CryptoConfig, Dict, Dict) -> Bool\n    \"\"\"Determines whether the values in the primary and sort keys (if they exist) are the same\n\n    :param CryptoConfig crypto_config: CryptoConfig used in encrypting the given items\n    :param dict item1: The first item to compare\n    :param dict item2: The second item to compare\n    :return: Bool response, True if the key attributes match\n    :rtype: bool\n    \"\"\"\n    partition_key_name = crypto_config.encryption_context.partition_key_name\n    sort_key_name = crypto_config.encryption_context.sort_key_name\n\n    partition_keys_match = item1[partition_key_name] == item2[partition_key_name]\n\n    if sort_key_name is None:\n        return partition_keys_match\n\n    return partition_keys_match and item1[sort_key_name] == item2[sort_key_name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether the unencrypted values in the plaintext item attributes are the same as those in the encrypted item.", "response": "def _item_attributes_match(crypto_config, plaintext_item, encrypted_item):\n    # type: (CryptoConfig, Dict, Dict) -> Bool\n    \"\"\"Determines whether the unencrypted values in the plaintext items attributes are the same as those in the\n    encrypted item. Essentially this uses brute force to cover when we don't know the primary and sort\n    index attribute names, since they can't be encrypted.\n\n    :param CryptoConfig crypto_config: CryptoConfig used in encrypting the given items\n    :param dict plaintext_item: The plaintext item\n    :param dict encrypted_item: The encrypted item\n    :return: Bool response, True if the unencrypted attributes in the plaintext item match those in\n    the encrypted item\n    :rtype: bool\n    \"\"\"\n\n    for name, value in plaintext_item.items():\n        if crypto_config.attribute_actions.action(name) == CryptoAction.ENCRYPT_AND_SIGN:\n            continue\n\n        if encrypted_item.get(name) != value:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a list of the item s key value pairs as tuples sorted by the keys transformed by transform.", "response": "def _sorted_key_map(item, transform=to_bytes):\n    \"\"\"Creates a list of the item's key/value pairs as tuples, sorted by the keys transformed by transform.\n\n    :param dict item: Source dictionary\n    :param function transform: Transform function\n    :returns: List of tuples containing transformed key, original value, and original key for each entry\n    :rtype: list(tuple)\n    \"\"\"\n    sorted_items = []\n    for key, value in item.items():\n        _key = transform(key)\n        sorted_items.append((_key, value, key))\n    sorted_items = sorted(sorted_items, key=lambda x: x[0])\n    return sorted_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize_attribute(attribute):  # noqa: C901 pylint: disable=too-many-locals\n    # type: (dynamodb_types.RAW_ATTRIBUTE) -> bytes\n    \"\"\"Serializes a raw attribute to a byte string as defined for the DynamoDB Client-Side Encryption Standard.\n\n    :param dict attribute: Item attribute value\n    :returns: Serialized attribute\n    :rtype: bytes\n    \"\"\"\n\n    def _transform_binary_value(value):\n        # type: (dynamodb_types.BINARY) -> bytes\n        \"\"\"\n        :param value: Input value\n        :type value: boto3.dynamodb.types.Binary\n        :returns: bytes value\n        :rtype: bytes\n        \"\"\"\n        if isinstance(value, Binary):\n            return bytes(value.value)\n        return bytes(value)\n\n    def _serialize_binary(_attribute):\n        # type: (dynamodb_types.BINARY) -> bytes\n        \"\"\"\n        :param _attribute: Attribute to serialize\n        :type _attribute: boto3.dynamodb.types.Binary\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _RESERVED + Tag.BINARY.tag + encode_value(_transform_binary_value(_attribute))\n\n    def _transform_number_value(value):\n        # type: (str) -> bytes\n        \"\"\"\n        :param value: Input value\n        :type value: numbers.Number\n        :returns: bytes value\n        :rtype: bytes\n        \"\"\"\n        # At this point we are receiving values which have already been transformed\n        # by dynamodb.TypeSerializer, so all numbers are str. However, TypeSerializer\n        # leaves trailing zeros if they are defined in the Decimal call, but we need to\n        # strip all trailing zeros.\n        decimal_value = DYNAMODB_CONTEXT.create_decimal(value).normalize()\n        return \"{0:f}\".format(decimal_value).encode(\"utf-8\")\n\n    def _serialize_number(_attribute):\n        # type: (str) -> bytes\n        \"\"\"\n        :param _attribute: Attribute to serialize\n        :type _attribute: numbers.Number\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _RESERVED + Tag.NUMBER.tag + encode_value(_transform_number_value(_attribute))\n\n    def _transform_string_value(value):\n        # type: (dynamodb_types.STRING) -> bytes\n        \"\"\"\n        :param value: Input value\n        :type value: bytes or str\n        :returns: bytes value\n        :rtype: bytes\n        \"\"\"\n        return to_bytes(value)\n\n    def _serialize_string(_attribute):\n        # type: (dynamodb_types.STRING) -> bytes\n        \"\"\"\n        :param _attribute: Attribute to serialize\n        :type _attribute: six.string_types\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _RESERVED + Tag.STRING.tag + encode_value(_transform_string_value(_attribute))\n\n    def _serialize_boolean(_attribute):\n        # type: (dynamodb_types.BOOLEAN) -> bytes\n        \"\"\"\n        :param bool _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        _attribute_value = TagValues.TRUE.value if _attribute else TagValues.FALSE.value\n        return _RESERVED + Tag.BOOLEAN.tag + _attribute_value\n\n    def _serialize_null(_attribute):\n        # type: (dynamodb_types.NULL) -> bytes\n        \"\"\"\n        :param _attribute: Attribute to serialize\n        :type _attribute: types.NoneType\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _RESERVED + Tag.NULL.tag\n\n    def _serialize_set(tag, _attribute, member_function):\n        # type: (Tag, dynamodb_types.SET[dynamodb_types.ATTRIBUTE], Callable) -> bytes\n        \"\"\"\n        :param bytes tag: Tag to identify this set\n        :param set _attribute: Attribute to serialize\n        :param member_function: Serialization function for members\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        serialized_attribute = io.BytesIO()\n        serialized_attribute.write(_RESERVED)\n        serialized_attribute.write(tag.tag)\n        serialized_attribute.write(encode_length(_attribute))\n\n        encoded_members = []\n        for member in _attribute:\n            encoded_members.append(member_function(member))\n        for member in sorted(encoded_members):\n            serialized_attribute.write(encode_value(member))\n\n        return serialized_attribute.getvalue()\n\n    def _serialize_binary_set(_attribute):\n        # type: (dynamodb_types.SET[dynamodb_types.ATTRIBUTE]) -> bytes\n        \"\"\"\n        :param set _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _serialize_set(Tag.BINARY_SET, _attribute, _transform_binary_value)\n\n    def _serialize_number_set(_attribute):\n        # type: (dynamodb_types.SET[dynamodb_types.ATTRIBUTE]) -> bytes\n        \"\"\"\n        :param set _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _serialize_set(Tag.NUMBER_SET, _attribute, _transform_number_value)\n\n    def _serialize_string_set(_attribute):\n        # type: (dynamodb_types.SET[dynamodb_types.ATTRIBUTE]) -> bytes\n        \"\"\"\n        :param set _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        return _serialize_set(Tag.STRING_SET, _attribute, _transform_string_value)\n\n    def _serialize_list(_attribute):\n        # type: (dynamodb_types.LIST) -> bytes\n        \"\"\"\n        :param list _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        serialized_attribute = io.BytesIO()\n        serialized_attribute.write(_RESERVED)\n        serialized_attribute.write(Tag.LIST.tag)\n        serialized_attribute.write(encode_length(_attribute))\n        for member in _attribute:\n            serialized_attribute.write(serialize_attribute(member))\n\n        return serialized_attribute.getvalue()\n\n    def _serialize_map(_attribute):\n        # type: (dynamodb_types.MAP) -> bytes\n        \"\"\"\n        :param list _attribute: Attribute to serialize\n        :returns: Serialized _attribute\n        :rtype: bytes\n        \"\"\"\n        serialized_attribute = io.BytesIO()\n        serialized_attribute.write(_RESERVED)\n        serialized_attribute.write(Tag.MAP.tag)\n        serialized_attribute.write(encode_length(_attribute))\n\n        sorted_items = _sorted_key_map(item=_attribute, transform=_transform_string_value)\n\n        for key, value, _original_key in sorted_items:\n            serialized_attribute.write(_serialize_string(key))\n            serialized_attribute.write(serialize_attribute(value))\n\n        return serialized_attribute.getvalue()\n\n    def _serialize_function(dynamodb_tag):\n        # type: (str) -> Callable[[dynamodb_types.ATTRIBUTE], bytes]\n        \"\"\"Locates the appropriate serialization function for the specified DynamoDB attribute tag.\"\"\"\n        serialize_functions = {\n            Tag.BINARY.dynamodb_tag: _serialize_binary,\n            Tag.BINARY_SET.dynamodb_tag: _serialize_binary_set,\n            Tag.NUMBER.dynamodb_tag: _serialize_number,\n            Tag.NUMBER_SET.dynamodb_tag: _serialize_number_set,\n            Tag.STRING.dynamodb_tag: _serialize_string,\n            Tag.STRING_SET.dynamodb_tag: _serialize_string_set,\n            Tag.BOOLEAN.dynamodb_tag: _serialize_boolean,\n            Tag.NULL.dynamodb_tag: _serialize_null,\n            Tag.LIST.dynamodb_tag: _serialize_list,\n            Tag.MAP.dynamodb_tag: _serialize_map,\n        }\n        try:\n            return serialize_functions[dynamodb_tag]\n        except KeyError:\n            raise SerializationError('Unsupported DynamoDB data type: \"{}\"'.format(dynamodb_tag))\n\n    if not isinstance(attribute, dict):\n        raise TypeError('Invalid attribute type \"{}\": must be dict'.format(type(attribute)))\n\n    if len(attribute) != 1:\n        raise SerializationError(\n            \"cannot serialize attribute: incorrect number of members {} != 1\".format(len(attribute))\n        )\n    key, value = list(attribute.items())[0]\n    return _serialize_function(key)(value)", "response": "Serializes a raw attribute to a byte string as defined for the DynamoDB Client - Side Encryption Standard."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs the given line into the callback machinery and set the result on a None line.", "response": "def _feed_line(self, line):\n        \"\"\"Put the given line into the callback machinery, and set the result on a None line.\"\"\"\n        if line is None:\n            self.set_result(self._callback(self.__spooled_lines))\n        else:\n            self.__spooled_lines.append(line)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_callback(self, function, wrap_result):\n    if not isinstance(function, Callable):\n        return None\n    def command_callback():\n        # command result callback expects response from MPD as iterable lines,\n        # thus read available lines from socket\n        res = function(self, self._read_lines())\n        # wrap result in iterator helper if desired\n        if wrap_result:\n            res = self._wrap_iterator(res)\n        return res\n    return command_callback", "response": "Create MPD command related response callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_command(wrapper, name, return_value, wrap_result):\n    def mpd_command(self, *args):\n        callback = _create_callback(self, return_value, wrap_result)\n        return wrapper(self, name, args, callback)\n    return mpd_command", "response": "Create MPD command related function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef i4_bit_lo0(n):\n    bit = 1\n    i = np.floor(n)\n    while i != 2 * (i // 2):\n        bit += 1\n        i //= 2\n    return bit", "response": "I4_BIT_LO0 returns the position of the low 0 bit base 2 in an integer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating multivariate standard normal quasi - random variables.", "response": "def i4_sobol_generate_std_normal(dim_num, n, skip=1):\n    \"\"\"\n    Generates multivariate standard normal quasi-random variables.\n\n    Parameters:\n      Input, integer dim_num, the spatial dimension.\n      Input, integer n, the number of points to generate.\n      Input, integer SKIP, the number of initial points to skip.\n\n      Output, real np array of shape (n, dim_num).\n    \"\"\"\n\n    sobols = i4_sobol_generate(dim_num, n, skip)\n\n    normals = norm.ppf(sobols)\n\n    return normals"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef i4_sobol(dim_num, seed):\n    global atmost\n    global dim_max\n    global dim_num_save\n    global initialized\n    global lastq\n    global log_max\n    global maxcol\n    global poly\n    global recipd\n    global seed_save\n    global v\n\n    if 'initialized' not in list(globals().keys()):\n        initialized = 0\n        dim_num_save = -1\n\n    if not initialized or dim_num != dim_num_save:\n        initialized = 1\n        dim_max = 40\n        dim_num_save = -1\n        log_max = 30\n        seed_save = -1\n\n        #  Initialize (part of) V.\n        v = np.zeros((dim_max, log_max))\n        v[0:40, 0] = np.transpose([\n            1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n        v[2:40, 1] = np.transpose([\n                  1, 3, 1, 3, 1, 3, 3, 1,\n            3, 1, 3, 1, 3, 1, 1, 3, 1, 3,\n            1, 3, 1, 3, 3, 1, 3, 1, 3, 1,\n            3, 1, 1, 3, 1, 3, 1, 3, 1, 3])\n\n        v[3:40, 2] = np.transpose([\n                     7, 5, 1, 3, 3, 7, 5,\n            5, 7, 7, 1, 3, 3, 7, 5, 1, 1,\n            5, 3, 3, 1, 7, 5, 1, 3, 3, 7,\n            5, 1, 1, 5, 7, 7, 5, 1, 3, 3])\n\n        v[5:40, 3] = np.transpose([\n                                1, 7,  9,  13, 11,\n            1, 3,  7,  9,  5,  13, 13, 11, 3,  15,\n            5, 3,  15, 7,  9,  13, 9,  1,  11, 7,\n            5, 15, 1,  15, 11, 5,  3,  1,  7,  9])\n\n        v[7:40, 4] = np.transpose([\n                                        9,  3,  27,\n            15, 29, 21, 23, 19, 11, 25, 7,  13, 17,\n            1,  25, 29, 3,  31, 11, 5,  23, 27, 19,\n            21, 5,  1,  17, 13, 7,  15, 9,  31, 9])\n\n        v[13:40, 5] = np.transpose([\n                        37, 33, 7,  5,  11, 39, 63,\n            27, 17, 15, 23, 29, 3,  21, 13, 31, 25,\n            9,  49, 33, 19, 29, 11, 19, 27, 15, 25])\n\n        v[19:40, 6] = np.transpose([\n                                                   13,\n            33, 115, 41, 79, 17, 29,  119, 75, 73, 105,\n            7,  59,  65, 21, 3,  113, 61,  89, 45, 107])\n\n        v[37:40, 7] = np.transpose([\n            7, 23, 39])\n\n        #  Set POLY.\n        poly = [\n            1,   3,   7,   11,  13,  19,  25,  37,  59,  47,\n            61,  55,  41,  67,  97,  91,  109, 103, 115, 131,\n            193, 137, 145, 143, 241, 157, 185, 167, 229, 171,\n            213, 191, 253, 203, 211, 239, 247, 285, 369, 299]\n\n        atmost = 2 ** log_max - 1\n\n        #  Find the number of bits in ATMOST.\n        maxcol = i4_bit_hi1(atmost)\n\n        #  Initialize row 1 of V.\n        v[0, 0:maxcol] = 1\n\n\n    #  Things to do only if the dimension changed.\n    if dim_num != dim_num_save:\n\n        #  Check parameters.\n        if dim_num < 1 or dim_max < dim_num:\n            print('I4_SOBOL - Fatal error!')\n            print('  The spatial dimension DIM_NUM should satisfy:')\n            print('    1 <= DIM_NUM <= %d' % dim_max)\n            print('  But this input value is DIM_NUM = %d' % dim_num)\n            return\n\n        dim_num_save = dim_num\n\n        #  Initialize the remaining rows of V.\n        for i in range(2, dim_num + 1):\n\n            #  The bits of the integer POLY(I) gives the form of polynomial I.\n            #  Find the degree of polynomial I from binary encoding.\n            j = poly[i - 1]\n            m = 0\n            j //= 2\n            while j > 0:\n                j //= 2\n                m += 1\n\n            #  Expand this bit pattern to separate components of the logical array INCLUD.\n            j = poly[i - 1]\n            includ = np.zeros(m)\n            for k in range(m, 0, -1):\n                j2 = j // 2\n                includ[k - 1] = (j != 2 * j2)\n                j = j2\n\n            #  Calculate the remaining elements of row I as explained\n            #  in Bratley and Fox, section 2.\n            for j in range(m + 1, maxcol + 1):\n                newv = v[i - 1, j - m - 1]\n                l = 1\n                for k in range(1, m + 1):\n                    l *= 2\n                    if includ[k - 1]:\n                        newv = np.bitwise_xor(\n                            int(newv), int(l * v[i - 1, j - k - 1]))\n                v[i - 1, j - 1] = newv\n\n        #  Multiply columns of V by appropriate power of 2.\n        l = 1\n        for j in range(maxcol - 1, 0, -1):\n            l *= 2\n            v[0:dim_num, j - 1] = v[0:dim_num, j - 1] * l\n\n        #  RECIPD is 1/(common denominator of the elements in V).\n        recipd = 1.0 / (2 * l)\n        lastq = np.zeros(dim_num)\n\n    seed = int(np.floor(seed))\n\n    if seed < 0:\n        seed = 0\n\n    l = 1\n    if seed == 0:\n        lastq = np.zeros(dim_num)\n\n    elif seed == seed_save + 1:\n\n        #  Find the position of the right-hand zero in SEED.\n        l = i4_bit_lo0(seed)\n\n    elif seed <= seed_save:\n\n        seed_save = 0\n        lastq = np.zeros(dim_num)\n\n        for seed_temp in range(int(seed_save), int(seed)):\n            l = i4_bit_lo0(seed_temp)\n            for i in range(1, dim_num + 1):\n                lastq[i - 1] = np.bitwise_xor(\n                    int(lastq[i - 1]), int(v[i - 1, l - 1]))\n\n        l = i4_bit_lo0(seed)\n\n    elif seed_save + 1 < seed:\n\n        for seed_temp in range(int(seed_save + 1), int(seed)):\n            l = i4_bit_lo0(seed_temp)\n            for i in range(1, dim_num + 1):\n                lastq[i - 1] = np.bitwise_xor(\n                    int(lastq[i - 1]), int(v[i - 1, l - 1]))\n\n        l = i4_bit_lo0(seed)\n\n    #  Check that the user is not calling too many times!\n    if maxcol < l:\n        print('I4_SOBOL - Fatal error!')\n        print('  Too many calls!')\n        print('  MAXCOL = %d\\n' % maxcol)\n        print('  L =      %d\\n' % l)\n        return\n\n    #  Calculate the new components of QUASI.\n    quasi = np.zeros(dim_num)\n    for i in range(1, dim_num + 1):\n        quasi[i - 1] = lastq[i - 1] * recipd\n        lastq[i - 1] = np.bitwise_xor(\n            int(lastq[i - 1]), int(v[i - 1, l - 1]))\n\n    seed_save = seed\n    seed += 1\n\n    return [quasi, seed]", "response": "i4_sobol generates a new quasirandom Sobol vector with each call."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if N is a prime number False otherwise.", "response": "def is_prime(n):\n    \"\"\"\n    is_prime returns True if N is a prime number, False otherwise\n\n    Parameters:\n       Input, integer N, the number to be checked.\n\n       Output, boolean value, True or False\n    \"\"\"\n    if n != int(n) or n < 2:\n        return False\n    if n == 2 or n == 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    # All primes >3 are of the form 6n+1 or 6n+5 (6n, 6n+2, 6n+4 are 2-divisible, 6n+3 is 3-divisible)\n    p = 5\n    root = int(np.ceil(np.sqrt(n)))\n    while p <= root:\n        if n % p == 0 or n % (p + 2) == 0:\n            return False\n        p += 6\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_bench_generate_token(alg):\n    def f(_):\n        \"\"\" Generate token \"\"\"\n        privk = priv_keys[alg].get('default', priv_key)\n        jwt.generate_jwt(payload, privk, alg, timedelta(seconds=5))\n    return f", "response": "Return function which will generate token for particular algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_jwt(claims, priv_key=None,\n                 algorithm='PS512', lifetime=None, expires=None,\n                 not_before=None,\n                 jti_size=16, other_headers=None):\n    \"\"\"\n    Generate a JSON Web Token.\n\n    :param claims: The claims you want included in the signature.\n    :type claims: dict\n\n    :param priv_key: The private key to be used to sign the token. Note: if you pass ``None`` then the token will be returned with an empty cryptographic signature and :obj:`algorithm` will be forced to the value ``none``.\n    :type priv_key: `jwcrypto.jwk.JWK <https://jwcrypto.readthedocs.io/en/latest/jwk.html>`_\n\n    :param algorithm: The algorithm to use for generating the signature. ``RS256``, ``RS384``, ``RS512``, ``PS256``, ``PS384``, ``PS512``, ``ES256``, ``ES384``, ``ES512``, ``HS256``, ``HS384``, ``HS512`` and ``none`` are supported.\n    :type algorithm: str\n\n    :param lifetime: How long the token is valid for.\n    :type lifetime: datetime.timedelta\n\n    :param expires: When the token expires (if :obj:`lifetime` isn't specified)\n    :type expires: datetime.datetime\n\n    :param not_before: When the token is valid from. Defaults to current time (if ``None`` is passed).\n    :type not_before: datetime.datetime\n\n    :param jti_size: Size in bytes of the unique token ID to put into the token (can be used to detect replay attacks). Defaults to 16 (128 bits). Specify 0 or ``None`` to omit the JTI from the token.\n    :type jti_size: int\n\n    :param other_headers: Any headers other than \"typ\" and \"alg\" may be specified, they will be included in the header.\n    :type other_headers: dict\n\n    :rtype: unicode\n    :returns: The JSON Web Token. Note this includes a header, the claims and a cryptographic signature. The following extra claims are added, per the `JWT spec <http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html>`_:\n\n    - **exp** (*IntDate*) -- The UTC expiry date and time of the token, in number of seconds from 1970-01-01T0:0:0Z UTC.\n    - **iat** (*IntDate*) -- The UTC date and time at which the token was generated.\n    - **nbf** (*IntDate*) -- The UTC valid-from date and time of the token.\n    - **jti** (*str*) -- A unique identifier for the token.\n\n    :raises:\n        ValueError: If other_headers contains either the \"typ\" or \"alg\" header\n    \"\"\"\n    header = {\n        'typ': 'JWT',\n        'alg': algorithm if priv_key else 'none'\n    }\n\n    if other_headers is not None:\n        redefined_keys = set(header.keys()) & set(other_headers.keys())\n        if redefined_keys:\n            raise ValueError('other_headers re-specified the headers: {}'.format(', '.join(redefined_keys)))\n        header.update(other_headers)\n\n    claims = dict(claims)\n\n    now = datetime.utcnow()\n\n    if jti_size:\n        claims['jti'] = base64url_encode(urandom(jti_size))\n\n    claims['nbf'] = timegm((not_before or now).utctimetuple())\n    claims['iat'] = timegm(now.utctimetuple())\n\n    if lifetime:\n        claims['exp'] = timegm((now + lifetime).utctimetuple())\n    elif expires:\n        claims['exp'] = timegm(expires.utctimetuple())\n\n    if header['alg'] == 'none':\n        signature = ''\n    else:\n        token = JWS(json_encode(claims))\n        token.add_signature(priv_key, protected=header)\n        signature = json_decode(token.serialize())['signature']\n\n    return u'%s.%s.%s' % (\n        base64url_encode(json_encode(header)),\n        base64url_encode(json_encode(claims)),\n        signature\n    )", "response": "Generates a JSON Web Token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_jwt(jwt,\n               pub_key=None,\n               allowed_algs=None,\n               iat_skew=timedelta(),\n               checks_optional=False,\n               ignore_not_implemented=False):\n    \"\"\"\n    Verify a JSON Web Token.\n\n    :param jwt: The JSON Web Token to verify.\n    :type jwt: str or unicode\n\n    :param pub_key: The public key to be used to verify the token. Note: if you pass ``None`` and **allowed_algs** contains ``none`` then the token's signature will not be verified.\n    :type pub_key: `jwcrypto.jwk.JWK <https://jwcrypto.readthedocs.io/en/latest/jwk.html>`_\n\n    :param allowed_algs: Algorithms expected to be used to sign the token. The ``in`` operator is used to test membership.\n    :type allowed_algs: list or NoneType (meaning an empty list)\n\n    :param iat_skew: The amount of leeway to allow between the issuer's clock and the verifier's clock when verifiying that the token was generated in the past. Defaults to no leeway.\n    :type iat_skew: datetime.timedelta\n\n    :param checks_optional: If ``False``, then the token must contain the **typ** header property and the **iat**, **nbf** and **exp** claim properties.\n    :type checks_optional: bool\n\n    :param ignore_not_implemented: If ``False``, then the token must *not* contain the **jku**, **jwk**, **x5u**, **x5c** or **x5t** header properties.\n    :type ignore_not_implemented: bool\n\n    :rtype: tuple\n    :returns: ``(header, claims)`` if the token was verified successfully. The token must pass the following tests:\n\n    - Its header must contain a property **alg** with a value in **allowed_algs**.\n    - Its signature must verify using **pub_key** (unless its algorithm is ``none`` and ``none`` is in **allowed_algs**).\n    - If the corresponding property is present or **checks_optional** is ``False``:\n\n      - Its header must contain a property **typ** with the value ``JWT``.\n      - Its claims must contain a property **iat** which represents a date in the past (taking into account :obj:`iat_skew`).\n      - Its claims must contain a property **nbf** which represents a date in the past.\n      - Its claims must contain a property **exp** which represents a date in the future.\n\n    :raises: If the token failed to verify.\n    \"\"\"\n    if allowed_algs is None:\n        allowed_algs = []\n\n    if not isinstance(allowed_algs, list):\n        # jwcrypto only supports list of allowed algorithms\n        raise _JWTError('allowed_algs must be a list')\n\n    header, claims, _ = jwt.split('.')\n\n    parsed_header = json_decode(base64url_decode(header))\n\n    alg = parsed_header.get('alg')\n    if alg is None:\n        raise _JWTError('alg header not present')\n    if alg not in allowed_algs:\n        raise _JWTError('algorithm not allowed: ' + alg)\n\n    if not ignore_not_implemented:\n        for k in parsed_header:\n            if k not in JWSHeaderRegistry:\n                raise _JWTError('unknown header: ' + k)\n            if not JWSHeaderRegistry[k].supported:\n                raise _JWTError('header not implemented: ' + k)\n\n    if pub_key:\n        token = JWS()\n        token.allowed_algs = allowed_algs\n        token.deserialize(jwt, pub_key)\n    elif 'none' not in allowed_algs:\n        raise _JWTError('no key but none alg not allowed')\n\n    parsed_claims = json_decode(base64url_decode(claims))\n\n    utcnow = datetime.utcnow()\n    now = timegm(utcnow.utctimetuple())\n\n    typ = parsed_header.get('typ')\n    if typ is None:\n        if not checks_optional:\n            raise _JWTError('typ header not present')\n    elif typ != 'JWT':\n        raise _JWTError('typ header is not JWT')\n\n    iat = parsed_claims.get('iat')\n    if iat is None:\n        if not checks_optional:\n            raise _JWTError('iat claim not present')\n    elif iat > timegm((utcnow + iat_skew).utctimetuple()):\n        raise _JWTError('issued in the future')\n\n    nbf = parsed_claims.get('nbf')\n    if nbf is None:\n        if not checks_optional:\n            raise _JWTError('nbf claim not present')\n    elif nbf > now:\n        raise _JWTError('not yet valid')\n\n    exp = parsed_claims.get('exp')\n    if exp is None:\n        if not checks_optional:\n            raise _JWTError('exp claim not present')\n    elif exp <= now:\n        raise _JWTError('expired')\n\n    return parsed_header, parsed_claims", "response": "Verifies a JSON Web Token."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a JSON Web Token without verifying it.", "response": "def process_jwt(jwt):\n    \"\"\"\n    Process a JSON Web Token without verifying it.\n\n    Call this before :func:`verify_jwt` if you need access to the header or claims in the token before verifying it. For example, the claims might identify the issuer such that you can retrieve the appropriate public key.\n\n    :param jwt: The JSON Web Token to verify.\n    :type jwt: str or unicode\n\n    :rtype: tuple\n    :returns: ``(header, claims)``\n    \"\"\"\n    header, claims, _ = jwt.split('.')\n    parsed_header = json_decode(base64url_decode(header))\n    parsed_claims = json_decode(base64url_decode(claims))\n    return parsed_header, parsed_claims"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn function which will generate token for particular algorithm", "response": "def make_bench_verify_token(alg):\n    \"\"\" Return function which will generate token for particular algorithm \"\"\"\n    privk = priv_keys[alg].get('default', priv_key)\n    token = jwt.generate_jwt(payload, privk, alg, timedelta(days=1))\n    def f(_):\n        \"\"\" Verify token \"\"\"\n        pubk = pub_keys[alg].get('default', pub_key)\n        jwt.verify_jwt(token, pubk, [alg])\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the benchmark functions for the given class.", "response": "def run(self, reporter=None):\r\n        \"\"\"This should generally not be overloaded.  Runs the benchmark functions\r\n        that are found in the child class.\r\n        \"\"\"\r\n        if not reporter: reporter = ConsoleReporter()\r\n        benchmarks = sorted(self._find_benchmarks())\r\n        reporter.write_titles(map(self._function_name_to_title, benchmarks))\r\n        \r\n        for value in self.input():\r\n            results = []\r\n            for b in benchmarks:\r\n                \r\n                method = getattr(self, b)\r\n                arg_count = len(inspect.getargspec(method)[0])\r\n                \r\n                if arg_count == 2:\r\n                    results.append(self._run_benchmark(method, value))\r\n                elif arg_count == 1:\r\n                    results.append(self._run_benchmark(method))\r\n                    \r\n            reporter.write_results(str(value), results)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a suite of all tests cases contained in this class", "response": "def _find_benchmarks(self):\r\n        \"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\r\n        def is_bench_method(attrname, prefix=\"bench\"):\r\n            return attrname.startswith(prefix) and hasattr(getattr(self.__class__, attrname), '__call__')\r\n        return list(filter(is_bench_method, dir(self.__class__)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send(self, body, req_type, seq_no=1):\n        # construct a packet\n        header = TACACSHeader(\n            self.version,\n            req_type,\n            self.session_id,\n            len(body.packed),\n            seq_no=seq_no\n        )\n        packet = TACACSPacket(header, body.packed, self.secret)\n\n        logger.debug('\\n'.join([\n            body.__class__.__name__,\n            'sent header <%s>' % header,\n            'sent body <%s>' % body,\n        ]))\n        self.sock.send(bytes(packet))\n\n        readable, _, _ = select.select([self.sock], [], [], self.timeout)\n        if readable:\n            # TACACS+ header packets are always 12 bytes\n            header_bytes = self.sock.recv(12)\n            resp_header = TACACSHeader.unpacked(header_bytes)\n            # If the reply header doesn't match, it's likely a non-TACACS+ TCP\n            # service answering with data we don't antipicate.  Bail out.\n            if any([\n                resp_header.version_max != header.version_max,\n                resp_header.type != header.type,\n                resp_header.session_id != header.session_id\n            ]):\n                logger.error('\\n'.join([\n                    resp_header.__class__.__name__,\n                    'recv header <%s>' % resp_header,\n                    str(resp_header.packed)\n                ]))\n                raise socket.error\n\n            # read the number of bytes specified in the response header\n            body_bytes = six.b('')\n            remaining = resp_header.length\n            while remaining > 0:\n                body_bytes += self.sock.recv(remaining)\n                remaining = resp_header.length - len(body_bytes)\n            return TACACSPacket(\n                resp_header,\n                body_bytes,\n                self.secret\n            )\n        raise socket.timeout", "response": "Send a TACACS + message body to the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authenticate(self, username, password, priv_lvl=TAC_PLUS_PRIV_LVL_MIN,\n                     authen_type=TAC_PLUS_AUTHEN_TYPE_ASCII,\n                     chap_ppp_id=None, chap_challenge=None,\n                     rem_addr=TAC_PLUS_VIRTUAL_REM_ADDR, port=TAC_PLUS_VIRTUAL_PORT):\n        \"\"\"\n        Authenticate to a TACACS+ server with a username and password.\n\n        :param username:\n        :param password:\n        :param priv_lvl:\n        :param authen_type:    TAC_PLUS_AUTHEN_TYPE_ASCII,\n                               TAC_PLUS_AUTHEN_TYPE_PAP,\n                               TAC_PLUS_AUTHEN_TYPE_CHAP\n        :param chap_ppp_id:    PPP ID when authen_type == 'chap'\n        :param chap_challenge: challenge value when authen_type == 'chap'\n        :param rem_addr:       AAA request source, default to TAC_PLUS_VIRTUAL_REM_ADDR\n        :param port:           AAA port, default to TAC_PLUS_VIRTUAL_PORT\n        :return:               TACACSAuthenticationReply\n        :raises:               socket.timeout, socket.error\n        \"\"\"\n        start_data = six.b('')\n        if authen_type in (TAC_PLUS_AUTHEN_TYPE_PAP,\n                           TAC_PLUS_AUTHEN_TYPE_CHAP):\n            self.version_min = TAC_PLUS_MINOR_VER_ONE\n\n            if authen_type == TAC_PLUS_AUTHEN_TYPE_PAP:\n                start_data = six.b(password)\n\n            if authen_type == TAC_PLUS_AUTHEN_TYPE_CHAP:\n                if not isinstance(chap_ppp_id, six.string_types):\n                    raise ValueError('chap_ppp_id must be a string')\n                if len(chap_ppp_id) != 1:\n                    raise ValueError('chap_ppp_id must be a 1-byte string')\n                if not isinstance(chap_challenge, six.string_types):\n                    raise ValueError('chap_challenge must be a string')\n                if len(chap_challenge) > 255:\n                    raise ValueError('chap_challenge may not be more 255 bytes')\n                start_data = (\n                    six.b(chap_ppp_id) +\n                    six.b(chap_challenge) +\n                    md5(six.b(\n                        chap_ppp_id + password + chap_challenge\n                    )).digest()\n                )\n        with self.closing():\n            packet = self.send(\n                TACACSAuthenticationStart(username, authen_type, priv_lvl,\n                                          start_data, rem_addr=rem_addr, port=port),\n                TAC_PLUS_AUTHEN\n            )\n            reply = TACACSAuthenticationReply.unpacked(packet.body)\n            logger.debug('\\n'.join([\n                reply.__class__.__name__,\n                'recv header <%s>' % packet.header,\n                'recv body <%s>' % reply\n            ]))\n            if authen_type == TAC_PLUS_AUTHEN_TYPE_ASCII and reply.getpass:\n                packet = self.send(TACACSAuthenticationContinue(password),\n                                   TAC_PLUS_AUTHEN,\n                                   packet.seq_no + 1)\n                reply = TACACSAuthenticationReply.unpacked(packet.body)\n                logger.debug('\\n'.join([\n                    reply.__class__.__name__,\n                    'recv header <%s>' % packet.header,\n                    'recv body <%s>' % reply\n                ]))\n                if reply.flags == TAC_PLUS_CONTINUE_FLAG_ABORT:\n                    reply.status = TAC_PLUS_AUTHEN_STATUS_FAIL\n\n        return reply", "response": "Authenticate to a TACACS server with a username and password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authorize(self, username, arguments=[],\n                  authen_type=TAC_PLUS_AUTHEN_TYPE_ASCII, priv_lvl=TAC_PLUS_PRIV_LVL_MIN,\n                  rem_addr=TAC_PLUS_VIRTUAL_REM_ADDR, port=TAC_PLUS_VIRTUAL_PORT):\n        \"\"\"\n        Authorize with a TACACS+ server.\n\n        :param username:\n        :param arguments:      The authorization arguments\n        :param authen_type:    TAC_PLUS_AUTHEN_TYPE_ASCII,\n                               TAC_PLUS_AUTHEN_TYPE_PAP,\n                               TAC_PLUS_AUTHEN_TYPE_CHAP\n        :param priv_lvl:       Minimal Required priv_lvl.\n        :param rem_addr:       AAA request source, default to TAC_PLUS_VIRTUAL_REM_ADDR\n        :param port:           AAA port, default to TAC_PLUS_VIRTUAL_PORT\n        :return:               TACACSAuthenticationReply\n        :raises:               socket.timeout, socket.error\n        \"\"\"\n        with self.closing():\n            packet = self.send(\n                TACACSAuthorizationStart(username,\n                                         TAC_PLUS_AUTHEN_METH_TACACSPLUS,\n                                         priv_lvl, authen_type, arguments,\n                                         rem_addr=rem_addr, port=port),\n                TAC_PLUS_AUTHOR\n            )\n            reply = TACACSAuthorizationReply.unpacked(packet.body)\n            logger.debug('\\n'.join([\n                reply.__class__.__name__,\n                'recv header <%s>' % packet.header,\n                'recv body <%s>' % reply\n            ]))\n            reply_arguments = dict([\n                arg.split(six.b('='), 1)\n                for arg in reply.arguments or []\n                if arg.find(six.b('=')) > -1]\n            )\n            user_priv_lvl = int(reply_arguments.get(\n                six.b('priv-lvl'), TAC_PLUS_PRIV_LVL_MAX))\n            if user_priv_lvl < priv_lvl:\n                reply.status = TAC_PLUS_AUTHOR_STATUS_FAIL\n        return reply", "response": "This method is used to send an authorization packet to the TACACS server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an accounting request to the TACACS + server.", "response": "def account(self, username, flags, arguments=[],\n                authen_type=TAC_PLUS_AUTHEN_TYPE_ASCII, priv_lvl=TAC_PLUS_PRIV_LVL_MIN,\n                rem_addr=TAC_PLUS_VIRTUAL_REM_ADDR, port=TAC_PLUS_VIRTUAL_PORT):\n        \"\"\"\n        Account with a TACACS+ server.\n\n        :param username:\n        :param flags:          TAC_PLUS_ACCT_FLAG_START,\n                               TAC_PLUS_ACCT_FLAG_WATCHDOG,\n                               TAC_PLUS_ACCT_FLAG_STOP\n        :param arguments:      The authorization arguments\n        :param authen_type:    TAC_PLUS_AUTHEN_TYPE_ASCII,\n                               TAC_PLUS_AUTHEN_TYPE_PAP,\n                               TAC_PLUS_AUTHEN_TYPE_CHAP\n        :param priv_lvl:       Minimal Required priv_lvl.\n        :param rem_addr:       AAA request source, default to TAC_PLUS_VIRTUAL_REM_ADDR\n        :param port:           AAA port, default to TAC_PLUS_VIRTUAL_PORT\n        :return:               TACACSAccountingReply\n        :raises:               socket.timeout, socket.error\n        \"\"\"\n        with self.closing():\n            packet = self.send(\n                TACACSAccountingStart(username, flags,\n                                      TAC_PLUS_AUTHEN_METH_TACACSPLUS,\n                                      priv_lvl, authen_type, arguments,\n                                      rem_addr=rem_addr, port=port),\n                TAC_PLUS_ACCT\n            )\n            reply = TACACSAccountingReply.unpacked(packet.body)\n            logger.debug('\\n'.join([\n                reply.__class__.__name__,\n                'recv header <%s>' % packet.header,\n                'recv body <%s>' % reply\n            ]))\n        return reply"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypt(header, body_bytes, secret):\n\n    # B = unsigned char\n    # !I = network-order (big-endian) unsigned int\n    body_length = len(body_bytes)\n    unhashed = (\n        struct.pack('!I', header.session_id) +\n        six.b(secret) +\n        struct.pack('B', header.version) +\n        struct.pack('B', header.seq_no)\n    )\n    pad = hashed = md5(unhashed).digest()\n\n    if (len(pad) < body_length):\n        # remake hash, appending it to pad until pad >= header.length\n        while True:\n            hashed = md5(unhashed + hashed).digest()\n            pad += hashed\n            if len(pad) >= body_length:\n                break\n\n    pad = pad[0:(body_length)]\n    pad = list(struct.unpack('B' * len(pad), pad))\n\n    packet_body = []\n    for x in struct.unpack('B' * body_length, body_bytes):\n        packet_body.append(x ^ pad.pop(0))\n\n    return struct.pack('B' * len(packet_body), *packet_body)", "response": "This function is used to encrypt and obfuscate a TACACS + header and body of packets according to the TACACS + spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts all files in the specified directory.", "response": "def extractall(filename, directory, backend='auto', auto_create_dir=False):\n    '''\n    :param backend: auto, patool or zipfile\n    :param filename: path to archive file\n    :param directory: directory to extract to\n    :param auto_create_dir: auto create directory\n    '''\n    Archive(filename, backend).extractall(directory,\n                                          auto_create_dir=auto_create_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting all the elements of the archive into the specified directory", "response": "def extractall(self, directory, auto_create_dir=False, patool_path=None):\n        '''\n        :param directory: directory to extract to\n        :param auto_create_dir: auto create directory\n        :param patool_path: the path to the patool backend\n        '''\n        log.debug('extracting %s into %s (backend=%s)', self.filename, directory, self.backend)\n        is_zipfile = zipfile.is_zipfile(self.filename)\n        directory = _fullpath(directory)\n        if not os.path.exists(self.filename):\n            raise ValueError(\n                'archive file does not exist:' + str(self.filename))\n        if not os.path.exists(directory):\n            if auto_create_dir:\n                os.makedirs(directory)\n            else:\n                raise ValueError('directory does not exist:' + str(directory))\n\n        if self.backend == 'auto':\n            if is_zipfile:\n                self.extractall_zipfile(directory)\n            else:\n                self.extractall_patool(directory, patool_path)\n\n        if self.backend == 'zipfile':\n            if not is_zipfile:\n                raise ValueError('file is not zip file:' + str(self.filename))\n            self.extractall_zipfile(directory)\n\n        if self.backend == 'patool':\n            self.extractall_patool(directory, patool_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the user defined environment variables.", "response": "def __get_user_env_vars(self):\r\n        \"\"\"Return the user defined environment variables\"\"\"\r\n        return (os.environ.get(self.GP_URL_ENV_VAR),\r\n            os.environ.get(self.GP_INSTANCE_ID_ENV_VAR),\r\n            os.environ.get(self.GP_USER_ID_ENV_VAR),\r\n            os.environ.get(self.GP_PASSWORD_ENV_VAR),\r\n            os.environ.get(self.GP_IAM_API_KEY_ENV_VAR))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __parse_vcap_services_env_var(self, serviceInstanceName=None):\r\n        vcapServices = os.environ.get(self.__VCAP_SERVICES_ENV_VAR)\r\n\r\n        if not vcapServices:\r\n            return (None, None, None, None, None)\r\n\r\n        parsedVcapServices = json.loads(vcapServices)\r\n\r\n        gpServicesInstances = []\r\n        for serviceName in parsedVcapServices:\r\n            if self.__gpServiceNameRegex.match(serviceName):\r\n                serviceInstances = parsedVcapServices.get(serviceName)\r\n                for serviceInstance in serviceInstances:\r\n                    gpServicesInstances.append(serviceInstance)\r\n\r\n        if not gpServicesInstances:\r\n            return (None, None, None, None, None)\r\n\r\n        targetGPServiceInstance = None\r\n        # use first service if no name is provided\r\n        if not serviceInstanceName:\r\n            targetGPServiceInstance = gpServicesInstances[0]\r\n        else:\r\n            # search for service name\r\n            for gpServiceInstance in gpServicesInstances:\r\n                if gpServiceInstance.get(self.__NAME_KEY)== serviceInstanceName:\r\n                    targetGPServiceInstance = gpServiceInstance\r\n                    break\r\n\r\n        # service was not found\r\n        if not targetGPServiceInstance:\r\n            return  (None, None, None, None, None)\r\n\r\n        credentials = targetGPServiceInstance.get(self.__CREDENTIALS_KEY)\r\n\r\n        if not credentials:\r\n            return (None, None, None, None, None)\r\n\r\n        url = credentials.get(self.__URL_KEY)\r\n        instanceId = credentials.get(self.__INSTANCE_ID_KEY)\r\n        userId = credentials.get(self.__USER_ID_KEY)\r\n        password = credentials.get(self.__PASSWORD_KEY)\r\n        apiKey = credentials.get(self.__IAM_API_KEY_KEY)\r\n\r\n        return (url, instanceId, userId, password, apiKey)", "response": "Parse the VCAP_SERVICES env var and return a tuple of the first two elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare the provided languageCode to the provided languageIds and returns the closest match.", "response": "def __get_language_match(self, languageCode, languageIds):\r\n        \"\"\"Compares ``languageCode`` to the provided ``languageIds`` to find\r\n        the closest match and returns it, if a match is not found returns\r\n        ``None``.\r\n\r\n        e.g. if ``languageCode`` is ``en_CA`` and ``languageIds`` contains\r\n        ``en``, the return value will be ``en``\r\n        \"\"\"\r\n        # special case\r\n        if languageCode == 'zh':\r\n            return 'zh-Hans'\r\n\r\n        # this will take care of cases such as mapping en_CA to en\r\n        if '-' in languageCode:\r\n            match = negotiate_locale([languageCode], languageIds, sep='-')\r\n        else:\r\n            match = negotiate_locale([languageCode], languageIds)\r\n\r\n        if match:\r\n            return match\r\n\r\n        # handle other cases\r\n        if '-' in languageCode:\r\n            locale = Locale.parse(languageCode, sep='-')\r\n        else:\r\n            locale = Locale.parse(languageCode)\r\n\r\n        for languageId in languageIds:\r\n            if '-' not in languageId:\r\n                continue\r\n\r\n            # normalize the languageId\r\n            nLanguageId = Locale.parse(languageId, sep='-')\r\n\r\n            # 1. lang subtag must match\r\n            # 2. either script or territory subtag must match AND\r\n            #    one of them must not be None, i.e. do not allow None == None\r\n            if locale.language == nLanguageId.language and \\\r\n                (((locale.script or nLanguageId.script) and\r\n                 (locale.script == nLanguageId.script)) or \\\r\n                 (locale.territory or nLanguageId.territory) and\r\n                 (locale.territory == nLanguageId.territory)):\r\n                    return languageId\r\n\r\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning base bundle url", "response": "def __get_base_bundle_url(self):\r\n        \"\"\"Returns ``{rest api url}/{serviceInstanceId}/v2/bundles`` \"\"\"\r\n        return self.__serviceAccount.get_url() + '/' + \\\r\n            self.__serviceAccount.get_instance_id() + self.__BUNDLES_PATH"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_gaas_hmac_headers(self, method, url, date=None, body=None,\r\n                                secret=None, userId=None):\r\n        \"\"\"Note: this documentation was copied for the Java client for GP.\r\n\r\n        Generate GaaS HMAC credentials used for HTTP Authorization header.\r\n        GaaS HMAC uses HMAC SHA1 algorithm signing a message composed by:\r\n\r\n        (HTTP method)[LF]       (in UPPERCASE)\r\n        (Target URL)[LF]\r\n        (RFC1123 date)[LF]\r\n        (Request Body)\r\n\r\n        If the request body is empty, it is simply omitted,\r\n        the 'message' then ends with new line code [LF].\r\n\r\n        The format for HTTP Authorization header is:\r\n\r\n        \"Authorization: GaaS-HMAC (user ID):(HMAC above)\"\r\n\r\n        For example, with user \"MyUser\" and secret \"MySecret\",\r\n        the method \"POST\",\r\n        the URL \"https://example.com/gaas\",\r\n        the date \"Mon, 30 Jun 2014 00:00:00 GMT\",\r\n        the body '{\"param\":\"value\"}',\r\n        the following text to be signed will be generated:\r\n\r\n        POST\r\n        https://example.com/gaas\r\n        Mon, 30 Jun 2014 00:00:00 GMT\r\n        {\"param\":\"value\"}\r\n\r\n        And the resulting headers are:\r\n\r\n        Authorization: GaaS-HMAC MyUser:ONBJapYEveDZfsPFdqZHQ64GDgc=\r\n        Date: Mon, 30 Jun 2014 00:00:00 GMT\r\n\r\n        The HTTP Date header, matching the one included in the message\r\n        to be signed, is required for GaaS HMAC authentication. GaaS\r\n        authentication code checks the Date header value and if it's too old,\r\n        it rejects the request.\r\n        \"\"\"\r\n        if not date:\r\n            date = self.__get_RFC1123_date()\r\n\r\n        message = str(method) + '\\n' + \\\r\n            str(url) + '\\n' + \\\r\n            str(date) + '\\n'\r\n        if body:\r\n            message += str(body)\r\n\r\n        if not secret:\r\n            secret = self.__serviceAccount.get_password()\r\n        secret = bytes(secret.encode(self.__ENCODINGFORMAT))\r\n        message = bytes(message.encode(self.__ENCODINGFORMAT))\r\n        digest = hmac.new(secret, message, sha1).digest()\r\n        urlSafeHmac = base64.b64encode(digest).strip()\r\n\r\n        if not userId:\r\n            userId = self.__serviceAccount.get_user_id()\r\n        urlSafeHmac = urlSafeHmac.strip().decode(self.__ENCODINGFORMAT)\r\n        authorizationValue = 'GP-HMAC ' + userId + ':' + urlSafeHmac\r\n\r\n        headers = {\r\n            self.__AUTHORIZATION_HEADER_KEY: str(authorizationValue),\r\n            self.__DATE_HEADER_KEY: str(date)\r\n        }\r\n\r\n        return headers", "response": "Generate GaaS HMAC headers for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing the request URL and returns Authorization type and GP headers", "response": "def __prepare_gprest_call(self, requestURL, params=None, headers=None, restType='GET', body=None):\r\n        \"\"\"Returns Authorization type and GP headers\r\n        \"\"\"\r\n        if self.__serviceAccount.is_iam_enabled():\r\n            auth = None\r\n            iam_api_key_header = {\r\n                self.__AUTHORIZATION_HEADER_KEY: str('API-KEY '+self.__serviceAccount.get_api_key())\r\n            }\r\n            if not headers is None:\r\n                headers.update(iam_api_key_header)\r\n            else:\r\n                headers = iam_api_key_header\r\n        elif self.__auth == self.BASIC_AUTH:\r\n            auth = (self.__serviceAccount.get_user_id(),\r\n                    self.__serviceAccount.get_password())\r\n        elif self.__auth == self.HMAC_AUTH:\r\n            auth = None\r\n\r\n            # need to prepare url by appending params to the end\r\n            # before creating the hmac headers\r\n            fakeRequest = requests.PreparedRequest()\r\n            fakeRequest.prepare_url(requestURL, params=params)\r\n            preparedUrl = fakeRequest.url\r\n\r\n            hmacHeaders = self.__get_gaas_hmac_headers(method=restType,\r\n                                                       url=preparedUrl, body=body)\r\n            if not headers is None:\r\n                headers.update(hmacHeaders)\r\n            else:\r\n                headers = hmacHeaders\r\n        return auth, headers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __process_gprest_response(self, r=None, restType='GET'):\r\n        if r is None:\r\n            logging.info('No response for REST '+restType+' request')\r\n            return None\r\n\r\n        httpStatus = r.status_code\r\n        logging.info('HTTP status code: %s', httpStatus)\r\n\r\n        if httpStatus == requests.codes.ok or \\\r\n            httpStatus == requests.codes.created:\r\n            jsonR = r.json()\r\n            if jsonR:\r\n                statusStr = 'REST response status: %s' % \\\r\n                    jsonR.get(self.__RESPONSE_STATUS_KEY)\r\n                msgStr = 'REST response message: %s' % \\\r\n                    jsonR.get(self.__RESPONSE_MESSAGE_KEY)\r\n                logging.info(statusStr)\r\n                logging.info(msgStr)\r\n                return jsonR\r\n            else:\r\n                logging.warning('Unable to parse JSON body.')\r\n                logging.warning(r.text)\r\n                return None\r\n        logging.warning('Invalid HTTP status code.')\r\n        logging.warning(r.text)\r\n        return r.json()", "response": "Returns the processed response for the given REST request"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a GET or POST request to the specified URL. Returns None if the request was not OK.", "response": "def __perform_rest_call(self, requestURL, params=None, headers=None, restType='GET', body=None):\r\n        \"\"\"Returns the JSON representation of the response if the response\r\n        status was ok, returns ``None`` otherwise.\r\n        \"\"\"\r\n        auth, headers = self.__prepare_gprest_call(requestURL, params=params, headers=headers, restType=restType, body=body)\r\n        if restType == 'GET':\r\n            r = requests.get(requestURL, auth=auth, headers=headers, params=params)\r\n        elif restType == 'PUT':\r\n            r = requests.put(requestURL, data=body, auth=auth, headers=headers, params=params)\r\n        elif restType == 'POST':\r\n            r = requests.post(requestURL, data=body, auth=auth, headers=headers, params=params)\r\n        elif restType == 'DELETE':\r\n            r = requests.delete(requestURL, auth=auth, headers=headers, params=params)\r\n        resp = self.__process_gprest_response(r, restType=restType)\r\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new reader user with access to the specified bundle Ids", "response": "def createReaderUser(self,accessibleBundles=None):\r\n        \"\"\"Creates a new reader user with access to the specified bundle Ids\"\"\"\r\n\r\n        url = self.__serviceAccount.get_url() + '/' + \\\r\n              self.__serviceAccount.get_instance_id()+ '/v2/users/new'\r\n\r\n        headers = {'content-type': 'application/json'}\r\n        data = {}\r\n        data['type'] = 'READER'\r\n        if accessibleBundles is not None:\r\n            data['bundles']=accessibleBundles\r\n        json_data = json.dumps(data)\r\n        response = self.__perform_rest_call(requestURL=url, restType='POST', body=json_data, headers=headers)\r\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the list of bundles data for the given service instance.", "response": "def __get_bundles_data(self):\r\n        \"\"\"``GET {url}/{serviceInstanceId}/v2/bundles``\r\n\r\n        Gets a list of bundle IDs.\r\n        \"\"\"\r\n\r\n        url = self.__get_base_bundle_url()\r\n        response = self.__perform_rest_call(requestURL=url)\r\n\r\n        if not response:\r\n            return None\r\n\r\n        bundlesData = response.get(self.__RESPONSE_BUNDLES_KEY)\r\n\r\n        return bundlesData"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the bundle s information.", "response": "def __get_bundle_data(self, bundleId):\r\n        \"\"\"``GET /{serviceInstanceId}/v2/bundles/{bundleId}``\r\n\r\n        Gets the bundle's information.\r\n        \"\"\"\r\n        url = self.__get_base_bundle_url() + '/' + bundleId\r\n        response = self.__perform_rest_call(requestURL=url)\r\n\r\n        if not response:\r\n            return None\r\n\r\n        bundleData = response.get(self.__RESPONSE_BUNDLE_KEY)\r\n\r\n        return bundleData"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the language data for a given language.", "response": "def __get_language_data(self, bundleId, languageId, fallback=False):\r\n        \"\"\"``GET /{serviceInstanceId}/v2/bundles/{bundleId}/{languageId}``\r\n\r\n        Gets the resource strings (key/value pairs) for the language. If\r\n        ``fallback`` is ``True``, source language value is used if translated\r\n        value is not available.\r\n        \"\"\"\r\n        url = self.__get_base_bundle_url() + '/' + bundleId + '/' + languageId\r\n        params = {'fallback': 'true'} if fallback else None\r\n        response = self.__perform_rest_call(requestURL=url, params=params)\r\n\r\n        if not response:\r\n            return None\r\n\r\n        languageData = response.get(self.__RESPONSE_RESOURCE_STRINGS_KEY)\r\n\r\n        return languageData"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the resource entry data from the server.", "response": "def __get_resource_entry_data(self, bundleId, languageId, resourceKey,\r\n                                  fallback=False):\r\n        \"\"\"``GET /{serviceInstanceId}/v2/bundles/{bundleId}/{languageId}\r\n        /{resourceKey}``\r\n\r\n        Gets the resource entry information.\r\n        \"\"\"\r\n        url = self.__get_base_bundle_url() + '/' + bundleId + '/' \\\r\n              + languageId + '/' + resourceKey\r\n        params = {'fallback': 'true'} if fallback else None\r\n        response = self.__perform_rest_call(requestURL=url, params=params)\r\n\r\n        if not response:\r\n            return None\r\n\r\n        resourceEntryData = response.get(self.__RESPONSE_RESOURCE_ENTRY_KEY)\r\n\r\n        return resourceEntryData"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __has_language(self, bundleId, languageId):\r\n        return True if self.__get_language_data(bundleId=bundleId,\r\n                                                languageId=languageId) \\\r\n                    else False", "response": "Returns True if the bundle has the language False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a key - value pairs map for the specified language.", "response": "def __get_keys_map(self, bundleId, languageId, fallback=False):\r\n        \"\"\"Returns key-value pairs for the specified language.\r\n        If fallback is ``True``, source language value is used if translated\r\n        value is not available.\r\n        \"\"\"\r\n        return self.__get_language_data(bundleId=bundleId,\r\n            languageId=languageId, fallback=fallback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the value for the given key.", "response": "def __get_value(self, bundleId, languageId, resourceKey, fallback=False):\r\n        \"\"\"Returns the value for the key. If fallback is ``True``, source\r\n        language value is used if translated value is not available. If the\r\n        key is not found, returns ``None``.\r\n        \"\"\"\r\n        resourceEntryData = self.__get_resource_entry_data(bundleId=bundleId,\r\n            languageId=languageId, resourceKey=resourceKey, fallback=fallback)\r\n\r\n        if not resourceEntryData:\r\n            return None\r\n\r\n        value = resourceEntryData.get(self.__RESPONSE_TRANSLATION_KEY)\r\n\r\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of avaliable languages in the bundle", "response": "def get_avaliable_languages(self, bundleId):\r\n        \"\"\"Returns a list of avaliable languages in the bundle\"\"\"\r\n        bundleData = self.__get_bundle_data(bundleId)\r\n\r\n        if not bundleData:\r\n            return []\r\n\r\n        sourceLanguage = bundleData.get(self.__RESPONSE_SRC_LANGUAGE_KEY)\r\n        languages = bundleData.get(self.__RESPONSE_TARGET_LANGUAGES_KEY)\r\n        languages.append(sourceLanguage)\r\n\r\n        return languages if languages else []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new bundle using Globalization Pipeline service", "response": "def create_bundle(self, bundleId, data=None):\r\n        \"\"\"Creates a bundle using Globalization Pipeline service\"\"\"\r\n        headers={'content-type':'application/json'}\r\n        url = self.__get_base_bundle_url() + \"/\" + bundleId\r\n        if data is None:\r\n            data = {}\r\n            data['sourceLanguage'] = 'en'\r\n            data['targetLanguages'] = []\r\n            data['notes']=[]\r\n            data['metadata']={}\r\n            data['partner']=''\r\n            data['segmentSeparatorPattern']=''\r\n            data['noTranslationPattern']=''\r\n        json_data = json.dumps(data)\r\n        response = self.__perform_rest_call(requestURL=url, restType='PUT', body=json_data, headers=headers)\r\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a specific bundle from the Globalization pipeline", "response": "def delete_bundle(self, bundleId):\r\n        \"\"\"Returns success(True) or failure(False) on deleting\r\n           a specific bundle present in the Globalization pipeline\"\"\"\r\n        if not bundleId:\r\n            return None\r\n        url = self.__get_base_bundle_url() + \"/\" + bundleId\r\n        response = self.__perform_rest_call(requestURL=url, restType='DELETE')\r\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_resource_entry(self, bundleId, languageId, resourceKey, data=None):\r\n        headers={'content-type':'application/json'}\r\n        url = self.__get_base_bundle_url() + \"/\" + bundleId + \"/\" + languageId + \"/\" + resourceKey\r\n        json_data = {}\r\n        if not data is None:\r\n            json_data = json.dumps(data)\r\n        response = self.__perform_rest_call(requestURL=url, restType='POST', body=json_data, headers=headers)\r\n        return response", "response": "Updates the resource entry for a particular key in a specific language in a specific globalization pipeline"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an instance of gp_translation that is used for obtaining translations for the given languages.", "response": "def gp_translation(self, bundleId, languages):\r\n        \"\"\"Returns an instance of ``GPTranslations`` to be used for obtaining\r\n        translations.\r\n\r\n        ``bundleId`` is the name of the bundle to use. ``languages`` is the\r\n        list of languages to use, with subsequent ones being fallbacks.\r\n        For example, to fallback to Spanish if French translated values are not\r\n        found, ``languages=['fr', 'es']``.\r\n        \"\"\"\r\n        return self.translation(bundleId=bundleId, languages=languages)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new Translation instance for the given bundleId and languages.", "response": "def translation(self, bundleId, languages, priority='gp', domain=None,\r\n        localedir=None, class_=None, codeset=None):\r\n        \"\"\"Returns the ``Translations`` instance to be used for obtaining\r\n        translations.\r\n\r\n        ``bundleId`` is the name of the bundle to use.\r\n        ``languages`` is the list of languages to use, with subsequent ones\r\n        being fallbacks. Additionally, based on the value of ``priority``,\r\n        local translated values can be given precedence over Globalization\r\n        Pipeline translated values.\r\n\r\n        For example, to fallback to Spanish if French translated values are not\r\n        found, ``languages=['fr', 'es']``. And if ``priority=gp``,\r\n        the fallback chain will be as follows:\r\n\r\n        - use ``gp`` French translated value, if not found:\r\n        - use ``local`` French translated value, if not found:\r\n        - use ``gp`` Spanish translated value, if not found:\r\n        - use ``local`` Spanish translated value, if not found:\r\n        - use source value, if not found:\r\n        - use provided key\r\n\r\n        In order to search for local translated values, the optional parameters\r\n        must be provided according to `gettext.translation\r\n        <https://docs.python.org/2/library/gettext.html#gettext.translation>`_\r\n        \"\"\"\r\n\r\n        availableLangs = self.get_avaliable_languages(bundleId)\r\n\r\n        translations = None\r\n\r\n        for language in languages:\r\n            # get local translation\r\n            localTranslations = None\r\n            if domain:\r\n                t = local_translation(domain=domain,\r\n                    localedir=localedir, languages=[language], class_=class_,\r\n                    fallback=True, codeset=codeset)\r\n\r\n                # only use t if it's not NullTranslations - NullTranslations\r\n                # indicates that a translation file was not found\r\n                if t is not NullTranslations:\r\n                    localTranslations = t\r\n\r\n            gpTranslations = None\r\n\r\n            # get gp translation if the bundle has the language\r\n            match = self.__get_language_match(languageCode=language,\r\n                languageIds=availableLangs)\r\n            if match:\r\n                gpTranslations = GPTranslations(bundleId=bundleId,\r\n                    languageId=match, client=self,\r\n                    cacheTimeout=self.__cacheTimeout)\r\n\r\n            # create the fallback chain\r\n            if not translations:\r\n                # set the first translation in the chain\r\n                if priority == 'local':\r\n                    if not localTranslations:\r\n                        translations = gpTranslations\r\n                    else:\r\n                        translations = localTranslations\r\n\r\n                        if gpTranslations:\r\n                            translations.add_fallback(gpTranslations)\r\n\r\n                elif priority == 'gp':\r\n                    if not gpTranslations:\r\n                        translations = localTranslations\r\n                    else:\r\n                        translations = gpTranslations\r\n\r\n                        if localTranslations:\r\n                            translations.add_fallback(localTranslations)\r\n            else:\r\n                # add fallback in the preferred order\r\n                if priority == 'local':\r\n                    if localTranslations:\r\n                        translations.add_fallback(localTranslations)\r\n                    if gpTranslations:\r\n                        translations.add_fallback(gpTranslations)\r\n                elif priority == 'gp':\r\n                    if gpTranslations:\r\n                        translations.add_fallback(gpTranslations)\r\n                    if localTranslations:\r\n                        translations.add_fallback(localTranslations)\r\n\r\n        if not translations:\r\n            logging.warning('No translations were found for bundleID <%s>' \\\r\n                            + ' and languages <%s> ', bundleId, languages)\r\n            translations = NullTranslations()\r\n\r\n        return translations"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the message key in the specified language.", "response": "def gettext(self, message):\r\n        \"\"\"Contacts the GP service instance to find the translated value for\r\n        the provided message key.\r\n\r\n        If translated value is not available, and a fallback has been set,\r\n        the fallback will be used. If there is no fallback, the source language\r\n        value is returned. And if the message key is not found, the message\r\n        key itself will be returned.\r\n\r\n        The caching feature may be used to cache translated values locally\r\n        in order to reduce the number of calls made to the GP service.\r\n\r\n        * ``cacheTimeout = 0``, do not cache\r\n        * ``cacheTimeout = -1``, cache forever\r\n        * ``cacheTimeout > 0``, store cache value for specified number of \\\r\n            minutes\r\n\r\n        \"\"\"\r\n        # cache forever or for specified time\r\n        if self.__cacheTimeout == -1 or self.__cacheTimeout > 0:\r\n            # get time passed since last cache\r\n            if self.__cacheMapTimestamp:\r\n                minutesPassed = (datetime.datetime.now() -\r\n                    self.__cacheMapTimestamp).total_seconds() / 60\r\n\r\n            # first call, or cache expired; initilize the cache\r\n            if not self.__cacheMapTimestamp or (self.__cacheTimeout != -1 and\r\n                minutesPassed >= self.__cacheTimeout):\r\n\r\n                # set sourceFallback True if there is no Translations fallback\r\n                sourceFallback = False if self._fallback else True\r\n\r\n                self.__cachedMap = self.__client._GPClient__get_keys_map(\r\n                    self.__bundleId, self.__languageId, fallback=sourceFallback)\r\n\r\n                # only record the timestamp if caching is enabled\r\n                if self.__cacheTimeout != 0:\r\n                    self.__cacheMapTimestamp = datetime.datetime.now()\r\n\r\n            # check cache for message key\r\n            if self.__cachedMap:\r\n                value = self.__cachedMap.get(message)\r\n            else:\r\n                value = None\r\n\r\n            return self.__get_return_value(message, value)\r\n        else:\r\n            # no caching, get the translated value directly from GP service\r\n            # set sourceFallback True if there is no Translations fallback\r\n            sourceFallback = False if self._fallback else True\r\n            tmpMap = self.__client._GPClient__get_keys_map(\r\n                self.__bundleId, self.__languageId, fallback=sourceFallback)\r\n\r\n            # check map for message key\r\n            if tmpMap:\r\n                value = tmpMap.get(message)\r\n            else:\r\n                value = None\r\n\r\n            return self.__get_return_value(message, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __get_return_value(self, messageKey, value):\r\n        # if value is not None, return it\r\n        # otherwise, either use the Translations fallback if there is one,\r\n        # or return the message key back\r\n        if value:\r\n            return value\r\n        else:\r\n            if self._fallback:\r\n                return self._fallback.gettext(messageKey)\r\n            else:\r\n                return messageKey", "response": "Determines the return value for a message key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding and returns filenames which might be the library you are looking for.", "response": "def _find_library_candidates(library_names,\n                             library_file_extensions,\n                             library_search_paths):\n    \"\"\"\n    Finds and returns filenames which might be the library you are looking for.\n    \"\"\"\n    candidates = set()\n    for library_name in library_names:\n        for search_path in library_search_paths:\n            glob_query = os.path.join(search_path, '*'+library_name+'*')\n            for filename in glob.iglob(glob_query):\n                filename = os.path.realpath(filename)\n                if filename in candidates:\n                    continue\n                basename = os.path.basename(filename)\n                if basename.startswith('lib'+library_name):\n                    basename_end = basename[len('lib'+library_name):]\n                elif basename.startswith(library_name):\n                    basename_end = basename[len(library_name):]\n                else:\n                    continue\n                for file_extension in library_file_extensions:\n                    if basename_end.startswith(file_extension):\n                        if basename_end[len(file_extension):][:1] in ('', '.'):\n                            candidates.add(filename)\n                    if basename_end.endswith(file_extension):\n                        basename_middle = basename_end[:-len(file_extension)]\n                        if all(c in '0123456789.' for c in basename_middle):\n                            candidates.add(filename)\n    return candidates"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a library from a list of library files.", "response": "def _load_library(library_names, library_file_extensions,\n                  library_search_paths, version_check_callback):\n    \"\"\"\n    Finds, loads and returns the most recent version of the library.\n    \"\"\"\n    candidates = _find_library_candidates(library_names,\n                                          library_file_extensions,\n                                          library_search_paths)\n    library_versions = []\n    for filename in candidates:\n        version = version_check_callback(filename)\n        if version is not None and version >= (3, 0, 0):\n            library_versions.append((version, filename))\n\n    if not library_versions:\n        return None\n    library_versions.sort()\n    return ctypes.CDLL(library_versions[-1][1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_library_search_paths():\n    search_paths = [\n        '',\n        '/usr/lib64',\n        '/usr/local/lib64',\n        '/usr/lib', '/usr/local/lib',\n        '/run/current-system/sw/lib',\n        '/usr/lib/x86_64-linux-gnu/',\n        os.path.abspath(os.path.dirname(__file__))\n    ]\n\n    if sys.platform == 'darwin':\n        path_environment_variable = 'DYLD_LIBRARY_PATH'\n    else:\n        path_environment_variable = 'LD_LIBRARY_PATH'\n    if path_environment_variable in os.environ:\n        search_paths.extend(os.environ[path_environment_variable].split(':'))\n    return search_paths", "response": "Returns a list of library search paths considering of the current working\n    directory default paths and paths from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the GLFW library.", "response": "def init():\n    \"\"\"\n    Initializes the GLFW library.\n\n    Wrapper for:\n        int glfwInit(void);\n    \"\"\"\n    cwd = _getcwd()\n    res = _glfw.glfwInit()\n    os.chdir(cwd)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminate():\n\n    for callback_repository in _callback_repositories:\n        for window_addr in list(callback_repository.keys()):\n            del callback_repository[window_addr]\n    for window_addr in list(_window_user_data_repository.keys()):\n        del _window_user_data_repository[window_addr]\n    _glfw.glfwTerminate()", "response": "Terminates the GLFW library."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the version of the GLFW library.", "response": "def get_version():\n    \"\"\"\n    Retrieves the version of the GLFW library.\n\n    Wrapper for:\n        void glfwGetVersion(int* major, int* minor, int* rev);\n    \"\"\"\n    major_value = ctypes.c_int(0)\n    major = ctypes.pointer(major_value)\n    minor_value = ctypes.c_int(0)\n    minor = ctypes.pointer(minor_value)\n    rev_value = ctypes.c_int(0)\n    rev = ctypes.pointer(rev_value)\n    _glfw.glfwGetVersion(major, minor, rev)\n    return major_value.value, minor_value.value, rev_value.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _raise_glfw_errors_as_exceptions(error_code, description):\n    global ERROR_REPORTING\n    if ERROR_REPORTING:\n        message = \"(%d) %s\" % (error_code, description)\n        raise GLFWError(message)", "response": "Raise GLFWError exceptions for glfw errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_monitors():\n    count_value = ctypes.c_int(0)\n    count = ctypes.pointer(count_value)\n    result = _glfw.glfwGetMonitors(count)\n    monitors = [result[i] for i in range(count_value.value)]\n    return monitors", "response": "Returns the currently connected monitors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the position of the monitor s viewport on the virtual screen.", "response": "def get_monitor_pos(monitor):\n    \"\"\"\n    Returns the position of the monitor's viewport on the virtual screen.\n\n    Wrapper for:\n        void glfwGetMonitorPos(GLFWmonitor* monitor, int* xpos, int* ypos);\n    \"\"\"\n    xpos_value = ctypes.c_int(0)\n    xpos = ctypes.pointer(xpos_value)\n    ypos_value = ctypes.c_int(0)\n    ypos = ctypes.pointer(ypos_value)\n    _glfw.glfwGetMonitorPos(monitor, xpos, ypos)\n    return xpos_value.value, ypos_value.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_monitor_physical_size(monitor):\n    width_value = ctypes.c_int(0)\n    width = ctypes.pointer(width_value)\n    height_value = ctypes.c_int(0)\n    height = ctypes.pointer(height_value)\n    _glfw.glfwGetMonitorPhysicalSize(monitor, width, height)\n    return width_value.value, height_value.value", "response": "Retrieves the physical size of the monitor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_monitor_callback(cbfun):\n    global _monitor_callback\n    previous_callback = _monitor_callback\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWmonitorfun(cbfun)\n    _monitor_callback = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetMonitorCallback(cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the monitor configuration callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_video_modes(monitor):\n    count_value = ctypes.c_int(0)\n    count = ctypes.pointer(count_value)\n    result = _glfw.glfwGetVideoModes(monitor, count)\n    videomodes = [result[i].unwrap() for i in range(count_value.value)]\n    return videomodes", "response": "Returns the available video modes for the specified monitor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_gamma_ramp(monitor, ramp):\n    gammaramp = _GLFWgammaramp()\n    gammaramp.wrap(ramp)\n    _glfw.glfwSetGammaRamp(monitor, ctypes.pointer(gammaramp))", "response": "Sets the current gamma ramp for the specified monitor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a window and its associated context.", "response": "def create_window(width, height, title, monitor, share):\n    \"\"\"\n    Creates a window and its associated context.\n\n    Wrapper for:\n        GLFWwindow* glfwCreateWindow(int width, int height, const char* title, GLFWmonitor* monitor, GLFWwindow* share);\n    \"\"\"\n    return _glfw.glfwCreateWindow(width, height, _to_char_p(title),\n                                  monitor, share)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the position of the client area of the specified window.", "response": "def get_window_pos(window):\n    \"\"\"\n    Retrieves the position of the client area of the specified window.\n\n    Wrapper for:\n        void glfwGetWindowPos(GLFWwindow* window, int* xpos, int* ypos);\n    \"\"\"\n    xpos_value = ctypes.c_int(0)\n    xpos = ctypes.pointer(xpos_value)\n    ypos_value = ctypes.c_int(0)\n    ypos = ctypes.pointer(ypos_value)\n    _glfw.glfwGetWindowPos(window, xpos, ypos)\n    return xpos_value.value, ypos_value.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_window_size(window):\n    width_value = ctypes.c_int(0)\n    width = ctypes.pointer(width_value)\n    height_value = ctypes.c_int(0)\n    height = ctypes.pointer(height_value)\n    _glfw.glfwGetWindowSize(window, width, height)\n    return width_value.value, height_value.value", "response": "Retrieves the size of the client area of the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_framebuffer_size(window):\n    width_value = ctypes.c_int(0)\n    width = ctypes.pointer(width_value)\n    height_value = ctypes.c_int(0)\n    height = ctypes.pointer(height_value)\n    _glfw.glfwGetFramebufferSize(window, width, height)\n    return width_value.value, height_value.value", "response": "Retrieves the size of the framebuffer of the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_window_user_pointer(window, pointer):\n\n    data = (False, pointer)\n    if not isinstance(pointer, ctypes.c_void_p):\n        data = (True, pointer)\n        # Create a void pointer for the python object\n        pointer = ctypes.cast(ctypes.pointer(ctypes.py_object(pointer)), ctypes.c_void_p)\n\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    _window_user_data_repository[window_addr] = data\n    _glfw.glfwSetWindowUserPointer(window, pointer)", "response": "Sets the user pointer of the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_window_user_pointer(window):\n\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n\n    if window_addr in _window_user_data_repository:\n        data = _window_user_data_repository[window_addr]\n        is_wrapped_py_object = data[0]\n        if is_wrapped_py_object:\n            return data[1]\n    return _glfw.glfwGetWindowUserPointer(window)", "response": "Retrieves the user pointer of the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_window_pos_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_pos_callback_repository:\n        previous_callback = _window_pos_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowposfun(cbfun)\n    _window_pos_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowPosCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the position callback for the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the size callback for the specified window.", "response": "def set_window_size_callback(window, cbfun):\n    \"\"\"\n    Sets the size callback for the specified window.\n\n    Wrapper for:\n        GLFWwindowsizefun glfwSetWindowSizeCallback(GLFWwindow* window, GLFWwindowsizefun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_size_callback_repository:\n        previous_callback = _window_size_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowsizefun(cbfun)\n    _window_size_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowSizeCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the close callback for the specified window.", "response": "def set_window_close_callback(window, cbfun):\n    \"\"\"\n    Sets the close callback for the specified window.\n\n    Wrapper for:\n        GLFWwindowclosefun glfwSetWindowCloseCallback(GLFWwindow* window, GLFWwindowclosefun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_close_callback_repository:\n        previous_callback = _window_close_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowclosefun(cbfun)\n    _window_close_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowCloseCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the refresh callback for the specified window.", "response": "def set_window_refresh_callback(window, cbfun):\n    \"\"\"\n    Sets the refresh callback for the specified window.\n\n    Wrapper for:\n        GLFWwindowrefreshfun glfwSetWindowRefreshCallback(GLFWwindow* window, GLFWwindowrefreshfun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_refresh_callback_repository:\n        previous_callback = _window_refresh_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowrefreshfun(cbfun)\n    _window_refresh_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowRefreshCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_window_focus_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_focus_callback_repository:\n        previous_callback = _window_focus_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowfocusfun(cbfun)\n    _window_focus_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowFocusCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the focus callback for the specified window."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the iconify callback for the specified window.", "response": "def set_window_iconify_callback(window, cbfun):\n    \"\"\"\n    Sets the iconify callback for the specified window.\n\n    Wrapper for:\n        GLFWwindowiconifyfun glfwSetWindowIconifyCallback(GLFWwindow* window, GLFWwindowiconifyfun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _window_iconify_callback_repository:\n        previous_callback = _window_iconify_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWwindowiconifyfun(cbfun)\n    _window_iconify_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetWindowIconifyCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_framebuffer_size_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _framebuffer_size_callback_repository:\n        previous_callback = _framebuffer_size_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWframebuffersizefun(cbfun)\n    _framebuffer_size_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetFramebufferSizeCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the framebuffer resize callback for the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cursor_pos(window):\n    xpos_value = ctypes.c_double(0.0)\n    xpos = ctypes.pointer(xpos_value)\n    ypos_value = ctypes.c_double(0.0)\n    ypos = ctypes.pointer(ypos_value)\n    _glfw.glfwGetCursorPos(window, xpos, ypos)\n    return xpos_value.value, ypos_value.value", "response": "Retrieves the last reported cursor position relative to the client\n    area of the window."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the key callback for the specified window.", "response": "def set_key_callback(window, cbfun):\n    \"\"\"\n    Sets the key callback.\n\n    Wrapper for:\n        GLFWkeyfun glfwSetKeyCallback(GLFWwindow* window, GLFWkeyfun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _key_callback_repository:\n        previous_callback = _key_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWkeyfun(cbfun)\n    _key_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetKeyCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the Unicode character callback for the specified window.", "response": "def set_char_callback(window, cbfun):\n    \"\"\"\n    Sets the Unicode character callback.\n\n    Wrapper for:\n        GLFWcharfun glfwSetCharCallback(GLFWwindow* window, GLFWcharfun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _char_callback_repository:\n        previous_callback = _char_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWcharfun(cbfun)\n    _char_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetCharCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_mouse_button_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _mouse_button_callback_repository:\n        previous_callback = _mouse_button_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWmousebuttonfun(cbfun)\n    _mouse_button_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetMouseButtonCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the mouse button callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_cursor_pos_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _cursor_pos_callback_repository:\n        previous_callback = _cursor_pos_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWcursorposfun(cbfun)\n    _cursor_pos_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetCursorPosCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the cursor position callback."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the cursor enter callback.", "response": "def set_cursor_enter_callback(window, cbfun):\n    \"\"\"\n    Sets the cursor enter/exit callback.\n\n    Wrapper for:\n        GLFWcursorenterfun glfwSetCursorEnterCallback(GLFWwindow* window, GLFWcursorenterfun cbfun);\n    \"\"\"\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _cursor_enter_callback_repository:\n        previous_callback = _cursor_enter_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWcursorenterfun(cbfun)\n    _cursor_enter_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetCursorEnterCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_scroll_callback(window, cbfun):\n    window_addr = ctypes.cast(ctypes.pointer(window),\n                              ctypes.POINTER(ctypes.c_long)).contents.value\n    if window_addr in _scroll_callback_repository:\n        previous_callback = _scroll_callback_repository[window_addr]\n    else:\n        previous_callback = None\n    if cbfun is None:\n        cbfun = 0\n    c_cbfun = _GLFWscrollfun(cbfun)\n    _scroll_callback_repository[window_addr] = (cbfun, c_cbfun)\n    cbfun = c_cbfun\n    _glfw.glfwSetScrollCallback(window, cbfun)\n    if previous_callback is not None and previous_callback[0] != 0:\n        return previous_callback[0]", "response": "Sets the scroll callback for the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the values of all axes of the specified joystick.", "response": "def get_joystick_axes(joy):\n    \"\"\"\n    Returns the values of all axes of the specified joystick.\n\n    Wrapper for:\n        const float* glfwGetJoystickAxes(int joy, int* count);\n    \"\"\"\n    count_value = ctypes.c_int(0)\n    count = ctypes.pointer(count_value)\n    result = _glfw.glfwGetJoystickAxes(joy, count)\n    return result, count_value.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the state of all buttons of the specified joystick.", "response": "def get_joystick_buttons(joy):\n    \"\"\"\n    Returns the state of all buttons of the specified joystick.\n\n    Wrapper for:\n        const unsigned char* glfwGetJoystickButtons(int joy, int* count);\n    \"\"\"\n    count_value = ctypes.c_int(0)\n    count = ctypes.pointer(count_value)\n    result = _glfw.glfwGetJoystickButtons(joy, count)\n    return result, count_value.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrap(self, video_mode):\n        size, bits, self.refresh_rate = video_mode\n        self.width, self.height = size\n        self.red_bits, self.green_bits, self.blue_bits = bits", "response": "Wrap a nested python sequence into a nested python sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a GLFWvidmode object.", "response": "def unwrap(self):\n        \"\"\"\n        Returns a GLFWvidmode object.\n        \"\"\"\n        size = self.Size(self.width, self.height)\n        bits = self.Bits(self.red_bits, self.green_bits, self.blue_bits)\n        return self.GLFWvidmode(size, bits, self.refresh_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unwrap(self):\n        red = [self.red[i] for i in range(self.size)]\n        green = [self.green[i] for i in range(self.size)]\n        blue = [self.blue[i] for i in range(self.size)]\n        if NORMALIZE_GAMMA_RAMPS:\n            red = [value / 65535.0 for value in red]\n            green = [value / 65535.0 for value in green]\n            blue = [value / 65535.0 for value in blue]\n        return self.GLFWgammaramp(red, green, blue)", "response": "Returns a GLFWgammaramp object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrap(self, image):\n        if hasattr(image, 'size') and hasattr(image, 'convert'):\n            # Treat image as PIL/pillow Image object\n            self.width, self.height = image.size\n            array_type = ctypes.c_ubyte * 4 * (self.width * self.height)\n            self.pixels_array = array_type()\n            pixels = image.convert('RGBA').getdata()\n            for i, pixel in enumerate(pixels):\n                self.pixels_array[i] = pixel\n        else:\n            self.width, self.height, pixels = image\n            array_type = ctypes.c_ubyte * 4 * self.width * self.height\n            self.pixels_array = array_type()\n            for i in range(self.height):\n                for j in range(self.width):\n                    for k in range(4):\n                        self.pixels_array[i][j][k] = pixels[i][j][k]\n        pointer_type = ctypes.POINTER(ctypes.c_ubyte)\n        self.pixels = ctypes.cast(self.pixels_array, pointer_type)", "response": "Wraps a nested python sequence or PIL image object into a new image object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unwrap(self):\n        pixels = [[[int(c) for c in p] for p in l] for l in self.pixels_array]\n        return self.GLFWimage(self.width, self.height, pixels)", "response": "Returns a GLFWimage object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef myfoo(i,s, course=['s'],gg=2,fdf=3,**d):\n    print ('data:',d)\n    if course:\n        print (course)\n        return course.get('id')\n    else:\n        print(\"No Course!\")\n        return None", "response": "Function to get the id of a node in the course"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that determines which argumnents have a default value", "response": "def get_args_kwargs(foo):\n    \"\"\"\n    callable->[],[]\n    function that determines which argumnents have a default value\n    :return: tuple of lists of names [args],[args_with_def]\n    \"\"\"\n    spec = inspect.signature(foo)\n    params = spec.parameters\n    args,args_with_def=[],[]\n    for n,p in params.items():\n        if p.default is p.empty:\n            args.append(n)\n        else:\n            args_with_def.append(n)\n    return args,args_with_def"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stream_data(self, host=HOST, port=GPSD_PORT, enable=True, gpsd_protocol=PROTOCOL, devicepath=None):\n        self.socket.connect(host, port)\n        self.socket.watch(enable, gpsd_protocol, devicepath)", "response": "Connect and watch and stream data to the GPS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunpacks the data of the object attributes of the object.", "response": "def unpack_data(self, usnap=.2):  # 2/10th second sleep between empty requests\n        \"\"\" Iterates over socket response and unpacks values of object attributes.\n        Sleeping here has the greatest response to cpu cycles short of blocking sockets\n        \"\"\"\n        for new_data in self.socket:\n            if new_data:\n                self.data_stream.unpack(new_data)\n            else:\n                sleep(usnap)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun thread with data", "response": "def run_thread(self, usnap=.2, daemon=True):\n        \"\"\"run thread with data\n        \"\"\"\n        # self.stream_data() # Unless other changes are made this would limit to localhost only.\n        try:\n            gps3_data_thread = Thread(target=self.unpack_data, args={usnap: usnap}, daemon=daemon)\n        except TypeError:\n            # threading.Thread() only accepts daemon argument in Python 3.3\n            gps3_data_thread = Thread(target=self.unpack_data, args={usnap: usnap})\n            gps3_data_thread.setDaemon(daemon)\n        gps3_data_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd commandline arguments and formatted help", "response": "def add_args():\n    \"\"\"Adds commandline arguments and formatted Help\"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-host', action='store', dest='host', default='127.0.0.1', help='DEFAULT \"127.0.0.1\"')\n    parser.add_argument('-port', action='store', dest='port', default='2947', help='DEFAULT 2947', type=int)\n    parser.add_argument('-json', dest='gpsd_protocol', const='json', action='store_const', default='json', help='DEFAULT JSON objects */')\n    parser.add_argument('-device', dest='devicepath', action='store', help='alternate devicepath e.g.,\"-device /dev/ttyUSB4\"')\n    # Infrequently used options\n    parser.add_argument('-nmea', dest='gpsd_protocol', const='nmea', action='store_const', help='*/ output in NMEA */')\n    # parser.add_argument('-rare', dest='gpsd_protocol', const='rare', action='store_const', help='*/ output of packets in hex */')\n    # parser.add_argument('-raw', dest='gpsd_protocol', const='raw', action='store_const', help='*/ output of raw packets */')\n    # parser.add_argument('-scaled', dest='gpsd_protocol', const='scaled', action='store_const', help='*/ scale output to floats */')\n    # parser.add_argument('-timing', dest='gpsd_protocol', const='timing', action='store_const', help='*/ timing information */')\n    # parser.add_argument('-split24', dest='gpsd_protocol', const='split24', action='store_const', help='*/ split AIS Type 24s */')\n    # parser.add_argument('-pps', dest='gpsd_protocol', const='pps', action='store_const', help='*/ enable PPS JSON */')\n    parser.add_argument('-v', '--version', action='version', version='Version: {}'.format(__version__))\n    cli_args = parser.parse_args()\n    return cli_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncounting number of satellites used in calculation from total visible satellites", "response": "def satellites_used(feed):\n    \"\"\"Counts number of satellites used in calculation from total visible satellites\n    Arguments:\n        feed feed=data_stream.TPV['satellites']\n    Returns:\n        total_satellites(int):\n        used_satellites (int):\n    \"\"\"\n    total_satellites = 0\n    used_satellites = 0\n\n    if not isinstance(feed, list):\n        return 0, 0\n\n    for satellites in feed:\n        total_satellites += 1\n        if satellites['used'] is True:\n            used_satellites += 1\n    return total_satellites, used_satellites"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_time(gps_datetime_str):\n    if not 'n/a' == gps_datetime_str:\n        datetime_string = gps_datetime_str\n        datetime_object = datetime.strptime(datetime_string, \"%Y-%m-%dT%H:%M:%S\")\n        return datetime_object", "response": "Makes datetime object from string object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef elapsed_time_from(start_time):\n    time_then = make_time(start_time)\n    time_now = datetime.utcnow().replace(microsecond=0)\n    if time_then is None:\n        return\n    delta_t = time_now - time_then\n    return delta_t", "response": "calculate time delta from latched time and current time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert base data between metric imperial or nautical units", "response": "def unit_conversion(thing, units, length=False):\n    \"\"\"converts base data between metric, imperial, or nautical units\"\"\"\n    if 'n/a' == thing:\n        return 'n/a'\n    try:\n        thing = round(thing * CONVERSION[units][0 + length], 2)\n    except TypeError:\n        thing = 'fubar'\n    return thing, CONVERSION[units][2 + length]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sexagesimal version of the given Latitude.", "response": "def sexagesimal(sexathang, latlon, form='DDD'):\n    \"\"\"\n    Arguments:\n        sexathang: (float), -15.560615 (negative = South), -146.241122 (negative = West)  # Apataki Carenage\n        latlon: (str) 'lat' | 'lon'\n        form: (str), 'DDD'|'DMM'|'DMS', decimal Degrees, decimal Minutes, decimal Seconds\n    Returns:\n        latitude: e.g., '15\u00b033'38.214\"S'\n        longitude: e.g., '146\u00b014'28.039\"W'\n    \"\"\"\n    cardinal = 'O'\n    if not isinstance(sexathang, float):\n        sexathang = 'n/a'\n        return sexathang\n\n    if latlon == 'lon':\n        if sexathang > 0.0:\n            cardinal = 'E'\n        if sexathang < 0.0:\n            cardinal = 'W'\n\n    if latlon == 'lat':\n        if sexathang > 0.0:\n            cardinal = 'N'\n        if sexathang < 0.0:\n            cardinal = 'S'\n\n    if 'RAW' in form:\n        sexathang = '{0:4.9f}\u00b0'.format(sexathang)  # 4 to allow -100\u00b0 through -179.999999\u00b0 to -180\u00b0\n        return sexathang\n\n    if 'DDD' in form:\n        sexathang = '{0:3.6f}\u00b0'.format(abs(sexathang))\n\n    if 'DMM' in form:\n        _latlon = abs(sexathang)\n        minute_latlon, degree_latlon = modf(_latlon)\n        minute_latlon *= 60\n        sexathang = '{0}\u00b0{1:2.5f}\\''.format(int(degree_latlon), minute_latlon)\n\n    if 'DMS' in form:\n        _latlon = abs(sexathang)\n        minute_latlon, degree_latlon = modf(_latlon)\n        second_latlon, minute_latlon = modf(minute_latlon * 60)\n        second_latlon *= 60.0\n        sexathang = '{0}\u00b0{1}\\'{2:2.3f}\\\"'.format(int(degree_latlon), int(minute_latlon), second_latlon)\n\n    return sexathang + cardinal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncursing terminal with standard outputs", "response": "def show_human():\n    \"\"\"Curses terminal with standard outputs \"\"\"\n    form = 'RAW'\n    units = 'raw'\n\n    data_window = curses.newwin(19, 39, 0, 0)\n    sat_window = curses.newwin(14, 39, 0, 40)\n    device_window = curses.newwin(6, 39, 13, 40)\n    packet_window = curses.newwin(7, 79, 19, 0)\n\n    for new_data in gpsd_socket:\n        if new_data:\n            data_stream.unpack(new_data)\n\n            screen.nodelay(1)\n            key_press = screen.getch()\n\n            if key_press == ord('q'):  # quit\n                shut_down()\n            elif key_press == ord('a'):  # NMEA\n                gpsd_socket.watch(enable=False, gpsd_protocol='json')\n                gpsd_socket.watch(gpsd_protocol='nmea')\n                show_nmea()\n            elif key_press == ord('0'):  # raw\n                form = 'RAW'\n                units = 'raw'\n                data_window.clear()\n            elif key_press == ord('1'):  # DDD\n                form = 'DDD'\n                data_window.clear()\n            elif key_press == ord('2'):  # DMM\n                form = 'DMM'\n                data_window.clear()\n            elif key_press == ord('3'):  # DMS\n                form = 'DMS'\n                data_window.clear()\n            elif key_press == ord('m'):  # Metric\n                units = 'metric'\n                data_window.clear()\n            elif key_press == ord('i'):  # Imperial\n                units = 'imperial'\n                data_window.clear()\n            elif key_press == ord('n'):  # Nautical\n                units = 'nautical'\n                data_window.clear()\n            elif key_press == ord('d'):  # Refresh device listings\n                gpsd_socket.send('?DEVICES;')\n                device_window.clear()\n\n            data_window.box()\n            data_window.addstr(0, 2, 'GPS3 Python {}.{}.{} GPSD Interface'.format(*sys.version_info), curses.A_BOLD)\n            data_window.addstr(1, 2, 'Time:  {time} '.format(**data_stream.TPV))\n            data_window.addstr(2, 2, 'Latitude:  {} '.format(sexagesimal(data_stream.TPV['lat'], 'lat', form)))\n            data_window.addstr(3, 2, 'Longitude: {} '.format(sexagesimal(data_stream.TPV['lon'], 'lon', form)))\n            data_window.addstr(4, 2, 'Altitude:  {} {}'.format(*unit_conversion(data_stream.TPV['alt'], units, length=True)))\n            data_window.addstr(5, 2, 'Speed:     {} {}'.format(*unit_conversion(data_stream.TPV['speed'], units)))\n            data_window.addstr(6, 2, 'Heading:   {track}\u00b0 True'.format(**data_stream.TPV))\n            data_window.addstr(7, 2, 'Climb:     {} {}/s'.format(*unit_conversion(data_stream.TPV['climb'], units, length=True)))\n            data_window.addstr(8, 2, 'Status:     {mode:<}D  '.format(**data_stream.TPV))\n            data_window.addstr(9, 2, 'Latitude Err:  +/-{} {} '.format(*unit_conversion(data_stream.TPV['epx'], units, length=True)))\n            data_window.addstr(10, 2, 'Longitude Err: +/-{} {}'.format(*unit_conversion(data_stream.TPV['epy'], units, length=True)))\n            data_window.addstr(11, 2, 'Altitude Err:  +/-{} {} '.format(*unit_conversion(data_stream.TPV['epv'], units, length=True)))\n            data_window.addstr(12, 2, 'Course Err:    +/-{epc}  '.format(**data_stream.TPV), curses.A_DIM)\n            data_window.addstr(13, 2, 'Speed Err:     +/-{} {} '.format(*unit_conversion(data_stream.TPV['eps'], units)), curses.A_DIM)\n            data_window.addstr(14, 2, 'Time Offset:   +/-{ept}  '.format(**data_stream.TPV), curses.A_DIM)\n            data_window.addstr(15, 2, 'gdop:{gdop}  pdop:{pdop}  tdop:{tdop}'.format(**data_stream.SKY))\n            data_window.addstr(16, 2, 'ydop:{ydop}  xdop:{xdop} '.format(**data_stream.SKY))\n            data_window.addstr(17, 2, 'vdop:{vdop}  hdop:{hdop} '.format(**data_stream.SKY))\n\n            sat_window.clear()\n            sat_window.box()\n            sat_window.addstr(0, 2, 'Using {0[1]}/{0[0]} satellites (truncated)'.format(satellites_used(data_stream.SKY['satellites'])))\n            sat_window.addstr(1, 2, 'PRN     Elev   Azimuth   SNR   Used')\n            line = 2\n            if isinstance(data_stream.SKY['satellites'], list):  # Nested lists of dictionaries are strings before data is present\n                for sats in data_stream.SKY['satellites'][0:10]:\n                    sat_window.addstr(line, 2, '{PRN:>2}   {el:>6}   {az:>5}   {ss:>5}   {used:}'.format(**sats))\n                    line += 1\n\n            # device_window.clear()\n            device_window.box()\n            if not isinstance(data_stream.DEVICES['devices'], list):  # Local machines need a 'device' kick start\n                gpsd_socket.send('?DEVICES;')  # to have valid data I don't know why.\n\n            if isinstance(data_stream.DEVICES['devices'], list):  # Nested lists of dictionaries are strings before data is present.\n                for gizmo in data_stream.DEVICES['devices']:\n                    start_time, _uicroseconds = gizmo['activated'].split('.')  # Remove '.000Z'\n                    elapsed = elapsed_time_from(start_time)\n\n                    device_window.addstr(1, 2, 'Activated: {}'.format(gizmo['activated']))\n                    device_window.addstr(2, 2, 'Host:{0.host}:{0.port} {1}'.format(args, gizmo['path']))\n                    device_window.addstr(3, 2, 'Driver:{driver}     BPS:{bps}'.format(**gizmo))\n                    device_window.addstr(4, 2, 'Cycle:{0} Hz {1!s:>14} Elapsed'.format(1 / gizmo['cycle'], elapsed))\n\n            packet_window.clear()\n            # packet_window.border(0)\n            packet_window.scrollok(True)\n            packet_window.addstr(0, 0, '{}'.format(new_data))\n\n#            sleep(.9)\n\n            data_window.refresh()\n            sat_window.refresh()\n            device_window.refresh()\n            packet_window.refresh()\n        else:  # Reduced CPU cycles with the non-blocking socket read, by putting 'sleep' here, rather than hitting\n            sleep(.1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shut_down():\n    curses.nocbreak()\n    curses.echo()\n    curses.endwin()\n    gpsd_socket.close()\n    print('Keyboard interrupt received\\nTerminated by user\\nGood Bye.\\n')\n    sys.exit(1)", "response": "Closes connection and restores terminal"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, host=HOST, port=GPSD_PORT):\n        for alotta_stuff in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            family, socktype, proto, _canonname, host_port = alotta_stuff\n            try:\n                self.streamSock = socket.socket(family, socktype, proto)\n                self.streamSock.connect(host_port)\n                self.streamSock.setblocking(False)\n            except (OSError, IOError) as error:\n                sys.stderr.write('\\r\\nGPSDSocket.connect exception is--> {}'.format(error))\n                sys.stderr.write('\\r\\nGPS3 gpsd connection at \\'{0}\\' on port \\'{1}\\' failed\\r\\n'.format(host, port))", "response": "Connect to a given host on a given port."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef watch(self, enable=True, gpsd_protocol=PROTOCOL, devicepath=None):\n        # N.B.: 'timing' requires special attention, as it is undocumented and lives with dragons.\n        command = '?WATCH={{\"enable\":true,\"{0}\":true}}'.format(gpsd_protocol)\n\n        if gpsd_protocol == 'rare':  # 1 for a channel, gpsd reports the unprocessed NMEA or AIVDM data stream\n            command = command.replace('\"rare\":true', '\"raw\":1')\n        if gpsd_protocol == 'raw':  # 2 channel that processes binary data, received data verbatim without hex-dumping.\n            command = command.replace('\"raw\":true', '\"raw\",2')\n        if not enable:\n            command = command.replace('true', 'false')  # sets -all- command values false .\n        if devicepath:\n            command = command.replace('}', ',\"device\":\"') + devicepath + '\"}'\n\n        return self.send(command)", "response": "watch gpsd in various gpsd_protocols or devices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a command to the daemon.", "response": "def send(self, command):\n        \"\"\"Ship commands to the daemon\n        Arguments:\n            command: e.g., '?WATCH={{'enable':true,'json':true}}'|'?VERSION;'|'?DEVICES;'|'?DEVICE;'|'?POLL;'\n        \"\"\"\n        # The POLL command requests data from the last-seen fixes on all active GPS devices.\n        # Devices must previously have been activated by ?WATCH to be pollable.\n        try:\n            self.streamSock.send(bytes(command, encoding='utf-8'))\n        except TypeError:\n            self.streamSock.send(command)  # 2.7 chokes on 'bytes' and 'encoding='\n        except (OSError, IOError) as error:  # MOE, LEAVE THIS ALONE!...for now.\n            sys.stderr.write('\\nGPS3 send command fail with {}\\n'.format(error))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self, timeout=0):\n        try:\n            waitin, _waitout, _waiterror = select.select((self.streamSock,), (), (), timeout)\n            if not waitin: return\n            else:\n                gpsd_response = self.streamSock.makefile()  # '.makefile(buffering=4096)' In strictly Python3\n                self.response = gpsd_response.readline()\n            return self.response\n\n        except StopIteration as error:\n            sys.stderr.write('The readline exception in GPSDSocket.next is--> {}'.format(error))", "response": "Returns the next available GPS - D entry in the file descriptor stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning off stream and close socket", "response": "def close(self):\n        \"\"\"turn off stream and close socket\"\"\"\n        if self.streamSock:\n            self.watch(enable=False)\n            self.streamSock.close()\n        self.streamSock = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unpack(self, gpsd_socket_response):\n        try:\n            fresh_data = json.loads(gpsd_socket_response)  # The reserved word 'class' is popped from JSON object class\n            package_name = fresh_data.pop('class', 'ERROR')  # gpsd data package errors are also 'ERROR'.\n            package = getattr(self, package_name, package_name)  # packages are named for JSON object class\n            for key in package.keys():\n                package[key] = fresh_data.get(key, 'n/a')  # Restores 'n/a' if key is absent in the socket response\n\n        except AttributeError:  # 'str' object has no attribute 'keys'\n            sys.stderr.write('There is an unexpected exception in DataStream.unpack')\n            return\n\n        except (ValueError, KeyError) as error:\n            sys.stderr.write(str(error))  # Extra data or aberrant data in stream.\n            return", "response": "Unpacks the data from a gpsd socket response into a new set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a list of commands to the daemon", "response": "def send(self, commands):\n        \"\"\"Ship commands to the daemon\n        Arguments:\n            commands: e.g., '?WATCH={{'enable':true,'json':true}}'|'?VERSION;'|'?DEVICES;'|'?DEVICE;'|'?POLL;'\n        \"\"\"\n        try:\n            self.streamSock.send(bytes(commands, encoding='utf-8'))\n        except TypeError:\n            self.streamSock.send(commands)  # 2.7 chokes on 'bytes' and 'encoding='\n        except (OSError, IOError) as error:  # HEY MOE, LEAVE THIS ALONE FOR NOW!\n            sys.stderr.write(f'\\nAGPS3 send command fail with {error}\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunpacks the data from a GPSD socket response into the current object attributes.", "response": "def unpack(self, gpsd_socket_response):\n        \"\"\"Sets new socket data as DataStream attributes in those initialised dictionaries\n        Arguments:\n            gpsd_socket_response (json object):\n        Provides:\n        self attributes, e.g., self.lat, self.gdop\n        Raises:\n        AttributeError: 'str' object has no attribute 'keys' when the device falls out of the system\n        ValueError, KeyError: most likely extra, or mangled JSON data, should not happen, but that\n        applies to a lot of things.\n        \"\"\"\n        try:\n            fresh_data = json.loads(gpsd_socket_response)  # 'class' is popped for iterator lead\n            class_name = fresh_data.pop('class')\n            for key in self.packages[class_name]:\n                # Fudge around the namespace collision with GST data package lat/lon being standard deviations\n                if class_name == 'GST' and key == 'lat' or 'lon':\n                    setattr(self, 'sd' + key, fresh_data.get(key, 'n/a'))\n                setattr(self, key, fresh_data.get(key, 'n/a'))  # Updates and restores 'n/a' if attribute is absent in the data\n\n        except AttributeError:  # 'str' object has no attribute 'keys'\n            sys.stderr.write('There is an unexpected exception unpacking JSON object')\n            return\n\n        except (ValueError, KeyError) as error:\n            sys.stderr.write(str(error))  # Extra data or aberrant data in stream.\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncurse terminal with standard outputs", "response": "def show_human():\n    \"\"\"Curses terminal with standard outputs \"\"\"\n    form = 'RAW'\n    units = 'raw'\n\n    data_window = curses.newwin(19, 39, 0, 0)\n    sat_window = curses.newwin(14, 39, 0, 40)\n    device_window = curses.newwin(6, 39, 13, 40)\n    packet_window = curses.newwin(7, 79, 19, 0)\n\n    for new_data in gpsd_socket:\n        if new_data:\n            data_stream.unpack(new_data)\n\n            screen.nodelay(1)\n            key_press = screen.getch()\n\n            if key_press == ord('q'):  # quit\n                shut_down()\n            elif key_press == ord('a'):  # NMEA\n                gpsd_socket.watch(enable=False, gpsd_protocol='json')\n                gpsd_socket.watch(gpsd_protocol='nmea')\n                show_nmea()\n            elif key_press == ord('0'):  # raw\n                form = 'RAW'\n                units = 'raw'\n                data_window.clear()\n            elif key_press == ord('1'):  # DDD\n                form = 'DDD'\n                data_window.clear()\n            elif key_press == ord('2'):  # DMM\n                form = 'DMM'\n                data_window.clear()\n            elif key_press == ord('3'):  # DMS\n                form = 'DMS'\n                data_window.clear()\n            elif key_press == ord('m'):  # Metric\n                units = 'metric'\n                data_window.clear()\n            elif key_press == ord('i'):  # Imperial\n                units = 'imperial'\n                data_window.clear()\n            elif key_press == ord('n'):  # Nautical\n                units = 'nautical'\n                data_window.clear()\n            elif key_press == ord('d'):  # Refresh device listings\n                gpsd_socket.send('?DEVICES;')\n                device_window.clear()\n\n            data_window.box()\n            data_window.addstr(0, 2, 'AGPS3 Python {}.{}.{} GPSD Interface'.format(*sys.version_info), curses.A_BOLD)\n            data_window.addstr(1, 2, 'Time:  {} '.format(data_stream.time))\n            data_window.addstr(2, 2, 'Latitude:  {} '.format(sexagesimal(data_stream.lat, 'lat', form)))\n            data_window.addstr(3, 2, 'Longitude: {} '.format(sexagesimal(data_stream.lon, 'lon', form)))\n            data_window.addstr(4, 2, 'Altitude:  {} {}'.format(*unit_conversion(data_stream.alt, units, length=True)))\n            data_window.addstr(5, 2, 'Speed:     {} {}'.format(*unit_conversion(data_stream.speed, units)))\n            data_window.addstr(6, 2, 'Heading:   {}\u00b0 True'.format(data_stream.track))\n            data_window.addstr(7, 2, 'Climb:     {} {}'.format(*unit_conversion(data_stream.climb, units, length=True)))\n            data_window.addstr(8, 2, 'Status:     {:<}D  '.format(data_stream.mode))\n            data_window.addstr(9, 2, 'Latitude Err:  +/-{} {}'.format(*unit_conversion(data_stream.epx, units, length=True)))\n            data_window.addstr(10, 2, 'Longitude Err: +/-{} {}'.format(*unit_conversion(data_stream.epy, units, length=True)))\n            data_window.addstr(11, 2, 'Altitude Err:  +/-{} {}'.format(*unit_conversion(data_stream.epv, units, length=True)))\n            data_window.addstr(12, 2, 'Course Err:    +/-{}   '.format(data_stream.epc), curses.A_DIM)\n            data_window.addstr(13, 2, 'Speed Err:     +/-{} {}'.format(*unit_conversion(data_stream.eps, units)), curses.A_DIM)\n            data_window.addstr(14, 2, 'Time Offset:   +/-{}  '.format(data_stream.ept), curses.A_DIM)\n            data_window.addstr(15, 2, 'gdop:{}  pdop:{}  tdop:{}'.format(data_stream.gdop, data_stream.pdop, data_stream.tdop))\n            data_window.addstr(16, 2, 'ydop:{}  xdop:{} '.format(data_stream.ydop, data_stream.xdop))\n            data_window.addstr(17, 2, 'vdop:{}  hdop:{} '.format(data_stream.vdop, data_stream.hdop))\n\n            sat_window.clear()\n            sat_window.box()\n            sat_window.addstr(0, 2, 'Using {0[1]}/{0[0]} satellites (truncated)'.format(satellites_used(data_stream.satellites)))\n            sat_window.addstr(1, 2, 'PRN     Elev   Azimuth   SNR   Used')\n            line = 2\n            if isinstance(data_stream.satellites, list):  # Nested lists of dictionaries are strings before data is present\n                for sats in data_stream.satellites[0:10]:\n                    sat_window.addstr(line, 2, '{PRN:>2}   {el:>6}   {az:>5}   {ss:>5}   {used:}'.format(**sats))\n                    line += 1\n\n            device_window.clear()\n            device_window.box()\n            if not isinstance(data_stream.devices, list):  # Local machines need a 'device' kick start\n                gpsd_socket.send('?DEVICES;')  # to have valid data.  I don't know why.\n\n            if isinstance(data_stream.devices, list):  # Nested lists of dictionaries are strings before data is present.\n                for gizmo in data_stream.devices:\n                    start_time, _uicroseconds = gizmo['activated'].split('.')  # Remove '.000Z'\n                    elapsed = elapsed_time_from(start_time)\n\n                    device_window.addstr(1, 2, 'Activated: {}'.format(gizmo['activated']))\n                    device_window.addstr(2, 2, 'Host:{0.host}:{0.port} {1}'.format(args, gizmo['path']))\n                    device_window.addstr(3, 2, 'Driver:{driver}     BPS:{bps}'.format(**gizmo))\n                    device_window.addstr(4, 2, 'Cycle:{0} Hz {1!s:>14} Elapsed'.format(1 / gizmo['cycle'], elapsed))\n\n            packet_window.clear()\n            # packet_window.border(0)\n            packet_window.scrollok(True)\n            packet_window.addstr(0, 0, '{}'.format(new_data))\n\n#            sleep(.9)\n\n            data_window.refresh()\n            sat_window.refresh()\n            device_window.refresh()\n            packet_window.refresh()\n        else:  # Reduced CPU cycles with the non-blocking socket read, by putting 'sleep' here, rather than hitting\n            sleep(.3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_widget(widget):\n    if widget in registry:\n        raise WidgetAlreadyRegistered(\n            _('The widget %s has already been registered.') % widget.__name__)\n    registry.append(widget)", "response": "Register a widget as a candidate to use in placeholder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_widget(name):\n    for widget in registry:\n        if widget.__name__ == name:\n            return widget\n    raise WidgetNotFound(\n        _('The widget %s has not been registered.') % name)", "response": "Get a widget class according to his name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_model(self, request, page, form, change):\n        language = form.cleaned_data['language']\n        target = form.data.get('target', None)\n        position = form.data.get('position', None)\n        page.save()\n\n        # if True, we need to move the page\n        if target and position:\n            try:\n                target = self.model.objects.get(pk=target)\n            except self.model.DoesNotExist:\n                pass\n            else:\n                target.invalidate()\n                page.move_to(target, position)\n\n        for name in self.mandatory_placeholders:\n            data = form.cleaned_data[name]\n            placeholder = PlaceholderNode(name)\n            extra_data = placeholder.get_extra_data(form.data)\n            placeholder.save(page, language, data, change,\n                extra_data=extra_data)\n\n        for placeholder in get_placeholders(page.get_template()):\n            if(placeholder.ctype in form.cleaned_data and placeholder.ctype\n                    not in self.mandatory_placeholders):\n                data = form.cleaned_data[placeholder.ctype]\n                extra_data = placeholder.get_extra_data(form.data)\n                placeholder.save(page, language, data, change,\n                    extra_data=extra_data)\n\n        page.invalidate()", "response": "Move the page in the tree if necessary and save every\n        placeholder :class:`Content <pages.models.Content>`."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fieldsets(self, request, obj=None):\n\n        # some ugly business to remove freeze_date\n        # from the field list\n        general_module = {\n            'fields': list(self.general_fields),\n            'classes': ('module-general',),\n        }\n\n        default_fieldsets = list(self.fieldsets)\n        if not request.user.has_perm('pages.can_freeze'):\n            general_module['fields'].remove('freeze_date')\n        if not request.user.has_perm('pages.can_publish'):\n            general_module['fields'].remove('status')\n        if not len(registry):\n            general_module['fields'].remove('delegate_to')\n\n        default_fieldsets[0][1] = general_module\n\n        placeholder_fieldsets = []\n        section_placeholder_fieldsets = defaultdict(list)\n        template = get_template_from_request(request, obj)\n        for placeholder in get_placeholders(template):\n            if placeholder.ctype not in self.mandatory_placeholders and not placeholder.section:\n                placeholder_fieldsets.append(placeholder.ctype)\n            elif placeholder.section:\n                section_placeholder_fieldsets[placeholder.section].append(placeholder.ctype)\n\n        additional_fieldsets = []\n        for title, fieldset in section_placeholder_fieldsets.items():\n            additional_fieldsets.append((title, {\n                'fields': fieldset,\n                'classes': ('module-content collapse grp-collapse grp-closed',),\n            }))\n        additional_fieldsets.append((_('Content'), {\n            'fields': placeholder_fieldsets,\n            'classes': ('module-content',),\n        }))\n        return default_fieldsets + additional_fieldsets", "response": "Returns a list of fieldsets that are available for the current page."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a ModelForm return an unsaved instance.", "response": "def save_form(self, request, form, change):\n        \"\"\"Given a ModelForm return an unsaved instance. ``change`` is True if\n        the object is being changed, and False if it's being added.\"\"\"\n        instance = super(PageAdmin, self).save_form(request, form, change)\n        instance.template = form.cleaned_data['template']\n        if not change:\n            instance.author = request.user\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_form(self, request, obj=None, **kwargs):\n\n        template = get_template_from_request(request, obj)\n\n        #model = create_page_model(get_placeholders(template))\n\n        form = make_form(self.model, get_placeholders(template))\n\n        # bound the form\n        language = get_language_from_request(request)\n        form.base_fields['language'].initial = language\n        if obj:\n            initial_slug = obj.slug(language=language, fallback=False)\n            initial_title = obj.title(language=language, fallback=False)\n            form.base_fields['slug'].initial = initial_slug\n            form.base_fields['title'].initial = initial_title\n\n        template = get_template_from_request(request, obj)\n        page_templates = settings.get_page_templates()\n        template_choices = list(page_templates)\n        # is default template is not in the list add it\n        if not [tpl for tpl in template_choices if tpl[0] == settings.PAGE_DEFAULT_TEMPLATE]:\n            template_choices.insert(0, (settings.PAGE_DEFAULT_TEMPLATE,\n                    _('Default template')))\n        form.base_fields['template'].choices = template_choices\n        form.base_fields['template'].initial = force_text(template)\n\n        for placeholder in get_placeholders(template):\n            ctype = placeholder.ctype\n            if obj:\n                initial = placeholder.get_content(obj, language, lang_fallback=False)\n            else:\n                initial = None\n            form.base_fields[ctype] = placeholder.get_field(obj,\n                language, initial=initial)\n\n        return form", "response": "Get a : class : PageForm for the the\n        the request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_view(self, request, form_url='', extra_context=None):\n        extra_context = {\n            'language': get_language_from_request(request),\n            'page_languages': settings.PAGE_LANGUAGES,\n        }\n        return super(PageAdmin, self).add_view(request, form_url,\n                                               extra_context)", "response": "The add admin view for the Page."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a setting from settings. py.", "response": "def get_setting(*args, **kwargs):\n    \"\"\"Get a setting and raise an appropriate user friendly error if\n    the setting is not found.\"\"\"\n    for name in args:\n        if hasattr(settings, name):\n            return getattr(settings, name)\n    if kwargs.get('raise_error', False):\n        setting_url = url % args[0].lower().replace('_', '-')\n        raise ImproperlyConfigured('Please make sure you specified at '\n            'least one of these settings: %s \\r\\nDocumentation: %s'\n            % (args, setting_url))\n    return kwargs.get('default_value', None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_page_templates():\n    PAGE_TEMPLATES = get_setting('PAGE_TEMPLATES',\n        default_value=())\n    if isinstance(PAGE_TEMPLATES, collections.Callable):\n        return PAGE_TEMPLATES()\n    else:\n        return PAGE_TEMPLATES", "response": "The callable that is used by the CMS."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nswitching the status of a page.", "response": "def change_status(request, page_id):\n    \"\"\"\n    Switch the status of a page.\n    \"\"\"\n    perm = request.user.has_perm('pages.change_page')\n    if perm and request.method == 'POST':\n        page = Page.objects.get(pk=page_id)\n        page.status = int(request.POST['status'])\n        page.invalidate()\n        page.save()\n        return HttpResponse(str(page.status))\n    raise Http404"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_pages_ajax(request, invalid_move=False):\n    language = get_language_from_request(request)\n    pages = Page.objects.root()\n    context = {\n        'can_publish': request.user.has_perm('pages.can_publish'),\n        'invalid_move': invalid_move,\n        'language': language,\n        'pages': pages,\n    }\n    return render(\n        request, \"admin/pages/page/change_list_table.html\",\n        context)", "response": "Render pages table for ajax function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef modify_placeholder(request, page_id):\n    content_type = request.GET.get('content_type')\n    language_id = request.GET.get('language_id')\n    page = get_object_or_404(Page, pk=page_id)\n    perm = request.user.has_perm('pages.change_page')\n    if perm and request.method == 'POST':\n        placeholders = get_placeholders(page.get_template())\n        for placeholder in placeholders:\n            if placeholder.name == content_type:\n                initial = placeholder.get_content(\n                    page, language_id, lang_fallback=False)\n                form = forms.Form(request.POST, request.FILES)\n                form.fields[content_type] = placeholder.get_field(\n                    page, language_id, initial=initial)\n                if not form.is_valid():\n                    return HttpResponse(form.as_p())\n\n                if placeholder.shared:\n                    save_page = None\n                else:\n                    save_page = page\n\n                extra_data = placeholder.get_extra_data(form.data)\n\n                placeholder.save(\n                    save_page, language_id,\n                    form.cleaned_data[content_type], True, extra_data)\n                if save_page:\n                    page.invalidate()\n                    # to update last modification date\n                    page.save()\n                else:\n                    fake_page.invalidate(content_type)\n\n                return HttpResponse('ok')\n        raise Http404(\"Content type not found in placeholders\")\n\n    raise Http404", "response": "Modify the content of a page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_last_content(request, page_id):\n    content_type = request.GET.get('content_type')\n    language_id = request.GET.get('language_id')\n    page = get_object_or_404(Page, pk=page_id)\n    placeholders = get_placeholders(page.get_template())\n    _template = template.loader.get_template(page.get_template())\n    for placeholder in placeholders:\n        if placeholder.name == content_type:\n            context = RequestContext(request, {\n                'current_page': page,\n                'lang': language_id\n            })\n            with context.bind_template(_template.template):\n                content = placeholder.render(context)\n                return HttpResponse(content)\n    raise Http404", "response": "Get the latest content for a particular type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_content(request, page_id, content_id):\n    content = Content.objects.get(pk=content_id)\n    return HttpResponse(content.body)", "response": "Get the content for a particular page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the children of the requested page with the sub_menu_ template.", "response": "def sub_menu(request, page_id):\n    \"\"\"Render the children of the requested page with the sub_menu\n    template.\"\"\"\n    page = Page.objects.get(id=page_id)\n    pages = page.children.all()\n    page_languages = settings.PAGE_LANGUAGES\n    return render(request, \"admin/pages/page/sub_menu.html\", {\n        'can_publish': request.user.has_perm('pages.can_publish'),\n        'page': page,\n        'pages': pages,\n        'page_languages': page_languages,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting pages in JSON format.", "response": "def handle(self, site=None, **options):\n        \"\"\"\n        Export pages in JSON format.  Site may be specified by id or domain.\n        Default: export pages from the current site specified by settings.SITE_ID.\n        \"\"\"\n        if options['all_sites']:\n            monkeypatch_remove_pages_site_restrictions()\n        qs = Page.objects.all()\n        if site:\n            for match in ('pk', 'domain'):\n                try:\n                    s = Site.objects.get(**{match:site})\n                    break\n                except (Site.objects.DoesNotExist, ValueError):\n                    continue\n            else:\n                raise CommandError(_(\"Site with id/domain = '%s' not found\")\n                    % site)\n            qs.filter(site=s)\n\n        sys.stdout.write(pages_to_json(qs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef media(request):\n    return {\n        'PAGES_MEDIA_URL': settings.PAGES_MEDIA_URL,\n        'PAGES_STATIC_URL': settings.PAGES_STATIC_URL,\n        'PAGE_USE_SITE_ID': settings.PAGE_USE_SITE_ID,\n        'PAGE_HIDE_SITES': settings.PAGE_HIDE_SITES,\n        'PAGE_LANGUAGES': settings.PAGE_LANGUAGES,\n    }", "response": "Adds media - related variables to the context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of PlaceholderNode objects found in the given template.", "response": "def get_placeholders(template_name):\n    \"\"\"Return a list of PlaceholderNode found in the given template.\n\n    :param template_name: the name of the template file\n    \"\"\"\n    dummy_context.template = template.Template(\"\")\n    try:\n        temp_wrapper = template.loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return []\n\n    plist, blist = [], []\n    temp = temp_wrapper.template\n    _placeholders_recursif(temp.nodelist, plist, blist)\n\n    previous = {}\n    block_to_remove = []\n    for block in blist:\n        if block.name in previous:\n            if not hasattr(block, 'has_super_var'):\n                block_to_remove.append(previous[block.name])\n        previous[block.name] = block\n\n    def keep(p):\n        return p.found_in_block not in block_to_remove\n\n    placeholders = [p for p in plist if keep(p)]\n    names = []\n    pfiltered = []\n    for p in placeholders:\n        if p.ctype not in names:\n            pfiltered.append(p)\n            names.append(p.ctype)\n\n    return pfiltered"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _placeholders_recursif(nodelist, plist, blist):\n    # I needed to do this lazy import to compile the documentation\n    from django.template.loader_tags import BlockNode\n\n    if len(blist):\n        block = blist[-1]\n    else:\n        block = None\n\n    for node in nodelist:\n\n        if isinstance(node, BlockNode):\n            if node not in blist:\n                blist.append(node)\n            if not block:\n                block = node\n\n        if block:\n            if isinstance(node, template.base.VariableNode):\n                if(node.filter_expression.var.var == u'block.super'):\n                    block.has_super_var = True\n\n        # extends node?\n        if hasattr(node, 'parent_name'):\n            # I do not know why I did this... but the tests are guarding it\n            dummy_context2 = Context()\n            dummy_context2.template = template.Template(\"\")\n            _placeholders_recursif(node.get_parent(dummy_context2).nodelist,\n                                   plist, blist)\n        # include node?\n        elif hasattr(node, 'template') and hasattr(node.template, 'nodelist'):\n            _placeholders_recursif(node.template.nodelist, plist, blist)\n\n        # Is it a placeholder?\n        if hasattr(node, 'page') and hasattr(node, 'parsed') and \\\n                hasattr(node, 'as_varname') and hasattr(node, 'name') \\\n                and hasattr(node, 'section'):\n            if block:\n                node.found_in_block = block\n            plist.append(node)\n            node.render(dummy_context)\n\n        for key in ('nodelist', 'nodelist_true', 'nodelist_false'):\n\n            if hasattr(node, key):\n                try:\n                    _placeholders_recursif(getattr(node, key), plist, blist)\n                except:\n                    pass", "response": "Recursively search into a template node list for PlaceholderNode\ninel node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a normalized url with trailing and without leading slash.", "response": "def normalize_url(url):\n    \"\"\"Return a normalized url with trailing and without leading slash.\n\n     >>> normalize_url(None)\n     '/'\n     >>> normalize_url('/')\n     '/'\n     >>> normalize_url('/foo/bar')\n     '/foo/bar'\n     >>> normalize_url('foo/bar')\n     '/foo/bar'\n     >>> normalize_url('/foo/bar/')\n     '/foo/bar'\n    \"\"\"\n    if not url or len(url) == 0:\n        return '/'\n    if not url.startswith('/'):\n        url = '/' + url\n    if len(url) > 1 and url.endswith('/'):\n        url = url[0:len(url) - 1]\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef slugify(value, allow_unicode=False):\n    value = force_text(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n        value = re.sub('[^\\w\\s-]', '', value, flags=re.U).strip().lower()\n        return mark_safe(re.sub('[-\\s]+', '-', value, flags=re.U))\n    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub('[^\\w\\s-]', '', value).strip().lower()\n    return mark_safe(re.sub('[-\\s]+', '-', value))", "response": "Convert to ASCII Convert spaces to hyphens Convert to lowercase Also strip leading and trailing whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if any page templates can t be loaded.", "response": "def page_templates_loading_check(app_configs, **kwargs):\n    \"\"\" Check if any page template can't be loaded. \"\"\"\n    errors = []\n\n    for page_template in settings.get_page_templates():\n        try:\n            loader.get_template(page_template[0])\n        except template.TemplateDoesNotExist:\n            errors.append(checks.Warning(\n                    'Django cannot find template %s' % page_template[0],\n                    obj=page_template, id='pages.W001'))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Page object from a slug or id.", "response": "def get_page_from_string_or_id(page_string, lang=None):\n    \"\"\"Return a Page object from a slug or an id.\"\"\"\n    if type(page_string) == int:\n        return Page.objects.get(pk=int(page_string))\n    # if we have a string coming from some templates templates\n    if (isinstance(page_string, SafeText) or\n            isinstance(page_string, str)):\n        if page_string.isdigit():\n            return Page.objects.get(pk=int(page_string))\n        return Page.objects.from_path(page_string, lang)\n    # in any other case we return the input becasue it's probably\n    # a Page object.\n    return page_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_content(context, page, content_type, lang, fallback=True):\n    if not page:\n        return ''\n\n    if not lang and 'lang' in context:\n        lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n\n    page = get_page_from_string_or_id(page, lang)\n\n    if not page:\n        return ''\n\n    content = Content.objects.get_content(page, lang, content_type, fallback)\n    return content", "response": "Helper function used by PlaceholderNode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_content_in(page, language):\n    if page is None:\n        return False\n    return Content.objects.filter(page=page, language=language).count() > 0", "response": "Fitler that returns True if the current page has any content in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pages_menu(context, page, url='/'):\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    if page:\n        children = page.get_children_for_frontend()\n        context.update({'children': children, 'page': page})\n    return context", "response": "Render a nested list of all the descendents of the given page including this page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the root page of the given page and also render a nested list of all root s children pages.", "response": "def pages_sub_menu(context, page, url='/'):\n    \"\"\"Get the root page of the given page and\n    render a nested list of all root's children pages.\n    Good for rendering a secondary menu.\n\n    :param page: the page where to start the menu from.\n    :param url: not used anymore.\n    \"\"\"\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    if page:\n        root = page.get_root()\n        children = root.get_children_for_frontend()\n        context.update({'children': children, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the parent page of the given page and render a nested list of its child pages. Good for rendering a secondary menu.", "response": "def pages_siblings_menu(context, page, url='/'):\n    \"\"\"Get the parent page of the given page and render a nested list of its\n    child pages. Good for rendering a secondary menu.\n\n    :param page: the page where to start the menu from.\n    :param url: not used anymore.\n    \"\"\"\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    if page:\n        siblings = page.get_siblings()\n        context.update({'children': siblings, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_content(context, page, content_type, lang=None, fallback=True):\n    return {'content': _get_content(\n        context, page, content_type, lang, fallback)\n    }", "response": "Display a content type from a page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_absolute_url(context, page, lang=None):\n    if not lang:\n        lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    if not page:\n        return {'content': ''}\n    url = page.get_url_path(language=lang)\n    if url:\n        return {'content': url}\n    return {'content': ''}", "response": "Show the absolute url of a page in the right language"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders a dynamic tree menu.", "response": "def pages_dynamic_tree_menu(context, page, url='/'):\n    \"\"\"\n    Render a \"dynamic\" tree menu, with all nodes expanded which are either\n    ancestors or the current page itself.\n\n    Override ``pages/dynamic_tree_menu.html`` if you want to change the\n    design.\n\n    :param page: the current page\n    :param url: not used anymore\n    \"\"\"\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    children = None\n    if page and 'current_page' in context:\n        current_page = context['current_page']\n        # if this node is expanded, we also have to render its children\n        # a node is expanded if it is the current node or one of its ancestors\n        if(\n            page.tree_id == current_page.tree_id and\n            page.lft <= current_page.lft and\n            page.rght >= current_page.rght\n        ):\n            children = page.get_children_for_frontend()\n    context.update({'children': children, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a breadcrumb like menu.", "response": "def pages_breadcrumb(context, page, url='/'):\n    \"\"\"\n    Render a breadcrumb like menu.\n\n    Override ``pages/breadcrumb.html`` if you want to change the\n    design.\n\n    :param page: the current page\n    :param url: not used anymore\n    \"\"\"\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    pages_navigation = None\n    if page:\n        pages_navigation = page.get_ancestors()\n    context.update({'pages_navigation': pages_navigation, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a page and insert into the template s context.", "response": "def do_get_page(parser, token):\n    \"\"\"Retrieve a page and insert into the template's context.\n\n    Example::\n\n        {% get_page \"news\" as news_page %}\n\n    :param page: the page object, slug or id\n    :param name: name of the context variable to store the page in\n    \"\"\"\n    bits = token.split_contents()\n    if 4 != len(bits):\n        raise TemplateSyntaxError('%r expects 4 arguments' % bits[0])\n    if bits[-2] != 'as':\n        raise TemplateSyntaxError(\n            '%r expects \"as\" as the second argument' % bits[0])\n    page_filter = parser.compile_filter(bits[1])\n    varname = bits[-1]\n    return GetPageNode(page_filter, varname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a Content object and insert it into the template s context variable.", "response": "def do_get_content(parser, token):\n    \"\"\"Retrieve a Content object and insert it into the template's context.\n\n    Example::\n\n        {% get_content page_object \"title\" as content %}\n\n    You can also use the slug of a page::\n\n        {% get_content \"my-page-slug\" \"title\" as content %}\n\n    Syntax::\n\n        {% get_content page type [lang] as name %}\n\n    :param page: the page object, slug or id\n    :param type: content_type used by a placeholder\n    :param name: name of the context variable to store the content in\n    :param lang: the wanted language\n    \"\"\"\n    bits = token.split_contents()\n    if not 5 <= len(bits) <= 6:\n        raise TemplateSyntaxError('%r expects 4 or 5 arguments' % bits[0])\n    if bits[-2] != 'as':\n        raise TemplateSyntaxError(\n            '%r expects \"as\" as the second last argument' % bits[0])\n    page = parser.compile_filter(bits[1])\n    content_type = parser.compile_filter(bits[2])\n    varname = bits[-1]\n    lang = None\n    lang_filter = None\n    if len(bits) == 6:\n        lang = bits[3]\n    else:\n        lang_filter = parser.compile_filter(\"lang\")\n    return GetContentNode(page, content_type, varname, lang, lang_filter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_markdownlaceholder(parser, token):\n    name, params = parse_placeholder(parser, token)\n    return MarkdownPlaceholderNode(name, **params)", "response": "Method that parse the markdownplaceholder template tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_imageplaceholder(parser, token):\n    name, params = parse_placeholder(parser, token)\n    return ImagePlaceholderNode(name, **params)", "response": "Method that parses the imageplaceholder template tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_fileplaceholder(parser, token):\n    name, params = parse_placeholder(parser, token)\n    return FilePlaceholderNode(name, **params)", "response": "Method that parse the fileplaceholder template tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_contactplaceholder(parser, token):\n    name, params = parse_placeholder(parser, token)\n    return ContactPlaceholderNode(name, **params)", "response": "Method that parse the contactplaceholder template tag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef language_content_up_to_date(page, language):\n    lang_code = getattr(settings, 'LANGUAGE_CODE', None)\n    if lang_code == language:\n        # official version is always \"up to date\"\n        return True\n    # get the last modified date for the official version\n    last_modified = Content.objects.filter(\n        language=lang_code,\n        page=page).order_by('-creation_date')\n    if not last_modified:\n        # no official version\n        return True\n    lang_modified = Content.objects.filter(\n        language=language,\n        page=page).order_by('-creation_date')[0].creation_date\n    return lang_modified > last_modified[0].creation_date", "response": "Tells if all the page content has been updated since the last modified date of the language version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_page_has_content(parser, token):\n    nodelist = parser.parse(('end_page_has_content',))\n    parser.delete_first_token()\n    args = token.split_contents()\n    try:\n        content_type = unescape_string_literal(args[1])\n    except IndexError:\n        raise template.TemplateSyntaxError(\n            \"%r tag requires the argument content_type\" % args[0]\n        )\n    if len(args) > 2:\n        page = args[2]\n    else:\n        page = None\n    return PageHasContentNode(page, content_type, nodelist)", "response": "Returns a conditional tag that only renders its nodes if the page has content for a particular content type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_request_mock():\n    from django.test.client import RequestFactory\n    from django.core.handlers.base import BaseHandler\n    factory = RequestFactory()\n\n    request = factory.get('/')\n    \n    class FakeUser():\n        is_authenticated = False\n        is_staff = False\n\n    request.user = FakeUser()\n    request.session = {}\n\n    return request", "response": "Build a request mock up for tests"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the slug from the path", "response": "def remove_slug(path):\n    \"\"\"\n    Return the remainin part of the path\n\n        >>> remove_slug('/test/some/function/')\n        test/some\n    \"\"\"\n    if path.endswith('/'):\n        path = path[:-1]\n    if path.startswith('/'):\n        path = path[1:]\n    if \"/\" not in path or not path:\n        return None\n    parts = path.split(\"/\")[:-1]\n    return \"/\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a valid template from different sources or falls back to the PAGE_DEFAULT_TEMPLATE if no valid template is found.", "response": "def get_template_from_request(request, page=None):\n    \"\"\"\n    Gets a valid template from different sources or falls back to the\n    default template.\n    \"\"\"\n    page_templates = settings.get_page_templates()\n    if len(page_templates) == 0:\n        return settings.PAGE_DEFAULT_TEMPLATE\n    template = request.POST.get('template', request.GET.get('template', None))\n    if template is not None and \\\n            (template in list(dict(page_templates).keys()) or\n            template == settings.PAGE_DEFAULT_TEMPLATE):\n        return template\n    if page is not None:\n        return page.get_template()\n    return settings.PAGE_DEFAULT_TEMPLATE"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_language_from_request(request):\n    language = request.GET.get('language', None)\n    if language:\n        return language\n\n    if hasattr(request, 'LANGUAGE_CODE'):\n        lang = settings.PAGE_LANGUAGE_MAPPING(str(request.LANGUAGE_CODE))\n        if lang not in LANGUAGE_KEYS:\n            return settings.PAGE_DEFAULT_LANGUAGE\n        else:\n            return lang\n    else:\n        return settings.PAGE_DEFAULT_LANGUAGE", "response": "Return the most obvious language according the request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef navigation(self):\n        return self.on_site().filter(\n            status=self.model.PUBLISHED).filter(parent__isnull=True)", "response": "Creates a : class : QuerySet of the published root pages."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_published(self, queryset):\n        if settings.PAGE_USE_SITE_ID:\n            queryset = queryset.filter(sites=global_settings.SITE_ID)\n\n        queryset = queryset.filter(status=self.model.PUBLISHED)\n\n        if settings.PAGE_SHOW_START_DATE:\n            queryset = queryset.filter(publication_date__lte=get_now())\n\n        if settings.PAGE_SHOW_END_DATE:\n            queryset = queryset.filter(\n                Q(publication_end_date__gt=get_now()) |\n                Q(publication_end_date__isnull=True)\n            )\n\n        return queryset", "response": "Filter the given pages : class : QuerySet to obtain only published\n        page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a QuerySet of drafts using the page s . publication_date.", "response": "def drafts(self):\n        \"\"\"Creates a :class:`QuerySet` of drafts using the page's\n        :attr:`Page.publication_date`.\"\"\"\n        pub = self.on_site().filter(status=self.model.DRAFT)\n        if settings.PAGE_SHOW_START_DATE:\n            pub = pub.filter(publication_date__gte=get_now())\n        return pub"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a : class : Page according to the page s path.", "response": "def from_path(self, complete_path, lang, exclude_drafts=True):\n        \"\"\"Return a :class:`Page <pages.models.Page>` according to\n        the page's path.\"\"\"\n        if complete_path.endswith(\"/\"):\n            complete_path = complete_path[:-1]\n        # just return the root page\n        if complete_path == '':\n            root_pages = self.root()\n            if root_pages:\n                return root_pages[0]\n            else:\n                return None\n\n        slug = get_slug(complete_path)\n        from pages.models import Content\n        page_ids = Content.objects.get_page_ids_by_slug(slug)\n        pages_list = self.on_site().filter(id__in=page_ids)\n        if exclude_drafts:\n            pages_list = pages_list.exclude(status=self.model.DRAFT)\n        if len(pages_list) == 1:\n            if(settings.PAGE_USE_STRICT_URL and\n                    pages_list[0].get_complete_slug(lang) != complete_path):\n                return None\n            return pages_list[0]\n        # if more than one page is matching the slug,\n        # we need to use the full URL\n        if len(pages_list) > 1:\n            for page in pages_list:\n                if page.get_complete_slug(lang) == complete_path:\n                    return page\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets or create a Content object for a specific page and language.", "response": "def set_or_create_content(self, page, language, ctype, body):\n        \"\"\"Set or create a :class:`Content <pages.models.Content>` for a\n        particular page and language.\n\n        :param page: the concerned page object.\n        :param language: the wanted language.\n        :param ctype: the content type.\n        :param body: the content of the Content object.\n        \"\"\"\n        try:\n            content = self.filter(page=page, language=language,\n                                  type=ctype).latest('creation_date')\n            content.body = body\n        except self.model.DoesNotExist:\n            content = self.model(page=page, language=language, body=body,\n                                 type=ctype)\n        content.save()\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_content_if_changed(self, page, language, ctype, body):\n        try:\n            content = self.filter(\n                page=page, language=language,\n                type=ctype).latest('creation_date')\n            if content.body == body:\n                return content\n        except self.model.DoesNotExist:\n            pass\n        content = self.create(\n            page=page, language=language, body=body,\n            type=ctype)\n\n        # Delete old revisions\n        if settings.PAGE_CONTENT_REVISION_DEPTH:\n            oldest_content = self.filter(\n                page=page, language=language,\n                type=ctype\n            ).order_by('-creation_date')[settings.PAGE_CONTENT_REVISION_DEPTH:]\n            for c in oldest_content:\n                c.delete()\n\n        return content", "response": "Create a Content object for a particular content type and body if the content has changed since the last time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the latest published content object for a particular page language and placeholder type.", "response": "def get_content_object(self, page, language, ctype):\n        \"\"\"Gets the latest published :class:`Content <pages.models.Content>`\n        for a particular page, language and placeholder type.\"\"\"\n        params = {\n            'language': language,\n            'type': ctype,\n            'page': None if page is fake_page else page\n        }\n        if page.freeze_date:\n            params['creation_date__lte'] = page.freeze_date\n        return self.filter(**params).latest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the latest content string for a particular page language and content type.", "response": "def get_content(self, page, language, ctype, language_fallback=False):\n        \"\"\"Gets the latest content string for a particular page, language and\n        placeholder.\n\n        :param page: the concerned page object.\n        :param language: the wanted language.\n        :param ctype: the content type.\n        :param language_fallback: fallback to another language if ``True``.\n        \"\"\"\n        if page is None:\n            page = fake_page\n\n        if \" \" in ctype:\n            raise ValueError(\"Ctype cannot contain spaces.\")\n        if not language:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        frozen = int(bool(page.freeze_date))\n        key = self.PAGE_CONTENT_DICT_KEY % (page.id, ctype, frozen)\n\n        # Spaces do not work with memcache\n        key = key.replace(' ', '-')\n\n        if page._content_dict is None:\n            page._content_dict = dict()\n        if page._content_dict.get(key, None):\n            content_dict = page._content_dict.get(key)\n        else:\n            content_dict = cache.get(key)\n\n        # fill a dict object for each language, that will create\n        # P * L queries.\n        # L == number of language, P == number of placeholder in the page.\n        # Once generated the result is cached.\n        if not content_dict:\n            content_dict = {}\n            for lang in settings.PAGE_LANGUAGES:\n                try:\n                    content = self.get_content_object(page, lang[0], ctype)\n                    content_dict[lang[0]] = content.body\n                except self.model.DoesNotExist:\n                    content_dict[lang[0]] = ''\n            page._content_dict[key] = content_dict\n            cache.set(key, content_dict)\n\n        if language in content_dict and content_dict[language]:\n            return content_dict[language]\n\n        if language_fallback:\n            for lang in settings.PAGE_LANGUAGES:\n                if lang[0] in content_dict and content_dict[lang[0]]:\n                    return content_dict[lang[0]]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the latest Content object that matches the given slug.", "response": "def get_content_slug_by_slug(self, slug):\n        \"\"\"Returns the latest :class:`Content <pages.models.Content>`\n        slug object that match the given slug for the current site domain.\n\n        :param slug: the wanted slug.\n        \"\"\"\n        content = self.filter(type='slug', body=slug)\n        if settings.PAGE_USE_SITE_ID:\n            content = content.filter(page__sites__id=global_settings.SITE_ID)\n        try:\n            content = content.latest('creation_date')\n        except self.model.DoesNotExist:\n            return None\n        else:\n            return content"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all page s id matching the given slug.", "response": "def get_page_ids_by_slug(self, slug):\n        \"\"\"Return all page's id matching the given slug.\n        This function also returns pages that have an old slug\n        that match.\n\n        :param slug: the wanted slug.\n        \"\"\"\n        ids = self.filter(type='slug', body=slug).values('page_id').annotate(\n            max_creation_date=Max('creation_date')\n        )\n        return [content['page_id'] for content in ids]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving a request to an alias.", "response": "def from_path(self, request, path, lang):\n        \"\"\"\n        Resolve a request to an alias. returns a\n        :class:`PageAlias <pages.models.PageAlias>` if the url matches\n        no page at all. The aliasing system supports plain\n        aliases (``/foo/bar``) as well as aliases containing GET parameters\n        (like ``index.php?page=foo``).\n\n        :param request: the request object\n        :param path: the complete path to the page\n        :param lang: not used\n        \"\"\"\n        from pages.models import PageAlias\n\n        url = normalize_url(path)\n        # \u00a71: try with complete query string\n        query = request.META.get('QUERY_STRING')\n        if query:\n            url = url + '?' + query\n        try:\n            alias = PageAlias.objects.get(url=url)\n            return alias\n        except PageAlias.DoesNotExist:\n            pass\n        # \u00a72: try with path only\n        url = normalize_url(path)\n        try:\n            alias = PageAlias.objects.get(url=url)\n            return alias\n        except PageAlias.DoesNotExist:\n            pass\n        # \u00a73: not alias found, we give up\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resolve_page(self, request, context, is_staff):\n        path = context['path']\n        lang = context['lang']\n        page = Page.objects.from_path(\n            path, lang,\n            exclude_drafts=(not is_staff))\n        if page:\n            return page\n        # if the complete path didn't worked out properly\n        # and if didn't used PAGE_USE_STRICT_URL setting we gonna\n        # try to see if it might be a delegation page.\n        # To do that we remove the right part of the url and try again\n        # to find a page that match\n        if not settings.PAGE_USE_STRICT_URL:\n            path = remove_slug(path)\n            while path is not None:\n                page = Page.objects.from_path(\n                    path, lang,\n                    exclude_drafts=(not is_staff))\n                # find a match. Is the page delegating?\n                if page:\n                    if page.delegate_to:\n                        return page\n                path = remove_slug(path)\n\n        return None", "response": "Return the appropriate page according to the path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef choose_language(self, lang, request):\n\n        # Can be an empty string or None\n        if not lang:\n            lang = get_language_from_request(request)\n\n        # Raise a 404 if the language is not in not in the list\n        if lang not in [key for (key, value) in settings.PAGE_LANGUAGES]:\n            raise Http404\n\n        # We're going to serve CMS pages in language lang;\n        # make django gettext use that language too\n        if lang and translation.check_for_language(lang):\n            translation.activate(lang)\n\n        return lang", "response": "Deal with the multiple corner case of choosing the language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extra_context(self, request, context):\n        if settings.PAGE_EXTRA_CONTEXT:\n            context.update(settings.PAGE_EXTRA_CONTEXT())", "response": "Update the context with the extra data from the PAGE_EXTRA_CONTEXT function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides the default save method.", "response": "def save(self, *args, **kwargs):\n        \"\"\"Override the default ``save`` method.\"\"\"\n        if not self.status:\n            self.status = self.DRAFT\n        # Published pages should always have a publication date\n        if self.publication_date is None and self.status == self.PUBLISHED:\n            self.publication_date = get_now()\n        # Drafts should not, unless they have been set to the future\n        if self.status == self.DRAFT:\n            if settings.PAGE_SHOW_START_DATE:\n                if (self.publication_date and\n                        self.publication_date <= get_now()):\n                    self.publication_date = None\n            else:\n                self.publication_date = None\n        self.last_modification_date = get_now()\n        super(Page, self).save(*args, **kwargs)\n        # fix sites many-to-many link when the're hidden from the form\n        if settings.PAGE_HIDE_SITES and self.sites.count() == 0:\n            self.sites.add(Site.objects.get(pk=global_settings.SITE_ID))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_calculated_status(self):\n        if settings.PAGE_SHOW_START_DATE and self.publication_date:\n            if self.publication_date > get_now():\n                return self.DRAFT\n\n        if settings.PAGE_SHOW_END_DATE and self.publication_end_date:\n            if self.publication_end_date < get_now():\n                return self.EXPIRED\n\n        return self.status", "response": "Get the calculated status of the page based on the current date and the current status."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a QuerySet of published children page", "response": "def published_children(self):\n        \"\"\"Return a :class:`QuerySet` of published children page\"\"\"\n        key = self.PUB_CHILDREN_KEY % self.pk\n        #children = cache.get(key, None)\n        # if children is None:\n        children = Page.objects.filter_published(self.get_children()).all()\n        #cache.set(key, children)\n        return children"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef move_to(self, target, position='first-child'):\n\n        # Invalidate both in case position matters,\n        # otherwise only target is needed.\n        self.invalidate()\n        target.invalidate()\n        super(Page, self).move_to(target, position=position)", "response": "Invalidate cache when moving to target."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninvalidates the cached data for this page.", "response": "def invalidate(self):\n        \"\"\"Invalidate cached data for this page.\"\"\"\n\n        cache.delete(self.PAGE_LANGUAGES_KEY % (self.pk))\n        cache.delete('PAGE_FIRST_ROOT_ID')\n        cache.delete(self.CHILDREN_KEY % self.pk)\n        cache.delete(self.PUB_CHILDREN_KEY % self.pk)\n        # XXX: Should this have a depth limit?\n        if self.parent_id:\n            self.parent.invalidate()\n        self._languages = None\n        self._complete_slug = None\n        self._content_dict = dict()\n\n        placeholders = get_placeholders(self.get_template())\n\n        p_names = [p.ctype for p in placeholders]\n        if 'slug' not in p_names:\n            p_names.append('slug')\n        if 'title' not in p_names:\n            p_names.append('title')\n\n        from pages.managers import fake_page\n        shared = [p for p in placeholders if p.shared]\n        for share in shared:\n            fake_page.invalidate(share.ctype)\n\n        # delete content cache, frozen or not\n        for name in p_names:\n            # frozen\n            cache.delete(\n                PAGE_CONTENT_DICT_KEY %\n                (self.pk, name, 1))\n            # not frozen\n            cache.delete(\n                PAGE_CONTENT_DICT_KEY %\n                (self.pk, name, 0))\n\n        cache.delete(self.PAGE_URL_KEY % (self.pk))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_languages(self):\n        if self._languages:\n            return self._languages\n        self._languages = cache.get(self.PAGE_LANGUAGES_KEY % (self.pk))\n        if self._languages is not None:\n            return self._languages\n\n        languages = [c['language'] for\n            c in Content.objects.filter(page=self,\n            type=\"slug\").values('language')]\n        # remove duplicates\n        languages = sorted(set(languages))\n        cache.set(self.PAGE_LANGUAGES_KEY % (self.pk), languages)\n        self._languages = languages\n        return languages", "response": "Return a list of all used languages for this page."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if this page is the first root pages.", "response": "def is_first_root(self):\n        \"\"\"Return ``True`` if this page is the first root pages.\"\"\"\n        parent_cache_key = 'PARENT_FOR_%d' % self.pk\n        has_parent = cache.get(parent_cache_key, None)\n        if has_parent is None:\n            has_parent = not not self.parent\n            cache.set(parent_cache_key, has_parent)\n\n        if has_parent:\n            return False\n        if self._is_first_root is not None:\n            return self._is_first_root\n        first_root_id = cache.get('PAGE_FIRST_ROOT_ID')\n        if first_root_id is not None:\n            self._is_first_root = first_root_id == self.pk\n            return self._is_first_root\n        try:\n            first_root_id = Page.objects.root().values('id')[0]['id']\n        except IndexError:\n            first_root_id = None\n        if first_root_id is not None:\n            cache.set('PAGE_FIRST_ROOT_ID', first_root_id)\n        self._is_first_root = self.pk == first_root_id\n        return self._is_first_root"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_template(self):\n        if self.template:\n            return self.template\n\n        template = None\n        for p in self.get_ancestors(ascending=True):\n            if p.template:\n                template = p.template\n                break\n\n        if not template:\n            template = settings.PAGE_DEFAULT_TEMPLATE\n\n        return template", "response": "Get the template of this page if it is defined or the closer parent s one if it is defined."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the template name of this page if defined or a closer parent has a defined template.", "response": "def get_template_name(self):\n        \"\"\"\n        Get the template name of this page if defined or if a closer\n        parent has a defined template or\n        :data:`pages.settings.PAGE_DEFAULT_TEMPLATE` otherwise.\n        \"\"\"\n        template = self.get_template()\n        page_templates = settings.get_page_templates()\n        for t in page_templates:\n            if t[0] == template:\n                return t[1]\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_targets(self):\n        exclude_list = [self.pk]\n        for p in self.get_descendants():\n            exclude_list.append(p.id)\n        return Page.objects.exclude(id__in=exclude_list)", "response": "Return a QuerySet of valid targets for moving a page into the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the URL s path component.", "response": "def get_url_path(self, language=None):\n        \"\"\"Return the URL's path component. Add the language prefix if\n        ``PAGE_USE_LANGUAGE_PREFIX`` setting is set to ``True``.\n\n        :param language: the wanted url language.\n        \"\"\"\n        if self.is_first_root():\n            # this is used to allow users to change URL of the root\n            # page. The language prefix is not usable here.\n            try:\n                return reverse('pages-root')\n            except Exception:\n                pass\n        url = self.get_complete_slug(language)\n        if not language:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n        if settings.PAGE_USE_LANGUAGE_PREFIX:\n            return reverse('pages-details-by-path',\n                args=[language, url])\n        else:\n            return reverse('pages-details-by-path', args=[url])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the complete slug of this page by concatenating all parent s slugs.", "response": "def get_complete_slug(self, language=None, hideroot=True):\n        \"\"\"Return the complete slug of this page by concatenating\n        all parent's slugs.\n\n        :param language: the wanted slug language.\"\"\"\n        if not language:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        if self._complete_slug and language in self._complete_slug:\n            return self._complete_slug[language]\n\n        self._complete_slug = cache.get(self.PAGE_URL_KEY % (self.pk))\n        if self._complete_slug is None:\n            self._complete_slug = {}\n        elif language in self._complete_slug:\n            return self._complete_slug[language]\n\n        if hideroot and settings.PAGE_HIDE_ROOT_SLUG and self.is_first_root():\n            url = ''\n        else:\n            url = '%s' % self.slug(language)\n\n        key = self.ANCESTORS_KEY % self.pk\n        ancestors = cache.get(key, None)\n        if ancestors is None:\n            ancestors = self.get_ancestors(ascending=True)\n            cache.set(key, ancestors)\n\n        for ancestor in ancestors:\n            url = ancestor.slug(language) + '/' + url\n\n        self._complete_slug[language] = url\n        cache.set(self.PAGE_URL_KEY % (self.pk), self._complete_slug)\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef slug_with_level(self, language=None):\n        level = ''\n        if self.level:\n            for n in range(0, self.level):\n                level += '&nbsp;&nbsp;&nbsp;'\n        return mark_safe(level + self.slug(language))", "response": "Display the slug of the page prepended with insecable\n        spaces to simluate the level of page in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the slug of the page depending on the language.", "response": "def slug(self, language=None, fallback=True):\n        \"\"\"\n        Return the slug of the page depending on the given language.\n\n        :param language: wanted language, if not defined default is used.\n        :param fallback: if ``True``, the slug will also be searched in other \\\n        languages.\n        \"\"\"\n\n        slug = self.get_content(language, 'slug', language_fallback=fallback)\n        if slug == '':\n            return \"Page {0}\".format(self.pk)\n\n        return slug"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the title of the page depending on the given language.", "response": "def title(self, language=None, fallback=True):\n        \"\"\"\n        Return the title of the page depending on the given language.\n\n        :param language: wanted language, if not defined default is used.\n        :param fallback: if ``True``, the slug will also be searched in \\\n        other languages.\n        \"\"\"\n        if not language:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        return self.get_content(language, 'title', language_fallback=fallback)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenforcing a unique slug accross all pages and websistes.", "response": "def unique_slug_required(form, slug):\n    \"\"\"Enforce a unique slug accross all pages and websistes.\"\"\"\n\n    if hasattr(form, 'instance') and form.instance.id:\n        if Content.objects.exclude(page=form.instance).filter(\n            body=slug, type=\"slug\").count():\n            raise forms.ValidationError(error_dict['another_page_error'])\n    elif Content.objects.filter(body=slug, type=\"slug\").count():\n        raise forms.ValidationError(error_dict['another_page_error'])\n    return slug"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a method to intersect sites.", "response": "def intersect_sites_method(form):\n    \"\"\"Return a method to intersect sites.\"\"\"\n    if settings.PAGE_USE_SITE_ID:\n        if settings.PAGE_HIDE_SITES:\n            site_ids = [global_settings.SITE_ID]\n        else:\n            site_ids = [int(x) for x in form.data.getlist('sites')]\n        def intersects_sites(sibling):\n            return sibling.sites.filter(id__in=site_ids).count() > 0\n    else:\n        def intersects_sites(sibling):\n            return True\n    return intersects_sites"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _has_changed(self, initial, data):\n        if data == initial:\n            return False\n        return bool(initial) != bool(data)", "response": "Check if the data has changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump_json_data(page):\n    def content_langs_ordered():\n        \"\"\"\n        Return a list of languages ordered by the page content\n        with the latest creation date in each.  This will be used\n        to maintain the state of the language_up_to_date template\n        tag when a page is restored or imported into another site.\n        \"\"\"\n        params = {'page': page}\n        if page.freeze_date:\n            params['creation_date__lte'] = page.freeze_date\n        cqs = Content.objects.filter(**params)\n        cqs = cqs.values('language').annotate(latest=Max('creation_date'))\n        return [c['language'] for c in cqs.order_by('latest')]\n    languages = content_langs_ordered()\n\n    def language_content(ctype):\n        return dict(\n            (lang, page.get_content(lang, ctype, language_fallback=False))\n            for lang in languages)\n\n    def placeholder_content():\n        \"\"\"Return content of each placeholder in each language.\"\"\"\n        out = {}\n        for p in get_placeholders(page.get_template()):\n            if p.ctype in ('title', 'slug'):\n                continue  # these were already included\n            out[p.name] = language_content(p.name)\n        return out\n\n    def isoformat(d):\n        return None if d is None else d.strftime(ISODATE_FORMAT)\n\n    def custom_email(user):\n        \"\"\"Allow a user's profile to return an email for the user.\"\"\"\n        return user.email\n\n    tags = []\n    if settings.PAGE_TAGGING:\n        tags = [tag.name for tag in page.tags.all()]\n\n    return {\n        'complete_slug': dict(\n            (lang, page.get_complete_slug(lang, hideroot=False))\n            for lang in languages),\n        'title': language_content('title'),\n        'author_email': custom_email(page.author),\n        'creation_date': isoformat(page.creation_date),\n        'publication_date': isoformat(page.publication_date),\n        'publication_end_date': isoformat(page.publication_end_date),\n        'last_modification_date': isoformat(page.last_modification_date),\n        'status': {\n            Page.PUBLISHED: 'published',\n            Page.HIDDEN: 'hidden',\n            Page.DRAFT: 'draft'}[page.status],\n        'template': page.template,\n        'sites': (\n            [site.domain for site in page.sites.all()]\n            if settings.PAGE_USE_SITE_ID else []),\n        'redirect_to_url': page.redirect_to_url,\n        'redirect_to_complete_slug': dict(\n            (lang, page.redirect_to.get_complete_slug(\n                lang, hideroot=False))\n            for lang in page.redirect_to.get_languages()\n            ) if page.redirect_to is not None else None,\n        'content': placeholder_content(),\n        'content_language_updated_order': languages,\n        'tags': tags,\n    }", "response": "Dump the page data into a python dict that can be used as part of a JSON export."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the redirect_to field of a page from a dictionary.", "response": "def update_redirect_to_from_json(page, redirect_to_complete_slugs):\n    \"\"\"\n    The second pass of create_and_update_from_json_data\n    used to update the redirect_to field.\n\n    Returns a messages list to be appended to the messages from the\n    first pass.\n    \"\"\"\n    messages = []\n    s = ''\n    for lang, s in list(redirect_to_complete_slugs.items()):\n        r = Page.objects.from_path(s, lang, exclude_drafts=False)\n        if r:\n            page.redirect_to = r\n            page.save()\n            break\n    else:\n        messages.append(_(\"Could not find page for redirect-to field\"\n            \" '%s'\") % (s,))\n    return messages"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate or update a page based on python dict d loaded from JSON data.", "response": "def create_and_update_from_json_data(d, user):\n    \"\"\"\n    Create or update page based on python dict d loaded from JSON data.\n    This applies all data except for redirect_to, which is done in a\n    second pass after all pages have been imported,\n\n    user is the User instance that will be used if the author can't\n    be found in the DB.\n\n    returns (page object, created, messages).\n\n    created is True if this was a new page or False if an existing page\n    was updated.\n\n    messages is a list of strings warnings/messages about this import\n    \"\"\"\n    page = None\n    parent = None\n    parent_required = True\n    created = False\n    messages = []\n\n    page_languages = set(lang[0] for lang in settings.PAGE_LANGUAGES)\n\n    for lang, s in list(d['complete_slug'].items()):\n        if lang not in page_languages:\n            messages.append(_(\"Language '%s' not imported\") % (lang,))\n            continue\n\n        page = Page.objects.from_path(s, lang, exclude_drafts=False)\n        if page and page.get_complete_slug(lang) == s:\n            break\n        if parent_required and parent is None:\n            if '/' in s:\n                parent = Page.objects.from_path(s.rsplit('/', 1)[0], lang,\n                    exclude_drafts=False)\n            else:\n                parent_required = False\n    else:\n        # can't find an existing match, need to create a new Page\n        page = Page(parent=parent)\n        created = True\n\n    user_model = get_user_model()\n\n    def custom_get_user_by_email(email):\n        \"\"\"\n        Simplified version\n        \"\"\"\n        return user_model.objects.get(email=email)\n\n    try:\n        page.author = custom_get_user_by_email(d['author_email'])\n    except (user_model.DoesNotExist, user_model.MultipleObjectsReturned):\n        page.author = user\n        messages.append(_(\"Original author '%s' not found\")\n            % (d['author_email'],))\n\n    page.creation_date = datetime.strptime(d['creation_date'],\n        ISODATE_FORMAT)\n    page.publication_date = datetime.strptime(d['publication_date'],\n        ISODATE_FORMAT) if d['publication_date'] else None\n    page.publication_end_date = datetime.strptime(d['publication_end_date'],\n        ISODATE_FORMAT) if d['publication_end_date'] else None\n    page.last_modification_date = datetime.strptime(\n        d['last_modification_date'], ISODATE_FORMAT)\n    page.status = {\n        'published': Page.PUBLISHED,\n        'hidden': Page.HIDDEN,\n        'draft': Page.DRAFT,\n        }[d['status']]\n    page.template = d['template']\n    page.redirect_to_url = d['redirect_to_url']\n\n    page.save()\n\n    # Add tags\n    if settings.PAGE_TAGGING:\n        from taggit.models import Tag\n        tags = d.get('tags', [])\n        page.tags.clear()\n        if tags:\n            for tag in tags:\n                Tag.objects.get_or_create(name=tag)\n                page.tags.add(tag)\n            page.save()\n\n    if settings.PAGE_USE_SITE_ID:\n        if d['sites']:\n            for site in d['sites']:\n                try:\n                    page.sites.add(Site.objects.get(domain=site))\n                except Site.DoesNotExist:\n                    messages.append(_(\"Could not add site '%s' to page\")\n                        % (site,))\n        if not settings.PAGE_HIDE_SITES and not page.sites.count():\n            # need at least one site\n            page.sites.add(Site.objects.get(pk=global_settings.SITE_ID))\n\n    def create_content(lang, ctype, body):\n        Content.objects.create_content_if_changed(page, lang, ctype, body)\n\n    for lang in d['content_language_updated_order']:\n        if lang not in page_languages:\n            continue\n        create_content(lang, 'slug',\n            d['complete_slug'][lang].rsplit('/', 1)[-1])\n        create_content(lang, 'title', d['title'][lang])\n        for ctype, langs_bodies in list(d['content'].items()):\n            create_content(lang, ctype, langs_bodies[lang])\n\n    return page, created, messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_placeholder(parser, token):\n    params = {}\n\n    bits = token.split_contents()\n    count = len(bits)\n    error_string = '%r tag requires at least one argument' % bits[0]\n    if count <= 1:\n        raise TemplateSyntaxError(error_string)\n    try:\n        name = unescape_string_literal(bits[1])\n    except ValueError:\n        name = bits[1]\n    remaining = bits[2:]\n    simple_options = ['parsed', 'inherited', 'untranslated', 'shared', 'block']\n    param_options = ['as', 'on', 'with', 'section']\n    all_options = simple_options + param_options\n    while remaining:\n        bit = remaining[0]\n        if bit not in all_options:\n            raise TemplateSyntaxError(\n                \"%r is not an correct option for a placeholder\" % bit)\n        if bit in param_options:\n            if len(remaining) < 2:\n                raise TemplateSyntaxError(\n                    \"Placeholder option '%s' need a parameter\" % bit)\n            if bit == 'as':\n                params['as_varname'] = remaining[1]\n            if bit == 'with':\n                params['widget'] = remaining[1]\n            if bit == 'on':\n                params['page'] = remaining[1]\n            if bit == 'section':\n                params['section'] = unescape_string_literal(remaining[1])\n            remaining = remaining[2:]\n        elif bit == 'parsed':\n            params['parsed'] = True\n            remaining = remaining[1:]\n        elif bit == 'inherited':\n            params['inherited'] = True\n            remaining = remaining[1:]\n        elif bit == 'untranslated':\n            params['untranslated'] = True\n            remaining = remaining[1:]\n        elif bit == 'shared':\n            params['shared'] = True\n            remaining = remaining[1:]\n        elif bit == 'block':\n            remaining = remaining[1:]\n            nodelist = parser.parse(('endplaceholder',))\n            parser.delete_first_token()\n            params['nodelist'] = nodelist\n\n    return name, params", "response": "Parse the PlaceholderNode parameters.\n    Return a tuple with the name and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a stable filename using the original filename of the type.", "response": "def get_filename(page, content_type, data):\n    \"\"\"\n    Generate a stable filename using the original filename of the type.\n\n\n    \"\"\"\n    avoid_collision = uuid.uuid4().hex[:8]\n\n    name_parts = data.name.split('.')\n    if len(name_parts) > 1:\n        name = slugify('.'.join(name_parts[:-1]), allow_unicode=True)\n        ext = slugify(name_parts[-1])\n        name = name + '.' + ext\n    else:\n        name = slugify(data.name)\n    filename = os.path.join(\n        settings.PAGE_UPLOAD_ROOT,\n        'page_' + str(page.id),\n        content_type + '-' + avoid_collision + '-' + name\n    )\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_widget(self, page, language, fallback=Textarea):\n        if isinstance(self.widget, str):\n            widget = get_widget(self.widget)\n        else:\n            widget = self.widget\n        try:\n            return widget(page=page, language=language)\n        except:\n            pass\n        return widget()", "response": "Given the name of a placeholder return a Widget subclass."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_extra_data(self, data):\n        result = {}\n        for key in list(data.keys()):\n            if key.startswith(self.ctype + '-'):\n                new_key = key.replace(self.ctype + '-', '')\n                result[new_key] = data[key]\n        return result", "response": "Get eventual extra data for this placeholder from the\n            admin form. This method is called when the Page is\n        saved in the admin form and passed to the placeholder save\n        method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_field(self, page, language, initial=None):\n        if self.parsed:\n            help_text = _('Note: This field is evaluated as template code.')\n        else:\n            help_text = ''\n        widget = self.get_widget(page, language)\n        return self.field(\n            widget=widget, initial=initial,\n            help_text=help_text, required=False)", "response": "Returns the field that will be shown within the admin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, context):\n        content = self.get_render_content(context)\n        request = context.get('request')\n        render_edit_tag = False\n        if request and request.user.is_staff and request.COOKIES.get('enable_edit_mode'):\n            render_edit_tag = True\n\n        if not content:\n            if not render_edit_tag:\n                return ''\n            return self.edit_tag()\n\n        if self.parsed:\n            content = self.render_parsed(context, content)\n\n        if self.as_varname is None:\n            if not render_edit_tag:\n                return content\n            return content + self.edit_tag()\n        context[self.as_varname] = content\n        return ''", "response": "Output the content of the PlaceholdeNode as a template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the import of a page in JSON format.", "response": "def handle(self, user, **options):\n        \"\"\"\n        Import pages in JSON format.  When creating a page and the original\n        author does not exist, use user as the new author.  User may be\n        specified by username, id or email address.\n        \"\"\"\n        monkeypatch_remove_pages_site_restrictions()\n        user_model = get_user_model()\n        for match in ('username', 'pk', 'email'):\n            try:\n                u = user_model.objects.get(**{match: user})\n                break\n            except (user_model.DoesNotExist, ValueError):\n                continue\n        else:\n            raise CommandError(_(\"User with username/id/email = '%s' not found\")\n                % user)\n\n        json = sys.stdin.read()\n        errors, pages_created = json_to_pages(json, u)\n        if errors:\n            for e in errors:\n                sys.stderr.write(e + '\\n')\n            raise CommandError(_(\"Errors encountered while importing JSON\"))\n        for page, created, messages in pages_created:\n            print((_(\"%s created.\") if created else _(\"%s modified.\")) % (\n                page.get_complete_slug()))\n            for m in messages:\n                print(\" - \" + m)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare(value, should_escape):\n\n    if value is None:\n        return u''\n    type_ = type(value)\n    if type_ is not strlist:\n        if type_ is not str_class:\n            if type_ is bool:\n                value = u'true' if value else u'false'\n            else:\n                value = str_class(value)\n        if should_escape:\n            value = escape(value)\n    return value", "response": "Prepares a value to be added to the result\nWorkItem"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grow(self, thing):\n        if type(thing) == str_class:\n            self.append(thing)\n\n        # This will only ever match in Python 2 since str_class is str in\n        # Python 3.\n        elif type(thing) == str:\n            self.append(unicode(thing))  # noqa: F821 undefined name 'unicode'\n\n        else:\n            # Recursively expand to a flat list; may deserve a C accelerator at\n            # some point.\n            for element in thing:\n                self.grow(element)", "response": "Make the list longer appending for unicode extending otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_word(self, source, position):\n        boundry = re.search('{{|{|\\s|$', source[:position][::-1])\n        start_offset = boundry.end() if boundry.group(0).startswith('{') else boundry.start()\n\n        boundry = re.search('}}|}|\\s|$', source[position:])\n        end_offset = boundry.end() if boundry.group(0).startswith('}') else boundry.start()\n\n        return source[position - start_offset:position + end_offset]", "response": "Extracts the word that falls at or around a specific position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the code for the template.", "response": "def _generate_code(self, source):\n        \"\"\"\n        Common compilation code shared between precompile() and compile()\n\n        :param source:\n            The template source as a unicode string\n\n        :return:\n            A tuple of (function, source_code)\n        \"\"\"\n\n        if not isinstance(source, str_class):\n            raise PybarsError(\"Template source must be a unicode string\")\n\n        tree, (position, _) = self._handlebars(source).apply('template')\n\n        self.clean_whitespace(tree)\n\n        if debug:\n            print('\\nAST')\n            print('---')\n            print(tree)\n            print('')\n\n        if position < len(source):\n            line_num = source.count('\\n') + 1\n            beginning_of_line = source.rfind('\\n', 0, position)\n            if beginning_of_line == -1:\n                char_num = position\n            else:\n                char_num = position - beginning_of_line\n            word = self._extract_word(source, position)\n            raise PybarsError(\"Error at character %s of line %s near %s\" % (char_num, line_num, word))\n\n        # Ensure the builder is in a clean state - kinda gross\n        self._compiler.globals['builder']._reset()\n\n        output = self._compiler(tree).apply('compile')[0]\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compile(self, source, path=None):\n\n        container = self._generate_code(source)\n\n        def make_module_name(name, suffix=None):\n            output = 'pybars._templates.%s' % name\n            if suffix:\n                output += '_%s' % suffix\n            return output\n\n        if not path:\n            path = '_template'\n            generate_name = True\n        else:\n            path = path.replace('\\\\', '/')\n            path = path.replace('/', '_')\n            mod_name = make_module_name(path)\n            generate_name = mod_name in sys.modules\n\n        if generate_name:\n            mod_name = make_module_name(path, self.template_counter)\n            while mod_name in sys.modules:\n                self.template_counter += 1\n                mod_name = make_module_name(path, self.template_counter)\n\n        mod = ModuleType(mod_name)\n        filename = '%s.py' % mod_name.replace('pybars.', '').replace('.', '/')\n        exec(compile(container.full_code, filename, 'exec', dont_inherit=True), mod.__dict__)\n        sys.modules[mod_name] = mod\n        linecache.getlines(filename, mod.__dict__)\n\n        return mod.__dict__[container.name]", "response": "Compile source to a ready to execute template."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans up whitespace around block open and close tags.", "response": "def clean_whitespace(self, tree):\n        \"\"\"\n        Cleans up whitespace around block open and close tags if they are the\n        only thing on the line\n\n        :param tree:\n            The AST - will be modified in place\n        \"\"\"\n\n        pointer = 0\n        end = len(tree)\n\n        while pointer < end:\n            piece = tree[pointer]\n            if piece[0] == 'block':\n                child_tree = piece[3]\n\n                # Look at open tag, if the only other thing on the line is whitespace\n                # then delete it so we don't introduce extra newlines to the output\n                open_pre_whitespace = False\n                open_pre_content = True\n                if pointer > 1 and tree[pointer - 1][0] == 'whitespace' and (tree[pointer - 2][0] == 'newline' or tree[pointer - 2] == 'template'):\n                    open_pre_whitespace = True\n                    open_pre_content = False\n                elif pointer > 0 and (tree[pointer - 1][0] == 'newline' or tree[pointer - 1] == 'template'):\n                    open_pre_content = False\n\n                open_post_whitespace = False\n                open_post_content = True\n                child_len = len(child_tree)\n                if child_len > 2 and child_tree[1][0] == 'whitespace' and child_tree[2][0] == 'newline':\n                    open_post_whitespace = True\n                    open_post_content = False\n                elif child_len > 1 and child_tree[1][0] == 'newline':\n                    open_post_content = False\n\n                if not open_pre_content and not open_post_content:\n                    if open_pre_whitespace:\n                        tree.pop(pointer - 1)\n                        pointer -= 1\n                        end -= 1\n                    if open_post_whitespace:\n                        child_tree.pop(1)\n                    child_tree.pop(1)  # trailing newline\n\n                # Do the same thing, but for the close tag\n                close_pre_whitespace = False\n                close_pre_content = True\n                child_len = len(child_tree)\n                if child_len > 2 and child_tree[child_len - 1][0] == 'whitespace' and child_tree[child_len - 2][0] == 'newline':\n                    close_pre_whitespace = True\n                    close_pre_content = False\n                elif child_len > 1 and child_tree[child_len - 1][0] == 'newline':\n                    close_pre_content = False\n\n                close_post_whitespace = False\n                close_post_content = True\n                tree_len = len(tree)\n                if tree_len > pointer + 2 and tree[pointer + 1][0] == 'whitespace' and tree[pointer + 2][0] == 'newline':\n                    close_post_whitespace = True\n                    close_post_content = False\n                elif tree_len == pointer + 2 and tree[pointer + 1][0] == 'whitespace':\n                    close_post_whitespace = True\n                    close_post_content = False\n                elif tree_len > pointer + 1 and tree[pointer + 1][0] == 'newline':\n                    close_post_content = False\n                elif tree_len == pointer + 1:\n                    close_post_content = False\n\n                if not close_pre_content and not close_post_content:\n                    if close_pre_whitespace:\n                        child_tree.pop()\n                    child_tree.pop()  # preceeding newline\n                    if close_post_whitespace:\n                        tree.pop(pointer + 1)\n                        end -= 1\n\n                self.clean_whitespace(child_tree)\n\n            pointer += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef basic_config(logger=None):\n    '''Set up the default handler (:py:class:`ClickHandler`) and formatter\n    (:py:class:`ColorFormatter`) on the given logger.'''\n    logger = _normalize_logger(logger)\n    logger.handlers = [_default_handler]\n    logger.propagate = False\n\n    return logger", "response": "Configure the given logger to use the default handler and formatter on the given logger."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simple_verbosity_option(logger=None, *names, **kwargs):\n    '''A decorator that adds a `--verbosity, -v` option to the decorated\n    command.\n\n    Name can be configured through ``*names``. Keyword arguments are passed to\n    the underlying ``click.option`` decorator.\n    '''\n\n    if not names:\n        names = ['--verbosity', '-v']\n    if isinstance(logger, str) and logger.startswith('-'):\n        raise ValueError('Since click-log 0.2.0, the first argument must now '\n                         'be a logger.')\n\n    kwargs.setdefault('default', 'INFO')\n    kwargs.setdefault('metavar', 'LVL')\n    kwargs.setdefault('expose_value', False)\n    kwargs.setdefault('help', 'Either CRITICAL, ERROR, WARNING, INFO or DEBUG')\n    kwargs.setdefault('is_eager', True)\n\n    logger = _normalize_logger(logger)\n\n    def decorator(f):\n        def _set_level(ctx, param, value):\n            x = getattr(logging, value.upper(), None)\n            if x is None:\n                raise click.BadParameter(\n                    'Must be CRITICAL, ERROR, WARNING, INFO or DEBUG, not {}'\n                )\n            logger.setLevel(x)\n\n        return click.option(*names, callback=_set_level, **kwargs)(f)\n    return decorator", "response": "A decorator that adds a simple verbosity option to the decorated command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that can be used to chain the input string to the next element of the chain.", "response": "def formatters(*chained_formatters):\n    \"\"\"\n    Chain formatter functions.\n    :param chained_formatters:\n    :type chained_formatters:\n    :return:\n    :rtype:\n    \"\"\"\n    def formatters_chain(input_string):  # pylint:disable=missing-docstring\n        for chained_formatter in chained_formatters:\n            input_string = chained_formatter(input_string)\n        return input_string\n\n    return formatters_chain"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _default_conflict_solver(match, conflicting_match):\n    if len(conflicting_match.initiator) < len(match.initiator):\n        return conflicting_match\n    if len(match.initiator) < len(conflicting_match.initiator):\n        return match\n    return None", "response": "Default conflict solver for matches longer than longer ones"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls a function or constructor with given args and kwargs and returns the default function", "response": "def call(function, *args, **kwargs):\n    \"\"\"\n    Call a function or constructor with given args and kwargs after removing args and kwargs that doesn't match\n    function or constructor signature\n\n    :param function: Function or constructor to call\n    :type function: callable\n    :param args:\n    :type args:\n    :param kwargs:\n    :type kwargs:\n    :return: sale vakye as default function call\n    :rtype: object\n    \"\"\"\n    func = constructor_args if inspect.isclass(function) else function_args\n    call_args, call_kwargs = func(function, *args, **kwargs)\n    return function(*call_args, **call_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn tuple of args and kwargs matching the function signature", "response": "def function_args(callable_, *args, **kwargs):\n    \"\"\"\n    Return (args, kwargs) matching the function signature\n\n    :param callable: callable to inspect\n    :type callable: callable\n    :param args:\n    :type args:\n    :param kwargs:\n    :type kwargs:\n    :return: (args, kwargs) matching the function signature\n    :rtype: tuple\n    \"\"\"\n    argspec = getargspec(callable_)  # pylint:disable=deprecated-method\n    return argspec_args(argspec, False, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef constructor_args(class_, *args, **kwargs):\n    argspec = getargspec(_constructor(class_))  # pylint:disable=deprecated-method\n    return argspec_args(argspec, True, *args, **kwargs)", "response": "Returns tuple of args and kwargs for the constructor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning tuple of args and kwargs for the specified argspec", "response": "def argspec_args(argspec, constructor, *args, **kwargs):\n    \"\"\"\n    Return (args, kwargs) matching the argspec object\n\n    :param argspec: argspec to use\n    :type argspec: argspec\n    :param constructor: is it a constructor ?\n    :type constructor: bool\n    :param args:\n    :type args:\n    :param kwargs:\n    :type kwargs:\n    :return: (args, kwargs) matching the function signature\n    :rtype: tuple\n    \"\"\"\n    if argspec.varkw:\n        call_kwarg = kwargs\n    else:\n        call_kwarg = dict((k, kwargs[k]) for k in kwargs if k in argspec.args)  # Python 2.6 dict comprehension\n    if argspec.varargs:\n        call_args = args\n    else:\n        call_args = args[:len(argspec.args) - (1 if constructor else 0)]\n    return call_args, call_kwarg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that the parameter is a dict and a default value.", "response": "def ensure_dict(param, default_value, default_key=None):\n    \"\"\"\n    Retrieves a dict and a default value from given parameter.\n\n    if parameter is not a dict, it will be promoted as the default value.\n\n    :param param:\n    :type param:\n    :param default_value:\n    :type default_value:\n    :param default_key:\n    :type default_key:\n    :return:\n    :rtype:\n    \"\"\"\n    if not param:\n        param = default_value\n    if not isinstance(param, dict):\n        if param:\n            default_value = param\n        return {default_key: param}, default_value\n    return param, default_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_index(collection, predicate=None, index=None):\n    if index is None and isinstance(predicate, int):\n        index = predicate\n        predicate = None\n    if predicate:\n        collection = collection.__class__(filter(predicate, collection))\n    if index is not None:\n        try:\n            collection = collection[index]\n        except IndexError:\n            collection = None\n    return collection", "response": "Filter a collection with predicate function and index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the defaults dict to kwargs dict", "response": "def set_defaults(defaults, kwargs):\n    \"\"\"\n    Set defaults from defaults dict to kwargs dict\n    :param defaults:\n    :type defaults:\n    :param kwargs:\n    :type kwargs:\n    :return:\n    :rtype:\n    \"\"\"\n    for key, value in defaults.items():\n        if key not in kwargs and value is not None:\n            kwargs[key] = value\n        elif isinstance(value, list) and isinstance(kwargs[key], list):\n            kwargs[key] = list(value) + kwargs[key]\n        elif isinstance(value, dict) and isinstance(kwargs[key], dict):\n            set_defaults(value, kwargs[key])\n        elif key in kwargs and value is None:\n            kwargs[key] = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of properties for Introspection results.", "response": "def properties(self):\n        \"\"\"\n        Properties for Introspection results.\n        :return:\n        :rtype:\n        \"\"\"\n        properties = defaultdict(list)\n        for pattern in self.patterns:\n            for key, values in pattern.properties.items():\n                extend_safe(properties[key], values)\n        for rule in self.rules:\n            for key, values in rule.properties.items():\n                extend_safe(properties[key], values)\n        return properties"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the items that depend on items in the topological order of the items in the input dictionary.", "response": "def toposort(data):\n    \"\"\"\n    Dependencies are expressed as a dictionary whose keys are items\n    and whose values are a set of dependent items. Output is a list of\n    sets in topological order. The first set consists of items with no\n    dependences, each subsequent set consists of items that depend upon\n    items in the preceeding sets.\n    :param data:\n    :type data:\n    :return:\n    :rtype:\n    \"\"\"\n\n    # Special case empty input.\n    if len(data) == 0:\n        return\n\n    # Copy the input so as to leave it unmodified.\n    data = data.copy()\n\n    # Ignore self dependencies.\n    for k, v in data.items():\n        v.discard(k)\n    # Find all items that don't depend on anything.\n    extra_items_in_deps = reduce(set.union, data.values()) - set(data.keys())\n    # Add empty dependences where needed.\n    data.update(dict((item, set()) for item in extra_items_in_deps))\n    while True:\n        ordered = set(item for item, dep in data.items() if len(dep) == 0)\n        if not ordered:\n            break\n        yield ordered\n        data = dict((item, (dep - ordered))\n                for item, dep in data.items()\n                if item not in ordered)\n    if len(data) != 0:\n        raise CyclicDependency(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the given rule.", "response": "def execute_rule(rule, matches, context):\n    \"\"\"\n    Execute the given rule.\n    :param rule:\n    :type rule:\n    :param matches:\n    :type matches:\n    :param context:\n    :type context:\n    :return:\n    :rtype:\n    \"\"\"\n    if rule.enabled(context):\n        log(rule.log_level, \"Checking rule condition: %s\", rule)\n        when_response = rule.when(matches, context)\n        if when_response:\n            log(rule.log_level, \"Rule was triggered: %s\", when_response)\n            log(rule.log_level, \"Running rule consequence: %s %s\", rule, when_response)\n            rule.then(matches, when_response, context)\n            return when_response\n    else:\n        log(rule.log_level, \"Rule is disabled: %s\", rule)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toposort_rules(rules):\n    graph = {}\n    class_dict = {}\n    for rule in rules:\n        if rule.__class__ in class_dict:\n            raise ValueError(\"Duplicate class rules are not allowed: %s\" % rule.__class__)\n        class_dict[rule.__class__] = rule\n    for rule in rules:\n        if not is_iterable(rule.dependency) and rule.dependency:\n            rule_dependencies = [rule.dependency]\n        else:\n            rule_dependencies = rule.dependency\n        dependencies = set()\n        if rule_dependencies:\n            for dependency in rule_dependencies:\n                if inspect.isclass(dependency):\n                    dependency = class_dict.get(dependency)\n                if dependency:\n                    dependencies.add(dependency)\n        graph[rule] = dependencies\n    return toposort(graph)", "response": "Sort given rules using toposort with dependency parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, *rules):\n        for rule in rules:\n            if inspect.ismodule(rule):\n                self.load_module(rule)\n            elif inspect.isclass(rule):\n                self.load_class(rule)\n            else:\n                self.append(rule)", "response": "Load rules from a Rule module class or instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a module s internal cache.", "response": "def load_module(self, module):\n        \"\"\"\n        Load a rules module\n\n        :param module:\n        :type module:\n        :return:\n        :rtype:\n        \"\"\"\n        # pylint: disable=unused-variable\n        for name, obj in inspect.getmembers(module,\n                                            lambda member: hasattr(member, '__module__')\n                                            and member.__module__ == module.__name__\n                                            and inspect.isclass):\n            self.load_class(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_all_rules(self, matches, context):\n        ret = []\n        for priority, priority_rules in groupby(sorted(self), lambda rule: rule.priority):\n            sorted_rules = toposort_rules(list(priority_rules))  # Group by dependency graph toposort\n            for rules_group in sorted_rules:\n                rules_group = list(sorted(rules_group, key=self.index))  # Sort rules group based on initial ordering.\n                group_log_level = None\n                for rule in rules_group:\n                    if group_log_level is None or group_log_level < rule.log_level:\n                        group_log_level = rule.log_level\n                log(group_log_level, \"%s independent rule(s) at priority %s.\", len(rules_group), priority)\n                for rule in rules_group:\n                    when_response = execute_rule(rule, matches, context)\n                    if when_response is not None:\n                        ret.append((rule, when_response))\n\n        return ret", "response": "Execute all rules from this rules list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a match to the set of matches.", "response": "def _add_match(self, match):\n        \"\"\"\n        Add a match\n        :param match:\n        :type match: Match\n        \"\"\"\n        if self.__name_dict is not None:\n            if match.name:\n                _BaseMatches._base_add(self._name_dict[match.name], (match))\n        if self.__tag_dict is not None:\n            for tag in match.tags:\n                _BaseMatches._base_add(self._tag_dict[tag], match)\n        if self.__start_dict is not None:\n            _BaseMatches._base_add(self._start_dict[match.start], match)\n        if self.__end_dict is not None:\n            _BaseMatches._base_add(self._end_dict[match.end], match)\n        if self.__index_dict is not None:\n            for index in range(*match.span):\n                _BaseMatches._base_add(self._index_dict[index], match)\n        if match.end > self._max_end:\n            self._max_end = match.end"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _remove_match(self, match):\n        if self.__name_dict is not None:\n            if match.name:\n                _BaseMatches._base_remove(self._name_dict[match.name], match)\n        if self.__tag_dict is not None:\n            for tag in match.tags:\n                _BaseMatches._base_remove(self._tag_dict[tag], match)\n        if self.__start_dict is not None:\n            _BaseMatches._base_remove(self._start_dict[match.start], match)\n        if self.__end_dict is not None:\n            _BaseMatches._base_remove(self._end_dict[match.end], match)\n        if self.__index_dict is not None:\n            for index in range(*match.span):\n                _BaseMatches._base_remove(self._index_dict[index], match)\n        if match.end >= self._max_end and not self._end_dict[match.end]:\n            self._max_end = max(self._end_dict.keys())", "response": "Removes a match from the list of matches."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef previous(self, match, predicate=None, index=None):\n        current = match.start\n        while current > -1:\n            previous_matches = self.ending(current)\n            if previous_matches:\n                return filter_index(previous_matches, predicate, index)\n            current -= 1\n        return filter_index(_BaseMatches._base(), predicate, index)", "response": "Returns the nearest previous matches."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self, match, predicate=None, index=None):\n        current = match.start + 1\n        while current <= self._max_end:\n            next_matches = self.starting(current)\n            if next_matches:\n                return filter_index(next_matches, predicate, index)\n            current += 1\n        return filter_index(_BaseMatches._base(), predicate, index)", "response": "Returns the nearest next matches."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a set of Match objects that have the given name.", "response": "def named(self, name, predicate=None, index=None):\n        \"\"\"\n        Retrieves a set of Match objects that have the given name.\n        :param name:\n        :type name: str\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index: int\n        :return: set of matches\n        :rtype: set[Match]\n        \"\"\"\n        return filter_index(_BaseMatches._base(self._name_dict[name]), predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of Match objects that have the given tag defined.", "response": "def tagged(self, tag, predicate=None, index=None):\n        \"\"\"\n        Retrieves a set of Match objects that have the given tag defined.\n        :param tag:\n        :type tag: str\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index: int\n        :return: set of matches\n        :rtype: set[Match]\n        \"\"\"\n        return filter_index(_BaseMatches._base(self._tag_dict[tag]), predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a set of Match objects that start at given index.", "response": "def starting(self, start, predicate=None, index=None):\n        \"\"\"\n        Retrieves a set of Match objects that starts at given index.\n        :param start: the starting index\n        :type start: int\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index: int\n        :return: set of matches\n        :rtype: set[Match]\n        \"\"\"\n        return filter_index(_BaseMatches._base(self._start_dict[start]), predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a set of Match objects that ends at given index.", "response": "def ending(self, end, predicate=None, index=None):\n        \"\"\"\n        Retrieves a set of Match objects that ends at given index.\n        :param end: the ending index\n        :type end: int\n        :param predicate:\n        :type predicate:\n        :return: set of matches\n        :rtype: set[Match]\n        \"\"\"\n        return filter_index(_BaseMatches._base(self._end_dict[end]), predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of Match objects that are available in given range.", "response": "def range(self, start=0, end=None, predicate=None, index=None):\n        \"\"\"\n        Retrieves a set of Match objects that are available in given range, sorted from start to end.\n        :param start: the starting index\n        :type start: int\n        :param end: the ending index\n        :type end: int\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index: int\n        :return: set of matches\n        :rtype: set[Match]\n        \"\"\"\n        if end is None:\n            end = self.max_end\n        else:\n            end = min(self.max_end, end)\n        ret = _BaseMatches._base()\n        for match in sorted(self):\n            if match.start < end and match.end > start:\n                ret.append(match)\n        return filter_index(ret, predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of chained matches before the given position.", "response": "def chain_before(self, position, seps, start=0, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of chained matches, before position, matching predicate and separated by characters from seps\n        only.\n        :param position:\n        :type position:\n        :param seps:\n        :type seps:\n        :param start:\n        :type start:\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index:\n        :return:\n        :rtype:\n        \"\"\"\n        if hasattr(position, 'start'):\n            position = position.start\n\n        chain = _BaseMatches._base()\n        position = min(self.max_end, position)\n\n        for i in reversed(range(start, position)):\n            index_matches = self.at_index(i)\n            filtered_matches = [index_match for index_match in index_matches if not predicate or predicate(index_match)]\n            if filtered_matches:\n                for chain_match in filtered_matches:\n                    if chain_match not in chain:\n                        chain.append(chain_match)\n            elif self.input_string[i] not in seps:\n                break\n\n        return filter_index(chain, predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max_end(self):\n        return max(len(self.input_string), self._max_end) if self.input_string else self._max_end", "response": "Retrieves the maximum index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the start of the hole index from position.", "response": "def _hole_start(self, position, ignore=None):\n        \"\"\"\n        Retrieves the start of hole index from position.\n        :param position:\n        :type position:\n        :param ignore:\n        :type ignore:\n        :return:\n        :rtype:\n        \"\"\"\n        for lindex in reversed(range(0, position)):\n            for starting in self.starting(lindex):\n                if not ignore or not ignore(starting):\n                    return lindex\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _hole_end(self, position, ignore=None):\n        for rindex in range(position, self.max_end):\n            for starting in self.starting(rindex):\n                if not ignore or not ignore(starting):\n                    return rindex\n        return self.max_end", "response": "Returns the end of the hole index from position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef holes(self, start=0, end=None, formatter=None, ignore=None, seps=None, predicate=None,\n              index=None):  # pylint: disable=too-many-branches,too-many-locals\n        \"\"\"\n        Retrieves a set of Match objects that are not defined in given range.\n        :param start:\n        :type start:\n        :param end:\n        :type end:\n        :param formatter:\n        :type formatter:\n        :param ignore:\n        :type ignore:\n        :param seps:\n        :type seps:\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index:\n        :return:\n        :rtype:\n        \"\"\"\n        assert self.input_string if seps else True, \"input_string must be defined when using seps parameter\"\n        if end is None:\n            end = self.max_end\n        else:\n            end = min(self.max_end, end)\n        ret = _BaseMatches._base()\n        hole = False\n        rindex = start\n\n        loop_start = self._hole_start(start, ignore)\n\n        for rindex in range(loop_start, end):\n            current = []\n            for at_index in self.at_index(rindex):\n                if not ignore or not ignore(at_index):\n                    current.append(at_index)\n\n            if seps and hole and self.input_string and self.input_string[rindex] in seps:\n                hole = False\n                ret[-1].end = rindex\n            else:\n                if not current and not hole:\n                    # Open a new hole match\n                    hole = True\n                    ret.append(Match(max(rindex, start), None, input_string=self.input_string, formatter=formatter))\n                elif current and hole:\n                    # Close current hole match\n                    hole = False\n                    ret[-1].end = rindex\n\n        if ret and hole:\n            # go the the next starting element ...\n            ret[-1].end = min(self._hole_end(rindex, ignore), end)\n        return filter_index(ret, predicate, index)", "response": "Returns a set of Match objects that are not defined in given range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of Match objects that conflict with given match.", "response": "def conflicting(self, match, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of ``Match`` objects that conflicts with given match.\n        :param match:\n        :type match:\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index:\n        :return:\n        :rtype:\n        \"\"\"\n        ret = _BaseMatches._base()\n\n        for i in range(*match.span):\n            for at_match in self.at_index(i):\n                if at_match not in ret:\n                    ret.append(at_match)\n\n        ret.remove(match)\n\n        return filter_index(ret, predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef at_match(self, match, predicate=None, index=None):\n        return self.at_span(match.span, predicate, index)", "response": "Retrieves a list of matches from given match."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef at_span(self, span, predicate=None, index=None):\n        starting = self._index_dict[span[0]]\n        ending = self._index_dict[span[1] - 1]\n\n        merged = list(starting)\n        for marker in ending:\n            if marker not in merged:\n                merged.append(marker)\n\n        return filter_index(merged, predicate, index)", "response": "Returns a list of matches from given start and end tuple."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a list of matches from given position", "response": "def at_index(self, pos, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of matches from given position\n        \"\"\"\n        return filter_index(self._index_dict[pos], predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self, details=False, first_value=False, enforce_list=False):\n        ret = MatchesDict()\n        for match in sorted(self):\n            value = match if details else match.value\n            ret.matches[match.name].append(match)\n            if not enforce_list and value not in ret.values_list[match.name]:\n                ret.values_list[match.name].append(value)\n            if match.name in ret.keys():\n                if not first_value:\n                    if not isinstance(ret[match.name], list):\n                        if ret[match.name] == value:\n                            continue\n                        ret[match.name] = [ret[match.name]]\n                    else:\n                        if value in ret[match.name]:\n                            continue\n                    ret[match.name].append(value)\n            else:\n                if enforce_list and not isinstance(value, list):\n                    ret[match.name] = [value]\n                else:\n                    ret[match.name] = value\n        return ret", "response": "Converts matches to a dict object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef children(self):\n        if self._children is None:\n            self._children = Matches(None, self.input_string)\n        return self._children", "response": "Get the children of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef value(self):\n        if self._value:\n            return self._value\n        if self.formatter:\n            return self.formatter(self.raw)\n        return self.raw", "response": "Get the value of the match using formatter if defined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all names of children", "response": "def names(self):\n        \"\"\"\n        Get all names of children\n        :return:\n        :rtype:\n        \"\"\"\n        if not self.children:\n            return set([self.name])\n        ret = set()\n        for child in self.children:\n            for name in child.names:\n                ret.add(name)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the raw value of the match without using hardcoded value nor formatter.", "response": "def raw(self):\n        \"\"\"\n        Get the raw value of the match, without using hardcoded value nor formatter.\n        :return:\n        :rtype:\n        \"\"\"\n        if self.input_string:\n            return self.input_string[self.raw_start:self.raw_end]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initiator(self):\n        match = self\n        while match.parent:\n            match = match.parent\n        return match", "response": "Retrieve the initiator parent of a match\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncrop the match with given list of crops or spans tuples", "response": "def crop(self, crops, predicate=None, index=None):\n        \"\"\"\n        crop the match with given Match objects or spans tuples\n        :param crops:\n        :type crops: list or object\n        :return: a list of Match objects\n        :rtype: list[Match]\n        \"\"\"\n        if not is_iterable(crops) or len(crops) == 2 and isinstance(crops[0], int):\n            crops = [crops]\n        initial = copy.deepcopy(self)\n        ret = [initial]\n        for crop in crops:\n            if hasattr(crop, 'span'):\n                start, end = crop.span\n            else:\n                start, end = crop\n            for current in list(ret):\n                if start <= current.start and end >= current.end:\n                    # self is included in crop, remove current ...\n                    ret.remove(current)\n                elif start >= current.start and end <= current.end:\n                    # crop is included in self, split current ...\n                    right = copy.deepcopy(current)\n                    current.end = start\n                    if not current:\n                        ret.remove(current)\n                    right.start = end\n                    if right:\n                        ret.append(right)\n                elif current.end >= end > current.start:\n                    current.start = end\n                elif current.start <= start < current.end:\n                    current.end = start\n        return filter_index(ret, predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits this match in multiple matches using given separators.", "response": "def split(self, seps, predicate=None, index=None):\n        \"\"\"\n        Split this match in multiple matches using given separators.\n        :param seps:\n        :type seps: string containing separator characters\n        :return: list of new Match objects\n        :rtype: list\n        \"\"\"\n        split_match = copy.deepcopy(self)\n        current_match = split_match\n        ret = []\n\n        for i in range(0, len(self.raw)):\n            if self.raw[i] in seps:\n                if not split_match:\n                    split_match = copy.deepcopy(current_match)\n                    current_match.end = self.start + i\n\n            else:\n                if split_match:\n                    split_match.start = self.start + i\n                    current_match = split_match\n                    ret.append(split_match)\n                    split_match = None\n\n        return filter_index(ret, predicate, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_match_kwargs(kwargs, children=False):\n    kwargs = kwargs.copy()\n    for key in ('pattern', 'start', 'end', 'parent', 'formatter', 'value'):\n        if key in kwargs:\n            del kwargs[key]\n    if children:\n        for key in ('name',):\n            if key in kwargs:\n                del kwargs[key]\n    return kwargs", "response": "Filters out kwargs for Match construction\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chars_before(chars, match):\n    if match.start <= 0:\n        return True\n    return match.input_string[match.start - 1] in chars", "response": "Validate the match if the left character is in a given sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chars_after(chars, match):\n    if match.end >= len(match.input_string):\n        return True\n    return match.input_string[match.end] in chars", "response": "Validate the match if the right character is in a given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a validator chain from several validator functions.", "response": "def validators(*chained_validators):\n    \"\"\"\n    Creates a validator chain from several validator functions.\n\n    :param chained_validators:\n    :type chained_validators:\n    :return:\n    :rtype:\n    \"\"\"\n    def validator_chain(match):  # pylint:disable=missing-docstring\n        for chained_validator in chained_validators:\n            if not chained_validator(match):\n                return False\n        return True\n    return validator_chain"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds regex pattern to the set", "response": "def regex(self, *pattern, **kwargs):\n        \"\"\"\n        Add re pattern\n\n        :param pattern:\n        :type pattern:\n        :return: self\n        :rtype: Rebulk\n        \"\"\"\n        self.pattern(self.build_re(*pattern, **kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_re(self, *pattern, **kwargs):\n        set_defaults(self._regex_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        return RePattern(*pattern, **kwargs)", "response": "Builds a new regular expression pattern\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef string(self, *pattern, **kwargs):\n        self.pattern(self.build_string(*pattern, **kwargs))\n        return self", "response": "Add string pattern to the set"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_string(self, *pattern, **kwargs):\n        set_defaults(self._string_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        return StringPattern(*pattern, **kwargs)", "response": "Builds a new string pattern"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds functional pattern to the set.", "response": "def functional(self, *pattern, **kwargs):\n        \"\"\"\n        Add functional pattern\n\n        :param pattern:\n        :type pattern:\n        :return: self\n        :rtype: Rebulk\n        \"\"\"\n        self.pattern(self.build_functional(*pattern, **kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a new functional pattern", "response": "def build_functional(self, *pattern, **kwargs):\n        \"\"\"\n        Builds a new functional pattern\n\n        :param pattern:\n        :type pattern:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n        set_defaults(self._functional_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        return FunctionalPattern(*pattern, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chain(self, **kwargs):\n        chain = self.build_chain(**kwargs)\n        self._patterns.append(chain)\n        return chain", "response": "Add patterns chain to the list of patterns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_chain(self, **kwargs):\n        set_defaults(self._chain_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        return Chain(self, **kwargs)", "response": "Builds a new chain with the given patterns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for all matches with current configuration against input_string", "response": "def matches(self, string, context=None):\n        \"\"\"\n        Search for all matches with current configuration against input_string\n        :param string: string to search into\n        :type string: str\n        :param context: context to use\n        :type context: dict\n        :return: A custom list of matches\n        :rtype: Matches\n        \"\"\"\n        matches = Matches(input_string=string)\n        if context is None:\n            context = {}\n\n        self._matches_patterns(matches, context)\n\n        self._execute_rules(matches, context)\n\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the effective rules for this rebulk object and its children.", "response": "def effective_rules(self, context=None):\n        \"\"\"\n        Get effective rules for this rebulk object and its children.\n        :param context:\n        :type context:\n        :return:\n        :rtype:\n        \"\"\"\n        rules = Rules()\n        rules.extend(self._rules)\n        for rebulk in self._rebulks:\n            if not rebulk.disabled(context):\n                extend_safe(rules, rebulk._rules)\n        return rules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting rules for this rebulk and children.", "response": "def _execute_rules(self, matches, context):\n        \"\"\"\n        Execute rules for this rebulk and children.\n        :param matches:\n        :type matches:\n        :param context:\n        :type context:\n        :return:\n        :rtype:\n        \"\"\"\n        if not self.disabled(context):\n            rules = self.effective_rules(context)\n            rules.execute_all_rules(matches, context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the effective patterns for this rebulk object and its children.", "response": "def effective_patterns(self, context=None):\n        \"\"\"\n        Get effective patterns for this rebulk object and its children.\n        :param context:\n        :type context:\n        :return:\n        :rtype:\n        \"\"\"\n        patterns = list(self._patterns)\n        for rebulk in self._rebulks:\n            if not rebulk.disabled(context):\n                extend_safe(patterns, rebulk._patterns)\n        return patterns"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for all matches with current paterns agains input_string", "response": "def _matches_patterns(self, matches, context):\n        \"\"\"\n        Search for all matches with current paterns agains input_string\n        :param matches: matches list\n        :type matches: Matches\n        :param context: context to use\n        :type context: dict\n        :return:\n        :rtype:\n        \"\"\"\n        if not self.disabled(context):\n            patterns = self.effective_patterns(context)\n            for pattern in patterns:\n                if not pattern.disabled(context):\n                    pattern_matches = pattern.matches(matches.input_string, context)\n                    if pattern_matches:\n                        log(pattern.log_level, \"Pattern has %s match(es). (%s)\", len(pattern_matches), pattern)\n                    else:\n                        pass\n                        # log(pattern.log_level, \"Pattern doesn't match. (%s)\" % (pattern,))\n                    for match in pattern_matches:\n                        if match.marker:\n                            log(pattern.log_level, \"Marker found. (%s)\", match)\n                            matches.markers.append(match)\n                        else:\n                            log(pattern.log_level, \"Match found. (%s)\", match)\n                            matches.append(match)\n                else:\n                    log(pattern.log_level, \"Pattern is disabled. (%s)\", pattern)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding patterns chain using configuration from this chain", "response": "def chain(self):\n        \"\"\"\n        Add patterns chain, using configuration from this chain\n\n        :return:\n        :rtype:\n        \"\"\"\n        # pylint: disable=protected-access\n        chain = self.rebulk.chain(**self._kwargs)\n        chain._defaults = dict(self._defaults)\n        chain._regex_defaults = dict(self._regex_defaults)\n        chain._functional_defaults = dict(self._functional_defaults)\n        chain._string_defaults = dict(self._string_defaults)\n        return chain"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef regex(self, *pattern, **kwargs):\n        set_defaults(self._kwargs, kwargs)\n        set_defaults(self._regex_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        pattern = self.rebulk.build_re(*pattern, **kwargs)\n        part = ChainPart(self, pattern)\n        self.parts.append(part)\n        return part", "response": "Add a regex pattern to the chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef functional(self, *pattern, **kwargs):\n        set_defaults(self._kwargs, kwargs)\n        set_defaults(self._functional_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        pattern = self.rebulk.build_functional(*pattern, **kwargs)\n        part = ChainPart(self, pattern)\n        self.parts.append(part)\n        return part", "response": "Add functional pattern\n\n        :param pattern:\n        :type pattern:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding string pattern :param pattern: :type pattern: :param kwargs: :type kwargs: :return: :rtype:", "response": "def string(self, *pattern, **kwargs):\n        \"\"\"\n        Add string pattern\n\n        :param pattern:\n        :type pattern:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n        set_defaults(self._kwargs, kwargs)\n        set_defaults(self._functional_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        pattern = self.rebulk.build_string(*pattern, **kwargs)\n        part = ChainPart(self, pattern)\n        self.parts.append(part)\n        return part"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _match_parent(self, match, yield_parent):\n        ret = super(Chain, self)._match_parent(match, yield_parent)\n        original_children = Matches(match.children)\n        original_end = match.end\n        while not ret and match.children:\n            last_pattern = match.children[-1].pattern\n            last_pattern_children = [child for child in match.children if child.pattern == last_pattern]\n            last_pattern_groups_iter = itertools.groupby(last_pattern_children, lambda child: child.match_index)\n            last_pattern_groups = {}\n            for index, matches in last_pattern_groups_iter:\n                last_pattern_groups[index] = list(matches)\n\n            for index in reversed(list(last_pattern_groups)):\n                last_matches = list(last_pattern_groups[index])\n                for last_match in last_matches:\n                    match.children.remove(last_match)\n                match.end = match.children[-1].end if match.children else match.start\n                ret = super(Chain, self)._match_parent(match, yield_parent)\n                if ret:\n                    return True\n        match.children = original_children\n        match.end = original_end\n        return ret", "response": "Handle a parent match\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndefines the repeater of the current chain part.", "response": "def repeater(self, value):\n        \"\"\"\n        Define the repeater of the current chain part.\n\n        :param value:\n        :type value:\n        :return:\n        :rtype:\n        \"\"\"\n        try:\n            value = int(value)\n            self.repeater_start = value\n            self.repeater_end = value\n            return self\n        except ValueError:\n            pass\n        if value == '+':\n            self.repeater_start = 1\n            self.repeater_end = None\n        if value == '*':\n            self.repeater_start = 0\n            self.repeater_end = None\n        elif value == '?':\n            self.repeater_start = 0\n            self.repeater_end = 1\n        else:\n            match = re.match(r'\\{\\s*(\\d*)\\s*,?\\s*(\\d*)\\s*\\}', value)\n            if match:\n                start = match.group(1)\n                end = match.group(2)\n                if start or end:\n                    self.repeater_start = int(start) if start else 0\n                    self.repeater_end = int(end) if end else None\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef defined_at():\n    if DEBUG:\n        frame = inspect.currentframe()\n        while frame:\n            try:\n                if frame.f_globals['__package__'] != __package__:\n                    break\n            except KeyError:  # pragma:no cover\n                # If package is missing, consider we are in. Workaround for python 3.3.\n                break\n            frame = frame.f_back\n        ret = Frame(frame.f_lineno,\n                    frame.f_globals.get('__package__'),\n                    frame.f_globals.get('__name__'),\n                    frame.f_code.co_filename)\n        del frame\n        return ret", "response": "Get definition location of a pattern or a match."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_all(string, sub, start=None, end=None, ignore_case=False, **kwargs):\n    #pylint: disable=unused-argument\n    if ignore_case:\n        sub = sub.lower()\n        string = string.lower()\n    while True:\n        start = string.find(sub, start, end)\n        if start == -1:\n            return\n        yield start\n        start += len(sub)", "response": "Find all the indices in string s where substring sub is contained in the slice s [ start end )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_first_defined(data, keys, default_value=None):\n    for key in keys:\n        if key in data:\n            return data[key]\n    return default_value", "response": "Get the first defined key in data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_iterable(obj):\n    # pylint: disable=consider-using-ternary\n    return hasattr(obj, '__iter__') and not isinstance(obj, str) or isinstance(obj, GeneratorType)", "response": "Check if an object is iterable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextending source list to target list only if elements doesn t exist in target list.", "response": "def extend_safe(target, source):\n    \"\"\"\n    Extends source list to target list only if elements doesn't exists in target list.\n    :param target:\n    :type target: list\n    :param source:\n    :type source: list\n    \"\"\"\n    for elt in source:\n        if elt not in target:\n            target.append(elt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_attachment(request, link_field=None, uri=None):\n    if link_field is None:\n        link_field = \"record_uri\"\n    if uri is None:\n        uri = record_uri(request)\n\n    # Remove file.\n    filters = [Filter(link_field, uri, core_utils.COMPARISON.EQ)]\n    storage = request.registry.storage\n    file_links, _ = storage.get_all(\"\", FILE_LINKS, filters=filters)\n    for link in file_links:\n        request.attachment.delete(link['location'])\n\n    # Remove link.\n    storage.delete_all(\"\", FILE_LINKS, filters=filters, with_deleted=False)", "response": "Delete existing file and link."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preprocess_tree(self, tree):\n        visitor = RinohTreePreprocessor(tree, self)\n        tree.walkabout(visitor)", "response": "Transform internal refuri targets in reference nodes to refids and\n        transform footnote rubrics so that they do not end up in the output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the size of the columns of the table.", "response": "def _size_columns(self, container):\n        \"\"\"Calculate the table's column sizes constrained by:\n\n        - requested (absolute, relative and automatic) column widths\n        - container width (= available width)\n        - cell contents\n\n        \"\"\"\n        def calculate_column_widths(max_cell_width):\n            \"\"\"Calculate required column widths given a maximum cell width\"\"\"\n            def cell_content_width(cell):\n                buffer = VirtualContainer(container, width=max_cell_width)\n                width, _, _ = cell.flow(buffer, None)\n                return float(width)\n\n            widths = [0] * self.body.num_columns\n            for row in chain(self.head or [], self.body):\n                for cell in (cell for cell in row if cell.colspan == 1):\n                    col = int(cell.column_index)\n                    widths[col] = max(widths[col], cell_content_width(cell))\n            for row in chain(self.head or [], self.body):\n                for cell in (cell for cell in row if cell.colspan > 1):\n                    c = int(cell.column_index)\n                    c_end = c + cell.colspan\n                    padding = cell_content_width(cell) - sum(widths[c:c_end])\n                    if padding > 0:\n                        per_column_padding = padding / cell.colspan\n                        for i in range(cell.colspan):\n                            widths[c + i] += per_column_padding\n            return widths\n\n        width = self._width(container)\n        try:\n            fixed_width = width.to_points(container.width)\n        except AttributeError:\n            fixed_width = width or FlowableWidth.AUTO\n        min_column_widths = calculate_column_widths(0)\n        max_column_widths = calculate_column_widths(float('+inf'))\n\n        # calculate relative column widths for auto-sized columns\n        auto_rel_colwidths = [sqrt(minimum * maximum) for minimum, maximum\n                              in zip(min_column_widths, max_column_widths)]\n        column_widths = self.column_widths or [None for _ in max_column_widths]\n        try:          # TODO: max() instead of min()?\n            relative_factor = min(auto_relative_width / width\n                                  for width, auto_relative_width\n                                  in zip(column_widths, auto_rel_colwidths)\n                                  if width and not isinstance(width, DimBase))\n        except ValueError:\n            relative_factor = 1\n        column_widths = [auto_relative_width * relative_factor\n                         if width is None else width\n                         for width, auto_relative_width\n                         in zip(column_widths, auto_rel_colwidths)]\n\n        # set min = max for columns with a fixed width\n        total_fixed_cols_width = 0\n        total_portions = 0\n        for i, column_width in enumerate(column_widths):\n            if isinstance(column_width, DimBase):\n                width_in_pt = column_width.to_points(container.width)\n                min_column_widths[i] = max_column_widths[i] = width_in_pt\n                total_fixed_cols_width += width_in_pt\n            else:\n                total_portions += column_width\n\n        # does the table fit within the available width without wrapping?\n        if sum(max_column_widths) <= container.width:\n            return max_column_widths\n\n        # determine table width\n        if fixed_width != FlowableWidth.AUTO:\n            table_width = fixed_width\n        elif total_portions:\n            max_factor = max(maximum / width for width, maximum,\n                             in zip(column_widths, max_column_widths)\n                             if not isinstance(width, DimBase))\n            no_wrap_rel_cols_width = total_portions * max_factor\n            table_width = min(total_fixed_cols_width + no_wrap_rel_cols_width,\n                              float(container.width))\n        else:\n            table_width = total_fixed_cols_width\n\n        # TODO: warn when a column's fixed width < min width\n\n        # determine absolute width of columns with relative widths\n        if total_portions:\n            total_relative_cols_width = table_width - total_fixed_cols_width\n            portion_width = total_relative_cols_width / total_portions\n            auto_widths = [width if isinstance(width, DimBase)\n                           else width * portion_width\n                           for width in column_widths]\n            extra = [0 if isinstance(width, DimBase)\n                     else auto_width - min_column_widths[index]\n                     for index, auto_width in enumerate(auto_widths)]\n            excess = [0 if isinstance(width, DimBase)\n                      else auto_width - max_column_widths[index]\n                      for index, auto_width in enumerate(auto_widths)]\n            excess_pos = sum(x for x in excess if x > 0)\n            extra_neg = sum(x for x in extra if x <= 0) + excess_pos\n            extra_pos = (sum(x for x, c in zip(extra, excess)\n                             if x > 0 and c <= 0))\n            if extra_pos + extra_neg < 0:\n                self.warn('Table contents are too wide to fit within the '\n                          'available width', container)\n                return [width.to_points(container.width)\n                        if isinstance(width, DimBase) else width\n                        for width in auto_widths]\n\n            def final_column_width(index, width):\n                if isinstance(width, DimBase):\n                    return width.to_points(container.width)\n                elif excess[index] > 0:\n                    return max_column_widths[index]\n                elif extra[index] <= 0:\n                    return min_column_widths[index]\n                else:\n                    return width + (extra_neg * extra[index] / extra_pos)\n\n            return [final_column_width(i, width)\n                    for i, width in enumerate(auto_widths)]\n        else:\n            return [width.to_points(container.width)\n                    for width in column_widths]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngrow row heights to cater for vertically spanned cells that do not fit in the available space.", "response": "def _vertically_size_cells(rendered_rows):\n        \"\"\"Grow row heights to cater for vertically spanned cells that do not\n        fit in the available space.\"\"\"\n        for r, rendered_row in enumerate(rendered_rows):\n            for rendered_cell in rendered_row:\n                if rendered_cell.rowspan > 1:\n                    row_height = sum(row.height for row in\n                                     rendered_rows[r:r + rendered_cell.rowspan])\n                    extra_height_needed = rendered_cell.height - row_height\n                    if extra_height_needed > 0:\n                        padding = extra_height_needed / rendered_cell.rowspan\n                        for i in range(r, r + rendered_cell.rowspan):\n                            rendered_rows[i].height += padding\n        return rendered_rows"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _place_rows_and_render_borders(container, rendered_rows):\n        def draw_cell_border(rendered_cell, cell_height, container):\n            cell_width = rendered_cell.width\n            background = TableCellBackground((0, 0), cell_width, cell_height,\n                                             parent=rendered_cell.cell)\n            background.render(container)\n            for position in ('top', 'right', 'bottom', 'left'):\n                border = TableCellBorder(rendered_cell, cell_height, position)\n                border.render(container)\n            return background\n\n        y_cursor = container.cursor\n        for r, rendered_row in enumerate(rendered_rows):\n            container.advance(rendered_row.height)\n            if rendered_row.index == 0:\n                container.register_styled(rendered_row.row.section)\n            container.register_styled(rendered_row.row)\n            for c, rendered_cell in enumerate(rendered_row):\n                cell_height = sum(rendered_row.height for rendered_row in\n                                  rendered_rows[r:r + rendered_cell.rowspan])\n                x_cursor = rendered_cell.x_position\n                y_pos = float(y_cursor + cell_height)\n                cell_container = VirtualContainer(container)\n                background = draw_cell_border(rendered_cell, cell_height,\n                                              cell_container)\n                cell_container.place_at(container, x_cursor, y_pos)\n                vertical_align = rendered_cell.cell.get_style('vertical_align',\n                                                              container)\n                if vertical_align == VerticalAlign.TOP:\n                    vertical_offset = 0\n                elif vertical_align == VerticalAlign.MIDDLE:\n                    vertical_offset = (cell_height - rendered_cell.height) / 2\n                elif vertical_align == VerticalAlign.BOTTOM:\n                    vertical_offset = (cell_height - rendered_cell.height)\n                y_offset = float(y_cursor + vertical_offset)\n                rendered_cell.container.place_at(container, x_cursor, y_offset)\n                container.register_styled(background)\n            y_cursor += rendered_row.height", "response": "Place the rendered cells onto the page canvas and draw borders around them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rowspanned_columns(self):\n        spanned_columns = {}\n        current_row_index = self._index\n        current_row_cols = sum(cell.colspan for cell in self)\n        prev_rows = iter(reversed(self.section[:current_row_index]))\n        while current_row_cols < self.section.num_columns:\n            row = next(prev_rows)\n            min_rowspan = current_row_index - int(row._index)\n            if row.maximum_rowspan > min_rowspan:\n                for cell in (c for c in row if c.rowspan > min_rowspan):\n                    col_index = int(cell.column_index)\n                    spanned_columns[col_index] = cell.colspan\n                    current_row_cols += cell.colspan\n        return spanned_columns", "response": "Return a dictionary mapping column indices to the number of columns\n        spanned."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_number(number, format):\n    if format == NumberFormat.NUMBER:\n        return str(number)\n    elif format == NumberFormat.LOWERCASE_CHARACTER:\n        string = ''\n        while number > 0:\n            number, ordinal = divmod(number, 26)\n            if ordinal == 0:\n                ordinal = 26\n                number -= 1\n            string = chr(ord('a') - 1 + ordinal) + string\n        return string\n    elif format == NumberFormat.UPPERCASE_CHARACTER:\n        return format_number(number, 'lowercase character').upper()\n    elif format == NumberFormat.LOWERCASE_ROMAN:\n        return romanize(number).lower()\n    elif format == NumberFormat.UPPERCASE_ROMAN:\n        return romanize(number)\n    elif format == NumberFormat.SYMBOL:\n        return symbolize(number)\n    elif format == NumberFormat.NONE:\n        return ''\n    else:\n        raise ValueError(\"Unknown number format '{}'\".format(format))", "response": "Format a number according to the given format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a number to a Roman numeral.", "response": "def romanize(number):\n    \"\"\"Convert `number` to a Roman numeral.\"\"\"\n    roman = []\n    for numeral, value in NUMERALS:\n        times, number = divmod(number, value)\n        roman.append(times * numeral)\n    return ''.join(roman)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts number to a foot or endnote symbol.", "response": "def symbolize(number):\n    \"\"\"Convert `number` to a foot/endnote symbol.\"\"\"\n    repeat, index = divmod(number - 1, len(SYMBOLS))\n    return SYMBOLS[index] * (1 + repeat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the contents of this container to its canvas.", "response": "def render(self, type, rerender=False):\n        \"\"\"Render the contents of this container to its canvas.\n\n        Note that the rendered contents need to be :meth:`place`d on the parent\n        container's canvas before they become visible.\"\"\"\n        for child in self.children:\n            child.render(type, rerender)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplaces this container s canvas onto the parent container s canvas.", "response": "def place(self):\n        \"\"\"Place this container's canvas onto the parent container's canvas.\"\"\"\n        self.place_children()\n        self.canvas.append(self.parent.canvas,\n                           float(self.left), float(self.top))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef advance(self, height, ignore_overflow=False):\n        if height <= self.remaining_height:\n            self._self_cursor.grow(height)\n        elif ignore_overflow:\n            self._self_cursor.grow(float(self.remaining_height))\n        else:\n            raise ContainerOverflow(self.page.number)", "response": "Advance the cursor by height."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef advance2(self, height, ignore_overflow=False):\n        if height <= self.remaining_height:\n            self._self_cursor.grow(height)\n        elif ignore_overflow:\n            self._self_cursor.grow(float(self.remaining_height))\n        else:\n            return False\n        return True", "response": "Advance the cursor by height. Returns True on success."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nflows the flowables into the containers that have been added to this page.", "response": "def render(self, container, rerender=False):\n        \"\"\"Flow the flowables into the containers that have been added to this\n        chain.\"\"\"\n        if rerender:\n            container.clear()\n            if not self._rerendering:\n                # restore saved state on this chain's 1st container on this page\n                self._state = copy(self._fresh_page_state)\n                self._rerendering = True\n        try:\n            self.done = False\n            self.flowables.flow(container, last_descender=None,\n                                state=self._state)\n            # all flowables have been rendered\n            if container == self.last_container:\n                self._init_state()    # reset state for the next rendering loop\n            self.done = True\n        except PageBreakException as exc:\n            self._state = exc.flowable_state\n            self._fresh_page_state = copy(self._state)\n            raise\n        except EndOfContainer as e:\n            self._state = e.flowable_state\n            if container == self.last_container:\n                # save state for when ReflowRequired occurs\n                self._fresh_page_state = copy(self._state)\n        except ReflowRequired:\n            self._rerendering = False\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default value for attribute.", "response": "def _get_default(cls, attribute):\n        \"\"\"Return the default value for `attribute`.\n\n        If no default is specified in this style, get the default from the\n        nearest superclass.\n        If `attribute` is not supported, raise a :class:`KeyError`.\"\"\"\n        try:\n            for klass in cls.__mro__:\n                if attribute in klass._attributes:\n                    return klass._attributes[attribute].default_value\n        except AttributeError:\n            raise KeyError(\"No attribute '{}' in {}\".format(attribute, cls))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pygments_style(style):\n    if isinstance(style, StyleMeta):\n        return style\n    if '.' in style:  # by python package/module\n        module, name = style.rsplit('.', 1)\n        return getattr(__import__(module, None, None, ['__name__']), name)\n    else:             # by name\n        if style == 'sphinx':\n            from sphinx.pygments_styles import SphinxStyle\n            return SphinxStyle\n        elif style == 'pyramid':\n            from sphinx.pygments_styles import PyramidStyle\n            return PyramidStyle\n        elif style == 'none':\n            from sphinx.pygments_styles import NoneStyle\n            return NoneStyle\n        else:\n            return get_style_by_name(style)", "response": "Retrieve a Pygments style by name by import path or by import path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flow(self, container, last_descender, state=None, **kwargs):\n        top_to_baseline = 0\n        state = state or self.initial_state(container)\n        if state.initial:\n            space_above = self.get_style('space_above', container)\n            if not container.advance2(float(space_above)):\n                raise EndOfContainer(state)\n            top_to_baseline += float(space_above)\n        margin_left = self.get_style('margin_left', container)\n        margin_right = self.get_style('margin_right', container)\n        reference_id = self.get_id(container.document, create=False)\n        right = container.width - margin_right\n        container.register_styled(self, continued=not state.initial)\n        margin_container = InlineDownExpandingContainer('MARGIN', container,\n                                                        left=margin_left,\n                                                        right=right)\n        initial_before = state.initial\n        state, width, inner_top_to_baseline, descender = \\\n            self.flow_inner(margin_container, last_descender,\n                            state=state, **kwargs)\n        self._align(margin_container, width)\n        initial_after = state is not None and state.initial\n        top_to_baseline += inner_top_to_baseline\n\n        if self.annotation:\n            height = float(margin_container.height)\n            margin_container.canvas.annotate(self.annotation,\n                                             0, 0, width, height)\n        self.mark_page_nonempty(container)\n        if initial_before and not initial_after:\n            if reference_id:\n                self.create_destination(margin_container, True)\n\n        if state is not None:\n            raise EndOfContainer(state)\n\n        space_below = self.get_style('space_below', container)\n        container.advance2(float(space_below), ignore_overflow=True)\n        container.document.progress(self, container)\n        return margin_left + width + margin_right, top_to_baseline, descender", "response": "Flow this flowable into container and return the vertical space\n        consumed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_operator(method_name):\n        def operator(self, other):\n            \"\"\"Operator delegating to the :class:`float` method `method_name`\"\"\"\n            float_operator = getattr(float, method_name)\n            try:\n                float_other = float(other)\n            except (ValueError, TypeError):\n                return False\n            return float_operator(float(self), float_other)\n        return operator", "response": "Return an operator method that evaluates the parameters of type\n        and returns the result of the method with the name method_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms an entry point name into an identifier suitable for inclusion in a PyPI package name.", "response": "def entry_point_name_to_identifier(entry_point_name):\n    \"\"\"Transform an entry point name into an identifier suitable for inclusion\n    in a PyPI package name.\"\"\"\n    try:\n        entry_point_name.encode('ascii')\n        ascii_name = entry_point_name\n    except UnicodeEncodeError:\n        ascii_name = entry_point_name.encode('punycode').decode('ascii')\n    return ''.join(char for char in ascii_name\n                   if char in string.ascii_lowercase + string.digits)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render(self, container, descender, state, space_below=0,\n               first_line_only=False):\n        \"\"\"Typeset the paragraph\n\n        The paragraph is typeset in the given container starting below the\n        current cursor position of the container. When the end of the container\n        is reached, the rendering state is preserved to continue setting the\n        rest of the paragraph when this method is called with a new container.\n\n        Args:\n            container (Container): the container to render to\n            descender (float or None): descender height of the preceeding line\n            state (ParagraphState): the state where rendering will continue\n            first_line_only (bool): typeset only the first line\n\n        \"\"\"\n        indent_first = (float(self.get_style('indent_first', container))\n                        if state.initial else 0)\n        line_width = float(container.width)\n        line_spacing = self.get_style('line_spacing', container)\n        text_align = self.get_style('text_align', container)\n        tab_stops = self.get_style('tab_stops', container)\n        if not tab_stops:\n            tab_width = 2 * self.get_style('font_size', container)\n            tab_stops = DefaultTabStops(tab_width)\n\n        # `saved_state` is updated after successfully rendering each line, so\n        # that when `container` overflows on rendering a line, the words in that\n        # line are yielded again on the next typeset() call.\n        saved_state = copy(state)\n        prev_state = copy(state)\n        max_line_width = 0\n\n        def typeset_line(line, last_line=False):\n            \"\"\"Typeset `line` and, if no exception is raised, update the\n            paragraph's internal rendering state.\"\"\"\n            nonlocal state, saved_state, max_line_width, descender, space_below\n            max_line_width = max(max_line_width, line.cursor)\n            advance = (line.ascender(container) if descender is None\n                       else line_spacing.advance(line, descender, container))\n            descender = line.descender(container)   # descender <= 0\n            line.advance = advance\n            total_advance = advance + (space_below if last_line else 0) - descender\n            if container.remaining_height < total_advance:\n                raise EndOfContainer(saved_state)\n            assert container.advance2(advance)\n            line.typeset(container, text_align, last_line)\n            assert container.advance2(- descender)\n            state.initial = False\n            saved_state = copy(state)\n            return Line(tab_stops, line_width, container,\n                        significant_whitespace=self.significant_whitespace)\n\n        first_line = line = Line(tab_stops, line_width, container,\n                                 indent_first, self.significant_whitespace)\n        while True:\n            try:\n                word = state.next_word()\n            except StopIteration:\n                break\n            try:\n                if not line.append_word(word):\n                    for first, second in word.hyphenate(container):\n                        if line.append_word(first):\n                            state.prepend_word(second)  # prepend second part\n                            break\n                    else:\n                        state = prev_state\n                    line = typeset_line(line)\n                    if first_line_only:\n                        break\n                    continue\n            except NewLineException:\n                line.append(word.glyphs_span)\n                line = typeset_line(line, last_line=True)\n                if first_line_only:\n                    break\n            prev_state = copy(state)\n        if line:\n            typeset_line(line, last_line=True)\n\n        # Correct the horizontal text placement for auto-width paragraphs\n        if self._width(container) == FlowableWidth.AUTO:\n            if text_align == TextAlign.CENTER:\n                container.left -= float(container.width - max_line_width) / 2\n            if text_align == TextAlign.RIGHT:\n                container.left -= float(container.width - max_line_width)\n\n        return max_line_width, first_line.advance, descender", "response": "Typeset the paragraph to the given container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntypeset the line in container below its current cursor position.", "response": "def typeset(self, container, text_align, last_line=False):\n        \"\"\"Typeset the line in `container` below its current cursor position.\n        Advances the container's cursor to below the descender of this line.\n\n        `justification` and `line_spacing` are passed on from the paragraph\n        style. `last_descender` is the previous line's descender, used in the\n        vertical positioning of this line. Finally, `last_line` specifies\n        whether this is the last line of the paragraph.\n\n        Returns the line's descender size.\"\"\"\n        document = container.document\n\n        # drop spaces (and empty spans) at the end of the line\n        while len(self) > 0:\n            last_span = self[-1]\n            if last_span and last_span.ends_with_space:\n                self.cursor -= last_span.space.width\n                self.pop()\n            else:\n                break\n        else:   # abort if the line is empty\n            return\n\n        for glyph_span in self:\n            glyph_span.span.before_placing(container)\n\n        # horizontal displacement\n        left = self.indent\n\n        if self._has_tab or text_align == TextAlign.JUSTIFY and last_line:\n            text_align = 'left'\n        extra_space = self.width - self.cursor\n        if text_align == TextAlign.JUSTIFY:\n            # TODO: padding added to spaces should be prop. to font size\n            nr_spaces = sum(glyph_span.number_of_spaces for glyph_span in self)\n            if nr_spaces > 0:\n                add_to_spaces = extra_space / nr_spaces\n                for glyph_span in self:\n                    if glyph_span.number_of_spaces > 0:\n                        glyph_span.space.width += add_to_spaces\n        elif text_align == TextAlign.CENTER:\n            left += extra_space / 2.0\n        elif text_align == TextAlign.RIGHT:\n            left += extra_space\n\n        canvas = container.canvas\n        cursor = container.cursor\n        current_annotation = AnnotationState(container)\n        for span, glyph_and_widths in group_spans(self):\n            try:\n                width = canvas.show_glyphs(left, cursor, span, glyph_and_widths,\n                                           container)\n            except InlineFlowableException:\n                ascender = span.ascender(document)\n                if ascender > 0:\n                    top = cursor - ascender\n                else:\n                    inline_height = span.virtual_container.height\n                    top = cursor - span.descender(document) - inline_height\n                span.virtual_container.place_at(container, left, top)\n                width = span.width\n            current_annotation.update(span, left, width)\n            left += width\n        current_annotation.place_if_any()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_destination(flowable, container, at_top_of_container=False):\n    vertical_position = 0 if at_top_of_container else container.cursor\n    ids = flowable.get_ids(container.document)\n    destination = NamedDestination(*(str(id) for id in ids))\n    container.canvas.annotate(destination, 0, vertical_position,\n                              container.width, None)\n    container.document.register_page_reference(container.page, flowable)", "response": "Create a destination anchor in the container to direct links to the flowable to."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef source(self):\n        if self._source is not None:\n            return self._source\n        elif self.parent is not None:\n            return self.parent.source\n        else:\n            return Location(self)", "response": "The source element this document element was created from."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine number labels and register references with the document", "response": "def prepare(self, flowable_target):\n        \"\"\"Determine number labels and register references with the document\"\"\"\n        if self.get_id(flowable_target.document, create=False):\n            flowable_target.document.register_element(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef warn(self, message, container=None):\n        if self.source is not None:\n            message = '[{}] '.format(self.source.location) + message\n        if container is not None:\n            try:\n                message += ' (page {})'.format(container.page.formatted_number)\n            except AttributeError:\n                pass\n        warn(message)", "response": "Presents the warning message to the user adding information on the the\nTaxonomy location of the related element in the input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unique(iterable):\n    seen = set()\n    for item in iterable:\n        if item not in seen:\n            seen.add(item)\n            yield item", "response": "Filter out duplicate items from an iterable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef consumer(function):\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        generator = function(*args, **kwargs)\n        next(generator)\n        return generator\n    return wrapper", "response": "Decorator that makes a generator function automatically advance to its\n    first yield point when initially called ( PEP 342 )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cached(function):\n    cache_variable = '_cached_' + function.__name__\n    @wraps(function)\n    def function_wrapper(obj, *args, **kwargs):\n        # values are cached in a dict stored in the object\n        try:\n            cache = getattr(obj, cache_variable)\n        except AttributeError:\n            cache = {}\n            setattr(obj, cache_variable, cache)\n        args_kwargs = args + tuple(kwargs.values())\n        try:\n            return cache[args_kwargs]\n        except KeyError:\n            cache_value = function(obj, *args, **kwargs)\n            cache[args_kwargs] = cache_value\n            return cache_value\n    return function_wrapper", "response": "Method decorator caching a method s return values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cached_generator(function):\n    cache_variable = '_cached_' + function.__name__\n    @wraps(function)\n    def function_wrapper(obj, *args, **kwargs):\n        # values are cached in a list stored in the object\n        try:\n            for item in getattr(obj, cache_variable):\n                yield item\n        except AttributeError:\n            setattr(obj, cache_variable, [])\n            cache = getattr(obj, cache_variable)\n            for item in function(obj, *args, **kwargs):\n                cache.append(item)\n                yield item\n    return function_wrapper", "response": "Method decorator caching a generator s yielded items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if this styled text is super or subscript.", "response": "def is_script(self, container):\n        \"\"\"Returns `True` if this styled text is super/subscript.\"\"\"\n        try:\n            style = self._style(container)\n            return style.get_value('position',\n                                   container) != TextPosition.NORMAL\n        except StyleException:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnests level of super or subscript.", "response": "def script_level(self, container):\n        \"\"\"Nesting level of super/subscript.\"\"\"\n        try:\n            level = self.parent.script_level(container)\n        except AttributeError:\n            level = -1\n        return level + 1 if self.is_script(container) else level"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef height(self, container):\n        height = float(self.get_style('font_size', container))\n        script_level = self.script_level(container)\n        if script_level > -1:\n            height *= self.position_size * (5 / 6)**script_level\n        return height", "response": "Font size after super or subscript size adjustment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the font described by this single - styled text s style attributes.", "response": "def font(self, container):\n        \"\"\"The :class:`Font` described by this single-styled text's style.\n\n        If the exact font style as described by the `font_weight`,\n        `font_slant` and `font_width` style attributes is not present in the\n        `typeface`, the closest font available is returned instead, and a\n        warning is printed.\"\"\"\n        typeface = self.get_style('typeface', container)\n        weight = self.get_style('font_weight', container)\n        slant = self.get_style('font_slant', container)\n        width = self.get_style('font_width', container)\n        return typeface.get_font(weight=weight, slant=slant, width=width)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spans(self, container):\n        for child in self.children(container):\n            container.register_styled(child)\n            for span in child.spans(container):\n                yield span", "response": "Recursively yield all the : class : SingleStyledText items in this ArcGIS mixed - styled text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, item):\n        if isinstance(item, str):\n            item = SingleStyledText(item)\n        item.parent = self\n        list.append(self, item)", "response": "Append item to the end of this mixed - styled text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a DocumentTemplate object based on the given document tree and this template configuration.", "response": "def document(self, document_tree, backend=None):\n        \"\"\"Create a :class:`DocumentTemplate` object based on the given\n        document tree and this template configuration\n\n        Args:\n            document_tree (DocumentTree): tree of the document's contents\n            backend: the backend to use when rendering the document\n\n        \"\"\"\n        return self.template(document_tree, configuration=self,\n                             backend=backend)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields all templates that match the given name", "response": "def _find_templates(self, name):\n        \"\"\"Yields all :class:`Template`\\ s in the template hierarchy:\n\n        - template matching `name` in this TemplateConfiguration\n        - templates in base TemplateConfigurations (recursively)\n        - the default template defined in the DocumentTemplate class\n\n        \"\"\"\n        for template in self.configuration._find_templates_recursive(name):\n            yield template\n        yield self.get_default_template(name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef positions(self, word):\n        word = word.lower()\n        points = self.cache.get(word)\n        if points is None:\n            prepWord = '.%s.' % word\n            res = [0] * (len(prepWord) + 1)\n            for i in range(len(prepWord) - 1):\n                for j in range(i + 1, min(i + self.maxlen, len(prepWord)) + 1):\n                    p = self.patterns.get(prepWord[i:j])\n                    if p:\n                        offset, value = p\n                        s = slice(i + offset, i + offset + len(value))\n                        res[s] = map(max, value, res[s])\n\n            points = [dint(i - 1, ref=r) for i, r in enumerate(res) if r % 2]\n            self.cache[word] = points\n        return points", "response": "Returns a list of positions where the word can be hyphenated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef positions(self, word):\n        right = len(word) - self.right\n        return [i for i in self.hd.positions(word) if self.left <= i <= right]", "response": "Returns a list of positions where the word can be hyphenated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over all hyphenation possibilities the longest first.", "response": "def iterate(self, word):\n        \"\"\"\n        Iterate over all hyphenation possibilities, the longest first.\n        \"\"\"\n        for p in reversed(self.positions(word)):\n            if p.data:\n                # get the nonstandard hyphenation data\n                change, index, cut = p.data\n                if word.isupper():\n                    change = change.upper()\n                c1, c2 = change.split('=')\n                yield word[:p+index] + c1, c2 + word[p+index+cut:]\n            else:\n                yield word[:p], word[p:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap the word in the longest possible first part and the last part of the hyphenated word.", "response": "def wrap(self, word, width, hyphen='-'):\n        \"\"\"\n        Return the longest possible first part and the last part of the\n        hyphenated word. The first part has the hyphen already attached.\n        Returns None, if there is no hyphenation point before width, or\n        if the word could not be hyphenated.\n        \"\"\"\n        width -= len(hyphen)\n        for w1, w2 in self.iterate(word):\n            if len(w1) <= width:\n                return w1 + hyphen, w2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inserted(self, word, hyphen='-'):\n        l = list(word)\n        for p in reversed(self.positions(word)):\n            if p.data:\n                # get the nonstandard hyphenation data\n                change, index, cut = p.data\n                if word.isupper():\n                    change = change.upper()\n                l[p + index : p + index + cut] = change.replace('=', hyphen)\n            else:\n                l.insert(p, hyphen)\n        return ''.join(l)", "response": "Returns the word as a string with all possible hyphens inserted."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls by render to create a new page which contains more specific metadata.", "response": "def new_page(self, page_number, new_chapter, **kwargs):\n        \"\"\"Called by :meth:`render` with the :class:`Chain`s that need more\n        :class:`Container`s. This method should create a new :class:`Page` which\n        contains a container associated with `chain`.\"\"\"\n        right_template = self.document.get_page_template(self, 'right')\n        left_template = self.document.get_page_template(self, 'left')\n        page_template = right_template if page_number % 2 else left_template\n        return page_template.page(self, page_number, self.chain, new_chapter,\n                                  **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the cached page references from filename. ptc.", "response": "def _load_cache(self, filename):\n        \"\"\"Load the cached page references from `<filename>.ptc`.\"\"\"\n        try:\n            with open(filename + self.CACHE_EXTENSION, 'rb') as file:\n                prev_number_of_pages, prev_page_references = pickle.load(file)\n        except (IOError, TypeError):\n            prev_number_of_pages, prev_page_references = {}, {}\n        return prev_number_of_pages, prev_page_references"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _save_cache(self, filename, section_number_of_pages, page_references):\n        cache_path = Path(filename).with_suffix(self.CACHE_EXTENSION)\n        with cache_path.open('wb') as file:\n            cache = (section_number_of_pages, page_references)\n            pickle.dump(cache, file)", "response": "Save the current state of the page references to filename. rtc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, filename_root=None, file=None):\n        self.error = False\n        filename_root = Path(filename_root) if filename_root else None\n        if filename_root and file is None:\n            extension = self.backend_document.extension\n            filename = filename_root.with_suffix(extension)\n            file = filename.open('wb')\n        elif file and filename_root is None:\n            filename = getattr(file, 'name', None)\n        else:\n            raise ValueError(\"You need to specify either 'filename_root' or \"\n                             \"'file'.\")\n\n        def has_converged(part_page_counts):\n            \"\"\"Return `True` if the last rendering iteration converged to a\n            stable result.\n\n            Practically, this tests whether the total number of pages and page\n            references to document elements have changed since the previous\n            rendering iteration.\"\"\"\n            nonlocal prev_number_of_pages, prev_page_references\n            return (part_page_counts == prev_number_of_pages and\n                    self.page_references == prev_page_references)\n\n        fake_container = FakeContainer(self)\n        try:\n            self.document_tree.build_document(fake_container)\n            (prev_number_of_pages,\n             prev_page_references) = self._load_cache(filename_root)\n\n            self.part_page_counts = prev_number_of_pages\n            self.prepare(fake_container)\n            self.page_elements.clear()\n            self.page_references = prev_page_references.copy()\n            self.part_page_counts = self._render_pages()\n            while not has_converged(self.part_page_counts):\n                prev_number_of_pages = self.part_page_counts\n                prev_page_references = self.page_references.copy()\n                print('Not yet converged, rendering again...')\n                del self.backend_document\n                self.backend_document = self.backend.Document(self.CREATOR)\n                self.part_page_counts = self._render_pages()\n            self.create_outlines()\n            if filename:\n                self._save_cache(filename_root, self.part_page_counts,\n                                 self.page_references)\n                self.style_log.write_log(filename_root)\n                print('Writing output: {}'.format(filename))\n            self.backend_document.write(file)\n        finally:\n            if filename_root:\n                file.close()\n        return not self.error", "response": "Render the document repeatedly until the output no longer changes due\n        to cross-references that need some iterations to converge."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_outlines(self):\n        sections = parent = []\n        current_level = 1\n        stack = []\n        fake_container = FakeContainer(self)\n        for section in self._sections:\n            if not section.show_in_toc(fake_container):\n                continue\n            section_id = section.get_id(self, create=False)\n            section_number = self.get_reference(section_id, 'number')\n            section_title = self.get_reference(section_id, 'title')\n            if section.level > current_level:\n                if section.level != current_level + 1:\n                    warn(\"Your document section hierarchy is missing levels. \"\n                         \"Please report this at https://github.com/brechtm\"\n                         \"/rinohtype/pull/67\")\n                    break\n                stack.append(parent)\n                parent = current\n            elif section.level < current_level:\n                for i in range(current_level - section.level):\n                    parent = stack.pop()\n            current = []\n            item = (str(section_id), section_number, section_title, current)\n            parent.append(item)\n            current_level = section.level\n        self.backend_document.create_outlines(sections)", "response": "Create an outline in the output file that allows for easy navigation\n            of the document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _render_pages(self):\n        self.style_log = StyleLog(self.stylesheet)\n        self.floats = set()\n        self.placed_footnotes = set()\n        self._start_time = time.time()\n\n        part_page_counts = {}\n        part_page_count = PartPageCount()\n        last_number_format = None\n        for part_template in self.part_templates:\n            part = part_template.document_part(self, part_page_count.count + 1)\n            if part is None:\n                continue\n            if part_template.page_number_format != last_number_format:\n                part_page_count = PartPageCount()\n            part_page_count += part.render(part_page_count.count + 1)\n            part_page_counts[part_template.name] = part_page_count\n            last_number_format = part_template.page_number_format\n        sys.stdout.write('\\n')     # for the progress indicator\n        return part_page_counts", "response": "Render the complete document once and return the number of pages\n        rendered."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake HTML files and a HTML help project", "response": "def htmlhelp():\n    \"\"\"make HTML files and a HTML help project\"\"\"\n    build('htmlhelp', 'Now you can run HTML Help Workshop with the .hhp '\n                      'project file in {}.')\n    print('Running HTML Help Workshop...')\n    builddir = os.path.join(BUILDDIR, 'htmlhelp')\n    rc = subprocess.call(['hhc', os.path.join(builddir, 'rinohtype.hhp')])\n    if rc != 1:\n        print('Error running HTML Help Workshop. Aborting.')\n        sys.exit(1)\n    print('HTML Help Workshop finished; the CHM file is in {}.'\n          .format(builddir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes LaTeX files and run them through pdflatex", "response": "def latexpdf():\n    \"\"\"make LaTeX files and run them through pdflatex\"\"\"\n    rc = latex()\n    print('Running LaTeX files through pdflatex...')\n    builddir = os.path.join(BUILDDIR, 'latex')\n    subprocess.call(['make', '-C', builddir, 'all-pdf'])\n    print('pdflatex finished; the PDF files are in {}.'.format(builddir))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking Texinfo files and run them through makeinfo", "response": "def info():\n    \"\"\"make Texinfo files and run them through makeinfo\"\"\"\n    rc = texinfo()\n    print('Running Texinfo files through makeinfo...')\n    builddir = os.path.join(BUILDDIR, 'texinfo')\n    subprocess.call(['make', '-C', builddir, 'info'])\n    print('makeinfo finished; the Info files are in {}.'.format(builddir))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fonts(self):\n        for width in (w for w in FontWidth if w in self):\n            for slant in (s for s in FontSlant if s in self[width]):\n                for weight in (w for w in FontWeight\n                               if w in self[width][slant]):\n                    yield self[width][slant][weight]", "response": "Generator yielding all fonts of this typeface"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_font(self, weight='medium', slant='upright', width='normal'):\n        def find_closest_style(style, styles, alternatives):\n            try:\n                return style, styles[style]\n            except KeyError:\n                for option in alternatives[style]:\n                    try:\n                        return option, styles[option]\n                    except KeyError:\n                        continue\n\n        def find_closest_weight(weight, weights):\n            index = FontWeight.values.index(weight)\n            min_distance = len(FontWeight.values)\n            closest = None\n            for i, option in enumerate(FontWeight.values):\n                if option in weights and abs(index - i) < min_distance:\n                    min_distance = abs(index - i)\n                    closest = option\n            return closest, weights[closest]\n\n        available_width, slants = find_closest_style(width, self,\n                                                     FontWidth.alternatives)\n        available_slant, weights = find_closest_style(slant, slants,\n                                                      FontSlant.alternatives)\n        available_weight, font = find_closest_weight(weight, weights)\n\n        if (available_width != width or available_slant != slant or\n            available_weight != weight):\n            warn('{} does not include a {} {} {} font. Falling back to {} {} '\n                 '{}'.format(self.name, width, weight, slant, available_width,\n                             available_weight, available_slant))\n        return font", "response": "Returns the font that is closest to the given style or if no font with the given weight or slant and width is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_selector(self, name):\n\n        try:\n            return self.matcher.by_name[name]\n        except (AttributeError, KeyError):\n            if self.base is not None:\n                return self.base.get_selector(name)\n            else:\n                raise KeyError(\"No selector found for style '{}'\".format(name))", "response": "Find a selector mapped to a style in this or a base style sheet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all next characters that are listed in chars as a string", "response": "def match(self, chars):\n        \"\"\"Return all next characters that are listed in `chars` as a string\"\"\"\n        start_index = self.next_index\n        for char in self:\n            if char not in chars:\n                self.next_index -= 1\n                break\n        return self[start_index:self.next_index]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse relative import in both Python 2 and Python 3", "response": "def _rel_import(module, tgt):\n    \"\"\"Using relative import in both Python 2 and Python 3\"\"\"\n    try:\n        exec(\"from .\" + module + \" import \" + tgt, globals(), locals())\n    except SyntaxError:\n        # On Python < 2.5 relative import cause syntax error\n        exec(\"from \" + module + \" import \" + tgt, globals(), locals())\n    except (ValueError, SystemError):\n        # relative import in non-package, try absolute\n        exec(\"from \" + module + \" import \" + tgt, globals(), locals())\n    return eval(tgt)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peekiter(iterable):\n    it = iter(iterable)\n    one = next(it)\n\n    def gen():\n        \"\"\"Generator that returns first and proxy other items from source\"\"\"\n        yield one\n        while True:\n            yield next(it)\n    return (one, gen())", "response": "Return first row and also iterable with same items as original"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that these arguments in supplied are consistent.", "response": "def check_sizes(size, width, height):\n    \"\"\"\n    Check that these arguments, in supplied, are consistent.\n\n    Return a (width, height) pair.\n    \"\"\"\n    if not size:\n        return width, height\n\n    if len(size) != 2:\n        raise ValueError(\n          \"size argument should be a pair (width, height)\")\n    if width is not None and width != size[0]:\n        raise ValueError(\n          \"size[0] (%r) and width (%r) should match when both are used.\"\n          % (size[0], width))\n    if height is not None and height != size[1]:\n        raise ValueError(\n          \"size[1] (%r) and height (%r) should match when both are used.\"\n          % (size[1], height))\n    return size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_color(c, greyscale, which):\n    if c is None:\n        return c\n    if greyscale:\n        try:\n            len(c)\n        except TypeError:\n            c = (c,)\n        if len(c) != 1:\n            raise ValueError(\"%s for greyscale must be 1-tuple\" %\n                which)\n        if not isinteger(c[0]):\n            raise ValueError(\n                \"%s colour for greyscale must be integer\" % which)\n    else:\n        if not (len(c) == 3 and\n                isinteger(c[0]) and\n                isinteger(c[1]) and\n                isinteger(c[2])):\n            raise ValueError(\n                \"%s colour must be a triple of integers\" % which)\n    return c", "response": "Checks that a colour argument is the right form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting time from most popular representations to datetime", "response": "def check_time(value):\n    \"\"\"Convert time from most popular representations to datetime\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, (time.struct_time, tuple)):\n        return value\n    if isinstance(value, datetime.datetime):\n        return value.timetuple()\n    if isinstance(value, datetime.date):\n        res = datetime.datetime.utcnow()\n        res.replace(year=value.year, month=value.month, day=value.day)\n        return res.timetuple()\n    if isinstance(value, datetime.time):\n        return datetime.datetime.combine(datetime.date.today(),\n                                         value).timetuple()\n    if isinteger(value):\n        # Handle integer as timestamp\n        return time.gmtime(value)\n    if isinstance(value, basestring):\n        if value.lower() == 'now':\n            return time.gmtime()\n        # TODO: parsinng some popular strings\n    raise ValueError(\"Unsupported time representation:\" + repr(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all keys with values from source dictionary as new dictionary.", "response": "def popdict(src, keys):\n    \"\"\"\n    Extract all keys (with values) from `src` dictionary as new dictionary\n\n    values are removed from source dictionary.\n    \"\"\"\n    new = {}\n    for key in keys:\n        if key in src:\n            new[key] = src.pop(key)\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef try_greyscale(pixels, alpha=False, dirty_alpha=True):\n    planes = 3 + bool(alpha)\n    res = list()\n    apix = list()\n    for row in pixels:\n        green = row[1::planes]\n        if alpha:\n            apix.append(row[4:planes])\n        if (green != row[0::planes] or green != row[2::planes]):\n            return False\n        else:\n            res.append(green)\n    if alpha:\n        return MergedPlanes(res, 1, apix, 1)\n    else:\n        return res", "response": "Try to convert flatboxed RGB pixels to greyscale."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting out the PNG file with the given chunks.", "response": "def write_chunks(out, chunks):\n    \"\"\"Create a PNG file by writing out the chunks.\"\"\"\n    out.write(png_signature)\n    for chunk in chunks:\n        write_chunk(out, *chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interleave_planes(ipixels, apixels, ipsize, apsize):\n    warnings.warn('please use `Merge Planes` class instead',\n                  DeprecationWarning)\n    newi = MergedPlanes([ipixels], ipsize, [apixels], apsize)\n    return next(newi)", "response": "Interleave planes from one set of pixels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining best filter by sum of all row values", "response": "def adapt_sum(line, cfg, filter_obj):\n    \"\"\"Determine best filter by sum of all row values\"\"\"\n    lines = filter_obj.filter_all(line)\n    res_s = [sum(it) for it in lines]\n    r = res_s.index(min(res_s))\n    return lines[r]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adapt_entropy(line, cfg, filter_obj):\n    lines = filter_obj.filter_all(line)\n    res_c = [len(set(it)) for it in lines]\n    r = res_c.index(min(res_c))\n    return lines[r]", "response": "Determine the best filter by dispersion of row values"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse PIL - style mode and return tuple", "response": "def parse_mode(mode, default_bitdepth=None):\n    \"\"\"Parse PIL-style mode and return tuple (grayscale, alpha, bitdeph)\"\"\"\n    # few special cases\n    if mode == 'P':\n        # Don't know what is pallette\n        raise Error('Unknown colour mode:' + mode)\n    elif mode == '1':\n        # Logical\n        return (True, False, 1)\n    elif mode == 'I':\n        # Integer\n        return (True, False, 16)\n    # here we go\n    if mode.startswith('L'):\n        grayscale = True\n        mode = mode[1:]\n    elif mode.startswith('RGB'):\n        grayscale = False\n        mode = mode[3:]\n    else:\n        raise Error('Unknown colour mode:' + mode)\n\n    if mode.startswith('A'):\n        alpha = True\n        mode = mode[1:]\n    else:\n        alpha = False\n\n    bitdepth = default_bitdepth\n    if mode.startswith(';'):\n        mode = mode[1:]\n    if mode:\n        try:\n            bitdepth = int(mode)\n        except (TypeError, ValueError):\n            raise Error('Unsupported bitdepth mode:' + mode)\n    return (grayscale, alpha, bitdepth)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a PNG image object from a 2 - dimensional array.", "response": "def from_array(a, mode=None, info=None):\n    \"\"\"\n    Create a PNG :class:`Image` object from a 2- or 3-dimensional array.\n\n    One application of this function is easy PIL-style saving:\n    ``png.from_array(pixels, 'L').save('foo.png')``.\n\n    .. note :\n\n      The use of the term *3-dimensional* is for marketing purposes\n      only.  It doesn't actually work.  Please bear with us.  Meanwhile\n      enjoy the complimentary snacks (on request) and please use a\n      2-dimensional array.\n\n    Unless they are specified using the *info* parameter, the PNG's\n    height and width are taken from the array size.  For a 3 dimensional\n    array the first axis is the height; the second axis is the width;\n    and the third axis is the channel number.  Thus an RGB image that is\n    16 pixels high and 8 wide will use an array that is 16x8x3.  For 2\n    dimensional arrays the first axis is the height, but the second axis\n    is ``width*channels``, so an RGB image that is 16 pixels high and 8\n    wide will use a 2-dimensional array that is 16x24 (each row will be\n    8*3 = 24 sample values).\n\n    *mode* is a string that specifies the image colour format in a\n    PIL-style mode.  It can be:\n\n    ``'L'``\n      greyscale (1 channel)\n    ``'LA'``\n      greyscale with alpha (2 channel)\n    ``'RGB'``\n      colour image (3 channel)\n    ``'RGBA'``\n      colour image with alpha (4 channel)\n\n    The mode string can also specify the bit depth (overriding how this\n    function normally derives the bit depth, see below).  Appending\n    ``';16'`` to the mode will cause the PNG to be 16 bits per channel;\n    any decimal from 1 to 16 can be used to specify the bit depth.\n\n    When a 2-dimensional array is used *mode* determines how many\n    channels the image has, and so allows the width to be derived from\n    the second array dimension.\n\n    The array is expected to be a ``numpy`` array, but it can be any\n    suitable Python sequence.  For example, a list of lists can be used:\n    ``png.from_array([[0, 255, 0], [255, 0, 255]], 'L')``.  The exact\n    rules are: ``len(a)`` gives the first dimension, height;\n    ``len(a[0])`` gives the second dimension; ``len(a[0][0])`` gives the\n    third dimension, unless an exception is raised in which case a\n    2-dimensional array is assumed.  It's slightly more complicated than\n    that because an iterator of rows can be used, and it all still\n    works.  Using an iterator allows data to be streamed efficiently.\n\n    The bit depth of the PNG is normally taken from the array element's\n    datatype (but if *mode* specifies a bitdepth then that is used\n    instead).  The array element's datatype is determined in a way which\n    is supposed to work both for ``numpy`` arrays and for Python\n    ``array.array`` objects.  A 1 byte datatype will give a bit depth of\n    8, a 2 byte datatype will give a bit depth of 16.  If the datatype\n    does not have an implicit size, for example it is a plain Python\n    list of lists, as above, then a default of 8 is used.\n\n    The *info* parameter is a dictionary that can be used to specify\n    metadata (in the same style as the arguments to the\n    :class:`png.Writer` class).  For this function the keys that are\n    useful are:\n\n    height\n      overrides the height derived from the array dimensions and allows\n      *a* to be an iterable.\n    width\n      overrides the width derived from the array dimensions.\n    bitdepth\n      overrides the bit depth derived from the element datatype (but\n      must match *mode* if that also specifies a bit depth).\n\n    Generally anything specified in the\n    *info* dictionary will override any implicit choices that this\n    function would otherwise make, but must match any explicit ones.\n    For example, if the *info* dictionary has a ``greyscale`` key then\n    this must be true when mode is ``'L'`` or ``'LA'`` and false when\n    mode is ``'RGB'`` or ``'RGBA'``.\n    \"\"\"\n    # typechecks *info* to some extent.\n    if info is None:\n        info = {}\n    else:\n        info = dict(info)\n\n    # Syntax check mode string.\n    grayscale, alpha, bitdepth = parse_mode(mode)\n\n    # Colour format.\n    if 'greyscale' in info:\n        if bool(info['greyscale']) != grayscale:\n            raise Error(\"info['greyscale'] should match mode.\")\n    info['greyscale'] = grayscale\n    if 'alpha' in info:\n        if bool(info['alpha']) != alpha:\n            raise Error(\"info['alpha'] should match mode.\")\n    info['alpha'] = alpha\n\n    # Get bitdepth from *mode* if possible.\n    if bitdepth:\n        if info.get('bitdepth') and bitdepth != info['bitdepth']:\n            raise Error(\"mode bitdepth (%d) should match info bitdepth (%d).\" %\n              (bitdepth, info['bitdepth']))\n        info['bitdepth'] = bitdepth\n\n    planes = (3, 1)[grayscale] + alpha\n    if 'planes' in info:\n        if info['planes'] != planes:\n            raise Error(\"info['planes'] should match mode.\")\n\n    # Dimensions.\n    if 'size' in info:\n        info['width'], info['height'] = check_sizes(info.get('size'),\n                                                    info.get('width'),\n                                                    info.get('height'))\n    if 'height' not in info:\n        try:\n            l = len(a)\n        except TypeError:\n            raise Error(\n              \"len(a) does not work, supply info['height'] instead.\")\n        info['height'] = l\n\n    # In order to work out whether we the array is 2D or 3D we need its\n    # first row, which requires that we take a copy of its iterator.\n    # We may also need the first row to derive width and bitdepth.\n    row, a = peekiter(a)\n    try:\n        row[0][0]\n        threed = True\n        testelement = row[0]\n    except (IndexError, TypeError):\n        threed = False\n        testelement = row\n    if 'width' not in info:\n        if threed:\n            width = len(row)\n        else:\n            width = len(row) // planes\n        info['width'] = width\n\n    # Not implemented yet\n    assert not threed\n\n    if 'bitdepth' not in info:\n        try:\n            dtype = testelement.dtype\n            # goto the \"else:\" clause.  Sorry.\n        except AttributeError:\n            try:\n                # Try a Python array.array.\n                bitdepth = 8 * testelement.itemsize\n            except AttributeError:\n                # We can't determine it from the array element's\n                # datatype, use a default of 8.\n                bitdepth = 8\n        else:\n            # If we got here without exception, we now assume that\n            # the array is a numpy array.\n            if dtype.kind == 'b':\n                bitdepth = 1\n            else:\n                bitdepth = 8 * dtype.itemsize\n        info['bitdepth'] = bitdepth\n\n    for thing in ('width', 'height', 'bitdepth', 'greyscale', 'alpha'):\n        assert thing in info\n    return Image(a, info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nundo the filter for a scanline.", "response": "def undo_filter(self, filter_type, line):\n        \"\"\"\n        Undo the filter for a scanline.\n\n        `scanline` is a sequence of bytes that does not include\n        the initial filter type byte.\n\n        The scanline will have the effects of filtering removed.\n        Scanline modified inplace and also returned as result.\n        \"\"\"\n        assert 0 <= filter_type <= 4\n        # For the first line of a pass, synthesize a dummy previous line.\n        if self.prev is None:\n            self.prev = newBarray(len(line))\n            # Also it's possible to switch some filters to easier\n            if filter_type == 2:  # \"up\"\n                filter_type = 0\n            elif filter_type == 4:  # \"paeth\"\n                filter_type = 1\n\n        # Call appropriate filter algorithm.\n        # 0 - do nothing\n        if filter_type == 1:\n            self.__undo_filter_sub(line)\n        elif filter_type == 2:\n            self.__undo_filter_up(line)\n        elif filter_type == 3:\n            self.__undo_filter_average(line)\n        elif filter_type == 4:\n            self.__undo_filter_paeth(line)\n\n        # This will not work writing cython attributes from python\n        # Only 'cython from cython' or 'python from python'\n        self.prev[:] = line[:]\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _filter_scanline(self, filter_type, line, result):\n        assert 0 <= filter_type < 5\n        if self.prev is None:\n            # We're on the first line.  Some of the filters can be reduced\n            # to simpler cases which makes handling the line \"off the top\"\n            # of the image simpler.  \"up\" becomes \"none\"; \"paeth\" becomes\n            # \"left\" (non-trivial, but true). \"average\" needs to be handled\n            # specially.\n            if filter_type == 2:  # \"up\"\n                filter_type = 0\n            elif filter_type == 3:\n                self.prev = newBarray(len(line))\n            elif filter_type == 4:  # \"paeth\"\n                filter_type = 1\n\n        if filter_type == 1:\n            self.__do_filter_sub(line, result)\n        elif filter_type == 2:\n            self.__do_filter_up(line, result)\n        elif filter_type == 3:\n            self.__do_filter_average(line, result)\n        elif filter_type == 4:\n            self.__do_filter_paeth(line, result)", "response": "Apply a filter to a scanline."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_la_to_rgba(self, row, result):\n        for i in range(len(row) // 3):\n            for j in range(3):\n                result[(4 * i) + j] = row[2 * i]\n            result[(4 * i) + 3] = row[(2 * i) + 1]", "response": "Convert a grayscale image with alpha to RGBA."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_l_to_rgba(self, row, result):\n        for i in range(len(row) // 3):\n            for j in range(3):\n                result[(4 * i) + j] = row[i]", "response": "Convert a grayscale image to RGBA."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an RGB image to RGBA.", "response": "def convert_rgb_to_rgba(self, row, result):\n        \"\"\"\n        Convert an RGB image to RGBA.\n\n        This method assumes the alpha channel in result is already\n        correctly initialized.\n        \"\"\"\n        for i in range(len(row) // 3):\n            for j in range(3):\n                result[(4 * i) + j] = row[(3 * i) + j]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_icc_profile(self, profile=None, name='ICC Profile'):\n        if isinstance(profile, (basestring, bytes)):\n            icc_profile = [name, profile]\n        # TODO: more check\n        else:\n            icc_profile = profile\n\n        if not icc_profile[0]:\n            raise Error(\"ICC profile should have a name\")\n        elif not isinstance(icc_profile[0], bytes):\n            icc_profile[0] = strtobytes(icc_profile[0])\n        self.icc_profile = icc_profile", "response": "Set the ICC Profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_text(self, text=None, **kwargs):\n        if text is None:\n            text = {}\n        text.update(popdict(kwargs, _registered_kw))\n        if 'Creation Time' in text and\\\n                not isinstance(text['Creation Time'], (basestring, bytes)):\n            text['Creation Time'] = datetime.datetime(\n                *(check_time(text['Creation Time'])[:6])).isoformat()\n        self.text = text", "response": "Set the text of the entry in the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_filter_type(self, filter_type=None):\n        if filter_type is None:\n            filter_type = 0\n        elif isinstance(filter_type, basestring):\n            str_ftype = str(filter_type).lower()\n            filter_names = {'none': 0,\n                            'sub': 1,\n                            'up': 2,\n                            'average': 3,\n                            'paeth': 4}\n            if str_ftype in filter_names:\n                filter_type = filter_names[str_ftype]\n        self.filter_type = filter_type", "response": "Set the filter type for better compression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_modification_time(self, modification_time=True):\n        if (isinstance(modification_time, basestring) and\n                modification_time.lower() == 'write') or\\\n                modification_time is True:\n            self.modification_time = True\n        else:\n            self.modification_time = check_time(modification_time)", "response": "Set modification time of the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the resolution of the current log entry.", "response": "def set_resolution(self, resolution=None):\n        \"\"\"\n        Add physical pixel dimensions\n\n        `resolution` supposed two be tuple of two parameterts: pixels per unit\n        and unit type; unit type may be omitted\n        pixels per unit could be simple integer or tuple of (ppu_x, ppu_y)\n        Also possible to use all three parameters im row\n\n        * resolution = ((1, 4), )  # wide pixels (4:1) without unit specifier\n        * resolution = (300, 'inch')  # 300dpi in both dimensions\n        * resolution = (4, 1, 0)  # tall pixels (1:4) without unit specifier\n        \"\"\"\n        if resolution is None:\n            self.resolution = None\n            return\n        # All in row\n        if len(resolution) == 3:\n            resolution = ((resolution[0], resolution[1]), resolution[2])\n        # Ensure length and convert all false to 0 (no unit)\n        if len(resolution) == 1 or not resolution[1]:\n            resolution = (resolution[0], 0)\n        # Single dimension\n        if isinstance(resolution[0], float) or isinteger(resolution[0]):\n            resolution = ((resolution[0], resolution[0]), resolution[1])\n        # Unit conversion\n        if resolution[1] in (1, 'm', 'meter'):\n            resolution = (resolution[0], 1)\n        elif resolution[1] in ('i', 'in', 'inch'):\n            resolution = ((int(resolution[0][0] / 0.0254 + 0.5),\n                int(resolution[0][1] / 0.0254 + 0.5)), 1)\n        elif resolution[1] in ('cm', 'centimeter'):\n            resolution = ((resolution[0][0] * 100,\n                     resolution[0][1] * 100), 1)\n        self.resolution = resolution"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset rendering intent variant for sRGB chunk", "response": "def set_rendering_intent(self, rendering_intent):\n        \"\"\"Set rendering intent variant for sRGB chunk\"\"\"\n        if rendering_intent not in (None,\n                                    PERCEPTUAL,\n                                    RELATIVE_COLORIMETRIC,\n                                    SATURATION,\n                                    ABSOLUTE_COLORIMETRIC):\n            raise FormatError('Unknown redering intent')\n        self.rendering_intent = rendering_intent"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset white point part of cHRM chunk", "response": "def set_white_point(self, white_point, point2=None):\n        \"\"\"Set white point part of cHRM chunk\"\"\"\n        if isinstance(white_point, float) and isinstance(point2, float):\n            white_point = (white_point, point2)\n        self.white_point = white_point"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting rgb points part of cHRM chunk", "response": "def set_rgb_points(self, rgb_points, *args):\n        \"\"\"Set rgb points part of cHRM chunk\"\"\"\n        if not args:\n            self.rgb_points = rgb_points\n        # separate tuples\n        elif len(args) == 2:\n            self.rgb_points = (rgb_points, args[0], args[1])\n        # separate numbers\n        elif len(args) == 5:\n            self.rgb_points = ((rgb_points, args[0]),\n                          (args[1], args[2]),\n                          (args[3], args[4]))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __write_palette(self, outfile):\n        # We must have true bitdepth within palette to use sBIT with palette\n        # if self.pixbitdepth != 8:\n        #     write_chunk(outfile, 'sBIT',\n        #             bytearray_to_bytes(bytearray((self.pixbitdepth,) * 3)))\n        p = bytearray()\n        t = bytearray()\n\n        for x in self.palette:\n            p.extend(x[0:3])\n            if len(x) > 3:\n                t.append(x[3])\n\n        write_chunk(outfile, 'PLTE', bytearray_to_bytes(p))\n        if t:\n            # tRNS chunk is optional. Only needed if palette entries\n            # have alpha.\n            write_chunk(outfile, 'tRNS', bytearray_to_bytes(t))", "response": "Write the palette entries to a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __write_srgb(self, outfile):\n        if self.rendering_intent is not None and self.icc_profile is not None:\n            raise FormatError(\"sRGB(via rendering_intent) and iCCP could not\"\n                    \"be present simultaneously\")\n        # http://www.w3.org/TR/PNG/#11sRGB\n        if self.rendering_intent is not None:\n            write_chunk(outfile, 'sRGB',\n                        struct.pack(\"B\", int(self.rendering_intent)))\n        # http://www.w3.org/TR/PNG/#11cHRM\n        if (self.white_point is not None and self.rgb_points is None) or\\\n                (self.white_point is None and self.rgb_points is not None):\n            logging.warn(\"White and RGB points should be both specified to\"\n                         \" write cHRM chunk\")\n            self.white_point = None\n            self.rgb_points = None\n        if (self.white_point is not None and self.rgb_points is not None):\n            data = (self.white_point[0], self.white_point[1],\n                    self.rgb_points[0][0], self.rgb_points[0][1],\n                    self.rgb_points[1][0], self.rgb_points[1][1],\n                    self.rgb_points[2][0], self.rgb_points[2][1],\n                    )\n            write_chunk(outfile, 'cHRM',\n                        struct.pack(\"!8L\",\n                                    *[int(round(it * 1e5)) for it in data]))\n        # http://www.w3.org/TR/PNG/#11gAMA\n        if self.gamma is not None:\n            write_chunk(outfile, 'gAMA',\n                        struct.pack(\"!L\", int(round(self.gamma * 1e5))))\n        # http://www.w3.org/TR/PNG/#11iCCP\n        if self.icc_profile is not None:\n            if self.compression is None or self.compression == -1:\n                comp_level = 6  # zlib.Z_DEFAULT_COMPRESSION\n            else:\n                comp_level = self.compression\n            write_chunk(outfile, 'iCCP',\n                        self.icc_profile[0] + zerobyte +\n                        zerobyte +\n                        zlib.compress(self.icc_profile[1], comp_level))", "response": "Write sRGB and cHRM to file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting text information into file.", "response": "def __write_text(self, outfile):\n        \"\"\"\n        Write text information into file\n\n        This method should be called only from ``write_idat`` method\n        or chunk order will be ruined.\n        \"\"\"\n        for k, v in self.text.items():\n            if not isinstance(v, bytes):\n                try:\n                    international = False\n                    v = v.encode('latin-1')\n                except UnicodeEncodeError:\n                    international = True\n                    v = v.encode('utf-8')\n            else:\n                international = False\n            if not isinstance(k, bytes):\n                k = strtobytes(k)\n            if international:\n                # No compress, language tag or translated keyword for now\n                write_chunk(outfile, 'iTXt', k + zerobyte +\n                            zerobyte + zerobyte +\n                            zerobyte + zerobyte + v)\n            else:\n                write_chunk(outfile, 'tEXt', k + zerobyte + v)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_passes(self, outfile, rows, packed=False):\n        # Try to optimize\n        if not packed:\n            if self.greyscale == 'try':\n                rows1, rows2 = tee(rows)\n                greyrows = try_greyscale(rows1, self.alpha)\n                if greyrows is not False:\n                    rows = greyrows\n                    self.greyscale = True\n                    self.planes -= 2\n                else:\n                    self.greyscale = False\n                    rows = rows2\n\n        if not self.palette:\n            # No palette, check for rescale\n            targetbitdepth = None\n            srcbitdepth = self.bitdepth\n            if self.alpha or not self.greyscale:\n                if self.bitdepth not in (8, 16):\n                    targetbitdepth = (8, 16)[self.bitdepth > 8]\n            else:\n                assert self.greyscale\n                assert not self.alpha\n                if self.bitdepth not in (1, 2, 4, 8, 16):\n                    if self.bitdepth > 8:\n                        targetbitdepth = 16\n                    elif self.bitdepth == 3:\n                        targetbitdepth = 4\n                    else:\n                        assert self.bitdepth in (5, 6, 7)\n                        targetbitdepth = 8\n\n            if targetbitdepth:\n                if packed:\n                    raise Error(\"writing packed pixels not suitable for\"\n                                \" bit depth %d\" % self.bitdepth)\n                self.bitdepth = targetbitdepth\n                factor = \\\n                    float(2**targetbitdepth - 1) / float(2**srcbitdepth - 1)\n\n                def scalerow(inrows):\n                    \"\"\"Rescale all pixels\"\"\"\n                    for row in inrows:\n                        yield [int(round(factor * x)) for x in row]\n\n                rows = scalerow(rows)\n\n        self.write_idat(outfile, self.comp_idat(self.idat(rows, packed)))\n        return self.irows", "response": "Write a PNG image to the output file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite IDAT to file outfile.", "response": "def write_idat(self, outfile, idat_sequence):\n        \"\"\"\n        Write png with IDAT to file\n\n        `idat_sequence` should be iterable that produce IDAT chunks\n        compatible with `Writer` configuration.\n        \"\"\"\n        # http://www.w3.org/TR/PNG/#5PNG-file-signature\n        outfile.write(png_signature)\n        color_type = 4 * self.alpha + 2 * (not self.greyscale) +\\\n            bool(self.palette)\n\n        # http://www.w3.org/TR/PNG/#11IHDR\n        write_chunk(outfile, 'IHDR',\n                    struct.pack(\"!2I5B\", self.width, self.height,\n                                self.bitdepth, color_type,\n                                0, 0, self.interlace))\n        # See :chunk:order\n        self.__write_srgb(outfile)\n        # See :chunk:order\n        # http://www.w3.org/TR/PNG/#11sBIT\n        if not self.palette and self.pixbitdepth != self.bitdepth:\n            # Write sBIT of palette within __write_palette\n            # TODO: support different bitdepth per plane\n            write_chunk(outfile, 'sBIT',\n                        struct.pack('%dB' % self.planes,\n                                    *[self.pixbitdepth] * self.planes))\n        # :chunk:order: Without a palette (PLTE chunk), ordering is\n        # relatively relaxed.  With one, gamma info must precede PLTE\n        # chunk which must precede tRNS and bKGD.\n        # See http://www.w3.org/TR/PNG/#5ChunkOrdering\n        if self.palette:\n            self.__write_palette(outfile)\n\n        # http://www.w3.org/TR/PNG/#11tRNS\n        if self.transparent is not None:\n            if self.greyscale:\n                write_chunk(outfile, 'tRNS',\n                            struct.pack(\"!1H\", *self.transparent))\n            else:\n                write_chunk(outfile, 'tRNS',\n                            struct.pack(\"!3H\", *self.transparent))\n\n        # http://www.w3.org/TR/PNG/#11bKGD\n        if self.background is not None:\n            if self.greyscale:\n                write_chunk(outfile, 'bKGD',\n                            struct.pack(\"!1H\", *self.background))\n            else:\n                write_chunk(outfile, 'bKGD',\n                            struct.pack(\"!3H\", *self.background))\n        # http://www.w3.org/TR/PNG/#11pHYs\n        if self.resolution is not None:\n            write_chunk(outfile, 'pHYs',\n                        struct.pack(\"!IIB\",\n                                    self.resolution[0][0],\n                                    self.resolution[0][1],\n                                    self.resolution[1]))\n        # http://www.w3.org/TR/PNG/#11tIME\n        if self.modification_time is not None:\n            if self.modification_time is True:\n                self.modification_time = check_time('now')\n            write_chunk(outfile, 'tIME',\n                        struct.pack(\"!H5B\", *(self.modification_time[:6])))\n        # http://www.w3.org/TR/PNG/#11textinfo\n        if self.text:\n            self.__write_text(outfile)\n        for idat in idat_sequence:\n            write_chunk(outfile, 'IDAT', idat)\n        # http://www.w3.org/TR/PNG/#11IEND\n        write_chunk(outfile, 'IEND')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef comp_idat(self, idat):\n        # http://www.w3.org/TR/PNG/#11IDAT\n        if self.compression is not None:\n            compressor = zlib.compressobj(self.compression)\n        else:\n            compressor = zlib.compressobj()\n        for dat in idat:\n            compressed = compressor.compress(dat)\n            if len(compressed):\n                yield compressed\n        flushed = compressor.flush()\n        if len(flushed):\n            yield flushed", "response": "Generator that produce compressed IDAT chunks from IDAT data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef idat(self, rows, packed=False):\n        # http://www.w3.org/TR/PNG/#11IDAT\n        filt = Filter(self.bitdepth * self.planes,\n                      self.interlace, self.height)\n        data = bytearray()\n\n        def byteextend(rowbytes):\n            \"\"\"Default extending data with bytes. Applying filter\"\"\"\n            data.extend(filt.do_filter(self.filter_type, rowbytes))\n\n        # Choose an extend function based on the bitdepth.  The extend\n        # function packs/decomposes the pixel values into bytes and\n        # stuffs them onto the data array.\n        if self.bitdepth == 8 or packed:\n            extend = byteextend\n        elif self.bitdepth == 16:\n            def extend(sl):\n                \"\"\"Decompose into bytes before byteextend\"\"\"\n                fmt = '!%dH' % len(sl)\n                byteextend(bytearray(struct.pack(fmt, *sl)))\n        else:\n            # Pack into bytes\n            assert self.bitdepth < 8\n            # samples per byte\n            spb = 8 // self.bitdepth\n\n            def extend(sl):\n                \"\"\"Pack into bytes before byteextend\"\"\"\n                a = bytearray(sl)\n                # Adding padding bytes so we can group into a whole\n                # number of spb-tuples.\n                l = float(len(a))\n                extra = math.ceil(l / float(spb)) * spb - l\n                a.extend([0] * int(extra))\n                # Pack into bytes\n                l = group(a, spb)\n                l = [reduce(lambda x, y: (x << self.bitdepth) + y, e)\n                     for e in l]\n                byteextend(l)\n\n        # Build the first row, testing mostly to see if we need to\n        # changed the extend function to cope with NumPy integer types\n        # (they cause our ordinary definition of extend to fail, so we\n        # wrap it).  See\n        # http://code.google.com/p/pypng/issues/detail?id=44\n        enumrows = enumerate(rows)\n        del rows\n\n        # :todo: Certain exceptions in the call to ``.next()`` or the\n        # following try would indicate no row data supplied.\n        # Should catch.\n        i, row = next(enumrows)\n        try:\n            # If this fails...\n            extend(row)\n        except:\n            # ... try a version that converts the values to int first.\n            # Not only does this work for the (slightly broken) NumPy\n            # types, there are probably lots of other, unknown, \"nearly\"\n            # int types it works for.\n            def wrapmapint(f):\n                return lambda sl: f([int(x) for x in sl])\n            extend = wrapmapint(extend)\n            del wrapmapint\n            extend(row)\n\n        for i, row in enumrows:\n            extend(row)\n            if len(data) > self.chunk_limit:\n                yield bytearray_to_bytes(data)\n                # Because of our very witty definition of ``extend``,\n                # above, we must re-use the same ``data`` object.  Hence\n                # we use ``del`` to empty this one, rather than create a\n                # fresh one (which would be my natural FP instinct).\n                del data[:]\n        if len(data):\n            yield bytearray_to_bytes(data)\n        self.irows = i + 1", "response": "Generates an IDAT table from a list of rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_array(self, outfile, pixels):\n\n        if self.interlace:\n            self.write_passes(outfile, self.array_scanlines_interlace(pixels))\n        else:\n            self.write_passes(outfile, self.array_scanlines(pixels))", "response": "Write an array in flat row flat pixel format as a PNG file on\n            the output file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_packed(self, outfile, rows):\n        return self.write_passes(outfile, rows, packed=True)", "response": "Write PNG file to outfile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate rows to fill right pixels in int mode", "response": "def rigthgen(self, value=0):\n        \"\"\"Generate rows to fill right pixels in int mode\"\"\"\n        while True:\n            yield self.newarray(self.nplanes_right * self.width, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating merged row of original iterstors", "response": "def next(self):\n        \"\"\"Generate merged row, consuming rows of original iterstors\"\"\"\n        left = next(self.seq_left)\n        if self.width is None:\n            self.width = len(left) / self.nplanes_left\n        if self.bitdepth is None:\n            # Detect bitdepth\n            if hasattr(left, 'itemsize'):  # array\n                self.bitdepth = left.itemsize * 8\n            elif isinstance(left, (bytes, bytearray)):  # bytearray\n                self.bitdepth = 8\n            else:\n                raise Error(\"Unknown bitdepth for merging planes\")\n        right = next(self.seq_right)\n        rowlength = self.nplanes_res * self.width\n        new = self.newarray(rowlength)\n        if type(left) == type(right) == type(new):\n            # slice assignment\n            for i in range(self.nplanes_left):\n                new[i::self.nplanes_res] = left[i::self.nplanes_left]\n            for i in range(self.nplanes_right):\n                i_ = i + self.nplanes_left\n                new[i_::self.nplanes_res] = right[i::self.nplanes_right]\n        else:\n            for i in range(self.nplanes_left):\n                for j in range(self.width):\n                    new[(i * self.nplanes_res) + j] =\\\n                        left[(i * self.nplanes_left) + j]\n            for i in range(self.nplanes_right):\n                i_ = i + self.nplanes_left\n                for j in range(self.width):\n                    new[(i_ * self.nplanes_res) + j] =\\\n                        right[(i * self.nplanes_right) + j]\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_all(self, line):\n        lines = [None] * 5\n        for filter_type in range(5):  # range save more than 'optimised' order\n            res = copyBarray(line)\n            self._filter_scanline(filter_type, line, res)\n            res.insert(0, filter_type)\n            lines[filter_type] = res\n        return lines", "response": "Do all filters for specified line return filtered lines as list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef adaptive_filter(self, strategy, line):\n        if isinstance(strategy, (basestring, bytes)):\n            strategy = {'name': str(strategy)}\n        if isinstance(strategy, dict):\n            cfg = strategy\n            strategy = Filter.adapt_methods.get(cfg['name'])\n        else:\n            cfg = {}\n        if strategy is None:\n            raise Error(\"Adaptive strategy not found\")\n        else:\n            return strategy(line, cfg, self)", "response": "Apply an adaptive filter to the current sequence of entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_filter(self, filter_type, line):\n        # Recall that filtering algorithms are applied to bytes,\n        # not to pixels, regardless of the bit depth or colour type\n        # of the image.\n\n        line = bytearray(line)\n        if isinstance(filter_type, int):\n            res = bytearray(line)\n            self._filter_scanline(filter_type, line, res)\n            res.insert(0, filter_type)  # Add filter type as the first byte\n        else:\n            res = self.adaptive_filter(filter_type, line)\n        self.prev = line\n        if self.restarts:\n            self.restarts[0] -= 1\n            if self.restarts[0] == 0:\n                del self.restarts[0]\n                self.prev = None\n        return res", "response": "Apply a basic or adaptive filter to the current line of the log file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the image to file.", "response": "def save(self, file):\n        \"\"\"\n        Save the image to *file*.\n\n        If *file* looks like an open file\n        descriptor then it is used, otherwise it is treated as a\n        filename and a fresh file is opened.\n\n        In general, you can only call this method once; after it has\n        been called the first time and the PNG image has been saved, the\n        source data will have been streamed, and cannot be streamed\n        again.\n        \"\"\"\n        w = Writer(**self.info)\n\n        try:\n            file.write\n\n            def close():\n                pass\n        except AttributeError:\n            file = open(file, 'wb')\n\n            def close():\n                file.close()\n\n        try:\n            w.write(file, self.rows)\n        finally:\n            close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread n chars from buffer", "response": "def read(self, n):\n        \"\"\"Read `n` chars from buffer\"\"\"\n        r = self.buf[self.offset:self.offset + n]\n        if isinstance(r, array):\n            r = r.tostring()\n        self.offset += n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the next PNG chunk from the input file and return a tuple of the size of the chunk.", "response": "def chunk(self, seek=None, lenient=False):\n        \"\"\"\n        Read the next PNG chunk from the input file\n\n        returns a (*chunk_type*, *data*) tuple. *chunk_type* is the chunk's\n        type as a byte string (all PNG chunk types are 4 bytes long).\n        *data* is the chunk's data content, as a byte string.\n\n        If the optional `seek` argument is\n        specified then it will keep reading chunks until it either runs\n        out of file or finds the chunk_type specified by the argument.  Note\n        that in general the order of chunks in PNGs is unspecified, so\n        using `seek` can cause you to miss chunks.\n\n        If the optional `lenient` argument evaluates to `True`,\n        checksum failures will raise warnings rather than exceptions.\n        \"\"\"\n        self.validate_signature()\n        while True:\n            # http://www.w3.org/TR/PNG/#5Chunk-layout\n            if not self.atchunk:\n                self.atchunk = self.chunklentype()\n            length, chunk_type = self.atchunk\n            self.atchunk = None\n            data = self.file.read(length)\n            if len(data) != length:\n                raise ChunkError('Chunk %s too short for required %i octets.'\n                  % (chunk_type, length))\n            checksum = self.file.read(4)\n            if len(checksum) != 4:\n                raise ChunkError('Chunk %s too short for checksum.',\n                                 chunk_type)\n            if seek and chunk_type != seek:\n                continue\n            verify = zlib.crc32(strtobytes(chunk_type))\n            verify = zlib.crc32(data, verify)\n            # Whether the output from zlib.crc32 is signed or not varies\n            # according to hideous implementation details, see\n            # http://bugs.python.org/issue1202 .\n            # We coerce it to be positive here (in a way which works on\n            # Python 2.3 and older).\n            verify &= 2**32 - 1\n            verify = struct.pack('!I', verify)\n            if checksum != verify:\n                (a, ) = struct.unpack('!I', checksum)\n                (b, ) = struct.unpack('!I', verify)\n                message = \"Checksum error in %s chunk: 0x%08X != 0x%08X.\" %\\\n                                        (chunk_type, a, b)\n                if lenient:\n                    warnings.warn(message, RuntimeWarning)\n                else:\n                    raise ChunkError(message)\n            return chunk_type, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads raw pixel data undo filters deinterlace and flatten.", "response": "def deinterlace(self, raw):\n        \"\"\"\n        Read raw pixel data, undo filters, deinterlace, and flatten.\n\n        Return in flat row flat pixel format.\n        \"\"\"\n        # Values per row (of the target image)\n        vpr = self.width * self.planes\n\n        # Make a result array, and make it big enough.  Interleaving\n        # writes to the output array randomly (well, not quite), so the\n        # entire output array must be in memory.\n        if self.bitdepth > 8:\n            a = newHarray(vpr * self.height)\n        else:\n            a = newBarray(vpr * self.height)\n        source_offset = 0\n        filt = Filter(self.bitdepth * self.planes)\n        for xstart, ystart, xstep, ystep in _adam7:\n            if xstart >= self.width:\n                continue\n            # The previous (reconstructed) scanline.  None at the\n            # beginning of a pass to indicate that there is no previous\n            # line.\n            filt.prev = None\n            # Pixels per row (reduced pass image)\n            ppr = int(math.ceil((self.width-xstart)/float(xstep)))\n            # Row size in bytes for this pass.\n            row_size = int(math.ceil(self.psize * ppr))\n            for y in range(ystart, self.height, ystep):\n                filter_type = raw[source_offset]\n                scanline = raw[source_offset + 1:source_offset + row_size + 1]\n                source_offset += (row_size + 1)\n                if filter_type not in (0, 1, 2, 3, 4):\n                    raise FormatError('Invalid PNG Filter Type.'\n                '  See http://www.w3.org/TR/2003/REC-PNG-20031110/#9Filters .')\n                filt.undo_filter(filter_type, scanline)\n                # Convert so that there is one element per pixel value\n                flat = self.serialtoflat(scanline, ppr)\n                end_offset = (y + 1) * vpr\n                if xstep == 1:\n                    # Last pass (0, 1, 1, 2))\n                    assert xstart == 0\n                    offset = y * vpr\n                    a[offset:end_offset] = flat\n                else:\n                    offset = y * vpr + xstart * self.planes\n                    for i in range(self.planes):\n                        a[offset + i:end_offset:self.planes * xstep] = \\\n                            flat[i::self.planes]\n        return a"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iterboxed(self, rows):\n        def asvalues(raw):\n            \"\"\"\n            Convert a row of raw bytes into a flat row.\n\n            Result may or may not share with argument\n            \"\"\"\n            if self.bitdepth == 8:\n                return raw\n            if self.bitdepth == 16:\n                raw = bytearray_to_bytes(raw)\n                return array('H', struct.unpack('!%dH' % (len(raw) // 2), raw))\n            assert self.bitdepth < 8\n            width = self.width\n            # Samples per byte\n            spb = 8 // self.bitdepth\n            out = newBarray()\n            mask = 2 ** self.bitdepth - 1\n            #                                      reversed range(spb)\n            shifts = [self.bitdepth * it for it in range(spb - 1, -1, -1)]\n            for o in raw:\n                out.extend([mask & (o >> i) for i in shifts])\n            return out[:width]\n\n        return map(asvalues, rows)", "response": "Iterate over the bytes of the scanlines in boxed pixel format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert serial ( byte stream ) pixel data to flat row flat pixel.", "response": "def serialtoflat(self, raw, width=None):\n        \"\"\"Convert serial (byte stream) pixel data to flat row flat pixel.\"\"\"\n        if self.bitdepth == 8:\n            return raw\n        if self.bitdepth == 16:\n            raw = bytearray_to_bytes(raw)\n            return array('H',\n              struct.unpack('!%dH' % (len(raw) // 2), raw))\n        assert self.bitdepth < 8\n        if width is None:\n            width = self.width\n        # Samples per byte\n        spb = 8 // self.bitdepth\n        out = newBarray()\n        mask = 2**self.bitdepth - 1\n        #                                      reversed range(spb)\n        shifts = [self.bitdepth * it for it in range(spb - 1, -1, -1)]\n        l = width\n        for o in raw:\n            out.extend([(mask&(o>>s)) for s in shifts][:l])\n            l -= spb\n            if l <= 0:\n                l = width\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_signature(self):\n        if self.signature:\n            return\n        self.signature = self.file.read(8)\n        if self.signature != png_signature:\n            raise FormatError(\"PNG file has invalid signature.\")", "response": "Validate the signature of the image file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the image metadata by reading the initial part of the IDAT chunk and processing the IDAT chunk.", "response": "def preamble(self, lenient=False):\n        \"\"\"\n        Extract the image metadata\n\n        Extract the image metadata by reading the initial part of\n        the PNG file up to the start of the ``IDAT`` chunk.  All the\n        chunks that precede the ``IDAT`` chunk are read and either\n        processed for metadata or discarded.\n\n        If the optional `lenient` argument evaluates to `True`, checksum\n        failures will raise warnings rather than exceptions.\n        \"\"\"\n        self.validate_signature()\n        while True:\n            if not self.atchunk:\n                self.atchunk = self.chunklentype()\n                if self.atchunk is None:\n                    raise FormatError(\n                      'This PNG file has no IDAT chunks.')\n            if self.atchunk[1] == 'IDAT':\n                return\n            self.process_chunk(lenient=lenient)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chunklentype(self):\n        x = self.file.read(8)\n        if not x:\n            return None\n        if len(x) != 8:\n            raise FormatError(\n              'End of file whilst reading chunk length and type.')\n        length, chunk_type = struct.unpack('!I4s', x)\n        chunk_type = bytestostr(chunk_type)\n        if length > 2**31-1:\n            raise FormatError('Chunk %s is too large: %d.' % (chunk_type,\n                                                              length))\n        return length, chunk_type", "response": "Reads just enough of the input to determine the next next\n        chunk s length and type and returns it as a tuple where length and type are a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef idat(self, lenient=False):\n        while True:\n            try:\n                chunk_type, data = self.chunk(lenient=lenient)\n            except ValueError:\n                e = sys.exc_info()[1]\n                raise ChunkError(e.args[0])\n            if chunk_type == 'IEND':\n                # http://www.w3.org/TR/PNG/#11IEND\n                break\n            if chunk_type != 'IDAT':\n                continue\n            # chunk_type == 'IDAT'\n            # http://www.w3.org/TR/PNG/#11IDAT\n            if self.colormap and not self.plte:\n                warnings.warn(\"PLTE chunk is required before IDAT chunk\")\n            yield data", "response": "Iterator that yields all the IDAT chunks as strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef idatdecomp(self, lenient=False, max_length=0):\n        # Currently, with no max_length paramter to decompress, this\n        # routine will do one yield per IDAT chunk.  So not very\n        # incremental.\n        d = zlib.decompressobj()\n        # Each IDAT chunk is passed to the decompressor, then any\n        # remaining state is decompressed out.\n        for data in self.idat(lenient):\n            # :todo: add a max_length argument here to limit output\n            # size.\n            yield bytearray(d.decompress(data))\n        yield bytearray(d.flush())", "response": "Iterator that yields decompressed IDAT strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, lenient=False):\n        self.preamble(lenient=lenient)\n        raw = self.idatdecomp(lenient)\n\n        if self.interlace:\n            raw = bytearray(itertools.chain(*raw))\n            arraycode = 'BH'[self.bitdepth > 8]\n            # Like :meth:`group` but producing an array.array object for\n            # each row.\n            pixels = map(lambda *row: array(arraycode, row),\n                       *[iter(self.deinterlace(raw))]*self.width*self.planes)\n        else:\n            pixels = self.iterboxed(self.iterstraight(raw))\n        meta = dict()\n        for attr in 'greyscale alpha planes bitdepth interlace'.split():\n            meta[attr] = getattr(self, attr)\n        meta['size'] = (self.width, self.height)\n        for attr in ('gamma', 'transparent', 'background', 'last_mod_time',\n                     'icc_profile', 'resolution', 'text',\n                     'rendering_intent', 'white_point', 'rgb_points'):\n            a = getattr(self, attr, None)\n            if a is not None:\n                meta[attr] = a\n        if self.plte:\n            meta['palette'] = self.palette()\n        return self.width, self.height, pixels, meta", "response": "Read the PNG file and decode it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_flat(self):\n        x, y, pixel, meta = self.read()\n        arraycode = 'BH'[meta['bitdepth'] > 8]\n        pixel = array(arraycode, itertools.chain(*pixel))\n        return x, y, pixel, meta", "response": "Read a PNG file and decode it into a flat row flat pixel format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef asRGB(self):\n        width, height, pixels, meta = self.asDirect()\n        if meta['alpha']:\n            raise Error(\"will not convert image with alpha channel to RGB\")\n        if not meta['greyscale']:\n            return width, height, pixels, meta\n        meta['greyscale'] = False\n        newarray = (newBarray, newHarray)[meta['bitdepth'] > 8]\n\n        def iterrgb():\n            for row in pixels:\n                a = newarray(3 * width)\n                for i in range(3):\n                    a[i::3] = row\n                yield a\n        return width, height, iterrgb(), meta", "response": "Return image as RGB pixels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns image as RGBA pixels.", "response": "def asRGBA(self):\n        \"\"\"\n        Return image as RGBA pixels.\n\n        Greyscales are expanded into RGB triplets;\n        an alpha channel is synthesized if necessary.\n        The return values are as for the :meth:`read` method\n        except that the *metadata* reflect the returned pixels, not the\n        source image.  In particular, for this method\n        ``metadata['greyscale']`` will be ``False``, and\n        ``metadata['alpha']`` will be ``True``.\n        \"\"\"\n        width, height, pixels, meta = self.asDirect()\n        if meta['alpha'] and not meta['greyscale']:\n            return width, height, pixels, meta\n        maxval = 2**meta['bitdepth'] - 1\n        if meta['bitdepth'] > 8:\n            def newarray():\n                return array('H', [maxval] * 4 * width)\n        else:\n            def newarray():\n                return bytearray([maxval] * 4 * width)\n\n        # Not best way, but we have only array of bytes accelerated now\n        if meta['bitdepth'] <= 8:\n            filt = BaseFilter()\n        else:\n            filt = iBaseFilter()\n\n        if meta['alpha'] and meta['greyscale']:\n            # LA to RGBA\n            def convert():\n                for row in pixels:\n                    # Create a fresh target row, then copy L channel\n                    # into first three target channels, and A channel\n                    # into fourth channel.\n                    a = newarray()\n                    filt.convert_la_to_rgba(row, a)\n                    yield a\n        elif meta['greyscale']:\n            # L to RGBA\n            def convert():\n                for row in pixels:\n                    a = newarray()\n                    filt.convert_l_to_rgba(row, a)\n                    yield a\n        else:\n            assert not meta['alpha'] and not meta['greyscale']\n            # RGB to RGBA\n\n            def convert():\n                for row in pixels:\n                    a = newarray()\n                    filt.convert_rgb_to_rgba(row, a)\n                    yield a\n        meta['alpha'] = True\n        meta['greyscale'] = False\n        return width, height, convert(), meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert chromaticity to XYZ.", "response": "def chromaticity_to_XYZ(white, red, green, blue):\n    \"\"\"From the \"CalRGB Color Spaces\" section of \"PDF Reference\", 6th ed.\"\"\"\n    xW, yW = white\n    xR, yR = red\n    xG, yG = green\n    xB, yB = blue\n    R = G = B = 1.0\n\n    z = yW * ((xG - xB) * yR - (xR - xB) * yG + (xR - xG) * yB)\n    YA = yR / R  * ((xG - xB) * yW - (xW - xB) * yG + (xW - xG) * yB) / z\n    XA = YA * xR / yR\n    ZA = YA * ((1 - xR) / yR - 1)\n    YB = - yG / G * ((xR - xB) * yW - (xW - xB) * yR + (xW - xR) * yB) / z\n    XB = YB * xG / yG\n    ZB = YB * ((1 - xG) / yG - 1)\n    YC = yB / B * ((xR - xG) * yW - (xW - xG) * yR + (xW - xR) * yG) / z\n    XC = YC * xB / yB\n    ZC = YC * ((1 - xB) / yB - 1)\n    XW = XA * R + XB * G + XC * B\n    YW = YA * R + YB * G + YC * B\n    ZW = ZA * R + ZB * G + ZC * B\n\n    return (XW, YW, ZW), (XA, YA, ZA), (XB, YB, ZB), (XC, YC, ZC)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncache content retrieved by the client, thus allowing the cached content to be used later if the live content cannot be retrieved.", "response": "def fallback(cache):\n    \"\"\"\n    Caches content retrieved by the client, thus allowing the cached\n    content to be used later if the live content cannot be retrieved.\n\n    \"\"\"\n\n    log_filter = ThrottlingFilter(cache=cache)\n    logger.filters = []\n    logger.addFilter(log_filter)\n\n    def get_cache_response(cache_key):\n        content = cache.get(cache_key)\n        if content:\n            response = CacheResponse()\n            response.__setstate__({\n                'status_code': 200,\n                '_content': content,\n            })\n            return response\n\n    def get_cache_control(etag_cache_key):\n        etag = cache.get(etag_cache_key)\n        if etag:\n            return ETagCacheControl(etag)\n\n    def closure(func):\n        @wraps(func)\n        def wrapper(client, url, params={}, *args, **kwargs):\n            cache_key = canonicalize_url(url + '?' + urlencode(params))\n            etag_cache_key = 'etag-' + cache_key\n            try:\n                remote_response = func(\n                    client,\n                    url=url,\n                    params=params,\n                    cache_control=get_cache_control(etag_cache_key),\n                    *args,\n                    **kwargs,\n                )\n            except RequestException:\n                # Failed to create the request e.g., the remote server is down,\n                # perhaps a timeout occurred, or even connection closed by\n                # remote, etc.\n                response = get_cache_response(cache_key)\n                if response:\n                    logger.error(MESSAGE_CACHE_HIT, extra={'url': url})\n                else:\n                    raise\n            else:\n                log_context = {\n                    'status_code': remote_response.status_code, 'url': url\n                }\n                if remote_response.status_code == 404:\n                    logger.error(MESSAGE_NOT_FOUND, extra=log_context)\n                    return LiveResponse.from_response(remote_response)\n                elif remote_response.status_code == 304:\n                    response = get_cache_response(cache_key)\n                elif not remote_response.ok:\n                    # Successfully requested the content, but the response is\n                    # not OK (e.g., 500, 403, etc)\n                    response = get_cache_response(cache_key)\n                    if response:\n                        logger.error(MESSAGE_CACHE_HIT, extra=log_context)\n                    else:\n                        logger.exception(MESSAGE_CACHE_MISS, extra=log_context)\n                        response = FailureResponse.from_response(\n                            remote_response\n                        )\n                else:\n                    cache.set_many({\n                        cache_key: remote_response.content,\n                        etag_cache_key: remote_response.headers.get('ETag'),\n                    }, settings.DIRECTORY_CLIENT_CORE_CACHE_EXPIRE_SECONDS)\n                    response = LiveResponse.from_response(remote_response)\n            return response\n        return wrapper\n    return closure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_url(base_url, partial_url):\n        if not base_url.endswith('/'):\n            base_url += '/'\n        if partial_url.startswith('/'):\n            partial_url = partial_url[1:]\n\n        return urlparse.urljoin(base_url, partial_url)", "response": "Builds the URL for the current URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare(constituents, t0, t = None, radians = True):\n\t\t#The equilibrium argument is constant and taken at the beginning of the\n\t\t#time series (t0).  The speed of the equilibrium argument changes very\n\t\t#slowly, so again we take it to be constant over any length of data. The\n\t\t#node factors change more rapidly.\n\t\tif isinstance(t0, Iterable):\n\t\t\tt0 = t0[0]\n\t\tif t is None:\n\t\t\tt = [t0]\n\t\tif not isinstance(t, Iterable):\n\t\t\tt = [t]\n\t\ta0 = astro(t0)\n\t\ta = [astro(t_i) for t_i in t]\n\n\t\t#For convenience give u, V0 (but not speed!) in [0, 360)\n\t\tV0 = np.array([c.V(a0) for c in constituents])[:, np.newaxis]\n\t\tspeed = np.array([c.speed(a0) for c in constituents])[:, np.newaxis]\n\t\tu = [np.mod(np.array([c.u(a_i) for c in constituents])[:, np.newaxis], 360.0)\n\t\t\t for a_i in a]\n\t\tf = [np.mod(np.array([c.f(a_i) for c in constituents])[:, np.newaxis], 360.0)\n\t\t\t for a_i in a]\n\n\t\tif radians:\n\t\t\tspeed = d2r*speed\n\t\t\tV0 = d2r*V0\n\t\t\tu = [d2r*each for each in u]\n\t\treturn speed, u, f, V0", "response": "This function is used to prepare the time series for the given constituents and time series."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef at(self, t):\n\t\tt0 = t[0]\n\t\thours = self._hours(t0, t)\n\t\tpartition = 240.0\n\t\tt = self._partition(hours, partition)\n\t\ttimes = self._times(t0, [(i + 0.5)*partition for i in range(len(t))])\n\t\tspeed, u, f, V0 = self.prepare(t0, times, radians = True)\n\t\tH = self.model['amplitude'][:, np.newaxis]\n\t\tp = d2r*self.model['phase'][:, np.newaxis]\n\n\t\treturn np.concatenate([\n\t\t\tTide._tidal_series(t_i, H, p, speed, u_i, f_i, V0)\n\t\t\tfor t_i, u_i, f_i in izip(t, u, f)\n\t\t])", "response": "get the modelled tidal height at given times"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding only the high tides.", "response": "def highs(self, *args):\n\t\t\"\"\"\n\t\tGenerator yielding only the high tides.\n\t\tArguments:\n\t\tsee Tide.extrema()\n\t\t\"\"\"\n\t\tfor t in ifilter(lambda e: e[2] == 'H', self.extrema(*args)):\n\t\t\tyield t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield only the low tides.", "response": "def lows(self, *args):\n\t\t\"\"\"\n\t\tGenerator yielding only the low tides.\n\t\tArguments:\n\t\tsee Tide.extrema()\n\t\t\"\"\"\n\t\tfor t in ifilter(lambda e: e[2] == 'L', self.extrema(*args)):\n\t\t\tyield t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef form_number(self):\n\t\tk1, o1, m2, s2 = (\n\t\t\tnp.extract(self.model['constituent'] == c, self.model['amplitude'])\n\t\t\tfor c in [constituent._K1, constituent._O1, constituent._M2, constituent._S2]\n\t\t)\n\t\treturn (k1+o1)/(m2+s2)", "response": "Returns the model s form number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extrema(self, t0, t1 = None, partition = 2400.0):\n\t\tif t1:\n\t\t\t#yield from in python 3.4\n\t\t\tfor e in takewhile(lambda t: t[0] < t1, self.extrema(t0)):\n\t\t\t\tyield e\n\t\telse:\n\t\t\t#We assume that extrema are separated by at least delta hours\n\t\t\tdelta = np.amin([\n\t\t\t\t90.0 / c.speed(astro(t0)) for c in self.model['constituent']\n\t\t\t\tif not c.speed(astro(t0)) == 0\n\t\t\t])\n\t\t\t#We search for stationary points from offset hours before t0 to\n\t\t\t#ensure we find any which might occur very soon after t0.\n\t\t\toffset = 24.0\n\t\t\tpartitions = (\n\t\t\t\tTide._times(t0, i*partition) for i in count()), (Tide._times(t0, i*partition) for i in count(1)\n\t\t\t)\n\n\t\t\t#We'll overestimate to be on the safe side;\n\t\t\t#values outside (start,end) won't get yielded.\n\t\t\tinterval_count = int(np.ceil((partition + offset) / delta)) + 1\n\t\t\tamplitude = self.model['amplitude'][:, np.newaxis]\n\t\t\tphase     = d2r*self.model['phase'][:, np.newaxis]\n\n\t\t\tfor start, end in izip(*partitions):\n\t\t\t\tspeed, [u], [f], V0 = self.prepare(start, Tide._times(start, 0.5*partition))\n\t\t\t\t#These derivatives don't include the time dependence of u or f,\n\t\t\t\t#but these change slowly.\n\t\t\t\tdef d(t):\n\t\t\t\t\treturn np.sum(-speed*amplitude*f*np.sin(speed*t + (V0 + u) - phase), axis=0)\n\t\t\t\tdef d2(t):\n\t\t\t\t\treturn np.sum(-speed**2.0 * amplitude*f*np.cos(speed*t + (V0 + u) - phase), axis=0)\n\t\t\t\t#We'll overestimate to be on the safe side;\n\t\t\t\t#values outside (start,end) won't get yielded.\n\t\t\t\tintervals = (\n\t\t\t\t\tdelta * i -offset for i in range(interval_count)), (delta*(i+1) - offset for i in range(interval_count)\n\t\t\t\t)\n\t\t\t\tfor a, b in izip(*intervals):\n\t\t\t\t\tif d(a)*d(b) < 0:\n\t\t\t\t\t\textrema = fsolve(d, (a + b) / 2.0, fprime = d2)[0]\n\t\t\t\t\t\ttime = Tide._times(start, extrema)\n\t\t\t\t\t\t[height] = self.at([time])\n\t\t\t\t\t\thilo = 'H' if d2(extrema) < 0 else 'L'\n\t\t\t\t\t\tif start < time < end:\n\t\t\t\t\t\t\tyield (time, height, hilo)", "response": "A generator for high and low tides."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the hourly offset(s) of a (list of) time from a given time. Arguments: t0 -- time from which offsets are sought t -- times to find hourly offsets from t0.", "response": "def _hours(t0, t):\n\t\t\"\"\"\n\t\tReturn the hourly offset(s) of a (list of) time from a given time.\n\t\tArguments:\n\t\tt0 -- time from which offsets are sought\n\t\tt -- times to find hourly offsets from t0.\n\t\t\"\"\"\n\t\tif not isinstance(t, Iterable):\n\t\t\treturn Tide._hours(t0, [t])[0]\n\t\telif isinstance(t[0], datetime):\n\t\t\treturn np.array([(ti-t0).total_seconds() / 3600.0 for ti in t])\n\t\telse:\n\t\t\treturn t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _partition(hours, partition = 3600.0):\n\t\tpartition = float(partition)\n\t\trelative = hours - hours[0]\n\t\ttotal_partitions = np.ceil(relative[-1] / partition + 10*np.finfo(np.float).eps).astype('int')\n\t\treturn [hours[np.floor(np.divide(relative, partition)) == i] for i in range(total_partitions)]", "response": "a sorted list of numbers or in this case hours."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a (list of) datetime(s) given an initial time and an (list of) hourly offset(s). Arguments: t0 -- initial time hours -- hourly offsets from t0", "response": "def _times(t0, hours):\n\t\t\"\"\"\n\t\tReturn a (list of) datetime(s) given an initial time and an (list of) hourly offset(s).\n\t\tArguments:\n\t\tt0 -- initial time\n\t\thours -- hourly offsets from t0\n\t\t\"\"\"\n\t\tif not isinstance(hours, Iterable):\n\t\t\treturn Tide._times(t0, [hours])[0]\n\t\telif not isinstance(hours[0], datetime):\n\t\t\treturn np.array([t0 + timedelta(hours=h) for h in hours])\n\t\telse:\n\t\t\treturn np.array(hours)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize(self):\n\t\tfor i, (_, amplitude, phase) in enumerate(self.model):\n\t\t\tif amplitude < 0:\n\t\t\t\tself.model['amplitude'][i] = -amplitude\n\t\t\t\tself.model['phase'][i] = phase + 180.0\n\t\t\tself.model['phase'][i] = np.mod(self.model['phase'][i], 360.0)", "response": "Normalizes the model so that amplitudes are in [ 0 360 )."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decompose(\n\t\t\tcls,\n\t\t\theights,\n\t\t\tt            = None,\n\t\t\tt0           = None,\n\t\t\tinterval     = None,\n\t\t\tconstituents = constituent.noaa,\n\t\t\tinitial      = None,\n\t\t\tn_period     = 2,\n\t\t\tcallback     = None,\n\t\t\tfull_output  = False\n\t\t):\n\t\t\"\"\"\n\t\tReturn an instance of Tide which has been fitted to a series of tidal observations.\n\t\tArguments:\n\t\tIt is not necessary to provide t0 or interval if t is provided.\n\t\theights -- ndarray of tidal observation heights\n\t\tt -- ndarray of tidal observation times\n\t\tt0 -- datetime representing the time at which heights[0] was recorded\n\t\tinterval -- hourly interval between readings\n\t\tconstituents -- list of constituents to use in the fit (default: constituent.noaa)\n\t\tinitial -- optional Tide instance to use as first guess for least squares solver\n\t\tn_period -- only include constituents which complete at least this many periods (default: 2)\n\t\tcallback -- optional function to be called at each iteration of the solver\n\t\tfull_output -- whether to return the output of scipy's leastsq solver (default: False)\n\t\t\"\"\"\n\t\tif t is not None:\n\t\t\tif isinstance(t[0], datetime):\n\t\t\t\thours = Tide._hours(t[0], t)\n\t\t\t\tt0 = t[0]\n\t\t\telif t0 is not None:\n\t\t\t\thours = t\n\t\t\telse:\n\t\t\t\traise ValueError(\"t can be an array of datetimes, or an array \"\n\t\t\t\t                 \"of hours since t0 in which case t0 must be \"\n\t\t\t\t                 \"specified.\")\n\t\telif None not in [t0, interval]:\n\t\t\thours = np.arange(len(heights)) * interval\n\t\telse:\n\t\t\traise ValueError(\"Must provide t(datetimes), or t(hours) and \"\n\t\t\t                 \"t0(datetime), or interval(hours) and t0(datetime) \"\n\t\t\t                 \"so that each height can be identified with an \"\n\t\t\t                 \"instant in time.\")\n\n\t\t#Remove duplicate constituents (those which travel at exactly the same\n\t\t#speed, irrespective of phase)\n\t\tconstituents = list(OrderedDict.fromkeys(constituents))\n\n\t\t#No need for least squares to find the mean water level constituent z0,\n\t\t#work relative to mean\n\t\tconstituents = [c for c in constituents if not c == constituent._Z0]\n\t\tz0 = np.mean(heights)\n\t\theights = heights - z0\n\n\t\t#Only analyse frequencies which complete at least n_period cycles over\n\t\t#the data period.\n\t\tconstituents = [\n\t\t\tc for c in constituents\n\t\t\tif 360.0 * n_period < hours[-1] * c.speed(astro(t0))\n\t\t]\n\t\tn = len(constituents)\n\n\t\tsort = np.argsort(hours)\n\t\thours = hours[sort]\n\t\theights = heights[sort]\n\n\t\t#We partition our time/height data into intervals over which we consider\n\t\t#the values of u and f to assume a constant value (that is, their true\n\t\t#value at the midpoint of the interval).  Constituent\n\t\t#speeds change much more slowly than the node factors, so we will\n\t\t#consider these constant and equal to their speed at t0, regardless of\n\t\t#the length of the time series.\n\n\t\tpartition = 240.0\n\n\t\tt     = Tide._partition(hours, partition)\n\t\ttimes = Tide._times(t0, [(i + 0.5)*partition for i in range(len(t))])\n\n\t\tspeed, u, f, V0 = Tide._prepare(constituents, t0, times, radians = True)\n\n\t\t#Residual to be minimised by variation of parameters (amplitudes, phases)\n\t\tdef residual(hp):\n\t\t\tH, p = hp[:n, np.newaxis], hp[n:, np.newaxis]\n\t\t\ts = np.concatenate([\n\t\t\t\tTide._tidal_series(t_i, H, p, speed, u_i, f_i, V0)\n\t\t\t\tfor t_i, u_i, f_i in izip(t, u, f)\n\t\t\t])\n\t\t\tres = heights - s\n\t\t\tif callback:\n\t\t\t\tcallback(res)\n\t\t\treturn res\n\n\t\t#Analytic Jacobian of the residual - this makes solving significantly\n\t\t#faster than just using gradient approximation, especially with many\n\t\t#measurements / constituents.\n\t\tdef D_residual(hp):\n\t\t\tH, p = hp[:n, np.newaxis], hp[n:, np.newaxis]\n\t\t\tds_dH = np.concatenate([\n\t\t\t\tf_i*np.cos(speed*t_i+u_i+V0-p)\n\t\t\t\tfor t_i, u_i, f_i in izip(t, u, f)],\n\t\t\t\taxis = 1)\n\n\t\t\tds_dp = np.concatenate([\n\t\t\t\tH*f_i*np.sin(speed*t_i+u_i+V0-p)\n\t\t\t\tfor t_i, u_i, f_i in izip(t, u, f)],\n\t\t\t\taxis = 1)\n\n\t\t\treturn np.append(-ds_dH, -ds_dp, axis=0)\n\n\t\t#Initial guess for solver, haven't done any analysis on this since the\n\t\t#solver seems to converge well regardless of the initial guess We do\n\t\t#however scale the initial amplitude guess with some measure of the\n\t\t#variation\n\t\tamplitudes = np.ones(n) * (np.sqrt(np.dot(heights, heights)) / len(heights))\n\t\tphases     = np.ones(n)\n\n\t\tif initial:\n\t\t\tfor (c0, amplitude, phase) in initial.model:\n\t\t\t\tfor i, c in enumerate(constituents):\n\t\t\t\t\tif c0 == c:\n\t\t\t\t\t\tamplitudes[i] = amplitude\n\t\t\t\t\t\tphases[i] = d2r*phase\n\n\t\tinitial = np.append(amplitudes, phases)\n\n\t\tlsq = leastsq(residual, initial, Dfun=D_residual, col_deriv=True, ftol=1e-7)\n\n\t\tmodel = np.zeros(1+n, dtype=cls.dtype)\n\t\tmodel[0] = (constituent._Z0, z0, 0)\n\t\tmodel[1:]['constituent'] = constituents[:]\n\t\tmodel[1:]['amplitude'] = lsq[0][:n]\n\t\tmodel[1:]['phase'] = lsq[0][n:]\n\n\t\tif full_output:\n\t\t\treturn cls(model = model, radians = True), lsq\n\t\treturn cls(model = model, radians = True)", "response": "This function will decompose a Tide into a series of Tide objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(s, _dict=dict):\n\n    implicitgroups = []\n    retval = _dict()\n    currentlevel = retval\n    if not isinstance(s, basestring):\n        raise TypeError(\"Expecting something like a string\")\n\n    if not isinstance(s, unicode):\n        s = s.decode('utf8')\n\n    sl = list(s)\n    openarr = 0\n    openstring = False\n    openstrchar = \"\"\n    multilinestr = False\n    arrayoftables = False\n    beginline = True\n    keygroup = False\n    keyname = 0\n    for i, item in enumerate(sl):\n        if item == '\\r' and sl[i + 1] == '\\n':\n            sl[i] = ' '\n            continue\n        if keyname:\n            if item == '\\n':\n                raise TomlDecodeError(\"Key name found without value.\"\n                                      \" Reached end of line.\")\n            if openstring:\n                if item == openstrchar:\n                    keyname = 2\n                    openstring = False\n                    openstrchar = \"\"\n                continue\n            elif keyname == 1:\n                if item.isspace():\n                    keyname = 2\n                    continue\n                elif item.isalnum() or item == '_' or item == '-':\n                    continue\n            elif keyname == 2 and item.isspace():\n                continue\n            if item == '=':\n                keyname = 0\n            else:\n                raise TomlDecodeError(\"Found invalid character in key name: '\" +\n                                      item + \"'. Try quoting the key name.\")\n        if item == \"'\" and openstrchar != '\"':\n            k = 1\n            try:\n                while sl[i - k] == \"'\":\n                    k += 1\n                    if k == 3:\n                        break\n            except IndexError:\n                pass\n            if k == 3:\n                multilinestr = not multilinestr\n                openstring = multilinestr\n            else:\n                openstring = not openstring\n            if openstring:\n                openstrchar = \"'\"\n            else:\n                openstrchar = \"\"\n        if item == '\"' and openstrchar != \"'\":\n            oddbackslash = False\n            k = 1\n            tripquote = False\n            try:\n                while sl[i - k] == '\"':\n                    k += 1\n                    if k == 3:\n                        tripquote = True\n                        break\n                if k == 1 or (k == 3 and tripquote):\n                    while sl[i - k] == '\\\\':\n                        oddbackslash = not oddbackslash\n                        k += 1\n            except IndexError:\n                pass\n            if not oddbackslash:\n                if tripquote:\n                    multilinestr = not multilinestr\n                    openstring = multilinestr\n                else:\n                    openstring = not openstring\n            if openstring:\n                openstrchar = '\"'\n            else:\n                openstrchar = \"\"\n        if item == '#' and (not openstring and not keygroup and\n                            not arrayoftables):\n            j = i\n            try:\n                while sl[j] != '\\n':\n                    sl[j] = ' '\n                    j += 1\n            except IndexError:\n                break\n        if item == '[' and (not openstring and not keygroup and\n                            not arrayoftables):\n            if beginline:\n                if sl[i + 1] == '[':\n                    arrayoftables = True\n                else:\n                    keygroup = True\n            else:\n                openarr += 1\n        if item == ']' and not openstring:\n            if keygroup:\n                keygroup = False\n            elif arrayoftables:\n                if sl[i - 1] == ']':\n                    arrayoftables = False\n            else:\n                openarr -= 1\n        if item == '\\n':\n            if openstring or multilinestr:\n                if not multilinestr:\n                    raise TomlDecodeError(\"Unbalanced quotes\")\n                if ((sl[i - 1] == \"'\" or sl[i - 1] == '\"') and (\n                        sl[i - 2] == sl[i - 1])):\n                    sl[i] = sl[i - 1]\n                    if sl[i - 3] == sl[i - 1]:\n                        sl[i - 3] = ' '\n            elif openarr:\n                sl[i] = ' '\n            else:\n                beginline = True\n        elif beginline and sl[i] != ' ' and sl[i] != '\\t':\n            beginline = False\n            if not keygroup and not arrayoftables:\n                if sl[i] == '=':\n                    raise TomlDecodeError(\"Found empty keyname. \")\n                keyname = 1\n    s = ''.join(sl)\n    s = s.split('\\n')\n    multikey = None\n    multilinestr = \"\"\n    multibackslash = False\n    for line in s:\n        if not multilinestr or multibackslash or '\\n' not in multilinestr:\n            line = line.strip()\n        if line == \"\" and (not multikey or multibackslash):\n            continue\n        if multikey:\n            if multibackslash:\n                multilinestr += line\n            else:\n                multilinestr += line\n            multibackslash = False\n            if len(line) > 2 and (line[-1] == multilinestr[0] and\n                                  line[-2] == multilinestr[0] and\n                                  line[-3] == multilinestr[0]):\n                value, vtype = _load_value(multilinestr, _dict)\n                currentlevel[multikey] = value\n                multikey = None\n                multilinestr = \"\"\n            else:\n                k = len(multilinestr) - 1\n                while k > -1 and multilinestr[k] == '\\\\':\n                    multibackslash = not multibackslash\n                    k -= 1\n                if multibackslash:\n                    multilinestr = multilinestr[:-1]\n                else:\n                    multilinestr += \"\\n\"\n            continue\n        if line[0] == '[':\n            arrayoftables = False\n            if line[1] == '[':\n                arrayoftables = True\n                line = line[2:].split(']]', 1)\n            else:\n                line = line[1:].split(']', 1)\n            if line[1].strip() != \"\":\n                raise TomlDecodeError(\"Key group not on a line by itself.\")\n            groups = line[0].split('.')\n            i = 0\n            while i < len(groups):\n                groups[i] = groups[i].strip()\n                if groups[i][0] == '\"' or groups[i][0] == \"'\":\n                    groupstr = groups[i]\n                    j = i + 1\n                    while not groupstr[0] == groupstr[-1]:\n                        j += 1\n                        groupstr = '.'.join(groups[i:j])\n                    groups[i] = groupstr[1:-1]\n                    groups[i + 1:j] = []\n                else:\n                    if not _groupname_re.match(groups[i]):\n                        raise TomlDecodeError(\"Invalid group name '\" +\n                                              groups[i] + \"'. Try quoting it.\")\n                i += 1\n            currentlevel = retval\n            for i in _range(len(groups)):\n                group = groups[i]\n                if group == \"\":\n                    raise TomlDecodeError(\"Can't have a keygroup with an empty \"\n                                          \"name\")\n                try:\n                    currentlevel[group]\n                    if i == len(groups) - 1:\n                        if group in implicitgroups:\n                            implicitgroups.remove(group)\n                            if arrayoftables:\n                                raise TomlDecodeError(\"An implicitly defined \"\n                                                      \"table can't be an array\")\n                        elif arrayoftables:\n                            currentlevel[group].append(_dict())\n                        else:\n                            raise TomlDecodeError(\"What? \" + group +\n                                                  \" already exists?\" +\n                                                  str(currentlevel))\n                except TypeError:\n                    currentlevel = currentlevel[-1]\n                    try:\n                        currentlevel[group]\n                    except KeyError:\n                        currentlevel[group] = _dict()\n                        if i == len(groups) - 1 and arrayoftables:\n                            currentlevel[group] = [_dict()]\n                except KeyError:\n                    if i != len(groups) - 1:\n                        implicitgroups.append(group)\n                    currentlevel[group] = _dict()\n                    if i == len(groups) - 1 and arrayoftables:\n                        currentlevel[group] = [_dict()]\n                currentlevel = currentlevel[group]\n                if arrayoftables:\n                    try:\n                        currentlevel = currentlevel[-1]\n                    except KeyError:\n                        pass\n        elif line[0] == \"{\":\n            if line[-1] != \"}\":\n                raise TomlDecodeError(\"Line breaks are not allowed in inline\"\n                                      \"objects\")\n            _load_inline_object(line, currentlevel, _dict, multikey,\n                                multibackslash)\n        elif \"=\" in line:\n            ret = _load_line(line, currentlevel, _dict, multikey,\n                             multibackslash)\n            if ret is not None:\n                multikey, multilinestr, multibackslash = ret\n    return retval", "response": "Parses a string as toml into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps(o, preserve=False):\n\n    retval = \"\"\n    addtoretval, sections = _dump_sections(o, \"\")\n    retval += addtoretval\n    while sections != {}:\n        newsections = {}\n        for section in sections:\n            addtoretval, addtosections = _dump_sections(sections[section],\n                                                        section, preserve)\n            if addtoretval or (not addtoretval and not addtosections):\n                if retval and retval[-2:] != \"\\n\\n\":\n                    retval += \"\\n\"\n                retval += \"[\" + section + \"]\\n\"\n                if addtoretval:\n                    retval += addtoretval\n            for s in addtosections:\n                newsections[section + \".\" + s] = addtosections[s]\n        sections = newsections\n    return retval", "response": "Stringifies input dict as toml"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump inline table in its compact syntax instead of expanding into subsection.", "response": "def _dump_inline_table(section):\n    \"\"\"Preserve inline table in its compact syntax instead of expanding\n    into subsection.\n\n    https://github.com/toml-lang/toml#user-content-inline-table\n    \"\"\"\n    retval = \"\"\n    if isinstance(section, dict):\n        val_list = []\n        for k, v in section.items():\n            val = _dump_inline_table(v)\n            val_list.append(k + \" = \" + val)\n        retval += \"{ \" + \", \".join(val_list) + \" }\\n\"\n        return retval\n    else:\n        return str(_dump_value(section))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the content of the current version for the given content and adds hashes to the latest version for the given content.", "response": "def update(cls, content, dependency, version, spec=\"==\", hashes=()):\n        \"\"\"\n        Updates the requirement to the latest version for the given content and adds hashes\n        if neccessary.\n        :param content: str, content\n        :return: str, updated content\n        \"\"\"\n        new_line = \"{name}{spec}{version}\".format(name=dependency.full_name, spec=spec, version=version)\n        appendix = ''\n        # leave environment markers intact\n        if \";\" in dependency.line:\n            # condense multiline, split out the env marker, strip comments and --hashes\n            new_line += \";\" + dependency.line.splitlines()[0].split(\";\", 1)[1] \\\n                .split(\"#\")[0].split(\"--hash\")[0].rstrip()\n        # add the comment\n        if \"#\" in dependency.line:\n            # split the line into parts: requirement and comment\n            parts = dependency.line.split(\"#\")\n            requirement, comment = parts[0], \"#\".join(parts[1:])\n            # find all whitespaces between the requirement and the comment\n            whitespaces = (hex(ord('\\t')), hex(ord(' ')))\n            trailing_whitespace = ''\n            for c in requirement[::-1]:\n                if hex(ord(c)) in whitespaces:\n                    trailing_whitespace += c\n                else:\n                    break\n            appendix += trailing_whitespace + \"#\" + comment\n        # if this is a hashed requirement, add a multiline break before the comment\n        if dependency.hashes and not new_line.endswith(\"\\\\\"):\n            new_line += \" \\\\\"\n        # if this is a hashed requirement, add the hashes\n        if hashes:\n            for n, new_hash in enumerate(hashes):\n                new_line += \"\\n    --hash={method}:{hash}\".format(\n                    method=new_hash['method'],\n                    hash=new_hash['hash']\n                )\n                # append a new multiline break if this is not the last line\n                if len(hashes) > n + 1:\n                    new_line += \" \\\\\"\n        new_line += appendix\n\n        regex = cls.SUB_REGEX.format(re.escape(dependency.line))\n\n        return re.sub(regex, new_line, content, flags=re.MULTILINE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setuptools_parse_requirements_backport(strs):  # pragma: no cover\n    # Copyright (C) 2016 Jason R Coombs <jaraco@jaraco.com>\n    #\n    # Permission is hereby granted, free of charge, to any person obtaining a copy of\n    # this software and associated documentation files (the \"Software\"), to deal in\n    # the Software without restriction, including without limitation the rights to\n    # use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n    # of the Software, and to permit persons to whom the Software is furnished to do\n    # so, subject to the following conditions:\n    #\n    # The above copyright notice and this permission notice shall be included in all\n    # copies or substantial portions of the Software.\n    #\n    # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    # SOFTWARE.\n    \"\"\"Yield ``Requirement`` objects for each specification in `strs`\n\n    `strs` must be a string, or a (possibly-nested) iterable thereof.\n    \"\"\"\n    # create a steppable iterator, so we can handle \\-continuations\n    def yield_lines(strs):\n        \"\"\"Yield non-empty/non-comment lines of a string or sequence\"\"\"\n        if isinstance(strs, six.string_types):\n            for s in strs.splitlines():\n                s = s.strip()\n                # skip blank lines/comments\n                if s and not s.startswith('#'):\n                    yield s\n        else:\n            for ss in strs:\n                for s in yield_lines(ss):\n                    yield s\n    lines = iter(yield_lines(strs))\n\n    for line in lines:\n        # Drop comments -- a hash without a space may be in a URL.\n        if ' #' in line:\n            line = line[:line.find(' #')]\n        # If there is a line continuation, drop it, and append the next line.\n        if line.endswith('\\\\'):\n            line = line[:-2].strip()\n            line += next(lines)\n        yield PackagingRequirement(line)", "response": "Yields all requirements in a string that are part of a setuptools version of the base setuptools."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self):\n        index_server = None\n        for num, line in enumerate(self.iter_lines()):\n            line = line.rstrip()\n            if not line:\n                continue\n            if line.startswith('#'):\n                # comments are lines that start with # only\n                continue\n            if line.startswith('-i') or \\\n                line.startswith('--index-url') or \\\n                line.startswith('--extra-index-url'):\n                # this file is using a private index server, try to parse it\n                index_server = self.parse_index_server(line)\n                continue\n            elif self.obj.path and (line.startswith('-r') or line.startswith('--requirement')):\n                self.obj.resolved_files.append(self.resolve_file(self.obj.path, line))\n            elif line.startswith('-f') or line.startswith('--find-links') or \\\n                line.startswith('--no-index') or line.startswith('--allow-external') or \\\n                line.startswith('--allow-unverified') or line.startswith('-Z') or \\\n                line.startswith('--always-unzip'):\n                continue\n            elif self.is_marked_line(line):\n                continue\n            else:\n                try:\n\n                    parseable_line = line\n\n                    # multiline requirements are not parseable\n                    if \"\\\\\" in line:\n                        parseable_line = line.replace(\"\\\\\", \"\")\n                        for next_line in self.iter_lines(num + 1):\n                            parseable_line += next_line.strip().replace(\"\\\\\", \"\")\n                            line += \"\\n\" + next_line\n                            if \"\\\\\" in next_line:\n                                continue\n                            break\n                        # ignore multiline requirements if they are marked\n                        if self.is_marked_line(parseable_line):\n                            continue\n\n                    hashes = []\n                    if \"--hash\" in parseable_line:\n                        parseable_line, hashes = Parser.parse_hashes(parseable_line)\n\n                    req = RequirementsTXTLineParser.parse(parseable_line)\n                    if req:\n                        req.hashes = hashes\n                        req.index_server = index_server\n                        # replace the requirements line with the 'real' line\n                        req.line = line\n                        self.obj.dependencies.append(req)\n                except ValueError:\n                    continue", "response": "Parses a requirements. txt - like file and returns a tuple of the names of the requirements and the index server that is used to create the index server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self):\n        try:\n            data = toml.loads(self.obj.content, _dict=OrderedDict)\n            if data:\n                for package_type in ['packages', 'dev-packages']:\n                    if package_type in data:\n                        for name, specs in data[package_type].items():\n                            # skip on VCS dependencies\n                            if not isinstance(specs, basestring):\n                                continue\n                            if specs == '*':\n                                specs = ''\n                            self.obj.dependencies.append(\n                                Dependency(\n                                    name=name, specs=SpecifierSet(specs),\n                                    dependency_type=filetypes.pipfile,\n                                    line=''.join([name, specs]),\n                                    section=package_type\n                                )\n                            )\n        except (toml.TomlDecodeError, IndexError) as e:\n            pass", "response": "Parse a Pipfile and add all the VCS dependencies to the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self):\n        try:\n            data = json.loads(self.obj.content, object_pairs_hook=OrderedDict)\n            if data:\n                for package_type in ['default', 'develop']:\n                    if package_type in data:\n                        for name, meta in data[package_type].items():\n                            # skip VCS dependencies\n                            if 'version' not in meta:\n                                continue\n                            specs = meta['version']\n                            hashes = meta['hashes']\n                            self.obj.dependencies.append(\n                                Dependency(\n                                    name=name, specs=SpecifierSet(specs),\n                                    dependency_type=filetypes.pipfile_lock,\n                                    hashes=hashes,\n                                    line=''.join([name, specs]),\n                                    section=package_type\n                                )\n                            )\n        except ValueError:\n            pass", "response": "Parse a Pipfile. lock file and add its dependencies to the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an INI file from path.", "response": "def _load_ini(path):\n    \"\"\"\n    Load an INI file from *path*.\n    \"\"\"\n    cfg = RawConfigParser()\n    with codecs.open(path, mode=\"r\", encoding=\"utf-8\") as f:\n        try:\n            cfg.read_file(f)\n        except AttributeError:\n            cfg.readfp(f)\n\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting env var to bool if it s not a bool in the first place.", "response": "def _env_to_bool(val):\n    \"\"\"\n    Convert *val* to a bool if it's not a bool in the first place.\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    val = val.strip().lower()\n    if val in (\"1\", \"true\", \"yes\"):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _format_help_dicts(help_dicts, display_defaults=False):\n    help_strs = []\n    for help_dict in help_dicts:\n        help_str = \"%s (%s\" % (\n            help_dict[\"var_name\"],\n            \"Required\" if help_dict[\"required\"] else \"Optional\",\n        )\n        if help_dict.get(\"default\") and display_defaults:\n            help_str += \", Default=%s)\" % help_dict[\"default\"]\n        else:\n            help_str += \")\"\n        if help_dict.get(\"help_str\"):\n            help_str += \": %s\" % help_dict[\"help_str\"]\n        help_strs.append(help_str)\n\n    return \"\\n\".join(help_strs)", "response": "Format the output of _generate_help_dicts into a str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_help_dicts(config_cls, _prefix=None):\n    help_dicts = []\n    if _prefix is None:\n        _prefix = config_cls._prefix\n    for a in attr.fields(config_cls):\n        try:\n            ce = a.metadata[CNF_KEY]\n        except KeyError:\n            continue\n        if ce.sub_cls is None:  # Base case for \"leaves\".\n            if ce.name is None:\n                var_name = \"_\".join((_prefix, a.name)).upper()\n            else:\n                var_name = ce.name\n            req = ce.default == RAISE\n            help_dict = {\"var_name\": var_name, \"required\": req}\n            if not req:\n                help_dict[\"default\"] = ce.default\n            if ce.help is not None:\n                help_dict[\"help_str\"] = ce.help\n            help_dicts.append(help_dict)\n        else:  # Construct the new prefix and recurse.\n            help_dicts += _generate_help_dicts(\n                ce.sub_cls, _prefix=\"_\".join((_prefix, a.name)).upper()\n            )\n    return help_dicts", "response": "Generate a list of dicts that can be used in building help strings for the specified config class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_help(config_cls, **kwargs):\n    try:\n        formatter = kwargs.pop(\"formatter\")\n    except KeyError:\n        formatter = _format_help_dicts\n    help_dicts = _generate_help_dicts(config_cls)\n    return formatter(help_dicts, **kwargs)", "response": "Generates a help string for a given config class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the main structure and returns the JSON response.", "response": "def render_to_json(response, request=None, **kwargs):\n    \"\"\"\n    Creates the main structure and returns the JSON response.\n    \"\"\"\n    # determine the status code\n    if hasattr(response, 'status_code'):\n        status_code = response.status_code\n    elif issubclass(type(response), Http404):\n        status_code = 404\n    elif issubclass(type(response), Exception):\n        status_code = 500\n        logger.exception(str(response), extra={'request': request})\n        \n        if settings.DEBUG:\n            import sys\n            reporter = ExceptionReporter(None, *sys.exc_info())\n            text = reporter.get_traceback_text()\n            response = HttpResponseServerError(text, content_type='text/plain')\n        else:\n            response = HttpResponseServerError(\"An error occured while processing an AJAX request.\", content_type='text/plain')\n    else:\n        status_code = 200\n\n    # creating main structure\n    data = {\n        'status': status_code,\n        'statusText': REASON_PHRASES.get(status_code, 'UNKNOWN STATUS CODE'),\n        'content': response\n    }\n\n    return JSONResponse(data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ajax(function=None, mandatory=True, **ajax_kwargs):\n    def decorator(func):\n        @wraps(func, assigned=available_attrs(func))\n        def inner(request, *args, **kwargs):\n            if mandatory and not request.is_ajax():\n                return HttpResponseBadRequest()\n\n            if request.is_ajax():\n                # return json response\n                try:\n                    return render_to_json(func(request, *args, **kwargs), request, **ajax_kwargs)\n                except Exception as exception:\n                    return render_to_json(exception, request)\n            else:\n                # return standard response\n                return func(request, *args, **kwargs)\n\n        return inner\n\n    if function:\n        return decorator(function)\n\n    return decorator", "response": "Decorator which returns a serialized version of the user s response type and translates to a serialized version of the user s response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch(self, request, *args, **kwargs):\n        ajax_kwargs = {'mandatory': self.ajax_mandatory}\n        if self.json_encoder:\n            ajax_kwargs['cls'] = self.json_encoder\n\n        return ajax(**ajax_kwargs)(super(\n            AJAXMixin, self).dispatch)(request, *args, **kwargs)", "response": "This is the main dispatch method that is used to dispatch the request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serialize_to_json(data, **kwargs):\n    kwargs['cls'] = kwargs.get('cls', LazyJSONEncoder)\n\n    return json.dumps(data, **kwargs)", "response": "A wrapper for simplejson. dumps with defaults as :\n    cls = LazyJSONEncoder"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_opendap(url):\n    '''\n    Returns True if the URL is a valid OPeNDAP URL\n\n    :param str url: URL for a remote OPeNDAP endpoint\n    '''\n    # If the server replies to a Data Attribute Structure request\n    if url.endswith('#fillmismatch'):\n        das_url = url.replace('#fillmismatch', '.das')\n    else:\n        das_url = url + '.das'\n    response = requests.get(das_url, allow_redirects=True)\n    if 'xdods-server' in response.headers:\n        return True\n    # Check if it is an access restricted ESGF thredds service\n    if response.status_code == 401 and \\\n        'text/html' in response.headers['content-type'] and \\\n            'The following URL requires authentication:' in response.text:\n        return True\n    return False", "response": "Returns True if the URL is a valid OPeNDAP URL"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattempt to parse a date formatted in ISO 8601 format", "response": "def datetime_is_iso(date_str):\n    \"\"\"Attempts to parse a date formatted in ISO 8601 format\"\"\"\n    try:\n        if len(date_str) > 10:\n            dt = isodate.parse_datetime(date_str)\n        else:\n            dt = isodate.parse_date(date_str)\n        return True, []\n    except:  # Any error qualifies as not ISO format\n        return False, ['Datetime provided is not in a valid ISO 8601 format']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kvp_convert(input_coll):\n    '''\n    Converts a list of string attributes and/or tuples into an OrderedDict.\n    If passed in an OrderedDict, function is idempotent.\n    Key/value pairs map to `first_tuple_element` -> `second_tuple_element` if\n    a tuple, or `scalar_value` -> None if not a tuple.\n\n    :param input_coll: An iterable with string and/or 2-tuple elements\n    :returns: collections.OrderedDict\n    '''\n    if isinstance(input_coll, OrderedDict):\n        return input_coll\n    else:\n        return OrderedDict((l, None) if not isinstance(l, tuple) else\n                           (l[0], l[1]) for l in input_coll)", "response": "Converts a list of string attributes and or tuples into an OrderedDict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_cdl(filename):\n    '''\n    Quick check for .cdl ascii file\n\n    Example:\n        netcdf sample_file {\n        dimensions:\n            name_strlen = 7 ;\n            time = 96 ;\n        variables:\n            float lat ;\n                lat:units = \"degrees_north\" ;\n                lat:standard_name = \"latitude\" ;\n                lat:long_name = \"station latitude\" ;\n        etc...\n\n    :param str filename: Absolute path of file to check\n    :param str data: First chuck of data from file to check\n    '''\n    if os.path.splitext(filename)[-1] != '.cdl':\n        return False\n\n    with open(filename, 'rb') as f:\n        data = f.read(32)\n    if data.startswith(b'netcdf') or b'dimensions' in data:\n        return True\n    return False", "response": "Quick check for. cdl ascii file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_checker(cls, ds_loc, checker_names, verbose, criteria,\n                    skip_checks=None, output_filename='-',\n                    output_format=['text']):\n        \"\"\"\n        Static check runner.\n\n        @param  ds_loc          Dataset location (url or file)\n        @param  checker_names   List of string names to run, should match keys of checkers dict (empty list means run all)\n        @param  verbose         Verbosity of the output (0, 1, 2)\n        @param  criteria        Determines failure (lenient, normal, strict)\n        @param  output_filename Path to the file for output\n        @param  skip_checks     Names of checks to skip\n        @param  output_format   Format of the output(s)\n\n        @returns                If the tests failed (based on the criteria)\n        \"\"\"\n        all_groups = []\n        cs = CheckSuite()\n        # using OrderedDict is important here to preserve the order\n        # of multiple datasets which may be passed in\n        score_dict = OrderedDict()\n        if not isinstance(ds_loc, six.string_types):\n            locs = ds_loc\n        # if single dataset, put in list\n        else:\n            locs = [ds_loc]\n\n        # Make sure output format is a list\n        if isinstance(output_format, six.string_types):\n            output_format = [output_format]\n\n        for loc in locs: # loop through each dataset and run specified checks\n            ds = cs.load_dataset(loc)\n\n            score_groups = cs.run(ds, skip_checks, *checker_names)\n            for group in score_groups.values():\n                all_groups.append(group[0])\n            # TODO: consider wrapping in a proper context manager instead\n            if hasattr(ds, 'close'):\n                ds.close()\n\n            if not score_groups:\n                raise ValueError(\"No checks found, please check the name of the checker(s) and that they are installed\")\n            else:\n                score_dict[loc] = score_groups\n\n        # define a score limit to truncate the ouput to the strictness level\n        # specified by the user\n        if criteria == 'normal':\n            limit = 2\n        elif criteria == 'strict':\n            limit = 1\n        elif criteria == 'lenient':\n            limit = 3\n\n        for out_fmt in output_format:\n            if out_fmt == 'text':\n                if output_filename == '-':\n                    cls.stdout_output(cs, score_dict, verbose, limit)\n                # need to redirect output from stdout since print functions are\n                # presently used to generate the standard report output\n                else:\n                    if len(output_format) > 1:\n                        # Update file name if needed\n                        output_filename = '{}.txt'.format(os.path.splitext(output_filename)[0])\n                    with io.open(output_filename, 'w', encoding='utf-8') as f:\n                        with stdout_redirector(f):\n                            cls.stdout_output(cs, score_dict, verbose, limit)\n\n            elif out_fmt == 'html':\n                # Update file name if needed\n                if len(output_format) > 1 and output_filename != '-':\n                    output_filename = '{}.html'.format(os.path.splitext(output_filename)[0])\n                cls.html_output(cs, score_dict, output_filename, ds_loc, limit)\n\n            elif out_fmt in {'json', 'json_new'}:\n                # Update file name if needed\n                if len(output_format) > 1 and output_filename != '-':\n                    output_filename = '{}.json'.format(os.path.splitext(output_filename)[0])\n                cls.json_output(cs, score_dict, output_filename, ds_loc, limit,\n                                out_fmt)\n\n            else:\n                raise TypeError('Invalid format %s' % out_fmt)\n\n            errors_occurred = cls.check_errors(score_groups, verbose)\n\n        return (all(cs.passtree(groups, limit) for groups in all_groups),\n                errors_occurred)", "response": "Static check runner.\n\n        @param  ds_loc          Dataset location (url or file)\n        @param  checker_names   List of string names to run, should match keys of checkers dict (empty list means run all)\n        @param  verbose         Verbosity of the output (0, 1, 2)\n        @param  criteria        Determines failure (lenient, normal, strict)\n        @param  output_filename Path to the file for output\n        @param  skip_checks     Names of checks to skip\n        @param  output_format   Format of the output(s)\n\n        @returns                If the tests failed (based on the criteria)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls standard output routine to display results in terminal including scoring.", "response": "def stdout_output(cls, cs, score_dict, verbose, limit):\n        '''\n        Calls output routine to display results in terminal, including scoring.\n        Goes to verbose function if called by user.\n\n        @param cs           Compliance Checker Suite\n        @param score_dict   Dict with dataset name as key, list of results as\n                            value\n        @param verbose      Integer value for verbosity level\n        @param limit        The degree of strictness, 1 being the strictest, and going up from there.\n        '''\n\n        for ds, score_groups in six.iteritems(score_dict):\n            for checker, rpair in six.iteritems(score_groups):\n                groups, errors = rpair\n                score_list, points, out_of = cs.standard_output(ds, limit,\n                                                                checker,\n                                                                groups)\n                # send list of grouped result objects to stdout & reasoning_routine\n                cs.standard_output_generation(groups, limit, points, out_of,\n                                              check=checker)\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef html_output(cls, cs, score_dict, output_filename, ds_loc, limit):\n        '''\n        Generates rendered HTML output for the compliance score(s)\n        @param cs              Compliance Checker Suite\n        @param score_groups    List of results\n        @param output_filename The file path to output to\n        @param ds_loc          List of source datasets\n        @param limit           The degree of strictness, 1 being the strictest, and going up from there.\n        '''\n        checkers_html = []\n        for ds, score_groups in six.iteritems(score_dict):\n            for checker, (groups, errors) in six.iteritems(score_groups):\n                checkers_html.append(cs.checker_html_output(checker, groups,\n                                                            ds, limit))\n\n        html = cs.html_output(checkers_html)\n        if output_filename == '-':\n            print(html)\n        else:\n            with io.open(output_filename, 'w', encoding='utf8') as f:\n                f.write(html)\n\n        return groups", "response": "Generates rendered HTML output for the compliance score."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json_output(cls, cs, score_dict, output_filename, ds_loc, limit,\n                    output_type='json'):\n        '''\n        Generates JSON output for the ocmpliance score(s)\n        @param cs              Compliance Checker Suite\n        @param score_groups    List of results\n        @param output_filename The file path to output to\n        @param ds_loc          List of source datasets\n        @param limit           The degree of strictness, 1 being the strictest,\n                               and going up from there.\n        @param output_type     Either 'json' or 'json_new'. json_new is the new\n                               json output format that supports multiple datasets\n        '''\n        results = {}\n        # json output keys out at the top level by\n        if len(score_dict) > 1 and output_type != 'json_new':\n            raise ValueError(\"output_type must be set to 'json_new' if outputting multiple datasets to a single json file or stdout\")\n\n        if output_type == 'json':\n            for ds, score_groups in six.iteritems(score_dict):\n                for checker, rpair in six.iteritems(score_groups):\n                    groups, errors = rpair\n                    results[checker] = cs.dict_output(\n                        checker, groups, ds, limit,\n                    )\n        elif output_type == 'json_new':\n            for ds, score_groups in six.iteritems(score_dict):\n                for checker, rpair in six.iteritems(score_groups):\n                    groups, errors = rpair\n                    results[ds] = {}\n                    results[ds][checker] = cs.dict_output(\n                        checker, groups, ds, limit\n                    )\n        json_results = json.dumps(results, indent=2, ensure_ascii=False)\n\n        if output_filename == '-':\n            print(json_results)\n        else:\n            with io.open(output_filename, 'w', encoding='utf8') as f:\n                f.write(json_results)\n\n        return groups", "response": "Generates a json output for the ocmpliance score of the current set of sources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if any errors occurred during a check.", "response": "def check_errors(cls, score_groups, verbose):\n        '''\n        Reports any errors (exceptions) that occurred during checking to stderr.\n        Goes to verbose function if called by user.\n\n        @param score_groups List of results\n        @param verbose      Integer value for verbosity level\n        '''\n        errors_occurred = False\n        for checker, rpair in score_groups.items():\n            errors = rpair[-1]\n            if len(errors):\n                errors_occurred = True\n                print(\"WARNING: The following exceptions occured during the %s checker (possibly indicate compliance checker issues):\" % checker, file=sys.stderr)\n                for check_name, epair in errors.items():\n                    print(\"%s.%s: %s\" % (checker, check_name, epair[0]), file=sys.stderr)\n\n                    if verbose > 0:\n                        traceback.print_tb(epair[1].tb_next.tb_next)    # skip first two as they are noise from the running itself @TODO search for check_name\n                        print(file=sys.stderr)\n\n        return errors_occurred"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _has_attr(cls, ds, attr, concept_name, priority=BaseCheck.HIGH):\n        val = cls.std_check(ds, attr)\n        msgs = []\n\n        if not val:\n            msgs.append(\"Attr '{}' (IOOS concept: '{}') not found in dataset\".format(attr, concept_name))\n\n        return Result(priority, val, concept_name, msgs)", "response": "Checks for the existence of attr in dataset using concept_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if an attribute exists on a variable vname in dataset with the name of the message using concept_name.", "response": "def _has_var_attr(cls, dataset, vname, attr, concept_name, priority=BaseCheck.HIGH):\n        \"\"\"\n        Checks for the existance of an attr on variable vname in dataset, with the name/message using concept_name.\n        \"\"\"\n        val = True\n        msgs = []\n        if vname not in dataset.variables:\n            val = False\n            msgs.append(\"Variable '{}' not present while checking for attr '{}' for IOOS concept: '{}'\".format(vname, attr, concept_name))\n        else:\n            v = dataset.variables[vname]\n            if attr not in v.ncattrs():\n                val = False\n                msgs.append(\"Attr '{}' not present on var '{}' while checking for IOOS concept: '{}'\".format(attr, vname, concept_name))\n\n        return Result(priority, val, concept_name, msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that time period attributes are both set.", "response": "def check_time_period(self, ds):\n        \"\"\"\n        Check that time period attributes are both set.\n        \"\"\"\n        start = self.std_check(ds, 'time_coverage_start')\n        end = self.std_check(ds, 'time_coverage_end')\n\n        msgs = []\n        count = 2\n        if not start:\n            count -= 1\n            msgs.append(\"Attr 'time_coverage_start' is missing\")\n        if not end:\n            count -= 1\n            msgs.append(\"Attr 'time_coverage_end' is missing\")\n\n        return Result(BaseCheck.HIGH, (count, 2), 'time coverage start/end', msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks station location lat attributes are set", "response": "def check_station_location_lat(self, ds):\n        \"\"\"\n        Checks station lat attributes are set\n        \"\"\"\n        gmin = self.std_check(ds, 'geospatial_lat_min')\n        gmax = self.std_check(ds, 'geospatial_lat_max')\n\n        msgs = []\n        count = 2\n        if not gmin:\n            count -= 1\n            msgs.append(\"Attr 'geospatial_lat_min' is missing\")\n        if not gmax:\n            count -= 1\n            msgs.append(\"Attr 'geospatial_lat_max' is missing\")\n\n        return Result(BaseCheck.HIGH, (count, 2), 'geospatial lat min/max', msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_global_attributes(self, ds):\n        return [\n            self._has_attr(ds, 'acknowledgement', 'Platform Sponsor'),\n            self._has_attr(ds, 'publisher_email', 'Station Publisher Email'),\n            self._has_attr(ds, 'publisher_email', 'Service Contact Email', BaseCheck.MEDIUM),\n            self._has_attr(ds, 'institution', 'Service Provider Name', BaseCheck.MEDIUM),\n            self._has_attr(ds, 'publisher_name', 'Service Contact Name', BaseCheck.MEDIUM),\n            self._has_attr(ds, 'Conventions', 'Data Format Template Version', BaseCheck.MEDIUM),\n            self._has_attr(ds, 'publisher_name', 'Station Publisher Name', BaseCheck.HIGH),\n        ]", "response": "Check all global attributes for existence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck IOOS concepts that come from NC variable attributes.", "response": "def check_variable_attributes(self, ds):\n        \"\"\"\n        Check IOOS concepts that come from NC variable attributes.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        \"\"\"\n        return [\n            self._has_var_attr(ds, 'platform', 'long_name', 'Station Long Name'),\n            self._has_var_attr(ds, 'platform', 'short_name', 'Station Short Name'),\n            self._has_var_attr(ds, 'platform', 'source', 'Platform Type'),\n            self._has_var_attr(ds, 'platform', 'ioos_name', 'Station ID'),\n            self._has_var_attr(ds, 'platform', 'wmo_id', 'Station WMO ID'),\n            self._has_var_attr(ds, 'platform', 'comment', 'Station Description'),\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_variable_names(self, ds):\n        msgs = []\n        count = 0\n\n        for k, v in ds.variables.items():\n            if 'standard_name' in v.ncattrs():\n                count += 1\n            else:\n                msgs.append(\"Variable '{}' missing standard_name attr\".format(k))\n\n        return Result(BaseCheck.MEDIUM, (count, len(ds.variables)), 'Variable Names', msgs)", "response": "Checks that all variables have a standard_name set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the dataset has the altitude units.", "response": "def check_altitude_units(self, ds):\n        \"\"\"\n        If there's a variable named z, it must have units.\n\n        @TODO: this is duplicated with check_variable_units\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        \"\"\"\n        if 'z' in ds.variables:\n            msgs = []\n            val = 'units' in ds.variables['z'].ncattrs()\n            if not val:\n                msgs.append(\"Variable 'z' has no units attr\")\n            return Result(BaseCheck.LOW, val, 'Altitude Units', msgs)\n\n        return Result(BaseCheck.LOW, (0, 0), 'Altitude Units', [\"Dataset has no 'z' variable\"])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the value of platform global attribute and global attribute of the cluster variables.", "response": "def check_platform_variables(self, ds):\n        '''\n        The value of platform attribute should be set to another variable which\n        contains the details of the platform. There can be multiple platforms\n        involved depending on if all the instances of the featureType in the\n        collection share the same platform or not. If multiple platforms are\n        involved, a variable should be defined for each platform and referenced\n        from the geophysical variable in a space separated string.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        platform_names = getattr(ds, 'platform', '').split(' ')\n        val = all(platform_name in ds.variables for platform_name in platform_names)\n        msgs = []\n        if not val:\n            msgs = [('The value of \"platform\" global attribute should be set to another variable '\n                     'which contains the details of the platform. If multiple platforms are '\n                     'involved, a variable should be defined for each platform and referenced '\n                     'from the geophysical variable in a space separated string.')]\n        return [Result(BaseCheck.HIGH, val, 'platform variables', msgs)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the platforms variable attributes are present in the netCDF4. Dataset ds.", "response": "def check_platform_variable_attributes(self, ds):\n        '''\n        Platform variables must contain the following attributes:\n            ioos_code\n            long_name\n            short_name\n            type\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        platform_name = getattr(ds, 'platform', '')\n        # There can be multiple platforms defined here (space separated)\n        for platform in platform_name.split(' '):\n            if platform in ds.variables:\n                results += [\n                    self._has_var_attr(ds, platform, 'long_name', 'Platform Long Name'),\n                    self._has_var_attr(ds, platform, 'short_name', 'Platform Short Name'),\n                    self._has_var_attr(ds, platform, 'ioos_code', 'Platform IOOS Code'),\n                    self._has_var_attr(ds, platform, 'type', 'Platform Type')\n                ]\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_geophysical_vars_fill_value(self, ds):\n        '''\n        Check that geophysical variables contain fill values.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        for geo_var in get_geophysical_variables(ds):\n            results.append(\n                self._has_var_attr(ds, geo_var, '_FillValue', '_FillValue', BaseCheck.MEDIUM),\n            )\n        return results", "response": "Check that geophysical variables contain fill values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_geophysical_vars_standard_name(self, ds):\n        '''\n        Check that geophysical variables contain standard names.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        for geo_var in get_geophysical_variables(ds):\n            results.append(\n                self._has_var_attr(ds, geo_var, 'standard_name', 'geophysical variables standard_name'),\n            )\n        return results", "response": "Check that geophysical variables contain standard names."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the attribute value is contained in value_set.", "response": "def attr_membership(attr_val, value_set, attr_type=basestring,\n                          modifier_fn=lambda x: x):\n    \"\"\"\n    Helper function passed to netCDF4.Dataset.get_attributes_by_value\n    Checks that `attr_val` exists, has the same type as `attr_type`,\n    and is contained in `value_set`\n    attr_val: The value of the attribute being checked\n    attr_type: A type object that the `attr_val` is expected to have the same\n               type as.  If the type is not the same, a warning is issued and\n               the code attempts to cast `attr_val` to the expected type.\n    value_set: The set against which membership for `attr_val` is tested\n    modifier_fn: A function to apply to attr_val prior to applying the set\n                 membership test\n\n    \"\"\"\n    if attr_val is None:\n        return False\n\n    if not isinstance(attr_val, attr_type):\n        warnings.warn(\"Attribute is of type {}, {} expected. \"\n                      \"Attempting to cast to expected type.\".format(type(attr_val),\n                                                                    attr_type))\n        try:\n            # if the expected type is basestring, try casting to unicode type\n            # since basestring can't be instantiated\n            if attr_type is basestring:\n                new_attr_val = six.text_type(attr_val)\n            else:\n                new_attr_val = attr_type(attr_val)\n        # catch casting errors\n        except (ValueError, UnicodeEncodeError) as e:\n            warnings.warn(\"Could not cast to type {}\".format(attr_type))\n            return False\n    else:\n        new_attr_val = attr_val\n\n    try:\n        is_in_set = modifier_fn(new_attr_val) in value_set\n    except Exception as e:\n        warnings.warn('Could not apply modifier function {} to value: '\n                      ' {}'.format(modifier_fn, e.msg))\n        return False\n\n    return is_in_set"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the units for the associated standard name are dimensionless.", "response": "def is_dimensionless_standard_name(xml_tree, standard_name):\n    '''\n    Returns True if the units for the associated standard name are\n    dimensionless.  Dimensionless standard names include those that have no\n    units and units that are defined as constant units in the CF standard name\n    table i.e. '1', or '1e-3'.\n    '''\n    # standard_name must be string, so if it is not, it is *wrong* by default\n    if not isinstance(standard_name, basestring):\n        return False\n    found_standard_name = xml_tree.find(\".//entry[@id='{}']\".format(standard_name))\n    if found_standard_name is not None:\n        canonical_units = found_standard_name.find('canonical_units')\n        # so far, standard name XML table includes\n        # 1 and 1e-3 for constant units, but expanding to valid udunits\n        # prefixes to be on the safe side\n        # taken from CF Table 3.1 of valid UDUnits prefixes\n        dimless_units = r'1(?:e-?(?:1|2|3|6|9|12|15|18|21|24))?$'\n        return canonical_units is None or re.match(dimless_units,\n                                                   canonical_units.text)\n    # if the standard name is not found, assume we need units for the time being\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sea_names():\n    '''\n    Returns a list of NODC sea names\n\n    source of list: https://www.nodc.noaa.gov/General/NODC-Archive/seanamelist.txt\n    '''\n    global _SEA_NAMES\n    if _SEA_NAMES is None:\n        buf = {}\n        with open(resource_filename('compliance_checker', 'data/seanames.csv'), 'r') as f:\n            reader = csv.reader(f)\n            for code, sea_name in reader:\n                buf[sea_name] = code\n        _SEA_NAMES = buf\n    return _SEA_NAMES", "response": "Returns a list of NODC sea names for a given NODC archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if the variable is unitless", "response": "def is_unitless(ds, variable):\n    '''\n    Returns true if the variable is unitless\n\n    Note units of '1' are considered whole numbers or parts but still represent\n    physical units and not the absence of units.\n\n    :param netCDF4.Dataset ds: An open netCDF dataset\n    :param str variable: Name of the variable\n    '''\n    units = getattr(ds.variables[variable], 'units', None)\n    return units is None or units == ''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_geophysical(ds, variable):\n    '''\n    Returns true if the dataset's variable is likely a geophysical variable\n\n    :param netCDF4.Dataset ds: An open netCDF dataset\n    :param str variable: Name of the variable\n    '''\n    ncvar = ds.variables[variable]\n\n    if getattr(ncvar, 'cf_role', None):\n        return False\n\n    # Check for axis\n    if getattr(ncvar, 'axis', None):\n        return False\n\n    standard_name_test = getattr(ncvar, 'standard_name', '')\n    unitless = is_unitless(ds, variable)\n\n    if not isinstance(standard_name_test, basestring):\n        warnings.warn(\"Variable {} has non string standard name, \"\n                      \"Attempting cast to string\".format(variable))\n        try:\n            standard_name = str(standard_name_test)\n        except ValueError:\n            warnings.warn(\"Unable to cast standard name to string, excluding \"\n                          \"from geophysical variables\")\n    else:\n        standard_name = standard_name_test\n\n    # Is the standard name associated with coordinates\n    if standard_name in {'time', 'latitude', 'longitude',\n                         'height', 'depth', 'altitude'}:\n        return False\n\n    if variable in get_coordinate_variables(ds):\n        return False\n\n    if variable in get_auxiliary_coordinate_variables(ds):\n        return False\n\n    # Is it dimensionless and unitless?\n    if len(ncvar.shape) == 0 and unitless:\n        return False\n\n    # Is it a QC Flag?\n    if 'status_flag' in standard_name or hasattr(ncvar, 'flag_meanings'):\n        return False\n\n    # Is it a \u00a77.1 Cell Boundaries variable\n    if variable in get_cell_boundary_variables(ds):\n        return False\n\n    if variable == get_climatology_variable(ds):\n        return False\n\n    # Is it a string but with no defined units?\n    if ncvar.dtype.char == 'S':\n        return False\n\n    # Is it an instrument descriptor?\n    if variable in get_instrument_variables(ds):\n        return False\n\n    # What about a platform descriptor?\n    if variable in get_platform_variables(ds):\n        return False\n\n    # Skip count variables too\n    if hasattr(ncvar, 'sample_dimension'):\n        return False\n\n    return True", "response": "Returns true if the dataset s variable is likely a geophysical variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of variable names that identify as coordinate variables.", "response": "def get_coordinate_variables(ds):\n    '''\n    Returns a list of variable names that identify as coordinate variables.\n\n    A coordinate variable is a netCDF variable with exactly one dimension. The\n    name of this dimension must be equivalent to the variable name.\n\n    From CF \u00a71.2 Terminology\n\n    It is a one-dimensional variable with the same name as its dimension [e.g.,\n    time(time) ], and it is defined as a numeric data type with values that are\n    ordered monotonically. Missing values are not allowed in coordinate\n    variables.\n\n    :param netCDF4.Dataset ds: An open netCDF dataset\n    '''\n    coord_vars = []\n    for dimension in ds.dimensions:\n        if dimension in ds.variables:\n            if ds.variables[dimension].dimensions == (dimension,):\n                coord_vars.append(dimension)\n    return coord_vars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of auxiliary coordinate variables that are not a coordinate variable.", "response": "def get_auxiliary_coordinate_variables(ds):\n    '''\n    Returns a list of auxiliary coordinate variables\n\n    An auxiliary coordinate variable is any netCDF variable that contains\n    coordinate data, but is not a coordinate variable (in the sense of the term\n    defined by CF).\n\n    :param netCDf4.Dataset ds: An open netCDF dataset\n    '''\n    aux_vars = []\n    # get any variables referecned by the coordinates attribute\n    for ncvar in ds.get_variables_by_attributes(coordinates=lambda x: isinstance(x, basestring)):\n        # split the coordinates into individual variable names\n        referenced_variables = ncvar.coordinates.split(' ')\n        # if the variable names exist, add them\n        for referenced_variable in referenced_variables:\n            if referenced_variable in ds.variables and referenced_variable not in aux_vars:\n                aux_vars.append(referenced_variable)\n\n    # axis variables are automatically in\n    for variable in get_axis_variables(ds):\n        if variable not in aux_vars:\n            aux_vars.append(variable)\n\n    # Last are any variables that define the common coordinate standard names\n    coordinate_standard_names = ['time', 'longitude', 'latitude', 'height', 'depth', 'altitude']\n    coordinate_standard_names += DIMENSIONLESS_VERTICAL_COORDINATES\n\n    # Some datasets like ROMS use multiple variables to define coordinates\n    for ncvar in ds.get_variables_by_attributes(standard_name=lambda x: x in coordinate_standard_names):\n        if ncvar.name not in aux_vars:\n            aux_vars.append(ncvar.name)\n\n    # Remove any that are purely coordinate variables\n    ret_val = []\n    for aux_var in aux_vars:\n        if ds.variables[aux_var].dimensions == (aux_var,):\n            continue\n        ret_val.append(aux_var)\n\n    return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cell_boundary_map(ds):\n    '''\n    Returns a dictionary mapping a variable to its boundary variable. The\n    returned dictionary maps a string variable name to the name of the boundary\n    variable.\n\n    :param netCDF4.Dataset nc: netCDF dataset\n    '''\n    boundary_map = {}\n    for variable in ds.get_variables_by_attributes(bounds=lambda x: x is not None):\n        if variable.bounds in ds.variables:\n            boundary_map[variable.name] = variable.bounds\n    return boundary_map", "response": "Returns a dictionary mapping a variable to its boundary variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cell_boundary_variables(ds):\n    '''\n    Returns a list of variable names for variables that represent cell\n    boundaries through the `bounds` attribute\n\n    :param netCDF4.Dataset nc: netCDF dataset\n    '''\n    boundary_variables = []\n    has_bounds = ds.get_variables_by_attributes(bounds=lambda x: x is not None)\n    for var in has_bounds:\n        if var.bounds in ds.variables:\n            boundary_variables.append(var.bounds)\n    return boundary_variables", "response": "Returns a list of variable names for variables that represent cell\n    boundaries through the bounds attribute\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of variable names for the variables detected as geophysical variables.", "response": "def get_geophysical_variables(ds):\n    '''\n    Returns a list of variable names for the variables detected as geophysical\n    variables.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    '''\n\n    parameters = []\n    for variable in ds.variables:\n        if is_geophysical(ds, variable):\n            parameters.append(variable)\n    return parameters"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_z_variable(nc):\n    '''\n    Returns the name of the variable that defines the Z axis or height/depth\n\n    :param netCDF4.Dataset nc: netCDF dataset\n    '''\n    z_variables = get_z_variables(nc)\n    if not z_variables:\n        return None\n\n    # Priority is standard_name, units\n    for var in z_variables:\n        ncvar = nc.variables[var]\n        if getattr(ncvar, 'standard_name', None) in ('depth', 'height', 'altitude'):\n            return var\n\n    for var in z_variables:\n        ncvar = nc.variables[var]\n        units = getattr(ncvar, 'units', None)\n        if isinstance(units, basestring):\n            if units_convertible(units, 'bar'):\n                return var\n            if units_convertible(units, 'm'):\n                return var\n\n    return z_variables[0]", "response": "Returns the name of the Z variable that defines the Z axis or height or altitude or depth."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_z_variables(nc):\n    '''\n    Returns a list of all variables matching definitions for Z\n\n    :param netcdf4.dataset nc: an open netcdf dataset object\n    '''\n    z_variables = []\n    # Vertical coordinates will be identifiable by units of pressure or the\n    # presence of the positive attribute with a value of up/down\n\n    # optionally, the vertical type may be indicated by providing the\n    # standard_name attribute or axis='Z'\n\n    total_coords = get_coordinate_variables(nc) + get_auxiliary_coordinate_variables(nc)\n    for coord_name in total_coords:\n        if coord_name in z_variables:\n            continue\n        coord_var = nc.variables[coord_name]\n        units = getattr(coord_var, 'units', None)\n        positive = getattr(coord_var, 'positive', None)\n        standard_name = getattr(coord_var, 'standard_name', None)\n        axis = getattr(coord_var, 'axis', None)\n        # If there are no units, we can't identify it as a vertical coordinate\n        # by checking pressure or positive\n        if units is not None:\n            if units_convertible(units, 'bar'):\n                z_variables.append(coord_name)\n            elif isinstance(positive, basestring):\n                if positive.lower() in ['up', 'down']:\n                    z_variables.append(coord_name)\n        # if axis='Z' we're good\n        if coord_name not in z_variables and axis == 'Z':\n            z_variables.append(coord_name)\n        if coord_name not in z_variables and standard_name in ('depth', 'height', 'altitude'):\n            z_variables.append(coord_name)\n        if coord_name not in z_variables and standard_name in DIMENSIONLESS_VERTICAL_COORDINATES:\n            z_variables.append(coord_name)\n\n    return z_variables", "response": "Returns a list of all variables matching definitions for Z\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_latitude_variables(nc):\n    '''\n    Returns a list of all variables matching definitions for latitude\n\n    :param netcdf4.dataset nc: an open netcdf dataset object\n    '''\n    latitude_variables = []\n    # standard_name takes precedence\n    for variable in nc.get_variables_by_attributes(standard_name=\"latitude\"):\n        latitude_variables.append(variable.name)\n\n    # Then axis\n    for variable in nc.get_variables_by_attributes(axis='Y'):\n        if variable.name not in latitude_variables:\n            latitude_variables.append(variable.name)\n\n    check_fn = partial(attr_membership, value_set=VALID_LAT_UNITS,\n                           modifier_fn=lambda s: s.lower())\n    for variable in nc.get_variables_by_attributes(units=check_fn):\n        if variable.name not in latitude_variables:\n            latitude_variables.append(variable.name)\n\n    return latitude_variables", "response": "Returns a list of all variables matching definitions for latitude\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_true_latitude_variables(nc):\n    '''\n    Returns a list of variables defining true latitude.\n\n    CF Chapter 4 refers to latitude as a coordinate variable that can also be\n    used in non-standard coordinate systems like rotated pole and other\n    projections. Chapter 5 refers to a concept of true latitude where the\n    variabe defines latitude in a standard projection.\n\n    True latitude, for lack of a better definition, is simply latitude where\n    the standard_name is latitude or the units are degrees_north.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    '''\n    lats = get_latitude_variables(nc)\n    true_lats = []\n    for lat in lats:\n        standard_name = getattr(nc.variables[lat], \"standard_name\", None)\n        units = getattr(nc.variables[lat], \"units\", None)\n        if standard_name == 'latitude':\n            true_lats.append(lat)\n        elif isinstance(units, basestring) and units.lower() in VALID_LAT_UNITS:\n            true_lats.append(lat)\n    return true_lats", "response": "Returns a list of variables defining true latitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all variables matching definitions for longitude", "response": "def get_longitude_variables(nc):\n    '''\n    Returns a list of all variables matching definitions for longitude\n\n    :param netcdf4.dataset nc: an open netcdf dataset object\n    '''\n    longitude_variables = []\n    # standard_name takes precedence\n    for variable in nc.get_variables_by_attributes(standard_name=\"longitude\"):\n        longitude_variables.append(variable.name)\n\n    # Then axis\n    for variable in nc.get_variables_by_attributes(axis='X'):\n        if variable.name not in longitude_variables:\n            longitude_variables.append(variable.name)\n\n    check_fn = partial(attr_membership, value_set=VALID_LON_UNITS,\n                       modifier_fn=lambda s: s.lower())\n    for variable in nc.get_variables_by_attributes(units=check_fn):\n        if variable.name not in longitude_variables:\n            longitude_variables.append(variable.name)\n\n    return longitude_variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_true_longitude_variables(nc):\n    '''\n    Returns a list of variables defining true longitude.\n\n    CF Chapter 4 refers to longitude as a coordinate variable that can also be\n    used in non-standard coordinate systems like rotated pole and other\n    projections. Chapter 5 refers to a concept of true longitude where the\n    variabe defines longitude in a standard projection.\n\n    True longitude, for lack of a better definition, is simply longitude where\n    the standard_name is longitude or the units are degrees_north.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    '''\n    lons = get_longitude_variables(nc)\n    true_lons = []\n    for lon in lons:\n        standard_name = getattr(nc.variables[lon], \"standard_name\", None)\n        units = getattr(nc.variables[lon], \"units\", None)\n        if standard_name == 'longitude':\n            true_lons.append(lon)\n        elif isinstance(units, basestring) and units.lower() in VALID_LON_UNITS:\n            true_lons.append(lon)\n    return true_lons", "response": "Returns a list of variables defining true longitude."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of platform variable NAMES", "response": "def get_platform_variables(ds):\n    '''\n    Returns a list of platform variable NAMES\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    candidates = []\n    for variable in ds.variables:\n        platform = getattr(ds.variables[variable], 'platform', '')\n        if platform and platform in ds.variables:\n            if platform not in candidates:\n                candidates.append(platform)\n\n    platform = getattr(ds, 'platform', '')\n    if platform and platform in ds.variables:\n        if platform not in candidates:\n            candidates.append(platform)\n    return candidates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_instrument_variables(ds):\n    '''\n    Returns a list of instrument variables\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    candidates = []\n    for variable in ds.variables:\n        instrument = getattr(ds.variables[variable], 'instrument', '')\n        if instrument and instrument in ds.variables:\n            if instrument not in candidates:\n                candidates.append(instrument)\n\n    instrument = getattr(ds, 'instrument', '')\n    if instrument and instrument in ds.variables:\n        if instrument not in candidates:\n            candidates.append(instrument)\n    return candidates", "response": "Returns a list of instrument variables in the dataset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_time_variable(ds):\n    '''\n    Returns the likeliest variable to be the time coordiante variable\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    for var in ds.variables:\n        if getattr(ds.variables[var], 'axis', '') == 'T':\n            return var\n    else:\n        candidates = ds.get_variables_by_attributes(standard_name='time')\n        if len(candidates) == 1:\n            return candidates[0].name\n        else:  # Look for a coordinate variable time\n            for candidate in candidates:\n                if candidate.dimensions == (candidate.name,):\n                    return candidate.name\n\n    # If we still haven't found the candidate\n    time_variables = set(get_time_variables(ds))\n    coordinate_variables = set(get_coordinate_variables(ds))\n    if len(time_variables.intersection(coordinate_variables)) == 1:\n        return list(time_variables.intersection(coordinate_variables))[0]\n\n    auxiliary_coordinates = set(get_auxiliary_coordinate_variables(ds))\n    if len(time_variables.intersection(auxiliary_coordinates)) == 1:\n        return list(time_variables.intersection(auxiliary_coordinates))[0]\n    return None", "response": "Returns the likeliest variable to be the time coordiante variable"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of variables describing the time coordinate", "response": "def get_time_variables(ds):\n    '''\n    Returns a list of variables describing the time coordinate\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    time_variables = set()\n    for variable in ds.get_variables_by_attributes(standard_name='time'):\n        time_variables.add(variable.name)\n\n    for variable in ds.get_variables_by_attributes(axis='T'):\n        if variable.name not in time_variables:\n            time_variables.add(variable.name)\n\n    regx = r'^(?:day|d|hour|hr|h|minute|min|second|s)s? since .*$'\n    for variable in ds.get_variables_by_attributes(units=lambda x: isinstance(x, basestring)):\n        if re.match(regx, variable.units) and variable.name not in time_variables:\n            time_variables.add(variable.name)\n\n    return time_variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_axis_variables(ds):\n    '''\n    Returns a list of variables that define an axis of the dataset\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    axis_variables = []\n    for ncvar in ds.get_variables_by_attributes(axis=lambda x: x is not None):\n        axis_variables.append(ncvar.name)\n    return axis_variables", "response": "Returns a list of variables that define an axis of the dataset\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_climatology_variable(ds):\n    '''\n    Returns the variable describing climatology bounds if it exists.\n\n    Climatology variables are similar to cell boundary variables that describe\n    the climatology bounds.\n\n    See Example 7.8 in CF 1.6\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    :rtype: str or None\n    '''\n    time = get_time_variable(ds)\n    # If there's no time dimension there's no climatology bounds\n    if not time:\n        return None\n    # Climatology variable is simply whatever time points to under the\n    # `climatology` attribute.\n    if hasattr(ds.variables[time], 'climatology'):\n        if ds.variables[time].climatology in ds.variables:\n            return ds.variables[time].climatology\n    return None", "response": "Returns the variable describing the climatology bounds if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_flag_variables(ds):\n    '''\n    Returns a list of variables that are defined as flag variables\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    flag_variables = []\n    for name, ncvar in ds.variables.items():\n        standard_name = getattr(ncvar, 'standard_name', None)\n        if isinstance(standard_name, basestring) and 'status_flag' in standard_name:\n            flag_variables.append(name)\n        elif hasattr(ncvar, 'flag_meanings'):\n            flag_variables.append(name)\n    return flag_variables", "response": "Returns a list of variables that are defined as flag variables\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of grid mapping variables that are present in the dataset", "response": "def get_grid_mapping_variables(ds):\n    '''\n    Returns a list of grid mapping variables\n\n    :param netCDF4.Dataset ds: An open netCDF4 Dataset\n    '''\n    grid_mapping_variables = []\n    for ncvar in ds.get_variables_by_attributes(grid_mapping=lambda x: x is not None):\n        if ncvar.grid_mapping in ds.variables:\n            grid_mapping_variables.append(ncvar.grid_mapping)\n    return grid_mapping_variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_axis_map(ds, variable):\n    '''\n    Returns an axis_map dictionary that contains an axis key and the coordinate\n    names as values.\n\n    For example::\n\n        {'X': ['longitude'], 'Y': ['latitude'], 'T': ['time']}\n\n    The axis C is for compressed coordinates like a reduced grid, and U is for\n    unknown axis. This can sometimes be physical quantities representing a\n    continuous discrete axis, like temperature or density.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: Variable name\n    '''\n    all_coords = get_coordinate_variables(ds) + get_auxiliary_coordinate_variables(ds)\n\n    latitudes = get_latitude_variables(ds)\n    longitudes = get_longitude_variables(ds)\n    times = get_time_variables(ds)\n    heights = get_z_variables(ds)\n\n    coordinates = getattr(ds.variables[variable], \"coordinates\", None)\n    if not isinstance(coordinates, basestring):\n        coordinates = ''\n\n    # For example\n    # {'x': ['longitude'], 'y': ['latitude'], 't': ['time']}\n    axis_map = defaultdict(list)\n    for coord_name in all_coords:\n        if is_compression_coordinate(ds, coord_name):\n            axis = 'C'\n        elif coord_name in times:\n            axis = 'T'\n        elif coord_name in longitudes:\n            axis = 'X'\n        elif coord_name in latitudes:\n            axis = 'Y'\n        elif coord_name in heights:\n            axis = 'Z'\n        else:\n            axis = 'U'\n\n        if coord_name in ds.variables[variable].dimensions:\n            if coord_name not in axis_map[axis]:\n                axis_map[axis].append(coord_name)\n\n        elif coord_name in coordinates:\n            if coord_name not in axis_map[axis]:\n                axis_map[axis].append(coord_name)\n    return axis_map", "response": "Returns a dictionary that contains an axis key and the coordinate names as values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_coordinate_variable(ds, variable):\n    '''\n    Returns True if the variable is a coordinate variable\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: Variable name\n    '''\n    if variable not in ds.variables:\n        return False\n    return ds.variables[variable].dimensions == (variable,)", "response": "Returns True if the variable is a coordinate variable"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_compression_coordinate(ds, variable):\n    '''\n    Returns True if the variable is a coordinate variable that defines a\n    compression scheme.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: Variable name\n    '''\n    # Must be a coordinate variable\n    if not is_coordinate_variable(ds, variable):\n        return False\n    # must have a string attribute compress\n    compress = getattr(ds.variables[variable], 'compress', None)\n    if not isinstance(compress, basestring):\n        return False\n    if not compress:\n        return False\n    # This should never happen or be allowed\n    if variable in compress:\n        return False\n    # Must point to dimensions\n    for dim in compress.split():\n        if dim not in ds.dimensions:\n            return False\n    return True", "response": "Returns True if the variable is a coordinate variable that defines a compression scheme."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coordinate_dimension_matrix(nc):\n    '''\n    Returns a dictionary of coordinates mapped to their dimensions\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    '''\n    retval = {}\n    x = get_lon_variable(nc)\n    if x:\n        retval['x'] = nc.variables[x].dimensions\n    y = get_lat_variable(nc)\n    if y:\n        retval['y'] = nc.variables[y].dimensions\n\n    z = get_z_variable(nc)\n    if z:\n        retval['z'] = nc.variables[z].dimensions\n\n    t = get_time_variable(nc)\n    if t:\n        retval['t'] = nc.variables[t].dimensions\n    return retval", "response": "Returns a dictionary of coordinates mapped to their dimensions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_point(nc, variable):\n    '''\n    Returns true if the variable is a point feature type\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(o), y(o), z(o), t(o)\n    # X(o)\n\n    dims = nc.variables[variable].dimensions\n\n    cmatrix = coordinate_dimension_matrix(nc)\n    first_coord = None\n    if 't' in cmatrix:\n        first_coord = cmatrix['t']\n        if len(cmatrix['t']) > 1:\n            return False\n    if 'x' in cmatrix:\n        if first_coord is None:\n            first_coord = cmatrix['x']\n        if first_coord != cmatrix['x']:\n            return False\n        if len(cmatrix['x']) > 1:\n            return False\n    if 'y' in cmatrix:\n        if first_coord is None:\n            first_coord = cmatrix['y']\n        if first_coord != cmatrix['y']:\n            return False\n        if len(cmatrix['y']) > 1:\n            return False\n    if 'z' in cmatrix:\n        if first_coord is None:\n            first_coord = cmatrix['z']\n        if first_coord != cmatrix['z']:\n            return False\n        if len(cmatrix['z']) > 1:\n            return False\n    if first_coord and dims != first_coord:\n        return False\n    # Point is indistinguishable from trajectories where the instance dimension\n    # is implied (scalar)\n    traj_ids = nc.get_variables_by_attributes(cf_role=\"trajectory_id\")\n    if traj_ids:\n        return False\n\n    return True", "response": "Returns true if the variable is a point feature type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn true if the variable is a time series feature type.", "response": "def is_timeseries(nc, variable):\n    '''\n    Returns true if the variable is a time series feature type.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n\n    # x, y, z, t(t)\n    # X(t)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n    time_variables = get_time_variables(nc)\n\n    if len(dims) != 1:\n        return False\n    dim = dims[0]\n    if dim not in time_variables:\n        return False\n    # No other coordinates can vary with time\n    if 'x' in cmatrix:\n        if len(cmatrix['x']) != 0:\n            return False\n    if 'y' in cmatrix:\n        if len(cmatrix['y']) != 0:\n            return False\n    if 'z' in cmatrix:\n        if len(cmatrix['z']) != 0:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns true if the variable is a orthogonal multidimensional array representation of time series.", "response": "def is_multi_timeseries_orthogonal(nc, variable):\n    '''\n    Returns true if the variable is a orthogonal multidimensional array\n    representation of time series. For more information on what this means see\n    CF 1.6 \u00a7H.2.1\n\n    http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html#_orthogonal_multidimensional_array_representation_of_time_series\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(i), y(i), z(i), t(o)\n    # X(i, o)\n    dims = nc.variables[variable].dimensions\n\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 't'):\n        if req not in cmatrix:\n            return False\n    if len(cmatrix['x']) != 1 or cmatrix['x'] != cmatrix['y']:\n        return False\n    if 'z' in cmatrix and cmatrix['x'] != cmatrix['z']:\n        return False\n\n    timevar = get_time_variable(nc)\n    if cmatrix['t'] != (timevar,):\n        return False\n\n    i = cmatrix['x'][0]\n    o = cmatrix['t'][0]\n    if dims == (i, o):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_multi_timeseries_incomplete(nc, variable):\n    '''\n    Returns true if the variable is an incomplete multidimensional array\n    representation of time series. For more information on what this means see\n    CF 1.6 \u00a7H.2.2\n\n    http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html#_incomplete_multidimensional_array_representation_of_time_series\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n\n    # x(i), y(i), z(i), t(i, o)\n    # X(i, o)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 't'):\n        if req not in cmatrix:\n            return False\n    if len(cmatrix['x']) != 1:\n        return False\n    if cmatrix['x'] != cmatrix['y']:\n        return False\n    if len(cmatrix['t']) != 2:\n        return False\n    if cmatrix['x'][0] != cmatrix['t'][0]:\n        return False\n\n    i = cmatrix['x'][0]\n    o = cmatrix['t'][1]\n\n    if dims == (i, o):\n        return True\n    return False", "response": "Returns True if the variable is an incomplete multidimensional array representation of time series."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_cf_trajectory(nc, variable):\n    '''\n    Returns true if the variable is a CF trajectory feature type\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(i, o), y(i, o), z(i, o), t(i, o)\n    # X(i, o)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 't'):\n        if req not in cmatrix:\n            return False\n    if len(cmatrix['x']) != 2:\n        return False\n    if cmatrix['x'] != cmatrix['y']:\n        return False\n    if cmatrix['x'] != cmatrix['t']:\n        return False\n    if 'z' in cmatrix and cmatrix['x'] != cmatrix['z']:\n        return False\n    if dims == cmatrix['x']:\n        return True\n    return False", "response": "Returns true if the variable is a CF trajectory feature type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the variable is a single trajectory feature", "response": "def is_single_trajectory(nc, variable):\n    '''\n    Returns true if the variable is a single trajectory feature\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(o), y(o), z(o), t(o)\n    # X(o)\n    # cf_role must be trajectory\n    dims = nc.variables[variable].dimensions\n\n    cmatrix = coordinate_dimension_matrix(nc)\n    # Time is required for trajectories\n    if 't' not in cmatrix:\n        return False\n    # Coordinates changing with time are optional\n    if 'x' in cmatrix:\n        if cmatrix['x'] != cmatrix['t']:\n            return False\n    if 'y' in cmatrix:\n        if cmatrix['y'] != cmatrix['t']:\n            return False\n    if 'z' in cmatrix:\n        if cmatrix['z'] != cmatrix['t']:\n            return False\n    if dims != cmatrix['t']:\n        return False\n    traj_ids = nc.get_variables_by_attributes(cf_role=\"trajectory_id\")\n    if len(traj_ids) != 1:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true if the variable is a time - series profile that represents a single station and each profile is the same length.", "response": "def is_timeseries_profile_single_station(nc, variable):\n    '''\n    Returns true if the variable is a time-series profile that represents a\n    single station and each profile is the same length.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n\n    # x, y, z(z), t(t)\n    # X(t, z)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 'z', 't'):\n        if req not in cmatrix:\n            return False\n    if len(cmatrix['x']) != 0:\n        return False\n    if cmatrix['x'] != cmatrix['y']:\n        return False\n\n    z = get_z_variable(nc)\n    if cmatrix['z'] != (z,):\n        return False\n    t = get_time_variable(nc)\n    if cmatrix['t'] != (t,):\n        return False\n\n    if dims == (t, z):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_timeseries_profile_ortho_depth(nc, variable):\n    '''\n    Returns true if the variable is a time-series profile with orthogonal depth\n    only.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(i), y(i), z(z), t(i, j)\n    # X(i, j, z)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 'z', 't'):\n        if req not in cmatrix:\n            return False\n\n    if len(cmatrix['x']) != 1:\n        return False\n    if cmatrix['x'] != cmatrix['y']:\n        return False\n\n    z = get_z_variable(nc)\n    if cmatrix['z'] != (z,):\n        return False\n\n    i = cmatrix['x'][0]\n\n    if len(cmatrix['t']) != 2:\n        return False\n    if cmatrix['t'][0] != i:\n        return False\n\n    j = cmatrix['t'][1]\n\n    if dims == (i, j, z):\n        return True\n    return False", "response": "Returns true if the variable is a time - series profile with orthogonal depth\n    only."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the variable is a trajectory profile with orthogonal depths.", "response": "def is_trajectory_profile_orthogonal(nc, variable):\n    '''\n    Returns true if the variable is a trajectory profile with orthogonal\n    depths.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(i, o), y(i, o), z(z), t(i, o)\n    # X(i, o, z)\n    dims = nc.variables[variable].dimensions\n    cmatrix = coordinate_dimension_matrix(nc)\n\n    for req in ('x', 'y', 'z', 't'):\n        if req not in cmatrix:\n            return False\n\n    if len(cmatrix['x']) != 2:\n        return False\n    if cmatrix['x'] != cmatrix['y']:\n        return False\n    if cmatrix['x'] != cmatrix['t']:\n        return False\n\n    i, o = cmatrix['x']\n\n    z = get_z_variable(nc)\n    if cmatrix['z'] != (z,):\n        return False\n\n    if dims == (i, o, z):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_mapped_grid(nc, variable):\n    '''\n    Returns true if the feature-type of variable corresponds to a mapped grid\n    type. Characterized by Appedix F of CF-1.6\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    # x(j, i), y(j, i), z?, t?\n    # F(t?, z?, j, i)\n\n    # The important and really defining characteristic of mapped grids is that\n    # the true latitude and longitude coordinates are functions of (j,i) and\n    # that the geophysical variables are also functions of (j,i) in their\n    # dimensions.\n    dims = nc.variables[variable].dimensions\n    # For cases like ROMS, the coordinates are mapped using the coordinates attribute\n    variable_coordinates = getattr(nc.variables[variable], 'coordinates', '').split()\n\n    lons = get_longitude_variables(nc)\n    for lon in lons:\n        if lon in variable_coordinates:\n            break\n    else:\n        lon = get_lon_variable(nc)\n\n    if lon is None:\n        return False\n\n    lats = get_latitude_variables(nc)\n    for lat in lats:\n        if lat in variable_coordinates:\n            break\n    else:\n        lat = get_lat_variable(nc)\n\n    if lat is None:\n        return False\n\n    x = nc.variables[lon].dimensions\n    y = nc.variables[lat].dimensions\n\n    if len(x) != 2:\n        return False\n    if x != y:\n        return False\n\n    comma_dimension = ','.join(dims)\n    # Dimensions must be in the same order and the mapping coordinates i and j\n    # must be in the same order\n    if ','.join(x) not in comma_dimension:\n        return False\n\n    return True", "response": "Returns true if the feature - type of variable corresponds to a mapped grid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_reduced_grid(nc, variable):\n    '''\n    Returns True if the feature-type of the variable corresponds to a reduced\n    horizontal grid.\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    axis_map = get_axis_map(nc, variable)\n\n    if 'X' not in axis_map:\n        return False\n    if 'Y' not in axis_map:\n        return False\n    if 'C' not in axis_map:\n        return False\n\n    compressed_coordinates = axis_map['C']\n    if len(compressed_coordinates) > 1:\n        return False\n    compressed_coordinate = axis_map['C'][0]\n    for dim in nc.variables[compressed_coordinate].compress.split():\n        if dim not in nc.dimensions:\n            return False\n    return True", "response": "Returns True if the feature - type of the variable corresponds to a reduced\n    horizontal grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nguess the feature type of the variable in the current dataset.", "response": "def guess_feature_type(nc, variable):\n    '''\n    Returns a string describing the feature type for this variable\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: name of the variable to check\n    '''\n    if is_point(nc, variable):\n        return 'point'\n    if is_timeseries(nc, variable):\n        return 'timeseries'\n    if is_multi_timeseries_orthogonal(nc, variable):\n        return 'multi-timeseries-orthogonal'\n    if is_multi_timeseries_incomplete(nc, variable):\n        return 'multi-timeseries-incomplete'\n    if is_cf_trajectory(nc, variable):\n        return 'cf-trajectory'\n    if is_single_trajectory(nc, variable):\n        return 'single-trajectory'\n    if is_profile_orthogonal(nc, variable):\n        return 'profile-orthogonal'\n    if is_profile_incomplete(nc, variable):\n        return 'profile-incomplete'\n    if is_timeseries_profile_single_station(nc, variable):\n        return 'timeseries-profile-single-station'\n    if is_timeseries_profile_multi_station(nc, variable):\n        return 'timeseries-profile-multi-station'\n    if is_timeseries_profile_single_ortho_time(nc, variable):\n        return 'timeseries-profile-single-ortho-time'\n    if is_timeseries_profile_multi_ortho_time(nc, variable):\n        return 'timeseries-profile-multi-ortho-time'\n    if is_timeseries_profile_ortho_depth(nc, variable):\n        return 'timeseries-profile-ortho-depth'\n    if is_timeseries_profile_incomplete(nc, variable):\n        return 'timeseries-profile-incomplete'\n    if is_trajectory_profile_orthogonal(nc, variable):\n        return 'trajectory-profile-orthogonal'\n    if is_trajectory_profile_incomplete(nc, variable):\n        return 'trajectory-profile-incomplete'\n    if is_2d_regular_grid(nc, variable):\n        return '2d-regular-grid'\n    if is_2d_static_grid(nc, variable):\n        return '2d-static-grid'\n    if is_3d_regular_grid(nc, variable):\n        return '3d-regular-grid'\n    if is_3d_static_grid(nc, variable):\n        return '3d-static-grid'\n    if is_mapped_grid(nc, variable):\n        return 'mapped-grid'\n    if is_reduced_grid(nc, variable):\n        return 'reduced-grid'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if a Unit representing the string units1 can be converted to a Unit representing the string units2 else False.", "response": "def units_convertible(units1, units2, reftimeistime=True):\n    \"\"\"\n    Return True if a Unit representing the string units1 can be converted\n    to a Unit representing the string units2, else False.\n\n    :param str units1: A string representing the units\n    :param str units2: A string representing the units\n    \"\"\"\n    try:\n        u1 = Unit(units1)\n        u2 = Unit(units2)\n    except ValueError:\n        return False\n    return u1.is_convertible(u2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self, ds):\n        self._find_coord_vars(ds)\n        self._find_aux_coord_vars(ds)\n        self._find_ancillary_vars(ds)\n        self._find_clim_vars(ds)\n        self._find_boundary_vars(ds)\n        self._find_metadata_vars(ds)\n        self._find_cf_standard_name_table(ds)\n        self._find_geophysical_vars(ds)", "response": "Initialize various special variable types within the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the cf standard name table in the netCDF file.", "response": "def _find_cf_standard_name_table(self, ds):\n        '''\n        Parse out the `standard_name_vocabulary` attribute and download that\n        version of the cf standard name table.  If the standard name table has\n        already been downloaded, use the cached version.  Modifies `_std_names`\n        attribute to store standard names.  Returns True if the file exists and\n        False if it fails to download.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: bool\n        '''\n        # Get the standard name vocab\n        standard_name_vocabulary = getattr(ds, 'standard_name_vocabulary', '')\n\n        # Try to parse this attribute to get version\n        version = None\n        try:\n            if 'cf standard name table' in standard_name_vocabulary.lower():\n                version = [s.strip('(').strip(')').strip('v').strip(',') for s in standard_name_vocabulary.split()]\n                # This assumes that table version number won't start with 0.\n                version = [s for s in version if s.isdigit() and len(s) <= 2 and not s.startswith('0')]\n                if len(version) > 1:\n                    return False\n                else:\n                    version = version[0]\n            else:\n                # Can't parse the attribute, use the packaged version\n                return False\n        # usually raised from .lower() with an incompatible (non-string)\n        # data type\n        except AttributeError:\n            warn(\"Cannot convert standard name table to lowercase.  This can \"\n                 \"occur if a non-string standard_name_vocabulary global \"\n                 \"attribute is supplied\")\n            return False\n\n        if version.startswith('v'):  # i.e 'v34' -> '34' drop the v\n            version = version[1:]\n\n        # If the packaged version is what we're after, then we're good\n        if version == self._std_names._version:\n            print(\"Using packaged standard name table v{0}\".format(version), file=sys.stderr)\n            return False\n\n        # Try to download the version specified\n        try:\n            data_directory = util.create_cached_data_dir()\n            location = os.path.join(data_directory, 'cf-standard-name-table-test-{0}.xml'.format(version))\n            # Did we already download this before?\n            if not os.path.isfile(location):\n                util.download_cf_standard_name_table(version, location)\n                print(\"Using downloaded standard name table v{0}\".format(version), file=sys.stderr)\n            else:\n                print(\"Using cached standard name table v{0} from {1}\".format(version, location), file=sys.stderr)\n\n            self._std_names = util.StandardNameTable(location)\n            return True\n        except Exception as e:\n            # There was an error downloading the CF table. That's ok, we'll just use the packaged version\n            warn(\"Problem fetching standard name table:\\n{0}\\n\"\n                 \"Using packaged v{1}\".format(e, self._std_names._version))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_coord_vars(self, ds, refresh=False):\n        '''\n        Returns a list of variable names that identify as coordinate variables.\n\n        The result is cached by the passed in dataset object inside of this\n        checker. Pass refresh=True to redo the cached value.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: A list of variables names (str) that are defined as coordinate\n                 variables in the dataset ds.\n        '''\n        if ds in self._coord_vars and refresh is False:\n            return self._coord_vars[ds]\n\n        self._coord_vars[ds] = cfutil.get_coordinate_variables(ds)\n\n        return self._coord_vars[ds]", "response": "Returns a list of variable names that identify as coordinate variables in the dataset ds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of auxiliary coordinate variables that are defined to be auxiliary variable names.", "response": "def _find_aux_coord_vars(self, ds, refresh=False):\n        '''\n        Returns a list of auxiliary coordinate variables\n\n        An auxiliary coordinate variable is any netCDF variable that contains\n        coordinate data, but is not a coordinate variable (in the sense of the term\n        defined by CF).\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: List of variable names (str) that are defined to be auxiliary\n                 coordinate variables.\n        '''\n        if self._aux_coords.get(ds, None) and refresh is False:\n            return self._aux_coords[ds]\n\n        self._aux_coords[ds] = cfutil.get_auxiliary_coordinate_variables(ds)\n        return self._aux_coords[ds]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of variable names that are defined as ancillary variables in the dataset ds.", "response": "def _find_ancillary_vars(self, ds, refresh=False):\n        '''\n        Returns a list of variable names that are defined as ancillary\n        variables in the dataset ds.\n\n        An ancillary variable generally is a metadata container and referenced\n        from other variables via a string reference in an attribute.\n\n        - via ancillary_variables (3.4)\n        - \"grid mapping var\" (5.6)\n        - TODO: more?\n\n        The result is cached by the passed in dataset object inside of this\n        checker. Pass refresh=True to redo the cached value.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: List of variable names (str) that are defined as ancillary\n                 variables in the dataset ds.\n        '''\n\n        # Used the cached version if it exists and is not empty\n        if self._ancillary_vars.get(ds, None) and refresh is False:\n            return self._ancillary_vars[ds]\n\n        # Invalidate the cache at all costs\n        self._ancillary_vars[ds] = []\n\n        for name, var in ds.variables.items():\n            if hasattr(var, 'ancillary_variables'):\n                for anc_name in var.ancillary_variables.split(\" \"):\n                    if anc_name in ds.variables:\n                        self._ancillary_vars[ds].append(anc_name)\n\n            if hasattr(var, 'grid_mapping'):\n                gm_name = var.grid_mapping\n                if gm_name in ds.variables:\n                    self._ancillary_vars[ds].append(gm_name)\n\n        return self._ancillary_vars[ds]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of netCDF variable names that are likely metadata variables.", "response": "def _find_metadata_vars(self, ds, refresh=False):\n        '''\n        Returns a list of netCDF variable instances for those that are likely metadata variables\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return:   List of variable names (str) that are likely metadata\n                   variable candidates.\n\n        '''\n        if self._metadata_vars.get(ds, None) and refresh is False:\n            return self._metadata_vars[ds]\n\n        self._metadata_vars[ds] = []\n        for name, var in ds.variables.items():\n\n            if name in self._find_ancillary_vars(ds) or name in self._find_coord_vars(ds):\n                continue\n\n            if name in ('platform_name', 'station_name', 'instrument_name', 'station_id', 'platform_id', 'surface_altitude'):\n                self._metadata_vars[ds].append(name)\n\n            elif getattr(var, 'cf_role', '') != '':\n                self._metadata_vars[ds].append(name)\n\n            elif getattr(var, 'standard_name', None) is None and len(var.dimensions) == 0:\n                self._metadata_vars[ds].append(name)\n\n        return self._metadata_vars[ds]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of geophysical variables.", "response": "def _find_geophysical_vars(self, ds, refresh=False):\n        '''\n        Returns a list of geophysical variables.  Modifies\n        `self._geophysical_vars`\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: A list containing strings with geophysical variable\n                 names.\n        '''\n        if self._geophysical_vars.get(ds, None) and refresh is False:\n            return self._geophysical_vars[ds]\n\n        self._geophysical_vars[ds] = cfutil.get_geophysical_variables(ds)\n\n        return self._geophysical_vars[ds]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_clim_vars(self, ds, refresh=False):\n        '''\n        Returns a list of variables that are likely to be climatology variables based on CF \u00a77.4\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: A list containing strings with geophysical variable\n                 names.\n        '''\n\n        if self._clim_vars.get(ds, None) and refresh is False:\n            return self._clim_vars[ds]\n\n        climatology_variable = cfutil.get_climatology_variable(ds)\n        if climatology_variable:\n            self._clim_vars[ds].append(climatology_variable)\n\n        return self._clim_vars[ds]", "response": "Returns a list of variables that are likely to be climatology variables based on CF \u00a77. 4\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_boundary_vars(self, ds, refresh=False):\n        '''\n        Returns dictionary of boundary variables mapping the variable instance\n        to the name of the variable acting as a boundary variable.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: A list containing strings with boundary variable names.\n        '''\n        if self._boundary_vars.get(ds, None) and refresh is False:\n            return self._boundary_vars[ds]\n\n        self._boundary_vars[ds] = cfutil.get_cell_boundary_variables(ds)\n\n        return self._boundary_vars[ds]", "response": "Returns a list of strings with the name of the variable acting as a boundary variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the data types of all netCDF variables to ensure they are valid.", "response": "def check_data_types(self, ds):\n        '''\n        Checks the data type of all netCDF variables to ensure they are valid\n        data types under CF.\n\n        CF \u00a72.2 The netCDF data types char, byte, short, int, float or real, and\n        double are all acceptable\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        fails = []\n        total = len(ds.variables)\n\n        for k, v in ds.variables.items():\n            if (v.dtype.kind != 'S' and\n                all(v.dtype.type != t for t in\n                                    (np.character, np.dtype('|S1'),\n                                     np.dtype('b'), np.dtype('i2'),\n                                     np.dtype('i4'), np.float32, np.double))):\n                fails.append('The variable {} failed because the datatype is {}'.format(k, v.datatype))\n        return Result(BaseCheck.HIGH, (total - len(fails), total), self.section_titles[\"2.2\"], msgs=fails)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the data types of the child variables of the current object are equivalent.", "response": "def check_child_attr_data_types(self, ds):\n        \"\"\"\n        For any variables which contain any of the following attributes:\n            - valid_min/valid_max\n            - valid_range\n            - scale_factor\n            - add_offset\n            - _FillValue\n        the data type of the attribute must match the type of its parent variable as specified in the\n        NetCDF User Guide (NUG) https://www.unidata.ucar.edu/software/netcdf/docs/attribute_conventions.html,\n        referenced in the CF Conventions in Section 2.5.2\n        (http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/cf-conventions.html#missing-data)\n\n        :param netCDF4.Dataset ds: open netCDF dataset object\n        :rtype: compliance_checker.base.Result\n        \"\"\"\n\n        ctx = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.5'])\n        special_attrs = {\"actual_range\", \"actual_min\", \"actual_max\",\n                         \"valid_min\", \"valid_max\", \"valid_range\",\n                         \"scale_factor\", \"add_offset\", \"_FillValue\"}\n\n        for var_name, var in ds.variables.items():\n            for att in special_attrs.intersection(var.ncattrs()):\n                val = var.getncattr(att)\n                if isinstance(val, basestring):\n                    type_match = var.dtype.kind == 'S'\n                    val_type = type(val)\n                else:\n                    val_type = val.dtype.type\n                    type_match = val_type == var.dtype.type\n\n                ctx.assert_true(type_match,\n                    \"Attribute '{}' (type: {}) and parent variable '{}' (type: {}) \"\n                    \"must have equivalent datatypes\".format(att,\n                                                            val_type,\n                                                            var_name,\n                                                            var.dtype.type)\n                    )\n        return ctx.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the variable names to ensure they are valid CF variable names under CF.", "response": "def check_naming_conventions(self, ds):\n        '''\n        Checks the variable names to ensure they are valid CF variable names under CF.\n\n        CF \u00a72.3 Variable, dimension and attribute names should begin with a letter\n        and be composed of letters, digits, and underscores.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        ret_val = []\n        variable_naming  = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.3'])\n        dimension_naming = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.3'])\n        attribute_naming = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.3'])\n\n        ignore_attributes = [\n            '_FillValue',\n            'DODS',\n            '_ChunkSizes',\n            '_Coordinate',\n            '_Unsigned'\n        ]\n\n        rname = regex.compile(\"^[A-Za-z][A-Za-z0-9_]*$\")\n\n        for name, variable in ds.variables.items():\n            variable_naming.assert_true(rname.match(name) is not None,\n                                        \"variable {} should begin with a letter and be composed of \"\n                                        \"letters, digits, and underscores\".format(name))\n\n            # Keep track of all the attributes, we'll need to check them\n            for attr in variable.ncattrs():\n                if attr in ignore_attributes:\n                    continue\n                # Special attributes made by THREDDS\n                if attr.startswith('DODS'):\n                    continue\n                # Ignore model produced attributes\n                if attr.startswith('_Coordinate'):\n                    continue\n                attribute_naming.assert_true(rname.match(attr) is not None,\n                                             \"attribute {}:{} should begin with a letter and be composed of \"\n                                             \"letters, digits, and underscores\".format(name, attr))\n\n        ret_val.append(variable_naming.to_result())\n\n        for dimension in ds.dimensions:\n            dimension_naming.assert_true(rname.match(dimension) is not None,\n                                         \"dimension {} should begin with a latter and be composed of \"\n                                         \"letters, digits, and underscores\".format(dimension))\n        ret_val.append(dimension_naming.to_result())\n\n        for global_attr in ds.ncattrs():\n            if global_attr.startswith('DODS'):\n                continue\n            attribute_naming.assert_true(rname.match(global_attr) is not None,\n                                         \"global attribute {} should begin with a letter and be composed of \"\n                                         \"letters, digits, and underscores\".format(global_attr))\n        ret_val.append(attribute_naming.to_result())\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the variable names for uniqueness regardless of case.", "response": "def check_names_unique(self, ds):\n        '''\n        Checks the variable names for uniqueness regardless of case.\n\n        CF \u00a72.3 names should not be distinguished purely by case, i.e., if case\n        is disregarded, no two names should be the same.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        fails = []\n        total = len(ds.variables)\n        names = defaultdict(int)\n\n        for k in ds.variables:\n            names[k.lower()] += 1\n\n        fails = ['Variables are not case sensitive. Duplicate variables named: %s' % k for k, v in names.items() if v > 1]\n        return Result(BaseCheck.MEDIUM, (total - len(fails), total), self.section_titles['2.3'], msgs=fails)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_dimension_names(self, ds):\n        '''\n        Checks variables contain no duplicate dimension names.\n\n        CF \u00a72.4 A variable may have any number of dimensions, including zero,\n        and the dimensions must all have different names.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        fails = []\n        total = len(ds.variables)\n\n        for k, v in ds.variables.items():\n            dims = defaultdict(int)\n            for d in v.dimensions:\n                dims[d] += 1\n\n            for dimension, count in dims.items():\n                if count > 1:\n                    fails.append(\"%s has two or more dimensions named %s\" % (k, dimension))\n\n        return Result(BaseCheck.HIGH, (total - len(fails), total), self.section_titles['2.4'], msgs=fails)", "response": "Checks variables contain no duplicate dimension names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_dimension_order(self, ds):\n        '''\n        Checks each variable's dimension order to ensure that the order is\n        consistent and in order under CF \u00a72.4\n\n        CF \u00a72.4 If any or all of the dimensions of a variable have the\n        interpretations of \"date or time\" (T), \"height or depth\" (Z),\n        \"latitude\" (Y), or \"longitude\" (X) then we recommend, those dimensions\n        to appear in the relative order T, then Z, then Y, then X in the CDL\n        definition corresponding to the file. All other dimensions should,\n        whenever possible, be placed to the left of the spatiotemporal\n        dimensions.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        valid_dimension_order = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.4'])\n        # Build a map from coordinate variable to axis\n        coord_axis_map = self._get_coord_axis_map(ds)\n\n        # Check each variable's dimension order, excluding climatology and\n        # bounds variables\n        any_clim = cfutil.get_climatology_variable(ds)\n        any_bounds = cfutil.get_cell_boundary_variables(ds)\n        for name, variable in ds.variables.items():\n            # Skip bounds/climatology variables, as they should implicitly\n            # have the same order except for the bounds specific dimension.\n            # This is tested later in the respective checks\n            if name in any_bounds or name == any_clim:\n                continue\n\n            # Skip strings/labels\n            if hasattr(variable.dtype, 'char') and variable.dtype.char == 'S':\n                continue\n            elif variable.dtype == str:\n                continue\n\n            if variable.dimensions:\n                dimension_order = self._get_dimension_order(ds, name, coord_axis_map)\n                valid_dimension_order.assert_true(self._dims_in_order(dimension_order),\n                                                  \"{}'s dimensions are not in the recommended order \"\n                                                  \"T, Z, Y, X. They are {}\"\n                                                  \"\".format(name, self._get_pretty_dimension_order(ds, name)))\n        return valid_dimension_order.to_result()", "response": "Checks each variable s dimension order to ensure that the order is consistent with the order under CF 2. 4."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_coord_axis_map(self, ds):\n        '''\n        Returns a dictionary mapping each coordinate to a letter identifier\n        describing the _kind_ of coordinate.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n\n        :rtype: dict\n        :return: A dictionary with variable names mapped to axis abbreviations,\n                 i.e. {'longitude': 'X', ... 'pressure': 'Z'}\n        '''\n        expected = ['T', 'Z', 'Y', 'X']\n        coord_vars = self._find_coord_vars(ds)\n        coord_axis_map = {}\n\n        # L - Unlimited Coordinates\n        # T - Time coordinates\n        # Z - Depth/Altitude Coordinate\n        # Y - Y-Coordinate (latitude)\n        # X - X-Coordinate (longitude)\n        # A - Auxiliary Coordinate\n        # I - Instance Coordinate\n\n        time_variables = cfutil.get_time_variables(ds)\n        lat_variables = cfutil.get_latitude_variables(ds)\n        lon_variables = cfutil.get_longitude_variables(ds)\n        z_variables = cfutil.get_z_variables(ds)\n\n        for coord_name in coord_vars:\n            coord_var = ds.variables[coord_name]\n            axis = getattr(coord_var, 'axis', None)\n            standard_name = getattr(coord_var, 'standard_name', None)\n\n            # Unlimited dimensions must come first\n            if ds.dimensions[coord_name].isunlimited():\n                coord_axis_map[coord_name] = 'L'\n            # axis takes precedence over standard_name\n            elif axis in expected:\n                coord_axis_map[coord_name] = axis\n            elif standard_name == 'time':\n                coord_axis_map[coord_name] = 'T'\n            elif standard_name == 'longitude':\n                coord_axis_map[coord_name] = 'X'\n            elif standard_name == 'latitude':\n                coord_axis_map[coord_name] = 'Y'\n            elif standard_name in ['height', 'depth', 'altitude']:\n                coord_axis_map[coord_name] = 'Z'\n            elif cfutil.is_compression_coordinate(ds, coord_name):\n                coord_axis_map[coord_name] = 'C'\n            elif coord_name in time_variables:\n                coord_axis_map[coord_name] = 'T'\n            elif coord_name in z_variables:\n                coord_axis_map[coord_name] = 'Z'\n            elif coord_name in lat_variables:\n                coord_axis_map[coord_name] = 'Y'\n            elif coord_name in lon_variables:\n                coord_axis_map[coord_name] = 'X'\n            else:\n                # mark the coordinate variable as unknown\n                coord_axis_map[coord_name] = 'U'\n\n        for dimension in self._get_instance_dimensions(ds):\n            if dimension not in coord_axis_map:\n                coord_axis_map[dimension] = 'I'\n\n        # Dimensions of auxiliary coordinate variables will be marked with A.\n        # This is useful to help determine if the dimensions are used like a\n        # mapping from grid coordinates to physical lat/lon\n        for coord_name in self._find_aux_coord_vars(ds):\n            coord_var = ds.variables[coord_name]\n            # Skip label auxiliary coordinates\n            if coord_var.dtype.char == 'S':\n                continue\n            for dimension in coord_var.dimensions:\n                if dimension not in coord_axis_map:\n                    coord_axis_map[dimension] = 'A'\n\n        # If a dimension does not have a coordinate variable mark it as unknown\n        # 'U'\n        for dimension in ds.dimensions:\n            if dimension not in coord_axis_map:\n                coord_axis_map[dimension] = 'U'\n\n        return coord_axis_map", "response": "Returns a dictionary mapping each coordinate to a letter identifier that describes the _kind_ of coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of dimensions marked as instance dimensions", "response": "def _get_instance_dimensions(self, ds):\n        '''\n        Returns a list of dimensions marked as instance dimensions\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n\n        :rtype: list\n        :returns: A list of variable dimensions\n        '''\n        ret_val = []\n        for variable in ds.get_variables_by_attributes(cf_role=lambda x: isinstance(x, basestring)):\n            if variable.ndim > 0:\n                ret_val.append(variable.dimensions[0])\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_pretty_dimension_order(self, ds, name):\n        '''\n        Returns a comma seperated string of the dimensions for a specified\n        variable\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: A string with a valid NetCDF variable name for the\n                         dataset\n        :rtype: str\n        :return: A comma separated string of the variable's dimensions\n        '''\n        dim_names = []\n        for dim in ds.variables[name].dimensions:\n            dim_name = dim\n            if ds.dimensions[dim].isunlimited():\n                dim_name += ' (Unlimited)'\n            dim_names.append(dim_name)\n        return ', '.join(dim_names)", "response": "Returns a comma seperated string of the dimensions for a specified variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of strings corresponding to the named axis of the dimensions for a variable.", "response": "def _get_dimension_order(self, ds, name, coord_axis_map):\n        '''\n        Returns a list of strings corresponding to the named axis of the dimensions for a variable.\n\n        Example::\n            self._get_dimension_order(ds, 'temperature', coord_axis_map)\n            --> ['T', 'Y', 'X']\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: Name of the variable\n        :param dict coord_axis_map: A dictionary mapping each coordinate variable and dimension to a named axis\n\n        :rtype: list\n        :return: A list of strings corresponding to the named axis of the dimensions for a variable\n        '''\n\n        retval = []\n        variable = ds.variables[name]\n        for dim in variable.dimensions:\n            retval.append(coord_axis_map[dim])\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dims_in_order(self, dimension_order):\n        '''\n        :param list dimension_order: A list of axes\n        :rtype: bool\n        :return: Returns True if the dimensions are in order U*, T, Z, Y, X,\n                 False otherwise\n        '''\n        regx = regex.compile(r'^[^TZYX]*T?Z?Y?X?$')\n        dimension_string = ''.join(dimension_order)\n        return regx.match(dimension_string) is not None", "response": "Returns True if the dimensions are in order U T Z Y X False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the global attribute conventions to contain CF - 1. 6. 1 the string value CF - 1. 6", "response": "def check_conventions_are_cf_16(self, ds):\n        '''\n        Check the global attribute conventions to contain CF-1.6\n\n        CF \u00a72.6.1 the NUG defined global attribute Conventions to the string\n        value \"CF-1.6\"\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n\n        valid = False\n        reasoning = []\n        if hasattr(ds, 'Conventions'):\n            conventions = regex.split(r',|\\s+', getattr(ds, 'Conventions', ''))\n            for convention in conventions:\n                if convention == 'CF-1.6':\n                    valid = True\n                    break\n            else:\n                reasoning = ['\u00a72.6.1 Conventions global attribute does not contain '\n                             '\"CF-1.6\". The CF Checker only supports CF-1.6 '\n                             'at this time.']\n        else:\n            valid = False\n            reasoning = ['\u00a72.6.1 Conventions field is not present']\n        return Result(BaseCheck.MEDIUM, valid, self.section_titles['2.6'], msgs=reasoning)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_convention_globals(self, ds):\n        '''\n        Check the common global attributes are strings if they exist.\n\n        CF \u00a72.6.2 title/history global attributes, must be strings. Do not need\n        to exist.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of Results\n        '''\n        attrs = ['title', 'history']\n\n        valid_globals = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.6'])\n\n        for attr in attrs:\n            dataset_attr = getattr(ds, attr, None)\n            is_string = isinstance(dataset_attr, basestring)\n            valid_globals.assert_true(is_string and len(dataset_attr),\n                                      \"\u00a72.6.2 global attribute {} should exist and be a non-empty string\" # subsection message\n                                      \"\".format(attr))\n        return valid_globals.to_result()", "response": "Check the common global attributes are strings if they exist. Do not need to have a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the variable and global attributes are strings for recommended attributes under CF \u00a72. 6. 2. 2 and CF \u00a72. 6. 2 institution source references and comment are non - empty strings.", "response": "def check_convention_possibly_var_attrs(self, ds):\n        \"\"\"\n        Check variable and global attributes are strings for recommended attributes under CF \u00a72.6.2\n\n        CF \u00a72.6.2 institution, source, references, and comment, either global\n        or assigned to individual variables.  When an attribute appears both\n        globally and as a variable attribute, the variable's version has\n        precedence.  Must be strings.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of Results\n        \"\"\"\n        # The attrs are optional and only needs to be a string and non-empty if it\n        # exists.\n        attrs = ['institution', 'source', 'references', 'comment']\n\n        valid_attributes = TestCtx(BaseCheck.MEDIUM, self.section_titles['2.6'])\n\n        attr_bin = set()\n        # If the attribute is defined for any variable, check it and mark in\n        # the set that we've seen it at least once.\n        for name, variable in ds.variables.items():\n            for attribute in variable.ncattrs():\n                varattr = getattr(variable, attribute)\n                if attribute in attrs:\n                    is_string = isinstance(varattr, basestring)\n                    valid_attributes.assert_true(is_string and len(varattr) > 0,\n                                                 \"\u00a72.6.2 {}:{} should be a non-empty string\"\n                                                 \"\".format(name, attribute))\n                    attr_bin.add(attribute)\n\n        # Check all the global attributes too and mark if we've seen them\n        for attribute in ds.ncattrs():\n            dsattr = getattr(ds, attribute)\n            if attribute in attrs:\n                is_string = isinstance(dsattr, basestring)\n                valid_attributes.assert_true(is_string and len(dsattr) > 0,\n                                             \"\u00a72.6.2 {} global attribute should be a non-empty string\"\n                                             \"\".format(attribute))\n                attr_bin.add(attribute)\n        return valid_attributes.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_units(self, ds):\n        '''\n        Check the units attribute for all variables to ensure they are CF\n        compliant under CF \u00a73.1\n\n        CF \u00a73.1 The units attribute is required for all variables that represent dimensional quantities\n        (except for boundary variables defined in Section 7.1, \"Cell Boundaries\" and climatology variables\n        defined in Section 7.4, \"Climatological Statistics\").\n\n        Units are not required for dimensionless quantities. A variable with no units attribute is assumed\n        to be dimensionless. However, a units attribute specifying a dimensionless unit may optionally be\n        included.\n\n        - units required\n        - type must be recognized by udunits\n        - if standard name specified, must be consistent with standard name table, must also be consistent with a\n          specified cell_methods attribute if present\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n\n        ret_val = []\n\n        coordinate_variables = self._find_coord_vars(ds)\n        auxiliary_coordinates = self._find_aux_coord_vars(ds)\n        geophysical_variables = self._find_geophysical_vars(ds)\n        unit_required_variables = coordinate_variables + auxiliary_coordinates + geophysical_variables\n\n        for name in set(unit_required_variables):\n            # For reduced horizontal grids, the compression index variable does\n            # not require units.\n            if cfutil.is_compression_coordinate(ds, name):\n                continue\n\n            variable = ds.variables[name]\n\n            # Skip instance coordinate variables\n            if getattr(variable, 'cf_role', None) is not None:\n                continue\n\n            # Skip labels\n            if variable.dtype.char == 'S':\n                continue\n\n            standard_name = getattr(variable, 'standard_name', None)\n            standard_name, standard_name_modifier = self._split_standard_name(standard_name)\n\n            units = getattr(variable, 'units', None)\n\n            valid_units = self._check_valid_cf_units(ds, name)\n            ret_val.append(valid_units)\n\n            units_attr_is_string = TestCtx(BaseCheck.MEDIUM, self.section_titles['3.1'])\n\n            # side effects, but better than teasing out the individual result\n            if units_attr_is_string.assert_true(\n                isinstance(units, basestring),\n                \"units ({}) attribute of '{}' must be a string compatible with UDUNITS\".format(units, variable.name)\n            ):\n                valid_udunits = self._check_valid_udunits(ds, name)\n                ret_val.append(valid_udunits)\n            ret_val.append(units_attr_is_string.to_result())\n\n            if isinstance(standard_name, basestring):\n                valid_standard_units = self._check_valid_standard_units(ds,\n                                                                        name)\n                ret_val.append(valid_standard_units)\n\n        return ret_val", "response": "Checks the units attribute for all variables that represent dimensional quantities and are not required for dimensionless quantities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit the standard_name into the standard_name and modifier as strings.", "response": "def _split_standard_name(self, standard_name):\n        '''\n        Returns a tuple of the standard_name and standard_name modifier\n\n        Nones are used to represent the absence of a modifier or standard_name\n\n        :rtype: tuple\n        :return: 2-tuple of standard_name and modifier as strings\n        '''\n\n        if isinstance(standard_name, basestring) and ' ' in standard_name:\n            return standard_name.split(' ', 1)\n        # if this isn't a string, then it doesn't make sense to split\n        # -- treat value as standard name with no modifier\n        else:\n            return standard_name, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_valid_cf_units(self, ds, variable_name):\n        '''\n        Checks that the variable contains units attribute, the attribute is a\n        string and the value is not deprecated by CF\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str variable_name: Name of the variable to be checked\n        :rtype:\n        :return: List of results\n        '''\n\n        # This list is straight from section 3\n        deprecated = ['level', 'layer', 'sigma_level']\n        variable = ds.variables[variable_name]\n\n        units = getattr(variable, 'units', None)\n        standard_name_full = getattr(variable, 'standard_name', None)\n        standard_name, standard_name_modifier = self._split_standard_name(standard_name_full)\n        std_name_units_dimensionless = cfutil.is_dimensionless_standard_name(self._std_names._root,\n                                                               standard_name)\n        # Is this even in the database? also, if there is no standard_name,\n        # there's no way to know if it is dimensionless.\n        should_be_dimensionless = (variable.dtype.char == 'S' or\n                                   std_name_units_dimensionless or\n                                   standard_name is None)\n\n        # 1) Units must exist\n        valid_units = TestCtx(BaseCheck.HIGH, self.section_titles['3.1'])\n        valid_units.assert_true(should_be_dimensionless or units is not None,\n                                'units attribute is required for {} when variable is not a dimensionless quantity'.format(variable_name))\n\n        # Don't bother checking the rest\n        if units is None and not should_be_dimensionless:\n            return valid_units.to_result()\n        # 2) units attribute must be a string\n        valid_units.assert_true(should_be_dimensionless or isinstance(units, basestring),\n                                'units attribute for {} needs to be a string'.format(variable_name))\n\n        # 3) units are not deprecated\n        valid_units.assert_true(units not in deprecated,\n                                'units for {}, \"{}\" are deprecated by CF 1.6'.format(variable_name, units))\n\n        return valid_units.to_result()", "response": "Checks that the variable contains units attribute and that the value is not deprecated by CF\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_valid_udunits(self, ds, variable_name):\n        '''\n        Checks that the variable's units are contained in UDUnits\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str variable_name: Name of the variable to be checked\n        '''\n        variable = ds.variables[variable_name]\n\n        units = getattr(variable, 'units', None)\n        standard_name = getattr(variable, 'standard_name', None)\n        standard_name, standard_name_modifier = self._split_standard_name(standard_name)\n        std_name_units_dimensionless = cfutil.is_dimensionless_standard_name(self._std_names._root,\n                                                               standard_name)\n\n        # If the variable is supposed to be dimensionless, it automatically passes\n        should_be_dimensionless = (variable.dtype.char == 'S' or\n                                   std_name_units_dimensionless)\n\n        valid_udunits = TestCtx(BaseCheck.HIGH, self.section_titles[\"3.1\"])\n        are_udunits = (units is not None and util.units_known(units))\n        valid_udunits.assert_true(should_be_dimensionless or are_udunits,\n                                  'units for {}, \"{}\" are not recognized by UDUNITS'.format(variable_name, units))\n        return valid_udunits.to_result()", "response": "Checks that the variable s units are contained in UDUnits"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_valid_standard_units(self, ds, variable_name):\n        '''\n        Checks that the variable's units are appropriate for the standard name\n        according to the CF standard name table and coordinate sections in CF\n        1.6\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str variable_name: Name of the variable to be checked\n        '''\n        variable = ds.variables[variable_name]\n        units = getattr(variable, 'units', None)\n        standard_name = getattr(variable, 'standard_name', None)\n\n        valid_standard_units = TestCtx(BaseCheck.HIGH, self.section_titles[\"3.1\"])\n\n        # If the variable is supposed to be dimensionless, it automatically passes\n        std_name_units_dimensionless = cfutil.is_dimensionless_standard_name(self._std_names._root,\n                                                               standard_name)\n\n        standard_name, standard_name_modifier = self._split_standard_name(standard_name)\n\n        standard_entry = self._std_names.get(standard_name, None)\n        if standard_entry is not None:\n            canonical_units = standard_entry.canonical_units\n        else:\n            # Any unit comparisons with None returns False\n            canonical_units = None\n\n        # Other standard_name modifiers have the same units as the\n        # unmodified standard name or are not checked for units.\n\n        if standard_name_modifier == 'number_of_observations':\n            canonical_units = '1'\n\n        # This section represents the different cases where simple udunits\n        # comparison isn't comprehensive enough to determine if the units are\n        # appropriate under CF\n\n        # UDUnits accepts \"s\" as a unit of time but it should be <unit> since <epoch>\n        if standard_name == 'time':\n            valid_standard_units.assert_true(util.units_convertible(units, 'seconds since 1970-01-01'),\n                                             'time must be in a valid units format <unit> since <epoch> '\n                                             'not {}'.format(units))\n\n        # UDunits can't tell the difference between east and north facing coordinates\n        elif standard_name == 'latitude':\n            # degrees is allowed if using a transformed grid\n            allowed_units = cfutil.VALID_LAT_UNITS | {'degrees'}\n            valid_standard_units.assert_true(units.lower() in allowed_units,\n                                             'variables defining latitude (\"{}\") must use degrees_north '\n                                             'or degrees if defining a transformed grid. Currently '\n                                             '{}'.format(variable_name, units))\n        # UDunits can't tell the difference between east and north facing coordinates\n        elif standard_name == 'longitude':\n            # degrees is allowed if using a transformed grid\n            allowed_units = cfutil.VALID_LON_UNITS | {'degrees'}\n            valid_standard_units.assert_true(units.lower() in allowed_units,\n                                             'variables defining longitude (\"{}\") must use degrees_east '\n                                             'or degrees if defining a transformed grid. Currently '\n                                             '{}'.format(variable_name, units))\n        # Standard Name table agrees the unit should be dimensionless\n        elif std_name_units_dimensionless:\n            valid_standard_units.assert_true(True, '')\n\n        elif canonical_units is not None:\n            valid_standard_units.assert_true(util.units_convertible(canonical_units, units),\n                                             'units for variable {} must be convertible to {} '\n                                             'currently they are {}'.format(variable_name, canonical_units, units))\n\n        return valid_standard_units.to_result()", "response": "Checks that the variable s units are appropriate for the standard name table and coordinate sections in CF - 3. 1."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the standard_name attribute of a variables object meets CF - 3. 3.", "response": "def check_standard_name(self, ds):\n        '''\n        Check a variables's standard_name attribute to ensure that it meets CF\n        compliance.\n\n        CF \u00a73.3 A standard name is associated with a variable via the attribute\n        standard_name which takes a string value comprised of a standard name\n        optionally followed by one or more blanks and a standard name modifier\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        coord_vars = self._find_coord_vars(ds)\n        aux_coord_vars = self._find_aux_coord_vars(ds)\n        axis_vars = cfutil.get_axis_variables(ds)\n        flag_vars = cfutil.get_flag_variables(ds)\n        geophysical_vars = self._find_geophysical_vars(ds)\n\n        variables_requiring_standard_names = coord_vars + aux_coord_vars + axis_vars + flag_vars + geophysical_vars\n        for name in set(variables_requiring_standard_names):\n            # Compression indices used in reduced horizontal grids or\n            # compression schemes do not require attributes other than compress\n            if cfutil.is_compression_coordinate(ds, name):\n                continue\n\n            ncvar = ds.variables[name]\n\n            # \u00a79 doesn't explicitly allow instance variables as coordinates but\n            # it's loosely implied. Just in case, skip it.\n            if hasattr(ncvar, 'cf_role'):\n                continue\n\n            # Unfortunately, \u00a76.1 allows for string types to be listed as\n            # coordinates.\n            if ncvar.dtype.char == 'S':\n                continue\n\n            standard_name = getattr(ncvar, 'standard_name', None)\n            standard_name, standard_name_modifier = self._split_standard_name(standard_name)\n            long_name = getattr(ncvar, 'long_name', None)\n            long_or_std_name = TestCtx(BaseCheck.HIGH, self.section_titles['3.3'])\n            if long_name is not None:\n                long_name_present = True\n                long_or_std_name.assert_true(isinstance(long_name, basestring),\n                                             \"Attribute long_name for variable {} must be a string\".format(name))\n            else:\n                long_name_present = False\n            # \u00a71.3 The long_name and standard_name attributes are used to\n            # describe the content of each variable. For backwards\n            # compatibility with COARDS neither is required, but use of at\n            # least one of them is strongly recommended.\n\n            # If standard_name is not defined but long_name is, don't continue\n            # the check for this variable\n            if standard_name is not None:\n                standard_name_present = True\n                valid_std_name = TestCtx(BaseCheck.HIGH, self.section_titles['3.3'])\n                valid_std_name.assert_true(isinstance(standard_name, basestring),\n                                        \"Attribute standard_name for variable {} must be a string\".format(name))\n                if isinstance(standard_name, basestring):\n                    valid_std_name.assert_true(standard_name in self._std_names,\n                                            \"standard_name {} is not defined in Standard Name Table v{}\".format(\n                                                standard_name or 'undefined',\n                                                self._std_names._version))\n\n                ret_val.append(valid_std_name.to_result())\n\n                # 2) optional - if modifiers, should be in table\n                if standard_name_modifier is not None:\n                    valid_modifier = TestCtx(BaseCheck.HIGH, self.section_titles[\"3.3\"])\n                    allowed = ['detection_minimum',\n                            'number_of_observations',\n                            'standard_error',\n                            'status_flag']\n                    valid_modifier.assert_true(standard_name_modifier in allowed,\n                                            \"standard_name modifier {} for variable {} is not a valid modifier \"\n                                            \"according to appendix C\".format(standard_name_modifier, name))\n\n                    ret_val.append(valid_modifier.to_result())\n            else:\n                standard_name_present = False\n\n            long_or_std_name.assert_true(long_name_present or\n                                            standard_name_present,\n                                            \"Attribute long_name or/and standard_name is highly recommended for variable {}\".format(name))\n            ret_val.append(long_or_std_name.to_result())\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_ancillary_variables(self, ds):\n        '''\n        Checks the ancillary_variable attribute for all variables to ensure\n        they are CF compliant.\n\n        CF \u00a73.4 It is a string attribute whose value is a blank separated list\n        of variable names.  The nature of the relationship between variables\n        associated via ancillary_variables must be determined by other\n        attributes. The variables listed by the ancillary_variables attribute\n        will often have the standard name of the variable which points to them\n        including a modifier (Appendix C, Standard Name Modifiers) to indicate\n        the relationship.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        for ncvar in ds.get_variables_by_attributes(ancillary_variables=lambda x: x is not None):\n            name = ncvar.name\n            valid_ancillary = TestCtx(BaseCheck.HIGH, self.section_titles[\"3.4\"])\n            ancillary_variables = ncvar.ancillary_variables\n\n            valid_ancillary.assert_true(isinstance(ancillary_variables, basestring),\n                                        \"ancillary_variables attribute defined by {} \"\n                                        \"should be string\".format(name))\n\n            # Can't perform the second check if it's not a string\n            if not isinstance(ancillary_variables, basestring):\n                ret_val.append(valid_ancillary.to_result())\n                continue\n\n            for ancillary_variable in ancillary_variables.split():\n                valid_ancillary.assert_true(ancillary_variable in ds.variables,\n                                            \"{} is not a variable in this dataset\".format(ancillary_variable))\n\n            ret_val.append(valid_ancillary.to_result())\n\n        return ret_val", "response": "Checks the ancillary_variable attribute for all variables to ensure they are CF compliant."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the flags for the related log entries for the specified set of attributes.", "response": "def check_flags(self, ds):\n        '''\n        Check the flag_values, flag_masks and flag_meanings attributes for\n        variables to ensure they are CF compliant.\n\n        CF \u00a73.5 The attributes flag_values, flag_masks and flag_meanings are\n        intended to make variables that contain flag values self describing.\n        Status codes and Boolean (binary) condition flags may be expressed with\n        different combinations of flag_values and flag_masks attribute\n        definitions.\n\n        The flag_values and flag_meanings attributes describe a status flag\n        consisting of mutually exclusive coded values.\n\n        The flag_meanings attribute is a string whose value is a blank\n        separated list of descriptive words or phrases, one for each flag\n        value. Each word or phrase should consist of characters from the\n        alphanumeric set and the following five: '_', '-', '.', '+', '@'.\n\n        The flag_masks and flag_meanings attributes describe a number of\n        independent Boolean conditions using bit field notation by setting\n        unique bits in each flag_masks value.\n\n        The flag_masks, flag_values and flag_meanings attributes, used\n        together, describe a blend of independent Boolean conditions and\n        enumerated status codes. A flagged condition is identified by a bitwise\n        AND of the variable value and each flag_masks value; a result that\n        matches the flag_values value indicates a true condition.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        for name in cfutil.get_flag_variables(ds):\n            variable = ds.variables[name]\n            flag_values = getattr(variable, \"flag_values\", None)\n            flag_masks = getattr(variable, \"flag_masks\", None)\n\n            valid_flags_var = TestCtx(BaseCheck.HIGH, self.section_titles['3.5'])\n            # Check that the variable defines mask or values\n            valid_flags_var.assert_true(flag_values is not None or flag_masks is not None,\n                                        \"{} does not define either flag_masks or flag_values\".format(name))\n            ret_val.append(valid_flags_var.to_result())\n\n            valid_meanings = self._check_flag_meanings(ds, name)\n            ret_val.append(valid_meanings)\n\n            # check flag_values\n            if flag_values is not None:\n                valid_values = self._check_flag_values(ds, name)\n                ret_val.append(valid_values)\n\n            # check flag_masks\n            if flag_masks is not None:\n                valid_masks = self._check_flag_masks(ds, name)\n                ret_val.append(valid_masks)\n\n            if flag_values is not None and flag_masks is not None:\n                allv = list(map(lambda a, b: a & b == a, list(zip(flag_values, flag_masks))))\n\n                allvr = Result(BaseCheck.MEDIUM, all(allv), self.section_titles['3.5'])\n                if not allvr.value:\n                    allvr.msgs = [\"flag masks and flag values for '{}' combined don't equal flag value\".format(name)]\n\n                ret_val.append(allvr)\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_flag_values(self, ds, name):\n        '''\n        Checks a variable's flag_values attribute for compliance under CF\n\n        - flag_values exists as an array\n        - unique elements in flag_values\n        - flag_values si the same dtype as the variable\n        - flag_values is the same length as flag_meanings\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: Name of variable to check\n        :rtype: compliance_checker.base.Result\n        '''\n        variable = ds.variables[name]\n\n        flag_values = variable.flag_values\n        flag_meanings = getattr(variable, 'flag_meanings', None)\n        valid_values = TestCtx(BaseCheck.HIGH, self.section_titles['3.5'])\n\n        # flag_values must be a list of values, not a string or anything else\n        valid_values.assert_true(isinstance(flag_values, np.ndarray),\n                                 \"{}'s flag_values must be an array of values not {}\".format(name, type(flag_values)))\n\n        # We can't perform any more checks\n        if not isinstance(flag_values, np.ndarray):\n            return valid_values.to_result()\n\n        # the flag values must be independent, no repeating values\n        flag_set = set(flag_values)\n        valid_values.assert_true(len(flag_set) == len(flag_values),\n                                 \"{}'s flag_values must be independent and can not be repeated\".format(name))\n\n        # the data type for flag_values should be the same as the variable\n        valid_values.assert_true(variable.dtype.type == flag_values.dtype.type,\n                                 \"flag_values ({}) must be the same data type as {} ({})\"\n                                 \"\".format(flag_values.dtype.type, name, variable.dtype.type))\n\n        if isinstance(flag_meanings, basestring):\n            flag_meanings = flag_meanings.split()\n            valid_values.assert_true(len(flag_meanings) == len(flag_values),\n                                     \"{}'s flag_meanings and flag_values should have the same number \".format(name)+\\\n                                     \"of elements.\")\n\n        return valid_values.to_result()", "response": "Checks a variable s flag_values attribute for compliance under CF\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_flag_masks(self, ds, name):\n        '''\n        Check a variable's flag_masks attribute for compliance under CF\n\n        - flag_masks exists as an array\n        - flag_masks is the same dtype as the variable\n        - variable's dtype can support bit-field\n        - flag_masks is the same length as flag_meanings\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: Variable name\n        :rtype: compliance_checker.base.Result\n        '''\n        variable = ds.variables[name]\n\n        flag_masks = variable.flag_masks\n        flag_meanings = getattr(ds, 'flag_meanings', None)\n\n        valid_masks = TestCtx(BaseCheck.HIGH, self.section_titles['3.5'])\n\n        valid_masks.assert_true(isinstance(flag_masks, np.ndarray),\n                                \"{}'s flag_masks must be an array of values not {}\".format(name, type(flag_masks)))\n\n        if not isinstance(flag_masks, np.ndarray):\n            return valid_masks.to_result()\n\n        valid_masks.assert_true(variable.dtype.type == flag_masks.dtype.type,\n                                \"flag_masks ({}) mustbe the same data type as {} ({})\"\n                                \"\".format(flag_masks.dtype.type, name, variable.dtype.type))\n\n        type_ok = (np.issubdtype(variable.dtype, np.integer) or\n                   np.issubdtype(variable.dtype, 'S') or\n                   np.issubdtype(variable.dtype, 'b'))\n\n        valid_masks.assert_true(type_ok, \"{}'s data type must be capable of bit-field expression\".format(name))\n\n        if isinstance(flag_meanings, basestring):\n            flag_meanings = flag_meanings.split()\n            valid_masks.assert_true(len(flag_meanings) == len(flag_masks),\n                                    \"{} flag_meanings and flag_masks should have the same number \".format(name)+\\\n                                    \"of elements.\")\n\n        return valid_masks.to_result()", "response": "Checks that a variable s flag_masks attribute for compliance under CF\n        is valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_flag_meanings(self, ds, name):\n        '''\n        Check a variable's flag_meanings attribute for compliance under CF\n\n        - flag_meanings exists\n        - flag_meanings is a string\n        - flag_meanings elements are valid strings\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: Variable name\n        :rtype: compliance_checker.base.Result\n        '''\n        variable = ds.variables[name]\n        flag_meanings = getattr(variable, 'flag_meanings', None)\n        valid_meanings = TestCtx(BaseCheck.HIGH, self.section_titles['3.5'])\n\n        valid_meanings.assert_true(flag_meanings is not None,\n                                   \"{}'s flag_meanings attribute is required for flag variables\".format(name))\n\n        valid_meanings.assert_true(isinstance(flag_meanings, basestring),\n                                   \"{}'s flag_meanings attribute must be a string\".format(name))\n\n        # We can't perform any additional checks if it's not a string\n        if not isinstance(flag_meanings, basestring):\n            return valid_meanings.to_result()\n\n        valid_meanings.assert_true(len(flag_meanings) > 0,\n                                   \"{}'s flag_meanings can't be empty\".format(name))\n\n        flag_regx = regex.compile(r\"^[0-9A-Za-z_\\-.+@]+$\")\n        meanings = flag_meanings.split()\n        for meaning in meanings:\n            if flag_regx.match(meaning) is None:\n                valid_meanings.assert_true(False,\n                                           \"{}'s flag_meanings attribute defined an illegal flag meaning \".format(name)+\\\n                                           \"{}\".format(meaning))\n        return valid_meanings.to_result()", "response": "Checks that the flag_meanings attribute for compliance under CF\n        is a string and if it is not a string it is checked as a string and if it is not a string it is checked as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_coordinate_types(self, ds):\n        '''\n        Check the axis attribute of coordinate variables\n\n        CF \u00a74 The attribute axis may be attached to a coordinate variable and\n        given one of the values X, Y, Z or T which stand for a longitude,\n        latitude, vertical, or time axis respectively. Alternatively the\n        standard_name attribute may be used for direct identification.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        for variable in ds.get_variables_by_attributes(axis=lambda x: x is not None):\n            name = variable.name\n            # Coordinate compressions should not be checked as a valid\n            # coordinate, which they are not. They are a mechanism to project\n            # an array of indices onto a 2-d grid containing valid coordinates.\n            if cfutil.is_compression_coordinate(ds, name):\n                continue\n\n            variable = ds.variables[name]\n            # Even though it's not allowed in CF 1.6, it is allowed in CF 1.7\n            # and we see people do it, often.\n            if hasattr(variable, 'cf_role'):\n                continue\n\n            # \u00a76.1 allows for labels to be referenced as auxiliary coordinate\n            # variables, which should not be checked like the rest of the\n            # coordinates.\n            if variable.dtype.char == 'S':\n                continue\n\n            axis = getattr(variable, 'axis', None)\n\n            if axis is not None:\n                valid_axis = self._check_axis(ds, name)\n                ret_val.append(valid_axis)\n\n        return ret_val", "response": "Check the axis attribute of coordinate variables and the variable axis attribute of coordinate variables and return a list of valid coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that the axis attribute is a string and an allowed value namely one of T X Y or Z.", "response": "def _check_axis(self, ds, name):\n        '''\n        Checks that the axis attribute is a string and an allowed value, namely\n        one of 'T', 'X', 'Y', or 'Z'.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str name: Name of the variable\n        :rtype: compliance_checker.base.Result\n        '''\n        allowed_axis = ['T', 'X', 'Y', 'Z']\n        variable = ds.variables[name]\n        axis = variable.axis\n\n        valid_axis = TestCtx(BaseCheck.HIGH, self.section_titles['4'])\n        axis_is_string = isinstance(axis, basestring),\n        valid_axis.assert_true(axis_is_string and len(axis) > 0,\n                               \"{}'s axis attribute must be a non-empty string\".format(name))\n\n        # If axis isn't a string we can't continue any checks\n        if not axis_is_string or len(axis) == 0:\n            return valid_axis.to_result()\n\n        valid_axis.assert_true(axis in allowed_axis,\n                               \"{}'s axis attribute must be T, X, Y, or Z, \".format(name)+\\\n                               \"currently {}\".format(axis))\n\n        return valid_axis.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_latitude(self, ds):\n        '''\n        Check variable(s) that define latitude and are defined correctly according to CF.\n\n        CF \u00a74.1 Variables representing latitude must always explicitly include\n        the units attribute; there is no default value.  The recommended unit\n        of latitude is degrees_north. Also acceptable are degree_north,\n        degree_N, degrees_N, degreeN, and degreesN.\n\n        Optionally, the latitude type may be indicated additionally by\n        providing the standard_name attribute with the value latitude, and/or\n        the axis attribute with the value Y.\n\n        - Four checks per latitude variable\n        - (H) latitude has units attribute\n        - (M) latitude has an allowed units attribute\n        - (L) latitude uses degrees_north (if not in rotated pole)\n        - (M) latitude defines either standard_name or axis\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        allowed_lat_units = [\n            'degrees_north',\n            'degree_north',\n            'degree_n',\n            'degrees_n',\n            'degreen',\n            'degreesn'\n        ]\n\n        # Determine the grid mappings in this dataset\n        grid_mapping = []\n        grid_mapping_variables = cfutil.get_grid_mapping_variables(ds)\n        for name in grid_mapping_variables:\n            variable = ds.variables[name]\n            grid_mapping_name = getattr(variable, 'grid_mapping_name', None)\n            if grid_mapping_name:\n                grid_mapping.append(grid_mapping_name)\n\n        latitude_variables = cfutil.get_latitude_variables(ds)\n        for latitude in latitude_variables:\n            variable = ds.variables[latitude]\n            units = getattr(variable, 'units', None)\n            units_is_string = isinstance(units, basestring)\n            standard_name = getattr(variable, 'standard_name', None)\n            axis = getattr(variable, 'axis', None)\n\n            # Check that latitude defines units\n            valid_latitude = TestCtx(BaseCheck.HIGH, self.section_titles['4.1'])\n            valid_latitude.assert_true(units is not None,\n                                       \"latitude variable '{}' must define units\".format(latitude))\n            ret_val.append(valid_latitude.to_result())\n\n            # Check that latitude uses allowed units\n            allowed_units = TestCtx(BaseCheck.MEDIUM, self.section_titles['4.1'])\n            if standard_name == 'grid_latitude':\n                e_n_units = cfutil.VALID_LAT_UNITS | cfutil.VALID_LON_UNITS\n                # check that the units aren't in east and north degrees units,\n                # but are convertible to angular units\n                allowed_units.assert_true(units not in e_n_units and\n                                          Unit(units) == Unit('degree'),\n                                          \"Grid latitude variable '{}' should use degree equivalent units without east or north components. \"\n                                          \"Current units are {}\".format(latitude, units))\n            else:\n                allowed_units.assert_true(units_is_string and units.lower() in allowed_lat_units,\n                                          \"latitude variable '{}' should define valid units for latitude\"\n                                          \"\".format(latitude))\n            ret_val.append(allowed_units.to_result())\n\n            # Check that latitude uses degrees_north\n            if standard_name == 'latitude' and units != 'degrees_north':\n                # This is only a recommendation and we won't penalize but we\n                # will include a recommended action.\n                msg = (\"CF recommends latitude variable '{}' to use units degrees_north\"\n                       \"\".format(latitude))\n                recommended_units = Result(BaseCheck.LOW, (1, 1), self.section_titles['4.1'], [msg])\n                ret_val.append(recommended_units)\n\n            y_variables = ds.get_variables_by_attributes(axis='Y')\n            # Check that latitude defines either standard_name or axis\n            definition = TestCtx(BaseCheck.MEDIUM, self.section_titles['4.1'])\n            definition.assert_true(standard_name == 'latitude' or axis == 'Y' or y_variables != [],\n                                   \"latitude variable '{}' should define standard_name='latitude' or axis='Y'\"\n                                   \"\".format(latitude))\n            ret_val.append(definition.to_result())\n\n        return ret_val", "response": "Check that variables that define latitude and are defined correctly according to CF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_longitude(self, ds):\n        '''\n        Check variable(s) that define longitude and are defined correctly according to CF.\n\n        CF \u00a74.2 Variables representing longitude must always explicitly include\n        the units attribute; there is no default value.  The recommended unit\n        of longitude is degrees_east. Also acceptable are degree_east,\n        degree_E, degrees_E, degreeE, and degreesE.\n\n        Optionally, the longitude type may be indicated additionally by\n        providing the standard_name attribute with the value longitude, and/or\n        the axis attribute with the value X.\n\n        - Four checks per longitude variable\n        - (H) longitude has units attribute\n        - (M) longitude has an allowed units attribute\n        - (L) longitude uses degrees_east (if not in rotated pole)\n        - (M) longitude defines either standard_name or axis\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n\n        # TODO we already have a check_latitude... I'm sure we can make DRYer\n\n        ret_val = []\n        allowed_lon_units = [\n            'degrees_east',\n            'degree_east',\n            'degree_e',\n            'degrees_e',\n            'degreee',\n            'degreese'\n        ]\n\n        # Determine the grid mappings in this dataset\n        grid_mapping = []\n        grid_mapping_variables = cfutil.get_grid_mapping_variables(ds)\n        for name in grid_mapping_variables:\n            variable = ds.variables[name]\n            grid_mapping_name = getattr(variable, 'grid_mapping_name', None)\n            if grid_mapping_name:\n                grid_mapping.append(grid_mapping_name)\n\n        longitude_variables = cfutil.get_longitude_variables(ds)\n        for longitude in longitude_variables:\n            variable = ds.variables[longitude]\n            units = getattr(variable, 'units', None)\n            units_is_string = isinstance(units, basestring)\n            standard_name = getattr(variable, 'standard_name', None)\n            axis = getattr(variable, 'axis', None)\n\n            # NOTE see docstring--should below be 4.1 or 4.2?\n            # Check that longitude defines units\n            valid_longitude = TestCtx(BaseCheck.HIGH, self.section_titles['4.1'])\n            valid_longitude.assert_true(units is not None,\n                                        \"longitude variable '{}' must define units\".format(longitude))\n            ret_val.append(valid_longitude.to_result())\n\n            # Check that longitude uses allowed units\n            allowed_units = TestCtx(BaseCheck.MEDIUM, self.section_titles['4.1'])\n            if standard_name == 'grid_longitude':\n                e_n_units = cfutil.VALID_LAT_UNITS | cfutil.VALID_LON_UNITS\n                # check that the units aren't in east and north degrees units,\n                # but are convertible to angular units\n                allowed_units.assert_true(units not in e_n_units and\n                                          Unit(units) == Unit('degree'),\n                                          \"Grid longitude variable '{}' should use degree equivalent units without east or north components. \"\n                                          \"Current units are {}\".format(longitude, units))\n            else:\n                allowed_units.assert_true(units_is_string and units.lower() in allowed_lon_units,\n                                          \"longitude variable '{}' should define valid units for longitude\"\n                                          \"\".format(longitude))\n            ret_val.append(allowed_units.to_result())\n\n            # Check that longitude uses degrees_east\n            if standard_name == 'longitude' and units != 'degrees_east':\n                # This is only a recommendation and we won't penalize but we\n                # will include a recommended action.\n                msg = (\"CF recommends longitude variable '{}' to use units degrees_east\"\n                       \"\".format(longitude))\n                recommended_units = Result(BaseCheck.LOW,\n                                           (1, 1),\n                                           self.section_titles['4.1'], [msg])\n                ret_val.append(recommended_units)\n\n            x_variables = ds.get_variables_by_attributes(axis='X')\n            # Check that longitude defines either standard_name or axis\n            definition = TestCtx(BaseCheck.MEDIUM, self.section_titles['4.1'])\n            definition.assert_true(standard_name == 'longitude' or axis == 'Y' or x_variables != [],\n                                   \"longitude variable '{}' should define standard_name='longitude' or axis='X'\"\n                                   \"\".format(longitude))\n            ret_val.append(definition.to_result())\n\n        return ret_val", "response": "Check that variables that define longitude are defined correctly according to CF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck units for variables defining vertical position are valid under CF.", "response": "def check_dimensional_vertical_coordinate(self, ds):\n        '''\n        Check units for variables defining vertical position are valid under\n        CF.\n\n        CF \u00a74.3.1 The units attribute for dimensional coordinates will be a string\n        formatted as per the udunits.dat file.\n\n        The acceptable units for vertical (depth or height) coordinate variables\n        are:\n        - units of pressure as listed in the file udunits.dat. For vertical axes\n          the most commonly used of these include include bar, millibar,\n          decibar, atmosphere (atm), pascal (Pa), and hPa.\n        - units of length as listed in the file udunits.dat. For vertical axes\n          the most commonly used of these include meter (metre, m), and\n          kilometer (km).\n        - other units listed in the file udunits.dat that may under certain\n          circumstances reference vertical position such as units of density or\n          temperature.\n\n        Plural forms are also acceptable.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n        z_variables = cfutil.get_z_variables(ds)\n        #dimless_standard_names = [name for name, regx in dimless_vertical_coordinates]\n        for name in z_variables:\n            variable = ds.variables[name]\n            standard_name = getattr(variable, 'standard_name', None)\n            units = getattr(variable, 'units', None)\n            positive = getattr(variable, 'positive', None)\n            # Skip the variable if it's dimensionless\n            if (hasattr(variable, 'formula_terms') or\n                standard_name in dimless_vertical_coordinates):\n                continue\n\n            valid_vertical_coord = TestCtx(BaseCheck.HIGH, self.section_titles[\"4.3\"])\n            valid_vertical_coord.assert_true(isinstance(units, basestring) and units,\n                                             \"\u00a74.3.1 {}'s units must be defined for vertical coordinates, \"\n                                             \"there is no default\".format(name))\n\n            if not util.units_convertible('bar', units):\n                valid_vertical_coord.assert_true(positive in ('up', 'down'),\n                                                 \"{}: vertical coordinates not defining pressure must include \"\n                                                 \"a positive attribute that is either 'up' or 'down'\".format(name))\n\n            # _check_valid_standard_units, part of the Chapter 3 checks,\n            # already verifies that this coordinate has valid units\n\n            ret_val.append(valid_vertical_coord.to_result())\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_dimensionless_vertical_coordinate(self, ds):\n        '''\n        Check the validity of dimensionless coordinates under CF\n\n        CF \u00a74.3.2 The units attribute is not required for dimensionless\n        coordinates.\n\n        The standard_name attribute associates a coordinate with its definition\n        from Appendix D, Dimensionless Vertical Coordinates. The definition\n        provides a mapping between the dimensionless coordinate values and\n        dimensional values that can positively and uniquely indicate the\n        location of the data.\n\n        A new attribute, formula_terms, is used to associate terms in the\n        definitions with variables in a netCDF file.  To maintain backwards\n        compatibility with COARDS the use of these attributes is not required,\n        but is strongly recommended.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        z_variables = cfutil.get_z_variables(ds)\n        deprecated_units = [\n            'level',\n            'layer',\n            'sigma_level'\n        ]\n        for name in z_variables:\n            variable = ds.variables[name]\n            standard_name = getattr(variable, 'standard_name', None)\n            units = getattr(variable, 'units', None)\n            formula_terms = getattr(variable, 'formula_terms', None)\n            # Skip the variable if it's dimensional\n            if (formula_terms is None and\n                standard_name not in dimless_vertical_coordinates):\n                continue\n\n            is_not_deprecated = TestCtx(BaseCheck.LOW, self.section_titles[\"4.3\"])\n\n            is_not_deprecated.assert_true(units not in deprecated_units,\n                                          \"\u00a74.3.2: units are deprecated by CF in variable {}: {}\"\n                                          \"\".format(name, units))\n            ret_val.append(is_not_deprecated.to_result())\n            ret_val.append(self._check_formula_terms(ds, name))\n\n        return ret_val", "response": "Check the validity of dimensionless vertical coordinates under CF \u00a74. 3. 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the variable with the given coordinates contains valid formula_terms.", "response": "def _check_formula_terms(self, ds, coord):\n        '''\n        Checks a dimensionless vertical coordinate contains valid formula_terms\n\n        - formula_terms is a non-empty string\n        - formula_terms matches regx\n        - every variable defined in formula_terms exists\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        '''\n        variable = ds.variables[coord]\n        standard_name = getattr(variable, 'standard_name', None)\n        formula_terms = getattr(variable, 'formula_terms', None)\n        valid_formula_terms = TestCtx(BaseCheck.HIGH, self.section_titles['4.3'])\n\n        valid_formula_terms.assert_true(isinstance(formula_terms, basestring) and formula_terms,\n                                        '\u00a74.3.2: {}\\'s formula_terms is a required attribute and must be a non-empty string'\n                                        ''.format(coord))\n        # We can't check any more\n        if not formula_terms:\n            return valid_formula_terms.to_result()\n\n        # check that the formula_terms are well formed and are present\n        # The pattern for formula terms is always component: variable_name\n        # the regex grouping always has component names in even positions and\n        # the corresponding variable name in even positions.\n        matches = regex.findall(r'([A-Za-z][A-Za-z0-9_]*: )([A-Za-z][A-Za-z0-9_]*)',\n                                variable.formula_terms)\n        terms = set(m[0][:-2] for m in matches)\n        # get the variables named in the formula terms and check if any\n        # are not present in the dataset\n        missing_vars = sorted(set(m[1] for m in matches) - set(ds.variables))\n        missing_fmt = \"The following variable(s) referenced in {}:formula_terms are not present in the dataset: {}\"\n        valid_formula_terms.assert_true(len(missing_vars) == 0,\n                                    missing_fmt.format(coord, ', '.join(missing_vars)))\n        # try to reconstruct formula_terms by adding space in between the regex\n        # matches.  If it doesn't exactly match the original, the formatting\n        # of the attribute is incorrect\n        reconstructed_formula = ' '.join(m[0] + m[1] for m in matches)\n        valid_formula_terms.assert_true(reconstructed_formula == formula_terms,\n                                        \"Attribute formula_terms is not well-formed\")\n\n        valid_formula_terms.assert_true(standard_name in\n                                        dimless_vertical_coordinates,\n                                        \"unknown standard_name '{}' for dimensionless vertical coordinate {}\"\n                                        \"\".format(standard_name, coord))\n        if standard_name not in dimless_vertical_coordinates:\n            return valid_formula_terms.to_result()\n\n        valid_formula_terms.assert_true(no_missing_terms(standard_name, terms),\n                                        \"{}'s formula_terms are invalid for {}, please see appendix D of CF 1.6\"\n                                        \"\".format(coord, standard_name))\n\n        return valid_formula_terms.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the variable units and standard_name attributes of the time of the local user has the correct units.", "response": "def check_time_coordinate(self, ds):\n        '''\n        Check variables defining time are valid under CF\n\n        CF \u00a74.4 Variables representing time must always explicitly include the\n        units attribute; there is no default value.\n\n        The units attribute takes a string value formatted as per the\n        recommendations in the Udunits package.\n\n        The acceptable units for time are listed in the udunits.dat file. The\n        most commonly used of these strings (and their abbreviations) includes\n        day (d), hour (hr, h), minute (min) and second (sec, s). Plural forms\n        are also acceptable. The reference time string (appearing after the\n        identifier since) may include date alone; date and time; or date, time,\n        and time zone. The reference time is required. A reference time in year\n        0 has a special meaning (see Section 7.4, \"Climatological Statistics\").\n\n        Recommend that the unit year be used with caution. It is not a calendar\n        year.  For similar reasons the unit month should also be used with\n        caution.\n\n        A time coordinate is identifiable from its units string alone.\n        Optionally, the time coordinate may be indicated additionally by\n        providing the standard_name attribute with an appropriate value, and/or\n        the axis attribute with the value T.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n\n        ret_val = []\n        for name in cfutil.get_time_variables(ds):\n            variable = ds.variables[name]\n            # Has units\n            has_units = hasattr(variable, 'units')\n            if not has_units:\n                result = Result(BaseCheck.HIGH,\n                                False,\n                                self.section_titles['4.4'],\n                                ['%s does not have units' % name])\n                ret_val.append(result)\n                continue\n            # Correct and identifiable units\n            result = Result(BaseCheck.HIGH,\n                            True,\n                            self.section_titles['4.4'])\n            ret_val.append(result)\n            correct_units = util.units_temporal(variable.units)\n            reasoning = None\n            if not correct_units:\n                reasoning = ['%s does not have correct time units' % name]\n            result = Result(BaseCheck.HIGH,\n                            correct_units,\n                            self.section_titles['4.4'],\n                            reasoning)\n            ret_val.append(result)\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the calendar attribute for variables defining time and ensure it is a valid calendar.", "response": "def check_calendar(self, ds):\n        '''\n        Check the calendar attribute for variables defining time and ensure it\n        is a valid calendar prescribed by CF.\n\n        CF \u00a74.4.1 In order to calculate a new date and time given a base date, base\n        time and a time increment one must know what calendar to use.\n\n        The values currently defined for calendar are:\n        - gregorian or standard\n        - proleptic_gregorian\n        - noleap or 365_day\n        - all_leap or 366_day\n        - 360_day\n        - julian\n        - none\n\n        The calendar attribute may be set to none in climate experiments that\n        simulate a fixed time of year.\n        The time of year is indicated by the date in the reference time of the\n        units attribute.\n\n        If none of the calendars defined above applies, a non-standard calendar\n        can be defined. The lengths of each month are explicitly defined with\n        the month_lengths attribute of the time axis.\n\n        If leap years are included, then two other attributes of the time axis\n        should also be defined:\n\n        leap_year, leap_month\n\n        The calendar attribute is not required when a non-standard calendar is\n        being used. It is sufficient to define the calendar using the\n        month_lengths attribute, along with leap_year, and leap_month as\n        appropriate. However, the calendar attribute is allowed to take\n        non-standard values and in that case defining the non-standard calendar\n        using the appropriate attributes is required.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        valid_calendars = [\n            'gregorian',\n            'standard',\n            'proleptic_gregorian',\n            'noleap',\n            '365_day',\n            'all_leap',\n            '366_day',\n            '360_day',\n            'julian',\n            'none'\n        ]\n\n        ret_val = []\n\n        # if has a calendar, check that it is within the valid values\n        # otherwise no calendar is valid\n        for time_var in ds.get_variables_by_attributes(calendar=lambda c: c is not None):\n            reasoning = None\n            valid_calendar = time_var.calendar in valid_calendars\n\n            if not valid_calendar:\n                reasoning = [\"\u00a74.4.1 Variable %s should have a valid calendar: '%s' is not a valid calendar\" % (time_var.name, time_var.calendar)]\n\n            # passes if the calendar is valid, otherwise notify of invalid\n            # calendar\n\n            result = Result(BaseCheck.LOW,\n                            valid_calendar,\n                            self.section_titles['4.4'],\n                            reasoning)\n            ret_val.append(result)\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that auxiliary coordinates of a single variable are a subset of the dimensions of the variable with which the coordinate is associated.", "response": "def check_aux_coordinates(self, ds):\n        '''\n        Chapter 5 paragraph 3\n\n        The dimensions of an auxiliary coordinate variable must be a subset of\n        the dimensions of the variable with which the coordinate is associated,\n        with two exceptions. First, string-valued coordinates (Section 6.1,\n        \"Labels\") have a dimension for maximum string length. Second, in the\n        ragged array representations of data (Chapter 9, Discrete Sampling\n        Geometries), special methods are needed to connect the data and\n        coordinates.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n\n        ret_val = []\n        geophysical_variables = self._find_geophysical_vars(ds)\n        for name in geophysical_variables:\n            variable = ds.variables[name]\n            coordinates = getattr(variable, 'coordinates', None)\n            # We use a set so we can assert\n            dim_set = set(variable.dimensions)\n            # No auxiliary coordinates, no check\n            if not isinstance(coordinates, basestring) or coordinates == '':\n                continue\n\n            valid_aux_coords = TestCtx(BaseCheck.HIGH,\n                                      self.section_titles[\"5\"])\n\n            for aux_coord in coordinates.split():\n                valid_aux_coords.assert_true(aux_coord in ds.variables,\n                                             \"{}'s auxiliary coordinate specified by the coordinates attribute, {}, \"\n                                             \"is not a variable in this dataset\"\n                                             \"\".format(name, aux_coord))\n                if aux_coord not in ds.variables:\n                    continue\n\n                # \u00a76.1 Allows for \"labels\" to be referenced as coordinates\n                if ds.variables[aux_coord].dtype.char == 'S':\n                    continue\n\n                aux_coord_dims = set(ds.variables[aux_coord].dimensions)\n                valid_aux_coords.assert_true(aux_coord_dims.issubset(dim_set),\n                                             \"dimensions for auxiliary coordinate variable {} ({}) \"\n                                             \"are not a subset of dimensions for variable {} ({})\"\n                                             \"\".format(aux_coord,\n                                                       ', '.join(aux_coord_dims),\n                                                       name,\n                                                       ', '.join(dim_set)))\n            ret_val.append(valid_aux_coords.to_result())\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_duplicate_axis(self, ds):\n        '''\n        Checks that no variable contains two coordinates defining the same\n        axis.\n\n        Chapter 5 paragraph 6\n\n        If an axis attribute is attached to an auxiliary coordinate variable,\n        it can be used by applications in the same way the `axis` attribute\n        attached to a coordinate variable is used. However, it is not\n        permissible for a [geophysical variable] to have both a coordinate\n        variable and an auxiliary coordinate variable, or more than one of\n        either type of variable, having an `axis` attribute with any given\n        value e.g. there must be no more than one axis attribute for X for any\n        [geophysical variable].\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        :return: List of results\n        '''\n\n        ret_val = []\n        geophysical_variables = self._find_geophysical_vars(ds)\n        for name in geophysical_variables:\n            no_duplicates = TestCtx(BaseCheck.HIGH, self.section_titles['5'])\n            axis_map = cfutil.get_axis_map(ds, name)\n            axes = []\n            # For every coordinate associated with this variable, keep track of\n            # which coordinates define an axis and assert that there are no\n            # duplicate axis attributes defined in the set of associated\n            # coordinates.\n            for axis, coordinates in axis_map.items():\n                for coordinate in coordinates:\n                    axis_attr = getattr(ds.variables[coordinate], 'axis', None)\n                    no_duplicates.assert_true(axis_attr is None or axis_attr not in axes,\n                                              \"'{}' has duplicate axis {} defined by {}\".format(name, axis_attr, coordinate))\n\n                    if axis_attr and axis_attr not in axes:\n                        axes.append(axis_attr)\n\n            ret_val.append(no_duplicates.to_result())\n\n        return ret_val", "response": "Checks that no variable has two coordinates defining the same axis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that no multi - dimensional coordinate shares a name with its dimensions.", "response": "def check_multi_dimensional_coords(self, ds):\n        '''\n        Checks that no multidimensional coordinate shares a name with its\n        dimensions.\n\n        Chapter 5 paragraph 4\n\n        We recommend that the name of a [multidimensional coordinate] should\n        not match the name of any of its dimensions.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        # This can only apply to auxiliary coordinate variables\n        for coord in self._find_aux_coord_vars(ds):\n            variable = ds.variables[coord]\n            if variable.ndim < 2:\n                continue\n            not_matching = TestCtx(BaseCheck.MEDIUM, self.section_titles['5'])\n\n            not_matching.assert_true(coord not in variable.dimensions,\n                                     '{} shares the same name as one of its dimensions'\n                                     ''.format(coord))\n            ret_val.append(not_matching.to_result())\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_grid_coordinates(self, ds):\n        ret_val = []\n        latitudes = cfutil.get_true_latitude_variables(ds)\n        longitudes = cfutil.get_true_longitude_variables(ds)\n\n        check_featues = [\n            '2d-regular-grid',\n            '2d-static-grid',\n            '3d-regular-grid',\n            '3d-static-grid',\n            'mapped-grid',\n            'reduced-grid'\n        ]\n\n        # This one is tricky because there's a very subtle difference between\n        # latitude as defined in Chapter 4 and \"true\" latitude as defined in\n        # chapter 5.\n\n        # For each geophysical variable that defines a grid, assert it is\n        # associated with a true latitude or longitude coordinate.\n\n        for variable in self._find_geophysical_vars(ds):\n            # We use a set so we can do set-wise comparisons with coordinate\n            # dimensions\n            dimensions = set(ds.variables[variable].dimensions)\n            # If it's not a grid, skip it\n            if cfutil.guess_feature_type(ds, variable) not in check_featues:\n                continue\n            has_coords = TestCtx(BaseCheck.HIGH, self.section_titles['5.6'])\n\n            # axis_map is a defaultdict(list) mapping the axis to a list of\n            # coordinate names. For example:\n            # {'X': ['lon'], 'Y':['lat'], 'Z':['lev']}\n            # The mapping comes from the dimensions of the variable and the\n            # contents of the `coordinates` attribute only.\n            axis_map = cfutil.get_axis_map(ds, variable)\n\n            msg = '{}\\'s coordinate variable \"{}\" is not one of the variables identifying true '+\\\n                  'latitude/longitude and its dimensions are not a subset of {}\\'s dimensions'\n\n            alt = '{} has no coordinate associated with a variable identified as true latitude/longitude; '+\\\n                  'its coordinate variable should also share a subset of {}\\'s dimensions'\n\n            # Make sure we can find latitude and its dimensions are a subset\n            _lat = None\n            found_lat = False\n            for lat in axis_map['Y']:\n                _lat = lat\n                is_subset_dims = set(ds.variables[lat].dimensions).issubset(dimensions)\n\n                if is_subset_dims and lat in latitudes:\n                    found_lat = True\n                    break\n            if _lat:\n                has_coords.assert_true(found_lat, msg.format(variable, _lat, variable))\n            else:\n                has_coords.assert_true(found_lat, alt.format(variable, variable))\n\n\n            # Make sure we can find longitude and its dimensions are a subset\n            _lon = None\n            found_lon = False\n            for lon in axis_map['X']:\n                _lon = lon\n                is_subset_dims = set(ds.variables[lon].dimensions).issubset(dimensions)\n\n                if is_subset_dims and lon in longitudes:\n                    found_lon = True\n                    break\n            if _lon:\n                has_coords.assert_true(found_lon, msg.format(variable, _lon, variable))\n            else:\n                has_coords.assert_true(found_lon, alt.format(variable, variable))\n\n            ret_val.append(has_coords.to_result())\n        return ret_val", "response": "Checks that the coordinates of the geophysical variables for a horizontal grid are not supplied via the coordinates attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the reduced longitude - latitude grid is not empty and that all of the variables in the variable coordinates are in the same location as the original grid.", "response": "def check_reduced_horizontal_grid(self, ds):\n        \"\"\"\n        5.3 A \"reduced\" longitude-latitude grid is one in which the points are\n        arranged along constant latitude lines with the number of points on a\n        latitude line decreasing toward the poles.\n\n        Recommend that this type of gridded data be stored using the compression\n        scheme described in Section 8.2, \"Compression by Gathering\". The\n        compressed latitude and longitude auxiliary coordinate variables are\n        identified by the coordinates attribute.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        \"\"\"\n        ret_val = []\n        # Create a set of coordinate varaibles defining `compress`\n        lats = set(cfutil.get_latitude_variables(ds))\n        lons = set(cfutil.get_longitude_variables(ds))\n\n        for name in self._find_geophysical_vars(ds):\n            coords = getattr(ds.variables[name], 'coordinates', None)\n            axis_map = cfutil.get_axis_map(ds, name)\n            # If this variable has no coordinate that defines compression\n            if 'C' not in axis_map:\n                continue\n\n            valid_rgrid = TestCtx(BaseCheck.HIGH, self.section_titles['5.3'])\n            # Make sure reduced grid features define coordinates\n            valid_rgrid.assert_true(isinstance(coords, basestring) and coords,\n                                    \"reduced grid feature {} must define coordinates attribute\"\n                                    \"\".format(name))\n            # We can't check anything else if there are no defined coordinates\n            if not isinstance(coords, basestring) and coords:\n                continue\n\n            coord_set = set(coords.split())\n\n            # Make sure it's associated with valid lat and valid lon\n            valid_rgrid.assert_true(len(coord_set.intersection(lons)) > 0,\n                                    '{} must be associated with a valid longitude coordinate'.format(name))\n            valid_rgrid.assert_true(len(coord_set.intersection(lats)) > 0,\n                                    '{} must be associated with a valid latitude coordinate'.format(name))\n            valid_rgrid.assert_true(len(axis_map['C']) == 1,\n                                    '{} can not be associated with more than one compressed coordinates: '\n                                    '({})'.format(name, ', '.join(axis_map['C'])))\n\n            for compressed_coord in axis_map['C']:\n                coord = ds.variables[compressed_coord]\n                compress = getattr(coord, 'compress', None)\n                valid_rgrid.assert_true(isinstance(compress, basestring) and compress,\n                                        \"compress attribute for compression coordinate {} must be a non-empty string\"\n                                        \"\".format(compressed_coord))\n                if not isinstance(compress, basestring):\n                    continue\n                for dim in compress.split():\n                    valid_rgrid.assert_true(dim in ds.dimensions,\n                                            \"dimension {} referenced by {}:compress must exist\"\n                                            \"\".format(dim, compressed_coord))\n            ret_val.append(valid_rgrid.to_result())\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_geographic_region(self, ds):\n        ret_val = []\n        region_list = [ # TODO maybe move this (and other info like it) into a config file?\n            'africa',\n            'antarctica',\n            'arabian_sea',\n            'aral_sea',\n            'arctic_ocean',\n            'asia',\n            'atlantic_ocean',\n            'australia',\n            'baltic_sea',\n            'barents_opening',\n            'barents_sea',\n            'beaufort_sea',\n            'bellingshausen_sea',\n            'bering_sea',\n            'bering_strait',\n            'black_sea',\n            'canadian_archipelago',\n            'caribbean_sea',\n            'caspian_sea',\n            'central_america',\n            'chukchi_sea',\n            'contiguous_united_states',\n            'denmark_strait',\n            'drake_passage',\n            'east_china_sea',\n            'english_channel',\n            'eurasia',\n            'europe',\n            'faroe_scotland_channel',\n            'florida_bahamas_strait',\n            'fram_strait',\n            'global',\n            'global_land',\n            'global_ocean',\n            'great_lakes',\n            'greenland',\n            'gulf_of_alaska',\n            'gulf_of_mexico',\n            'hudson_bay',\n            'iceland_faroe_channel',\n            'indian_ocean',\n            'indonesian_throughflow',\n            'indo_pacific_ocean',\n            'irish_sea',\n            'lake_baykal',\n            'lake_chad',\n            'lake_malawi',\n            'lake_tanganyika',\n            'lake_victoria',\n            'mediterranean_sea',\n            'mozambique_channel',\n            'north_america',\n            'north_sea',\n            'norwegian_sea',\n            'pacific_equatorial_undercurrent',\n            'pacific_ocean',\n            'persian_gulf',\n            'red_sea',\n            'ross_sea',\n            'sea_of_japan',\n            'sea_of_okhotsk',\n            'south_america',\n            'south_china_sea',\n            'southern_ocean',\n            'taiwan_luzon_straits',\n            'weddell_sea',\n            'windward_passage',\n            'yellow_sea'\n        ]\n\n        for var in ds.get_variables_by_attributes(standard_name='region'):\n            valid_region = TestCtx(BaseCheck.MEDIUM, self.section_titles[\"6.1\"])\n            region = var[:]\n            if np.ma.isMA(region):\n                region = region.data\n            valid_region.assert_true(''.join(region.astype(str)).lower() in region_list,\n                                     \"6.1.1 '{}' specified by '{}' is not a valid region\".format(\n                                         ''.join(region.astype(str)), var.name\n                                         )\n                                    )\n            ret_val.append(valid_region.to_result())\n        return ret_val", "response": "6. 1. 1 When data is representative of geographic regions which can be identified by names but which have complex locations that cannot practically be specified using longitude and latitude boundaries. This method checks the geographic region names and labels for the locations in the region attribute of the netCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_cell_boundaries(self, ds):\n\n        # Note that test does not check monotonicity\n        ret_val = []\n        reasoning = []\n        for variable_name, boundary_variable_name in cfutil.get_cell_boundary_map(ds).items():\n            variable = ds.variables[variable_name]\n            valid = True\n            reasoning = []\n            if boundary_variable_name not in ds.variables:\n                valid = False\n                reasoning.append(\"Boundary variable {} referenced by {} not \".format(\n                                    boundary_variable_name, variable.name\n                                    )+\\\n                                 \"found in dataset variables\")\n            else:\n                boundary_variable = ds.variables[boundary_variable_name]\n            # The number of dimensions in the bounds variable should always be\n            # the number of dimensions in the referring variable + 1\n            if (boundary_variable.ndim < 2):\n                valid = False\n                reasoning.append('Boundary variable {} specified by {}'.format(boundary_variable.name, variable.name)+\\\n                                 ' should have at least two dimensions to enclose the base '+\\\n                                 'case of a one dimensionsal variable')\n            if (boundary_variable.ndim != variable.ndim + 1):\n                valid = False\n                reasoning.append('The number of dimensions of the variable %s is %s, but the '\n                                 'number of dimensions of the boundary variable %s is %s. The boundary variable '\n                                 'should have %s dimensions' %\n                                 (variable.name, variable.ndim,\n                                  boundary_variable.name,\n                                  boundary_variable.ndim,\n                                  variable.ndim + 1))\n            if (variable.dimensions[:] != boundary_variable.dimensions[:variable.ndim]):\n                valid = False\n                reasoning.append(\n                    u\"Boundary variable coordinates (for {}) are in improper order: {}. Bounds-specific dimensions should be last\"\n                    \"\".format(variable.name, boundary_variable.dimensions))\n\n            # ensure p vertices form a valid simplex given previous a...n\n            # previous auxiliary coordinates\n            if (ds.dimensions[boundary_variable.dimensions[-1]].size < len(boundary_variable.dimensions[:-1]) + 1):\n                valid = False\n                reasoning.append(\"Dimension {} of boundary variable (for {}) must have at least {} elements to form a simplex/closed cell with previous dimensions {}.\".format(\n                    boundary_variable.name,\n                    variable.name,\n                    len(variable.dimensions) + 1,\n                    boundary_variable.dimensions[:-1])\n                )\n            result = Result(BaseCheck.MEDIUM, valid, self.section_titles[\"7.1\"], reasoning)\n            ret_val.append(result)\n        return ret_val", "response": "Checks the dimensions of cell boundary variables to ensure they are CF compliant."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_cell_measures(self, ds):\n        ret_val = []\n        reasoning = []\n        variables = ds.get_variables_by_attributes(cell_measures=lambda c:\n                                                   c is not None)\n        for var in variables:\n            search_str = r'^(?:area|volume): (\\w+)$'\n            search_res = regex.search(search_str, var.cell_measures)\n            if not search_res:\n                valid = False\n                reasoning.append(\"The cell_measures attribute for variable {} \"\n                                 \"is formatted incorrectly.  It should take the\"\n                                 \" form of either 'area: cell_var' or \"\n                                 \"'volume: cell_var' where cell_var is the \"\n                                 \"variable describing the cell measures\".format(\n                                     var.name))\n            else:\n                valid = True\n                cell_meas_var_name = search_res.groups()[0]\n                # TODO: cache previous results\n                if cell_meas_var_name not in ds.variables:\n                    valid = False\n                    reasoning.append(\n                        \"Cell measure variable {} referred to by \"\n                        \"{} is not present in dataset variables\".format(\n                            var.name, cell_meas_var_name)\n                    )\n                else:\n                    cell_meas_var = ds.variables[cell_meas_var_name]\n                    if not hasattr(cell_meas_var, 'units'):\n                        valid = False\n                        reasoning.append(\n                            \"Cell measure variable {} is required \"\n                            \"to have units attribute defined.\".format(\n                                cell_meas_var_name)\n                        )\n                    if not set(cell_meas_var.dimensions).issubset(var.dimensions):\n                        valid = False\n                        reasoning.append(\n                            \"Cell measure variable {} must have \"\n                            \"dimensions which are a subset of \"\n                            \"those defined in variable {}.\".format(\n                                cell_meas_var_name, var.name)\n                        )\n\n            result = Result(BaseCheck.MEDIUM,\n                            valid,\n                            (self.section_titles['7.2']),\n                            reasoning)\n            ret_val.append(result)\n\n        return ret_val", "response": "7. 2 To indicate extra information about the spatial properties of the grid cells a cell_measures attribute may be defined for a specific variable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the spacing and comment info contained inside the cell_methods parentheses in cell_methods is well - formed and that the spacing and or comment info contained inside the parentheses in cell_methods are well - formed.", "response": "def _check_cell_methods_paren_info(self, paren_contents, var):\n        \"\"\"\n        Checks that the spacing and/or comment info contained inside the\n        parentheses in cell_methods is well-formed\n        \"\"\"\n        valid_info = TestCtx(BaseCheck.MEDIUM,\n                             self.section_titles['7.3'])\n        # if there are no colons, this is a simple comment\n        # TODO: are empty comments considered valid?\n        if ':' not in paren_contents:\n            valid_info.out_of += 1\n            valid_info.score += 1\n            return valid_info\n        # otherwise, split into k/v pairs\n        kv_pair_pat = r'(\\S+:)\\s+(.*(?=\\s+\\w+:)|[^:]+$)\\s*'\n        # otherwise, we must split further with intervals coming\n        # first, followed by non-standard comments\n        # we need the count of the matches, and re.findall() only returns\n        # groups if they are present and we wish to see if the entire match\n        # object concatenated together is the same as the original string\n        pmatches = [m for m in regex.finditer(kv_pair_pat, paren_contents)]\n        for i, pmatch in enumerate(pmatches):\n            keyword, val = pmatch.groups()\n            if keyword == 'interval:':\n                valid_info.out_of += 2\n                interval_matches = regex.match(r'^\\s*(?P<interval_number>\\S+)\\s+(?P<interval_units>\\S+)\\s*$', val)\n                # attempt to get the number for the interval\n                if not interval_matches:\n                    valid_info.messages.append('\u00a77.3.3 {}:cell_methods contains an interval specification that does not parse: \"{}\". Should be in format \"interval: <number> <units>\"'.format(var.name, val))\n                else:\n                    try:\n                        float(interval_matches.group('interval_number'))\n                    except ValueError:\n                        valid_info.messages.append('\u00a77.3.3 {}:cell_methods contains an interval value that does not parse as a numeric value: \"{}\".'.format(var.name, interval_matches.group('interval_number')))\n                    else:\n                        valid_info.score += 1\n\n                    # then the units\n                    try:\n                        Unit(interval_matches.group('interval_units'))\n                    except ValueError:\n                        valid_info.messages.append('\u00a77.3.3 {}:cell_methods interval units \"{}\" is not parsable by UDUNITS.'.format(var.name, interval_matches.group('interval_units')))\n                    else:\n                        valid_info.score += 1\n            elif keyword == 'comment:':\n                # comments can't really be invalid, except\n                # if they come first or aren't last, and\n                # maybe if they contain colons embedded in the\n                # comment string\n                valid_info.out_of += 1\n                if len(pmatches) == 1:\n                    valid_info.messages.append('\u00a77.3.3 If there is no standardized information, the keyword comment: should be omitted for variable {}'.format(var.name))\n                # otherwise check that the comment is the last\n                # item in the parentheses\n                elif i != len(pmatches) - 1:\n                    valid_info.messages.append('\u00a77.3.3 The non-standard \"comment:\" element must come after any standard elements in cell_methods for variable {}'.format(var.name))\n                #\n                else:\n                    valid_info.score += 1\n            else:\n                valid_info.out_of += 1\n                valid_info.messages.append('\u00a77.3.3 Invalid cell_methods keyword \"{}\" for variable {}. Must be one of [interval, comment]'.format(keyword, var.name))\n\n\n        # Ensure concatenated reconstructed matches are the same as the\n        # original string.  If they're not, there's likely a formatting error\n        valid_info.assert_true(''.join(m.group(0)\n                                       for m in pmatches) == paren_contents,\n                   \"\u00a77.3.3 Parenthetical content inside {}:cell_methods is not well formed: {}\".format(var.name, paren_contents))\n\n        return valid_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck packed data for the related variable.", "response": "def check_packed_data(self, ds):\n        \"\"\"\n        8.1 Simple packing may be achieved through the use of the optional NUG defined attributes scale_factor and\n        add_offset. After the data values of a variable have been read, they are to be multiplied by the scale_factor,\n        and have add_offset added to them.\n\n        The units of a variable should be representative of the unpacked data.\n\n        If the scale_factor and add_offset attributes are of the same data type as the associated variable, the unpacked\n        data is assumed to be of the same data type as the packed data. However, if the scale_factor and add_offset\n        attributes are of a different data type from the variable (containing the packed data) then the unpacked data\n        should match the type of these attributes, which must both be of type float or both be of type double. An additional\n        restriction in this case is that the variable containing the packed data must be of type byte, short or int. It is\n        not advised to unpack an int into a float as there is a potential precision loss.\n\n        When data to be packed contains missing values the attributes that indicate missing values (_FillValue, valid_min,\n        valid_max, valid_range) must be of the same data type as the packed data.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        \"\"\"\n        ret_val = []\n        for name, var in ds.variables.items():\n\n            add_offset = getattr(var, 'add_offset', None)\n            scale_factor = getattr(var, 'scale_factor', None)\n            if not (add_offset or scale_factor):\n                continue\n\n            valid = True\n            reasoning = []\n\n            # if only one of these attributes is defined, assume they\n            # are the same type (value doesn't matter here)\n            if not add_offset:\n                add_offset = scale_factor\n            if not scale_factor:\n                scale_factor = add_offset\n\n            if type(add_offset) != type(scale_factor):\n                valid = False\n                reasoning.append(\"Attributes add_offset and scale_factor have different data type.\")\n            elif type(scale_factor) != var.dtype.type:\n                # Check both attributes are type float or double\n                if not isinstance(scale_factor, (float, np.floating)):\n                    valid = False\n                    reasoning.append(\"Attributes add_offset and scale_factor are not of type float or double.\")\n                else:\n                    # Check variable type is byte, short or int\n                    if var.dtype.type not in [np.int, np.int8, np.int16, np.int32, np.int64]:\n                        valid = False\n                        reasoning.append(\"Variable is not of type byte, short, or int.\")\n\n            result = Result(BaseCheck.MEDIUM, valid, self.section_titles['8.1'], reasoning)\n            ret_val.append(result)\n            reasoning = []\n\n            valid = True\n            # test further with  _FillValue , valid_min , valid_max , valid_range\n            if hasattr(var, \"_FillValue\"):\n                if var._FillValue.dtype.type != var.dtype.type:\n                    valid = False\n                    reasoning.append(\"Type of %s:_FillValue attribute (%s) does not match variable type (%s)\" %\n                                     (name, var._FillValue.dtype.type, var.dtype.type))\n            if hasattr(var, \"valid_min\"):\n                if var.valid_min.dtype.type != var.dtype.type:\n                    valid = False\n                    reasoning.append(\"Type of %svalid_min attribute (%s) does not match variable type (%s)\" %\n                                     (name, var.valid_min.dtype.type, var.dtype.type))\n            if hasattr(var, \"valid_max\"):\n                if var.valid_max.dtype.type != var.dtype.type:\n                    valid = False\n                    reasoning.append(\"Type of %s:valid_max attribute (%s) does not match variable type (%s)\" %\n                                     (name, var.valid_max.dtype.type, var.dtype.type))\n            if hasattr(var, \"valid_range\"):\n                if var.valid_range.dtype.type != var.dtype.type:\n                    valid = False\n                    reasoning.append(\"Type of %s:valid_range attribute (%s) does not match variable type (%s)\" %\n                                     (name, var.valid_range.dtype.type, var.dtype.type))\n\n            result = Result(BaseCheck.MEDIUM,\n                            valid,\n                            self.section_titles['8.1'],\n                            reasoning)\n            ret_val.append(result)\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_all_features_are_same_type(self, ds):\n        all_the_same = TestCtx(BaseCheck.HIGH,\n                               self.section_titles['9.1'])\n        feature_types_found = defaultdict(list)\n        for name in self._find_geophysical_vars(ds):\n            feature = cfutil.guess_feature_type(ds, name)\n            # If we can't figure out the feature type, don't penalize, just\n            # make a note of it in the messages\n            if feature is not None:\n                feature_types_found[feature].append(name)\n            else:\n                all_the_same.messages.append(\"Unidentifiable feature for variable {}\"\n                                             \"\".format(name))\n        feature_description = ', '.join(['{} ({})'.format(ftr, ', '.join(vrs)) for ftr, vrs in feature_types_found.items()])\n\n        all_the_same.assert_true(len(feature_types_found) < 2,\n                                 \"Different feature types discovered in this dataset: {}\"\n                                 \"\".format(feature_description))\n\n        return all_the_same.to_result()", "response": "Checks that all features in a dataset are of the same type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the featureType attribute for valid CF featureTypes 9. 4", "response": "def check_feature_type(self, ds):\n        \"\"\"\n        Check the global attribute featureType for valid CF featureTypes\n\n        9.4 A global attribute, featureType, is required for all Discrete Geometry representations except the orthogonal\n        multidimensional array representation, for which it is highly recommended.\n\n        The value assigned to the featureType attribute is case-insensitive.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: compliance_checker.base.Result\n        \"\"\"\n        # Due to case insensitive requirement, we list the possible featuretypes\n        # in lower case and check using the .lower() method\n        feature_list = ['point', 'timeseries', 'trajectory', 'profile', 'timeseriesprofile', 'trajectoryprofile']\n\n        feature_type = getattr(ds, 'featureType', None)\n        valid_feature_type = TestCtx(BaseCheck.HIGH, '\u00a79.1 Dataset contains a valid featureType')\n        valid_feature_type.assert_true(feature_type is None or feature_type.lower() in feature_list,\n                                       \"{} is not a valid CF featureType. It must be one of {}\"\n                                       \"\".format(feature_type, ', '.join(feature_list)))\n        return valid_feature_type.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_cf_role(self, ds):\n        valid_roles = ['timeseries_id', 'profile_id', 'trajectory_id']\n        variable_count = 0\n        for variable in ds.get_variables_by_attributes(cf_role=lambda x: x is not None):\n            variable_count += 1\n            name = variable.name\n            valid_cf_role = TestCtx(BaseCheck.HIGH, self.section_titles['9.5'])\n            cf_role = variable.cf_role\n            valid_cf_role.assert_true(cf_role in valid_roles,\n                                      \"{} is not a valid cf_role value. It must be one of {}\"\n                                      \"\".format(name, ', '.join(valid_roles)))\n        if variable_count > 0:\n            m = \"\u00a79.5 The only acceptable values of cf_role for Discrete Geometry CF\"+\\\n                \" data sets are timeseries_id, profile_id, and trajectory_id\"\n            valid_cf_role.assert_true(variable_count < 3, m)\n            return valid_cf_role.to_result()", "response": "Checks that the variable cf_role is one of the allowed values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_variable_features(self, ds):\n        '''\n        Checks the variable feature types match the dataset featureType attribute\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n        feature_list = ['point', 'timeSeries', 'trajectory', 'profile', 'timeSeriesProfile', 'trajectoryProfile']\n        # Don't bother checking if it's not a legal featureType\n        feature_type = getattr(ds, 'featureType', None)\n        if feature_type not in feature_list:\n            return []\n\n        feature_type_map = {\n            'point': [\n                'point'\n            ],\n            'timeSeries': [\n                'timeseries',\n                'multi-timeseries-orthogonal',\n                'multi-timeseries-incomplete',\n            ],\n            'trajectory': [\n                'cf-trajectory',\n                'single-trajectory',\n            ],\n            'profile': [\n                'profile-orthogonal',\n                'profile-incomplete'\n            ],\n            'timeSeriesProfile': [\n                'timeseries-profile-single-station',\n                'timeseries-profile-multi-station',\n                'timeseries-profile-single-ortho-time',\n                'timeseries-profile-multi-ortho-time',\n                'timeseries-profile-ortho-depth',\n                'timeseries-profile-incomplete'\n            ],\n            'trajectoryProfile': [\n                'trajectory-profile-orthogonal',\n                'trajectory-profile-incomplete'\n            ]\n        }\n        for name in self._find_geophysical_vars(ds):\n            variable_feature = cfutil.guess_feature_type(ds, name)\n            # If we can't figure it out, don't check it.\n            if variable_feature is None:\n                continue\n            matching_feature = TestCtx(BaseCheck.MEDIUM,\n                                       self.section_titles['9.1'])\n            matching_feature.assert_true(variable_feature in feature_type_map[feature_type],\n                                         '{} is not a {}, it is detected as a {}'\n                                         ''.format(name, feature_type, variable_feature))\n            ret_val.append(matching_feature.to_result())\n\n        return ret_val", "response": "Checks the variable feature types match the dataset featureType attribute and returns a list of results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck for potentially mislabeled metadata and makes suggestions for how to correct", "response": "def check_hints(self, ds):\n        '''\n        Checks for potentially mislabeled metadata and makes suggestions for how to correct\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n\n        ret_val.extend(self._check_hint_bounds(ds))\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for variables ending with _bounds and make the recommendation", "response": "def _check_hint_bounds(self, ds):\n        '''\n        Checks for variables ending with _bounds, if they are not cell methods,\n        make the recommendation\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n        ret_val = []\n        boundary_variables = cfutil.get_cell_boundary_variables(ds)\n        for name in ds.variables:\n            if name.endswith('_bounds') and name not in boundary_variables:\n                msg = ('{} might be a cell boundary variable but there are no variables that define it '\n                       'as a boundary using the `bounds` attribute.'.format(name))\n                result = Result(BaseCheck.LOW,\n                                True,\n                                self.section_titles['7.1'],\n                                [msg])\n                ret_val.append(result)\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_netcdf(url):\n    '''\n    Returns True if the URL points to a valid local netCDF file\n\n    :param str url: Location of file on the file system\n    '''\n    # Try an obvious exclusion of remote resources\n    if url.startswith('http'):\n        return False\n\n    # If it's a known extension, give it a shot\n    if url.endswith('nc'):\n        return True\n\n    # Brute force\n    with open(url, 'rb') as f:\n        magic_number = f.read(4)\n        if len(magic_number) < 4:\n            return False\n        if is_classic_netcdf(magic_number):\n            return True\n        elif is_hdf5(magic_number):\n            return True\n\n        return False", "response": "Returns True if the URL points to a valid local netCDF file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a value with in a nested dict structure from a dot separated path expression such as system. server. host or a list of key entries Returns a value if found or None if not found", "response": "def get_safe(dict_instance, keypath, default=None):\n    \"\"\"\n    Returns a value with in a nested dict structure from a dot separated\n    path expression such as \"system.server.host\" or a list of key entries\n    @retval Value if found or None\n    \"\"\"\n    try:\n        obj = dict_instance\n        keylist = keypath if type(keypath) is list else keypath.split('.')\n        for key in keylist:\n            obj = obj[key]\n        return obj\n    except Exception:\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload the specified CF standard name table version and saves it to file location", "response": "def download_cf_standard_name_table(version, location=None):\n    '''\n    Downloads the specified CF standard name table version and saves it to file\n\n    :param str version: CF standard name table version number (i.e 34)\n    :param str location: Path/filename to write downloaded xml file to\n    '''\n\n    if location is None:  # This case occurs when updating the packaged version from command line\n        location = resource_filename('compliance_checker', 'data/cf-standard-name-table.xml')\n\n    url = \"http://cfconventions.org/Data/cf-standard-names/{0}/src/cf-standard-name-table.xml\".format(version)\n    r = requests.get(url, allow_redirects=True)\n    if r.status_code == 200:\n        print(\"Downloading cf-standard-names table version {0} from: {1}\".format(version, url), file=sys.stderr)\n        with open(location, 'wb') as f:\n            f.write(r.content)\n    else:\n        r.raise_for_status()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path to the data directory to download CF standard names.", "response": "def create_cached_data_dir():\n    '''\n    Returns the path to the data directory to download CF standard names.\n    Use $XDG_DATA_HOME.\n    '''\n    writable_directory = os.path.join(os.path.expanduser('~'), '.local', 'share')\n    data_directory = os.path.join(os.environ.get(\"XDG_DATA_HOME\", writable_directory),\n                                  'compliance-checker')\n    if not os.path.isdir(data_directory):\n        os.makedirs(data_directory)\n\n    return data_directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map_axes(dim_vars, reverse_map=False):\n    ret_val = defaultdict(list)\n    axes = ['X', 'Y', 'Z', 'T']\n\n    for k, v in dim_vars.items():\n        axis = getattr(v, 'axis', '')\n        if not axis:\n            continue\n\n        axis = axis.upper()\n        if axis in axes:\n            if reverse_map:\n                ret_val[k].append(axis)\n            else:\n                ret_val[axis].append(k)\n\n    return dict(ret_val)", "response": "Map axes to their names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_coord_vars(ncds):\n    coord_vars = []\n\n    for d in ncds.dimensions:\n        if d in ncds.variables and ncds.variables[d].dimensions == (d,):\n            coord_vars.append(ncds.variables[d])\n\n    return coord_vars", "response": "Finds all coordinate variables in a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nidentifying if a variable is represents time", "response": "def is_time_variable(varname, var):\n    \"\"\"\n    Identifies if a variable is represents time\n    \"\"\"\n    satisfied = varname.lower() == 'time'\n    satisfied |= getattr(var, 'standard_name', '') == 'time'\n    satisfied |= getattr(var, 'axis', '') == 'T'\n    satisfied |= units_convertible('seconds since 1900-01-01', getattr(var, 'units', ''))\n    return satisfied"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if a variable is a vertical coordinate variable 4. 3", "response": "def is_vertical_coordinate(var_name, var):\n    \"\"\"\n    Determines if a variable is a vertical coordinate variable\n\n    4.3\n    A vertical coordinate will be identifiable by: units of pressure; or the presence of the positive attribute with a\n    value of up or down (case insensitive).  Optionally, the vertical type may be indicated additionally by providing\n    the standard_name attribute with an appropriate value, and/or the axis attribute with the value Z.\n    \"\"\"\n    # Known name\n    satisfied = var_name.lower() in _possiblez\n    satisfied |= getattr(var, 'standard_name', '') in _possiblez\n    # Is the axis set to Z?\n    satisfied |= getattr(var, 'axis', '').lower() == 'z'\n    is_pressure = units_convertible(getattr(var, 'units', '1'), 'dbar')\n    # Pressure defined or positive defined\n    satisfied |= is_pressure\n    if not is_pressure:\n        satisfied |= getattr(var, 'positive', '').lower() in ('up', 'down')\n    return satisfied"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of variable names that are applicable to ACDD Metadata Checks for variables. This includes geophysical and coordinate variables only.", "response": "def get_applicable_variables(self, ds):\n        '''\n        Returns a list of variable names that are applicable to ACDD Metadata\n        Checks for variables. This includes geophysical and coordinate\n        variables only.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        if self._applicable_variables is None:\n            self.applicable_variables = cfutil.get_geophysical_variables(ds)\n            varname = cfutil.get_time_variable(ds)\n            # avoid duplicates by checking if already present\n            if varname and (varname not in self.applicable_variables):\n                self.applicable_variables.append(varname)\n            varname = cfutil.get_lon_variable(ds)\n            if varname and (varname not in self.applicable_variables):\n                self.applicable_variables.append(varname)\n            varname = cfutil.get_lat_variable(ds)\n            if varname and (varname not in self.applicable_variables):\n                self.applicable_variables.append(varname)\n            varname = cfutil.get_z_variable(ds)\n            if varname and (varname not in self.applicable_variables):\n                self.applicable_variables.append(varname)\n\n        return self.applicable_variables"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_var_long_name(self, ds):\n        '''\n        Checks each applicable variable for the long_name attribute\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n\n        # ACDD Variable Metadata applies to all coordinate variables and\n        # geophysical variables only.\n\n        for variable in self.get_applicable_variables(ds):\n            msgs = []\n            long_name = getattr(ds.variables[variable], 'long_name', None)\n            check = long_name is not None\n            if not check:\n                msgs.append(\"long_name\")\n            results.append(Result(BaseCheck.HIGH, check, self._var_header.format(variable), msgs))\n\n        return results", "response": "Checks each applicable variable for the long_name attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_var_standard_name(self, ds):\n        '''\n        Checks each applicable variable for the standard_name attribute\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        for variable in self.get_applicable_variables(ds):\n            msgs = []\n            std_name = getattr(ds.variables[variable], 'standard_name', None)\n            check = std_name is not None\n            if not check:\n                msgs.append(\"standard_name\")\n            results.append(Result(BaseCheck.HIGH, check, self._var_header.format(variable), msgs))\n\n        return results", "response": "Checks each applicable variable for the standard_name attribute"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_var_units(self, ds):\n        '''\n        Checks each applicable variable for the units attribute\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        for variable in self.get_applicable_variables(ds):\n            msgs = []\n            # Check units and dims for variable\n            unit_check = hasattr(ds.variables[variable], 'units')\n            no_dim_check = (getattr(ds.variables[variable], 'dimensions') == tuple())\n            # Check if we have no dimensions.  If no dims, skip test\n            if no_dim_check:\n                continue\n            # Check if we have no units\n            if not unit_check:\n                msgs.append(\"units\")\n            results.append(Result(BaseCheck.HIGH, unit_check, self._var_header.format(variable), msgs))\n\n        return results", "response": "Checks each applicable variable for the units attribute\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_acknowledgment(self, ds):\n        '''\n        Check if acknowledgment/acknowledgment attribute is present. Because\n        acknowledgement has its own check, we are keeping it out of the Global\n        Attributes (even though it is a Global Attr).\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        check = False\n        messages = []\n        if hasattr(ds, 'acknowledgment') or hasattr(ds, 'acknowledgement'):\n            check = True\n        else:\n            messages.append(\"acknowledgment/acknowledgement not present\")\n\n        # name=\"Global Attributes\" so gets grouped with Global Attributes\n        return Result(BaseCheck.MEDIUM, check, \"Global Attributes\", msgs=messages)", "response": "Check if acknowledgment attribute is present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the values of geospatial_lat_min and geospatial_lat_max approximately match the data.", "response": "def check_lat_extents(self, ds):\n        '''\n        Check that the values of geospatial_lat_min/geospatial_lat_max\n        approximately match the data.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n\n        if not (hasattr(ds, 'geospatial_lat_min') or hasattr(ds, 'geospatial_lat_max')):\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lat_extents_match',\n                          ['geospatial_lat_min/max attribute not found, CF-1.6 spec chapter 4.1'])\n\n        try: # type cast\n            lat_min = float(ds.geospatial_lat_min)\n            lat_max = float(ds.geospatial_lat_max)\n        except ValueError:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lat_extents_match',\n                          ['Could not convert one of geospatial_lat_min ({}) or max ({}) to float see CF-1.6 spec chapter 4.1'\n                          ''.format(ds.geospatial_lat_min, ds.geospatial_lat_max)])\n\n\n        # identify lat var(s) as per CF 4.1\n        lat_vars = {}       # var -> number of criteria passed\n        for name, var in ds.variables.items():\n\n            # must have units\n            if not hasattr(var, 'units'):\n                continue\n\n            lat_vars[var] = 0\n\n            # units in this set\n            if var.units in _possibleyunits:\n                lat_vars[var] += 1\n\n            # standard name of \"latitude\"\n            if hasattr(var, 'standard_name') and var.standard_name == 'latitude':\n                lat_vars[var] += 1\n\n            # axis of \"Y\"\n            if hasattr(var, 'axis') and var.axis == 'Y':\n                lat_vars[var] += 1\n\n        # trim out any zeros\n        lat_vars = {k: v for k, v in lat_vars.items() if v > 0}\n\n        if len(lat_vars) == 0:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lat_extents_match',\n                          ['Could not find lat variable to test extent of geospatial_lat_min/max, see CF-1.6 spec chapter 4.1'])\n\n        # sort by criteria passed\n        final_lats = sorted(lat_vars, key=lambda x: lat_vars[x], reverse=True)\n\n        obs_mins = {var._name: np.nanmin(var) for var in final_lats if not np.isnan(var).all()}\n        obs_maxs = {var._name: np.nanmax(var) for var in final_lats if not np.isnan(var).all()}\n\n        min_pass = any((np.isclose(lat_min, min_val) for min_val in obs_mins.values()))\n        max_pass = any((np.isclose(lat_max, max_val) for max_val in obs_maxs.values()))\n\n        allpass = sum((min_pass, max_pass))\n\n        msgs = []\n        if not min_pass:\n            msgs.append(\"Data for possible latitude variables (%s) did not match geospatial_lat_min value (%s)\" % (obs_mins, lat_min))\n        if not max_pass:\n            msgs.append(\"Data for possible latitude variables (%s) did not match geospatial_lat_max value (%s)\" % (obs_maxs, lat_max))\n\n        return Result(BaseCheck.MEDIUM,\n                      (allpass, 2),\n                      'geospatial_lat_extents_match',\n                      msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the values of geospatial_lon_min and geospatial_lon_max approximately match the data.", "response": "def check_lon_extents(self, ds):\n        '''\n        Check that the values of geospatial_lon_min/geospatial_lon_max\n        approximately match the data.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n\n        if not (hasattr(ds, 'geospatial_lon_min') and hasattr(ds, 'geospatial_lon_max')):\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lon_extents_match',\n                          ['geospatial_lon_min/max attribute not found, CF-1.6 spec chapter 4.1'])\n\n        try: # type cast\n            lon_min = float(ds.geospatial_lon_min)\n            lon_max = float(ds.geospatial_lon_max)\n        except ValueError:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lon_extents_match',\n                          ['Could not convert one of geospatial_lon_min ({}) or max ({}) to float see CF-1.6 spec chapter 4.1'\n                          ''.format(ds.geospatial_lon_min, ds.geospatial_lon_max)])\n\n        # identify lon var(s) as per CF 4.2\n        lon_vars = {}       # var -> number of criteria passed\n        for name, var in ds.variables.items():\n\n            # must have units\n            if not hasattr(var, 'units'):\n                continue\n\n            lon_vars[var] = 0\n\n            # units in this set\n            if var.units in _possiblexunits:\n                lon_vars[var] += 1\n\n            # standard name of \"longitude\"\n            if hasattr(var, 'standard_name') and var.standard_name == 'longitude':\n                lon_vars[var] += 1\n\n            # axis of \"Y\"\n            if hasattr(var, 'axis') and var.axis == 'X':\n                lon_vars[var] += 1\n\n        # trim out any zeros\n        lon_vars = {k: v for k, v in lon_vars.items() if v > 0}\n\n        if len(lon_vars) == 0:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_lon_extents_match',\n                          ['Could not find lon variable to test extent of geospatial_lon_min/max, see CF-1.6 spec chapter 4.2'])\n\n        # sort by criteria passed\n        final_lons = sorted(lon_vars, key=lambda x: lon_vars[x], reverse=True)\n\n        obs_mins = {var._name: np.nanmin(var) for var in final_lons if not np.isnan(var).all()}\n        obs_maxs = {var._name: np.nanmax(var) for var in final_lons if not np.isnan(var).all()}\n\n        min_pass = any((np.isclose(lon_min, min_val) for min_val in obs_mins.values()))\n        max_pass = any((np.isclose(lon_max, max_val) for max_val in obs_maxs.values()))\n\n        allpass = sum((min_pass, max_pass))\n\n        msgs = []\n        if not min_pass:\n            msgs.append(\"Data for possible longitude variables (%s) did not match geospatial_lon_min value (%s)\" % (obs_mins, lon_min))\n        if not max_pass:\n            msgs.append(\"Data for possible longitude variables (%s) did not match geospatial_lon_max value (%s)\" % (obs_maxs, lon_max))\n\n        return Result(BaseCheck.MEDIUM,\n                      (allpass, 2),\n                      'geospatial_lon_extents_match',\n                      msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_geospatial_bounds(self, ds):\n        var = getattr(ds, 'geospatial_bounds', None)\n        check = var is not None\n        if not check:\n            return ratable_result(False,\n                                  \"Global Attributes\", # grouped with Globals\n                                  [\"geospatial_bounds not present\"])\n\n        try:\n            # TODO: verify that WKT is valid given CRS (defaults to EPSG:4326\n            #       in ACDD.\n            from_wkt(ds.geospatial_bounds)\n        except AttributeError:\n            return ratable_result(False,\n                                  \"Global Attributes\", # grouped with Globals\n                                  ['Could not parse WKT, possible bad value for WKT'])\n        # parsed OK\n        else:\n            return ratable_result(True, \"Global Attributes\", tuple())", "response": "Checks that the geospatial bounds is well formed OGC WKT"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the scalar value of Z compared to the vertical extents which should also be equivalent", "response": "def _check_scalar_vertical_extents(self, ds, z_variable):\n        '''\n        Check the scalar value of Z compared to the vertical extents which\n        should also be equivalent\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str z_variable: Name of the variable representing the Z-Axis\n        '''\n        vert_min = ds.geospatial_vertical_min\n        vert_max = ds.geospatial_vertical_max\n        msgs = []\n        total = 2\n\n        zvalue = ds.variables[z_variable][:].item()\n        if not np.isclose(vert_min, vert_max):\n            msgs.append(\"geospatial_vertical_min != geospatial_vertical_max for scalar depth values, %s != %s\" % (\n                vert_min,\n                vert_max\n            ))\n\n        if not np.isclose(vert_max, zvalue):\n            msgs.append(\"geospatial_vertical_max != %s values, %s != %s\" % (\n                z_variable,\n                vert_max,\n                zvalue\n            ))\n\n        return Result(BaseCheck.MEDIUM,\n                      (total - len(msgs), total),\n                      'geospatial_vertical_extents_match',\n                      msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the values of geospatial_vertical_min and geospatial_vertical_max approximately match the data.", "response": "def check_vertical_extents(self, ds):\n        \"\"\"\n        Check that the values of geospatial_vertical_min/geospatial_vertical_max approximately match the data.\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        \"\"\"\n        if not (hasattr(ds, 'geospatial_vertical_min') and hasattr(ds, 'geospatial_vertical_max')):\n            return\n\n        z_variable = cfutil.get_z_variable(ds)\n        if not z_variable:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'geospatial_vertical_extents_match',\n                          ['Could not find vertical variable to test extent of geospatial_vertical_min/geospatial_vertical_max, see CF-1.6 spec chapter 4.3'])\n        if ds.variables[z_variable].dimensions == tuple():\n            return self._check_scalar_vertical_extents(ds, z_variable)\n\n        return self._check_total_z_extents(ds, z_variable)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the values of time_coverage_start and time_coverage_end approximately match the data.", "response": "def check_time_extents(self, ds):\n        \"\"\"\n        Check that the values of time_coverage_start/time_coverage_end approximately match the data.\n        \"\"\"\n        if not (hasattr(ds, 'time_coverage_start') and hasattr(ds, 'time_coverage_end')):\n            return\n\n        # Parse the ISO 8601 formatted dates\n        try:\n            t_min = dateparse(ds.time_coverage_start)\n            t_max = dateparse(ds.time_coverage_end)\n        except:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'time_coverage_extents_match',\n                          ['time_coverage attributes are not formatted properly. Use the ISO 8601:2004 date format, preferably the extended format.'])\n\n        timevar = cfutil.get_time_variable(ds)\n\n        if not timevar:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'time_coverage_extents_match',\n                          ['Could not find time variable to test extent of time_coverage_start/time_coverage_end, see CF-1.6 spec chapter 4.4'])\n\n        # Time should be monotonically increasing, so we make that assumption here so we don't have to download THE ENTIRE ARRAY\n        try:\n            # num2date returns as naive date, but with time adjusted to UTC\n            # we need to attach timezone information here, or the date\n            # subtraction from t_min/t_max will assume that a naive timestamp is\n            # in the same time zone and cause erroneous results.\n            # Pendulum uses UTC by default, but we are being explicit here\n            time0 = pendulum.instance(num2date(ds.variables[timevar][0],\n                                      ds.variables[timevar].units), 'UTC')\n            time1 = pendulum.instance(num2date(ds.variables[timevar][-1],\n                                      ds.variables[timevar].units), 'UTC')\n        except:\n            return Result(BaseCheck.MEDIUM,\n                          False,\n                          'time_coverage_extents_match',\n                          ['Failed to retrieve and convert times for variables %s.' % timevar])\n\n        start_dt = abs(time0 - t_min)\n        end_dt = abs(time1 - t_max)\n\n        score = 2\n        msgs = []\n        if start_dt > timedelta(hours=1):\n            msgs.append(\"Date time mismatch between time_coverage_start and actual \"\n                        \"time values %s (time_coverage_start) != %s (time[0])\" % (t_min.isoformat(), time0.isoformat()))\n            score -= 1\n        if end_dt > timedelta(hours=1):\n            msgs.append(\"Date time mismatch between time_coverage_end and actual \"\n                        \"time values %s (time_coverage_end) != %s (time[N])\" % (t_max.isoformat(), time1.isoformat()))\n            score -= 1\n\n        return Result(BaseCheck.MEDIUM,\n                      (score, 2),\n                      'time_coverage_extents_match',\n                      msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_convention_version(self, ds):\n        try:\n            for convention in getattr(ds, \"Conventions\", '').replace(' ', '').split(','):\n                if convention == 'ACDD-' + self._cc_spec_version:\n                    return ratable_result((2, 2), None, []) # name=None so grouped with Globals\n\n            # if no/wrong ACDD convention, return appropriate result\n            # Result will have name \"Global Attributes\" to group with globals\n            m = [\"Conventions does not contain 'ACDD-{}'\".format(self._cc_spec_version)]\n            return ratable_result((1, 2), \"Global Attributes\", m)\n        except AttributeError: # NetCDF attribute not found\n            m = [\"No Conventions attribute present; must contain ACDD-{}\".format(self._cc_spec_version)]\n            # Result will have name \"Global Attributes\" to group with globals\n            return ratable_result((0, 2), \"Global Attributes\", m)", "response": "Verify that the version in the Conventions field is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_metadata_link(self, ds):\n        '''\n        Checks if metadata link is formed in a rational manner\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        if not hasattr(ds, u'metadata_link'):\n            return\n        msgs = []\n        meta_link = getattr(ds, 'metadata_link')\n        if 'http' not in meta_link:\n            msgs.append('Metadata URL should include http:// or https://')\n        valid_link = (len(msgs) == 0)\n        return Result(BaseCheck.LOW, valid_link, 'metadata_link_valid', msgs)", "response": "Checks if the metadata link is formed in a rational manner\n            is not present in the netCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_id_has_no_blanks(self, ds):\n        '''\n        Check if there are blanks in the id field\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        if not hasattr(ds, u'id'):\n            return\n        if ' ' in getattr(ds, u'id'):\n            return Result(BaseCheck.MEDIUM, False, 'no_blanks_in_id',\n                          msgs=[u'There should be no blanks in the id field'])\n        else:\n            return Result(BaseCheck.MEDIUM, True, 'no_blanks_in_id', msgs=[])", "response": "Check if there are no blanks in the id field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_var_coverage_content_type(self, ds):\n        '''\n        Check coverage content type against valid ISO-19115-1 codes\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        '''\n        results = []\n        for variable in cfutil.get_geophysical_variables(ds):\n            msgs = []\n            ctype = getattr(ds.variables[variable],\n                            'coverage_content_type', None)\n            check = ctype is not None\n            if not check:\n                msgs.append(\"coverage_content_type\")\n                results.append(Result(BaseCheck.HIGH, check,\n                                      self._var_header.format(variable), msgs))\n                continue\n\n            # ISO 19115-1 codes\n            valid_ctypes = {\n                'image',\n                'thematicClassification',\n                'physicalMeasurement',\n                'auxiliaryInformation',\n                'qualityInformation',\n                'referenceInformation',\n                'modelResult',\n                'coordinate'\n            }\n            if ctype not in valid_ctypes:\n                msgs.append(\"coverage_content_type in \\\"%s\\\"\"\n                            % (variable, sorted(valid_ctypes)))\n                results.append(Result(BaseCheck.HIGH, check, # append to list\n                                      self._var_header.format(variable), msgs))\n\n        return results", "response": "Checks coverage content type against valid ISO - 19115 - 1 codes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef no_missing_terms(formula_name, term_set):\n    reqd_terms = dimless_vertical_coordinates[formula_name]\n    def has_all_terms(reqd_termset):\n        return len(reqd_termset ^ term_set) == 0\n\n    if isinstance(reqd_terms, set):\n        return has_all_terms(reqd_terms)\n    # if it's not a set, it's likely some other form of iterable with multiple\n    # possible definitions i.e. a/ap are interchangeable in\n    else:\n        return any(has_all_terms(req) for req in reqd_terms)", "response": "Returns True if the set of terms corresponding to the\n    entries in Appendix D are not present in term_set False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if an attribute is in a list of allowed values", "response": "def std_check_in(dataset, name, allowed_vals):\n    \"\"\"\n    Returns 0 if attr not present, 1 if present but not in correct value, 2 if good\n    \"\"\"\n    if not hasattr(dataset, name):\n        return 0\n\n    ret_val = 1\n    if getattr(dataset, name) in allowed_vals:\n        ret_val += 1\n\n    return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if an attribute is present in the dataset and if yes checks if it is present in the dataset and returns a list of Result objects.", "response": "def attr_check(kvp, ds, priority, ret_val, gname=None):\n    \"\"\"\n    Handles attribute checks for simple presence of an attribute, presence of\n    one of several attributes, and passing a validation function.  Returns a\n    status along with an error message in the event of a failure.  Mutates\n    ret_val parameter\n\n    :param tuple(str, func) or str l: the attribute being checked\n    :param netCDF4 dataset ds       : dataset being checked\n    :param int priority             : priority level of check\n    :param list ret_val             : result to be returned\n    :param str or None gname        : group name assigned to a group of attribute Results\n    \"\"\"\n\n    msgs = []\n    name, other = kvp\n    if other is None:\n        res = std_check(ds, name)\n        if not res:\n            msgs = [\"%s not present\" % name]\n        else:\n            try:\n                # see if this attribute is a string, try stripping\n                # whitespace, and return an error if empty\n                att_strip = getattr(ds, name).strip()\n                if not att_strip:\n                    res = False\n                    msgs = [\"%s is empty or completely whitespace\" % name]\n            # if not a string/has no strip method we should be OK\n            except AttributeError:\n                pass\n\n        # gname arg allows the global attrs to be grouped together\n        ret_val.append(Result(\n            priority,\n            value=res,\n            name=gname if gname else name,\n            msgs=msgs\n        ))\n    elif hasattr(other, '__iter__'):\n        # redundant, we could easily do this with a hasattr\n        # check instead\n        res = std_check_in(ds, name, other)\n        if res == 0:\n            msgs.append(\"%s not present\" % name)\n        elif res == 1:\n            msgs.append(\"%s present, but not in expected value list (%s)\" % (name, other))\n\n        ret_val.append(\n            Result(\n                priority,\n                (res, 2),\n                gname if gname else name, # groups Globals if supplied\n                msgs\n            )\n        )\n    # if we have an XPath expression, call it on the document\n    elif type(other) is etree.XPath:\n        # TODO: store tree instead of creating it each time?\n        res = xpath_check(ds._root, other)\n        if not res:\n            msgs = [\"XPath for {} not found\".format(name)]\n        ret_val.append(\n            Result(\n                priority,\n                res,\n                gname if gname else name,\n                msgs\n            )\n        )\n    # if the attribute is a function, call it\n    # right now only supports single attribute\n    # important note: current magic approach uses all functions\n    # starting with \"check\".  Avoid naming check functions\n    # starting with check if you want to pass them in with\n    # a tuple to avoid them being checked more than once\n    elif hasattr(other, '__call__'):\n        # check that the attribute is actually present.\n        # This reduces boilerplate in functions by not needing\n        # to check whether the attribute is present every time\n        # and instead focuses on the core functionality of the\n        # test\n\n        res = other(ds) # call the method on the dataset\n        if not res:\n            msgs = [\"%s not present\" % name]\n            ret_val.append(\n                Result(\n                    priority,\n                    res,\n                    gname if gname else name,\n                    msgs\n                )\n            )\n        else:\n            ret_val.append(res(priority))\n    # unsupported second type in second\n    else:\n        raise TypeError(\"Second arg in tuple has unsupported type: {}\".format(type(other)))\n\n\n    return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_has(priority=BaseCheck.HIGH, gname=None):\n\n    def _inner(func):\n        def _dec(s, ds):\n            attr_process = kvp_convert(func(s, ds))\n\n            ret_val = []\n            # could potentially run tests in parallel if we eliminated side\n            # effects on `ret_val`\n            for kvp in attr_process.items():\n                # function mutates ret_val\n                attr_check(kvp, ds, priority, ret_val, gname)\n            return ret_val\n\n        return wraps(func)(_dec)\n\n    return _inner", "response": "Decorator to check if a dataset has given attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform scalar return values into Result.", "response": "def fix_return_value(v, method_name, method=None, checker=None):\n    \"\"\"\n    Transforms scalar return values into Result.\n    \"\"\"\n    # remove common check prefix\n    method_name = (method_name or method.__func__.__name__).replace(\"check_\",\"\")\n    if v is None or not isinstance(v, Result):\n        v = Result(value=v, name=method_name)\n\n    v.name         = v.name or method_name\n\n    v.checker      = checker\n    v.check_method = method\n\n    return v"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a partial function with a Result that has not been weighted.", "response": "def ratable_result(value, name, msgs):\n    \"\"\"Returns a partial function with a Result that has not been weighted.\"\"\"\n    return lambda w: Result(w, value, name, msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score_group(group_name=None):\n    '''\n    Warning this is deprecated as of Compliance Checker v3.2!\n\n    Please do not using scoring groups and update your plugins\n    if necessary\n    '''\n    warnings.warn('Score_group is deprecated as of Compliance Checker v3.2.')\n    def _inner(func):\n        def _dec(s, ds):\n            ret_val = func(s, ds)\n            \"\"\"\n            if group_name != None and not isinstance(ret_val[0], tuple):\n                return tuple([(group_name, ret_val[0])] + list(ret_val[1:]))\n            \"\"\"\n            # multiple returns\n            if not isinstance(ret_val, list):\n                ret_val = [ret_val]\n\n            def dogroup(r):\n                cur_grouping = r.name\n\n                if isinstance(cur_grouping, tuple):\n                    cur_grouping = list(cur_grouping)\n                elif not isinstance(cur_grouping, list):\n                    cur_grouping = [cur_grouping]\n\n                cur_grouping.insert(0, group_name)\n\n                return Result(r.weight, r.value, tuple(cur_grouping), r.msgs)\n\n            ret_val = [fix_return_value(x, func.__name__, func, s) for x in\n                       ret_val]\n            ret_val = list(map(dogroup, ret_val))\n\n            return ret_val\n        return wraps(func)(_dec)\n    return _inner", "response": "Decorator for scoring group functions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if an attribute is in the set of allowed values", "response": "def std_check_in(cls, dataset, name, allowed_vals):\n        \"\"\"\n        Returns 0 if attr not present, 1 if present but not in correct value, 2 if good\n        \"\"\"\n        if name not in dataset.ncattrs():\n            return 0\n\n        ret_val = 1\n        if dataset.getncattr(name) in allowed_vals:\n            ret_val += 1\n\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize(self):\n        '''\n        Returns a serializable dictionary that represents the result object\n        '''\n        return {\n            'name' : self.name,\n            'weight' : self.weight,\n            'value' : self.value,\n            'msgs' : self.msgs,\n            'children' : [i.serialize() for i in self.children]\n        }", "response": "Returns a serializable dictionary that represents the result object\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_generator_plugins(cls):\n\n        if not hasattr(cls, 'suite_generators'):\n            gens = working_set.iter_entry_points('compliance_checker.generators')\n            cls.suite_generators = [x.resolve() for x in gens]\n\n        return cls.suite_generators", "response": "Return a list of classes from external plugins that are used to\n        generate checker classes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload checker classes from generator plugins and store them in cls. checkers.", "response": "def load_generated_checkers(cls, args):\n        \"\"\"\n        Load checker classes from generator plugins\n        \"\"\"\n\n        for gen in cls._get_generator_plugins():\n            checkers = gen.get_checkers(args)\n            cls.checkers.update(checkers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload all available checker classes and set them as the class attributes.", "response": "def load_all_available_checkers(cls):\n        \"\"\"\n        Helper method to retrieve all sub checker classes derived from various\n        base classes.\n        \"\"\"\n        for x in working_set.iter_entry_points('compliance_checker.suites'):\n            try:\n                xl = x.resolve()\n                cls.checkers[':'.join((xl._cc_spec, xl._cc_spec_version))] = xl\n            # TODO: remove this once all checkers move over to the new\n            #       _cc_spec, _cc_spec_version\n            except AttributeError:\n                # if there are versioned classes, it will get overwritten by the\n                # latest version later.  If there are not, it will be assigned\n                # the checker as the main class\n                # TODO: nix name attribute in plugins.  Keeping in for now\n                #       to provide backwards compatibility\n                cls.checkers[getattr(xl, 'name', None) or xl._cc_spec] = xl\n\n            except Exception as e:\n                print(\"Could not load\", x, \":\", e, file=sys.stderr)\n        # find the latest version of versioned checkers and set that as the\n        # default checker for compliance checker if no version is specified\n        ver_checkers = sorted([c.split(':', 1) for c\n                               in cls.checkers if ':' in c])\n        for spec, versions in itertools.groupby(ver_checkers, itemgetter(0)):\n            version_nums = [v[-1] for v in versions]\n            try:\n                latest_version = str(max(StrictVersion(v) for v\n                                         in version_nums))\n            # if the version can't be parsed as a StrictVersion, parse\n            # according to character collation\n            except ValueError:\n                latest_version = max(version_nums)\n            cls.checkers[spec] = cls.checkers[spec + ':latest'] = \\\n                cls.checkers[':'.join((spec, latest_version))]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_checks(self, checkclass, skip_checks):\n        meths = inspect.getmembers(checkclass, inspect.ismethod)\n        # return all check methods not among the skipped checks\n        returned_checks = []\n        for fn_name, fn_obj in meths:\n            if (fn_name.startswith(\"check_\") and\n                skip_checks[fn_name] != BaseCheck.HIGH):\n                returned_checks.append((fn_obj, skip_checks[fn_name]))\n\n        return returned_checks", "response": "Helper method to retreive check methods from a Checker class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a check and appends a result to the list of Result objects.", "response": "def _run_check(self, check_method, ds, max_level):\n        \"\"\"\n        Runs a check and appends a result to the values list.\n        @param bound method check_method: a given check method\n        @param netCDF4 dataset ds\n        @param int max_level: check level\n        @return list: list of Result objects\n        \"\"\"\n        val = check_method(ds)\n        if isinstance(val, list):\n            check_val = []\n            for v in val:\n                res = fix_return_value(v, check_method.__func__.__name__,\n                                       check_method, check_method.__self__)\n                if max_level is None or res.weight > max_level:\n                    check_val.append(res)\n\n            return check_val\n        else:\n            check_val = fix_return_value(val, check_method.__func__.__name__,\n                                         check_method, check_method.__self__)\n            if max_level is None or check_val.weight > max_level:\n                return [check_val]\n            else:\n                return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_check_versioned_name(self, check_name):\n        if ':' not in check_name or ':latest' in check_name:\n            check_name = ':'.join((check_name.split(':')[0],\n                                   self.checkers[check_name]._cc_spec_version))\n        return check_name", "response": "Returns the check name with the version number"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_valid_checkers(self, ds, checker_names):\n\n        assert len(self.checkers) > 0, \"No checkers could be found.\"\n\n        if len(checker_names) == 0:\n            checker_names = list(self.checkers.keys())\n\n        args = [(name, self.checkers[name]) for name in checker_names if name in self.checkers]\n        valid = []\n\n        all_checked = set([a[1] for a in args])  # only class types\n        checker_queue = set(args)\n        while len(checker_queue):\n            name, a = checker_queue.pop()\n            # is the current dataset type in the supported filetypes\n            # for the checker class?\n            if type(ds) in a().supported_ds:\n                valid.append((name, a))\n\n            # add any subclasses of the checker class\n            for subc in a.__subclasses__():\n                if subc not in all_checked:\n                    all_checked.add(subc)\n                    checker_queue.add((name, subc))\n\n        return valid", "response": "Returns a filtered list of 2 - tuples of name valid checker and checker class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_skip_checks(cls, skip_checks):\n\n        check_dict = defaultdict(lambda: None)\n        # A is for \"all\", \"M\" is for medium, \"L\" is for low\n        check_lookup = {'A': BaseCheck.HIGH,\n                        'M': BaseCheck.MEDIUM,\n                        'L': BaseCheck.LOW}\n\n        for skip_check_spec in skip_checks:\n            split_check_spec = skip_check_spec.split(':')\n            check_name = split_check_spec[0]\n            if len(split_check_spec) < 2:\n               check_max_level = BaseCheck.HIGH\n            else:\n                try:\n                    check_max_level = check_lookup[split_check_spec[1]]\n                except KeyError:\n                    warnings.warn(\"Skip specifier '{}' on check '{}' not found,\"\n                                  \" defaulting to skip entire check\".format(split_check_spec[1], check_name))\n                    check_max_level = BaseCheck.HIGH\n\n            check_dict[check_name] = check_max_level\n\n        return check_dict", "response": "Processes an iterable of skip_checks with strings and returns a dict containing check_name = > max_skip_level pairs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, ds, skip_checks, *checker_names):\n\n        ret_val = {}\n        checkers = self._get_valid_checkers(ds, checker_names)\n\n        if skip_checks is not None:\n            skip_check_dict = CheckSuite._process_skip_checks(skip_checks)\n        else:\n            skip_check_dict = defaultdict(lambda: None)\n\n        if len(checkers) == 0:\n            print(\"No valid checkers found for tests '{}'\".format(\",\".join(checker_names)))\n\n        for checker_name, checker_class in checkers:\n\n            checker = checker_class() # instantiate a Checker object\n            checker.setup(ds)         # setup method to prep\n\n            checks = self._get_checks(checker, skip_check_dict)\n            vals = []\n            errs = {}   # check method name -> (exc, traceback)\n\n            for c, max_level in checks:\n                try:\n                    vals.extend(self._run_check(c, ds, max_level))\n                except Exception as e:\n                    errs[c.__func__.__name__] = (e, sys.exc_info()[2])\n\n            # score the results we got back\n            groups = self.scores(vals)\n\n            ret_val[checker_name] = groups, errs\n\n        return ret_val", "response": "Runs this CheckSuite on the dataset with all the passed Checker instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_structure(self, check_name, groups, source_name, limit=1):\n        '''\n        Compiles the checks, results and scores into an aggregate structure which looks like:\n\n            {\n              \"scored_points\": 396,\n              \"low_count\": 0,\n              \"possible_points\": 400,\n              \"testname\": \"gliderdac\",\n              \"medium_count\": 2,\n              \"source_name\": \".//rutgers/ru01-20140120T1444/ru01-20140120T1649.nc\",\n              \"high_count\": 0,\n              \"all_priorities\" : [...],\n              \"high_priorities\": [...],\n              \"medium_priorities\" : [...],\n              \"low_priorities\" : [...]\n            }\n\n        @param check_name  The test which was run\n        @param groups      List of results from compliance checker\n        @param source_name Source of the dataset, used for title\n        '''\n        aggregates = {}\n\n        aggregates['scored_points'] = 0\n        aggregates['possible_points'] = 0\n        high_priorities = []\n        medium_priorities = []\n        low_priorities = []\n        all_priorities = []\n\n        aggregates['high_count'] = 0\n        aggregates['medium_count'] = 0\n        aggregates['low_count'] = 0\n\n        def named_function(result):\n            for child in result.children:\n                all_priorities.append(child)\n                named_function(child)\n\n        # For each result, bin them into the appropriate category, put them all\n        # into the all_priorities category and add up the point values\n        for res in groups:\n            if res.weight < limit:\n                continue\n            # If the result has 0 possible points, then it was not valid for\n            # this dataset and contains no meaningful information\n            if res.value[1] == 0:\n                continue\n            aggregates['scored_points'] += res.value[0]\n            aggregates['possible_points'] += res.value[1]\n            if res.weight == 3:\n                high_priorities.append(res)\n                if res.value[0] < res.value[1]:\n                    aggregates['high_count'] += 1\n            elif res.weight == 2:\n                medium_priorities.append(res)\n                if res.value[0] < res.value[1]:\n                    aggregates['medium_count'] += 1\n            else:\n                low_priorities.append(res)\n                if res.value[0] < res.value[1]:\n                    aggregates['low_count'] += 1\n            all_priorities.append(res)\n            # Some results have children\n            # We don't render children inline with the top three tables, but we\n            # do total the points and display the messages\n            named_function(res)\n\n        aggregates['high_priorities'] = high_priorities\n        aggregates['medium_priorities'] = medium_priorities\n        aggregates['low_priorities'] = low_priorities\n        aggregates['all_priorities'] = all_priorities\n        aggregates['testname'] = self._get_check_versioned_name(check_name)\n        aggregates['source_name'] = source_name\n        aggregates['scoreheader'] = self.checkers[check_name]._cc_display_headers\n        aggregates['cc_spec_version'] = self.checkers[check_name]._cc_spec_version\n        aggregates['cc_url'] = self._get_check_url(aggregates['testname'])\n        return aggregates", "response": "Builds the structure of the check and the results of the compliance checker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_output(self, check_name, groups, source_name, limit):\n        '''\n        Builds the results into a JSON structure and writes it to the file buffer.\n\n        @param check_name      The test which was run\n        @param groups          List of results from compliance checker\n        @param output_filename Path to file to save output\n        @param source_name     Source of the dataset, used for title\n        @param limit           Integer value for limiting output\n        '''\n        aggregates = self.build_structure(check_name, groups, source_name, limit)\n        return self.serialize(aggregates)", "response": "Builds the results into a JSON structure and writes it to the file buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a safe serializable object that can be serialized into JSON.", "response": "def serialize(self, o):\n        '''\n        Returns a safe serializable object that can be serialized into JSON.\n\n        @param o Python object to serialize\n        '''\n        if isinstance(o, (list, tuple)):\n            return [self.serialize(i) for i in o]\n        if isinstance(o, dict):\n            return {k: self.serialize(v) for k, v in o.items()}\n        if isinstance(o, datetime):\n            return o.isoformat()\n        if isinstance(o, Result):\n            return self.serialize(o.serialize())\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checker_html_output(self, check_name, groups, source_name, limit):\n        '''\n        Renders the HTML output for a single test using Jinja2 and returns it\n        as a string.\n\n        @param check_name      The test which was run\n        @param groups          List of results from compliance checker\n        @param source_name     Source of the dataset, used for title\n        @param limit           Integer value for limiting output\n        '''\n        from jinja2 import Environment, PackageLoader\n        self.j2 = Environment(loader=PackageLoader(self.templates_root, 'data/templates'))\n        template = self.j2.get_template('ccheck.html.j2')\n\n        template_vars = self.build_structure(check_name, groups, source_name, limit)\n        return template.render(**template_vars)", "response": "Renders the HTML output for a single test using Jinja2 and returns it\n        as a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering the HTML output for multiple tests and returns it as a string.", "response": "def html_output(self, checkers_html):\n        '''\n        Renders the HTML output for multiple tests and returns it as a string.\n\n        @param checkers_html     List of HTML for single tests as returned by\n                                 checker_html_output\n        '''\n        # Note: This relies on checker_html_output having been called so that\n        # self.j2 is initialised\n        template = self.j2.get_template('ccheck_wrapper.html.j2')\n        return template.render(checkers=checkers_html)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef standard_output(self, ds, limit, check_name, groups):\n\n        score_list, points, out_of = self.get_points(groups, limit)\n        issue_count = out_of - points\n\n        # Let's add the version number to the check name if it's missing\n        check_name = self._get_check_versioned_name(check_name)\n        check_url = self._get_check_url(check_name)\n        width = 2 * self.col_width\n        print('\\n')\n        print(\"-\" * width)\n        print('{:^{width}}'.format(\"IOOS Compliance Checker Report\", width=width))\n        print('{:^{width}}'.format(check_name, width=width))\n        print('{:^{width}}'.format(check_url, width=width))\n        print(\"-\" * width)\n        if issue_count > 0:\n            print('{:^{width}}'.format(\"Corrective Actions\", width=width))\n            plural = '' if issue_count == 1 else 's'\n            print(\"{} has {} potential issue{}\".format(os.path.basename(ds), issue_count, plural))\n\n        return [groups, points, out_of]", "response": "Generates the Terminal Output for Standard cases"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef standard_output_generation(self, groups, limit, points, out_of, check):\n        '''\n        Generates the Terminal Output\n        '''\n        if points < out_of:\n            self.reasoning_routine(groups, check, priority_flag=limit)\n        else:\n            print(\"All tests passed!\")", "response": "Generates the Terminal Output\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reasoning_routine(self, groups, check, priority_flag=3,\n                          _top_level=True):\n        \"\"\"\n        print routine performed\n        @param list groups: the Result groups\n        @param str check: checker name\n        @param int priority_flag: indicates the weight of the groups\n        @param bool _top_level: indicates the level of the group so as to\n                                print out the appropriate header string\n        \"\"\"\n\n        sort_fn = lambda x: x.weight\n        groups_sorted = sorted(groups, key=sort_fn, reverse=True)\n\n        # create dict of the groups -> {level: [reasons]}\n        result = {key: [v for v in valuesiter if v.value[0] != v.value[1]]\n                    for key, valuesiter in itertools.groupby(groups_sorted,\n                                                             key=sort_fn)}\n        priorities = self.checkers[check]._cc_display_headers\n\n        def process_table(res, check):\n            \"\"\"Recursively calls reasoning_routine to parse out child reasons\n            from the parent reasons.\n            @param Result res: Result object\n            @param str check: checker name\"\"\"\n\n            issue = res.name\n            if not res.children:\n                reasons = res.msgs\n            else:\n                child_reasons = self.reasoning_routine(res.children,\n                                                       check, _top_level=False)\n                # there shouldn't be messages if there are children\n                # is this a valid assumption?\n                reasons = child_reasons\n\n            return issue, reasons\n\n        # iterate in reverse to the min priority requested;\n        # the higher the limit, the more lenient the output\n        proc_strs = \"\"\n        for level in range(3, priority_flag - 1, -1):\n            level_name = priorities.get(level, level)\n            # print headers\n            proc_strs = []\n\n            # skip any levels that aren't in the result\n            if level not in result:\n                continue\n\n            # skip any empty result levels\n            if len(result[level]) > 0:\n                # only print priority headers at top level, i.e. non-child\n                # datasets\n                if _top_level:\n                    width = 2 * self.col_width\n                    print(\"\\n\")\n                    print('{:^{width}}'.format(level_name, width=width))\n                    print(\"-\" * width)\n\n                data_issues = [process_table(res, check) for res in result[level]]\n\n                has_printed = False\n                for issue, reasons in data_issues:\n                    # if this isn't the first printed issue, add a newline\n                    # separating this and the previous level\n                    if has_printed:\n                        print(\"\")\n                    # join alphabetized reasons together\n                    reason_str = \"\\n\".join('* {}'.format(r) for r in sorted(reasons, key=lambda x: x[0]))\n                    proc_str = \"{}\\n{}\".format(issue, reason_str)\n                    print(proc_str)\n                    proc_strs.append(proc_str)\n                    has_printed = True\n        return \"\\n\".join(proc_strs)", "response": "This function will recursively parse out the reasons of the child checks and print out the appropriate header string for the issue and the reasons of the child checks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse an xml string conforming to either an SOS or SensorML dataset and return the results", "response": "def process_doc(self, doc):\n        \"\"\"\n        Attempt to parse an xml string conforming to either an SOS or SensorML\n        dataset and return the results\n        \"\"\"\n        xml_doc = ET.fromstring(doc)\n        if xml_doc.tag == \"{http://www.opengis.net/sos/1.0}Capabilities\":\n            ds = SensorObservationService(None, xml=doc)\n            # SensorObservationService does not store the etree doc root,\n            # so maybe use monkey patching here for now?\n            ds._root = xml_doc\n\n        elif xml_doc.tag == \"{http://www.opengis.net/sensorML/1.0.1}SensorML\":\n            ds = SensorML(xml_doc)\n        else:\n            raise ValueError(\"Unrecognized XML root element: {}\".format(xml_doc.tag))\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_dataset(self, cdl_path):\n        '''\n        Use ncgen to generate a netCDF file from a .cdl file\n        Returns the path to the generated netcdf file\n\n        :param str cdl_path: Absolute path to cdl file that is used to generate netCDF file\n        '''\n        if '.cdl' in cdl_path:  # it's possible the filename doesn't have the .cdl extension\n            ds_str = cdl_path.replace('.cdl', '.nc')\n        else:\n            ds_str = cdl_path + '.nc'\n        subprocess.call(['ncgen', '-o', ds_str, cdl_path])\n        return ds_str", "response": "Generate a netCDF file from a. cdl file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a remote dataset from the server.", "response": "def load_remote_dataset(self, ds_str):\n        '''\n        Returns a dataset instance for the remote resource, either OPeNDAP or SOS\n\n        :param str ds_str: URL to the remote resource\n        '''\n\n        if opendap.is_opendap(ds_str):\n            return Dataset(ds_str)\n        else:\n            # Check if the HTTP response is XML, if it is, it's likely SOS so\n            # we'll attempt to parse the response as SOS\n            response = requests.get(ds_str, allow_redirects=True)\n            if 'text/xml' in response.headers['content-type']:\n                return self.process_doc(response.content)\n\n            raise ValueError(\"Unknown service with content-type: {}\".format(response.headers['content-type']))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_local_dataset(self, ds_str):\n        '''\n        Returns a dataset instance for the local resource\n\n        :param ds_str: Path to the resource\n        '''\n        if cdl.is_cdl(ds_str):\n            ds_str = self.generate_dataset(ds_str)\n\n        if netcdf.is_netcdf(ds_str):\n            return MemoizedDataset(ds_str)\n\n        # Assume this is just a Generic File if it exists\n        if os.path.isfile(ds_str):\n            return GenericFile(ds_str)\n\n        raise ValueError(\"File is an unknown format\")", "response": "Returns a dataset instance for the local resource\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _group_raw(self, raw_scores, cur=None, level=1):\n\n        # BEGIN INTERNAL FUNCS ########################################\n        def trim_groups(r):\n            if isinstance(r.name, tuple) or isinstance(r.name, list):\n                new_name = r.name[1:]\n            else:\n                new_name = []\n\n            return Result(r.weight, r.value, new_name, r.msgs)\n\n        # CHECK FOR TERMINAL CONDITION: all raw_scores.name are single length\n        # @TODO could have a problem here with scalar name, but probably still works\n        terminal = [len(x.name) for x in raw_scores]\n        if terminal == [0] * len(raw_scores):\n            return []\n\n        def group_func(r):\n            \"\"\"\n            Takes a Result object and slices off the first element of its name\n            if its's a tuple. Otherwise, does nothing to the name. Returns the\n            Result's name and weight in a tuple to be used for sorting in that\n            order in a groupby function.\n            @param Result r\n            @return tuple (str, int)\n            \"\"\"\n            if isinstance(r.name, tuple) or isinstance(r.name, list):\n                if len(r.name) == 0:\n                    retval = ''\n                else:\n                    retval = r.name[0:1][0]\n            else:\n                retval = r.name\n            return retval, r.weight\n        # END INTERNAL FUNCS ##########################################\n\n        # NOTE until this point, *ALL* Results in raw_scores are\n        # individual Result objects.\n\n        # sort then group by name, then by priority weighting\n        grouped = itertools.groupby(sorted(raw_scores, key=group_func),\n                                    key=group_func)\n\n        # NOTE: post-grouping, grouped looks something like\n        # [(('Global Attributes', 1), <itertools._grouper at 0x7f10982b5390>),\n        #  (('Global Attributes', 3), <itertools._grouper at 0x7f10982b5438>),\n        #  (('Not a Global Attr', 1), <itertools._grouper at 0x7f10982b5470>)]\n        #  (('Some Variable', 2),     <itertools._grouper at 0x7f10982b5400>),\n\n        ret_val = []\n\n        for k, v in grouped: # iterate through the grouped tuples\n\n            k = k[0]         # slice (\"name\", weight_val) --> \"name\"\n            v = list(v)      # from itertools._grouper to list\n\n            cv = self._group_raw(list(map(trim_groups, v)), k, level + 1)\n            if len(cv):\n                # if this node has children, max weight of children + sum of all the scores\n                max_weight = max([x.weight for x in cv])\n                sum_scores = tuple(map(sum, list(zip(*([x.value for x in cv])))))\n                msgs = []\n\n            else:\n                max_weight = max([x.weight for x in v])\n                sum_scores = tuple(map(sum, list(zip(*([self._translate_value(x.value) for x in v])))))\n                msgs = sum([x.msgs for x in v], [])\n\n            ret_val.append(Result(name=k, weight=max_weight, value=sum_scores, children=cv, msgs=msgs))\n\n        return ret_val", "response": "Internal recursive method to group raw scores into a cascading score summary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning list of all loggers that have been accessed.", "response": "def get_all_logger_names(include_root=False):\n    \"\"\"Return ``list`` of names of all loggers than have been accessed.\n\n    Warning: this is sensitive to internal structures in the standard logging module.\n    \"\"\"\n    # noinspection PyUnresolvedReferences\n    rv = list(logging.Logger.manager.loggerDict.keys())\n    if include_root:\n        rv.insert(0, '')\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef queuify_logger(logger, queue_handler, queue_listener):\n    if isinstance(logger, str):\n        logger = logging.getLogger(logger)\n\n    # Get handlers that aren't being listened for.\n    handlers = [handler for handler in logger.handlers\n                if handler not in queue_listener.handlers]\n\n    if handlers:\n        # The default QueueListener stores handlers as a tuple.\n        queue_listener.handlers = \\\n            tuple(list(queue_listener.handlers) + handlers)\n\n    # Remove logger's handlers and replace with single queue handler.\n    del logger.handlers[:]\n    logger.addHandler(queue_handler)", "response": "Replace existing logger s handlers with a queue handler while adding existing QueueListener s handlers to a queue listener."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecides the lock filename.", "response": "def getLockFilename(self):\n        \"\"\"\n        Decide the lock filename. If the logfile is file.log, then we use `.__file.lock` and\n        not `file.log.lock`. This only removes the extension if it's `*.log`.\n\n        :return: the path to the lock file.\n        \"\"\"\n        if self.baseFilename.endswith(\".log\"):\n            lock_file = self.baseFilename[:-4]\n        else:\n            lock_file = self.baseFilename\n        lock_file += \".lock\"\n        lock_path, lock_name = os.path.split(lock_file)\n        # hide the file on Unix and generally from file completion\n        lock_name = \".__\" + lock_name\n        return os.path.join(lock_path, lock_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen the current base file with the original mode and encoding.", "response": "def do_open(self, mode=None):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n\n        Note:  Copied from stdlib.  Added option to override 'mode'\n        \"\"\"\n        if mode is None:\n            mode = self.mode\n\n        with self._alter_umask():\n            # noinspection PyArgumentList\n            stream = io.open(\n                self.baseFilename, mode=mode, encoding=self.encoding, newline=self.newline)\n\n        self._do_chown_and_chmod(self.baseFilename)\n\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the file stream. Unlike close we don t tear down the log.", "response": "def _close(self):\n        \"\"\" Close file stream.  Unlike close(), we don't tear anything down, we\n        expect the log to be re-opened after rotation.\"\"\"\n\n        if self.stream:\n            try:\n                if not self.stream.closed:\n                    # Flushing probably isn't technically necessary, but it feels right\n                    self.stream.flush()\n                    self.stream.close()\n            finally:\n                self.stream = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emit(self, record):\n        # noinspection PyBroadException\n        try:\n            msg = self.format(record)\n            try:\n                self._do_lock()\n\n                try:\n                    if self.shouldRollover(record):\n                        self.doRollover()\n                except Exception as e:\n                    self._console_log(\"Unable to do rollover: %s\" % (e,), stack=True)\n                    # Continue on anyway\n\n                self.do_write(msg)\n\n            finally:\n                self._do_unlock()\n        except Exception:\n            self.handleError(record)", "response": "Emit a record.\n\n        Override from parent class to handle file locking for the duration of rollover and write.\n        This also does the formatting *before* locks are obtained, in case the format itself does\n        logging calls from within. Rollover also occurs while the lock is held."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_write(self, msg):\n        self.stream = self.do_open()\n        stream = self.stream\n        stream.write(msg)\n        if self.terminator:\n            stream.write(self.terminator)\n        stream.flush()\n        self._close()\n        return", "response": "Handling writing an individual record ; we do a fresh open every time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclosing the log stream and stream_lock.", "response": "def close(self):\n        \"\"\"\n        Close log stream and stream_lock. \"\"\"\n        self._console_log(\"In close()\", stack=True)\n        try:\n            self._close()\n        finally:\n            super(ConcurrentRotatingFileHandler, self).close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo a rollover of the log file.", "response": "def doRollover(self):\n        \"\"\"\n        Do a rollover, as described in __init__().\n        \"\"\"\n        self._close()\n        if self.backupCount <= 0:\n            # Don't keep any backups, just overwrite the existing backup file\n            # Locking doesn't much matter here; since we are overwriting it anyway\n            self.stream = self.do_open(\"w\")\n            self._close()\n            return\n\n        # Determine if we can rename the log file or not. Windows refuses to\n        # rename an open file, Unix is inode base so it doesn't care.\n\n        # Attempt to rename logfile to tempname:\n        # There is a slight race-condition here, but it seems unavoidable\n        tmpname = None\n        while not tmpname or os.path.exists(tmpname):\n            tmpname = \"%s.rotate.%08d\" % (self.baseFilename, randbits(64))\n        try:\n            # Do a rename test to determine if we can successfully rename the log file\n            os.rename(self.baseFilename, tmpname)\n\n            if self.use_gzip:\n                self.do_gzip(tmpname)\n        except (IOError, OSError):\n            exc_value = sys.exc_info()[1]\n            self._console_log(\n                \"rename failed.  File in use? exception=%s\" % (exc_value,), stack=True)\n            return\n\n        gzip_ext = ''\n        if self.use_gzip:\n            gzip_ext = '.gz'\n\n        def do_rename(source_fn, dest_fn):\n            self._console_log(\"Rename %s -> %s\" % (source_fn, dest_fn + gzip_ext))\n            if os.path.exists(dest_fn):\n                os.remove(dest_fn)\n            if os.path.exists(dest_fn + gzip_ext):\n                os.remove(dest_fn + gzip_ext)\n            source_gzip = source_fn + gzip_ext\n            if os.path.exists(source_gzip):\n                os.rename(source_gzip, dest_fn + gzip_ext)\n            elif os.path.exists(source_fn):\n                os.rename(source_fn, dest_fn)\n\n        # Q: Is there some way to protect this code from a KeyboardInterrupt?\n        # This isn't necessarily a data loss issue, but it certainly does\n        # break the rotation process during stress testing.\n\n        # There is currently no mechanism in place to handle the situation\n        # where one of these log files cannot be renamed. (Example, user\n        # opens \"logfile.3\" in notepad); we could test rename each file, but\n        # nobody's complained about this being an issue; so the additional\n        # code complexity isn't warranted.\n        for i in range(self.backupCount - 1, 0, -1):\n            sfn = \"%s.%d\" % (self.baseFilename, i)\n            dfn = \"%s.%d\" % (self.baseFilename, i + 1)\n            if os.path.exists(sfn + gzip_ext):\n                do_rename(sfn, dfn)\n        dfn = self.baseFilename + \".1\"\n        do_rename(tmpname, dfn)\n\n        if self.use_gzip:\n            logFilename = self.baseFilename + \".1.gz\"\n            self._do_chown_and_chmod(logFilename)\n\n        self._console_log(\"Rotation completed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the set of imports for the current locale.", "response": "def get_imports(self, option):\n        \"\"\"\n        See if we have been passed a set of currencies or a setting variable\n        or look for settings CURRENCIES or SHOP_CURRENCIES.\n        \"\"\"\n        if option:\n            if len(option) == 1 and option[0].isupper() and len(option[0]) > 3:\n                return getattr(settings, option[0])\n            else:\n                codes = [e for e in option if e.isupper() and len(e) == 3]\n                if len(codes) != len(option):\n                    raise ImproperlyConfigured(\"Invalid currency codes found: %s\" % codes)\n                return codes\n        for attr in ('CURRENCIES', 'SHOP_CURRENCIES'):\n            try:\n                return getattr(settings, attr)\n            except AttributeError:\n                continue\n        return option"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_handler(self, options):\n        # Import the CurrencyHandler and get an instance\n        handler_module = import_module(sources[options[self._source_param]], self._package_name)\n        return handler_module.CurrencyHandler(self.log)", "response": "Return the specified handler"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle(self, *args, **options):\n        # get the command arguments\n        self.verbosity = int(options.get('verbosity', 1))\n        force = options['force']\n        imports = self.get_imports(options['import'])\n\n        # Import the CurrencyHandler and get an instance\n        handler = self.get_handler(options)\n\n        self.log(logging.INFO, \"Getting currency data from %s\", handler.endpoint)\n        timestamp = datetime.now().isoformat()\n\n        # find available codes\n        if imports:\n            allcodes = set(handler.get_allcurrencycodes())\n            reqcodes = set(imports)\n            available = reqcodes & allcodes\n            unavailable = reqcodes - allcodes\n        else:\n            self.log(logging.WARNING, \"Importing all. Some currencies may be out-of-date (MTL) or spurious (XPD)\")\n            available = handler.get_allcurrencycodes()\n            unavailable = None\n\n        for code in available:\n            obj, created = Currency._default_manager.get_or_create(code=code)\n            name = handler.get_currencyname(code)\n            description = \"%r (%s)\" % (name, code)\n            if created or force:\n                kwargs = {}\n                if created:\n                    kwargs['is_active'] = False\n                    msg = \"Creating %s\"\n                    obj.info.update( {'Created': timestamp} )\n                else:\n                    msg = \"Updating %s\"\n                obj.info.update( {'Modified': timestamp} )\n\n                if name:\n                    kwargs['name'] = name\n\n                symbol = handler.get_currencysymbol(code)\n                if symbol:\n                    kwargs['symbol'] = symbol\n\n                try:\n                    obj.info.update(handler.get_info(code))\n                except AttributeError:\n                    pass\n\n                kwargs['info'] = obj.info\n\n                self.log(logging.INFO, msg, description)\n                Currency._default_manager.filter(pk=obj.pk).update(**kwargs)\n            else:\n                msg = \"Skipping %s\"\n                self.log(logging.INFO, msg, description)\n\n        if unavailable:\n            raise ImproperlyConfigured(\"Currencies %s not found in %s source\" % (unavailable, handler.name))", "response": "Handles the currency command"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ratetimestamp(self, base, code):\n        self.get_latestcurrencyrates(base)\n        try:\n            return datetime.fromtimestamp(self.rates[\"timestamp\"])\n        except KeyError:\n            return None", "response": "Return rate timestamp as a datetime or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Decimal currency exchange rate factor of code compared to 1 base unit.", "response": "def get_ratefactor(self, base, code):\n        \"\"\"Return the Decimal currency exchange rate factor of 'code' compared to 1 'base' unit, or RuntimeError\"\"\"\n        self.get_latestcurrencyrates(base)\n        try:\n            ratefactor = self.rates[\"rates\"][code]\n        except KeyError:\n            raise RuntimeError(\"%s: %s not found\" % (self.name, code))\n\n        if base == self.base:\n            return ratefactor\n        else:\n            return self.ratechangebase(ratefactor, self.base, base)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_currencysymbol(self, code):\n        if not self._symbols:\n            symbolpath = os.path.join(self._dir, 'currencies.json')\n            with open(symbolpath, encoding='utf8') as df:\n                self._symbols = json.load(df)\n        return self._symbols.get(code)", "response": "Retrieve the currency symbol from the local file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn new rate in new base", "response": "def ratechangebase(self, ratefactor, current_base, new_base):\n        \"\"\"\n        Local helper function for changing currency base, returns new rate in new base\n        Defaults to ROUND_HALF_EVEN\n        \"\"\"\n        if self._multiplier is None:\n            self.log(logging.WARNING, \"CurrencyHandler: changing base ourselves\")\n            # Check the current base is 1\n            if Decimal(1) != self.get_ratefactor(current_base, current_base):\n                raise RuntimeError(\"CurrencyHandler: current baserate: %s not 1\" % current_base)\n            self._multiplier = Decimal(1) / self.get_ratefactor(current_base, new_base)\n        return (ratefactor * self._multiplier).quantize(Decimal(\".0001\"))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if a set of tables is not in the blacklist or False otherwise.", "response": "def disallowed_table(*tables):\n    \"\"\"Returns True if a set of tables is in the blacklist or, if a whitelist is set,\n    any of the tables is not in the whitelist. False otherwise.\"\"\"\n    # XXX: When using a black or white list, this has to be done EVERY query;\n    # It'd be nice to make this as fast as possible.  In general, queries\n    # should have relatively few tables involved, and I don't imagine that\n    # blacklists would grow very vast.  The fastest i've been able to come\n    # up with is to pre-create a blacklist set and use intersect.\n    return not bool(settings.WHITELIST.issuperset(tables)) if settings.WHITELIST\\\n        else bool(settings.BLACKLIST.intersection(tables))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invalidate(*tables, **kwargs):\n    backend = get_backend()\n    db = kwargs.get('using', 'default')\n\n    if backend._patched:\n        for t in map(resolve_table, tables):\n            backend.keyhandler.invalidate_table(t, db)", "response": "Invalidate the current generation for one or more tables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tables_for_query(query):\n    from django.db.models.sql.where import WhereNode, SubqueryConstraint\n    from django.db.models.query import QuerySet\n    tables = set([v[0] for v in getattr(query,'alias_map',{}).values()])\n\n    def get_sub_query_tables(node):\n        query = node.query_object\n        if not hasattr(query, 'field_names'):\n            query = query.values(*node.targets)\n        else:\n            query = query._clone()\n        query = query.query\n        return set(v[0] for v in getattr(query, 'alias_map',{}).values())\n\n    def get_tables(node, tables):\n        if isinstance(node, SubqueryConstraint):\n            return get_sub_query_tables(node)\n        for child in node.children:\n            if isinstance(child, WhereNode):  # and child.children:\n                tables |= set(get_tables(child, tables))\n            elif not hasattr(child, '__iter__'):\n                continue\n            else:\n                for item in (c for c in child if isinstance(c, QuerySet)):\n                    tables |= get_tables_for_query(item.query)\n        return tables\n\n    if query.where and query.where.children:\n        where_nodes = [c for c in query.where.children if isinstance(c, (WhereNode, SubqueryConstraint))]\n        for node in where_nodes:\n            tables |= get_tables(node, tables)\n\n    return list(tables)", "response": "Returns a list of tables that will be used in\n    that query is a Django query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tables_for_query_pre_16(query):\n    from django.db.models.sql.where import WhereNode\n    from django.db.models.query import QuerySet\n    tables = set([v[0] for v in getattr(query,'alias_map',{}).values()])\n\n    def get_tables(node, tables):\n        for child in node.children:\n            if isinstance(child, WhereNode):  # and child.children:\n                tables = get_tables(child, tables)\n            elif not hasattr(child, '__iter__'):\n                continue\n            else:\n                for item in (c for c in child if isinstance(c, QuerySet)):\n                    tables |= set(get_tables_for_query(item.query))\n        return tables\n\n    if query.where and query.where.children:\n        where_nodes = [c for c in query.where.children if isinstance(c, WhereNode)]\n        for node in where_nodes:\n            tables |= get_tables(node, tables)\n\n    return list(tables)", "response": "Returns all tables that will be used in\n    that query is a list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_table_key(self, table, db='default'):\n        table = force_text(table)\n        db = force_text(settings.DB_CACHE_KEYS[db])\n        if len(table) > 100:\n            table = table[0:68] + self.gen_key(table[68:])\n        if db and len(db) > 100:\n            db = db[0:68] + self.gen_key(db[68:])\n        return '%s_%s_table_%s' % (self.prefix, db, table)", "response": "Generate a key that is standard for a given table name and database alias. Total length up to 250 is the standard for a given table name and databaseCOOKIENAME."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a list of generations and returns a key.", "response": "def gen_multi_key(self, values, db='default'):\n        \"\"\"Takes a list of generations (not table keys) and returns a key.\"\"\"\n        db = settings.DB_CACHE_KEYS[db]\n        if db and len(db) > 100:\n            db = db[0:68] + self.gen_key(db[68:])\n        return '%s_%s_multi_%s' % (self.prefix, db, self.gen_key(*values))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_key(self, *values):\n        key = md5()\n        KeyGen._recursive_convert(values, key)\n        return key.hexdigest()", "response": "Generate a key from one or more values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_generation(self, *tables, **kwargs):\n        db = kwargs.get('db', 'default')\n        if len(tables) > 1:\n            return self.get_multi_generation(tables, db)\n        return self.get_single_generation(tables[0], db)", "response": "Get the generation key for any number of tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_single_generation(self, table, db='default'):\n        key = self.keygen.gen_table_key(table, db)\n        val = self.cache_backend.get(key, None, db)\n        #if local.get('in_test', None): print force_bytes(val).ljust(32), key\n        if val is None:\n            val = self.keygen.random_generator()\n            self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)\n        return val", "response": "Creates a random generation value for a single table name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_multi_generation(self, tables, db='default'):\n        generations = []\n        for table in tables:\n            generations.append(self.get_single_generation(table, db))\n        key = self.keygen.gen_multi_key(generations, db)\n        val = self.cache_backend.get(key, None, db)\n        #if local.get('in_test', None): print force_bytes(val).ljust(32), key\n        if val is None:\n            val = self.keygen.random_generator()\n            self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)\n        return val", "response": "Takes a list of table names and returns an aggregate\n        value for the generation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the specific cache key for the sql query described by the generation key.", "response": "def sql_key(self, generation, sql, params, order, result_type,\n                using='default'):\n        \"\"\"\n        Return the specific cache key for the sql query described by the\n        pieces of the query and the generation key.\n        \"\"\"\n        # these keys will always look pretty opaque\n        suffix = self.keygen.gen_key(sql, params, order, result_type)\n        using = settings.DB_CACHE_KEYS[using]\n        return '%s_%s_query_%s.%s' % (self.prefix, using, generation, suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch(self):\n        from django.db.models.sql import compiler\n\n        self._read_compilers = (\n            compiler.SQLCompiler,\n            compiler.SQLAggregateCompiler,\n            compiler.SQLDateCompiler,\n        )\n        self._write_compilers = (\n            compiler.SQLInsertCompiler,\n            compiler.SQLDeleteCompiler,\n            compiler.SQLUpdateCompiler,\n        )\n        if not self._patched:\n            self._original = {}\n            for reader in self._read_compilers:\n                self._original[reader] = reader.execute_sql\n                reader.execute_sql = self._monkey_select(reader.execute_sql)\n            for updater in self._write_compilers:\n                self._original[updater] = updater.execute_sql\n                updater.execute_sql = self._monkey_write(updater.execute_sql)\n            self._patched = True\n            self.cache_backend.patch()\n            self._handle_signals()", "response": "monkey patches django. db. models. sql. compiler. SQL*Compiler series\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpatch(self):\n        if not self._patched:\n            return\n        for func in self._read_compilers + self._write_compilers:\n            func.execute_sql = self._original[func]\n        self.cache_backend.unpatch()\n        self._patched = False", "response": "un - applies this patch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef memoize_nullary(f):\n    def func():\n        if not hasattr(func, 'retval'):\n            func.retval = f()\n        return func.retval\n    return func", "response": "Memoizes a function that takes no arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef currency_context(context):\n    request = context['request']\n    currency_code = memoize_nullary(lambda: get_currency_code(request))\n\n    context['CURRENCIES'] = Currency.active.all() # querysets are already lazy\n    context['CURRENCY_CODE'] = currency_code # lazy\n    context['CURRENCY'] = memoize_nullary(lambda: get_currency(currency_code)) # lazy\n\n    return ''", "response": "This is only valid within the block scope."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the current currency data of the service.", "response": "def currencies(self):\n        \"\"\"Fetches current currency data of the service\n\n        :Example Data:\n\n        {\n            AED: \"United Arab Emirates Dirham\",\n            AFN: \"Afghan Afghani\",\n            ALL: \"Albanian Lek\",\n            AMD: \"Armenian Dram\",\n            ANG: \"Netherlands Antillean Guilder\",\n            AOA: \"Angolan Kwanza\",\n            ARS: \"Argentine Peso\",\n            AUD: \"Australian Dollar\",\n            AWG: \"Aruban Florin\",\n            AZN: \"Azerbaijani Manat\"\n            ...\n        }\n        \"\"\"\n        try:\n            resp = self.client.get(self.ENDPOINT_CURRENCIES)\n        except requests.exceptions.RequestException as e:\n            raise OpenExchangeRatesClientException(e)\n\n        return resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef historical(self, date, base='USD'):\n        try:\n            resp = self.client.get(self.ENDPOINT_HISTORICAL %\n                                   date.strftime(\"%Y-%m-%d\"),\n                                   params={'base': base})\n            resp.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            raise OpenExchangeRatesClientException(e)\n        return resp.json(parse_int=decimal.Decimal,\n                         parse_float=decimal.Decimal)", "response": "Fetches historical exchange rates from service"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a price in the default currency to another currency", "response": "def calculate(price, to_code, **kwargs):\n    \"\"\"Converts a price in the default currency to another currency\"\"\"\n    qs = kwargs.get('qs', get_active_currencies_qs())\n    kwargs['qs'] = qs\n    default_code = qs.default().code\n    return convert(price, default_code, to_code, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert(amount, from_code, to_code, decimals=2, qs=None):\n    if from_code == to_code:\n        return amount\n\n    if qs is None:\n        qs = get_active_currencies_qs()\n\n    from_, to = qs.get(code=from_code), qs.get(code=to_code)\n\n    amount = D(amount) * (to.factor / from_.factor)\n    return price_rounding(amount, decimals=decimals)", "response": "Converts amount from one currency to another."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a decimal price and rounds to a number of decimal places", "response": "def price_rounding(price, decimals=2):\n    \"\"\"Takes a decimal price and rounds to a number of decimal places\"\"\"\n    try:\n        exponent = D('.' + decimals * '0')\n    except InvalidOperation:\n        # Currencies with no decimal places, ex. JPY, HUF\n        exponent = D()\n    return price.quantize(exponent, rounding=ROUND_UP)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_currencies(self):\n        try:\n            resp = get(self.endpoint)\n            resp.raise_for_status()\n        except exceptions.RequestException as e:\n            self.log(logging.ERROR, \"%s: Problem whilst contacting endpoint:\\n%s\", self.name, e)\n        else:\n            with open(self._cached_currency_file, 'wb') as fd:\n                fd.write(resp.content)\n\n        try:\n            root = ET.parse(self._cached_currency_file).getroot()\n        except FileNotFoundError as e:\n            raise RuntimeError(\"%s: XML not found at endpoint or as cached file:\\n%s\" % (self.name, e))\n\n        return root", "response": "Downloads xml currency data or tries to use cached file copy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_doc(self, root):\n        if (root.tag != 'ISO_4217' or\n            root[0].tag != 'CcyTbl' or\n            root[0][0].tag != 'CcyNtry' or\n            not root.attrib['Pblshd'] or\n            # Actual length in Oct 2016: 279\n            len(root[0]) < 270):\n\n            raise TypeError(\"%s: XML %s appears to be invalid\" % (self.name, self._cached_currency_file))\n\n        return datetime.strptime(root.attrib['Pblshd'], '%Y-%m-%d').date()", "response": "Validates the xml tree and returns the published date"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an iterable of distinct 3 character ISO 4217 currency codes", "response": "def get_allcurrencycodes(self):\n        \"\"\"Return an iterable of distinct 3 character ISO 4217 currency codes\"\"\"\n        foundcodes = []\n        codeelements = self.currencies[0].iter('Ccy')\n        for codeelement in codeelements:\n            code = codeelement.text\n            if code not in foundcodes:\n                foundcodes += [code]\n                yield code"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_currency(self, code):\n        missing = True\n        for currency in self.currencies[0]:\n            try:\n                if code != currency.find('Ccy').text:\n                    continue\n            except AttributeError:\n                continue\n\n            missing = False\n            yield currency\n\n        if missing:\n            raise RuntimeError(\"%s: %s not found\" % (self.name, code))", "response": "Returns an iterable of currency elements matching the code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of information about the currency", "response": "def get_info(self, code):\n        \"\"\"Return a dict of information about the currency\"\"\"\n        for i, currency in enumerate(self.get_currency(code)):\n            if i == 0:\n                try:\n                    exp = int(currency.find('CcyMnrUnts').text)\n                except ValueError:\n                    exp = 0\n                info = {\n                    'CountryNames': [],\n                    'ISO4217Number': int(currency.find('CcyNbr').text),\n                    'ISO4217Exponent': exp,\n                    'ISOUpdate': self.published.isoformat(),\n                }\n            try:\n                ctry_name = currency.find('CtryNm').text\n            except AttributeError:\n                continue\n            if ctry_name:\n                info['CountryNames'] += [ctry_name]\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef celery_enable_all():\n    from celery.signals import task_prerun, task_postrun, task_failure\n    task_prerun.connect(prerun_handler)\n    task_postrun.connect(postrun_handler)\n    # Also have to cleanup on failure.\n    task_failure.connect(postrun_handler)", "response": "Enable johnny - cache in all celery tasks clearing the local - store\n    after each task."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef celery_task_wrapper(f):\n    from celery.utils import fun_takes_kwargs\n\n    @wraps(f, assigned=available_attrs(f))\n    def newf(*args, **kwargs):\n        backend = get_backend()\n        was_patched = backend._patched\n        get_backend().patch()\n        # since this function takes all keyword arguments,\n        # we will pass only the ones the function below accepts,\n        # just as celery does\n        supported_keys = fun_takes_kwargs(f, kwargs)\n        new_kwargs = dict((key, val) for key, val in kwargs.items()\n                                if key in supported_keys)\n\n        try:\n            ret = f(*args, **new_kwargs)\n        finally:\n            local.clear()\n        if not was_patched:\n            get_backend().unpatch()\n        return ret\n    return newf", "response": "A task wrapper for celery that sets up cache and ensures\n    that the local store is cleared after completion\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the actual django cache object johnny is configured to use.", "response": "def _get_backend():\n    \"\"\"\n    Returns the actual django cache object johnny is configured to use.\n    This relies on the settings only;  the actual active cache can\n    theoretically be changed at runtime.\n    \"\"\"\n    enabled = [n for n, c in sorted(CACHES.items())\n               if c.get('JOHNNY_CACHE', False)]\n    if len(enabled) > 1:\n        warn(\"Multiple caches configured for johnny-cache; using %s.\" %\n             enabled[0])\n    if enabled:\n        return get_cache(enabled[0])\n    if CACHE_BACKEND:\n        backend = get_cache(CACHE_BACKEND)\n        if backend not in CACHES:\n            from django.core import signals\n            # Some caches -- python-memcached in particular -- need to do a\n            # cleanup at the end of a request cycle. If the cache provides a\n            # close() method, wire it up here.\n            if hasattr(backend, 'close'):\n                signals.request_finished.connect(backend.close)\n        return backend\n    return cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd command line arguments", "response": "def add_arguments(self, parser):\n        \"\"\"Add command arguments\"\"\"\n        parser.add_argument(self._source_param, **self._source_kwargs)\n        parser.add_argument('--base', '-b', action='store',\n            help=   'Supply the base currency as code or a settings variable name. '\n                    'The default is taken from settings CURRENCIES_BASE or SHOP_DEFAULT_CURRENCY, '\n                    'or the db, otherwise USD')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_base(self, option):\n        if option:\n            if option.isupper():\n                if len(option) > 3:\n                    return getattr(settings, option), True\n                elif len(option) == 3:\n                    return option, True\n            raise ImproperlyConfigured(\"Invalid currency code found: %s\" % option)\n        for attr in ('CURRENCIES_BASE', 'SHOP_DEFAULT_CURRENCY'):\n            try:\n                return getattr(settings, attr), True\n            except AttributeError:\n                continue\n        return 'USD', False", "response": "Parse the base command option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling the get_base command", "response": "def handle(self, *args, **options):\n        \"\"\"Handle the command\"\"\"\n        # get the command arguments\n        self.verbosity = int(options.get('verbosity', 1))\n        base, base_was_arg = self.get_base(options['base'])\n\n        # Import the CurrencyHandler and get an instance\n        handler = self.get_handler(options)\n\n        # See if the db already has a base currency\n        try:\n            db_base_obj = Currency._default_manager.get(is_base=True)\n            db_base = db_base_obj.code\n        except Currency.DoesNotExist:\n            db_base = None\n\n        if db_base and not base_was_arg:\n            base = db_base\n            base_obj = db_base_obj\n            base_in_db = True\n        else:\n            # see if the argument base currency exists in the db\n            try:\n                base_in_db = True\n                base_obj = Currency._default_manager.get(code=base)\n            except Currency.DoesNotExist:\n                base_in_db = False\n                raise ImproperlyConfigured(\n                    \"Base currency %r does not exist in the db! Rates will be erroneous without it.\" % base)\n\n        if db_base and base_was_arg and base_in_db and (db_base != base):\n            self.log(logging.INFO, \"Changing db base currency from %s to %s\", db_base, base)\n            db_base_obj.is_base = False\n            db_base_obj.save()\n            base_obj.is_base = True\n            base_obj.save()\n        elif (not db_base) and base_in_db:\n            base_obj.is_base = True\n            base_obj.save()\n\n        self.log(logging.INFO, \"Using %s as base for all currencies\", base)\n        self.log(logging.INFO, \"Getting currency rates from %s\", handler.endpoint)\n        timestamp = datetime.now().isoformat()\n\n        obj = None\n        for obj in Currency._default_manager.all():\n            try:\n                rate = handler.get_ratefactor(base, obj.code)\n            except AttributeError:\n                self.log(logging.CRITICAL, \"%s source does not provide currency rate information\", handler.name)\n                return\n            except RuntimeError as e:\n                self.log(logging.ERROR, str(e))\n                return\n\n            factor = rate.quantize(Decimal(\".0001\"))\n            if obj.factor != factor:\n                kwargs = {'factor': factor}\n                try:\n                    ratetimestamp = handler.get_ratetimestamp(base, obj.code)\n                except AttributeError:\n                    ratetimestamp = None\n                if ratetimestamp:\n                    obj.info.update( {'RateUpdate': ratetimestamp.isoformat()} )\n                    update_str = \", source timestamp %s\" % ratetimestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n                else:\n                    update_str = \"\"\n\n                obj.info.update( {'RateModified': timestamp} )\n                kwargs['info'] = obj.info\n\n                self.log(logging.INFO, \"Updating %r rate to %s%s\", obj.name, factor, update_str)\n\n                Currency._default_manager.filter(pk=obj.pk).update(**kwargs)\n        if not obj:\n            self.log(logging.ERROR, \"No currencies found in the db to update; try the currencies command!\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, key, val, timeout=None, using=None):\n        if timeout is None:\n            timeout = self.timeout\n        if self.is_managed(using=using) and self._patched_var:\n            self.local[key] = val\n        else:\n            self.cache_backend.set(key, val, timeout)", "response": "Set the value of a key in the local cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nflushes the internal cache to memcache or rolls backs back the internal cache.", "response": "def _flush(self, commit=True, using=None):\n        \"\"\"\n        Flushes the internal cache, either to the memcache or rolls back\n        \"\"\"\n        if commit:\n            # XXX: multi-set?\n            if self._uses_savepoints():\n                self._commit_all_savepoints(using)\n            c = self.local.mget('%s_%s_*' %\n                                (self.prefix, self._trunc_using(using)))\n            for key, value in c.items():\n                self.cache_backend.set(key, value, self.timeout)\n        else:\n            if self._uses_savepoints():\n                self._rollback_all_savepoints(using)\n        self._clear(using)\n        self._clear_sid_stack(using)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch(self):\n        if not self._patched_var:\n            self._originals['rollback'] = self._getreal('rollback')\n            self._originals['rollback_unless_managed'] = self._getreal('rollback_unless_managed')\n            self._originals['commit'] = self._getreal('commit')\n            self._originals['commit_unless_managed'] = self._getreal('commit_unless_managed')\n            self._originals['savepoint'] = self._getreal('savepoint')\n            self._originals['savepoint_rollback'] = self._getreal('savepoint_rollback')\n            self._originals['savepoint_commit'] = self._getreal('savepoint_commit')\n            transaction.rollback = self._patched(transaction.rollback, False)\n            transaction.rollback_unless_managed = self._patched(transaction.rollback_unless_managed,\n                                                                       False, unless_managed=True)\n            transaction.commit = self._patched(transaction.commit, True)\n            transaction.commit_unless_managed = self._patched(transaction.commit_unless_managed,\n                                                                     True, unless_managed=True)\n            transaction.savepoint = self._savepoint(transaction.savepoint)\n            transaction.savepoint_rollback = self._savepoint_rollback(transaction.savepoint_rollback)\n            transaction.savepoint_commit = self._savepoint_commit(transaction.savepoint_commit)\n\n            self._patched_var = True", "response": "Monkey patches commit and rollback and savepoint writes to the cache should not happen until commit."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dictionary mapping all k : v pairs with key matching glob style expression pat.", "response": "def mget(self, pat=None):\n        \"\"\"\n        Get a dictionary mapping of all k:v pairs with key matching\n        glob style expression `pat`.\n        \"\"\"\n        if pat is None:\n            return {}\n        expr = re.compile(fnmatch.translate(pat))\n        m = {}\n        for key in tuple(self.keys()):\n            #make sure the key is a str first\n            if isinstance(key, string_types):\n                if expr.match(key):\n                    m[key] = self[key]\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the cache for all the keys that match pat.", "response": "def clear(self, pat=None):\n        \"\"\"\n        Minor diversion with built-in dict here;  clear can take a glob\n        style expression and remove keys based on that expression.\n        \"\"\"\n        if pat is None:\n            return self.__dict__.clear()\n\n        expr = re.compile(fnmatch.translate(pat))\n        for key in tuple(self.keys()):\n            #make sure the key is a str first\n            if isinstance(key, string_types):\n                if expr.match(key):\n                    del self.__dict__[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bulkcurrencies(self):\n        start = r'YAHOO\\.Finance\\.CurrencyConverter\\.addCurrencies\\('\n        _json = r'\\[[^\\]]*\\]'\n        try:\n            resp = get(self.currencies_url)\n            resp.raise_for_status()\n        except exceptions.RequestException as e:\n            self.log(logging.ERROR, \"%s Deprecated: API withdrawn in February 2018:\\n%s\", self.name, e)\n        else:\n            # Find the javascript that contains the json object\n            soup = BS4(resp.text, 'html.parser')\n            re_start = re.compile(start)\n            try:\n                jscript = soup.find('script', type='text/javascript', text=re_start).string\n            except AttributeError:\n                self.log(logging.WARNING, \"%s: Live currency data not found, using cached copy.\", self.name)\n            else:\n                # Separate the json object\n                re_json = re.compile(_json)\n                match = re_json.search(jscript)\n                if match:\n                    json_str = match.group(0)\n                    with open(self._cached_currency_file, 'w') as fd:\n                        fd.write(json_str.encode('utf-8'))\n\n        # Parse the json file\n        try:\n            with open(self._cached_currency_file, 'r') as fd:\n                j = json.load(fd)\n        except FileNotFoundError:\n            j = None\n        if not j:\n            raise RuntimeError(\"%s: JSON not found at endpoint or as cached file:\\n%s\" % (\n                self.name, self._cached_currency_file))\n        return j", "response": "Get the supported currencies from a JSON object on the page in javascript tag\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting & format the rates dict", "response": "def get_bulkrates(self):\n        \"\"\"Get & format the rates dict\"\"\"\n        try:\n            resp = get(self.bulk_url)\n            resp.raise_for_status()\n        except exceptions.RequestException as e:\n            raise RuntimeError(e)\n        return resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a single rate used as fallback", "response": "def get_singlerate(self, base, code):\n        \"\"\"Get a single rate, used as fallback\"\"\"\n        try:\n            url = self.onerate_url % (base, code)\n            resp = get(url)\n            resp.raise_for_status()\n        except exceptions.HTTPError as e:\n            self.log(logging.ERROR, \"%s: problem with %s:\\n%s\", self.name, url, e)\n            return None\n        rate = resp.text.rstrip()\n\n        if rate == 'N/A':\n            raise RuntimeError(\"%s: %s not found\" % (self.name, code))\n        else:\n            return Decimal(rate)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_currency(self, code):\n        for currency in self.currencies:\n            if currency['shortname'] == code:\n                return currency\n        raise RuntimeError(\"%s: %s not found\" % (self.name, code))", "response": "Helper function to get a specific currency from the list of currencies"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of information about the currency", "response": "def get_info(self, code):\n        \"\"\"Return a dict of information about the currency\"\"\"\n        currency = self.get_currency(code)\n        info = {}\n\n        users = list(filter(None, currency['users'].split(',')))\n        if users:\n            info['Users'] = users\n\n        alt = list(filter(None, currency['alternatives'].split(',')))\n        if alt:\n            info['Alternatives'] = alt\n\n        if self.modified:\n            info['YFUpdate'] = self.modified.isoformat()\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_baserate(self):\n        rateslist = self.rates['list']['resources']\n        for rate in rateslist:\n            rateobj = rate['resource']['fields']\n            if rateobj['symbol'].partition('=')[0] == rateobj['name']:\n                return rateobj['name']\n        raise RuntimeError(\"%s: baserate not found\" % self.name)", "response": "Helper function to populate the base rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ratetimestamp(self, base, code):\n        try:\n            ts = int(self.get_rate(code)['ts'])\n        except RuntimeError:\n            return None\n        return datetime.fromtimestamp(ts)", "response": "Return rate timestamp as a datetime or None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_ratebase(self, rate):\n        split = rate['name'].partition('/')\n        if split[1]:\n            ratebase = split[0]\n            if ratebase != self.base:\n                raise RuntimeError(\"%s: %s has different base rate:\\n%s\" % (\n                    self.name, ratebase, rate))\n        elif rate['name'] == self.base:\n            pass\n        # Codes beginning with 'X' are treated specially, e.g.\n        # Gold (XAU), Copper (XCP), Palladium (XPD), Platinum (XPT), Silver (XAG)\n        # They still appear to be based on USD but are reported with no base\n        elif not rate['symbol'].startswith('X'):\n            self.log(logging.WARNING, \"%s: currency found with no base:\\n%s\", self.name, rate)", "response": "Check if the ratebase of the current object is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ratefactor(self, base, code):\n        raise RuntimeError(\"%s Deprecated: API withdrawn in February 2018\" % self.name)\n        try:\n            rate = self.get_rate(code)\n        except RuntimeError:\n            # fallback\n            return self.get_singlerate(base, code)\n\n        self.check_ratebase(rate)\n        ratefactor = Decimal(rate['price'])\n\n        if base == self.base:\n            return ratefactor\n        else:\n            return self.ratechangebase(ratefactor, self.base, base)", "response": "Get Decimal currency exchange rate factor of code compared to 1 base unit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a header of specified level.", "response": "def header(heading_text, header_level, style=\"atx\"):\n    \"\"\"Return a header of specified level.\n\n    Keyword arguments:\n        style -- Specifies the header style (default atx).\n            The \"atx\" style uses hash signs, and has 6 levels.\n            The \"setext\" style uses dashes or equals signs for headers of\n            levels 1 and 2 respectively, and is limited to those two levels.\n\n    Specifying a level outside of the style's range results in a ValueError.\n\n    >>> header(\"Main Title\", 1)\n    '# Main Title'\n    >>> header(\"Smaller subtitle\", 4)\n    '#### Smaller subtitle'\n    >>> header(\"Setext style\", 2, style=\"setext\")\n    'Setext style\\\\n---'\n    \"\"\"\n    if not isinstance(header_level, int):\n        raise TypeError(\"header_level must be int\")\n    if style not in [\"atx\", \"setext\"]:\n        raise ValueError(\"Invalid style %s (choose 'atx' or 'setext')\" % style)\n    if style == \"atx\":\n        if not 1 <= header_level <= 6:\n            raise ValueError(\"Invalid level %d for atx\" % header_level)\n        return (\"#\" * header_level) + \" \" + esc_format(heading_text)\n    else:\n        if not 0 < header_level < 3:\n            raise ValueError(\"Invalid level %d for setext\" % header_level)\n        header_character = \"=\" if header_level == 1 else \"-\"\n        return esc_format(heading_text) + (\"\\n%s\" % (header_character * 3))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a code block of text.", "response": "def code_block(text, language=\"\"):\n    \"\"\"Return a code block.\n\n    If a language is specified a fenced code block is produced, otherwise the\n    block is indented by four spaces.\n\n    Keyword arguments:\n    language -- Specifies the language to fence the code in (default blank).\n\n    >>> code_block(\"This is a simple codeblock.\")\n    '    This is a simple codeblock.'\n    >>> code_block(\"This is a simple codeblock.\\\\nBut it has a linebreak!\")\n    '    This is a simple codeblock.\\\\n    But it has a linebreak!'\n    >>> code_block(\"This block of code has a specified language.\", \"python\")\n    '```python\\\\nThis block of code has a specified language.\\\\n```'\n    >>> code_block(\"So\\\\nmany\\\\nlinebreaks.\", \"python\")\n    '```python\\\\nSo\\\\nmany\\\\nlinebreaks.\\\\n```'\n    \"\"\"\n    if language:\n        return \"```\" + language + \"\\n\" + text + \"\\n```\"\n    return \"\\n\".join([\"    \" + item for item in text.split(\"\\n\")])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an inline image.", "response": "def image(alt_text, link_url, title=\"\"):\n    \"\"\"Return an inline image.\n\n    Keyword arguments:\n    title -- Specify the title of the image, as seen when hovering over it.\n\n    >>> image(\"This is an image\", \"https://tinyurl.com/bright-green-tree\")\n    '![This is an image](https://tinyurl.com/bright-green-tree)'\n    >>> image(\"This is an image\", \"https://tinyurl.com/bright-green-tree\", \"tree\")\n    '![This is an image](https://tinyurl.com/bright-green-tree) \"tree\"'\n    \"\"\"\n    image_string = \"![\" + esc_format(alt_text) + \"](\" + link_url + \")\"\n    if title:\n        image_string += ' \"' + esc_format(title) + '\"'\n    return image_string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an ordered list from an array.", "response": "def ordered_list(text_array):\n    \"\"\"Return an ordered list from an array.\n\n    >>> ordered_list([\"first\", \"second\", \"third\", \"fourth\"])\n    '1.  first\\\\n2.  second\\\\n3.  third\\\\n4.  fourth'\n    \"\"\"\n    text_list = []\n    for number, item in enumerate(text_array):\n        text_list.append(\n            (esc_format(number + 1) + \".\").ljust(3) + \" \" + esc_format(item)\n        )\n    return \"\\n\".join(text_list)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a task list.", "response": "def task_list(task_array):\n    \"\"\"Return a task list.\n\n    The task_array should be 2-dimensional; the first item should be the task\n    text, and the second the boolean completion state.\n\n    >>> task_list([[\"Be born\", True], [\"Be dead\", False]])\n    '- [X] Be born\\\\n- [ ] Be dead'\n\n    When displayed using `print`, this will appear as:\n\n        - [X] Be born\n        - [ ] Be dead\n    \"\"\"\n    tasks = []\n    for item, completed in task_array:\n        task = \"- [ ] \" + esc_format(item)\n        if completed:\n            task = task[:3] + \"X\" + task[4:]\n        tasks.append(task)\n    return \"\\n\".join(tasks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef table_row(text_array, pad=-1):\n    if pad == -1:\n        pad = [0] * len(text_array)\n    row = \"|\"\n    for column_number in range(len(text_array)):\n        padding = pad[column_number] + 1\n        row += (\" \" + esc_format(text_array[column_number])).ljust(padding) + \" |\"\n    return row", "response": "Return a single table row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a formatted table from a 2 - dimensional array representing columns.", "response": "def table(big_array):\n    \"\"\"Return a formatted table, generated from arrays representing columns.\n    The function requires a 2-dimensional array, where each array is a column\n    of the table. This will be used to generate a formatted table in string\n    format. The number of items in each columns does not need to be consitent.\n\n    >>> table([[\"Name\", \"abactel\", \"Bob\"], [\"User\", \"4b4c73l\", \"\"]])\n    '| Name    | User    |\\\\n| ------- | ------- |\\\\n| abactel | 4b4c73l |\\\\n| Bob     |         |'\n\n    When displayed using `print`, this will appear:\n        | Name    | User    |\n        | ------- | ------- |\n        | abactel | 4b4c73l |\n        | Bob     |         |\n    \"\"\"\n    number_of_columns = len(big_array)\n    number_of_rows_in_column = [len(column) for column in big_array]\n    max_cell_size = [len(max(column, key=len)) for column in big_array]\n    table = []\n\n    # title row\n    row_array = [column[0] for column in big_array]\n    table.append(table_row(row_array, pad=max_cell_size))\n\n    # delimiter row\n    row_array = []\n    for column_number in range(number_of_columns):\n        row_array.append(\"-\" * max_cell_size[column_number])\n    table.append(table_row(row_array, pad=max_cell_size))\n\n    # body rows\n    for row in range(1, max(number_of_rows_in_column)):\n        row_array = []\n        for column_number in range(number_of_columns):\n            if number_of_rows_in_column[column_number] > row:\n                row_array.append(big_array[column_number][row])\n            else:\n                row_array.append()\n        table.append(table_row(row_array, pad=max_cell_size))\n    return \"\\n\".join(table)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_icon_font(icon_font, directory):\n    try:\n        downloader = AVAILABLE_ICON_FONTS[icon_font]['downloader'](directory)\n        downloader.download_files()\n        return downloader\n    except KeyError:  # pragma: no cover\n        raise Exception(\"We don't support downloading font '{name}'\".format(\n            name=icon_font)\n        )", "response": "Download given ( implemented ) icon font into passed directory"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare(sig1, sig2):\n\n    if isinstance(sig1, six.text_type):\n        sig1 = sig1.encode(\"ascii\")\n    if isinstance(sig2, six.text_type):\n        sig2 = sig2.encode(\"ascii\")\n\n    if not isinstance(sig1, six.binary_type):\n        raise TypeError(\n            \"First argument must be of string, unicode or bytes type not \"\n            \"'%s'\" % type(sig1)\n        )\n\n    if not isinstance(sig2, six.binary_type):\n        raise TypeError(\n            \"Second argument must be of string, unicode or bytes type not \"\n            \"'%r'\" % type(sig2)\n        )\n\n    res = binding.lib.fuzzy_compare(sig1, sig2)\n    if res < 0:\n        raise InternalError(\"Function returned an unexpected error code\")\n\n    return res", "response": "Compares two fuzzy hash signatures and returns the match score of the two signatures."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the fuzzy hash of a buffer.", "response": "def hash(buf, encoding=\"utf-8\"):\n    \"\"\"\n    Compute the fuzzy hash of a buffer\n\n    :param String|Bytes buf: The data to be fuzzy hashed\n    :return: The fuzzy hash\n    :rtype: String\n    :raises InternalError: If lib returns an internal error\n    :raises TypeError: If buf is not String or Bytes\n\n    \"\"\"\n\n    if isinstance(buf, six.text_type):\n        buf = buf.encode(encoding)\n\n    if not isinstance(buf, six.binary_type):\n        raise TypeError(\n            \"Argument must be of string, unicode or bytes type not \"\n            \"'%r'\" % type(buf)\n        )\n\n    # allocate memory for result\n    result = ffi.new(\"char[]\", binding.lib.FUZZY_MAX_RESULT)\n    if binding.lib.fuzzy_hash_buf(buf, len(buf), result) != 0:\n        raise InternalError(\"Function returned an unexpected error code\")\n\n    return ffi.string(result).decode(\"ascii\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the fuzzy hash of a file.", "response": "def hash_from_file(filename):\n    \"\"\"\n    Compute the fuzzy hash of a file.\n\n    Opens, reads, and hashes the contents of the file 'filename'\n\n    :param String|Bytes filename: The name of the file to be hashed\n    :return: The fuzzy hash of the file\n    :rtype: String\n    :raises IOError: If Python is unable to read the file\n    :raises InternalError: If lib returns an internal error\n\n    \"\"\"\n\n    if not os.path.exists(filename):\n        raise IOError(\"Path not found\")\n    if not os.path.isfile(filename):\n        raise IOError(\"File not found\")\n    if not os.access(filename, os.R_OK):\n        raise IOError(\"File is not readable\")\n\n    result = ffi.new(\"char[]\", binding.lib.FUZZY_MAX_RESULT)\n    if binding.lib.fuzzy_hash_filename(filename.encode(\"utf-8\"), result) != 0:\n        raise InternalError(\"Function returned an unexpected error code\")\n\n    return ffi.string(result).decode(\"ascii\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfeeding the data contained in the given buffer to the state.", "response": "def update(self, buf, encoding=\"utf-8\"):\n        \"\"\"\n         Feed the data contained in the given buffer to the state.\n\n        :param String|Byte buf: The data to be hashed\n        :param String encoding: Encoding is used if buf is String\n        :raises InternalError: If lib returns an internal error\n        :raises TypeError: If buf is not Bytes, String or Unicode\n\n        \"\"\"\n\n        if self._state == ffi.NULL:\n            raise InternalError(\"State object is NULL\")\n\n        if isinstance(buf, six.text_type):\n            buf = buf.encode(encoding)\n\n        if not isinstance(buf, six.binary_type):\n            raise TypeError(\n                \"Argument must be of string, unicode or bytes type not \"\n                \"'%r'\" % type(buf)\n            )\n\n        if binding.lib.fuzzy_update(self._state, buf, len(buf)) != 0:\n            binding.lib.fuzzy_free(self._state)\n            raise InternalError(\"Invalid state object\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the fuzzy hash of the current state object.", "response": "def digest(self, elimseq=False, notrunc=False):\n        \"\"\"\n        Obtain the fuzzy hash.\n\n        This operation does not change the state at all. It reports the hash\n        for the concatenation of the data previously fed using update().\n\n        :return: The fuzzy hash\n        :rtype: String\n        :raises InternalError: If lib returns an internal error\n\n        \"\"\"\n\n        if self._state == ffi.NULL:\n            raise InternalError(\"State object is NULL\")\n\n        flags = (binding.lib.FUZZY_FLAG_ELIMSEQ if elimseq else 0) | \\\n                (binding.lib.FUZZY_FLAG_NOTRUNC if notrunc else 0)\n\n        result = ffi.new(\"char[]\", binding.lib.FUZZY_MAX_RESULT)\n        if binding.lib.fuzzy_digest(self._state, result, flags) != 0:\n            raise InternalError(\"Function returned an unexpected error code\")\n\n        return ffi.string(result).decode(\"ascii\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfeeding the data contained in the given buffer to the state.", "response": "def update(self, buf, encoding=\"utf-8\"):\n        \"\"\"\n         Feed the data contained in the given buffer to the state.\n\n        :param String|Byte buf: The data to be hashed\n        :param String encoding: Encoding is used if buf is String\n        :raises TypeError: If buf is not Bytes, String or Unicode\n\n        \"\"\"\n\n        if isinstance(buf, six.text_type):\n            buf = buf.encode(encoding)\n\n        if not isinstance(buf, six.binary_type):\n            raise TypeError(\n                \"Argument must be of string, unicode or bytes type not \"\n                \"'%r'\" % type(buf)\n            )\n\n        self._data = self._data + buf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_css(self):\n        icons = dict()\n        common_prefix = None\n        parser = tinycss.make_parser('page3')\n        stylesheet = parser.parse_stylesheet_file(self.css_file)\n\n        is_icon = re.compile(\"\\.(.*):before,?\")\n\n        for rule in stylesheet.rules:\n            selector = rule.selector.as_css()\n\n            # Skip CSS classes that are not icons\n            if not is_icon.match(selector):\n                continue\n\n            # Find out what the common prefix is\n            if common_prefix is None:\n                common_prefix = selector[1:]\n            else:\n                common_prefix = os.path.commonprefix((common_prefix,\n                                                      selector[1:]))\n\n            for match in is_icon.finditer(selector):\n                name = match.groups()[0]\n                for declaration in rule.declarations:\n                    if declaration.name == \"content\":\n                        val = declaration.value.as_css()\n                        # Strip quotation marks\n                        if re.match(\"^['\\\"].*['\\\"]$\", val):\n                            val = val[1:-1]\n                        icons[name] = unichr(int(val[1:], 16))\n\n        common_prefix = common_prefix or ''\n\n        # Remove common prefix\n        if not self.keep_prefix and len(common_prefix) > 0:\n            non_prefixed_icons = {}\n            for name in icons.keys():\n                non_prefixed_icons[name[len(common_prefix):]] = icons[name]\n            icons = non_prefixed_icons\n\n        sorted_icons = OrderedDict(sorted(icons.items(), key=lambda t: t[0]))\n\n        return sorted_icons, common_prefix", "response": "Loads the CSS file and returns a dict of all icons available in CSS file and finds out what s their common prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports given icon with provided parameters.", "response": "def export_icon(self, icon, size, color='black', scale='auto',\n                    filename=None, export_dir='exported'):\n        \"\"\"\n        Exports given icon with provided parameters.\n\n        If the desired icon size is less than 150x150 pixels, we will first\n        create a 150x150 pixels image and then scale it down, so that\n        it's much less likely that the edges of the icon end up cropped.\n\n        :param icon: valid icon name\n        :param filename: name of the output file\n        :param size: icon size in pixels\n        :param color: color name or hex value\n        :param scale: scaling factor between 0 and 1,\n                      or 'auto' for automatic scaling\n        :param export_dir: path to export directory\n        \"\"\"\n        org_size = size\n        size = max(150, size)\n\n        image = Image.new(\"RGBA\", (size, size), color=(0, 0, 0, 0))\n        draw = ImageDraw.Draw(image)\n\n        if scale == 'auto':\n            scale_factor = 1\n        else:\n            scale_factor = float(scale)\n\n        font = ImageFont.truetype(self.ttf_file, int(size * scale_factor))\n        width, height = draw.textsize(self.css_icons[icon], font=font)\n\n        # If auto-scaling is enabled, we need to make sure the resulting\n        # graphic fits inside the boundary. The values are rounded and may be\n        # off by a pixel or two, so we may need to do a few iterations.\n        # The use of a decrementing multiplication factor protects us from\n        # getting into an infinite loop.\n        if scale == 'auto':\n            iteration = 0\n            factor = 1\n\n            while True:\n                width, height = draw.textsize(self.css_icons[icon], font=font)\n\n                # Check if the image fits\n                dim = max(width, height)\n                if dim > size:\n                    font = ImageFont.truetype(self.ttf_file,\n                                              int(size * size/dim * factor))\n                else:\n                    break\n\n                # Adjust the factor every two iterations\n                iteration += 1\n                if iteration % 2 == 0:\n                    factor *= 0.99\n\n        draw.text((float(size - width) / 2, float(size - height) / 2),\n                  self.css_icons[icon], font=font, fill=color)\n\n        # Get bounding box\n        bbox = image.getbbox()\n\n        # Create an alpha mask\n        image_mask = Image.new(\"L\", (size, size), 0)\n        draw_mask = ImageDraw.Draw(image_mask)\n\n        # Draw the icon on the mask\n        draw_mask.text((float(size - width) / 2, float(size - height) / 2),\n                       self.css_icons[icon], font=font, fill=255)\n\n        # Create a solid color image and apply the mask\n        icon_image = Image.new(\"RGBA\", (size, size), color)\n        icon_image.putalpha(image_mask)\n\n        if bbox:\n            icon_image = icon_image.crop(bbox)\n\n        border_w = int((size - (bbox[2] - bbox[0])) / 2)\n        border_h = int((size - (bbox[3] - bbox[1])) / 2)\n\n        # Create output image\n        out_image = Image.new(\"RGBA\", (size, size), (0, 0, 0, 0))\n        out_image.paste(icon_image, (border_w, border_h))\n\n        # If necessary, scale the image to the target size\n        if org_size != size:\n            out_image = out_image.resize((org_size, org_size), Image.ANTIALIAS)\n\n        # Make sure export directory exists\n        if not os.path.exists(export_dir):\n            os.makedirs(export_dir)\n\n        # Default filename\n        if not filename:\n            filename = icon + '.png'\n\n        # Save file\n        out_image.save(os.path.join(export_dir, filename))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_distance(trig_pin, echo_pin, celsius=20):\n    '''Return an unformatted distance in cm's as read directly from\n    RPi.GPIO.'''\n\n    speed_of_sound = 331.3 * math.sqrt(1+(celsius / 273.15))\n    GPIO.setup(trig_pin, GPIO.OUT)\n    GPIO.setup(echo_pin, GPIO.IN)\n    GPIO.output(trig_pin, GPIO.LOW)\n    time.sleep(0.1)\n    GPIO.output(trig_pin, True)\n    time.sleep(0.00001)\n    GPIO.output(trig_pin, False)\n    echo_status_counter = 1\n    while GPIO.input(echo_pin) == 0:\n        if echo_status_counter < 1000:\n            sonar_signal_off = time.time()\n            echo_status_counter += 1\n        else:\n            raise SystemError('Echo pulse was not received')\n    while GPIO.input(echo_pin) == 1:\n        sonar_signal_on = time.time()\n\n    time_passed = sonar_signal_on - sonar_signal_off\n    return time_passed * ((speed_of_sound * 100) / 2)", "response": "Return an unformatted distance in cm s as read directly from\n    RPi. GPIO."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_distance(self, sample_size=11, sample_wait=0.1):\n        '''Return an error corrected unrounded distance, in cm, of an object \n        adjusted for temperature in Celcius.  The distance calculated\n        is the median value of a sample of `sample_size` readings.\n\n        \n        Speed of readings is a result of two variables.  The sample_size\n        per reading and the sample_wait (interval between individual samples).\n\n        Example: To use a sample size of 5 instead of 11 will increase the \n        speed of your reading but could increase variance in readings;\n\n        value = sensor.Measurement(trig_pin, echo_pin)\n        r = value.raw_distance(sample_size=5)\n        \n        Adjusting the interval between individual samples can also\n        increase the speed of the reading.  Increasing the speed will also\n        increase CPU usage.  Setting it too low will cause errors.  A default\n        of sample_wait=0.1 is a good balance between speed and minimizing \n        CPU usage.  It is also a safe setting that should not cause errors.\n        \n        e.g.\n\n        r = value.raw_distance(sample_wait=0.03)\n        '''\n\n        if self.unit == 'imperial':\n            self.temperature = (self.temperature - 32) * 0.5556\n        elif self.unit == 'metric':\n            pass\n        else:\n            raise ValueError(\n                'Wrong Unit Type. Unit Must be imperial or metric')\n\n        speed_of_sound = 331.3 * math.sqrt(1+(self.temperature / 273.15))\n        sample = []\n        # setup input/output pins\n        GPIO.setwarnings(False)\n        GPIO.setmode(self.gpio_mode)\n        GPIO.setup(self.trig_pin, GPIO.OUT)\n        GPIO.setup(self.echo_pin, GPIO.IN)\n        \n        for distance_reading in range(sample_size):\n            GPIO.output(self.trig_pin, GPIO.LOW)\n            time.sleep(sample_wait)\n            GPIO.output(self.trig_pin, True)\n            time.sleep(0.00001)\n            GPIO.output(self.trig_pin, False)\n            echo_status_counter = 1\n            while GPIO.input(self.echo_pin) == 0:\n                if echo_status_counter < 1000:\n                    sonar_signal_off = time.time()\n                    echo_status_counter += 1\n                else:\n                    raise SystemError('Echo pulse was not received')\n            while GPIO.input(self.echo_pin) == 1:\n                sonar_signal_on = time.time()\n            time_passed = sonar_signal_on - sonar_signal_off\n            distance_cm = time_passed * ((speed_of_sound * 100) / 2)\n            sample.append(distance_cm)\n        sorted_sample = sorted(sample)\n        # Only cleanup the pins used to prevent clobbering\n        # any others in use by the program\n        GPIO.cleanup((self.trig_pin, self.echo_pin))\n        return sorted_sample[sample_size // 2]", "response": "Return an error corrected unrounded distance in cm of an object \n        adjusted for temperature in Celcius."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    '''Calculate the depth of a liquid in centimeters using a HCSR04 sensor\n       and a Raspberry Pi'''\n\n    trig_pin = 17\n    echo_pin = 27\n\n    # Default values\n    # unit = 'metric'\n    # temperature = 20\n    # round_to = 1\n\n    hole_depth = 80  # centimeters\n\n    # Create a distance reading with the hcsr04 sensor module\n\n    value = sensor.Measurement(trig_pin,\n                               echo_pin\n                               )\n\n    raw_measurement = value.raw_distance()\n\n    # To overide default values you can pass the following to value\n    # value = sensor.Measurement(trig_pin,\n    #                            echo_pin,\n    #                            temperature=10,\n    #                            round_to=2\n    #                            )\n\n    # Calculate the liquid depth, in centimeters, of a hole filled\n    # with liquid\n    liquid_depth = value.depth_metric(raw_measurement, hole_depth)\n    print(\"Depth = {} centimeters\".format(liquid_depth))", "response": "This function is used to calculate the depth of a liquid in centimeters using a HCSR04 sensor\n       and a Raspberry Pi sensor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    '''Calculate the distance of an object in centimeters using a HCSR04 sensor\n       and a Raspberry Pi. This script allows for a quicker reading by\n       decreasing the number of samples and forcing the readings to be\n       taken at quicker intervals.'''\n\n    trig_pin = 17\n    echo_pin = 27\n\n    #  Create a distance reading with the hcsr04 sensor module\n    value = sensor.Measurement(trig_pin, echo_pin)\n\n    # The default sample_size is 11 and sample_wait is 0.1\n    # Increase speed by lowering the sample_size and sample_wait\n    # The effect of reducing sample_size is larger variance in readings\n    # The effect of lowering sample_wait is higher cpu usage and sensor\n    # instability if you push it with too fast of a value.\n    # These two options have been added to allow you to tweak a \n    # more optimal setting for your application.\n\n    # e.g.\n    raw_measurement = value.raw_distance(sample_size=5, sample_wait=0.03)\n\n    # Calculate the distance in centimeters\n    metric_distance = value.distance_metric(raw_measurement)\n    print(\"The Distance = {} centimeters\".format(metric_distance))", "response": "This script calculates the distance of an object in centimeters using a HCSR04 sensor and a Raspberry Pi."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse command line arguments.", "response": "def get_args():\n    '''Parse command line arguments.'''\n\n    parser = argparse.ArgumentParser(\n        description='Script tests the HCSR04 sensor under different configurations')\n\n\n    parser.add_argument('-t',\n                        '--trig',\n                        type=int,\n                        help='Trig Pin (Required - must be an integer, must \\\n                             use BCM pin values)',\n                        required=True\n                        )\n\n    parser.add_argument('-e',\n                        '--echo',\n                        type=int,\n                        help='Echo Pin (Required - must be an integer, must \\\n                             use BCM pin values)',\n                        required=True\n                        )\n\n    parser.add_argument('-sp',\n                        '--speed',\n                        type=float,\n                        help='Time between individual reading samples \\\n                             (Optional - must be a float, default\\\n                              is 0.1 seconds)',\n                        required=False,\n                        default=0.1\n                        )\n\n\n    parser.add_argument('-ss',\n                        '--samples',\n                        type=int,\n                        help='Reading Sample Size (Optional - must be an \\\n                                integer, default is 11)',\n                        required=False,\n                        default=11\n                        )\n \n\n    args = parser.parse_args()\n    trig = args.trig\n    echo = args.echo\n    speed = args.speed\n    samples = args.samples\n    return trig, echo, speed, samples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    '''Main function to run the sensor with passed arguments'''\n    \n    trig, echo, speed, samples = get_args()\n    print('trig pin = gpio {}'.format(trig))\n    print('echo pin = gpio {}'.format(echo))\n    print('speed = {}'.format(speed))\n    print('samples = {}'.format(samples))\n    print('')\n    \n    value = sensor.Measurement(trig, echo)\n    raw_distance = value.raw_distance(sample_size=samples, sample_wait=speed)\n\n    imperial_distance = value.distance_imperial(raw_distance)\n    metric_distance = value.distance_metric(raw_distance)\n    print('The imperial distance is {} inches.'.format(imperial_distance))\n    print('The metric distance is {} centimetres.'.format(metric_distance))", "response": "Main function to run the sensor with passed arguments"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the depth of a liquid in inches using a HCSR04 sensor and a Raspberry Pi", "response": "def main():\n    '''Calculate the depth of a liquid in inches using a HCSR04 sensor\n       and a Raspberry Pi'''\n\n    trig_pin = 17\n    echo_pin = 27\n    hole_depth = 31.5  # inches\n\n    # Default values\n    # unit = 'metric'\n    # temperature = 20\n    # round_to = 1\n\n    # Create a distance reading with the hcsr04 sensor module\n    # and overide the default values for temp, unit and rounding)\n    value = sensor.Measurement(trig_pin,\n                               echo_pin,\n                               temperature=68,\n                               unit='imperial',\n                               round_to=2\n                               )\n    raw_measurement = value.raw_distance()\n\n    # Calculate the liquid depth, in inches, of a hole filled\n    # with liquid\n    liquid_depth = value.depth_imperial(raw_measurement, hole_depth)\n    print(\"Depth = {} inches\".format(liquid_depth))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    '''Calculate the distance of an object in inches using a HCSR04 sensor\n       and a Raspberry Pi'''\n\n    trig_pin = 17\n    echo_pin = 27\n\n    # Default values\n    # unit = 'metric'\n    # temperature = 20\n    # round_to = 1\n\n    # Create a distance reading with the hcsr04 sensor module\n    # and overide the default values for temp, unit and rounding)\n    value = sensor.Measurement(trig_pin,\n                               echo_pin,\n                               temperature=68,\n                               unit='imperial',\n                               round_to=2\n                               )\n\n    raw_measurement = value.raw_distance()\n\n    # Calculate the distance in inches\n    imperial_distance = value.distance_imperial(raw_measurement)\n    print(\"The Distance = {} inches\".format(imperial_distance))", "response": "Calculate the distance of an object in inches using a HCSR04 sensor\n       and a Raspberry Pi"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the current brightness of the LED.", "response": "def read(self):\n        \"\"\"Read the brightness of the LED.\n\n        Returns:\n            int: Current brightness.\n\n        Raises:\n            LEDError: if an I/O or OS error occurs.\n\n        \"\"\"\n        # Read value\n        try:\n            buf = os.read(self._fd, 8)\n        except OSError as e:\n            raise LEDError(e.errno, \"Reading LED brightness: \" + e.strerror)\n\n        # Rewind\n        try:\n            os.lseek(self._fd, 0, os.SEEK_SET)\n        except OSError as e:\n            raise LEDError(e.errno, \"Rewinding LED brightness: \" + e.strerror)\n\n        return int(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, brightness):\n        if not isinstance(brightness, (bool, int)):\n            raise TypeError(\"Invalid brightness type, should be bool or int.\")\n\n        if isinstance(brightness, bool):\n            brightness = self._max_brightness if brightness else 0\n        else:\n            if not 0 <= brightness <= self._max_brightness:\n                raise ValueError(\"Invalid brightness value, should be between 0 and %d.\" % self._max_brightness)\n\n        # Write value\n        try:\n            os.write(self._fd, b\"%d\\n\" % brightness)\n        except OSError as e:\n            raise LEDError(e.errno, \"Writing LED brightness: \" + e.strerror)\n\n        # Rewind\n        try:\n            os.lseek(self._fd, 0, os.SEEK_SET)\n        except OSError as e:\n            raise LEDError(e.errno, \"Rewinding LED brightness: \" + e.strerror)", "response": "Writes the brightness of the LED to the specified value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self._fd is None:\n            return\n\n        try:\n            os.close(self._fd)\n        except OSError as e:\n            raise LEDError(e.errno, \"Closing LED: \" + e.strerror)\n\n        self._fd = None", "response": "Closes the sysfs LED."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transfer(self, address, messages):\n        if not isinstance(messages, list):\n            raise TypeError(\"Invalid messages type, should be list of I2C.Message.\")\n        elif len(messages) == 0:\n            raise ValueError(\"Invalid messages data, should be non-zero length.\")\n\n        # Convert I2C.Message messages to _CI2CMessage messages\n        cmessages = (_CI2CMessage * len(messages))()\n        for i in range(len(messages)):\n            # Convert I2C.Message data to bytes\n            if isinstance(messages[i].data, bytes):\n                data = messages[i].data\n            elif isinstance(messages[i].data, bytearray):\n                data = bytes(messages[i].data)\n            elif isinstance(messages[i].data, list):\n                data = bytes(bytearray(messages[i].data))\n\n            cmessages[i].addr = address\n            cmessages[i].flags = messages[i].flags | (I2C._I2C_M_RD if messages[i].read else 0)\n            cmessages[i].len = len(data)\n            cmessages[i].buf = ctypes.cast(ctypes.create_string_buffer(data, len(data)), ctypes.POINTER(ctypes.c_ubyte))\n\n        # Prepare transfer structure\n        i2c_xfer = _CI2CIocTransfer()\n        i2c_xfer.nmsgs = len(cmessages)\n        i2c_xfer.msgs = cmessages\n\n        # Transfer\n        try:\n            fcntl.ioctl(self._fd, I2C._I2C_IOC_RDWR, i2c_xfer, False)\n        except IOError as e:\n            raise I2CError(e.errno, \"I2C transfer: \" + e.strerror)\n\n        # Update any read I2C.Message messages\n        for i in range(len(messages)):\n            if messages[i].read:\n                data = [cmessages[i].buf[j] for j in range(cmessages[i].len)]\n                # Convert read data to type used in I2C.Message messages\n                if isinstance(messages[i].data, list):\n                    messages[i].data = data\n                elif isinstance(messages[i].data, bytearray):\n                    messages[i].data = bytearray(data)\n                elif isinstance(messages[i].data, bytes):\n                    messages[i].data = bytes(bytearray(data))", "response": "Transfer messages to the specified address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self, length, timeout=None):\n        data = b\"\"\n\n        # Read length bytes if timeout is None\n\n        # Read up to length bytes if timeout is not None\n        while True:\n            if timeout is not None:\n                # Select\n                (rlist, _, _) = select.select([self._fd], [], [], timeout)\n                # If timeout\n                if self._fd not in rlist:\n                    break\n\n            try:\n                data += os.read(self._fd, length - len(data))\n            except OSError as e:\n                raise SerialError(e.errno, \"Reading serial port: \" + e.strerror)\n\n            if len(data) == length:\n                break\n\n        return data", "response": "Reads up to length number of bytes from the serial port."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting data to the serial port and return the number of bytes written.", "response": "def write(self, data):\n        \"\"\"Write `data` to the serial port and return the number of bytes\n        written.\n\n        Args:\n            data (bytes, bytearray, list): a byte array or list of 8-bit integers to write.\n\n        Returns:\n            int: number of bytes written.\n\n        Raises:\n            SerialError: if an I/O or OS error occurs.\n            TypeError: if `data` type is invalid.\n            ValueError: if data is not valid bytes.\n\n        \"\"\"\n        if not isinstance(data, (bytes, bytearray, list)):\n            raise TypeError(\"Invalid data type, should be bytes, bytearray, or list.\")\n\n        if isinstance(data, list):\n            data = bytearray(data)\n\n        try:\n            return os.write(self._fd, data)\n        except OSError as e:\n            raise SerialError(e.errno, \"Writing serial port: \" + e.strerror)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npolls for data available for reading from the serial port.", "response": "def poll(self, timeout=None):\n        \"\"\"Poll for data available for reading from the serial port.\n\n        `timeout` can be positive for a timeout in seconds, 0 for a\n        non-blocking poll, or negative or None for a blocking poll. Default is\n        a blocking poll.\n\n        Args:\n            timeout (int, float, None): timeout duration in seconds.\n\n        Returns:\n            bool: ``True`` if data is available for reading from the serial port, ``False`` if not.\n\n        \"\"\"\n        p = select.poll()\n        p.register(self._fd, select.POLLIN | select.POLLPRI)\n        events = p.poll(int(timeout * 1000))\n\n        if len(events) > 0:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flush(self):\n        try:\n            termios.tcdrain(self._fd)\n        except termios.error as e:\n            raise SerialError(e.errno, \"Flushing serial port: \" + e.strerror)", "response": "Flush the write buffer of the serial port blocking until all bytes are written."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the number of bytes waiting to be read from the serial port.", "response": "def input_waiting(self):\n        \"\"\"Query the number of bytes waiting to be read from the serial port.\n\n        Returns:\n            int: number of bytes waiting to be read.\n\n        Raises:\n            SerialError: if an I/O or OS error occurs.\n\n        \"\"\"\n        # Get input waiting\n        buf = array.array('I', [0])\n        try:\n            fcntl.ioctl(self._fd, termios.TIOCINQ, buf, True)\n        except OSError as e:\n            raise SerialError(e.errno, \"Querying input waiting: \" + e.strerror)\n\n        return buf[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the number of bytes waiting to be written to the serial port.", "response": "def output_waiting(self):\n        \"\"\"Query the number of bytes waiting to be written to the serial port.\n\n        Returns:\n            int: number of bytes waiting to be written.\n\n        Raises:\n            SerialError: if an I/O or OS error occurs.\n\n        \"\"\"\n        # Get input waiting\n        buf = array.array('I', [0])\n        try:\n            fcntl.ioctl(self._fd, termios.TIOCOUTQ, buf, True)\n        except OSError as e:\n            raise SerialError(e.errno, \"Querying output waiting: \" + e.strerror)\n\n        return buf[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the state of the GPIO.", "response": "def read(self):\n        \"\"\"Read the state of the GPIO.\n\n        Returns:\n            bool: ``True`` for high state, ``False`` for low state.\n\n        Raises:\n            GPIOError: if an I/O or OS error occurs.\n\n        \"\"\"\n        # Read value\n        try:\n            buf = os.read(self._fd, 2)\n        except OSError as e:\n            raise GPIOError(e.errno, \"Reading GPIO: \" + e.strerror)\n\n        # Rewind\n        try:\n            os.lseek(self._fd, 0, os.SEEK_SET)\n        except OSError as e:\n            raise GPIOError(e.errno, \"Rewinding GPIO: \" + e.strerror)\n\n        if buf[0] == b\"0\"[0]:\n            return False\n        elif buf[0] == b\"1\"[0]:\n            return True\n\n        raise GPIOError(None, \"Unknown GPIO value: \\\"%s\\\"\" % buf[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the state of the GPIO to value.", "response": "def write(self, value):\n        \"\"\"Set the state of the GPIO to `value`.\n\n        Args:\n            value (bool): ``True`` for high state, ``False`` for low state.\n\n        Raises:\n            GPIOError: if an I/O or OS error occurs.\n            TypeError: if `value` type is not bool.\n\n        \"\"\"\n        if not isinstance(value, bool):\n            raise TypeError(\"Invalid value type, should be bool.\")\n\n        # Write value\n        try:\n            if value:\n                os.write(self._fd, b\"1\\n\")\n            else:\n                os.write(self._fd, b\"0\\n\")\n        except OSError as e:\n            raise GPIOError(e.errno, \"Writing GPIO: \" + e.strerror)\n\n        # Rewind\n        try:\n            os.lseek(self._fd, 0, os.SEEK_SET)\n        except OSError as e:\n            raise GPIOError(e.errno, \"Rewinding GPIO: \" + e.strerror)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef poll(self, timeout=None):\n        if not isinstance(timeout, (int, float, type(None))):\n            raise TypeError(\"Invalid timeout type, should be integer, float, or None.\")\n\n        # Setup epoll\n        p = select.epoll()\n        p.register(self._fd, select.EPOLLIN | select.EPOLLET | select.EPOLLPRI)\n\n        # Poll twice, as first call returns with current state\n        for _ in range(2):\n            events = p.poll(timeout)\n\n        # If GPIO edge interrupt occurred\n        if events:\n            # Rewind\n            try:\n                os.lseek(self._fd, 0, os.SEEK_SET)\n            except OSError as e:\n                raise GPIOError(e.errno, \"Rewinding GPIO: \" + e.strerror)\n\n            return True\n\n        return False", "response": "Poll a GPIO for an edge event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading 32 - bits from the specified offset in bytes relative to the base physical address of the MMIO region.", "response": "def read32(self, offset):\n        \"\"\"Read 32-bits from the specified `offset` in bytes, relative to the\n        base physical address of the MMIO region.\n\n        Args:\n            offset (int, long): offset from base physical address, in bytes.\n\n        Returns:\n            int: 32-bit value read.\n\n        Raises:\n            TypeError: if `offset` type is invalid.\n            ValueError: if `offset` is out of bounds.\n\n        \"\"\"\n        if not isinstance(offset, (int, long)):\n            raise TypeError(\"Invalid offset type, should be integer.\")\n\n        offset = self._adjust_offset(offset)\n        self._validate_offset(offset, 4)\n        return struct.unpack(\"=L\", self.mapping[offset:offset + 4])[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a string of bytes from the specified offset in bytes relative to the base physical address of the MMIO region.", "response": "def read(self, offset, length):\n        \"\"\"Read a string of bytes from the specified `offset` in bytes,\n        relative to the base physical address of the MMIO region.\n\n        Args:\n            offset (int, long): offset from base physical address, in bytes.\n            length (int): number of bytes to read.\n\n        Returns:\n            bytes: bytes read.\n\n        Raises:\n            TypeError: if `offset` type is invalid.\n            ValueError: if `offset` is out of bounds.\n\n        \"\"\"\n        if not isinstance(offset, (int, long)):\n            raise TypeError(\"Invalid offset type, should be integer.\")\n\n        offset = self._adjust_offset(offset)\n        self._validate_offset(offset, length)\n        return bytes(self.mapping[offset:offset + length])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting an 8 - bit value to the specified offset in bytes relative to the base physical address of the MMIO region.", "response": "def write8(self, offset, value):\n        \"\"\"Write 8-bits to the specified `offset` in bytes, relative to the\n        base physical address of the MMIO region.\n\n        Args:\n            offset (int, long): offset from base physical address, in bytes.\n            value (int, long): 8-bit value to write.\n\n        Raises:\n            TypeError: if `offset` or `value` type are invalid.\n            ValueError: if `offset` or `value` are out of bounds.\n\n        \"\"\"\n        if not isinstance(offset, (int, long)):\n            raise TypeError(\"Invalid offset type, should be integer.\")\n        if not isinstance(value, (int, long)):\n            raise TypeError(\"Invalid value type, should be integer.\")\n        if value < 0 or value > 0xff:\n            raise ValueError(\"Value out of bounds.\")\n\n        offset = self._adjust_offset(offset)\n        self._validate_offset(offset, 1)\n        self.mapping[offset:offset + 1] = struct.pack(\"B\", value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, offset, data):\n        if not isinstance(offset, (int, long)):\n            raise TypeError(\"Invalid offset type, should be integer.\")\n        if not isinstance(data, (bytes, bytearray, list)):\n            raise TypeError(\"Invalid data type, expected bytes, bytearray, or list.\")\n\n        offset = self._adjust_offset(offset)\n        self._validate_offset(offset, len(data))\n\n        data = bytes(bytearray(data))\n        self.mapping[offset:offset + len(data)] = data", "response": "Writes a string of bytes to the specified offset in bytes relative to the base physical address of the MMIO region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pointer(self):\n        return ctypes.cast(ctypes.pointer(ctypes.c_uint8.from_buffer(self.mapping, 0)), ctypes.c_void_p)", "response": "Get a ctypes void pointer to the memory mapped region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshifting out data and return shifted in data.", "response": "def transfer(self, data):\n        \"\"\"Shift out `data` and return shifted in data.\n\n        Args:\n            data (bytes, bytearray, list): a byte array or list of 8-bit integers to shift out.\n\n        Returns:\n            bytes, bytearray, list: data shifted in.\n\n        Raises:\n            SPIError: if an I/O or OS error occurs.\n            TypeError: if `data` type is invalid.\n            ValueError: if data is not valid bytes.\n\n        \"\"\"\n        if not isinstance(data, (bytes, bytearray, list)):\n            raise TypeError(\"Invalid data type, should be bytes, bytearray, or list.\")\n\n        # Create mutable array\n        try:\n            buf = array.array('B', data)\n        except OverflowError:\n            raise ValueError(\"Invalid data bytes.\")\n\n        buf_addr, buf_len = buf.buffer_info()\n\n        # Prepare transfer structure\n        spi_xfer = _CSpiIocTransfer()\n        spi_xfer.tx_buf = buf_addr\n        spi_xfer.rx_buf = buf_addr\n        spi_xfer.len = buf_len\n\n        # Transfer\n        try:\n            fcntl.ioctl(self._fd, SPI._SPI_IOC_MESSAGE_1, spi_xfer)\n        except OSError as e:\n            raise SPIError(e.errno, \"SPI transfer: \" + e.strerror)\n\n        # Return shifted out data with the same type as shifted in data\n        if isinstance(data, bytes):\n            return bytes(bytearray(buf))\n        elif isinstance(data, bytearray):\n            return bytearray(buf)\n        elif isinstance(data, list):\n            return buf.tolist()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_session_key(request):\n    key = request.session.session_key\n    if key is None:\n        # @@@ Django forces us to handle session key collision amongst\n        # multiple processes (not handled)\n        request.session.save()\n        # force session to persist for client\n        request.session.modified = True\n        key = request.session.session_key\n    return key", "response": "Ensures that the session key for the given request is set to the client s session key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef proxy_urls_from_dns(local_hostname=None):\r\n    if not local_hostname:\r\n        local_hostname = socket.getfqdn()\r\n    if '.' not in local_hostname or len(local_hostname) < 3 or \\\r\n            local_hostname.startswith('.') or local_hostname.endswith('.'):\r\n        return []\r\n    try:\r\n        parsed = get_tld('http://' + local_hostname, as_object=True)\r\n        subdomain, tld = parsed.subdomain, parsed.fld\r\n    except TldDomainNotFound:\r\n        final_dot_index = local_hostname.rfind('.')\r\n        subdomain, tld = local_hostname[0:final_dot_index], local_hostname[final_dot_index+1:]\r\n    return wpad_search_urls(subdomain, tld)", "response": "Returns a list of URLs that can be used to look up a PAC file based on a hostname."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating URLs from which to look for a PAC file based on the subdomain and TLD parts of the fully - qualified host name.", "response": "def wpad_search_urls(subdomain_or_host, fld):\r\n    \"\"\"\r\n    Generate URLs from which to look for a PAC file, based on the subdomain and TLD parts of\r\n    a fully-qualified host name.\r\n\r\n    :param str subdomain_or_host: Subdomain portion of the fully-qualified host name.\r\n        For foo.bar.example.com, this is foo.bar.\r\n    :param str fld: FLD portion of the fully-qualified host name.\r\n        For foo.bar.example.com, this is example.com. \r\n    :return: PAC URLs to try in order, according to the WPAD protocol.\r\n    :rtype: list[str]\r\n    \"\"\"\r\n    parts = subdomain_or_host.split('.')\r\n    search_urls = []\r\n    for i in range(1, len(parts)+1):\r\n        # Chop off host and move up the subdomain hierarchy.\r\n        url = 'http://wpad.{}/wpad.dat'.format('.'.join(parts[i:] + [fld]))\r\n        search_urls.append(url)\r\n\r\n    return search_urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autoconfig_url_from_registry():\r\n    if not ON_WINDOWS:\r\n        raise NotWindowsError()\r\n\r\n    try:\r\n        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,\r\n                            'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Internet Settings') as key:\r\n            return winreg.QueryValueEx(key, 'AutoConfigURL')[0]\r\n    except WindowsError:\r\n        return", "response": "Get the AutoConfigURL value from the Windows Registry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autoconfig_url_from_preferences():\r\n    if not ON_DARWIN:\r\n        raise NotDarwinError()\r\n\r\n    try:\r\n        config = SystemConfiguration.SCDynamicStoreCopyProxies(None)\r\n    except AttributeError:\r\n        return  # Key or value not found.\r\n\r\n    if all(('ProxyAutoConfigEnable' in config,\r\n            'ProxyAutoConfigURLString' in config,\r\n            not config.get('ProxyAutoDiscoveryEnable', 0))):\r\n        # Only return a value if it is enabled, not empty, and auto discovery is disabled.\r\n        return str(config['ProxyAutoConfigURLString'])", "response": "Returns the URL value from the PAC AutoConfigURL field in the registry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef file_url_to_local_path(file_url):\r\n    parts = urlparse(file_url)\r\n    path = unquote(parts.path)\r\n    if path.startswith('/') and not path.startswith('//'):\r\n        if ON_DARWIN:\r\n            return path\r\n        if len(parts.netloc) == 2 and parts.netloc[1] == ':':  # Drive letter and colon.\r\n            return parts.netloc + path\r\n        return 'C:' + path  # Assume C drive if it's just a path starting with /.\r\n    if len(path) > 2 and path[1] == ':':\r\n        return path", "response": "Parse a AutoConfigURL value with file:// scheme into a usable local filesystem path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding username and password to a proxy URL.", "response": "def add_proxy_auth(possible_proxy_url, proxy_auth):\n    \"\"\"\n    Add a username and password to a proxy URL, if the input value is a proxy URL.\n\n    :param str possible_proxy_url: Proxy URL or ``DIRECT``.\n    :param requests.auth.HTTPProxyAuth proxy_auth: Proxy authentication info.\n    :returns: Proxy URL with auth info added, or ``DIRECT``.\n    :rtype: str\n    \"\"\"\n    if possible_proxy_url == 'DIRECT':\n        return possible_proxy_url\n    parsed = urlparse(possible_proxy_url)\n    return '{0}://{1}:{2}@{3}'.format(parsed.scheme, proxy_auth.username, proxy_auth.password, parsed.netloc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the proxies that are applicable to a given URL.", "response": "def get_proxies(self, url):\n        \"\"\"\n        Get the proxies that are applicable to a given URL, according to the PAC file.\n\n        :param str url: The URL for which to find appropriate proxies.\n        :return: All the proxies that apply to the given URL.\n            Can be empty, which means to abort the request.\n        :rtype: list[str]\n        \"\"\"\n        hostname = urlparse(url).hostname\n        if hostname is None:\n            # URL has no hostname, and PAC functions don't expect to receive nulls.\n            hostname = \"\"\n\n        value_from_js_func = self.pac.find_proxy_for_url(url, hostname)\n        if value_from_js_func in self._cache:\n            return self._cache[value_from_js_func]\n\n        config_values = parse_pac_value(self.pac.find_proxy_for_url(url, hostname), self.socks_scheme)\n        if self._proxy_auth:\n            config_values = [add_proxy_auth(value, self._proxy_auth) for value in config_values]\n\n        self._cache[value_from_js_func] = config_values\n\n        return config_values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_proxy(self, url):\n        proxies = self.get_proxies(url)\n        for proxy in proxies:\n            if proxy == 'DIRECT' or proxy not in self._offline_proxies:\n                return proxy", "response": "Get a proxy to use for a given URL excluding any banned ones."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the proxy configuration for a given URL.", "response": "def get_proxy_for_requests(self, url):\n        \"\"\"\n        Get proxy configuration for a given URL, in a form ready to use with the Requests library.\n\n        :param str url: The URL for which to obtain proxy configuration.\n        :returns: Proxy configuration in a form recognized by Requests, for use with the ``proxies`` parameter.\n        :rtype: dict\n        :raises ProxyConfigExhaustedError: If no proxy is configured or available,\n            and 'DIRECT' is not configured as a fallback.\n        \"\"\"\n        proxy = self.get_proxy(url)\n        if not proxy:\n            raise ProxyConfigExhaustedError(url)\n        return proxy_parameter_for_requests(proxy)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a parsed PAC file for the given URL and JS.", "response": "def get_pac(url=None, js=None, from_os_settings=True, from_dns=True, timeout=2, allowed_content_types=None, **kwargs):\r\n    \"\"\"\r\n    Convenience function for finding and getting a parsed PAC file (if any) that's ready to use.\r\n\r\n    :param str url: Download PAC from a URL.\r\n        If provided, `from_os_settings` and `from_dns` are ignored.\r\n    :param str js: Parse the given string as a PAC file.\r\n        If provided, `from_os_settings` and `from_dns` are ignored.\r\n    :param bool from_os_settings: Look for a PAC URL or filesystem path from the OS settings, and use it if present.\r\n        Doesn't do anything on non-Windows or non-macOS/OSX platforms.\r\n    :param bool from_dns: Look for a PAC file using the WPAD protocol.\r\n    :param timeout: Time to wait for host resolution and response for each URL.\r\n    :param allowed_content_types: If the response has a ``Content-Type`` header,\r\n        then consider the response to be a PAC file only if the header is one of these values.\r\n        If not specified, the allowed types are\r\n        ``application/x-ns-proxy-autoconfig`` and ``application/x-javascript-config``.\r\n    :return: The first valid parsed PAC file according to the criteria, or `None` if nothing was found.\r\n    :rtype: PACFile|None\r\n    :raises MalformedPacError: If something that claims to be a PAC file was obtained but could not be parsed.\r\n    \"\"\"\r\n    if url:\r\n        downloaded_pac = download_pac([url], timeout=timeout, allowed_content_types=allowed_content_types)\r\n        if not downloaded_pac:\r\n            return\r\n        return PACFile(downloaded_pac, **kwargs)\r\n    if js:\r\n        return PACFile(js, **kwargs)\r\n\r\n    # Deprecated in 0.8.2\r\n    from_registry = kwargs.get('from_registry')\r\n    if from_registry is not None:\r\n        import warnings\r\n        warnings.warn('from_registry is deprecated, use from_os_settings instead.')\r\n        from_os_settings = from_registry\r\n\r\n    if from_os_settings:\r\n        if ON_WINDOWS:\r\n            path = autoconfig_url_from_registry()\r\n        elif ON_DARWIN:\r\n            path = autoconfig_url_from_preferences()\r\n        else:\r\n            path = None\r\n\r\n        if path and path.lower().startswith('file://'):\r\n            path = file_url_to_local_path(path)\r\n\r\n        if path and os.path.isfile(path):\r\n            with open(path) as f:\r\n                return PACFile(f.read(), **kwargs)\r\n\r\n    pac_candidate_urls = collect_pac_urls(from_os_settings=True, from_dns=from_dns)\r\n    downloaded_pac = download_pac(pac_candidate_urls, timeout=timeout, allowed_content_types=allowed_content_types)\r\n    if not downloaded_pac:\r\n        return\r\n    return PACFile(downloaded_pac, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of URLs that potentially yield a PAC file.", "response": "def collect_pac_urls(from_os_settings=True, from_dns=True, **kwargs):\r\n    \"\"\"\r\n    Get all the URLs that potentially yield a PAC file.\r\n\r\n    :param bool from_os_settings: Look for a PAC URL from the OS settings.\r\n        If a value is found and is a URL, it comes first in the returned list.\r\n        Doesn't do anything on non-Windows or non-macOS/OSX platforms.\r\n    :param bool from_dns: Assemble a list of PAC URL candidates using the WPAD protocol.\r\n    :return: A list of URLs that should be tried in order.\r\n    :rtype: list[str]\r\n    \"\"\"\r\n    # Deprecated in 0.8.2\r\n    from_registry = kwargs.get('from_registry')\r\n    if from_registry is not None:\r\n        import warnings\r\n        warnings.warn('from_registry is deprecated, use from_os_settings instead.')\r\n        from_os_settings = from_registry\r\n\r\n    pac_urls = []\r\n    if from_os_settings:\r\n        if ON_WINDOWS:\r\n            url_or_path = autoconfig_url_from_registry()\r\n        elif ON_DARWIN:\r\n            url_or_path = autoconfig_url_from_preferences()\r\n        else:\r\n            url_or_path = None\r\n\r\n        if url_or_path and (url_or_path.lower().startswith('http://') or url_or_path.lower().startswith('https://')):\r\n            pac_urls.append(url_or_path)\r\n    if from_dns:\r\n        pac_urls.extend(proxy_urls_from_dns())\r\n    return pac_urls"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_pac(candidate_urls, timeout=1, allowed_content_types=None):\r\n    if not allowed_content_types:\r\n        allowed_content_types = {'application/x-ns-proxy-autoconfig', 'application/x-javascript-config'}\r\n\r\n    sess = requests.Session()\r\n    sess.trust_env = False  # Don't inherit proxy config from environment variables.\r\n    for pac_url in candidate_urls:\r\n        try:\r\n            resp = sess.get(pac_url, timeout=timeout)\r\n            content_type = resp.headers.get('content-type', '').lower()\r\n            if content_type and True not in [allowed_type in content_type for allowed_type in allowed_content_types]:\r\n                continue\r\n            if resp.ok:\r\n                return resp.text\r\n        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):\r\n            continue", "response": "Try to download a PAC file from one of the given URLs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pac_context_for_url(url, proxy_auth=None):\r\n    prev_http_proxy, prev_https_proxy = os.environ.get('HTTP_PROXY'), os.environ.get('HTTPS_PROXY')\r\n    pac = get_pac()\r\n    if pac:\r\n        resolver = ProxyResolver(pac, proxy_auth=proxy_auth)\r\n        proxies = resolver.get_proxy_for_requests(url)\r\n        # Cannot set None for environ. (#27)\r\n        os.environ['HTTP_PROXY'] = proxies.get('http') or ''\r\n        os.environ['HTTPS_PROXY'] = proxies.get('https') or ''\r\n    yield\r\n    if prev_http_proxy:\r\n        os.environ['HTTP_PROXY'] = prev_http_proxy\r\n    elif 'HTTP_PROXY' in os.environ:\r\n        del os.environ['HTTP_PROXY']\r\n    if prev_https_proxy:\r\n        os.environ['HTTPS_PROXY'] = prev_https_proxy\r\n    elif 'HTTPS_PROXY' in os.environ:\r\n        del os.environ['HTTPS_PROXY']", "response": "Returns a new context manager that returns a new PACSession for the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, method, url, proxies=None, **kwargs):\r\n        if not self._tried_get_pac:\r\n            self.get_pac()\r\n\r\n        # Whether we obtained or are trying to obtain a proxy from the PAC.\r\n        # PAC is not in use if this request has proxies provided as a parameter, or if the session has disabled PAC.\r\n        using_pac = proxies is None and self._proxy_resolver and self.pac_enabled\r\n\r\n        if using_pac:\r\n            proxies = self._proxy_resolver.get_proxy_for_requests(url)\r\n\r\n        while True:\r\n            proxy_url = list(proxies.values())[0] if proxies else None\r\n            try:\r\n                response = super(PACSession, self).request(method, url, proxies=proxies, **kwargs)\r\n            except Exception as request_exc:\r\n                # Use PAC's proxy failover rules if the proxy used for the request is from the PAC,\r\n                # and this exception represents a proxy failure.\r\n                if using_pac and proxy_url and self._exc_proxy_failure_filter(request_exc):\r\n                    try:\r\n                        proxies = self.do_proxy_failover(proxy_url, url)\r\n                        continue\r\n                    except ProxyConfigExhaustedError:\r\n                        # No failover option, not even DIRECT. Bubble up original exception.\r\n                        self._proxy_resolver.unban_all()\r\n                raise request_exc  # In PY2, just saying 'raise' may re-raise ProxyConfigExhaustedError.\r\n\r\n            # Use PAC's proxy failover rules if the proxy used for the request is from the PAC,\r\n            # and this response represents a proxy failure.\r\n            if using_pac and proxy_url and self._response_proxy_failure_filter(response):\r\n                try:\r\n                    proxies = self.do_proxy_failover(proxy_url, url)\r\n                    continue\r\n                except ProxyConfigExhaustedError:\r\n                    # No failover option, not even DIRECT. Return response as-is.\r\n                    self._proxy_resolver.unban_all()\r\n                    return response\r\n\r\n            return response", "response": "A method that returns a response from the PAC."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to ban the proxy and returns the next proxy config to try.", "response": "def do_proxy_failover(self, proxy_url, for_url):\r\n        \"\"\"\r\n        :param str proxy_url: Proxy to ban.\r\n        :param str for_url: The URL being requested.\r\n        :returns: The next proxy config to try, or 'DIRECT'.\r\n        :raises ProxyConfigExhaustedError: If the PAC file provided no usable proxy configuration.\r\n        \"\"\"\r\n        self._proxy_resolver.ban_proxy(proxy_url)\r\n        return self._proxy_resolver.get_proxy_for_requests(for_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pac(self):\r\n        if self._tried_get_pac:\r\n            return self._proxy_resolver.pac if self._proxy_resolver else None\r\n\r\n        if not self.pac_enabled:\r\n            return\r\n\r\n        pac = get_pac()\r\n        self._tried_get_pac = True\r\n        if pac:\r\n            self._proxy_resolver = self._get_proxy_resolver(pac)\r\n        return pac", "response": "Returns the first available PAC file for the most recent HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninjecting a Python function into a global scope of a dukpy JavaScript interpreter context.", "response": "def _inject_function_into_js(context, name, func):\n    \"\"\"\n    Inject a Python function into the global scope of a dukpy JavaScript interpreter context.\n\n    :type context: dukpy.JSInterpreter\n    :param name: Name to give the function in JavaScript.\n    :param func: Python function.\n    \"\"\"\n    context.export_function(name, func)\n    context.evaljs(\"\"\";\n        {name} = function() {{\n            var args = Array.prototype.slice.call(arguments);\n            args.unshift('{name}');\n            return call_python.apply(null, args);\n        }};\n    \"\"\".format(name=name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_pac_value(value, socks_scheme=None):\n    config = []\n    for element in value.split(';'):\n        element = element.strip()\n        if not element:\n            continue\n        try:\n            config.append(proxy_url(element, socks_scheme))\n        except ValueError as e:\n            warnings.warn(str(e))\n    return config", "response": "Parses the return value of FindProxyForURL method into a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef proxy_url(value, socks_scheme=None):\n    if value.upper() == 'DIRECT':\n        return 'DIRECT'\n    parts = value.split()\n\n    if len(parts) == 2:\n        keyword, proxy = parts[0].upper(), parts[1]\n        if keyword == 'PROXY':\n            return 'http://' + proxy\n        if keyword == 'SOCKS':\n            if not socks_scheme:\n                socks_scheme = 'socks5'\n            return '{0}://{1}'.format(socks_scheme, proxy)\n\n    raise ValueError(\"Unrecognized proxy config value '{}'\".format(value))", "response": "Parses a single proxy config value from FindProxyForURL method into a more usable element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_proxy_for_url(self, url, host):\n        return self._context.evaljs(\"FindProxyForURL(dukpy['url'], dukpy['host'])\", url=url, host=host)", "response": "This method is used to find the proxy for a given URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if the domain of hostname matches the given host.", "response": "def dnsDomainIs(host, domain):\n    \"\"\"\n    :param str host: is the hostname from the URL.\n    :param str domain: is the domain name to test the hostname against.\n    :return: true iff the domain of hostname matches.\n    :rtype: bool\n    \"\"\"\n    if domain.startswith('.'):\n        domain = '*' + domain\n    return shExpMatch(host, domain)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the given IP address is in the network.", "response": "def _address_in_network(ip, netaddr, mask):\n    \"\"\"\n    Like :func:`requests.utils.address_in_network` but takes a quad-dotted netmask.\n    \"\"\"\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netmask = struct.unpack('=L', socket.inet_aton(mask))[0]\n    network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True iff the IP address of the host matches the specified IP address pattern and mask.", "response": "def isInNet(host, pattern, mask):\n    \"\"\"\n    Pattern and mask specification is done the same way as for SOCKS configuration.\n\n    :param str host: a DNS hostname, or IP address.\n        If a hostname is passed, it will be resolved into an IP address by this function.\n    :param str pattern: an IP address pattern in the dot-separated format\n    :param str mask: mask for the IP address pattern informing which parts of\n        the IP address should be matched against. 0 means ignore, 255 means match.\n    :returns: True iff the IP address of the host matches the specified IP address pattern.\n    :rtype: bool\n    \"\"\"\n    host_ip = host if is_ipv4_address(host) else dnsResolve(host)\n    if not host_ip or not is_ipv4_address(pattern) or not is_ipv4_address(mask):\n        return False\n    return _address_in_network(host_ip, pattern, mask)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a boolean value on the weekday that the parameter represents.", "response": "def weekdayRange(start_day, end_day=None, gmt=None):\n    \"\"\"\n    Accepted forms:\n\n    * ``weekdayRange(wd1)``\n    * ``weekdayRange(wd1, gmt)``\n    * ``weekdayRange(wd1, wd2)``\n    * ``weekdayRange(wd1, wd2, gmt)``\n\n    If only one parameter is present, the function yields a true value on the weekday that the parameter represents.\n    If the string \"GMT\" is specified as a second parameter, times are taken to be in GMT, otherwise in local timezone.\n\n    If both ``wd1`` and wd2`` are defined, the condition is true if the current weekday is in between those two weekdays.\n    Bounds are inclusive. If the ``gmt`` parameter is specified, times are taken to be in GMT,\n    otherwise the local timezone is used.\n\n    Weekday arguments are one of ``MON TUE WED THU FRI SAT SUN``.\n\n    :param str start_day: Weekday string.\n    :param str end_day: Weekday string.\n    :param str gmt: is either the string: GMT or is left out.\n    :rtype: bool\n    \"\"\"\n    now_weekday_num = _now('GMT' if end_day == 'GMT' else gmt).weekday()\n    weekdays = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']\n\n    if start_day not in weekdays or (end_day not in weekdays and end_day != 'GMT'):\n        return False\n\n    start_day_num = weekdays.index(start_day)\n    if start_day and (not end_day or end_day == 'GMT'):\n        return start_day_num == now_weekday_num\n\n    end_day_num = weekdays.index(end_day)\n    if end_day_num < start_day_num:  # Range past Sunday.\n        return now_weekday_num >= start_day_num or now_weekday_num <= end_day_num\n\n    return start_day_num <= now_weekday_num <= end_day_num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new base date in the alphabetical order of the given arguments.", "response": "def dateRange(*args):\n    \"\"\"\n    Accepted forms:\n\n    * ``dateRange(day)``\n    * ``dateRange(day1, day2)``\n    * ``dateRange(mon)``\n    * ``dateRange(month1, month2)``\n    * ``dateRange(year)``\n    * ``dateRange(year1, year2)``\n    * ``dateRange(day1, month1, day2, month2)``\n    * ``dateRange(month1, year1, month2, year2)``\n    * ``dateRange(day1, month1, year1, day2, month2, year2)``\n    * ``dateRange(day1, month1, year1, day2, month2, year2, gmt)``\n\n    ``day``\n        is the day of month between 1 and 31 (as an integer).\n    ``month``\n        is one of the month strings:\n            ``JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC``\n    ``year``\n        is the full year number, for example 1995 (but not 95). Integer.\n    ``gmt``\n        is either the string \"GMT\", which makes time comparison occur in GMT timezone;\n        if left unspecified, times are taken to be in the local timezone.\n\n    Even though the above examples don't show,\n    the \"GMT\" parameter can be specified in any of the 9 different call profiles, always as the last parameter.\n\n    If only a single value is specified (from each category: ``day``, ``month``, ``year``),\n    the function returns a true value only on days that match that specification.\n    If both values are specified, the result is true between those times, including bounds.\n\n    :rtype: bool\n    \"\"\"\n    months = [None, 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n    gmt_arg_present = (len(args) == 2 and args[1] == 'GMT') or (len(args) % 2 == 1 and len(args) > 1)\n    if gmt_arg_present:\n        # Remove and handle GMT argument.\n        today = _now(args[-1])\n        args = args[:-1]\n    else:\n        today = _now()\n    today = today.date()\n\n    num_args = len(args)\n    try:\n        if num_args == 1:\n            # Match only against the day, month, or year.\n            val = args[0]\n            if val in months:\n                return today.month == months.index(val)\n            if 1 <= val <= 31:\n                return today.day == val\n            return today.year == val\n        if num_args == 2:\n            # Match against inclusive range of day, month, or year.\n            a1, a2 = args\n            if a1 in months and a2 in months:\n                return months.index(a1) <= today.month <= months.index(a2)\n            if 1 <= a1 <= 31:\n                return a1 <= today.day <= a2\n            return a1 <= today.year <= a2\n        if num_args == 4:\n            # Match against inclusive range of day-month or month-year.\n            if args[0] in months and args[2] in months:\n                m1, y1, m2, y2 = args\n                m1, m2 = months.index(m1), months.index(m2)\n                return date(y1, m1, 1) <= today <= date(y2, m2, monthrange(y2, m2)[1])\n            if args[1] in months and args[3] in months:\n                d1, m1, d2, m2 = args\n                m1, m2 = months.index(m1), months.index(m2)\n                return date(today.year, m1, d1) <= today <= date(today.year, m2, d2)\n        if num_args == 6:\n            # Match against inclusive range of start date and end date.\n            d1, m1, y1, d2, m2, y2 = args\n            m1, m2 = months.index(m1), months.index(m2)\n            return date(y1, m1, d1) <= today <= date(y2, m2, d2)\n    except (ValueError, TypeError):\n        # Probably an invalid M/D/Y argument.\n        return False\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timeRange(*args):\n    gmt_arg_present = (len(args) == 2 and args[1] == 'GMT') or (len(args) % 2 == 1 and len(args) > 1)\n    if gmt_arg_present:\n        # Remove and handle GMT argument.\n        today = _now(args[-1])\n        args = args[:-1]\n    else:\n        today = _now()\n\n    num_args = len(args)\n    if num_args == 1:\n        h1 = args[0]\n        return h1 == today.hour\n    if num_args == 2:\n        h1, h2 = args\n        return h1 <= today.hour < h2\n    if num_args == 4:\n        h1, m1, h2, m2 = args\n        return time(h1, m1) <= today.time() <= time(h2, m2)\n    if num_args == 6:\n        h1, m1, s1, h2, m2, s2 = args\n        return time(h1, m1, s1) <= today.time() <= time(h2, m2, s2)\n    return False", "response": "Returns True if the specified time is within the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_ssh_config(config_path='~/.ssh/config'):\n        ssh_config = paramiko.SSHConfig()\n        try:\n            with open(os.path.realpath(os.path.expanduser(config_path))) as f:\n                ssh_config.parse(f)\n        except IOError:\n            pass\n        return ssh_config", "response": "Extract the configuration located at config_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a binary file - like object.", "response": "def openbin(self, path, mode='r', buffering=-1, **options):  # noqa: D102\n        \"\"\"Open a binary file-like object.\n\n        Arguments:\n            path (str): A path on the filesystem.\n            mode (str): Mode to open the file (must be a valid, non-text mode).\n                Since this method only opens binary files, the ``b`` in the mode\n                is implied.\n            buffering (int): the buffering policy (-1 to use default buffering,\n                0 to disable completely, 1 to enable line based buffering, or\n                any larger positive integer for a custom buffer size).\n\n        Keyword Arguments:\n            pipelined (bool): Set the transfer in pipelined mode (should\n                improve transfer speed). Defaults to ``True``.\n\n        Raises:\n            fs.errors.FileExpected: if the path if not a file.\n            fs.errors.FileExists: if the file already exists and\n                *exclusive mode* is specified (``x`` in the mode).\n            fs.errors.ResourceNotFound: if the path does not exist.\n\n        Returns:\n            io.IOBase: a file handle.\n        \"\"\"\n        self.check()\n        _path = self.validatepath(path)\n        _mode = Mode(mode)\n        _mode.validate_bin()\n\n        with self._lock:\n            if _mode.exclusive:\n                if self.exists(_path):\n                    raise errors.FileExists(path)\n                else:\n                    _mode = Mode(''.join(set(mode.replace('x', 'w'))))\n            elif _mode.reading and not _mode.create and not self.exists(_path):\n                raise errors.ResourceNotFound(path)\n            elif self.isdir(_path):\n                raise errors.FileExpected(path)\n            with convert_sshfs_errors('openbin', path):\n                _sftp = self._client.open_sftp()\n                handle = _sftp.open(\n                    _path,\n                    mode=_mode.to_platform_bin(),\n                    bufsize=buffering\n                )\n                handle.set_pipelined(options.get(\"pipelined\", True))\n                return SSHFile(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the platform of the remote server as in sys. platform.", "response": "def platform(self):\n        \"\"\"The platform the server is running on.\n\n        Returns:\n            str: the platform of the remote server, as in `sys.platform`.\n        \"\"\"\n        uname_sys = self._exec_command(\"uname -s\")\n        sysinfo = self._exec_command(\"sysinfo\")\n        if uname_sys is not None:\n            if uname_sys == b\"FreeBSD\":\n                return \"freebsd\"\n            elif uname_sys == b\"Darwin\":\n                return \"darwin\"\n            elif uname_sys == b\"Linux\":\n                return \"linux\"\n            elif uname_sys.startswith(b\"CYGWIN\"):\n                return \"cygwin\"\n        elif sysinfo is not None and sysinfo:\n            return \"win32\"\n        return \"unknown\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef locale(self):\n        if self.platform in (\"linux\", \"darwin\", \"freebsd\"):\n            locale = self._exec_command('echo $LANG')\n            if locale is not None:\n                return locale.split(b'.')[-1].decode('ascii').lower()\n        return None", "response": "Returns the locale of the remote server if detected or None if not detected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _exec_command(self, cmd):\n        _, out, err = self._client.exec_command(cmd, timeout=self._timeout)\n        return out.read().strip() if not err.read().strip() else None", "response": "Run a command on the remote SSH server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_info(self, name, stat_result, namespaces):\n        info = {\n            'basic': {\n                'name': name,\n                'is_dir': stat.S_ISDIR(stat_result.st_mode)\n            }\n        }\n        if 'details' in namespaces:\n            info['details'] = self._make_details_from_stat(stat_result)\n        if 'stat' in namespaces:\n            info['stat'] = {\n                k: getattr(stat_result, k)\n                for k in dir(stat_result) if k.startswith('st_')\n            }\n        if 'access' in namespaces:\n            info['access'] = self._make_access_from_stat(stat_result)\n        return Info(info)", "response": "Create an Info object from a stat result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_details_from_stat(self, stat_result):\n        details = {\n            '_write': ['accessed', 'modified'],\n            'accessed': stat_result.st_atime,\n            'modified': stat_result.st_mtime,\n            'size': stat_result.st_size,\n            'type': int(OSFS._get_type_from_stat(stat_result)),\n        }\n\n        details['created'] = getattr(stat_result, 'st_birthtime', None)\n        ctime_key = 'created' if self.platform==\"win32\" else 'metadata_changed'\n        details[ctime_key] = getattr(stat_result, 'st_ctime', None)\n        return details", "response": "Make a details dictionnary from a stat result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_access_from_stat(self, stat_result):\n        access = {}\n        access['permissions'] = Permissions(mode=stat_result.st_mode).dump()\n        access['gid'] = stat_result.st_gid\n        access['uid'] = stat_result.st_uid\n\n        if self.platform in (\"linux\", \"darwin\", \"freebsd\"):\n            def entry_name(db, _id):\n                entry = self._exec_command('getent {} {}'.format(db, _id))\n                name = next(iter(entry.split(b':')))\n                return name.decode(self.locale or 'utf-8')\n            access['group'] = entry_name('group', access['gid'])\n            access['user'] = entry_name('passwd', access['uid'])\n\n        return access", "response": "Make an access dictionary from a stat result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the owner of a resource.", "response": "def _chown(self, path, uid, gid):\n        \"\"\"Change the *owner* of a resource.\n        \"\"\"\n        if uid is None or gid is None:\n            info = self.getinfo(path, namespaces=('access',))\n            uid = uid or info.get('access', 'uid')\n            gid = gid or info.get('access', 'gid')\n        self._sftp.chown(path, uid, gid)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _utime(self, path, modified, accessed):\n        if accessed is not None or modified is not None:\n            accessed = int(modified if accessed is None else accessed)\n            modified = int(accessed if modified is None else modified)\n            self._sftp.utime(path, (accessed, modified))\n        else:\n            self._sftp.utime(path, None)", "response": "Set the accessed and modified times of a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns single value or list of values from the attribute.", "response": "def value(self):\n        '''Return single value or list of values from the attribute.\n           If FORCE_ATTRIBUTE_VALUE_AS_LIST is True, always return a\n           list with values.\n        '''\n        if len(self.__dict__['values']) == 1 and current_app.config['FORCE_ATTRIBUTE_VALUE_AS_LIST'] is False:\n            return self.__dict__['values'][0]\n        else:\n            return self.__dict__['values']"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds another value to the attribute", "response": "def append(self, value):\n        '''Add another value to the attribute'''\n        if self.__dict__['values']:\n            self.__dict__['changetype'] = MODIFY_REPLACE\n\n        self.__dict__['values'].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattempts to bind a user to the LDAP server.", "response": "def authenticate(self,\n                     username,\n                     password,\n                     attribute=None,\n                     base_dn=None,\n                     search_filter=None,\n                     search_scope=SUBTREE):\n        '''Attempts to bind a user to the LDAP server.\n\n        Args:\n            username (str): DN or the username to attempt to bind with.\n            password (str): The password of the username.\n            attribute (str): The LDAP attribute for the username.\n            base_dn (str): The LDAP basedn to search on.\n            search_filter (str): LDAP searchfilter to attempt the user\n                search with.\n\n        Returns:\n            bool: ``True`` if successful or ``False`` if the\n                credentials are invalid.\n        '''\n        # If the username is no valid DN we can bind with, we need to find\n        # the user first.\n        valid_dn = False\n\n        try:\n            parse_dn(username)\n            valid_dn = True\n        except LDAPInvalidDnError:\n            pass\n\n        if valid_dn is False:\n            user_filter = '({0}={1})'.format(attribute, username)\n            if search_filter is not None:\n                user_filter = '(&{0}{1})'.format(user_filter, search_filter)\n\n            try:\n                self.connection.search(base_dn, user_filter, search_scope,\n                                       attributes=[attribute])\n                response = self.connection.response\n                username = response[0]['dn']\n            except (LDAPInvalidDnError, LDAPInvalidFilterError, IndexError):\n                return False\n\n        try:\n            conn = self.connect(username, password)\n            conn.unbind()\n            return True\n        except LDAPBindError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, ldap_dn):\n        '''Return an LDAP entry by DN\n\n        Args:\n            ldap_dn (str): LDAP DN\n        '''\n        self.base_dn = ldap_dn\n        self.sub_tree = BASE\n        return self.first()", "response": "Return an LDAP entry by DN\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the query filter to perform the query with", "response": "def filter(self, *query_filter):\n        '''Set the query filter to perform the query with\n\n        Args:\n            *query_filter: Simplified Query Language filter\n        '''\n        for query in query_filter:\n            self.query.append(query)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all(self, components_in_and=True):\n        '''Return all of the results of a query in a list'''\n        self.components_in_and = components_in_and\n        return [obj for obj in iter(self)]", "response": "Return all of the results of a query in a list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_monitor(module):\n    def monitor(name, tensor,\n                track_data=True,\n                track_grad=True):\n        \"\"\"\n        Register the tensor under the name given (now a string)\n        and track it based on the track_data and track_grad arguments.\n        \"\"\"\n        module.monitored_vars[name] = {\n            'tensor':tensor,\n            'track_data':track_data,\n            'track_grad':track_grad,\n        }\n    module.monitor = monitor", "response": "Defines the monitor method on the module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines the monitoring method on the module.", "response": "def set_monitoring(module):\n    \"\"\" Defines the monitoring method on the module. \"\"\"\n    def monitoring(is_monitoring,\n                   track_data=None,\n                   track_grad=None,\n                   track_update=None,\n                   track_update_ratio=None):\n        \"\"\"\n        Turn monitoring on or off. If any of the keyword arguments\n        are not None, they will be overwritten.\n        \"\"\"\n        module.is_monitoring = is_monitoring\n        module.track_data = track_data if track_data is not None else module.track_data\n        module.track_grad = track_grad if track_grad is not None else module.track_grad\n        module.track_update = track_update if track_update is not None else module.track_update\n        module.track_update_ratio = track_update_ratio if track_update_ratio is not None else module.track_update_ratio\n    module.monitoring = monitoring"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a gradient hook that will be used for the gradient of a module.", "response": "def grad_hook(module, name, writer, bins):\n    \"\"\" Factory for grad_hook closures \"\"\"\n    def hook(grad):\n        writer.add_histogram('{}/grad'.format(name.replace('.','/')),\n                             grad.detach().cpu().numpy(),\n                             module.global_step-1,\n                             bins=bins)\n    return hook"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_grad_hooks(module, input):\n    for hook in list(module.param_hooks.keys()):\n        module.param_hooks[hook].remove()\n        module.param_hooks.pop(hook)\n    for hook in list(module.var_hooks.keys()):\n        module.var_hooks[hook].remove()\n        module.var_hooks.pop(hook)", "response": "Remove gradient hooks to all of the parameters and monitored vars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the method for monitoring the forward values of the network.", "response": "def get_monitor_forward_and_backward(summary_writer, bins):\n    \"\"\" Get the method for monitoring the forward values of the network \"\"\"\n    def monitor_forward_and_backward(module, input, output):\n        \"\"\"\n        Iterate over the module parameters and monitor their forward values.\n        Then iterate over all of the monitored_vars, monitor their forward values\n        and set their grad_hooks\n        \"\"\"\n        # Parameters\n        if module.is_monitoring:\n            param_names = [ name for name, _ in module.named_parameters()]\n            for name, param in zip(param_names, module.parameters()):\n                if module.track_grad and param.requires_grad:\n                    hook = grad_hook(module, name, summary_writer, bins)\n                    module.param_hooks[name] = param.register_hook(hook)\n                if module.track_data:\n                    summary_writer.add_histogram('{}/data'.format(name.replace('.','/')),\n                                                 param.detach().cpu().numpy(),\n                                                 module.global_step,\n                                                 bins=bins)\n                if name in module.last_state_dict:\n                    if module.track_update:\n                        update = param - module.last_state_dict[name]\n                        summary_writer.add_histogram('{}/update-val'.format(name.replace('.','/')),\n                                                     update.detach().cpu().numpy(),\n                                                     module.global_step-1,\n                                                     bins=bins)\n                    if module.track_update and module.track_update_ratio:\n                        update_ratio = update / (module.last_state_dict[name]+1e-15)\n\n                        summary_writer.add_histogram('{}/update-ratio'.format(name.replace('.','/')),\n                                                     update_ratio.detach().cpu().numpy(),\n                                                     module.global_step-1,\n                                                     bins=bins)\n                if module.track_update:\n                    module.last_state_dict[name] = param.clone()\n\n            # Intermediate Vars\n            for prefix, mod in module.named_modules():\n                for tensor_name, entry in mod.monitored_vars.items():\n                    name = '{}/{}'.format(prefix, tensor_name) if prefix else tensor_name\n                    tensor = entry['tensor']\n                    if entry['track_grad'] and tensor.requires_grad:\n                        hook = grad_hook(module, name, summary_writer, bins)\n                        module.var_hooks[name] = tensor.register_hook(hook)\n                    if entry['track_data']:\n                        summary_writer.add_histogram('{}/data'.format(name.replace('.','/')),\n                                                     tensor.detach().cpu().numpy(),\n                                                     module.global_step,\n                                                     bins=bins)\n            # Update step\n        module.global_step += 1\n    return monitor_forward_and_backward"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef monitor_module(module, summary_writer,\n                   track_data=True,\n                   track_grad=True,\n                   track_update=True,\n                   track_update_ratio=False, # this is usually unnecessary\n                   bins=51):\n    \"\"\" Allows for remote monitoring of a module's params and buffers.\n    The following may be monitored:\n      1. Forward Values - Histograms of the values for parameter and buffer tensors\n      2. Gradient Values - Histograms of the gradients for parameter and buffer tensors\n      3. Update Values - Histograms of the change in parameter and buffer tensor\n           values from the last recorded forward pass\n      4. Update Ratios - Histograms of the ratio of change in value for parameter\n           and value tensors from the last iteration to the actual values.\n           I.e., what is the relative size of the update.\n           Generally we like to see values of about .001.\n           See [cite Andrej Karpathy's babysitting dnn's blog post]\n    \"\"\"\n\n\n    # The module will need additional information\n    module.track_data = track_data\n    module.track_grad = track_grad\n    module.track_update = track_update\n    module.track_update_ratio = track_update_ratio\n    if not hasattr(module, 'global_step'):\n        module.global_step = 0\n    if not hasattr(module, 'is_monitoring'):\n        module.is_monitoring = True\n    if not hasattr(module, 'monitoring'):\n        set_monitoring(module)\n    if not hasattr(module, 'last_state_dict'):\n        module.last_state_dict = dict()\n    if not hasattr(module, 'var_hooks'):\n        module.var_hooks = dict()\n    if not hasattr(module, 'param_hooks'):\n        module.param_hooks = dict()\n\n    # All submodules need to have these\n    for name, mod in module.named_modules():\n        if not hasattr(mod, 'monitor'):\n            set_monitor(mod)\n        if not hasattr(mod, 'monitored_vars'):\n            mod.monitored_vars = dict()\n\n    module.monitoring(True)\n\n    # remove previous grad hooks before handles go stale\n    module.register_forward_pre_hook(remove_grad_hooks)\n\n    # set forward hook that monitors forward activations and sets new grad hooks\n    monitor_forward_and_backward = get_monitor_forward_and_backward(summary_writer, bins)\n    module.register_forward_hook(monitor_forward_and_backward)", "response": "Allows for remote monitoring of a module's params and buffers.\n    The following may be monitored:\n      1. Forward Values - Histograms of the values for parameter and buffer tensors\n      2. Gradient Values - Histograms of the gradients for parameter and buffer tensors\n      3. Update Values - Histograms of the change in parameter and buffer tensor\n           values from the last recorded forward pass\n      4. Update Ratios - Histograms of the ratio of change in value for parameter\n           and value tensors from the last iteration to the actual values.\n           I.e., what is the relative size of the update.\n           Generally we like to see values of about .001.\n           See [cite Andrej Karpathy's babysitting dnn's blog post]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef commit(experiment_name, time):\n    try:\n        sh.git.commit('-a',\n                m='\"auto commit tracked files for new experiment: {} on {}\"'.format(experiment_name, time),\n                allow_empty=True\n            )\n        commit_hash = sh.git('rev-parse', 'HEAD').strip()\n        return commit_hash\n    except:\n        return '<Unable to commit>'", "response": "Commit the current revision of the current file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary representation of the object.", "response": "def as_dict(self):\n        \"\"\"\n        Returns:\n            dict\n        \"\"\"\n        dict_fw = {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'rules_direction': self.rules_direction,\n            'rules_ip_protocol': self.rules_ip_protocol,\n            'rules_from_port': self.rules_from_port,\n            'rules_to_port': self.rules_to_port,\n            'rules_grants_group_id': self.rules_grants_group_id,\n            'rules_grants_name': self.rules_grants_name,\n            'rules_grants_cidr_ip': self.rules_grants_cidr_ip,\n            'rules_description': self.rules_description\n        }\n\n        return dict_fw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noutput the security rules as a json string.", "response": "def json(self):\n        \"\"\"\n        Output the security rules as a json string.\n\n        Return:\n            str\n        \"\"\"\n        return json.dumps(self.dict_rules,\n                          sort_keys=True,\n                          indent=2,\n                          separators=(',', ': '))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a sorted list of firewall rules.", "response": "def rules(self):\n        \"\"\"\n        Returns a sorted list of firewall rules.\n\n        Returns:\n            list\n        \"\"\"\n        list_of_rules = []\n\n        for main_row in self.dict_rules:\n            if 'rules' in main_row:\n                for rule_row in main_row['rules']:\n                    if 'grants' in rule_row:\n                        for grant_row in rule_row['grants']:\n                            if 'group_id' in grant_row:\n                                # Set a var to not go over 80 chars\n                                group_id = grant_row['group_id']\n\n                                # Some VPC grants don't specify a name\n                                if 'name' in grant_row:\n                                    row_name = grant_row['name']\n                                else:\n                                    row_name = None\n\n                                fr = FirewallRule(\n                                    main_row['id'],\n                                    main_row['name'],\n                                    main_row['description'],\n                                    rules_direction=rule_row['direction'],\n                                    rules_ip_protocol=rule_row['ip_protocol'],\n                                    rules_from_port=rule_row['from_port'],\n                                    rules_to_port=rule_row['to_port'],\n                                    rules_grants_group_id=group_id,\n                                    rules_grants_name=row_name,\n                                    rules_description=grant_row['description'])\n\n                                list_of_rules.append(fr)\n                            elif 'cidr_ip' in grant_row:\n                                fr = FirewallRule(\n                                    main_row['id'],\n                                    main_row['name'],\n                                    main_row['description'],\n                                    rules_direction=rule_row['direction'],\n                                    rules_ip_protocol=rule_row['ip_protocol'],\n                                    rules_from_port=rule_row['from_port'],\n                                    rules_to_port=rule_row['to_port'],\n                                    rules_grants_cidr_ip=grant_row['cidr_ip'],\n                                    rules_description=grant_row['description'])\n                                list_of_rules.append(fr)\n                            else:\n                                raise ValueError(\"Unsupported grant:\",\n                                                 grant_row)\n                    else:\n                        fr = FirewallRule(\n                            main_row['id'],\n                            main_row['name'],\n                            main_row['description'],\n                            rules_direction=rule_row['direction'],\n                            rules_ip_protocol=rule_row['ip_protocol'],\n                            rules_from_port=rule_row['from_port'],\n                            rules_to_port=rule_row['to_port'])\n                        list_of_rules.append(fr)\n            else:\n                fr = FirewallRule(main_row['id'],\n                                  main_row['name'],\n                                  main_row['description'])\n                list_of_rules.append(fr)\n\n        # Sort the data in order to get a consistent output\n        sorted_list = sorted(list_of_rules,\n                             key=lambda fr: (str(fr.id),\n                                             str(fr.name),\n                                             str(fr.description),\n                                             str(fr.rules_direction),\n                                             str(fr.rules_ip_protocol),\n                                             str(fr.rules_from_port),\n                                             str(fr.rules_to_port),\n                                             str(fr.rules_grants_group_id),\n                                             str(fr.rules_grants_name),\n                                             str(fr.rules_grants_cidr_ip)))\n\n        return sorted_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef csv(self):\n        # Generate a csv file in memory with all the data in\n        output = StringIO.StringIO()\n        fieldnames = ['id',\n                      'name',\n                      'description',\n                      'rules_direction',\n                      'rules_ip_protocol',\n                      'rules_from_port',\n                      'rules_to_port',\n                      'rules_grants_group_id',\n                      'rules_grants_name',\n                      'rules_grants_cidr_ip',\n                      'rules_description']\n        writer = csv.DictWriter(output, fieldnames=fieldnames)\n        writer.writeheader()\n        for fr in self.rules:\n            writer.writerow(fr.as_dict())\n\n        # Get the CSV in a string\n        csv_content = output.getvalue()\n\n        # Removing some useless newline at the end\n        stripped_csv_content = csv_content.strip()\n\n        return stripped_csv_content", "response": "Returns the security rules as a CSV string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the EC2 security rules off AWS into a list of dict.", "response": "def _get_rules_from_aws(self):\n        \"\"\"\n        Load the EC2 security rules off AWS into a list of dict.\n\n        Returns:\n            list\n        \"\"\"\n        list_of_rules = list()\n\n        if self.profile:\n            boto3.setup_default_session(profile_name=self.profile)\n\n        if self.region:\n            ec2 = boto3.client('ec2', region_name=self.region)\n        else:\n            ec2 = boto3.client('ec2')\n\n        security_groups = ec2.describe_security_groups(Filters=self.filters)\n\n        for group in security_groups['SecurityGroups']:\n            group_dict = dict()\n            group_dict['id'] = group['GroupId']\n            group_dict['name'] = group['GroupName']\n            group_dict['description'] = group.get('Description', None)\n\n            if (group.get('IpPermissions', None) or\n                    group.get('IpPermissionsEgress', None)):\n                group_dict['rules'] = list()\n\n            for rule in group.get('IpPermissions', None):\n                rule_dict = self._build_rule(rule)\n                rule_dict['direction'] = \"INGRESS\"\n\n                group_dict['rules'].append(rule_dict)\n\n            for rule in group.get('IpPermissionsEgress', None):\n                rule_dict = self._build_rule(rule)\n                rule_dict['direction'] = \"EGRESS\"\n\n                group_dict['rules'].append(rule_dict)\n\n            list_of_rules.append(group_dict)\n\n        return list_of_rules"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the attribute of the filter SDK if passed.", "response": "def getattr_sdk(attr, name):\n    \"\"\"\n    Filter SDK attributes\n\n    Args:\n        attr(attribute): Attribute as returned by :func:`getattr`.\n        name(str): Attribute name.\n\n    Returns:\n        `attr` if passed.\n    \"\"\"\n    if inspect.isroutine(attr):\n        if hasattr(attr, '_sdkmeta'):\n            return attr\n    raise AttributeError(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ssh_client(\n    ip_addr,\n    ssh_key=None,\n    host_name=None,\n    ssh_tries=None,\n    propagate_fail=True,\n    username='root',\n    password='123456',\n):\n    \"\"\"\n    Get a connected SSH client\n\n    Args:\n        ip_addr(str): IP address of the endpoint\n        ssh_key(str or list of str): Path to a file which\n            contains the private key\n        hotname(str): The hostname of the endpoint\n        ssh_tries(int): The number of attempts to connect to the endpoint\n        propagate_fail(bool): If set to true, this event will be in the log\n            and fail the outer stage. Otherwise, it will be discarded.\n        username(str): The username to authenticate with\n        password(str): Used for password authentication\n            or for private key decryption\n\n    Raises:\n        :exc:`~LagoSSHTimeoutException`: If the client failed to connect after\n            \"ssh_tries\"\n    \"\"\"\n    host_name = host_name or ip_addr\n    with LogTask(\n        'Get ssh client for %s' % host_name,\n        level='debug',\n        propagate_fail=propagate_fail,\n    ):\n        ssh_timeout = int(config.get('ssh_timeout'))\n        if ssh_tries is None:\n            ssh_tries = int(config.get('ssh_tries', 10))\n\n        start_time = time.time()\n        client = paramiko.SSHClient()\n        client.set_missing_host_key_policy(paramiko.AutoAddPolicy(), )\n        while ssh_tries > 0:\n            try:\n                client.connect(\n                    ip_addr,\n                    username=username,\n                    password=password,\n                    key_filename=ssh_key,\n                    timeout=ssh_timeout,\n                )\n                break\n            except (socket.error, socket.timeout) as err:\n                LOGGER.debug(\n                    'Socket error connecting to %s: %s',\n                    host_name,\n                    err,\n                )\n            except paramiko.ssh_exception.SSHException as err:\n                LOGGER.debug(\n                    'SSH error connecting to %s: %s',\n                    host_name,\n                    err,\n                )\n            except EOFError as err:\n                LOGGER.debug('EOFError connecting to %s: %s', host_name, err)\n            ssh_tries -= 1\n            LOGGER.debug(\n                'Still got %d tries for %s',\n                ssh_tries,\n                host_name,\n            )\n            time.sleep(1)\n        else:\n            end_time = time.time()\n            raise LagoSSHTimeoutException(\n                'Timed out (in %d s) trying to ssh to %s' %\n                (end_time - start_time, host_name)\n            )\n\n    return client", "response": "Get a connected SSH client for the given ip_addr and ssh_key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sysprep(disk, distro, loader=None, backend='direct', **kwargs):\n\n    if loader is None:\n        loader = PackageLoader('lago', 'templates')\n    sysprep_file = _render_template(distro, loader=loader, **kwargs)\n\n    cmd = ['virt-sysprep', '-a', disk]\n    cmd.extend(['--commands-from-file', sysprep_file])\n\n    env = os.environ.copy()\n    if 'LIBGUESTFS_BACKEND' not in env:\n        env['LIBGUESTFS_BACKEND'] = backend\n\n    ret = utils.run_command(cmd, env=env)\n    if ret:\n        raise RuntimeError(\n            'Failed to bootstrap %s\\ncommand:%s\\nstdout:%s\\nstderr:%s' % (\n                disk,\n                ' '.join('\"%s\"' % elem for elem in cmd),\n                ret.out,\n                ret.err,\n            )\n        )", "response": "Run virt - sysprep on the disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a Jinja2 domain template", "response": "def get_domain_template(distro, libvirt_ver, **kwargs):\n    \"\"\"\n    Get a rendered Jinja2 domain template\n\n    Args:\n        distro(str): domain distro\n        libvirt_ver(int): libvirt version\n        kwargs(dict): args for template render\n\n    Returns:\n        str: rendered template\n    \"\"\"\n    env = Environment(\n        loader=PackageLoader('lago', 'providers/libvirt/templates'),\n        trim_blocks=True,\n        lstrip_blocks=True,\n    )\n\n    template_name = 'dom_template-{0}.xml.j2'.format(distro)\n    try:\n        template = env.get_template(template_name)\n    except TemplateNotFound:\n        LOGGER.debug('could not find template %s using default', template_name)\n        template = env.get_template('dom_template-base.xml.j2')\n    return template.render(libvirt_ver=libvirt_ver, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts dict to XML tree", "response": "def dict_to_xml(spec, full_document=False):\n    \"\"\"\n    Convert dict to XML\n\n    Args:\n        spec(dict): dict to convert\n        full_document(bool): whether to add XML headers\n\n    Returns:\n        lxml.etree.Element: XML tree\n    \"\"\"\n\n    middle = xmltodict.unparse(spec, full_document=full_document, pretty=True)\n    return lxml.etree.fromstring(middle)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guestfs_conn_ro(disk):\n\n    disk_path = os.path.expandvars(disk)\n    conn = guestfs.GuestFS(python_return_dict=True)\n    conn.add_drive_ro(disk_path)\n    conn.set_backend(os.environ.get('LIBGUESTFS_BACKEND', 'direct'))\n    try:\n        conn.launch()\n    except RuntimeError as err:\n        LOGGER.debug(err)\n        raise GuestFSError(\n            'failed starting guestfs in readonly mode for disk: {0}'.\n            format(disk)\n        )\n    try:\n        yield conn\n    finally:\n        conn.shutdown()\n        conn.close()", "response": "Open a GuestFS handle and add disk in read only mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guestfs_conn_mount_ro(disk_path, disk_root, retries=5, wait=1):\n\n    for attempt in range(retries):\n        with guestfs_conn_ro(disk_path) as conn:\n            rootfs = find_rootfs(conn, disk_root)\n            try:\n                conn.mount_ro(rootfs, '/')\n            except RuntimeError as err:\n                LOGGER.debug(err)\n                if attempt < retries - 1:\n                    LOGGER.debug(\n                        (\n                            'failed mounting %s:%s using guestfs, '\n                            'attempt %s/%s'\n                        ), disk_path, rootfs, attempt + 1, retries\n                    )\n                    time.sleep(wait)\n                    continue\n                else:\n                    raise GuestFSError(\n                        'failed mounting {0}:{1} using guestfs'.format(\n                            disk_path, rootfs\n                        )\n                    )\n            yield conn\n            try:\n                conn.umount(rootfs)\n            except RuntimeError as err:\n                LOGGER.debug(err)\n                raise GuestFSError(\n                    ('failed unmounting {0}:{1} using'\n                     'guestfs').format(disk_path, rootfs)\n                )\n            break", "response": "Open a new GuestFS handle with disk_path and try to mount the root filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_rootfs(conn, disk_root):\n    rootfs = conn.inspect_os()\n    if not rootfs or len(rootfs) > 1:\n        filesystems = conn.list_filesystems()\n        if disk_root in filesystems:\n            rootfs = [disk_root]\n        else:\n            rootfs = [fs for fs in filesystems.keys() if disk_root in fs]\n            if not rootfs:\n                raise GuestFSError(\n                    'no root fs {0} could be found from list {1}'.format(\n                        disk_root, str(filesystems)\n                    )\n                )\n    return sorted(rootfs)[0]", "response": "Find the image s root filesystem and return its path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting paths from a disk using guestfs", "response": "def extract_paths(disk_path, disk_root, paths, ignore_nopath):\n    \"\"\"\n    Extract paths from a disk using guestfs\n\n    Args:\n        disk_path(str): path to the disk\n        disk_root(str): root partition\n        paths(list of tuples): files to extract in\n            `[(src1, dst1), (src2, dst2)...]` format, if ``srcN`` is a\n            directory in the guest, and ``dstN`` does not exist on the host,\n            it will be created. If ``srcN`` is a file on the guest, it will be\n            copied exactly to ``dstN``\n        ignore_nopath(bool): If set to True, ignore paths in the guest that\n            do not exit\n\n    Returns:\n        None\n\n    Raises:\n        :exc:`~lago.plugins.vm.ExtractPathNoPathError`: if a none existing\n            path was found on the guest, and `ignore_nopath` is False.\n        :exc:`~lago.plugins.vm.ExtractPathError`: on all other failures.\n    \"\"\"\n\n    with guestfs_conn_mount_ro(disk_path, disk_root) as conn:\n        for (guest_path, host_path) in paths:\n            msg = ('Extracting guestfs://{0} to {1}').format(\n                guest_path, host_path\n            )\n\n            LOGGER.debug(msg)\n            try:\n                _copy_path(conn, guest_path, host_path)\n            except ExtractPathNoPathError as err:\n                if ignore_nopath:\n                    LOGGER.debug('%s - ignoring', err)\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_plugin_add_argument(*args, **kwargs):\n\n    def decorator(func):\n        if not isinstance(func, CLIPluginFuncWrapper):\n            func = CLIPluginFuncWrapper(do_run=func)\n\n        func.add_argument(*args, **kwargs)\n        return func\n\n    return decorator", "response": "Decorator that creates or extends the cliplugin for the\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_plugin_add_help(help):\n\n    def decorator(func):\n\n        if not isinstance(func, CLIPluginFuncWrapper):\n            func = CLIPluginFuncWrapper(do_run=func)\n\n        func.set_help(help)\n        return func\n\n    return decorator", "response": "Decorator that adds the cli help to the cli plugin based on the passed in help string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli_plugin(func=None, **kwargs):\n    # this allows calling this function as a decorator generator\n    if func is None:\n        return functools.partial(cli_plugin, **kwargs)\n\n    if not isinstance(func, CLIPluginFuncWrapper):\n        func = CLIPluginFuncWrapper(do_run=func, init_args=kwargs)\n    else:\n        func.set_init_args(kwargs)\n\n    return func", "response": "Decorator that wraps the given function in a CLIPlugin function that handles that method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves a service class from the class name.", "response": "def _resolve_service_class(class_name, service_providers):\n    \"\"\"\n    **NOTE**: This must be remved once the service_class spec entry is fully\n    deprecated\n    Retrieves a service plugin class from the class name instead of the\n    provider name\n    Args:\n        class_name(str): Class name of the service plugin to retrieve\n        service_providers(dict): provider_name->provider_class of the loaded\n            service providers\n    Returns:\n        class: Class of the plugin that matches that name\n    Raises:\n        lago.plugins.NoSuchPluginError: if there was no service plugin that\n            matched the search\n    \"\"\"\n    for plugin in service_providers.itervalues():\n        if plugin.__class__.__name__ == class_name:\n            return plugin\n\n    raise plugins.NoSuchPluginError(\n        'No service provider plugin with class name %s found, loaded '\n        'providers: %s' % (\n            class_name,\n            [\n                plugin.__class__.__name__\n                for plugin in service_providers.itervalues()\n            ],\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_paths(self, paths, ignore_nopath):\n        try:\n            if self._has_tar_and_gzip():\n                self._extract_paths_tar_gz(paths, ignore_nopath)\n            else:\n                self._extract_paths_scp(paths, ignore_nopath)\n        except (ssh.LagoSSHTimeoutException, LagoVMNotRunningError):\n            raise ExtractPathError(\n                'Unable to extract paths from {0}: unreachable with SSH'.\n                format(self.vm.name())\n            )", "response": "Extracts the given paths from the domain\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_snapshot(self, name, *args, **kwargs):\n        return self.provider.create_snapshot(name, *args, **kwargs)", "response": "This method is used to create a snapshot of the current version of the current version of the current version of the current version of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef revert_snapshot(self, name, *args, **kwargs):\n        return self.provider.revert_snapshot(name, *args, **kwargs)", "response": "This method is used to revert a snapshot of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the paths from the cache", "response": "def extract_paths(self, paths, *args, **kwargs):\n        \"\"\"\n        Thin method that just uses the provider\n        \"\"\"\n        return self.provider.extract_paths(paths, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nthin method that just uses the provider", "response": "def extract_paths_dead(self, paths, *args, **kwargs):\n        \"\"\"\n        Thin method that just uses the provider\n        \"\"\"\n        return self.provider.extract_paths_dead(paths, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nthins method that just uses the provider", "response": "def export_disks(\n        self,\n        standalone=True,\n        dst_dir=None,\n        compress=False,\n        collect_only=False,\n        with_threads=True,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Thin method that just uses the provider\n        \"\"\"\n        return self.provider.export_disks(\n            standalone, dst_dir, compress, collect_only, with_threads, *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the VM is reachable with ssh", "response": "def ssh_reachable(self, tries=None, propagate_fail=True):\n        \"\"\"\n        Check if the VM is reachable with ssh\n        Args:\n            tries(int): Number of tries to try connecting to the host\n            propagate_fail(bool): If set to true, this event will appear\n            in the log and fail the outter stage. Otherwise, it will be\n            discarded.\n        Returns:\n            bool: True if the VM is reachable.\n        \"\"\"\n        if not self.running():\n            return False\n\n        try:\n            ssh.get_ssh_client(\n                ip_addr=self.ip(),\n                host_name=self.name(),\n                ssh_tries=tries,\n                propagate_fail=propagate_fail,\n                ssh_key=self.virt_env.prefix.paths.ssh_id_rsa(),\n                username=self._spec.get('ssh-user'),\n                password=self._spec.get('ssh-password'),\n            )\n        except ssh.LagoSSHTimeoutException:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_service_provider(self):\n        service_class = self._spec.get('service_class', None)\n        if service_class is not None:\n            warnings.warn(\n                'The service_class key for a domain is deprecated, you should '\n                'change it to service_provider instead'\n            )\n            service_provider = _resolve_service_class(\n                class_name=service_class,\n                service_providers=self.service_providers,\n            )\n        else:\n            service_provider = self.service_providers.get(\n                self._spec.get('service_provider', None),\n                None,\n            )\n\n        return service_provider", "response": "Get the service provider for a specific service_class key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchoosing the invoking method (using libvirt or ssh) to shutdown / poweroff the domain. If acpi is defined in the domain use libvirt, otherwise use ssh. Args: libvirt_cmd (function): Libvirt function the invoke ssh_cmd (list of str): Shell command to invoke on the domain msg (str): Name of the command that should be inserted to the log message. Returns None Raises: RuntimeError: If acpi is not configured an ssh isn't available", "response": "def _shutdown(self, libvirt_cmd, ssh_cmd, msg):\n        \"\"\"\n        Choose the invoking method (using libvirt or ssh)\n        to shutdown / poweroff the domain.\n\n        If acpi is defined in the domain use libvirt, otherwise use ssh.\n\n        Args:\n            libvirt_cmd (function): Libvirt function the invoke\n            ssh_cmd (list of str): Shell command to invoke on the domain\n            msg (str): Name of the command that should be inserted to the log\n                message.\n\n        Returns\n            None\n\n        Raises:\n            RuntimeError: If acpi is not configured an ssh isn't available\n        \"\"\"\n        if not self.alive():\n            return\n\n        with LogTask('{} VM {}'.format(msg, self.vm.name())):\n            dom = self.libvirt_con.lookupByName(self._libvirt_name())\n            dom_xml = dom.XMLDesc()\n\n            idx = dom_xml.find('<acpi/>')\n            if idx == -1:\n                LOGGER.debug(\n                    'acpi is not enabled on the host, '\n                    '{} using ssh'.format(msg)\n                )\n                # TODO: change the ssh timeout exception from runtime exception\n                # TODO: to custom exception and catch it.\n                self.vm.ssh(ssh_cmd)\n            else:\n                LOGGER.debug('{} using libvirt'.format(msg))\n                libvirt_cmd(dom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the object representation of this provider VM.", "response": "def _get_domain(self):\n        \"\"\"\n        Return the object representation of this provider VM.\n\n        Returns:\n            libvirt.virDomain: Libvirt domain object\n\n        Raises:\n            :exc:`~lago.plugins.vm.LagoFailedToGetVMStateError:\n                If the VM exist, but the query returned an error.\n        \"\"\"\n        try:\n            return self.libvirt_con.lookupByName(self._libvirt_name())\n        except libvirt.libvirtError as e:\n            raise vm_plugin.LagoVMDoesNotExistError(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the state of the domain in Libvirt s terms", "response": "def raw_state(self):\n        \"\"\"\n        Return the state of the domain in Libvirt's terms\n\n        Retruns:\n            tuple of ints: The state and its reason\n\n        Raises:\n            :exc:`~lago.plugins.vm.LagoVMDoesNotExistError`:\n                If the VM of this provider doesn't exist.\n            :exc:`~lago.plugins.vm.LagoFailedToGetVMStateError:\n                If the VM exist, but the query returned an error.\n        \"\"\"\n        try:\n            return self._get_domain().state()\n        except libvirt.libvirtError as e:\n            raise vm_plugin.LagoFailedToGetVMStateError(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a small description of the current status of the domain", "response": "def state(self):\n        \"\"\"\n        Return a small description of the current status of the domain\n\n        Returns:\n            str: small description of the domain status, 'down' if it's not\n            found at all.\n        \"\"\"\n        try:\n            return libvirt_utils.Domain.resolve_state(self.raw_state())\n        except vm_plugin.LagoVMDoesNotExistError:\n            return 'down'\n        except vm_plugin.LagoFailedToGetVMStateError:\n            return 'failed to get state'\n        except KeyError:\n            return 'unknown state'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_paths(self, paths, ignore_nopath):\n\n        try:\n\n            super().extract_paths(\n                paths=paths,\n                ignore_nopath=ignore_nopath,\n            )\n        except ExtractPathError as err:\n            LOGGER.debug(\n                '%s: failed extracting files: %s', self.vm.name(), err.message\n            )\n            if self._has_guestfs:\n                self.extract_paths_dead(paths, ignore_nopath)\n            else:\n                raise", "response": "This method will attempt to extract all files defined in paths with the method\n           . Extracts all files defined in paths with the method\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_paths_dead(self, paths, ignore_nopath):\n        if not self._has_guestfs:\n            raise LagoException(\n                ('guestfs module not available, cannot '\n                 )('extract files with libguestfs')\n            )\n\n        LOGGER.debug(\n            '%s: attempting to extract files with libguestfs', self.vm.name()\n        )\n        guestfs_tools.extract_paths(\n            disk_path=self.vm.spec['disks'][0]['path'],\n            disk_root=self.vm.spec['disks'][0]['metadata'].get(\n                'root-partition', 'root'\n            ),\n            paths=paths,\n            ignore_nopath=ignore_nopath\n        )", "response": "Extracts the given paths from the guestfs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_disks(\n        self,\n        standalone,\n        dst_dir,\n        compress,\n        collect_only=False,\n        with_threads=True,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Export all the disks of self.\n\n        Args:\n            standalone (bool): if true, merge the base images and the layered\n                image into a new file (Supported only in qcow2 format)\n            dst_dir (str): dir to place the exported disks\n            compress(bool): if true, compress each disk.\n            collect_only(bool): If true, return only a dict which maps between\n                the name of the vm to the paths of the disks that will be\n                exported (don't export anything).\n            with_threads(bool): If True, export disks in parallel\n\n        Returns:\n            (dict): which maps between the name of the vm to the paths of\n            the disks that will be exported\n        \"\"\"\n        vm_export_mgr = export.VMExportManager(\n            disks=self.vm.disks,\n            dst=dst_dir,\n            compress=compress,\n            with_threads=with_threads,\n            standalone=standalone,\n            *args,\n            **kwargs\n        )\n\n        if collect_only:\n            return {self.vm.name(): vm_export_mgr.collect_paths()}\n        else:\n            return {self.vm.name(): vm_export_mgr.export()}", "response": "Exports all the disks of the current node in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef interactive_console(self):\n        if not self.running():\n            raise RuntimeError('VM %s is not running' % self._libvirt_.name)\n        virsh_command = [\n            \"virsh\",\n            \"-c\",\n            config.get('libvirt_url'),\n            \"console\",\n            self._libvirt_name(),\n        ]\n        return utils.run_interactive_command(command=virsh_command, )", "response": "Opens an interactive console"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_version(project_dir=os.curdir):\n    if 'LAGO_VERSION' in os.environ and os.environ['LAGO_VERSION']:\n        return os.environ['LAGO_VERSION']\n\n    version = None\n    pkg_info_file = os.path.join(project_dir, 'PKG-INFO')\n    version_manager = os.path.join(project_dir, 'scripts/version_manager.py')\n    if os.path.exists(pkg_info_file):\n        with open(pkg_info_file) as info_fd:\n            for line in info_fd.readlines():\n                if line.startswith('Version: '):\n                    version = line.split(' ', 1)[-1]\n\n    elif os.path.exists(version_manager):\n        version = check_output([version_manager, project_dir,\n                                'version']).strip()\n\n    if version is None:\n        raise RuntimeError('Failed to get package version')\n\n    # py3 compatibility step\n    if not isinstance(version, str) and isinstance(version, bytes):\n        version = version.decode()\n\n    return version", "response": "Retrieves the version of the package from the PKG - INFO file or generates a version script if it doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_stream_logger(level=logging.DEBUG, name=None):\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    handler = logging.StreamHandler()\n    handler.setFormatter(get_default_log_formatter())\n    handler.setLevel(level)\n    logger.addHandler(handler)", "response": "Add a stream logger to the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the Lago environment and return a new Lago SDK object.", "response": "def init(config, workdir=None, logfile=None, loglevel=logging.INFO, **kwargs):\n    \"\"\"\n    Initialize the Lago environment\n\n    Args:\n        config(str): Path to LagoInitFile\n        workdir(str): Path to initalize the workdir, defaults to \"$PWD/.lago\"\n        **kwargs(dict): Pass arguments to :func:`~lago.cmd.do_init`\n        logfile(str): A path to setup a log file.\n        loglevel(int): :mod:`logging` log level.\n\n    Returns:\n        :class:`~lago.sdk.SDK`: Initialized Lago enviornment\n\n    Raises:\n       :exc:`~lago.utils.LagoException`: If initialization failed\n    \"\"\"\n\n    setup_sdk_logging(logfile, loglevel)\n    defaults = lago_config.get_section('init')\n    if workdir is None:\n        workdir = os.path.abspath('.lago')\n    defaults['workdir'] = workdir\n    defaults['virt_config'] = config\n    defaults.update(kwargs)\n    workdir, prefix = cmd.do_init(**defaults)\n    return SDK(workdir, prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_env(workdir, logfile=None, loglevel=logging.INFO):\n\n    setup_sdk_logging(logfile, loglevel)\n    workdir = os.path.abspath(workdir)\n    loaded_workdir = lago_workdir.Workdir(path=workdir)\n    prefix = loaded_workdir.get_prefix('current')\n    return SDK(loaded_workdir, prefix)", "response": "Loads an existing Lago environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ansible_inventory_temp_file(\n        self, keys=['vm-type', 'groups', 'vm-provider']\n    ):\n        \"\"\"\n        Context manager which returns Ansible inventory written on a tempfile.\n        This is the same as :func:`~ansible_inventory`, only the inventory file\n        is written to a tempfile.\n\n        Args:\n            keys (list of str): Path to the keys that will be used to\n                create groups.\n\n        Yields:\n            tempfile.NamedTemporaryFile: Temp file containing the inventory\n        \"\"\"\n        lansible = LagoAnsible(self._prefix)\n        return lansible.get_inventory_temp_file(keys=keys)", "response": "Context manager which returns Ansible inventory written on a tempfile."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an Ansible inventory string for the specified set of keys.", "response": "def ansible_inventory(\n        self,\n        keys=['vm-type', 'groups', 'vm-provider'],\n    ):\n        \"\"\"\n        Get an Ansible inventory as a string, ``keys`` should be list on which\n        to group the hosts by. You can use any key defined in LagoInitFile.\n\n        Examples of possible `keys`:\n\n            `keys=['disks/0/metadata/arch']`, would group the hosts by the\n            architecture.\n\n            `keys=['/disks/0/metadata/distro', 'disks/0/metadata/arch']`,\n                would create groups by architecture and also by distro.\n\n            `keys=['groups']` - would group hosts by the groups defined for\n                each VM in the LagoInitFile, i.e.:\n\n                    domains:\n\n                        vm-01:\n                            ...\n                            groups: web-server\n                            ..\n                        vm-02:\n                            ..\n                            groups: db-server\n\n\n        Args:\n            keys (list of str): Path to the keys that will be used to\n                create groups.\n\n        Returns:\n            str: INI-like Ansible inventory\n        \"\"\"\n\n        lansible = LagoAnsible(self._prefix)\n        return lansible.get_inventory_str(keys=keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an exported object of the appropriate type.", "response": "def get_instance_by_type(dst, disk, do_compress, *args, **kwargs):\n        \"\"\"\n        Args:\n            dst (str): The path of the new exported disk.\n                can contain env variables.\n            disk (dict): Disk attributes\n                (of the disk that should be exported) as\n                found in workdir/current/virt/VM-NAME\n            do_compress(bool): If true, apply compression to the exported\n                disk.\n        Returns\n            An instance of a subclass of DiskExportManager which\n            matches the disk type.\n        \"\"\"\n        disk_type = disk['type']\n        cls = HANDLERS.get(disk_type)\n        if not cls:\n            raise utils.LagoUserException(\n                'Export is not supported for disk type {}'.format(disk_type)\n            )\n        return cls(dst, disk_type, disk, do_compress, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the checksum of the new exported disk write it to the file and update this managers exported_metadata.", "response": "def calc_sha(self, checksum):\n        \"\"\"\n        Calculate the checksum of the new exported disk, write it to\n        a file, and update this managers 'exported_metadata'.\n\n        Args:\n            checksum(str): The type of the checksum\n        \"\"\"\n        with LogTask('Calculating {}'.format(checksum)):\n            with open(self.dst + '.hash', 'wt') as f:\n                sha = utils.get_hash(self.dst, checksum)\n                f.write(sha)\n            self.exported_metadata[checksum] = sha"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompresses the new exported image", "response": "def compress(self):\n        \"\"\"\n        Compress the new exported image,\n        Block size was taken from virt-builder page\n        \"\"\"\n        if not self.do_compress:\n            return\n        with LogTask('Compressing disk'):\n            utils.compress(self.dst, 16777216)\n            os.unlink(self.dst)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rebase(self):\n        if self.standalone:\n            rebase_msg = 'Merging layered image with base'\n        else:\n            rebase_msg = 'Rebase'\n\n        with LogTask(rebase_msg):\n            if len(self.src_qemu_info) == 1:\n                # Base image (doesn't have predecessors)\n                return\n\n            if self.standalone:\n                # Consolidate the layers and base image\n                utils.qemu_rebase(target=self.dst, backing_file=\"\")\n            else:\n                if len(self.src_qemu_info) > 2:\n                    raise utils.LagoUserException(\n                        'Layered export is currently supported for one '\n                        'layer only.  You can try to use Standalone export.'\n                    )\n                # Put an identifier in the metadata of the copied layer,\n                # this identifier will be used later by Lago in order\n                # to resolve and download the base image\n                parent = self.src_qemu_info[0]['backing-filename']\n\n                # Hack for working with lago images naming convention\n                # For example: /var/lib/lago/store/phx_repo:el7.3-base:v1\n                # Extract only the image name and the version\n                # (in the example el7.3-base:v1)\n                parent = os.path.basename(parent)\n                try:\n                    parent = parent.split(':', 1)[1]\n                except IndexError:\n                    pass\n\n                parent = './{}'.format(parent)\n                utils.qemu_rebase(\n                    target=self.dst, backing_file=parent, safe=False\n                )", "response": "Change the backing - file entry of the exported disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self):\n        with LogTask('Exporting disk {} to {}'.format(self.name, self.dst)):\n            with utils.RollbackContext() as rollback:\n                rollback.prependDefer(\n                    shutil.rmtree, self.dst, ignore_errors=True\n                )\n                self.copy()\n                if not self.disk['format'] == 'iso':\n                    self.sparse()\n                self.calc_sha('sha1')\n                self.update_lago_metadata()\n                self.write_lago_metadata()\n                self.compress()\n                rollback.clear()", "response": "Export the disk to a new file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exported_disks_paths(self):\n        return [\n            os.path.join(self._dst, os.path.basename(disk['path']))\n            for disk in self._collect()\n        ]", "response": "Returns a list of the paths of the exported disks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_export_mgr(self):\n        return (\n            DiskExportManager.get_instance_by_type(\n                dst=self._dst,\n                disk=disk,\n                do_compress=self._compress,\n                *self._args,\n                **self._kwargs\n            ) for disk in self._collect()\n        )", "response": "Returns a DiskExportManager object for each disk in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export(self):\n        if self._with_threads:\n            utils.invoke_different_funcs_in_parallel(\n                *map(lambda mgr: mgr.export, self._get_export_mgr())\n            )\n        else:\n            for mgr in self._get_export_mgr():\n                mgr.export()\n\n        return self.exported_disks_paths()", "response": "Run the export process on the related resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes the options dict to a list and adds the prefix options to the list of options.", "response": "def normalize_options(options):\n        \"\"\"\n        Turns a mapping of 'option: arg' to a list and prefix the options.\n        arg can be a list of arguments.\n\n        for example:\n\n        dict = {\n            o1: a1,\n            o2: ,\n            o3: [a31, a32]\n            o4: []\n        }\n\n        will be transformed to:\n\n        [\n            prefix_option(o1), a1, prefix_option(o2),\n            prefix_option(o3), a31, prefix_option(o3), a32\n            prefix_option(o4)\n        ]\n\n        note that empty arguments are omitted\n\n        Args:\n            options (dict): A mapping between options and arguments\n\n        Returns:\n            lst: A normalized version of 'options' as mentioned above\n        \"\"\"\n        normalized_options = []\n\n        def _add(option, arg=None):\n            normalized_options.append(option)\n            arg and normalized_options.append(arg)\n\n        for option, arg in options.viewitems():\n            prefixed_option = Build.prefix_option(option)\n            if isinstance(arg, list) and arg:\n                for a in arg:\n                    _add(prefixed_option, a)\n            else:\n                _add(prefixed_option, arg)\n\n        return normalized_options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an instance of the base class from a build spec.", "response": "def get_instance_from_build_spec(cls, name, disk_path, build_spec, paths):\n        \"\"\"\n        Args:\n            name (str): The name of the vm this builder belongs\n            disk_path (str): The path to the disk that needs to be customized\n            paths (lago.paths.Paths): The paths of the current prefix\n            build_spec (dict): The build spec part, associated with the\n                disk located at disk_path, from the init file.\n\n        Returns:\n            An instance of Build with a normalized build spec i.e ready to\n                be invoked.\n        \"\"\"\n        instance = cls(name, disk_path, paths)\n        instance.normalize_build_spec(build_spec)\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_build_spec(self, build_spec):\n        for cmd in build_spec:\n            if not cmd:\n                continue\n            cmd_name = cmd.keys()[0]\n            cmd_options = cmd.values()[0]\n            cmd_handler = self.get_cmd_handler(cmd_name)\n            self.build_cmds.append(cmd_handler(cmd_options))", "response": "Normalizes a build spec into a list of Command tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cmd_handler(self, cmd):\n        cmd = cmd.replace('-', '_')\n        handler = getattr(self, cmd, None)\n        if not handler:\n            raise BuildException(\n                'Command {} is not supported as a '\n                'build command'.format(cmd)\n            )\n        return handler", "response": "Returns a callable that handles the given command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a command that can be used to customize the virtual machine.", "response": "def virt_customize(self, options):\n        \"\"\"\n        Handler for 'virt-customize'\n        note: if 'ssh-inject' option was specified without a path to a key,\n        the prefix' key will be copied to the vm.\n\n        Args:\n            options (lst of str): Options and arguments for 'virt-customize'\n\n        Returns:\n            callable: which handles cmd\n\n        Raises:\n            lago.build.BuildException: If an handler for cmd doesn't exist\n        \"\"\"\n        cmd = ['virt-customize', '-a', self.disk_path]\n        if 'ssh-inject' in options and not options['ssh-inject']:\n            options['ssh-inject'] = 'root:file:{}'.format(\n                self.paths.ssh_id_rsa_pub()\n            )\n\n        options = self.normalize_options(options)\n        cmd.extend(options)\n        return Command('virt-customize', cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(self):\n        if not self.build_cmds:\n            LOGGER.debug('No build commands were found, skipping build step')\n\n        with LogTask('Building {} disk {}'.format(self.name, self.disk_path)):\n            for command in self.build_cmds:\n                with LogTask('Running command {}'.format(command.name)):\n                    LOGGER.debug(command.cmd)\n                    result = utils.run_command(command.cmd)\n                    if result:\n                        raise BuildException(result.err)", "response": "Run all the commands in self. build_cmds and return the ID of the build process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_repo_by_name(name, repo_dir=None):\n    if repo_dir is None:\n        repo_dir = config.get('template_repos')\n\n    ret, out, _ = utils.run_command([\n        'find',\n        repo_dir,\n        '-name',\n        '*.json',\n    ], )\n\n    repos = [\n        TemplateRepository.from_url(line.strip()) for line in out.split('\\n')\n        if len(line.strip())\n    ]\n\n    for repo in repos:\n        if repo.name == name:\n            return repo\n    raise RuntimeError('Could not find repo %s' % name)", "response": "Searches the given repo name inside the repo_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying the image over the handl to the destination", "response": "def download_image(self, handle, dest):\n        \"\"\"\n        Copies over the handl to the destination\n\n        Args:\n            handle (str): path to copy over\n            dest (str): path to copy to\n\n        Returns:\n            None\n        \"\"\"\n        shutil.copyfile(self._prefixed(handle), dest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hash(self, handle):\n        handle = os.path.expanduser(os.path.expandvars(handle))\n        with open(self._prefixed('%s.hash' % handle)) as f:\n            return f.read()", "response": "Returns the associated hash for the given handle"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_metadata(self, handle):\n        handle = os.path.expanduser(os.path.expandvars(handle))\n        with open(self._prefixed('%s.metadata' % handle)) as f:\n            return json.load(f)", "response": "Returns the associated metadata info for the given handle"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_url(self, url, suffix='', dest=None):\n        if not url.endswith('.xz'):\n            try:\n                return self.open_url(\n                    url=url + '.xz',\n                    suffix=suffix,\n                    dest=dest,\n                )\n            except RuntimeError:\n                pass\n        full_url = posixpath.join(self.baseurl, url) + suffix\n        response = urllib.urlopen(full_url)\n        if response.code >= 300:\n            raise RuntimeError(\n                'Failed no retrieve URL %s:\\nCode: %d' %\n                (full_url, response.code)\n            )\n\n        meta = response.info()\n        file_size_kb = int(meta.getheaders(\"Content-Length\")[0]) / 1024\n        if file_size_kb > 0:\n            sys.stdout.write(\n                \"Downloading %s Kilobytes from %s \\n\" %\n                (file_size_kb, full_url)\n            )\n\n        def report(count, block_size, total_size):\n            percent = (count * block_size * 100 / float(total_size))\n            sys.stdout.write(\n                \"\\r% 3.1f%%\" % percent + \" complete (%d \" %\n                (count * block_size / 1024) + \"Kilobytes)\"\n            )\n            sys.stdout.flush()\n\n        if dest:\n            response.close()\n            urllib.urlretrieve(full_url, dest, report)\n            sys.stdout.write(\"\\n\")\n        return response", "response": "Opens the given url and returns a file object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_image(self, handle, dest):\n        with log_utils.LogTask('Download image %s' % handle, logger=LOGGER):\n            self.open_url(url=handle, dest=dest)\n\n        self.extract_image_xz(dest)", "response": "Downloads the image from the http server and extracts the xz file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hash(self, handle):\n        response = self.open_url(url=handle, suffix='.hash')\n        try:\n            return response.read()\n        finally:\n            response.close()", "response": "Get the associated hash for the given handle"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the associated metadata info for the given handle", "response": "def get_metadata(self, handle):\n        \"\"\"\n        Returns the associated metadata info for the given handle, the metadata\n        file must exist (``handle + '.metadata'``). If the given handle has an\n        ``.xz`` extension, it will get removed when calculating the handle\n        metadata path\n\n        Args:\n            handle (str): Path to the template to get the metadata from\n\n        Returns:\n            dict: Metadata for the given handle\n        \"\"\"\n        response = self.open_url(url=handle, suffix='.metadata')\n        try:\n            return json.load(response)\n        finally:\n            response.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_by_name(self, name):\n        try:\n            spec = self._dom.get('templates', {})[name]\n        except KeyError:\n            raise LagoMissingTemplateError(name, self._path)\n\n        return Template(\n            name=name,\n            versions={\n                ver_name: TemplateVersion(\n                    name='%s:%s:%s' % (self.name, name, ver_name),\n                    source=self._providers[ver_spec['source']],\n                    handle=ver_spec['handle'],\n                    timestamp=ver_spec['timestamp'],\n                )\n                for ver_name, ver_spec in spec['versions'].items()\n            },\n        )", "response": "Retrieve a template by its name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hash(self):\n        if self._hash is None:\n            self._hash = self._source.get_hash(self._handle).strip()\n        return self._hash", "response": "Returns the associated hash for this template version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_metadata(self):\n\n        if self._metadata is None:\n            self._metadata = self._source.get_metadata(self._handle)\n        return self._metadata", "response": "Returns the associated metadata info for this template version"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_path(self, temp_ver):\n        if temp_ver not in self:\n            raise RuntimeError(\n                'Template: {} not present'.format(temp_ver.name)\n            )\n        return self._prefixed(temp_ver.name)", "response": "Get the path to the given version in this store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading the given template version and store the metadata for the given one.", "response": "def download(self, temp_ver, store_metadata=True):\n        \"\"\"\n        Retrieve the given template version\n\n        Args:\n            temp_ver (TemplateVersion): template version to retrieve\n            store_metadata (bool): If set to ``False``, will not refresh the\n                local metadata with the retrieved one\n\n        Returns:\n            None\n        \"\"\"\n        dest = self._prefixed(temp_ver.name)\n        temp_dest = '%s.tmp' % dest\n\n        with utils.LockFile(dest + '.lock'):\n            # Image was downloaded while we were waiting\n            if os.path.exists(dest):\n                return\n\n            temp_ver.download(temp_dest)\n            if store_metadata:\n                with open('%s.metadata' % dest, 'w') as f:\n                    utils.json_dump(temp_ver.get_metadata(), f)\n\n            sha1 = utils.get_hash(temp_dest)\n            if temp_ver.get_hash() != sha1:\n                raise RuntimeError(\n                    'Image %s does not match the expected hash %s' % (\n                        temp_ver.name,\n                        sha1,\n                    )\n                )\n\n            with open('%s.hash' % dest, 'w') as f:\n                f.write(sha1)\n\n            with log_utils.LogTask('Convert image', logger=LOGGER):\n                result = utils.run_command(\n                    [\n                        'qemu-img',\n                        'convert',\n                        '-O',\n                        'raw',\n                        temp_dest,\n                        dest,\n                    ],\n                )\n\n                os.unlink(temp_dest)\n                if result:\n                    raise RuntimeError(result.err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stored_metadata(self, temp_ver):\n        with open(self._prefixed('%s.metadata' % temp_ver.name)) as f:\n            return json.load(f)", "response": "Retrieves the metadata for the given template version from the store\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the hash for the given template version from the store", "response": "def get_stored_hash(self, temp_ver):\n        \"\"\"\n        Retrieves the hash for the given template version from the store\n\n        Args:\n            temp_ver (TemplateVersion): template version to retrieve the hash\n                for\n\n        Returns:\n            str: hash of the given template version\n        \"\"\"\n        with open(self._prefixed('%s.hash' % temp_ver.name)) as f:\n            return f.read().strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_inventory_str(self, keys=None):\n        inventory = self.get_inventory(keys)\n        lines = []\n        for name, hosts in inventory.viewitems():\n            lines.append('[{name}]'.format(name=name))\n            for host in sorted(hosts):\n                lines.append(host)\n\n        return '\\n'.join(lines)", "response": "Convert a dict generated by ansible. LagoAnsible. get_inventory\n        to an INI - like file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_inventory(self, keys=None):\n        inventory = defaultdict(list)\n        keys = keys or ['vm-type', 'groups', 'vm-provider']\n        vms = self.prefix.get_vms().values()\n\n        for vm in vms:\n            entry = self._generate_entry(vm)\n            vm_spec = vm.spec\n            for key in keys:\n                value = self.get_key(key, vm_spec)\n                if value is None:\n                    continue\n                if isinstance(value, list):\n                    for sub_value in value:\n                        inventory['{}={}'.format(key, sub_value)].append(entry)\n                else:\n                    inventory['{}={}'.format(key, value)].append(entry)\n\n            # Adding a special case for the group key.\n            # Most of the times the user wants that the host\n            # will be a part of the group \"group\", and not a part of\n            # \"group=something\"\n            for group in vm_spec.get('groups', []):\n                inventory[group].append(entry)\n\n        return inventory", "response": "Create an Ansible inventory based on python dicts and lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate entry for the given VM", "response": "def _generate_entry(self, vm):\n        \"\"\"\n        Generate host entry for the given VM\n        Args:\n            vm (lago.plugins.vm.VMPlugin): The VM for which the entry\n                should be created for.\n\n        Returns:\n            str: An entry for vm\n        \"\"\"\n        return \\\n            '{name} ' \\\n            'ansible_host={ip} ' \\\n            'ansible_ssh_private_key_file={key}'.format(\n                name=vm.name(),\n                ip=vm.ip(),\n                key=self.prefix.paths.ssh_id_rsa()\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_key(key, data_structure):\n        if key == '/':\n            return data_structure\n\n        path = key.split('/')\n        # If the path start with '/', remove the empty string from the list\n        path[0] or path.pop(0)\n        current_value = data_structure\n        while path:\n            current_key = path.pop(0)\n            try:\n                current_key = int(current_key)\n            except ValueError:\n                pass\n\n            try:\n                current_value = current_value[current_key]\n            except (KeyError, IndexError):\n                LOGGER.debug('failed to extract path {}'.format(key))\n                return None\n\n        return current_value", "response": "Helper method for extracting values from a nested data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncatches SIGTERM and SIGHUP and call sys. exit which raises SystemExit exception.", "response": "def exit_handler(signum, frame):\n    \"\"\"\n    Catch SIGTERM and SIGHUP and call \"sys.exit\" which raises\n    \"SystemExit\" exception.\n    This will trigger all the cleanup code defined in ContextManagers\n    and \"finally\" statements.\n\n    For more details about the arguments see \"signal\" documentation.\n\n    Args:\n        signum(int): The signal's number\n        frame(frame): The current stack frame, can be None\n    \"\"\"\n\n    LOGGER.debug('signal {} was caught'.format(signum))\n    sys.exit(128 + signum)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_changelog(repo_path, from_commit=None):\n    repo = dulwich.repo.Repo(repo_path)\n    tags = get_tags(repo)\n    refs = get_refs(repo)\n    changelog = []\n    maj_version = 0\n    feat_version = 0\n    fix_version = 0\n    start_including = False\n\n    cur_line = ''\n    if from_commit is None:\n        start_including = True\n\n    for commit_sha, children in reversed(\n        get_children_per_first_parent(repo_path).items()\n    ):\n        commit = repo.get_object(commit_sha)\n        maj_version, feat_version, fix_version = get_version(\n            commit=commit,\n            tags=tags,\n            maj_version=maj_version,\n            feat_version=feat_version,\n            fix_version=fix_version,\n        )\n        version = '%s.%s.%s' % (maj_version, feat_version, fix_version)\n\n        if (\n            start_including or commit_sha.startswith(from_commit)\n            or fuzzy_matches_refs(from_commit, refs.get(commit_sha, []))\n        ):\n            cur_line = pretty_commit(\n                commit,\n                version,\n            )\n            for child in children:\n                cur_line += pretty_commit(repo.get_object(child), version=None)\n            start_including = True\n            changelog.append(cur_line)\n\n    return '\\n'.join(reversed(changelog))", "response": "Returns a string representation of the changelog from a git repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a repo will return the current version of the object", "response": "def get_current_version(repo_path):\n    \"\"\"\n    Given a repo will return the version string, according to semantic\n    versioning, counting as non-backwards compatible commit any one with a\n    message header that matches (case insensitive)::\n\n        sem-ver: .*break.*\n\n    And as features any commit with a header matching::\n\n        sem-ver: feature\n\n    And counting any other as a bugfix\n    \"\"\"\n    repo = dulwich.repo.Repo(repo_path)\n    tags = get_tags(repo)\n    maj_version = 0\n    feat_version = 0\n    fix_version = 0\n\n    for commit_sha in reversed(get_first_parents(repo_path)):\n        commit = repo.get_object(commit_sha)\n        maj_version, feat_version, fix_version = get_version(\n            commit=commit,\n            tags=tags,\n            maj_version=maj_version,\n            feat_version=feat_version,\n            fix_version=fix_version,\n        )\n\n    return '%s.%s.%s' % (maj_version, feat_version, fix_version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self, attempts=5, timeout=2):\n\n        if not self.alive():\n            with LogTask('Create network %s' % self.name()):\n                net = self.libvirt_con.networkCreateXML(self._libvirt_xml())\n                if net is None:\n                    raise RuntimeError(\n                        'failed to create network, XML: %s' %\n                        (self._libvirt_xml())\n                    )\n                for _ in range(attempts):\n                    if net.isActive():\n                        return\n                    LOGGER.debug(\n                        'waiting for network %s to become active', net.name()\n                    )\n                    time.sleep(timeout)\n                raise RuntimeError(\n                    'failed to verify network %s is active' % net.name()\n                )", "response": "Start the network, will check if the network is active ``attempts``\n        times, waiting ``timeout`` between each attempt.\n\n        Args:\n            attempts (int): number of attempts to check the network is active\n            timeout  (int): timeout for each attempt\n\n        Returns:\n            None\n\n        Raises:\n            RuntimeError: if network creation failed, or failed to verify it is\n            active."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_cpu_xml(self):\n        if self.cpu_custom:\n            return self.generate_custom(\n                cpu=self.cpu,\n                vcpu_num=self.vcpu_num,\n                fill_topology=self.vcpu_set\n            )\n        elif self.cpu_model:\n            return self.generate_exact(\n                self.cpu_model, vcpu_num=self.vcpu_num, host_cpu=self.host_cpu\n            )\n        else:\n            return self.generate_host_passthrough(self.vcpu_num)", "response": "Get CPU XML\n\n        Returns:\n            lxml.etree.Element: cpu node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_host_passthrough(self, vcpu_num):\n\n        cpu = ET.Element('cpu', mode='host-passthrough')\n        cpu.append(self.generate_topology(vcpu_num))\n        if vcpu_num > 1:\n            cpu.append(self.generate_numa(vcpu_num))\n        return cpu", "response": "Generate host - passthrough XML cpu node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating custom CPU model. This method attempts to convert the dict to XML, as defined by ``xmltodict.unparse`` method. Args: cpu(dict): CPU spec vcpu_num(int): number of virtual cpus fill_topology(bool): if topology is not defined in ``cpu`` and ``vcpu`` was not set, will add CPU topology to the generated CPU. Returns: lxml.etree.Element: CPU XML node Raises: :exc:`~LagoInitException`: when failed to convert dict to XML", "response": "def generate_custom(self, cpu, vcpu_num, fill_topology):\n        \"\"\"\n        Generate custom CPU model. This method attempts to convert the dict to\n        XML, as defined by ``xmltodict.unparse`` method.\n\n        Args:\n            cpu(dict): CPU spec\n            vcpu_num(int): number of virtual cpus\n            fill_topology(bool): if topology is not defined in ``cpu`` and\n                ``vcpu`` was not set, will add CPU topology to the generated\n                CPU.\n\n        Returns:\n            lxml.etree.Element: CPU XML node\n\n        Raises:\n            :exc:`~LagoInitException`: when failed to convert dict to XML\n        \"\"\"\n\n        try:\n            cpu = utils.dict_to_xml({'cpu': cpu})\n        except:\n            # TO-DO: print an example here\n            raise LagoInitException('conversion of \\'cpu\\' to XML failed')\n\n        if not cpu.xpath('topology') and fill_topology:\n            cpu.append(self.generate_topology(vcpu_num))\n        return cpu"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_exact(self, model, vcpu_num, host_cpu):\n\n        nested = {'Intel': 'vmx', 'AMD': 'svm'}\n        cpu = ET.Element('cpu', match='exact')\n        ET.SubElement(cpu, 'model').text = model\n        cpu.append(self.generate_topology(vcpu_num))\n\n        vendor = host_cpu.findtext('vendor')\n        if not nested.get(vendor):\n            LOGGER.debug(\n                'Unknown vendor: {0}, did not configure nested '\n                'virtualization cpu flag on guest.'.format(vendor)\n            )\n            return cpu\n\n        model_vendor = LibvirtCPU.get_cpu_vendor(family=model)\n        if vendor != model_vendor:\n            LOGGER.debug(\n                (\n                    'Not enabling nested virtualization feature, host '\n                    'vendor is: {0}, guest vendor: '\n                    '{1}'.format(vendor, model_vendor)\n                )\n            )\n            return cpu\n\n        flag = nested[vendor]\n        if host_cpu.find('feature/[@name=\"{0}\"]'.format(flag)) is not None:\n            cpu.append(self.generate_feature(name=flag))\n        else:\n            LOGGER.debug(\n                (\n                    'missing {0} cpu flag on host, nested '\n                    'virtualization will probably not '\n                    'work.'\n                ).format(flag)\n            )\n\n        return cpu", "response": "Generate exact CPU model with nested virtualization CPU feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_topology(self, vcpu_num, cores=1, threads=1):\n\n        return ET.Element(\n            'topology',\n            sockets=str(vcpu_num),\n            cores=str(cores),\n            threads=str(threads),\n        )", "response": "Generate CPU <topology > XML child"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate guest CPU <numa > XML child element.", "response": "def generate_numa(self, vcpu_num):\n        \"\"\"\n        Generate guest CPU <numa> XML child\n        Configures 1, 2 or 4 vCPUs per cell.\n\n        Args:\n            vcpu_num(str): number of virtual CPUs\n\n        Returns:\n            lxml.etree.Element: numa XML element\n        \"\"\"\n\n        if int(vcpu_num) == 2:\n            # 2 vCPUs is a special case.\n            # We wish to have 2 cells,\n            # with 1 vCPU in each.\n            # This is also the common case.\n            total_cells = 2\n            cpus_per_cell = 1\n        elif int(vcpu_num) == 4:\n            # 4 vCPU is a special case.\n            # We wish to have 2 cells,\n            # with 2 vCPUs in each.\n            total_cells = 2\n            cpus_per_cell = 2\n        else:\n            cell_info = divmod(int(vcpu_num), 4)\n            if cell_info[1] == 0:\n                # 4 vCPUs in each cell\n                total_cells = cell_info[0]\n                cpus_per_cell = 4\n            elif cell_info[1] == 2:\n                # 2 vCPUs in each cell\n                total_cells = (cell_info[0] * 2) + 1\n                cpus_per_cell = 2\n            else:\n                # 1 vCPU per cell...\n                total_cells = int(vcpu_num)\n                cpus_per_cell = 1\n\n        numa = ET.Element('numa')\n        memory_per_cell = divmod(int(self.memory), total_cells)\n        LOGGER.debug(\n            'numa\\n: cpus_per_cell: {0}, total_cells: {1}'.format(\n                cpus_per_cell, total_cells\n            )\n        )\n        for cell_id in xrange(0, total_cells):\n            first_cpu_in_cell = cell_id * cpus_per_cell\n            if cpus_per_cell == 1:\n                cpus_in_cell = str(first_cpu_in_cell)\n            else:\n                cpus_in_cell = '{0}-{1}'.format(\n                    first_cpu_in_cell, first_cpu_in_cell + cpus_per_cell - 1\n                )\n            cell = ET.Element(\n                'cell',\n                id=str(cell_id),\n                cpus=cpus_in_cell,\n                memory=str(memory_per_cell[0]),\n                unit='MiB',\n            )\n            numa.append(cell)\n\n        LOGGER.debug('numa:\\n{}'.format(ET.tostring(numa, pretty_print=True)))\n        return numa"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating <vcpu > domain XML child", "response": "def generate_vcpu(self, vcpu_num):\n        \"\"\"\n        Generate <vcpu> domain XML child\n\n        Args:\n            vcpu_num(str): number of virtual cpus\n\n        Returns:\n            lxml.etree.Element: vcpu XML element\n        \"\"\"\n\n        vcpu = ET.Element('vcpu')\n        vcpu.text = str(vcpu_num)\n        return vcpu"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_feature(self, name, policy='require'):\n\n        return ET.Element('feature', policy=policy, name=name)", "response": "Generate CPU feature element"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget CPU vendor from properties of the node.", "response": "def get_cpu_vendor(cls, family, arch='x86'):\n        \"\"\"\n        Get CPU vendor, if vendor is not available will return 'generic'\n\n        Args:\n            family(str): CPU family\n            arch(str): CPU arch\n\n        Returns:\n            str: CPU vendor if found otherwise 'generic'\n        \"\"\"\n\n        props = cls.get_cpu_props(family, arch)\n        vendor = 'generic'\n        try:\n            vendor = props.xpath('vendor/@name')[0]\n        except IndexError:\n            pass\n        return vendor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cpu_props(cls, family, arch='x86'):\n\n        cpus = cls.get_cpus_by_arch(arch)\n        try:\n            return cpus.xpath('model[@name=\"{0}\"]'.format(family))[0]\n        except IndexError:\n            raise LagoException('No such CPU family: {0}'.format(family))", "response": "Get CPU info XML\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cpus_by_arch(cls, arch):\n\n        with open('/usr/share/libvirt/cpu_map.xml', 'r') as cpu_map:\n            cpu_xml = ET.parse(cpu_map)\n        try:\n            return cpu_xml.xpath('/cpus/arch[@name=\"{0}\"]'.format(arch))[0]\n        except IndexError:\n            raise LagoException('No such arch: {0}'.format(arch))", "response": "Get all CPUs info by arch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a log task in the base log tree.", "response": "def start_log_task(task, logger=logging, level='info'):\n    \"\"\"\n    Starts a log task\n\n    Args:\n        task (str): name of the log task to start\n        logger (logging.Logger): logger to use\n        level (str): log level to use\n\n    Returns:\n        None\n    \"\"\"\n    getattr(logger, level)(START_TASK_TRIGGER_MSG % task)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nend a log task in the", "response": "def end_log_task(task, logger=logging, level='info'):\n    \"\"\"\n    Ends a log task\n\n    Args:\n        task (str): name of the log task to end\n        logger (logging.Logger): logger to use\n        level (str): log level to use\n\n    Returns:\n        None\n    \"\"\"\n    getattr(logger, level)(END_TASK_TRIGGER_MSG % task)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhides the logs of stevedore", "response": "def hide_stevedore_logs():\n    \"\"\"\n    Hides the logs of stevedore, this function was\n    added in order to support older versions of stevedore\n\n    We are using the NullHandler in order to get rid from\n    'No handlers could be found for logger...' msg\n\n    Returns:\n        None\n    \"\"\"\n    stevedore_logger = logging.getLogger('stevedore.extension')\n    stevedore_logger.propagate = False\n    stevedore_logger.setLevel(logging.ERROR)\n    stevedore_logger.addHandler(logging.NullHandler())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_prefix_logging(logdir):\n    if not os.path.exists(logdir):\n        os.mkdir(logdir)\n\n    file_handler = logging.FileHandler(\n        filename=os.path.join(logdir, 'lago.log'),\n    )\n    file_formatter = get_default_log_formatter()\n    file_handler.setFormatter(file_formatter)\n    logging.root.addHandler(file_handler)\n    hide_paramiko_logs()\n    hide_stevedore_logs()", "response": "Setup a file logger that will create a log in the given logdir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef colored(cls, color, message):\n        return getattr(cls, color.upper()) + message + cls.DEFAULT", "response": "Small function to wrap a string around a color"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(self, record):\n        level = record.levelno\n\n        if level >= logging.CRITICAL:\n            color = self.CRITICAL\n        elif level >= logging.ERROR:\n            color = self.ERROR\n        elif level >= logging.WARNING:\n            color = self.WARNING\n        elif level >= logging.INFO:\n            color = self.INFO\n        elif level >= logging.DEBUG:\n            color = self.DEBUG\n        else:\n            color = self.DEFAULT\n\n        message = super().format(record)\n        if record.args:\n            try:\n                message = message % record.args\n            except TypeError:\n                # this happens when the message itself has some %s symbols, as\n                # in traces for tracedumps\n                pass\n\n        return color + message + self.DEFAULT", "response": "Adds colors to a log record and formats it with the default value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of tasks for the given thread.", "response": "def get_tasks(self, thread_name):\n        \"\"\"\n        Args:\n            thread_name (str): name of the thread to get the tasks for\n\n        Returns:\n            OrderedDict of str, Task: list of task names and log records for\n                each for the given thread\n        \"\"\"\n        if thread_name not in self.tasks_by_thread:\n            with self._tasks_lock:\n                self.tasks_by_thread[thread_name] = OrderedDict()\n\n        return self.tasks_by_thread[thread_name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cur_depth_level(self):\n        cur_level = self.initial_depth\n        if not self.am_i_main_thread:\n            cur_level += len(self.get_tasks(thread_name='MainThread'))\n\n        return cur_level + len(self.tasks)", "response": "Returns the depth level of the current task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef should_show_by_depth(self, cur_level=None):\n        if cur_level is None:\n            cur_level = self.cur_depth_level\n\n        return (self.task_tree_depth < 0 or self.task_tree_depth >= cur_level)", "response": "Returns True if the given depth level should show messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the given log record should be shown according to the base_level.", "response": "def should_show_by_level(self, record_level, base_level=None):\n        \"\"\"\n        Args:\n            record_level (int): log level of the record to check\n            base_level (int or None): log level to check against, will use the\n                object's :class:`dump_level` if None is passed\n\n        Returns:\n            bool: True if the given log record should be shown according to the\n                log level\n        \"\"\"\n        if base_level is None:\n            base_level = self.dump_level\n\n        return record_level >= base_level"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles a new task.", "response": "def handle_new_task(self, task_name, record):\n        \"\"\"\n        Do everything needed when a task is starting\n\n        Params:\n            task_name (str): name of the task that is starting\n            record (logging.LogRecord): log record with all the info\n\n        Returns:\n            None\n        \"\"\"\n        record.msg = ColorFormatter.colored('default', START_TASK_MSG)\n        record.task = task_name\n\n        self.tasks[task_name] = Task(name=task_name, maxlen=self.buffer_size)\n        if self.should_show_by_depth():\n            self.pretty_emit(record, is_header=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mark_parent_tasks_as_failed(self, task_name, flush_logs=False):\n        for existing_task_name in self.tasks:\n            if existing_task_name == task_name:\n                break\n\n            if flush_logs:\n                self.tasks[existing_task_name].clear()\n\n            self.tasks[existing_task_name].failed = True\n\n        self.mark_main_tasks_as_failed()", "response": "Mark all the parent tasks as failed and clear the logs form\n                parent tasks\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses all the children tasks that were open by the given parent task.", "response": "def close_children_tasks(self, parent_task_name):\n        \"\"\"\n        Closes all the children tasks that were open\n\n        Args:\n            parent_task_name (str): Name of the parent task\n\n        Returns:\n            None\n        \"\"\"\n        if parent_task_name not in self.tasks:\n            return\n\n        while self.tasks:\n            next_task = reversed(self.tasks.keys()).next()\n            if next_task == parent_task_name:\n                break\n            del self.tasks[next_task]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles a closed task.", "response": "def handle_closed_task(self, task_name, record):\n        \"\"\"\n        Do everything needed when a task is closed\n\n        Params:\n            task_name (str): name of the task that is finishing\n            record (logging.LogRecord): log record with all the info\n\n        Returns:\n            None\n        \"\"\"\n        if task_name not in self.tasks:\n            return\n\n        if self.main_failed:\n            self.mark_parent_tasks_as_failed(self.cur_task)\n\n        if self.tasks[task_name].failed:\n            record.msg = ColorFormatter.colored('red', END_TASK_ON_ERROR_MSG)\n        else:\n            record.msg = ColorFormatter.colored('green', END_TASK_MSG)\n\n        record.msg += ' (in %s)' % self.tasks[task_name].elapsed_time()\n\n        if self.should_show_by_depth() or self.tasks[task_name].force_show:\n            if self.tasks[task_name].force_show:\n                self.handle_error()\n\n            self.pretty_emit(record, is_header=True)\n\n        self.close_children_tasks(task_name)\n        self.tasks.pop(task_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_error(self):\n        if not self.tasks:\n            return\n\n        # All the parents inherit the failure\n        self.mark_parent_tasks_as_failed(\n            self.cur_task,\n            flush_logs=True,\n        )\n\n        # Show the start headers for all the parent tasks if they were not\n        # shown by the depth level limit\n        for index, task in enumerate(self.tasks.values()):\n            if self.should_show_by_depth(index + 1):\n                continue\n\n            start_task_header = logging.LogRecord(\n                '', logging.INFO, '', 0, '', [], None\n            )\n            start_task_header.msg = ColorFormatter.colored(\n                'default',\n                START_TASK_MSG,\n            )\n            start_task_header.task = task.name\n            self.pretty_emit(\n                start_task_header,\n                is_header=True,\n                task_level=index + 1,\n            )\n\n        # Show now all the cached logs for the current task\n        for old_record in self.tasks[self.cur_task]:\n            self.pretty_emit(old_record)\n\n        self.tasks[self.cur_task].clear()", "response": "Handles an error log record that should be shown by the user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_task_indicator(self, task_level=None):\n        if task_level is None:\n            task_level = len(self.tasks)\n        return self.TASK_INDICATORS[task_level % len(self.TASK_INDICATORS)]", "response": "Returns the task indicator for the specified task level"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps around the logging. StreamHandler. emit method to add the message to the nested cache.", "response": "def pretty_emit(self, record, is_header=False, task_level=None):\n        \"\"\"\n        Wrapper around the :class:`logging.StreamHandler` emit method to add\n        some decoration stuff to the message\n\n        Args:\n            record (logging.LogRecord): log record to emit\n            is_header (bool): if this record is a header, usually, a start or\n                end task message\n            task_level (int): If passed, will take that as the current nested\n                task level instead of calculating it from the current tasks\n\n        Returns:\n            None\n        \"\"\"\n        task = record.task or self.cur_task\n        if task_level is None:\n            task_level = self.cur_depth_level\n\n        if is_header:\n            extra_prefix = (\n                self.get_task_indicator(task_level - 1) + ' ' +\n                ('' if self.am_i_main_thread else '[%s] ' % self.cur_thread) +\n                task + ': '\n            )\n            record.levelno = logging.INFO\n        else:\n            extra_prefix = '  ' + self.get_task_indicator(task_level) + ' '\n\n        if task:\n            record.msg = (\n                '  ' * (task_level - 1) + extra_prefix + str(record.msg)\n            )\n\n        super().emit(record)\n        super().flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef emit(self, record):\n        record.task = self.cur_task\n\n        if record.levelno >= self.dump_level and self.cur_task:\n            self.tasks[self.cur_task].failed = True\n            self.tasks[self.cur_task].force_show = True\n\n        # Makes no sense to start a task with an error log\n        is_start = START_TASK_REG.match(str(record.msg))\n        if is_start:\n            self.handle_new_task(is_start.groupdict()['task_name'], record)\n            return\n\n        is_end = END_TASK_REG.match(str(record.msg))\n        if is_end:\n            self.handle_closed_task(is_end.groupdict()['task_name'], record)\n            return\n\n        force_show_record = ALWAYS_SHOW_REG.match(str(record.msg))\n        if force_show_record:\n            record.msg = force_show_record.groupdict()['message']\n            self.pretty_emit(record)\n\n        if (\n            not force_show_record and self.should_show_by_level(record)\n            and self.should_show_by_depth()\n        ):\n            self.pretty_emit(record)\n            return\n\n        if self.cur_task:\n            self.tasks[self.cur_task].append(record)", "response": "Handles the given record and returns the entry point from the python logging facility."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that the directory used by this store exist and that the lease directory exists.", "response": "def _validate_lease_dir(self):\n        \"\"\"\n        Validate that the directory used by this store exist,\n            otherwise create it.\n        \"\"\"\n        try:\n            if not os.path.isdir(self.path):\n                os.makedirs(self.path)\n        except OSError as e:\n            raise_from(\n                LagoSubnetLeaseBadPermissionsException(self.path, e.strerror),\n                e\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acquire(self, uuid_path, subnet=None):\n        try:\n            with self._create_lock():\n                if subnet:\n                    LOGGER.debug('Trying to acquire subnet {}'.format(subnet))\n                    acquired_subnet = self._acquire_given_subnet(\n                        uuid_path, subnet\n                    )\n                else:\n                    LOGGER.debug('Trying to acquire a free subnet')\n                    acquired_subnet = self._acquire(uuid_path)\n\n                return acquired_subnet\n        except (utils.TimerException, IOError):\n            raise LagoSubnetLeaseLockException(self.path)", "response": "Attempts to acquire a free subnet for the given uuid path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to acquire a free network for the given uuid path.", "response": "def _acquire(self, uuid_path):\n        \"\"\"\n        Lease a free network for the given uuid path\n\n        Args:\n            uuid_path (str): Path to the uuid file of a :class:`lago.Prefix`\n\n        Returns:\n            netaddr.IPNetwork: Which represents the selected subnet\n\n        Raises:\n            LagoSubnetLeaseException: If the store is full\n        \"\"\"\n        for index in range(self._min_third_octet, self._max_third_octet + 1):\n            lease = self.create_lease_object_from_idx(index)\n            if self._lease_valid(lease):\n                continue\n            self._take_lease(lease, uuid_path, safe=False)\n            return lease.to_ip_network()\n\n        raise LagoSubnetLeaseStoreFullException(self.get_allowed_range())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to create a lease for a given subnet and return the IPNetwork object.", "response": "def _acquire_given_subnet(self, uuid_path, subnet):\n        \"\"\"\n        Try to create a lease for subnet\n\n        Args:\n            uuid_path (str): Path to the uuid file of a :class:`lago.Prefix`\n            subnet (str): dotted ipv4 subnet\n                (for example ```192.168.200.0```)\n\n        Returns:\n            netaddr.IPNetwork: Which represents the selected subnet\n\n        Raises:\n            LagoSubnetLeaseException: If the requested subnet is not in the\n                range of this store or its already been taken\n        \"\"\"\n        lease = self.create_lease_object_from_subnet(subnet)\n        self._take_lease(lease, uuid_path)\n\n        return lease.to_ip_network()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lease_valid(self, lease):\n        if not lease.exist:\n            return None\n\n        if lease.has_env:\n            return lease.uuid_path\n        else:\n            self._release(lease)\n            return None", "response": "Checks if the given lease exists and has a prefix that owns it. If it does return the path to the uuid of the prefix. If the given lease does not exist return None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a given lease and saves it to the store and make the prefix in uuid_path his owner.", "response": "def _take_lease(self, lease, uuid_path, safe=True):\n        \"\"\"\n        Persist the given lease to the store and make the prefix in uuid_path\n        his owner\n\n        Args:\n            lease(lago.subnet_lease.Lease): Object representation of the lease\n            uuid_path (str): Path to the prefix uuid\n            safe (bool): If true (the default), validate the the lease\n                isn't taken.\n\n        Raises:\n            LagoSubnetLeaseException: If safe == True and the lease is already\n                taken.\n        \"\"\"\n        if safe:\n            lease_taken_by = self._lease_valid(lease)\n            if lease_taken_by and lease_taken_by != uuid_path:\n                raise LagoSubnetLeaseTakenException(\n                    lease.subnet, lease_taken_by\n                )\n\n        with open(uuid_path) as f:\n            uuid = f.read()\n        with open(lease.path, 'wt') as f:\n            utils.json_dump((uuid_path, uuid), f)\n\n        LOGGER.debug(\n            'Assigned subnet lease {} to {}'.format(lease.path, uuid_path)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the leases in the current subnet.", "response": "def list_leases(self, uuid=None):\n        \"\"\"\n        List current subnet leases\n\n        Args:\n            uuid(str): Filter the leases by uuid\n\n        Returns:\n            list of :class:~Lease: current leases\n        \"\"\"\n\n        try:\n            lease_files = os.listdir(self.path)\n        except OSError as e:\n            raise_from(\n                LagoSubnetLeaseBadPermissionsException(self.path, e.strerror),\n                e\n            )\n\n        leases = [\n            self.create_lease_object_from_idx(lease_file.split('.')[0])\n            for lease_file in lease_files if lease_file != LOCK_NAME\n        ]\n        if not uuid:\n            return leases\n        else:\n            return [lease for lease in leases if lease.uuid == uuid]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef release(self, subnets):\n\n        if isinstance(subnets, str) or isinstance(subnets, IPNetwork):\n            subnets = [subnets]\n        subnets_iter = (\n            str(subnet) if isinstance(subnet, IPNetwork) else subnet\n            for subnet in subnets\n        )\n        try:\n            with self._create_lock():\n                for subnet in subnets_iter:\n                    self._release(self.create_lease_object_from_subnet(subnet))\n        except (utils.TimerException, IOError):\n            raise LagoSubnetLeaseLockException(self.path)", "response": "Free the lease of the given subnets."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfrees the given lease.", "response": "def _release(self, lease):\n        \"\"\"\n        Free the given lease\n\n        Args:\n            lease (lago.subnet_lease.Lease): The lease to free\n        \"\"\"\n        if lease.exist:\n            os.unlink(lease.path)\n            LOGGER.debug('Removed subnet lease {}'.format(lease.path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lease_owned(self, lease, current_uuid_path):\n\n        prev_uuid_path, prev_uuid = lease.metadata\n\n        with open(current_uuid_path) as f:\n            current_uuid = f.read()\n\n        return \\\n            current_uuid_path == prev_uuid_path and \\\n            prev_uuid == current_uuid", "response": "Checks if the given lease is owned by the given prefix whose uuid is in\n            the given path\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a Lease object from a subnet.", "response": "def create_lease_object_from_subnet(self, subnet):\n        \"\"\"\n        Create a lease from ip in a dotted decimal format,\n        (for example `192.168.200.0/24`). the _cidr will be added if not exist\n        in `subnet`.\n\n        Args:\n            subnet (str): The value of the third octet\n\n        Returns:\n            Lease: Lease object which represents the requested subnet.\n\n        Raises:\n            LagoSubnetLeaseOutOfRangeException: If the resultant subnet is\n            malformed or out of the range of the store.\n        \"\"\"\n        if '/' not in subnet:\n            subnet = '{}/{}'.format(subnet, self._cidr)\n\n        try:\n            if not self.is_leasable_subnet(subnet):\n                raise LagoSubnetLeaseOutOfRangeException(\n                    subnet, self.get_allowed_range()\n                )\n        except AddrFormatError:\n            raise LagoSubnetLeaseMalformedAddrException(subnet)\n\n        return Lease(store_path=self.path, subnet=subnet)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a command in a single process and returns the output of the command.", "response": "def _run_command(\n    command,\n    input_data=None,\n    stdin=None,\n    out_pipe=subprocess.PIPE,\n    err_pipe=subprocess.PIPE,\n    env=None,\n    uuid=None,\n    **kwargs\n):\n    \"\"\"\n    Runs a command\n\n    Args:\n        command(list of str): args of the command to execute, including the\n            command itself as command[0] as `['ls', '-l']`\n        input_data(str): If passed, will feed that data to the subprocess\n            through stdin\n        out_pipe(int or file): File descriptor as passed to\n            :ref:subprocess.Popen to use as stdout\n        stdin(int or file): File descriptor as passed to\n            :ref:subprocess.Popen to use as stdin\n        err_pipe(int or file): File descriptor as passed to\n            :ref:subprocess.Popen to use as stderr\n        env(dict of str:str): If set, will use the given dict as env for the\n            subprocess\n        uuid(uuid): If set the command will be logged with the given uuid\n            converted to string, otherwise, a uuid v4 will be generated.\n        **kwargs: Any other keyword args passed will be passed to the\n            :ref:subprocess.Popen call\n\n    Returns:\n        lago.utils.CommandStatus: result of the interactive execution\n    \"\"\"\n\n    # add libexec to PATH if needed\n    if uuid is None:\n        uuid = uuid_m.uuid4()\n\n    if constants.LIBEXEC_DIR not in os.environ['PATH'].split(':'):\n        os.environ['PATH'\n                   ] = '%s:%s' % (constants.LIBEXEC_DIR, os.environ['PATH'])\n\n    if input_data and not stdin:\n        kwargs['stdin'] = subprocess.PIPE\n    elif stdin:\n        kwargs['stdin'] = stdin\n\n    if env is None:\n        env = os.environ.copy()\n    else:\n        env['PATH'] = ':'.join(\n            list(\n                set(\n                    env.get('PATH', '').split(':') + os.environ['PATH']\n                    .split(':')\n                ),\n            ),\n        )\n\n    popen = subprocess.Popen(\n        ' '.join('\"%s\"' % arg for arg in command),\n        stdout=out_pipe,\n        stderr=err_pipe,\n        shell=True,\n        env=env,\n        **kwargs\n    )\n    out, err = popen.communicate(input_data)\n    LOGGER.debug(\n        '%s: command exit with return code: %d', str(uuid), popen.returncode\n    )\n    if out:\n        LOGGER.debug('%s: command stdout: %s', str(uuid), out)\n    if err:\n        LOGGER.debug('%s: command stderr: %s', str(uuid), err)\n    return CommandStatus(popen.returncode, out, err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_command(\n    command,\n    input_data=None,\n    out_pipe=subprocess.PIPE,\n    err_pipe=subprocess.PIPE,\n    env=None,\n    **kwargs\n):\n    \"\"\"\n    Runs a command non-interactively\n\n    Args:\n        command(list of str): args of the command to execute, including the\n            command itself as command[0] as `['ls', '-l']`\n        input_data(str): If passed, will feed that data to the subprocess\n            through stdin\n        out_pipe(int or file): File descriptor as passed to\n            :ref:subprocess.Popen to use as stdout\n        err_pipe(int or file): File descriptor as passed to\n            :ref:subprocess.Popen to use as stderr\n        env(dict of str:str): If set, will use the given dict as env for the\n            subprocess\n        **kwargs: Any other keyword args passed will be passed to the\n            :ref:subprocess.Popen call\n\n    Returns:\n        lago.utils.CommandStatus: result of the interactive execution\n    \"\"\"\n    if env is None:\n        env = os.environ.copy()\n\n    with LogTask(\n        'Run command: %s' % ' '.join('\"%s\"' % arg for arg in command),\n        logger=LOGGER,\n        level='debug',\n    ) as task:\n        command_result = _run_command(\n            command=command,\n            input_data=input_data,\n            out_pipe=out_pipe,\n            err_pipe=err_pipe,\n            env=env,\n            uuid=task.uuid,\n            **kwargs\n        )\n        return command_result", "response": "Runs a command in a non - interactive manner."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a command interactively reusing the current stdin stdout and stderr", "response": "def run_interactive_command(command, env=None, **kwargs):\n    \"\"\"\n    Runs a command interactively, reusing the current stdin, stdout and stderr\n\n    Args:\n        command(list of str): args of the command to execute, including the\n            command itself as command[0] as `['ls', '-l']`\n        env(dict of str:str): If set, will use the given dict as env for the\n            subprocess\n        **kwargs: Any other keyword args passed will be passed to the\n            :ref:subprocess.Popen call\n\n    Returns:\n        lago.utils.CommandStatus: result of the interactive execution\n    \"\"\"\n    command_result = _run_command(\n        command=command,\n        out_pipe=sys.stdout,\n        err_pipe=sys.stderr,\n        stdin=sys.stdin,\n        env=env,\n        **kwargs\n    )\n    return command_result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deepcopy(original_obj):\n    if isinstance(original_obj, list):\n        return list(deepcopy(item) for item in original_obj)\n    elif isinstance(original_obj, dict):\n        return dict((key, deepcopy(val)) for key, val in original_obj.items())\n    else:\n        return original_obj", "response": "Returns a deep copy of an object with no crossed referenced lists or dicts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_virt_stream(virt_fd):\n    try:\n        virt_conf = json.load(virt_fd)\n    except ValueError:\n        virt_fd.seek(0)\n        virt_conf = yaml.load(virt_fd)\n\n    return deepcopy(virt_conf)", "response": "Loads the given virt config stream into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_qemu_info(path, backing_chain=False, fail_on_error=True):\n\n    cmd = ['qemu-img', 'info', '--output=json', path]\n\n    if backing_chain:\n        cmd.insert(-1, '--backing-chain')\n\n    result = run_command_with_validation(\n        cmd, fail_on_error, msg='Failed to get info for {}'.format(path)\n    )\n\n    return json.loads(result.out)", "response": "Get info on a qemu disk"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qemu_rebase(target, backing_file, safe=True, fail_on_error=True):\n    cmd = ['qemu-img', 'rebase', '-b', backing_file, target]\n    if not safe:\n        cmd.insert(2, '-u')\n\n    return run_command_with_validation(\n        cmd,\n        fail_on_error,\n        msg='Failed to rebase {target} onto {backing_file}'.format(\n            target=target, backing_file=backing_file\n        )\n    )", "response": "Rebase the base disk of the specified target image onto the specified backing file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a hash for the given file", "response": "def get_hash(file_path, checksum='sha1'):\n    \"\"\"\n    Generate a hash for the given file\n\n    Args:\n        file_path (str): Path to the file to generate the hash for\n        checksum (str): hash to apply, one of the supported by hashlib, for\n            example sha1 or sha512\n\n    Returns:\n        str: hash for that file\n    \"\"\"\n\n    sha = getattr(hashlib, checksum)()\n    with open(file_path) as file_descriptor:\n        while True:\n            chunk = file_descriptor.read(65536)\n            if not chunk:\n                break\n            sha.update(chunk)\n    return sha.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves keys from a spec file.", "response": "def filter_spec(spec, paths, wildcard='*', separator='/'):\n    \"\"\"\n    Remove keys from a spec file.\n    For example, with the following path: domains/*/disks/*/metadata\n    all the metadata dicts from all domains disks will be removed.\n\n    Args:\n        spec (dict): spec to remove keys from\n        paths (list): list of paths to the keys that should be removed\n        wildcard (str): wildcard character\n        separator (str): path separator\n\n    Returns:\n        None\n\n    Raises:\n        utils.LagoUserException: If a malformed path was detected\n    \"\"\"\n\n    def remove_key(path, spec):\n        if len(path) == 0:\n            return\n        elif len(path) == 1:\n            key = path.pop()\n            if not isinstance(spec, collections.Mapping):\n                raise LagoUserException(\n                    'You have tried to remove the following key - \"{key}\".\\n'\n                    'Keys can not be removed from type {spec_type}\\n'\n                    'Please verify that path - \"{{path}}\" is valid'.format(\n                        key=key, spec_type=type(spec)\n                    )\n                )\n            if key == wildcard:\n                spec.clear()\n            else:\n                spec.pop(key, None)\n        else:\n            current = path[0]\n            if current == wildcard:\n                if isinstance(spec, list):\n                    iterator = iter(spec)\n                elif isinstance(spec, collections.Mapping):\n                    iterator = spec.itervalues()\n                else:\n                    raise LagoUserException(\n                        'Glob char {char} should refer only to dict or list, '\n                        'not to {spec_type}\\n'\n                        'Please fix path - \"{{path}}\"'.format(\n                            char=wildcard, spec_type=type(spec)\n                        )\n                    )\n\n                for i in iterator:\n                    remove_key(path[1:], i)\n            else:\n                try:\n                    remove_key(path[1:], spec[current])\n                except KeyError:\n                    raise LagoUserException(\n                        'Malformed path \"{{path}}\", key \"{key}\" '\n                        'does not exist'.format(key=current)\n                    )\n                except TypeError:\n                    raise LagoUserException(\n                        'Malformed path \"{{path}}\", can not get '\n                        'by key from type {spec_type}'.format(\n                            spec_type=type(spec)\n                        )\n                    )\n\n    for path in paths:\n        try:\n            remove_key(path.split(separator), spec)\n        except LagoUserException as e:\n            e.message = e.message.format(path=path)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ver_cmp(ver1, ver2):\n\n    return cmp(\n        pkg_resources.parse_version(ver1), pkg_resources.parse_version(ver2)\n    )", "response": "Compare lago versions of a single version string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acquire(self):\n        self._fd = open(self._path, mode='w+')\n        os.chmod(self._path, 0o660)\n        fcntl.flock(self._fd, self._op)", "response": "Acquire the lock for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(self, info_dict, delimiter='/'):\n\n        def dfs(father, path, acc):\n            if isinstance(father, list):\n                for child in father:\n                    dfs(child, path, acc)\n            elif isinstance(father, collections.Mapping):\n                for child in sorted(father.items(), key=itemgetter(0)), :\n                    dfs(child, path, acc)\n            elif isinstance(father, tuple):\n                path = copy.copy(path)\n                path.append(father[0])\n                dfs(father[1], path, acc)\n            else:\n                # join the last key with it's value\n                path[-1] = '{}: {}'.format(path[-1], str(father))\n                acc.append(delimiter.join(path))\n\n        result = []\n        dfs(info_dict.get('Prefix') or info_dict, [], result)\n\n        return '\\n'.join(result)", "response": "This method will take a data structure that represents a tree and will print all the paths that are needed to get to it and the keys that need to get to it will be output as a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a uuid pefixed identifier for the given unprefixed name.", "response": "def prefixed_name(self, unprefixed_name, max_length=0):\n        \"\"\"\n        Returns a uuid pefixed identifier\n        Args:\n            unprefixed_name(str): Name to add a prefix to\n            max_length(int): maximum length of the resultant prefixed name,\n                will adapt the given name and the length of the uuid ot fit it\n        Returns:\n            str: prefixed identifier for the given unprefixed name\n        \"\"\"\n        if max_length == 0:\n            prefixed_name = '%s-%s' % (self.uuid[:8], unprefixed_name)\n        else:\n            if max_length < 6:\n                raise RuntimeError(\n                    \"Can't prefix with less than 6 chars (%s)\" %\n                    unprefixed_name\n                )\n            if max_length < 16:\n                _uuid = self.uuid[:4]\n            else:\n                _uuid = self.uuid[:8]\n\n            name_max_length = max_length - len(_uuid) - 1\n\n            if name_max_length < len(unprefixed_name):\n                hashed_name = hashlib.sha1(unprefixed_name).hexdigest()\n                unprefixed_name = hashed_name[:name_max_length]\n\n            prefixed_name = '%s-%s' % (_uuid, unprefixed_name)\n\n        return prefixed_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an init file which represents this env and cancellable.", "response": "def generate_init(self, dst, out_format, vms_to_include, filters=None):\n        \"\"\"\n        Generate an init file which represents this env and can\n        be used with the images created by self.export_vms\n        Args:\n            dst (str): path and name of the new init file\n            out_format (plugins.output.OutFormatPlugin):\n                formatter for the output (the default is yaml)\n            filters (list): list of paths to keys that should be removed from\n                the init file\n            vms_to_include (list of :class:lago.plugins.vm.VMPlugin):\n                list of vms to include in the init file\n        Returns:\n            None\n        \"\"\"\n        # todo: move this logic to PrefixExportManager\n        with LogTask('Exporting init file to: {}'.format(dst)):\n            # Set the default formatter to yaml. The default formatter\n            # doesn't generate a valid init file, so it's not reasonable\n            # to use it\n            if isinstance(out_format, plugins.output.DefaultOutFormatPlugin):\n                out_format = plugins.output.YAMLOutFormatPlugin()\n\n            if not filters:\n                filters = [\n                    'domains/*/disks/*/metadata',\n                    'domains/*/metadata/deploy-scripts', 'domains/*/snapshots',\n                    'domains/*/name', 'nets/*/mapping', 'nets/*/dns_records'\n                ]\n\n            spec = self.get_env_spec(filters)\n            temp = {}\n\n            for vm in vms_to_include:\n                temp[vm.name()] = spec['domains'][vm.name()]\n\n            spec['domains'] = temp\n\n            for _, domain in spec['domains'].viewitems():\n                domain['disks'] = [\n                    d for d in domain['disks'] if not d.get('skip-export')\n                ]\n\n                for disk in domain['disks']:\n                    if disk['type'] == 'template':\n                        disk['template_type'] = 'qcow2'\n                    elif disk['type'] == 'empty':\n                        disk['type'] = 'file'\n                        disk['make_a_copy'] = 'True'\n\n                    # Insert the relative path to the exported images\n                    disk['path'] = os.path.join(\n                        '$LAGO_INITFILE_PATH', os.path.basename(disk['path'])\n                    )\n\n            with open(dst, 'wt') as f:\n                if isinstance(out_format, plugins.output.YAMLOutFormatPlugin):\n                    # Dump the yaml file without type tags\n                    # TODO: Allow passing parameters to output plugins\n                    f.write(yaml.safe_dump(spec))\n                else:\n                    f.write(out_format.format(spec))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_env_spec(self, filters=None):\n        spec = {\n            'domains':\n                {\n                    vm_name: deepcopy(vm_object.spec)\n                    for vm_name, vm_object in self._vms.viewitems()\n                },\n            'nets':\n                {\n                    net_name: deepcopy(net_object.spec)\n                    for net_name, net_object in self._nets.viewitems()\n                }\n        }\n\n        if filters:\n            utils.filter_spec(spec, filters)\n\n        return spec", "response": "Get the spec of the current env."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the common arguments for stop and shutdown commands.", "response": "def _get_stop_shutdown_common_args(self, vm_names):\n        \"\"\"\n        Get the common arguments for stop and shutdown commands\n        Args:\n            vm_names (list of str): The names of the requested vms\n        Returns\n            list of plugins.vm.VMProviderPlugin:\n                vms objects that should be stopped\n            list of virt.Network: net objects that should be stopped\n            str: log message\n        Raises:\n            utils.LagoUserException: If a vm name doesn't exist\n        \"\"\"\n\n        vms_to_stop = self.get_vms(vm_names).values()\n\n        if not vm_names:\n            log_msg = '{} prefix'\n            nets = self._nets.values()\n        else:\n            log_msg = '{} specified VMs'\n            nets = self._get_unused_nets(vms_to_stop)\n\n        return vms_to_stop, nets, log_msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of nets that are not used by the vms in vms_to_stop.", "response": "def _get_unused_nets(self, vms_to_stop):\n        \"\"\"\n        Return a list of nets that used only by the vms in vms_to_stop\n        Args:\n            vms_to_stop (list of str): The names of the requested vms\n        Returns\n            list of virt.Network: net objects that used only by\n                vms in vms_to_stop\n        Raises:\n            utils.LagoUserException: If a vm name doesn't exist\n        \"\"\"\n\n        vm_names = [vm.name() for vm in vms_to_stop]\n        unused_nets = set()\n\n        for vm in vms_to_stop:\n            unused_nets = unused_nets.union(vm.nets())\n        for vm in self._vms.values():\n            if not vm.running() or vm.name() in vm_names:\n                continue\n            for net in vm.nets():\n                unused_nets.discard(net)\n        nets = [self._nets[net] for net in unused_nets]\n\n        return nets"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the vms associated with vm_names or all the vms in the prefix.", "response": "def get_vms(self, vm_names=None):\n        \"\"\"\n        Returns the vm objects associated with vm_names\n        if vm_names is None, return all the vms in the prefix\n        Args:\n            vm_names (list of str): The names of the requested vms\n        Returns\n            dict: Which contains the requested vm objects indexed by name\n        Raises:\n            utils.LagoUserException: If a vm name doesn't exist\n        \"\"\"\n        if not vm_names:\n            return self._vms.copy()\n\n        missing_vms = []\n        vms = {}\n        for name in vm_names:\n            try:\n                vms[name] = self._vms[name]\n            except KeyError:\n                # TODO: add resolver by suffix\n                missing_vms.append(name)\n\n        if missing_vms:\n            raise utils.LagoUserException(\n                'The following vms do not exist: \\n{}'.format(\n                    '\\n'.join(missing_vms)\n                )\n            )\n\n        return vms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of snapshots for each domain in the cluster", "response": "def get_snapshots(self, domains=None):\n        \"\"\"\n        Get the list of snapshots for each domain\n        Args:\n            domanins(list of str): list of the domains to get the snapshots\n            for, all will be returned if none or empty list passed\n        Returns:\n            dict of str -> list(str): with the domain names and the list of\n            snapshots for each\n        \"\"\"\n        snapshots = {}\n        for vm_name, vm in self.get_vms().items():\n            if domains and vm_name not in domains:\n                continue\n\n            snapshots[vm_name] = vm._spec['snapshots']\n\n        return snapshots"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef workdir_loaded(func):\n\n    @wraps(func)\n    def decorator(workdir, *args, **kwargs):\n        if not workdir.loaded:\n            workdir.load()\n\n        return func(workdir, *args, **kwargs)\n\n    return decorator", "response": "Decorator to make sure that the workdir is loaded when calling the\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, prefix_name='default', *args, **kwargs):\n        if self.loaded:\n            raise WorkdirError('Workdir %s already initialized' % self.path)\n\n        if not os.path.exists(self.path):\n            LOGGER.debug('Creating workdir %s', self.path)\n            os.makedirs(self.path)\n\n        self.prefixes[prefix_name] = self.prefix_class(\n            self.join(prefix_name), *args, **kwargs\n        )\n        self.prefixes[prefix_name].initialize()\n        if self.current is None:\n            self._set_current(prefix_name)\n\n        self.load()\n\n        return self.prefixes[prefix_name]", "response": "Initializes a new workdir by adding a new prefix to the workdir."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self):\n        if self.loaded:\n            LOGGER.debug('Already loaded')\n            return\n\n        try:\n            basepath, dirs, _ = os.walk(self.path).next()\n        except StopIteration:\n            raise MalformedWorkdir('Empty dir %s' % self.path)\n\n        full_path = partial(os.path.join, basepath)\n        found_current = False\n\n        for dirname in dirs:\n            if dirname == 'current' and os.path.islink(full_path('current')):\n                self.current = os.path.basename(\n                    os.readlink(full_path('current'))\n                )\n                found_current = True\n                continue\n            elif dirname == 'current':\n                raise MalformedWorkdir(\n                    '\"%s/current\" should be a soft link' % self.path\n                )\n\n            self.prefixes[dirname] = self.prefix_class(\n                prefix=self.join(dirname)\n            )\n\n        if not found_current:\n            raise MalformedWorkdir(\n                '\"%s/current\" should exist and be a soft link' % self.path\n            )\n\n        self._update_current()", "response": "Loads the prefixes that are available is the workdir\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_current(self):\n        if not self.current or self.current not in self.prefixes:\n            if 'default' in self.prefixes:\n                selected_current = 'default'\n            elif self.prefixes:\n                selected_current = sorted(self.prefixes.keys()).pop()\n            else:\n                # should never get here\n                raise MalformedWorkdir(\n                    'No current link and no prefixes in workdir %s' % self.path\n                )\n\n            logging.info(\n                'Missing current link, setting it to %s',\n                selected_current,\n            )\n            self._set_current(selected_current)", "response": "Updates the current link for the current workdir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the current prefix for internal usage", "response": "def _set_current(self, new_current):\n        \"\"\"\n        Change the current default prefix, for internal usage\n\n        Args:\n            new_current(str): Name of the new current prefix, it must already\n                exist\n\n        Returns:\n            None\n\n        Raises:\n            PrefixNotFound: if the given prefix name does not exist in the\n                workdir\n        \"\"\"\n        new_cur_full_path = self.join(new_current)\n        if not os.path.exists(new_cur_full_path):\n            raise PrefixNotFound(\n                'Prefix \"%s\" does not exist in workdir %s' %\n                (new_current, self.path)\n            )\n\n        if os.path.lexists(self.join('current')):\n            os.unlink(self.join('current'))\n\n        os.symlink(new_current, self.join('current'))\n        self.current = new_current"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new prefix to the workdir.", "response": "def add_prefix(self, name, *args, **kwargs):\n        \"\"\"\n        Adds a new prefix to the workdir.\n\n        Args:\n            name(str): Name of the new prefix to add\n            *args: args to pass along to the prefix constructor\n            *kwargs: kwargs to pass along to the prefix constructor\n\n        Returns:\n            The newly created prefix\n\n        Raises:\n            LagoPrefixAlreadyExistsError: if prefix name already exists in the\n            workdir\n        \"\"\"\n        if os.path.exists(self.join(name)):\n            raise LagoPrefixAlreadyExistsError(name, self.path)\n\n        self.prefixes[name] = self.prefix_class(\n            self.join(name), *args, **kwargs\n        )\n        self.prefixes[name].initialize()\n        if self.current is None:\n            self.set_current(name)\n\n        return self.prefixes[name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a prefix resolving the current one if needed", "response": "def get_prefix(self, name):\n        \"\"\"\n        Retrieve a prefix, resolving the current one if needed\n\n        Args:\n            name(str): name of the prefix to retrieve, or current to get the\n                current one\n\n        Returns:\n            self.prefix_class: instance of the prefix with the given name\n        \"\"\"\n        if name == 'current':\n            name = self.current\n\n        try:\n            return self.prefixes[name]\n        except KeyError:\n            raise KeyError(\n                'Unable to find prefix \"%s\" in workdir %s' % (name, self.path)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndestroys all the given prefixes and remove any left files if no more ones are left.", "response": "def destroy(self, prefix_names=None):\n        \"\"\"\n        Destroy all the given prefixes and remove any left files if no more\n        prefixes are left\n\n        Args:\n            prefix_names(list of str): list of prefix names to destroy, if None\n            passed (default) will destroy all of them\n        \"\"\"\n        if prefix_names is None:\n            self.destroy(prefix_names=self.prefixes.keys())\n            return\n\n        for prefix_name in prefix_names:\n            if prefix_name == 'current' and self.current in prefix_names:\n                continue\n\n            elif prefix_name == 'current':\n                prefix_name = self.current\n\n            self.get_prefix(prefix_name).destroy()\n            self.prefixes.pop(prefix_name)\n            if self.prefixes:\n                self._update_current()\n\n        if not self.prefixes:\n            shutil.rmtree(self.path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for an existing workdir in the given path.", "response": "def resolve_workdir_path(cls, start_path=os.curdir):\n        \"\"\"\n        Look for an existing workdir in the given path, in a path/.lago dir,\n        or in a .lago dir under any of it's parent directories\n\n        Args:\n            start_path (str): path to start the search from, if None passed, it\n                will use the current dir\n\n        Returns:\n            str: path to the found prefix\n\n        Raises:\n            LagoUserException: if no prefix was found\n        \"\"\"\n        if start_path == 'auto':\n            start_path = os.curdir\n\n        cur_path = start_path\n\n        LOGGER.debug(\n            'Checking if %s is a workdir',\n            os.path.abspath(cur_path),\n        )\n        if cls.is_workdir(cur_path):\n            return os.path.abspath(cur_path)\n\n        # now search for a .lago directory that's a workdir on any parent dir\n        cur_path = os.path.join(start_path, '.lago')\n        while not cls.is_workdir(cur_path):\n            LOGGER.debug('%s is not a workdir', cur_path)\n            cur_path = os.path.normpath(\n                os.path.join(cur_path, '..', '..', '.lago')\n            )\n            LOGGER.debug('Checking %s for a workdir', cur_path)\n            if os.path.realpath(os.path.join(cur_path, '..')) == '/':\n                # no workdir found - look workdirs up the current path + 1,\n                # print informative message and exit.\n                candidates = []\n                for path in os.listdir(os.curdir):\n                    if os.path.isdir(path):\n                        dirs = os.listdir(path)\n                        if 'current' in dirs:\n                            candidates.append(\n                                os.path.abspath(os.path.join(os.curdir, path))\n                            )\n                        elif '.lago' in dirs:\n                            candidates.append(\n                                os.path.abspath(\n                                    os.path.join(os.curdir, path, '.lago')\n                                )\n                            )\n                candidates = filter(Workdir.is_possible_workdir, candidates)\n                for idx in range(len(candidates)):\n                    if os.path.split(candidates[idx])[1] == '.lago':\n                        candidates[idx] = os.path.dirname(candidates[idx])\n\n                msg = 'Unable to find workdir in {0}'.format(\n                    os.path.abspath(start_path)\n                )\n                if candidates:\n                    msg += '\\nFound possible workdirs in: {0}'.format(\n                        ', '.join(candidates)\n                    )\n                raise LagoUserException(msg)\n\n        return os.path.abspath(cur_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the given path is a workdir", "response": "def is_workdir(cls, path):\n        \"\"\"\n        Check if the given path is a workdir\n\n        Args:\n            path(str): Path to check\n\n        Return:\n            bool: True if the given path is a workdir\n        \"\"\"\n        try:\n            cls(path=path).load()\n        except MalformedWorkdir:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cleanup(self):\n\n        current = self.join('current')\n        if not os.path.exists(current):\n            LOGGER.debug('found broken current symlink, removing: %s', current)\n            os.unlink(self.join('current'))\n            self.current = None\n            try:\n                self._update_current()\n            except PrefixNotFound:\n                if not os.listdir(self.path):\n                    LOGGER.debug('workdir is empty, removing %s', self.path)\n                    os.rmdir(self.path)\n                else:\n                    raise MalformedWorkdir(\n                        (\n                            'Unable to find any prefixes in {0}, '\n                            'but the directory looks malformed. '\n                            'Try deleting it manually.'\n                        ).format(self.path)\n                    )", "response": "Attempt to set a new current symlink if it is broken. If no other other\n        prefixes exist try to delete the entire workdir."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_plugins(namespace, instantiate=True):\n    mgr = ExtensionManager(\n        namespace=namespace,\n        on_load_failure_callback=(\n            lambda _, ep, err: LOGGER.\n            warning('Could not load plugin {}: {}'.format(ep.name, err))\n        )\n    )\n    if instantiate:\n        plugins = dict(\n            (\n                ext.name,\n                ext.plugin if isinstance(ext.plugin, Plugin) else ext.plugin()\n            ) for ext in mgr\n        )\n    else:\n        plugins = dict((ext.name, ext.plugin) for ext in mgr)\n\n    return plugins", "response": "Loads all the plugins for the given namespace"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metadata(self):\n        if self._metadata is None:\n            try:\n                with open(self.paths.metadata()) as metadata_fd:\n                    self._metadata = json.load(metadata_fd)\n            except IOError:\n                self._metadata = {}\n        return self._metadata", "response": "Retrieve the metadata info for this prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _save_metadata(self):\n        with open(self.paths.metadata(), 'w') as metadata_fd:\n            utils.json_dump(self.metadata, metadata_fd)", "response": "Write this prefix metadata to disk\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave this prefix to persistent storage", "response": "def save(self):\n        \"\"\"\n        Save this prefix to persistent storage\n\n        Returns:\n            None\n        \"\"\"\n        if not os.path.exists(self.paths.virt()):\n            os.makedirs(self.paths.virt())\n\n        self._save_metadata()\n        self.virt_env.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a pair of ssh keys for this prefix and return None if it fails", "response": "def _create_ssh_keys(self):\n        \"\"\"\n        Generate a pair of ssh keys for this prefix\n\n        Returns:\n            None\n\n        Raises:\n            RuntimeError: if it fails to create the keys\n        \"\"\"\n        ret, _, _ = utils.run_command(\n            [\n                'ssh-keygen',\n                '-t',\n                'rsa',\n                '-m',\n                'PEM',\n                '-N',\n                '',\n                '-f',\n                self.paths.ssh_id_rsa(),\n            ]\n        )\n        if ret != 0:\n            raise RuntimeError(\n                'Failed to crate ssh keys at %s',\n                self.paths.ssh_id_rsa(),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize(self):\n        prefix = self.paths.prefix_path()\n        os.environ['LAGO_PREFIX_PATH'] = prefix\n        os.environ['LAGO_WORKDIR_PATH'] = os.path.dirname(prefix)\n\n        with utils.RollbackContext() as rollback:\n            with LogTask('Create prefix dirs'):\n                try:\n                    os.mkdir(prefix)\n                except OSError as error:\n                    raise RuntimeError(\n                        'Could not create prefix at %s:\\n%s' % (prefix, error)\n                    )\n            rollback.prependDefer(shutil.rmtree, prefix)\n\n            with open(self.paths.uuid(), 'w') as f, \\\n                    LogTask('Generate prefix uuid'):\n                f.write(uuid.uuid1().hex)\n\n            with LogTask('Create ssh keys'):\n                self._create_ssh_keys()\n\n            with LogTask('Tag prefix as initialized'):\n                with open(self.paths.prefix_lagofile(), 'w') as fd:\n                    fd.write('')\n\n            rollback.clear()", "response": "Initialize the log file with the current prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cleanup(self):\n        with LogTask('Stop prefix'):\n            self.stop()\n        with LogTask(\"Tag prefix as uninitialized\"):\n            os.unlink(self.paths.prefix_lagofile())", "response": "Stops any running entities in the prefix and uninitializes the prefix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_net_specs(conf):\n        for net_name, net_spec in conf.get('nets', {}).items():\n            net_spec['name'] = net_name\n            net_spec['mapping'] = {}\n            net_spec.setdefault('type', 'nat')\n\n        return conf", "response": "Initializes all the net_specs in the configuration specification so they can be used comfortably\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _allocate_subnets(self, conf):\n        allocated_subnets = []\n        try:\n            for net_spec in conf.get('nets', {}).itervalues():\n                if net_spec['type'] != 'nat':\n                    continue\n\n                gateway = net_spec.get('gw')\n                if gateway:\n                    allocated_subnet = self._subnet_store.acquire(\n                        self.paths.uuid(), gateway\n                    )\n                else:\n                    allocated_subnet = self._subnet_store.acquire(\n                        self.paths.uuid()\n                    )\n                    net_spec['gw'] = str(allocated_subnet.iter_hosts().next())\n\n                allocated_subnets.append(allocated_subnet)\n        except:\n            self._subnet_store.release(allocated_subnets)\n            raise\n        return allocated_subnets, conf", "response": "Allocate all the subnets needed by the given configuration spec."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_nic_to_mapping(self, net, dom, nic):\n        dom_name = dom['name']\n        idx = dom['nics'].index(nic)\n        name = '{0}-eth{1}'.format(dom_name, idx)\n        net['mapping'][name] = nic['ip']\n        if dom['nics'][idx]['net'] == dom['mgmt_net']:\n            net['mapping'][dom_name] = nic['ip']\n\n        name_by_net = '{0}-{1}'.format(dom_name, nic['net'])\n        named_nets = sorted(\n            [\n                net_name for net_name in net['mapping'].keys()\n                if net_name.startswith(name_by_net) and net_name != name_by_net\n            ]\n        )\n\n        if len(named_nets) == 0:\n            named_idx = 0\n            net['mapping'][name_by_net] = nic['ip']\n        else:\n            named_idx = len(named_nets)\n        named_net = '{0}-{1}'.format(name_by_net, named_idx)\n\n        net['mapping'][named_net] = nic['ip']", "response": "Adds the given nic to the given net spec mapping entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _select_mgmt_networks(self, conf):\n\n        nets = conf['nets']\n        mgmts = sorted(\n            [\n                name for name, net in nets.iteritems()\n                if net.get('management') is True\n            ]\n        )\n\n        if len(mgmts) == 0:\n            mgmt_name = sorted((nets.keys()))[0]\n            LOGGER.debug(\n                'No management network configured, selecting network %s',\n                mgmt_name\n            )\n            nets[mgmt_name]['management'] = True\n            mgmts.append(mgmt_name)\n\n        for mgmt_name in mgmts:\n            if nets[mgmt_name].get('dns_domain_name', None) is None:\n                nets[mgmt_name]['dns_domain_name'] = 'lago.local'\n\n        return mgmts", "response": "Select management networks. If no management network is found, it will\n        mark the first network found by sorted the network lists. Also adding\n        default DNS domain, if none is set.\n\n        Args:\n            conf(spec): spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding DNS records dict to conf for each management network.", "response": "def _add_dns_records(self, conf, mgmts):\n        \"\"\"\n        Add DNS records dict('dns_records') to ``conf`` for each\n        management network. Add DNS forwarder IP('dns_forward') for each none\n        management network.\n\n\n        Args:\n            conf(spec): spec\n            mgmts(list): management networks names\n\n        Returns:\n            None\n        \"\"\"\n\n        nets = conf['nets']\n        dns_mgmt = mgmts[-1]\n        LOGGER.debug('Using network %s as main DNS server', dns_mgmt)\n        forward = conf['nets'][dns_mgmt].get('gw')\n        dns_records = {}\n        for net_name, net_spec in nets.iteritems():\n            dns_records.update(net_spec['mapping'].copy())\n            if net_name not in mgmts:\n                net_spec['dns_forward'] = forward\n\n        for mgmt in mgmts:\n            if nets[mgmt].get('dns_records'):\n                nets[mgmt]['dns_records'].update(dns_records)\n            else:\n                nets[mgmt]['dns_records'] = dns_records"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _register_preallocated_ips(self, conf):\n        for dom_name, dom_spec in conf.get('domains', {}).items():\n            for idx, nic in enumerate(dom_spec.get('nics', [])):\n                if 'ip' not in nic:\n                    continue\n\n                net = conf['nets'][nic['net']]\n                if self._subnet_store.is_leasable_subnet(net['gw']):\n                    nic['ip'] = _create_ip(\n                        net['gw'], int(nic['ip'].split('.')[-1])\n                    )\n\n                dom_name = dom_spec['name']\n                if not _ip_in_subnet(net['gw'], nic['ip']):\n                    raise RuntimeError(\n                        \"%s:nic%d's IP [%s] is outside the subnet [%s]\" % (\n                            dom_name,\n                            dom_spec['nics'].index(nic),\n                            nic['ip'],\n                            net['gw'],\n                        ),\n                    )\n\n                if nic['ip'] in net['mapping'].values():\n                    conflict_list = [\n                        name for name, ip in net['mapping'].items()\n                        if ip == net['ip']\n                    ]\n                    raise RuntimeError(\n                        'IP %s was to several domains: %s %s' % (\n                            nic['ip'],\n                            dom_name,\n                            ' '.join(conflict_list),\n                        ),\n                    )\n\n                self._add_nic_to_mapping(net, dom_spec, nic)", "response": "Parse all the domains in the given conf and preallocate all their ips into the networks mappings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nallocate all the ips that have dynamic ip and add it to the network mapping", "response": "def _allocate_ips_to_nics(self, conf):\n        \"\"\"\n        For all the nics of all the domains in the conf that have dynamic ip,\n        allocate one and addit to the network mapping\n\n        Args:\n            conf (dict): Configuration spec to extract the domains from\n\n        Returns:\n            None\n        \"\"\"\n        for dom_name, dom_spec in conf.get('domains', {}).items():\n            for idx, nic in enumerate(dom_spec.get('nics', [])):\n                if 'ip' in nic:\n                    continue\n                net = self._get_net(conf, dom_name, nic)\n                if net['type'] != 'nat':\n                    continue\n\n                allocated = net['mapping'].values()\n                vacant = _create_ip(\n                    net['gw'],\n                    set(range(2, 255)).difference(\n                        set([int(ip.split('.')[-1]) for ip in allocated])\n                    ).pop()\n                )\n                nic['ip'] = vacant\n                self._add_nic_to_mapping(net, dom_spec, nic)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the MTU of all the nics in the configuration spec to the NIC definition.", "response": "def _set_mtu_to_nics(self, conf):\n        \"\"\"\n        For all the nics of all the domains in the conf that have MTU set,\n        save the MTU on the NIC definition.\n\n        Args:\n            conf (dict): Configuration spec to extract the domains from\n\n        Returns:\n            None\n        \"\"\"\n        for dom_name, dom_spec in conf.get('domains', {}).items():\n            for idx, nic in enumerate(dom_spec.get('nics', [])):\n                net = self._get_net(conf, dom_name, nic)\n                mtu = net.get('mtu', 1500)\n                if mtu != 1500:\n                    nic['mtu'] = mtu"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _config_net_topology(self, conf):\n        conf = self._init_net_specs(conf)\n        mgmts = self._select_mgmt_networks(conf)\n        self._validate_netconfig(conf)\n        allocated_subnets, conf = self._allocate_subnets(conf)\n        try:\n            self._add_mgmt_to_domains(conf, mgmts)\n            self._register_preallocated_ips(conf)\n            self._allocate_ips_to_nics(conf)\n            self._set_mtu_to_nics(conf)\n            self._add_dns_records(conf, mgmts)\n        except:\n            self._subnet_store.release(allocated_subnets)\n            raise\n        return conf", "response": "Initialize and populate all the network related elements like\n        reserving ips and populating network specs of the given confiiguration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd management network key to each domain.", "response": "def _add_mgmt_to_domains(self, conf, mgmts):\n        \"\"\"\n        Add management network key('mgmt_net') to each domain. Note this\n        assumes ``conf`` was validated.\n\n        Args:\n            conf(dict): spec\n            mgmts(list): list of management networks names\n\n        \"\"\"\n\n        for dom_name, dom_spec in conf['domains'].iteritems():\n            domain_mgmt = [\n                nic['net'] for nic in dom_spec['nics'] if nic['net'] in mgmts\n            ].pop()\n\n            dom_spec['mgmt_net'] = domain_mgmt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_netconfig(self, conf):\n\n        nets = conf.get('nets', {})\n        if len(nets) == 0:\n            # TO-DO: add default networking if no network is configured\n            raise LagoInitException('No networks configured.')\n\n        no_mgmt_dns = [\n            name for name, net in nets.iteritems()\n            if net.get('management', None) is None and\n            (net.get('main_dns') or net.get('dns_domain_name'))\n        ]\n        if len(no_mgmt_dns) > 0 and len(nets.keys()) > 1:\n            raise LagoInitException(\n                (\n                    'Networks: {0}, misconfigured, they '\n                    'are not marked as management, but have '\n                    'DNS attributes. DNS is supported '\n                    'only in management networks.'\n                ).format(','.join(no_mgmt_dns))\n            )\n\n        for dom_name, dom_spec in conf['domains'].items():\n            mgmts = []\n            for nic in dom_spec['nics']:\n                net = self._get_net(conf, dom_name, nic)\n                if net.get('management', False) is True:\n                    mgmts.append(nic['net'])\n            if len(mgmts) == 0:\n                raise LagoInitException(\n                    (\n                        'VM {0} has no management network, '\n                        'please connect it to '\n                        'one.'\n                    ).format(dom_name)\n                )\n\n            if len(mgmts) > 1:\n                raise LagoInitException(\n                    (\n                        'VM {0} has more than one management '\n                        'network: {1}. It should have exactly '\n                        'one.'\n                    ).format(dom_name, ','.join(mgmts))\n                )", "response": "Validate network configuration\n\n        Args:\n            conf(dict): spec\n\n        Returns:\n            None\n\n\n        Raises:\n            :exc:`~lago.utils.LagoInitException`: If a VM has more than\n            one management network configured, or a network which is not\n            management has DNS attributes, or a VM is configured with a\n            none-existence NIC, or a VM has no management network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_disk(\n        self,\n        name,\n        spec,\n        template_repo=None,\n        template_store=None,\n    ):\n        \"\"\"\n        Creates a disc with the given name from the given repo or store\n\n        Args:\n            name (str): Name of the domain to create the disk for\n            spec (dict): Specification of the disk to create\n            template_repo (TemplateRepository or None): template repo instance\n                to use\n            template_store (TemplateStore or None): template store instance to\n                use\n\n        Returns:\n            Tuple(str, dict): Path to the disk and disk metadata\n\n        Raises:\n            RuntimeError: If the type of the disk is not supported or failed to\n                create the disk\n        \"\"\"\n        LOGGER.debug(\"Spec: %s\" % spec)\n        with LogTask(\"Create disk %s\" % spec['name']):\n            disk_metadata = {}\n\n            if spec['type'] == 'template':\n                disk_path, disk_metadata = self._handle_template(\n                    host_name=name,\n                    template_spec=spec,\n                    template_repo=template_repo,\n                    template_store=template_store,\n                )\n            elif spec['type'] == 'empty':\n                disk_path, disk_metadata = self._handle_empty_disk(\n                    host_name=name,\n                    disk_spec=spec,\n                )\n            elif spec['type'] == 'file':\n                disk_path, disk_metadata = self._handle_file_disk(\n                    disk_spec=spec,\n                )\n            else:\n                raise RuntimeError('Unknown drive spec %s' % str(spec))\n\n            return disk_path, disk_metadata", "response": "Create a disk from the given template repository or store instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves the parent of a virtual disk.", "response": "def resolve_parent(self, disk_path, template_store, template_repo):\n        \"\"\"\n        Given a virtual disk, checks if it has a backing file, if so check\n        if the backing file is in the store, if not download it\n        from the provided template_repo.\n\n        After verifying that the backing-file is in the store,\n        create a symlink to that file and locate it near the layered image.\n\n        Args:\n            disk_path (str): path to the layered disk\n            template_repo (TemplateRepository or None): template repo instance\n                to use\n            template_store (TemplateStore or None): template store instance to\n                use\n        \"\"\"\n        qemu_info = utils.get_qemu_info(disk_path)\n        parent = qemu_info.get('backing-filename')\n        if not parent:\n            return\n\n        if os.path.isfile(parent):\n            if os.path.samefile(\n                os.path.realpath(parent),\n                os.path.realpath(os.path.expandvars(disk_path))\n            ):\n                raise LagoInitException(\n                    dedent(\n                        \"\"\"\n                        Disk {} and its backing file are the same file.\n                        \"\"\".format(disk_path)\n                    )\n                )\n            # The parent exist and we have the correct pointer\n            return\n\n        LOGGER.info('Resolving Parent')\n        try:\n            name, version = os.path.basename(parent).split(':', 1)\n        except ValueError:\n            raise LagoInitException(\n                dedent(\n                    \"\"\"\n                    Backing file resolution of disk {} failed.\n                    Backing file {} is not a Lago image.\n                    \"\"\".format(disk_path, parent)\n                )\n            )\n\n        _, _, base = self._handle_lago_template(\n            '', {'template_name': name,\n                 'template_version': version}, template_store, template_repo\n        )\n\n        # The child has the right pointer to his parent\n        base = os.path.expandvars(base)\n        if base == parent:\n            return\n\n        # The child doesn't have the right pointer to his\n        # parent, We will fix it with a symlink\n        link_name = os.path.join(\n            os.path.dirname(disk_path), '{}:{}'.format(name, version)\n        )\n        link_name = os.path.expandvars(link_name)\n\n        self._create_link_to_parent(base, link_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the given ova and makes a template of it.", "response": "def _ova_to_spec(self, filename):\n        \"\"\"\n        Retrieve the given ova and makes a template of it.\n        Creates a disk from network provided ova.\n        Calculates the needed memory from the ovf.\n        The disk will be cached in the template repo\n\n        Args:\n            filename(str): the url to retrive the data from\n\n        TODO:\n            * Add hash checking against the server\n              for faster download and latest version\n            * Add config script running on host - other place\n            * Add cloud init support - by using cdroms in other place\n            * Handle cpu in some way - some other place need to pick it up\n            * Handle the memory units properly - we just assume MegaBytes\n\n        Returns:\n            list of dict: list with the disk specification\n            int: VM memory, None if none defined\n            int: Number of virtual cpus, None if none defined\n\n        Raises:\n            RuntimeError: If the ova format is not supported\n            TypeError: If the memory units in the ova are noot supported\n                (currently only 'MegaBytes')\n        \"\"\"\n        # extract if needed\n        ova_extracted_dir = os.path.splitext(filename)[0]\n\n        if not os.path.exists(ova_extracted_dir):\n            os.makedirs(ova_extracted_dir)\n            subprocess.check_output(\n                [\"tar\", \"-xvf\", filename, \"-C\", ova_extracted_dir],\n                stderr=subprocess.STDOUT\n            )\n\n        # lets find the ovf file\n        # we expect only one to be\n        ovf = glob.glob(ova_extracted_dir + \"/master/vms/*/*.ovf\")\n        if len(ovf) != 1:\n            raise RuntimeError(\"We support only one vm in ova\")\n\n        image_file = None\n        memory = None\n        vcpus = None\n        # we found our ovf\n        # lets extract the resources\n        with open(ovf[0]) as fd:\n            # lets extract the items\n            obj = xmltodict.parse(fd.read())\n            hardware_items = [\n                section\n                for section in obj[\"ovf:Envelope\"][\"Content\"][\"Section\"]\n                if section[\"@xsi:type\"] == \"ovf:VirtualHardwareSection_Type\"\n            ]\n\n            if len(hardware_items) != 1:\n                raise RuntimeError(\"We support only one machine desc in ova\")\n            hardware_items = hardware_items[0]\n\n            for item in hardware_items[\"Item\"]:\n                # lets test resource types\n                CPU_RESOURCE = 3\n                MEMORY_RESOURCE = 4\n                DISK_RESOURCE = 17\n\n                resource_type = int(item[\"rasd:ResourceType\"])\n                if resource_type == CPU_RESOURCE:\n                    vcpus = int(item[\"rasd:cpu_per_socket\"]) * \\\n                        int(item[\"rasd:num_of_sockets\"])\n\n                elif resource_type == MEMORY_RESOURCE:\n                    memory = int(item[\"rasd:VirtualQuantity\"])\n                    if item[\"rasd:AllocationUnits\"] != \"MegaBytes\":\n                        raise TypeError(\n                            \"Fix me : we need to suport other units too\"\n                        )\n\n                elif resource_type == DISK_RESOURCE:\n                    image_file = item[\"rasd:HostResource\"]\n\n        if image_file is not None:\n            disk_meta = {\"root-partition\": \"/dev/sda1\"}\n            disk_spec = [\n                {\n                    \"type\": \"template\",\n                    \"template_type\": \"qcow2\",\n                    \"format\": \"qcow2\",\n                    \"dev\": \"vda\",\n                    \"name\": os.path.basename(image_file),\n                    \"path\": ova_extracted_dir + \"/images/\" + image_file,\n                    \"metadata\": disk_meta\n                }\n            ]\n\n        return disk_spec, memory, vcpus"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the given spec with the values of the prototypes .", "response": "def _use_prototype(self, spec, prototypes):\n        \"\"\"\n        Populates the given spec with the values of it's declared prototype\n\n        Args:\n            spec (dict): spec to update\n            prototypes (dict): Configuration spec containing the prototypes\n\n        Returns:\n            dict: updated spec\n        \"\"\"\n        prototype = spec['based-on']\n        del spec['based-on']\n        for attr in prototype:\n            if attr not in spec:\n                spec[attr] = copy.deepcopy(prototype[attr])\n\n        return spec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the given url to the prefix", "response": "def fetch_url(self, url):\n        \"\"\"\n        Retrieves the given url to the prefix\n\n        Args:\n            url(str): Url to retrieve\n\n        Returns:\n            str: path to the downloaded file\n        \"\"\"\n        url_path = urlparse.urlsplit(url).path\n        dst_path = os.path.basename(url_path)\n        dst_path = self.paths.prefixed(dst_path)\n        with LogTask('Downloading %s' % url):\n            urllib.urlretrieve(url=os.path.expandvars(url), filename=dst_path)\n\n        return dst_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the virt config from a file - like object.", "response": "def virt_conf_from_stream(\n        self,\n        conf_fd,\n        template_repo=None,\n        template_store=None,\n        do_bootstrap=True,\n        do_build=True,\n    ):\n        \"\"\"\n        Initializes all the virt infrastructure of the prefix, creating the\n        domains disks, doing any network leases and creating all the virt\n        related files and dirs inside this prefix.\n\n        Args:\n            conf_fd (File): File like object to read the config from\n            template_repo (TemplateRepository): template repository intance\n            template_store (TemplateStore): template store instance\n\n        Returns:\n            None\n        \"\"\"\n        virt_conf = utils.load_virt_stream(conf_fd)\n        LOGGER.debug('Loaded virt config:\\n%s', virt_conf)\n        return self.virt_conf(\n            conf=virt_conf,\n            template_repo=template_repo,\n            template_store=template_store,\n            do_bootstrap=do_bootstrap,\n            do_build=do_build\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef virt_conf(\n        self,\n        conf,\n        template_repo=None,\n        template_store=None,\n        do_bootstrap=True,\n        do_build=True\n    ):\n        \"\"\"\n        Initializes all the virt infrastructure of the prefix, creating the\n        domains disks, doing any network leases and creating all the virt\n        related files and dirs inside this prefix.\n\n        Args:\n            conf (dict): Configuration spec\n            template_repo (TemplateRepository): template repository intance\n            template_store (TemplateStore): template store instance\n            do_bootstrap(bool): If true run virt-sysprep on the images\n            do_build(bool): If true run build commands on the images,\n                see lago.build.py for more info.\n\n        Returns:\n            None\n        \"\"\"\n        os.environ['LAGO_PREFIX_PATH'] = self.paths.prefix_path()\n        with utils.RollbackContext() as rollback:\n            rollback.prependDefer(\n                shutil.rmtree, self.paths.prefix_path(), ignore_errors=True\n            )\n            self._metadata = {\n                'lago_version': pkg_resources.get_distribution(\"lago\").version,\n            }\n\n            conf = self._prepare_domains_images(\n                conf=conf,\n                template_repo=template_repo,\n                template_store=template_store,\n            )\n            conf = self._config_net_topology(conf)\n\n            conf['domains'] = self._copy_deploy_scripts_for_hosts(\n                domains=conf['domains']\n            )\n            self._virt_env = self.VIRT_ENV_CLASS(\n                prefix=self,\n                vm_specs=conf['domains'],\n                net_specs=conf['nets'],\n            )\n\n            if do_bootstrap:\n                self.virt_env.bootstrap()\n\n            if do_build:\n                self.build(conf['domains'])\n\n            self.save()\n            rollback.clear()", "response": "Initializes all the virt infrastructure of the prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_vms(\n        self,\n        vms_names=None,\n        standalone=False,\n        export_dir='.',\n        compress=False,\n        init_file_name='LagoInitFile',\n        out_format=YAMLOutFormatPlugin(),\n        collect_only=False,\n        with_threads=True,\n    ):\n        \"\"\"\n        Export vm images disks and init file.\n        The exported images and init file can be used to recreate\n        the environment.\n\n        Args:\n            vms_names(list of str): Names of the vms to export, if None\n                export all the vms in the env (default=None)\n            standalone(bool): If false, export a layered image\n                (default=False)\n            export_dir(str): Dir to place the exported images and init file\n            compress(bool): If True compress the images with xz\n                  (default=False)\n            init_file_name(str): The name of the exported init file\n                (default='LagoInitfile')\n            out_format(:class:`lago.plugins.output.OutFormatPlugin`):\n                The type of the exported init file (the default is yaml)\n            collect_only(bool): If True, return only a mapping from vm name\n                to the disks that will be exported. (default=False)\n            with_threads(bool): If True, run the export in parallel\n                (default=True)\n\n        Returns\n            Unless collect_only == True, a mapping between vms' disks.\n\n        \"\"\"\n        return self.virt_env.export_vms(\n            vms_names, standalone, export_dir, compress, init_file_name,\n            out_format, collect_only, with_threads\n        )", "response": "Exports vms in the environment and returns a dict of exported VM images disks and init file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshut down the virtual environment for the vms in vm_names.", "response": "def shutdown(self, vm_names=None, reboot=False):\n        \"\"\"\n        Shutdown this prefix\n\n        Args:\n            vm_names(list of str): List of the vms to shutdown\n            reboot(bool): If true, reboot the requested vms\n\n        Returns:\n            None\n        \"\"\"\n        self.virt_env.shutdown(vm_names, reboot)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef virt_env(self):\n        if self._virt_env is None:\n            self._virt_env = self._create_virt_env()\n        return self._virt_env", "response": "Get the virt env instance used by this instance. Creates it if needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef destroy(self):\n\n        subnets = (\n            str(net.gw()) for net in self.virt_env.get_nets().itervalues()\n        )\n\n        self._subnet_store.release(subnets)\n        self.cleanup()\n        shutil.rmtree(self.paths.prefix_path())", "response": "Destroy this prefix running any cleanups and removing any files\n        inside it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves the prefix path for the current directory and returns the absolute path to the prefix.", "response": "def resolve_prefix_path(cls, start_path=None):\n        \"\"\"\n        Look for an existing prefix in the given path, in a path/.lago dir, or\n        in a .lago dir under any of it's parent directories\n\n        Args:\n            start_path (str): path to start the search from, if None passed, it\n                will use the current dir\n\n        Returns:\n            str: path to the found prefix\n\n        Raises:\n            RuntimeError: if no prefix was found\n        \"\"\"\n        if not start_path or start_path == 'auto':\n            start_path = os.path.curdir\n\n        cur_path = start_path\n        LOGGER.debug('Checking if %s is a prefix', os.path.abspath(cur_path))\n        if cls.is_prefix(cur_path):\n            return os.path.abspath(cur_path)\n\n        # now search for a .lago directory that's a prefix on any parent dir\n        cur_path = join(start_path, '.lago')\n        while not cls.is_prefix(cur_path):\n            LOGGER.debug('%s  is not a prefix', cur_path)\n            cur_path = os.path.normpath(\n                os.path.join(cur_path, '..', '..', '.lago')\n            )\n            LOGGER.debug('Checking %s for a prefix', cur_path)\n            if os.path.realpath(join(cur_path, '..')) == '/':\n                raise RuntimeError(\n                    'Unable to find prefix for %s' %\n                    os.path.abspath(start_path)\n                )\n\n        return os.path.abspath(cur_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_prefix(cls, path):\n        lagofile = paths.Paths(path).prefix_lagofile()\n        return os.path.isfile(lagofile)", "response": "Check if a path is a valid prefix for a log file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_scripts(self, host_metadata):\n        deploy_scripts = host_metadata.get('deploy-scripts', [])\n        if deploy_scripts:\n            return deploy_scripts\n\n        ovirt_scripts = host_metadata.get('ovirt-scripts', [])\n        if ovirt_scripts:\n            warnings.warn(\n                'Deprecated entry \"ovirt-scripts\" will not be supported in '\n                'the future, replace with \"deploy-scripts\"'\n            )\n\n        return ovirt_scripts", "response": "Returns the scripts for the host if any"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_scripts(self, host_metadata, scripts):\n        scripts_key = 'deploy-scripts'\n        if 'ovirt-scritps' in host_metadata:\n            scripts_key = 'ovirt-scripts'\n\n        host_metadata[scripts_key] = scripts\n        return host_metadata", "response": "Set the scripts in the metadata dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying the deploy scripts for all the domains into the prefix scripts dir", "response": "def _copy_deploy_scripts_for_hosts(self, domains):\n        \"\"\"\n        Copy the deploy scripts for all the domains into the prefix scripts dir\n\n        Args:\n            domains(dict): spec with the domains info as when loaded from the\n                initfile\n\n        Returns:\n            None\n        \"\"\"\n        with LogTask('Copying any deploy scripts'):\n            for host_name, host_spec in domains.iteritems():\n                host_metadata = host_spec.get('metadata', {})\n                deploy_scripts = self._get_scripts(host_metadata)\n                new_scripts = self._copy_delpoy_scripts(deploy_scripts)\n                self._set_scripts(\n                    host_metadata=host_metadata,\n                    scripts=new_scripts,\n                )\n\n        return domains"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies the given scripts to the scripts dir in the prefix", "response": "def _copy_delpoy_scripts(self, scripts):\n        \"\"\"\n        Copy the given deploy scripts to the scripts dir in the prefix\n\n        Args:\n            scripts(list of str): list of paths of the scripts to copy to the\n                prefix\n\n        Returns:\n            list of str: list with the paths to the copied scripts, with a\n                prefixed with $LAGO_PREFIX_PATH so the full path is not\n                hardcoded\n        \"\"\"\n        if not os.path.exists(self.paths.scripts()):\n            os.makedirs(self.paths.scripts())\n\n        new_scripts = []\n        for script in scripts:\n            script = os.path.expandvars(script)\n            if not os.path.exists(script):\n                raise RuntimeError('Script %s does not exist' % script)\n\n            sanitized_name = script.replace('/', '_')\n            new_script_cur_path = os.path.expandvars(\n                self.paths.scripts(sanitized_name)\n            )\n            shutil.copy(script, new_script_cur_path)\n\n            new_script_init_path = os.path.join(\n                '$LAGO_PREFIX_PATH',\n                os.path.basename(self.paths.scripts()),\n                sanitized_name,\n            )\n            new_scripts.append(new_script_init_path)\n\n        return new_scripts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of possible configuration files from the following sources", "response": "def _get_configs_path():\n    \"\"\"Get a list of possible configuration files, from the following\n    sources:\n    1. All files that exists in constants.CONFS_PATH.\n    2. All XDG standard config files for \"lago.conf\", in reversed\n    order of importance.\n\n\n    Returns:\n        list(str): list of files\n\n    \"\"\"\n\n    paths = []\n    xdg_paths = [\n        path for path in base_dirs.load_config_paths('lago', 'lago.conf')\n    ]\n    paths.extend([path for path in CONFS_PATH if os.path.exists(path)])\n    paths.extend(reversed(xdg_paths))\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_env_dict(root_section):\n    env_lago = defaultdict(dict)\n    decider = re.compile(\n        (\n            r'^{0}(?:_(?!_)|(?P<has>__))'\n            r'(?(has)(?P<section>.+?)__)'\n            r'(?P<name>.+)$'\n        ).format(root_section.upper())\n    )\n\n    for key, value in os.environ.iteritems():\n        match = decider.match(key)\n        if not match:\n            continue\n        if not match.group('name') or not value:\n            warn(\n                'empty environment variable definition:'\n                '{0}, ignoring.'.format(key)\n            )\n        else:\n            section = match.group('section') or root_section\n            env_lago[section.lower()][match.group('name').lower()] = value\n    return dict(env_lago)", "response": "Read all Lago variables from the environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self):\n\n        configp = configparser.ConfigParser()\n        configp.read_dict(self._defaults)\n        for path in _get_configs_path():\n            try:\n                with open(path, 'r') as config_file:\n                    configp.read_file(config_file)\n            except IOError:\n                pass\n        configp.read_dict(get_env_dict(self.root_section))\n        return {s: dict(configp.items(s)) for s in configp.sections()}", "response": "Load all configurations from available resources skip if empty"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate config dictionary with parsed args as resolved by argparse.", "response": "def update_args(self, args):\n        \"\"\"Update config dictionary with parsed args, as resolved by argparse.\n        Only root positional arguments that already exist will overridden.\n\n        Args:\n            args (namespace): args parsed by argparse\n\n        \"\"\"\n\n        for arg in vars(args):\n            if self.get(arg) and getattr(args, arg) is not None:\n                self._config[self.root_section][arg] = getattr(args, arg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_parser(self, parser):\n\n        self._parser = parser\n        ini_str = argparse_to_ini(parser)\n        configp = configparser.ConfigParser(allow_no_value=True)\n        configp.read_dict(self._config)\n        configp.read_string(ini_str)\n        self._config.update(\n            {s: dict(configp.items(s))\n             for s in configp.sections()}\n        )", "response": "Update the parser with declared arguments in an argparse. parser\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the config dictionary in INI format", "response": "def get_ini(self, incl_unset=False):\n        \"\"\"Return the config dictionary in INI format\n        Args:\n            incl_unset (bool): include variables with no defaults.\n\n        Returns:\n            str: string of the config file in INI format\n\n        \"\"\"\n\n        configp = configparser.ConfigParser(allow_no_value=True)\n        configp.read_dict(self._config)\n        with StringIO() as config_ini:\n            if self._parser:\n                self._parser.set_defaults(\n                    **self.get_section(self.root_section)\n                )\n                argparse_ini = argparse_to_ini(\n                    parser=self._parser, incl_unset=incl_unset\n                )\n                return argparse_ini\n            else:\n                configp.write(config_ini)\n                return config_ini.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request_submit(self, func, **kwargs):\n        resp = func(headers=self._get_headers(), **kwargs)\n        self._log_response_from_method(func.__name__, resp)\n        self._validate_response_success(resp)\n        return resp", "response": "A helper function that will wrap any requests. request to invoke\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the specified DNS records to a domain.", "response": "def add_records(self, domain, records):\n        \"\"\"Adds the specified DNS records to a domain.\n\n        :param domain: the domain to add the records to\n        :param records: the records to add\n        \"\"\"\n        url = self.API_TEMPLATE + self.RECORDS.format(domain=domain)\n        self._patch(url, json=records)\n        self.logger.debug('Added records @ {}'.format(records))\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the GoDaddy supplied information about a specific domain.", "response": "def get_domain_info(self, domain):\n        \"\"\"Get the GoDaddy supplied information about a specific domain.\n\n        :param domain: The domain to obtain info about.\n        :type domain: str\n\n        :return A JSON string representing the domain information\n        \"\"\"\n        url = self.API_TEMPLATE + self.DOMAIN_INFO.format(domain=domain)\n        return self._get_json_from_response(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_domains(self):\n        url = self.API_TEMPLATE + self.DOMAINS\n        data = self._get_json_from_response(url)\n\n        domains = list()\n        for item in data:\n            domain = item['domain']\n            domains.append(domain)\n            self.logger.debug('Discovered domains: {}'.format(domain))\n\n        return domains", "response": "Returns a list of domains for the authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_domain(self, domain, **kwargs):\n        update = {}\n        for k, v in kwargs.items():\n            update[k] = v\n        url = self.API_TEMPLATE + self.DOMAIN_INFO.format(domain=domain)\n        self._patch(url, json=update)\n        self.logger.info(\"Updated domain {} with {}\".format(domain, update))", "response": "Update an existing domain with new values"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_records(self, domain, record_type=None, name=None):\n\n        url = self._build_record_url(domain, record_type=record_type, name=name)\n        data = self._get_json_from_response(url)\n        self.logger.debug('Retrieved {} record(s) from {}.'.format(len(data), domain))\n\n        return data", "response": "Retrieves records from a single domain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_records(self, domain, records, record_type=None, name=None):\n\n        url = self._build_record_url(domain, name=name, record_type=record_type)\n        self._put(url, json=records)\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True", "response": "This will replace all records at the domain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the IP address in all records specified by type to the value of ip. Returns True if no exceptions occurred during the update.", "response": "def update_ip(self, ip, record_type='A', domains=None, subdomains=None):\n        \"\"\"Update the IP address in all records, specified by type, to the value of ip.  Returns True if no\n        exceptions occurred during the update.  If no domains are provided, all domains returned from\n        self.get_domains() will be updated.  By default, only A records are updated.\n\n        :param record_type: The type of records to update (eg. 'A')\n        :param ip: The new IP address (eg. '123.1.2.255')\n        :param domains: A list of the domains you want to update (eg. ['123.com','abc.net'])\n        :param subdomains: A list of the subdomains you want to update (eg. ['www','dev'])\n\n        :type record_type: str or unicode\n        :type ip: str or unicode\n        :type domains: str, list of str\n        :type subdomains: str, list of str\n\n        :return: True if no exceptions occurred\n        \"\"\"\n\n        if domains is None:\n            domains = self.get_domains()\n        elif sys.version_info < (3, 0):\n            if isinstance(domains, (str, unicode)):\n                domains = [domains]\n        elif sys.version_info >= (3, 0):\n            if isinstance(domains, str):\n                domains = [domains]\n        else:\n            # we have a tuple, set, or something else, try to convert it to a list\n            domains = list(domains)\n\n        for domain in domains:\n            a_records = self.get_records(domain, record_type=record_type)\n            for record in a_records:\n                r_name = str(record['name'])\n                r_ip = str(record['data'])\n\n                if not r_ip == ip:\n                    if (subdomains is None or\n                            (isinstance(subdomains, (unicode, str)) and r_name == subdomains) or\n                            r_name in subdomains):\n                        record.update(data=str(ip))\n                        self.update_record(domain, record)\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_records(self, domain, name, record_type=None):\n\n        records = self.get_records(domain)\n        if records is None:\n            return False  # we don't want to replace the records with nothing at all\n        save = list()\n        deleted = 0\n        for record in records:\n            if (record_type == str(record['type']) or record_type is None) and name == str(record['name']):\n                deleted += 1\n            else:\n                save.append(record)\n\n        self.replace_records(domain, records=save)\n        self.logger.info(\"Deleted {} records @ {}\".format(deleted, domain))\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True", "response": "Deletes records by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall to GoDaddy API to update a single DNS record", "response": "def update_record(self, domain, record, record_type=None, name=None):\n        \"\"\"Call to GoDaddy API to update a single DNS record\n\n        :param name: only required if the record is None (deletion)\n        :param record_type: only required if the record is None (deletion)\n        :param domain: the domain where the DNS belongs to (eg. 'example.com')\n        :param record: dict with record info (ex. {'name': 'dynamic', 'ttl': 3600, 'data': '1.1.1.1', 'type': 'A'})\n\n        :return: True if no exceptions occurred\n        \"\"\"\n        if record_type is None:\n            record_type = record['type']\n        if name is None:\n            name = record['name']\n\n        url = self.API_TEMPLATE + self.RECORDS_TYPE_NAME.format(domain=domain, type=record_type, name=name)\n        self._put(url, json=[record])\n        self.logger.info(\n            'Updated record. Domain {} name {} type {}'.format(domain, str(record['name']), str(record['type'])))\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the IP address of the record specified by type and name.", "response": "def update_record_ip(self, ip, domain, name, record_type):\n        \"\"\"Update the IP address(es) for (a) domain(s) specified by type and name.\n\n        :param ip: the new IP for the DNS record (ex. '123.1.2.255')\n        :param domain: the domain where the DNS belongs to (ex. 'example.com')\n        :param name: the DNS record name to be updated (ex. 'dynamic')\n        :param record_type: Record type (ex. 'CNAME', 'A'...)\n\n        :return: True if no exceptions occurred\n        \"\"\"\n\n        records = self.get_records(domain, name=name, record_type=record_type)\n        data = {'data': str(ip)}\n        for rec in records:\n            rec.update(data)\n            self.update_record(domain, rec)\n\n        # If we didn't get any exceptions, return True to let the user know\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse ports text e. g. 12345 13000 - 15000 20000 - 30000", "response": "def parse_ports(ports_text):\n    \"\"\"Parse ports text\n\n    e.g. ports_text = \"12345,13000-15000,20000-30000\"\n    \"\"\"\n    ports_set = set()\n    for bit in ports_text.split(','):\n        if '-' in bit:\n            low, high = bit.split('-', 1)\n            ports_set = ports_set.union(range(int(low), int(high) + 1))\n        else:\n            ports_set.add(int(bit))\n    return sorted(list(ports_set))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a new server instance", "response": "def make_server(\n        server_class, handler_class, authorizer_class, filesystem_class,\n        host_port, file_access_user=None, **handler_options):\n    \"\"\"make server instance\n\n    :host_port: (host, port)\n    :file_access_user: 'spam'\n\n    handler_options:\n\n      * timeout\n      * passive_ports\n      * masquerade_address\n      * certfile\n      * keyfile\n    \"\"\"\n    from . import compat\n    if isinstance(handler_class, compat.string_type):\n        handler_class = import_class(handler_class)\n\n    if isinstance(authorizer_class, compat.string_type):\n        authorizer_class = import_class(authorizer_class)\n\n    if isinstance(filesystem_class, compat.string_type):\n        filesystem_class = import_class(filesystem_class)\n\n    authorizer = authorizer_class(file_access_user)\n    handler = handler_class\n    for key, value in handler_options.items():\n        setattr(handler, key, value)\n    handler.authorizer = authorizer\n    if filesystem_class is not None:\n        handler.abstracted_fs = filesystem_class\n    return server_class(host_port, handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply(cls, fs):\n        logger.debug(\n            'Patching %s with %s.', fs.__class__.__name__, cls.__name__)\n        fs._patch = cls\n        for method_name in cls.patch_methods:\n            # if fs hasn't method, raise AttributeError.\n            origin = getattr(fs, method_name)\n            method = getattr(cls, method_name)\n            bound_method = method.__get__(fs, fs.__class__)\n            setattr(fs, method_name, bound_method)\n            setattr(fs, '_origin_' + method_name, origin)", "response": "replace bound methods of fs.\n            with cls."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a file exists in the S3.", "response": "def _exists(self, path):\n        \"\"\"S3 directory is not S3Ojbect.\n        \"\"\"\n        if path.endswith('/'):\n            return True\n        return self.storage.exists(path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_patch(self):\n        patch = self.patches.get(self.storage.__class__.__name__)\n        if patch:\n            patch.apply(self)", "response": "apply adjustment patch for storage\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_user(self, username):\n        return self.model.objects.filter(\n            **self._filter_user_by(username)\n        ).exists()", "response": "Returns True if user exists in database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_account(self, username):\n        try:\n            account = self.model.objects.get(\n                **self._filter_user_by(username)\n            )\n        except self.model.DoesNotExist:\n            return None\n        return account", "response": "return user by username."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_authentication(self, username, password, handler):\n        user = authenticate(\n            **{self.username_field: username, 'password': password}\n        )\n        account = self.get_account(username)\n        if not (user and account):\n            raise AuthenticationFailed(\"Authentication failed.\")", "response": "authenticate user with password"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_msg_login(self, username):\n        account = self.get_account(username)\n        if account:\n            account.update_last_login()\n            account.save()\n        return 'welcome.'", "response": "get message for welcome."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget permissions for a user", "response": "def get_perms(self, username):\n        \"\"\"return user permissions\n        \"\"\"\n        account = self.get_account(username)\n        return account and account.get_perms()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef impersonate_user(self, username, password):\n        if self.personate_user:\n            self.personate_user.impersonate_user(username, password)", "response": "impersonate user from the local cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hex_to_rgb(self, h):\n        rgb = (self.hex_to_red(h), self.hex_to_green(h), self.hex_to_blue(h))\n        return rgb", "response": "Converts a valid hex color string to an RGB array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cross product of two XYPoints.", "response": "def cross_product(self, p1, p2):\n        \"\"\"Returns the cross product of two XYPoints.\"\"\"\n        return (p1.x * p2.y - p1.y * p2.x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the distance between two XYPoints.", "response": "def get_distance_between_two_points(self, one, two):\n        \"\"\"Returns the distance between two XYPoints.\"\"\"\n        dx = one.x - two.x\n        dy = one.y - two.y\n        return math.sqrt(dx * dx + dy * dy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xy_point_from_rgb(self, red_i, green_i, blue_i):\n\n        red = red_i / 255.0\n        green = green_i / 255.0\n        blue = blue_i / 255.0\n\n        r = ((red + 0.055) / (1.0 + 0.055))**2.4 if (red > 0.04045) else (red / 12.92)\n        g = ((green + 0.055) / (1.0 + 0.055))**2.4 if (green > 0.04045) else (green / 12.92)\n        b = ((blue + 0.055) / (1.0 + 0.055))**2.4 if (blue > 0.04045) else (blue / 12.92)\n\n        X = r * 0.664511 + g * 0.154324 + b * 0.162028\n        Y = r * 0.283881 + g * 0.668433 + b * 0.047685\n        Z = r * 0.000088 + g * 0.072310 + b * 0.986039\n\n        cx = X / (X + Y + Z)\n        cy = Y / (X + Y + Z)\n\n        # Check if the given XY value is within the colourreach of our lamps.\n        xy_point = XYPoint(cx, cy)\n        in_reach = self.check_point_in_lamps_reach(xy_point)\n\n        if not in_reach:\n            xy_point = self.get_closest_point_to_point(xy_point)\n\n        return xy_point", "response": "Returns an XYPoint object containing the closest available CIE 1931 x y coordinates based on the RGB input values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninverse of `get_xy_point_from_rgb`. Returns (r, g, b) for given x, y values. Implementation of the instructions found on the Philips Hue iOS SDK docs: http://goo.gl/kWKXKl", "response": "def get_rgb_from_xy_and_brightness(self, x, y, bri=1):\n        \"\"\"Inverse of `get_xy_point_from_rgb`. Returns (r, g, b) for given x, y values.\n        Implementation of the instructions found on the Philips Hue iOS SDK docs: http://goo.gl/kWKXKl\n        \"\"\"\n        # The xy to color conversion is almost the same, but in reverse order.\n        # Check if the xy value is within the color gamut of the lamp.\n        # If not continue with step 2, otherwise step 3.\n        # We do this to calculate the most accurate color the given light can actually do.\n        xy_point = XYPoint(x, y)\n\n        if not self.check_point_in_lamps_reach(xy_point):\n            # Calculate the closest point on the color gamut triangle\n            # and use that as xy value See step 6 of color to xy.\n            xy_point = self.get_closest_point_to_point(xy_point)\n\n        # Calculate XYZ values Convert using the following formulas:\n        Y = bri\n        X = (Y / xy_point.y) * xy_point.x\n        Z = (Y / xy_point.y) * (1 - xy_point.x - xy_point.y)\n\n        # Convert to RGB using Wide RGB D65 conversion\n        r = X * 1.656492 - Y * 0.354851 - Z * 0.255038\n        g = -X * 0.707196 + Y * 1.655397 + Z * 0.036152\n        b = X * 0.051713 - Y * 0.121364 + Z * 1.011530\n\n        # Apply reverse gamma correction\n        r, g, b = map(\n            lambda x: (12.92 * x) if (x <= 0.0031308) else ((1.0 + 0.055) * pow(x, (1.0 / 2.4)) - 0.055),\n            [r, g, b]\n        )\n\n        # Bring all negative components to zero\n        r, g, b = map(lambda x: max(0, x), [r, g, b])\n\n        # If one component is greater than 1, weight components by that value.\n        max_component = max(r, g, b)\n        if max_component > 1:\n            r, g, b = map(lambda x: x / max_component, [r, g, b])\n\n        r, g, b = map(lambda x: int(x * 255), [r, g, b])\n\n        # Convert the RGB values to your color object The rgb values from the above formulas are between 0.0 and 1.0.\n        return (r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting hexadecimal colors represented as a String to approximate CIE 1931 x and y coordinates.", "response": "def hex_to_xy(self, h):\n        \"\"\"Converts hexadecimal colors represented as a String to approximate CIE\n        1931 x and y coordinates.\n        \"\"\"\n        rgb = self.color.hex_to_rgb(h)\n        return self.rgb_to_xy(rgb[0], rgb[1], rgb[2])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rgb_to_xy(self, red, green, blue):\n        point = self.color.get_xy_point_from_rgb(red, green, blue)\n        return (point.x, point.y)", "response": "Converts red green and blue integer values to approximate CIE 1931\n        x and y coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xy_to_hex(self, x, y, bri=1):\n        r, g, b = self.color.get_rgb_from_xy_and_brightness(x, y, bri)\n        return self.color.rgb_to_hex(r, g, b)", "response": "Converts CIE 1931 x and y coordinates and brightness value from 0 to 1\n        to a CSS hex color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xy_to_rgb(self, x, y, bri=1):\n        r, g, b = self.color.get_rgb_from_xy_and_brightness(x, y, bri)\n        return (r, g, b)", "response": "Converts x and y coordinates and brightness value from 0 to 1\n        to a CSS hex color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_random_xy_color(self):\n        r = self.color.random_rgb_value()\n        g = self.color.random_rgb_value()\n        b = self.color.random_rgb_value()\n        return self.rgb_to_xy(r, g, b)", "response": "Returns the approximate CIE 1931 x y coordinates represented by the\n        supplied hexColor parameter or of a random color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef become_daemon(*args, **kwargs):\n    if django.VERSION >= (1, 9):\n        from .daemonize import become_daemon\n    else:\n        from django.utils.daemonize import become_daemon\n    return become_daemon(*args, **kwargs)", "response": "Wrapper for the become_daemon function in Django 1. 9."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_keywords(icss):\n    # Force to lower case & split into keywords\n    kws=set(icss.lower().split(','))\n\n    #parameterized keywords require special handling\n    paramdict={}\n    parlist = [k for k in kws if '#' in k]\n    for k in parlist:\n        key,val=k.split('#')\n        #Take the parameterized value off the keyword...\n        kws.remove(k)\n        kws.add(key+'#')\n        #...and save it in the dictionary\n        paramdict[key+'#']=val\n    return kws, paramdict", "response": "Helper function to extract keywords from the string\n            comma - separated string\n        and return them as a list of keyword - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the entry in the internal dictionary of the internal dictionary.", "response": "def _setrow(self, row):\n        \"\"\"\n        Parameters\n        ----------\n        row : a list or tuple\n            the list or tuple containing ordered elements::\n\n                kwd, innode, outnode, compname, thcomp\n\n            followed by comments & other ignored things\n        \"\"\"\n        try:\n            compname, kwd, innode, outnode, thcomp = row[0:5]\n        except ValueError:\n            raise ValueError('Error unpacking row: %s'%row)\n\n        #Innode is an integer\n        k=int(innode)\n\n        #\"Clear\" should become None\n        if compname == 'clear':\n            compname = None\n        if thcomp == 'clear':\n            thcomp = None\n\n        #Now create the GraphNode defined by this row,\n        #and add it to the table. Default nodes are special.\n        if kwd == 'default':\n            self.tab[k].set_default(int(outnode),compname, thcomp)\n        else:\n            try:\n                self.tab[k].set_named(kwd,int(outnode),compname, thcomp)\n            except IndexError:\n                old = self.tab[k].get_named(kwd)\n                plist = (k, kwd, old, (outnode, compname, thcomp))\n                self.problemset.add(plist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_default_waveset(minwave=500, maxwave=26000, num=10000,\n                        delta=None, log=True):\n    \"\"\"Set the default wavelength set, ``pysynphot.refs._default_waveset``.\n\n    Parameters\n    ----------\n    minwave, maxwave : float, optional\n        The start (inclusive) and end (exclusive) points of the wavelength set.\n        Values should be given in linear space regardless of ``log``.\n\n    num : int, optional\n        The number of elements in the wavelength set.\n        Only used if ``delta=None``.\n\n    delta : float, optional\n        Delta between values in the wavelength set.\n        If ``log=True``, this defines wavelegth spacing in log space.\n\n    log : bool, optional\n        Determines whether the wavelength set is evenly spaced in log or linear\n        space.\n\n    \"\"\"\n    global _default_waveset\n    global _default_waveset_str\n\n    # Must be int for numpy>=1.12\n    num = int(num)\n\n    s = 'Min: %s, Max: %s, Num: %s, Delta: %s, Log: %s'\n\n    if log and not delta:\n        s = s % tuple([str(x) for x in (minwave, maxwave, num, None, log)])\n\n        logmin = np.log10(minwave)\n        logmax = np.log10(maxwave)\n\n        _default_waveset = np.logspace(logmin, logmax, num, endpoint=False)\n\n    elif log and delta:\n        s = s % tuple([str(x) for x in (minwave, maxwave, None, delta, log)])\n\n        logmin = np.log10(minwave)\n        logmax = np.log10(maxwave)\n\n        _default_waveset = 10 ** np.arange(logmin, logmax, delta)\n\n    elif not log and not delta:\n        s = s % tuple([str(x) for x in (minwave, maxwave, num, None, log)])\n\n        _default_waveset = np.linspace(minwave, maxwave, num, endpoint=False)\n\n    elif not log and delta:\n        s = s % tuple([str(x) for x in (minwave, maxwave, None, delta, log)])\n\n        _default_waveset = np.arange(minwave, maxwave, delta)\n\n    _default_waveset_str = s", "response": "Set the default wavelength set for the current system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefault refdata set on import.", "response": "def _set_default_refdata():\n    \"\"\"Default refdata set on import.\"\"\"\n    global GRAPHTABLE, COMPTABLE, THERMTABLE, PRIMARY_AREA\n    # Component tables are defined here.\n\n    try:\n        GRAPHTABLE = _refTable(os.path.join('mtab','*_tmg.fits'))\n        COMPTABLE  = _refTable(os.path.join('mtab','*_tmc.fits'))\n    except IOError as e:\n        GRAPHTABLE = None\n        COMPTABLE = None\n        warnings.warn('No graph or component tables found; '\n                      'functionality will be SEVERELY crippled. ' + str(e))\n\n    try:\n        THERMTABLE = _refTable(os.path.join('mtab','*_tmt.fits'))\n    except IOError as e:\n        THERMTABLE = None\n        warnings.warn('No thermal tables found, '\n                      'no thermal calculations can be performed. ' + str(e))\n\n    PRIMARY_AREA = 45238.93416  # cm^2 - default to HST mirror\n\n    set_default_waveset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets default refdata for a single entry in IRAF.", "response": "def setref(graphtable=None, comptable=None, thermtable=None,\n           area=None, waveset=None):\n    \"\"\"Set default graph and component tables, primary area, and\n    wavelength set.\n\n    This is similar to setting ``refdata`` in IRAF STSDAS SYNPHOT.\n    If all parameters set to `None`, they are reverted to software default.\n    If any of the parameters are not `None`, they are set to desired\n    values while the rest (if any) remain at current setting.\n\n    Parameters\n    ----------\n    graphtable, comptable, thermtable : str or `None`\n        Graph, component, and thermal table names, respectively,\n        for `~pysynphot.observationmode` throughput look-up.\n        Do not use \"*\" wildcard.\n\n    area : float or `None`\n        Telescope collecting area, i.e., the primary\n        mirror, in :math:`\\\\textnormal{cm}^{2}`.\n\n    waveset : tuple or `None`\n        Parameters for :func:`set_default_waveset` as follow:\n            * ``(minwave, maxwave, num)`` - This assumes log scale.\n            * ``(minwave, maxwave, num, 'log')``\n            * ``(minwave, maxwave, num, 'linear')``\n\n    Raises\n    ------\n    ValueError\n        Invalid ``waveset`` parameters.\n\n    \"\"\"\n    global GRAPHTABLE, COMPTABLE, THERMTABLE, PRIMARY_AREA, GRAPHDICT, COMPDICT, THERMDICT\n\n    GRAPHDICT = {}\n    COMPDICT = {}\n    THERMDICT = {}\n\n    #Check for all None, which means reset\n    kwds=set([graphtable,comptable,thermtable,area,waveset])\n    if kwds == set([None]):\n        #then we should reset everything.\n        _set_default_refdata()\n        return\n\n    #Otherwise, check them all separately\n    if graphtable is not None:\n        GRAPHTABLE = irafconvert(graphtable)\n\n    if comptable is not None:\n        COMPTABLE = irafconvert(comptable)\n\n    if thermtable is not None:\n        THERMTABLE = irafconvert(thermtable)\n\n    #Area is a bit different:\n    if area is not None:\n        PRIMARY_AREA = area\n\n    if waveset is not None:\n        if len(waveset) not in (3, 4):\n            raise ValueError('waveset tuple must contain 3 or 4 values')\n\n        minwave = waveset[0]\n        maxwave = waveset[1]\n        num = waveset[2]\n\n        if len(waveset) == 3:\n            log = True\n        elif len(waveset) == 4:\n            if waveset[3].lower() == 'log':\n                log = True\n            elif waveset[3].lower() == 'linear':\n                log = False\n            else:\n                raise ValueError('fourth waveset option must be \"log\" or \"linear\"')\n\n        set_default_waveset(minwave,maxwave,num,log=log)\n\n    #That's it.\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary mapping of parameter names to their current values.", "response": "def getref():\n    \"\"\"Current default values for graph and component tables, primary area,\n    and wavelength set.\n\n    .. note::\n\n        Also see  :func:`setref`.\n\n    Returns\n    -------\n    ans : dict\n        Mapping of parameter names to their current values.\n\n    \"\"\"\n    ans=dict(graphtable=GRAPHTABLE,\n             comptable=COMPTABLE,\n             thermtable=THERMTABLE,\n             area=PRIMARY_AREA,\n             waveset=_default_waveset_str)\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes getref but print results to screen instead of returning a dictionary.", "response": "def showref():\n    \"\"\"Like :func:`getref` but print results to screen instead of returning\n    a dictionary.\n\n    \"\"\"\n    refdata = getref()\n    for k, v in refdata.items():\n        print(\"%10s: %s\" % (k,v))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ObsBandpass(obstring, graphtable=None, comptable=None, component_dict={}):\n    ##Temporarily create an Obsmode to determine whether an\n    ##ObsModeBandpass or a TabularSpectralElement will be returned.\n    ob=ObservationMode(obstring,graphtable=graphtable,\n                       comptable=comptable,component_dict=component_dict)\n    if len(ob) > 1:\n        return ObsModeBandpass(ob)\n    else:\n        return TabularSpectralElement(ob.components[0].throughput_name)", "response": "Generate a bandpass object from an observation mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of pixels within a given wavelength range.", "response": "def pixel_range(bins, waverange, round='round'):\n    \"\"\"Returns the number of wavelength bins within ``waverange``.\n\n    Parameters\n    ----------\n    bins : ndarray\n        Wavelengths of pixel centers.\n        Must be in the same units as ``waverange``.\n\n    waverange : array_like\n        A sequence containing the wavelength range of interest. Only the\n        first and last elements are used. Assumed to be in increasing order.\n        Must be in the same units as ``bins``.\n\n    round : {'round', 'min', 'max', `None`}, optional\n        How to deal with pixels at the edges of the wavelength range. All\n        of the options, except `None`, will return an integer number of pixels.\n        Defaults to ``'round'``.\n\n            * ``'round'`` - Wavelength ends that fall in the middle of a\n              pixel are counted if more than half of the pixel is within\n              ``waverange``. Ends that fall in the center of a pixel are\n              rounded up to the nearest pixel edge.\n            * ``'min'`` - Only pixels wholly within ``waverange`` are counted.\n            * ``'max'`` - End pixels that are within ``waverange`` by any\n              margin are counted.\n            * `None` - The exact number of encompassed pixels, including\n              fractional pixels, is returned.\n\n    Returns\n    -------\n    num : int or float\n        Number of wavelength bins within ``waverange``.\n\n    Raises\n    ------\n    ValueError\n        If ``round`` is not an allowed value.\n\n    pysynphot.exceptions.OverlapError\n        If ``waverange`` exceeds the bounds of ``bins``.\n\n    \"\"\"\n    # make sure that the round keyword is valid\n    if round not in ('round','min','max',None):\n        raise ValueError(\"round keyword must be one of ('round','ciel','floor',None)\")\n\n    wave1 = waverange[0]\n    wave2 = waverange[-1]\n\n    # make sure the specified waverange is within our .binset\n    minwave = bins[0] - (bins[0:2].mean() - bins[0])\n    if wave1 < minwave:\n        raise exceptions.OverlapError(\"Lower bound of waverange is outside of binset. Min = %f\" % minwave)\n\n    maxwave = bins[-1] + (bins[-1] - bins[-2:].mean())\n    if wave2 > maxwave:\n        raise exceptions.OverlapError(\"Upper bound of waverange is outside of binset. Max = %f\" % maxwave)\n\n    # if the wavelength ends are the same return 0\n    if wave1 == wave2:\n        return 0\n\n    # find where the wavelength endpoints fall.\n    if round == 'round':\n        ind1 = bins.searchsorted(wave1, side='right')\n        ind2 = bins.searchsorted(wave2, side='right')\n\n        num = ind2 - ind1\n\n    elif round == 'min':\n        ind1 = bins.searchsorted(wave1, side='left')\n        ind2 = bins.searchsorted(wave2, side='left')\n\n        # for ind1, figure out if pixel ind1 is wholly included or not.\n        # do this by figuring out where wave1 is between ind1 and ind1-1.\n        frac = (bins[ind1] - wave1) / (bins[ind1] - bins[ind1-1])\n\n        if frac < 0.5:\n            # ind1 is only partially included\n            ind1 += 1\n\n        # similar but reversed procedure for ind2\n        frac = (wave2 - bins[ind2-1]) / (bins[ind2] - bins[ind2-1])\n\n        if frac < 0.5:\n            # ind2 is only partially included\n            ind2 -= 1\n\n        num = ind2 - ind1\n\n    elif round == 'max':\n        ind1 = bins.searchsorted(wave1, side='left')\n        ind2 = bins.searchsorted(wave2, side='left')\n\n        # for ind1, figure out if pixel ind1-1 is partially included or not.\n        # do this by figuring out where wave1 is between ind1 and ind1-1.\n        frac = (wave1 - bins[ind1-1]) / (bins[ind1] - bins[ind1-1])\n\n        if frac < 0.5:\n            # ind1 is partially included\n            ind1 -= 1\n\n        # similar but reversed procedure for ind2\n        frac = (bins[ind2] - wave2) / (bins[ind2] - bins[ind2-1])\n\n        if frac < 0.5:\n            # ind2 is partially included\n            ind2 += 1\n\n        num = ind2 - ind1\n\n    elif round is None:\n        ind1 = bins.searchsorted(wave1, side='left')\n        ind2 = bins.searchsorted(wave2, side='left')\n\n        # calculate fractional indices\n        frac1 = ind1 - (bins[ind1] - wave1) / (bins[ind1] - bins[ind1-1])\n\n        frac2 = ind2 - (bins[ind2] - wave2) / (bins[ind2] - bins[ind2-1])\n\n        num = frac2 - frac1\n\n    return num"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wave_range(bins, cenwave, npix, round='round'):\n    # make sure that the round keyword is valid\n    if round not in (None,'round','min','max'):\n        raise ValueError(\"round keyword must be one of (None,'round','min','max')\")\n\n    # make sure cenwave is within binset\n    if cenwave < bins[0]:\n        raise exceptions.OverlapError(\"cenwave is not within binset. Min = %f\" % bins[0])\n    elif cenwave > bins[-1]:\n        raise exceptions.OverlapError(\"cenwave is not within binset. Max = %f\" % bins[-1])\n\n    # first figure out what index the central wavelength falls at\n    diff = cenwave - bins\n    ind = np.where(np.abs(diff) == np.abs(diff).min())[0][0]\n\n    # now figure out the fractional index\n    if diff[ind] < 0:\n        frac_ind = float(ind) + diff[ind] / (bins[ind] - bins[ind-1])\n    elif diff[ind] > 0:\n        frac_ind = float(ind) + diff[ind] / (bins[ind+1] - bins[ind])\n    else:\n        frac_ind = float(ind)\n\n    # then figure out the fractional indices of the ends\n    frac_ind1 = frac_ind - npix/2.\n    frac_ind2 = frac_ind + npix/2.\n\n    # check ends\n    if frac_ind1 < -0.5:\n        raise exceptions.OverlapError(\"Lower wavelength range is below allowed binset.\")\n\n    if frac_ind2 > bins.shape[0] - 0.5:\n        raise exceptions.OverlapError(\"Upper wavelength range is above allowed binset.\")\n\n    frac1, int1 = np.modf(frac_ind1)\n    frac2, int2 = np.modf(frac_ind2)\n\n    # translate ends to wavelength\n    if round is None:\n        # translate ends directly to wavelength without regard to bin edges\n        f, i = frac1, int(int1)\n\n        wave1 = bins[i] + frac1 * (bins[i+1] - bins[i])\n\n        f, i = frac2, int(int2)\n\n        wave2 = bins[i] + frac2 * (bins[i+1] - bins[i])\n\n    elif round == 'round':\n        # calculate lower end of wavelength range\n        f, i = frac1, int(int1)\n\n        if f >= 0:\n            # end is somewhere greater than binset[0] so we can just\n            # interpolate between two neighboring values going with upper edge\n            wave1 = bins[i:i+2].mean()\n        else:\n            # end is below the lowest binset value, but not by enough to\n            # trigger an exception\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n\n        # calculate upper end of wavelength range\n        f, i = frac2, int(int2)\n\n        if i < bins.shape[0] - 1:\n            # end is somewhere below binset[-1] so we can just interpolate\n            # between two neighboring values, going with the upper edge.\n            wave2 = bins[i:i+2].mean()\n        else:\n            # end is above highest binset value but not by enough to\n            # trigger an exception\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n\n    elif round == 'min':\n        # calculate lower end of wavelength range\n        f, i = frac1, int(int1)\n\n        if f <= 0.5 and i < bins.shape[0] - 1:\n            # not at the lowest possible edge and pixel i included\n            wave1 = bins[i:i+2].mean()\n        elif f > 0.5 and i < bins.shape[0] - 2:\n            # not at the lowest possible edge and pixel i not included\n            wave1 = bins[i+1:i+3].mean()\n        elif f == -0.5:\n            # at the lowest possible edge\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n\n        # calculate upper end of wavelength range\n        f, i = frac2, int(int2)\n\n        if f >= 0.5 and i < bins.shape[0] - 1:\n            # not out at the end and pixel i included\n            wave2 = bins[i:i+2].mean()\n        elif f < 0.5 and i < bins.shape[0]:\n            # not out at end and pixel i not included\n            wave2 = bins[i-1:i+1].mean()\n        elif f == 0.5 and i == bins.shape[0] - 1:\n            # at the very end\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n\n    elif round == 'max':\n        # calculate lower end of wavelength range\n        f, i = frac1, int(int1)\n\n        if f < 0.5 and i < bins.shape[0]:\n            # not at the lowest possible edge and pixel i included\n            wave1 = bins[i-1:i+1].mean()\n        elif f >= 0.5 and i < bins.shape[0] - 1:\n            # not at the lowest possible edge and pixel i not included\n            wave1 = bins[i:i+2].mean()\n        elif f == -0.5:\n            # at the lowest possible edge\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n\n        # calculate upper end of wavelength range\n        f, i = frac2, int(int2)\n\n        if f > 0.5 and i < bins.shape[0] - 2:\n            # not out at the end and pixel i included\n            wave2 = bins[i+1:i+3].mean()\n        elif f <= 0.5 and i < bins.shape[0] - 1:\n            # not out at end and pixel i not included\n            wave2 = bins[i:i+2].mean()\n        elif f == 0.5 and i == bins.shape[0] - 1:\n            # at the very end\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n\n    return wave1, wave2", "response": "Return the wavelength range covered by the given number of pixels cenwave and npix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate thermal background count rate for a given set of components.", "response": "def thermback(self):\n        \"\"\"Calculate thermal background count rate for ``self.obsmode``.\n\n        Calculation uses\n        :func:`~pysynphot.observationmode.ObservationMode.ThermalSpectrum`\n        to extract thermal component source spectrum in\n        ``photlam`` per square arcsec. Then this spectrum is\n        integrated and multiplied by detector pixel scale\n        and telescope collecting area to produce a count rate\n        in count/s/pix. This unit is non-standard but used widely\n        by STScI Exposure Time Calculator.\n\n        .. note::\n\n            Similar to IRAF STSDAS SYNPHOT ``thermback`` task.\n\n        Returns\n        -------\n        ans : float\n            Thermal background count rate.\n\n        Raises\n        ------\n        NotImplementedError\n            Bandpass has no thermal information in graph table.\n\n        \"\"\"\n        #The obsmode.ThermalSpectrum method will raise an exception if there is\n        #no thermal information, and that will just propagate up.\n        sp=self.obsmode.ThermalSpectrum()\n\n        #Thermback is always provided in this non-standard set of units.\n        #This code was copied from etc.py.\n        ans = sp.integrate() * (self.obsmode.pixscale**2 *\n                                self.obsmode.primary_area)\n        return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pixel_range(self, waverange, waveunits=None, round='round'):\n        # make sure we have a binset to work with\n        if self.binset is None:\n            raise exceptions.UndefinedBinset('No binset specified for this bandpass.')\n\n        # start by converting waverange to self.waveunits, if necessary\n        if waveunits is not None:\n            waveunits = units.Units(waveunits)\n\n            if not isinstance(waverange,np.ndarray):\n                waverange = np.array(waverange)\n\n            # convert to angstroms and then whatever self.waveunits is\n            waverange = waveunits.ToAngstrom(waverange)\n\n            waverange = units.Angstrom().Convert(waverange, self.waveunits.name)\n\n        return pixel_range(self.binset, waverange, round=round)", "response": "Return the number of pixels within a given wavelength range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the wavelength range covered by the given number of pixels cenwave and waveunits.", "response": "def wave_range(self, cenwave, npix, waveunits=None, round='round'):\n        \"\"\"Get the wavelength range covered by the given number of pixels\n        centered on the given wavelength.\n\n        .. note::\n\n           This calls :func:`wave_range` with ``self.binset``\n           as the first argument.\n\n        Parameters\n        ----------\n        cenwave, npix, round\n            See :func:`wave_range`.\n\n        waveunits : str, optional\n            Wavelength units of ``cenwave`` and the returned wavelength range.\n            If `None` (default), the wavelengths are assumed to be in\n            the units of ``self.waveunits``.\n\n        Returns\n        -------\n        waverange : tuple of floats\n            The range of wavelengths spanned by ``npix`` centered on\n            ``cenwave``.\n\n        Raises\n        ------\n        pysynphot.exceptions.UndefinedBinset\n            If ``self.binset`` is None.\n\n        \"\"\"\n        # make sure we have a binset to work with\n        if self.binset is None:\n            raise exceptions.UndefinedBinset('No binset specified for this bandpass.')\n\n        # convert cenwave from waveunits to self.waveunits, if necessary\n        if waveunits is not None:\n            waveunits = units.Units(waveunits)\n\n            # convert to angstroms and then whatever self.waveunits is\n            cenwave = waveunits.ToAngstrom(cenwave)\n            cenwave = units.Angstrom().Convert(cenwave, self.waveunits.name)\n\n        wave1, wave2 = wave_range(self.binset, cenwave, npix, round=round)\n\n        # translate ends to waveunits, if necessary\n        if waveunits is not None:\n            # convert to angstroms\n            wave1 = self.waveunits.ToAngstrom(wave1)\n            wave2 = self.waveunits.ToAngstrom(wave2)\n\n            # then to waveunits\n            wave1 = units.Angstrom().Convert(wave1, waveunits.name)\n            wave2 = units.Angstrom().Convert(wave2, waveunits.name)\n\n        return wave1, wave2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_bin_edges(centers):\n    centers = np.asanyarray(centers)\n\n    if centers.ndim != 1:\n        raise ValueError('centers input array must be 1D.')\n\n    if centers.size < 2:\n        raise ValueError('centers input must have at least two values.')\n\n    edges = np.empty(centers.size + 1)\n\n    edges[1:-1] = (centers[1:] + centers[:-1]) / 2.\n\n    #compute the first and last by making them symmetric\n    edges[0] = centers[0] - (edges[1] - centers[0])\n    edges[-1] = centers[-1] + (centers[-1] - edges[-2])\n\n    return edges", "response": "Calculates the edges of wavelength bins given the centers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the widths of wavelengths bins given their edges.", "response": "def calculate_bin_widths(edges):\n    \"\"\"\n    Calculate the widths of wavelengths bins given their edges.\n\n    Parameters\n    ----------\n    edges : array_like\n        Sequence of bin edges. Must be 1D and have at least two values.\n\n    Returns\n    -------\n    widths : ndarray\n        Array of bin widths. Will be 1D and have one less value than ``edges``.\n\n    \"\"\"\n    edges = np.asanyarray(edges)\n\n    if edges.ndim != 1:\n        raise ValueError('edges input array must be 1D.')\n\n    if edges.size < 2:\n        raise ValueError('edges input must have at least two values.')\n\n    return edges[1:] - edges[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the centers of wavelengths bins given their edges.", "response": "def calculate_bin_centers(edges):\n    \"\"\"\n    Calculate the centers of wavelengths bins given their edges.\n\n    Parameters\n    ----------\n    edges : array_like\n        Sequence of bin edges. Must be 1D and have at least two values.\n\n    Returns\n    -------\n    centers : ndarray\n        Array of bin centers. Will be 1D and have one less value than ``edges``.\n\n    \"\"\"\n    edges = np.asanyarray(edges, dtype=np.float)\n\n    if edges.ndim != 1:\n        raise ValueError('edges input array must be 1D.')\n\n    if edges.size < 2:\n        raise ValueError('edges input must have at least two values.')\n\n    centers = np.empty(edges.size - 1, dtype=np.float)\n\n    centers[0] = edges[:2].mean()\n\n    for i in range(1, centers.size):\n        centers[i] = 2. * edges[i] - centers[i - 1]\n\n    return centers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the union of the two wavelength sets.", "response": "def MergeWaveSets(waveset1, waveset2):\n    \"\"\"Return the union of the two wavelength sets.\n\n    The union is computed using `numpy.union1d`, unless one or\n    both of them is `None`.\n\n    The merged result may sometimes contain numbers which are nearly\n    equal but differ at levels as small as 1E-14. Having values this\n    close together can cause problems due to effectively duplicate\n    wavelength values. Therefore, wavelength values having differences\n    smaller than or equal to ``pysynphot.spectrum.MERGETHRESH``\n    (defaults to 1E-12) are considered as the same.\n\n    Parameters\n    ----------\n    waveset1, waveset2 : array_like or `None`\n        Wavelength sets to combine.\n\n    Returns\n    -------\n    MergedWaveSet : array_like or `None`\n        Merged wavelength set. It is `None` if both inputs are such.\n\n    \"\"\"\n    if waveset1 is None and waveset2 is not None:\n        MergedWaveSet = waveset2\n    elif waveset2 is None and waveset1 is not None:\n        MergedWaveSet = waveset1\n    elif waveset1 is None and waveset2 is None:\n        MergedWaveSet = None\n    else:\n        MergedWaveSet = N.union1d(waveset1, waveset2)\n\n        # The merged wave sets may sometimes contain numbers which are nearly\n        # equal but differ at levels as small as 1e-14. Having values this\n        # close together can cause problems down the line so here we test\n        # whether any such small differences are present, with a small\n        # difference defined as less than MERGETHRESH.\n        #\n        # If small differences are present we make a copy of the union'ed array\n        # with the lower of the close together pairs removed.\n        delta = MergedWaveSet[1:] - MergedWaveSet[:-1]\n\n        if not (delta > MERGETHRESH).all():\n            newlen = len(delta[delta > MERGETHRESH]) + 1\n            newmerged = N.zeros(newlen, dtype=MergedWaveSet.dtype)\n            newmerged[:-1] = MergedWaveSet[:-1][delta > MERGETHRESH]\n            newmerged[-1] = MergedWaveSet[-1]\n\n            MergedWaveSet = newmerged\n\n    return MergedWaveSet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new spectrum with trimmed upper and lower limits.", "response": "def trimSpectrum(sp, minw, maxw):\n    \"\"\"Create a new spectrum with trimmed upper and lower ranges.\n\n    Parameters\n    ----------\n    sp : `SourceSpectrum`\n        Spectrum to trim.\n\n    minw, maxw : number\n        Lower and upper limits (inclusive) for the wavelength set\n        in the trimmed spectrum.\n\n    Returns\n    -------\n    result : `TabularSourceSpectrum`\n        Trimmed spectrum.\n\n    \"\"\"\n    wave = sp.GetWaveSet()\n    flux = sp(wave)\n\n    new_wave = N.compress(wave >= minw, wave)\n    new_flux = N.compress(wave >= minw, flux)\n\n    new_wave = N.compress(new_wave <= maxw, new_wave)\n    new_flux = N.compress(new_wave <= maxw, new_flux)\n\n    result = TabularSourceSpectrum()\n\n    result._wavetable = new_wave\n    result._fluxtable = new_flux\n\n    result.waveunits = units.Units(sp.waveunits.name)\n    result.fluxunits = units.Units(sp.fluxunits.name)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming trapezoid integration. Parameters ---------- x : array_like Wavelength set. y : array_like Integrand. For example, throughput or throughput multiplied by wavelength. Returns ------- sum : float Integrated sum.", "response": "def trapezoidIntegration(self, x, y):\n        \"\"\"Perform trapezoid integration.\n\n        Parameters\n        ----------\n        x : array_like\n            Wavelength set.\n\n        y : array_like\n            Integrand. For example, throughput or throughput\n            multiplied by wavelength.\n\n        Returns\n        -------\n        sum : float\n            Integrated sum.\n\n        \"\"\"\n        npoints = x.size\n        if npoints > 0:\n            indices = N.arange(npoints)[:-1]\n            deltas = x[indices+1] - x[indices]\n            integrand = 0.5*(y[indices+1] + y[indices])*deltas\n            sum = integrand.sum()\n            if x[-1] < x[0]:\n                sum *= -1.0\n            return sum\n        else:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _columnsFromASCII(self, filename):\n\n        wlist = []\n        flist = []\n        lcount = 0\n        if filename.lower().startswith(('http://', 'ftp://')):\n            lines = get_file_contents(filename)\n        else:\n            with open(filename) as fs:\n                lines = fs.readlines()\n        for line in lines:\n            lcount += 1\n            cline = line.strip()\n            if ((len(cline) > 0) and (not cline.startswith('#'))):\n                try:\n                    cols = cline.split()\n                    if len(cols) >= 2:\n                        wlist.append(float(cols[0]))\n                        flist.append(float(cols[1]))\n                except Exception as e:\n                    raise exceptions.BadRow(\"Error reading %s: %s\" % (\n                        filename, str(e)), rows=lcount)\n\n        return wlist, flist", "response": "This routine extracts the first two columns from an ASCII file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the wavelength array.", "response": "def validate_wavetable(self):\n        \"\"\"Enforce monotonic, ascending wavelength array with no zero or\n        negative values.\n\n        Raises\n        ------\n        pysynphot.exceptions.DuplicateWavelength\n            Wavelength array contains duplicate entries.\n\n        pysynphot.exceptions.UnsortedWavelength\n            Wavelength array is not monotonic ascending or descending.\n\n        pysynphot.exceptions.ZeroWavelength\n            Wavelength array has zero or negative value(s).\n\n        \"\"\"\n        # First check for invalid values\n        wave = self._wavetable\n        if N.any(wave <= 0):\n            wrong = N.where(wave <= 0)[0]\n            raise exceptions.ZeroWavelength(\n                'Negative or Zero wavelength occurs in wavelength array',\n                rows=wrong)\n\n        # Now check for monotonicity & enforce ascending\n        sorted = N.sort(wave)\n        if not N.alltrue(sorted == wave):\n            if N.alltrue(sorted[::-1] == wave):\n                # monotonic descending is allowed\n                pass\n            else:\n                wrong = N.where(sorted != wave)[0]\n                raise exceptions.UnsortedWavelength(\n                    'Wavelength array is not monotonic', rows=wrong)\n\n        # Check for duplicate values\n        dw = sorted[1:] - sorted[:-1]\n        if N.any(dw == 0):\n            wrong = N.where(dw == 0)[0]\n            raise exceptions.DuplicateWavelength(\n                \"Wavelength array contains duplicate entries\", rows=wrong)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking for non - negative fluxes.", "response": "def validate_fluxtable(self):\n        \"\"\"Check for non-negative fluxes.\n        If found, the negative flux values are set to zero, and\n        a warning is printed to screen. This check is not done\n        if flux unit is a magnitude because negative magnitude\n        values are legal.\n\n        \"\"\"\n        # neg. magnitudes are legal\n        if ((not self.fluxunits.isMag) and (self._fluxtable.min() < 0)):\n            idx = N.where(self._fluxtable < 0)\n            self._fluxtable[idx] = 0.0\n            print(\"Warning, %d of %d bins contained negative fluxes; they \"\n                  \"have been set to zero.\" % (\n                      len(idx[0]), len(self._fluxtable)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addmag(self, magval):\n        if N.isscalar(magval):\n            factor = 10**(-0.4*magval)\n            return self*factor\n        else:\n            raise TypeError(\".addmag() only takes a constant scalar argument\")", "response": "Adds a scalar magnitude to existing flux values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns wavelength and flux arrays in user units.", "response": "def getArrays(self):\n        \"\"\"Return wavelength and flux arrays in user units.\n\n        Returns\n        -------\n        wave : array_like\n            Wavelength array in ``self.waveunits``.\n\n        flux : array_like\n            Flux array in ``self.fluxunits``.\n            When necessary, ``self.primary_area`` is used for unit conversion.\n\n        \"\"\"\n        if hasattr(self, 'primary_area'):\n            area = self.primary_area\n        else:\n            area = None\n\n        wave = self.GetWaveSet()\n        flux = self(wave)\n\n        flux = units.Photlam().Convert(\n            wave, flux, self.fluxunits.name, area=area)\n        wave = units.Angstrom().Convert(wave, self.waveunits.name)\n\n        return wave, flux"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that wavelenth and flux units belong to the correct classes.", "response": "def validate_units(self):\n        \"\"\"Ensure that wavelenth and flux units belong to the\n        correct classes.\n\n        Raises\n        ------\n        TypeError\n            Wavelength unit is not `~pysynphot.units.WaveUnits` or\n            flux unit is not `~pysynphot.units.FluxUnits`.\n\n        \"\"\"\n        if (not isinstance(self.waveunits, units.WaveUnits)):\n            raise TypeError(\"%s is not a valid WaveUnit\" % self.waveunits)\n        if (not isinstance(self.fluxunits, units.FluxUnits)):\n            raise TypeError(\"%s is not a valid FluxUnit\" % self.fluxunits)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef writefits(self, filename, clobber=True, trimzero=True,\n                  binned=False, precision=None, hkeys=None):\n        \"\"\"Write the spectrum to a FITS table.\n\n        Primary header in EXT 0. ``FILENAME``,  ``ORIGIN``, and any\n        extra keyword(s) from ``hkeys`` will also be added.\n\n        Table header and data are in EXT 1. The table has 2 columns,\n        i.e., ``WAVELENGTH`` and ``FLUX``. Data are stored in user units.\n        Its header also will have these additional keywords:\n\n        * ``EXPR`` - Description of the spectrum.\n        * ``TDISP1`` and ``TDISP2`` - Columns display format,\n          always \"G15.7\".\n        * ``GRFTABLE`` and ``CMPTABLE`` - Graph and component\n          table names to use with associated observation mode.\n          These are only added if applicable.\n\n        If data is already double-precision but user explicitly\n        set output precision to single, ``pysynphot.spectrum.syn_epsilon``\n        defines the allowed minimum wavelength separation.\n        This limit (:math:`3.2 \\\\times 10^{-4}`) was taken from\n        IRAF STSDAS SYNPHOT FAQ.\n        Values equal or smaller than this limit are considered as the\n        same, and duplicates are ignored, resulting in data loss.\n        In the way that this comparison is coded, when such precision clash\n        happens, even when no duplicates are detected, the last row is\n        always omitted (also data loss). Therefore, it is *not* recommended\n        for user to force single-precision when the data is in\n        double-precision.\n\n        Parameters\n        ----------\n        filename : str\n            Output filename.\n\n        clobber : bool\n            Overwrite existing file. Default is `True`.\n\n        trimzero : bool\n            Trim off duplicate rows with flux values of zero from both ends\n            of the spectrum. This keeps one row of zero-flux at each end,\n            if it exists; However, it does not add a zero-flux row if it\n            does not. Default is `True`.\n\n        binned : bool\n            Write ``self.binwave`` and ``self.binflux`` (binned) dataset,\n            instead of ``self.wave`` and ``self.flux`` (native). Using\n            this option when object does not have binned data will cause\n            an exception to be raised. Default is `False`.\n\n        precision : {'s', 'd', `None`}\n            Write data out in single (``'s'``) or double (``'d'``)\n            precision. Default is `None`, which will enforce native\n            precision from ``self.flux``.\n\n        hkeys : dict\n            Additional keyword(s) to be added to primary FITS header,\n            in the format of ``{keyword:(value,comment)}``.\n\n        \"\"\"\n        pcodes={'d':'D', 's':'E'}\n        if precision is None:\n            precision = self.flux.dtype.char\n        _precision = precision.lower()[0]\n        pcodes = {'d':'D','s':'E','f':'E'}\n\n        if clobber:\n            try:\n                os.remove(filename)\n            except OSError:\n                pass\n\n        if binned:\n            wave = self.binwave\n            flux = self.binflux\n        else:\n            wave = self.wave\n            flux = self.flux\n\n        # Add a check for single/double precision clash, so\n        # that if written out in single precision, the wavelength table\n        # will still be sorted with no duplicates\n        # The value of epsilon is taken from the Synphot FAQ.\n\n        if wave.dtype == N.float64 and _precision == 's':\n            idx = N.where(abs(wave[1:]-wave[:-1]) > syn_epsilon)\n        else:\n            idx = N.where(wave) #=> idx=[:]\n\n        wave = wave[idx]\n        flux = flux[idx]\n\n        first, last = 0, len(flux)\n\n        if trimzero:\n            # Keep one zero at each end\n            nz = flux.nonzero()[0]\n            try:\n                first = max(nz[0] - 1, first)\n                last = min(nz[-1] + 2, last)\n            except IndexError:\n                pass\n\n        # Construct the columns and HDUlist\n        cw = pyfits.Column(name='WAVELENGTH',\n                           array=wave[first:last],\n                           unit=self.waveunits.name,\n                           format=pcodes[_precision])\n        cf = pyfits.Column(name='FLUX',\n                           array=flux[first:last],\n                           unit=self.fluxunits.name,\n                           format=pcodes[_precision])\n\n        # Make the primary header\n        hdu = pyfits.PrimaryHDU()\n        hdulist = pyfits.HDUList([hdu])\n\n        # User-provided keys are written to the primary header\n        # so are filename and origin\n        bkeys = dict(filename=(os.path.basename(filename), 'name of file'),\n                     origin=('pysynphot', 'Version (%s, %s)' %\n                             (__version__, __svn_revision__)))\n        # User-values if present may override default values\n        if hkeys is not None:\n            bkeys.update(hkeys)\n\n        # Now update the primary header\n        for key, val in bkeys.items():\n            hdu.header[key] = val\n\n        # Make the extension HDU\n        cols = pyfits.ColDefs([cw, cf])\n        hdu = pyfits.BinTableHDU.from_columns(cols)\n\n        # There are some standard keywords that should be added\n        # to the extension header.\n        bkeys = dict(expr=(str(self), 'pysyn expression'),\n                     tdisp1=('G15.7',),\n                     tdisp2=('G15.7',))\n\n        try:\n            bkeys['grftable'] = (self.bandpass.obsmode.gtname,)\n            bkeys['cmptable'] = (self.bandpass.obsmode.ctname,)\n        except AttributeError:\n            pass  # Not all spectra have these\n\n        for key, val in bkeys.items():\n            hdu.header[key] = val\n\n        # Add the header to the list, and write the file\n        hdulist.append(hdu)\n        hdulist.writeto(filename)", "response": "Write the spectrum to a FITS file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nintegrate the flux in given units.", "response": "def integrate(self, fluxunits='photlam'):\n        \"\"\"Integrate the flux in given unit.\n\n        Integration is done using :meth:`~Integrator.trapezoidIntegration`\n        with ``x=wave`` and ``y=flux``, where flux has been\n        convert to given unit first.\n\n        .. math::\n\n            \\\\textnormal{result} = \\\\int F_{\\\\lambda} d\\\\lambda\n\n        Parameters\n        ----------\n        fluxunits : str\n            Flux unit to integrate in.\n\n        Returns\n        -------\n        result : float\n            Integrated sum. Its unit should take account of the\n            integration over wavelength. For example, if\n            ``fluxunits='photlam'`` is given, then its unit\n            is ``photon/s/cm^2``.\n\n        \"\"\"\n        # Extract the flux in the desired units\n        u = self.fluxunits\n        self.convert(fluxunits)\n        wave, flux = self.getArrays()\n        self.convert(u)\n        # then do the integration\n        return self.trapezoidIntegration(wave, flux)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, wave, interp=True):\n        if interp:\n            # convert input wavelengths to Angstroms since the __call__ method\n            # will be expecting that\n            angwave = self.waveunits.ToAngstrom(wave)\n\n            # First use the __call__ to get it in photlam\n            flux = self(angwave)\n\n            if hasattr(self, 'primary_area'):\n                area = self.primary_area\n            else:\n                area = None\n\n            # Then convert to the desired units\n            ans = units.Photlam().Convert(angwave, flux,\n                                          self.fluxunits.name, area=area)\n\n        else:\n            # Get the arrays in the proper units\n            wave_array, flux_array = self.getArrays()\n            if N.isscalar(wave):\n                # Find the correct index\n                diff = abs(wave-wave_array)\n                idx = diff.argmin()\n\n                ans = flux_array[idx]\n\n            else:\n                raise NotImplementedError(\n                    \"Interp=False not yet supported for non-scalars\")\n\n        return ans", "response": "Sample the user unit from the given wavelengths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert(self, targetunits):\n        nunits = units.Units(targetunits)\n\n        if nunits.isFlux:\n            self.fluxunits = nunits\n        else:\n            self.waveunits = nunits", "response": "Set new user unit for either wavelength or flux."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redshift(self, z):\n        # By default, apply only the doppler shift.\n\n        waveunits = self.waveunits\n        fluxunits = self.fluxunits\n        self.convert('angstrom')\n        self.convert('photlam')\n        newwave = self.wave.astype(N.float64) * (1.0 + z)\n        copy = ArraySourceSpectrum(wave=newwave,\n                                   flux=self.flux,\n                                   waveunits=self.waveunits,\n                                   fluxunits=self.fluxunits,\n                                   name=\"%s at z=%g\" % (self.name, z))\n\n        self.convert(waveunits)\n        self.convert(fluxunits)\n        return copy", "response": "Return a copy of the array with redshift z."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setMagnitude(self, band, value):\n        objectFlux = band.calcTotalFlux(self)\n        vegaFlux = band.calcVegaFlux()\n        magDiff = -2.5*math.log10(objectFlux/vegaFlux)\n        factor = 10**(-0.4*(value - magDiff))\n        return self * factor", "response": "Makes the magnitude of the source in the band equal to value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new copy of the Spectrum with the given value and unit and bandpass renamed to the given value.", "response": "def renorm(self, RNval, RNUnits, band, force=False):\n        \"\"\":ref:`Renormalize <pysynphot-renorm>` the spectrum to the\n        specified value, unit, and bandpass.\n\n        This wraps :func:`~pysynphot.renorm.StdRenorm` for convenience.\n        Basically, the spectrum is multiplied by a numeric factor so that\n        the total integrated flux will be the given value in the given\n        unit in the given bandpass.\n\n        When ``force=False``, if spectrum is not fully defined within the\n        given bandpass, but the overlap is at least 99%, a warning is\n        printed to screen and ``self.warnings['PartialRenorm']`` is set\n        to `True`.\n\n        Parameters\n        ----------\n        RNval : number\n            Flux value for renormalization.\n\n        RNUnits : str\n            Unit name, as accepted by `~pysynphot.units.Units`, for ``RNval``.\n\n        band : `SpectralElement`\n            Bandpass that ``RNval`` is based on.\n\n        force : bool\n            Force renormalization regardless of overlap status with given\n            bandpass. If `True`, overlap check is skipped. Default is `False`.\n\n        Returns\n        -------\n        newsp : `~pysynphot.spectrum.CompositeSourceSpectrum`\n            Renormalized spectrum.\n\n        Raises\n        ------\n        ValueError\n            Integrated flux is zero, negative, NaN, or infinite.\n\n        pysynphot.exceptions.DisjointError\n            Spectrum and bandpass are disjoint.\n\n        pysynphot.exceptions.OverlapError\n            Spectrum and bandpass do not fully overlap.\n\n        \"\"\"\n        from .renorm import StdRenorm\n        return StdRenorm(self, band, RNval, RNUnits, force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all components and sub - components.", "response": "def complist(self):\n        \"\"\"Return a list of all components and sub-components.\n        This is for use with :py:meth:`~object.__iter__`.\n\n        \"\"\"\n        ans = []\n        for comp in (self.component1, self.component2):\n            try:\n                ans.extend(comp.complist())\n            except AttributeError:\n                ans.append(comp)\n        return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetWaveSet(self):\n        waveset1 = self.component1.GetWaveSet()\n        waveset2 = self.component2.GetWaveSet()\n        return MergeWaveSets(waveset1, waveset2)", "response": "Obtain the wavelength set for the composite spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tabulate(self):\n        sp = ArraySourceSpectrum(wave=self.wave,\n                                 flux=self.flux,\n                                 waveunits=self.waveunits,\n                                 fluxunits=self.fluxunits,\n                                 name='%s (tabulated)' % self.name)\n        return sp", "response": "Return a simplified version of the spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _readASCII(self, filename):\n        self.waveunits = units.Units('angstrom')\n        self.fluxunits = units.Units('flam')\n        wlist, flist = self._columnsFromASCII(filename)\n        self._wavetable = N.array(wlist, dtype=N.float64)\n        self._fluxtable = N.array(flist, dtype=N.float64)", "response": "Read the ASCII file and store the wavelength and flux table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef taper(self):\n        OutSpec = TabularSourceSpectrum()\n        wcopy = N.zeros(self._wavetable.size+2, dtype=N.float64)\n        fcopy = N.zeros(self._fluxtable.size+2, dtype=N.float64)\n        wcopy[1:-1] = self._wavetable\n        fcopy[1:-1] = self._fluxtable\n        fcopy[0] = 0.0\n        fcopy[-1] = 0.0\n\n        # The wavelengths to use for the first and last points are\n        # calculated by using the same ratio as for the 2 interior points\n        wcopy[0] = wcopy[1]*wcopy[1]/wcopy[2]\n        wcopy[-1] = wcopy[-2]*wcopy[-2]/wcopy[-3]\n\n        OutSpec._wavetable = wcopy\n        OutSpec._fluxtable = fcopy\n        OutSpec.waveunits = units.Units(str(self.waveunits))\n        OutSpec.fluxunits = units.Units(str(self.fluxunits))\n\n        return OutSpec", "response": "Taper the spectrum by adding zero flux to each end."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ToInternal(self):\n        self.validate_units()\n\n        savewunits = self.waveunits\n        savefunits = self.fluxunits\n\n        if hasattr(self, 'primary_area'):\n            area = self.primary_area\n        else:\n            area = None\n\n        angwave = self.waveunits.Convert(self.GetWaveSet(), 'angstrom')\n        phoflux = self.fluxunits.Convert(angwave, self._fluxtable, 'photlam',\n                                         area=area)\n\n        self._wavetable = angwave.copy()\n        self._fluxtable = phoflux.copy()\n\n        self.waveunits = savewunits\n        self.fluxunits = savefunits", "response": "Convert to the internal representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the wavelength set that optimally samples the Gaussian curve.", "response": "def GetWaveSet(self):\n        \"\"\"Return the wavelength set that optimally samples the Gaussian curve.\n        It has 101 values, as defined below:\n\n        .. math::\n\n            x_{\\\\textnormal{first,last}} = x_{0} \\\\; \\\\pm \\\\; 5 \\\\; \\\\sigma\n\n            \\\\delta x = 0.1 \\\\; \\\\sigma\n\n        Returns\n        -------\n        waveset : array_like\n            Wavelength set in internal unit.\n\n        \"\"\"\n        increment = 0.1*self.sigma\n        first = self.center - 50.0*increment\n        last = self.center + 50.0*increment\n        waveset = N.arange(first, last, increment)\n        return self._input_wave_units.Convert(waveset, 'angstrom')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef redshift(self, z):\n        tmp = SourceSpectrum.redshift(self, z)\n        ans = FlatSpectrum(tmp.flux.max(), fluxunits=tmp.fluxunits)\n        return ans", "response": "Apply redshift to the redshifted flat spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_units(self):\n        if (not isinstance(self.waveunits, units.WaveUnits)):\n            raise TypeError(\"%s is not a valid WaveUnit\" % self.waveunits)", "response": "Ensure that the wavelenth unit belongs to the correct class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef integrate(self, wave=None):\n\n        if wave is None:\n            wave = self.wave\n        ans = self.trapezoidIntegration(wave, self(wave))\n        return ans", "response": "Integrate the throughput over the specified wavelength set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avgwave(self):\n        mywaveunits = self.waveunits.name\n        self.convert('angstroms')\n\n        wave = self.wave\n        thru = self.throughput\n        self.convert(mywaveunits)\n\n        num = self.trapezoidIntegration(wave, thru*wave)\n        den = self.trapezoidIntegration(wave, thru)\n\n        if 0.0 in (num, den):\n            return 0.0\n        else:\n            return num/den", "response": "Calculate :ref:`pysynphot-formula-avgwv`.\n\n        Returns\n        -------\n        ans : float\n            Average wavelength."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pivot(self, binned=False):\n        if binned:\n            try:\n                wave = self.binwave\n            except AttributeError:\n                raise AttributeError('Class ' + str(type(self)) +\n                                     ' does not support binning.')\n        else:\n            wave = self.wave\n\n        countmulwave = self(wave)*wave\n        countdivwave = self(wave)/wave\n\n        num = self.trapezoidIntegration(wave, countmulwave)\n        den = self.trapezoidIntegration(wave, countdivwave)\n\n        if num == 0.0 or den == 0.0:\n            return 0.0\n\n        return math.sqrt(num/den)", "response": "Calculate the pivot of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the RMS band width of a single object.", "response": "def rmswidth(self, floor=0):\n        \"\"\"Calculate :ref:`pysynphot-formula-rmswidth`.\n\n        Parameters\n        ----------\n        floor : float\n            Throughput values equal or below this threshold are not\n            included in the calculation. By default (0), all points\n            are included.\n\n        Returns\n        -------\n        ans : float\n            RMS band width.\n\n        \"\"\"\n        mywaveunits = self.waveunits.name\n        self.convert('angstroms')\n\n        wave = self.wave\n        thru = self.throughput\n        self.convert(mywaveunits)\n\n        if floor != 0:\n            idx = N.where(thru >= floor)\n            wave = wave[idx]\n            thru = thru[idx]\n\n        integrand = (wave-self.avgwave())**2 * thru\n        num = self.trapezoidIntegration(wave, integrand)\n        den = self.trapezoidIntegration(wave, thru)\n\n        if 0.0 in (num, den):\n            return 0.0\n        else:\n            ans = math.sqrt(num/den)\n            return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the photbw of the current object.", "response": "def photbw(self, floor=0):\n        \"\"\"Calculate :ref:`pysynphot-formula-bandw`.\n\n        .. note:: For backward-compatibility with IRAF STSDAS SYNPHOT only.\n\n        Parameters\n        ----------\n        floor : float\n            Same as :meth:`rmswidth`.\n\n        Returns\n        -------\n        ans : float\n            RMS band width (deprecated).\n\n        \"\"\"\n        mywaveunits = self.waveunits.name\n        self.convert('angstroms')\n\n        wave = self.wave\n        thru = self.throughput\n        self.convert(mywaveunits)\n\n        # calculate the average wavelength\n        num = self.trapezoidIntegration(wave, thru * N.log(wave) / wave)\n        den = self.trapezoidIntegration(wave, thru / wave)\n\n        if num == 0 or den == 0:\n            return 0.0\n\n        avg_wave = N.exp(num/den)\n\n        if floor != 0:\n            idx = N.where(thru >= floor)\n            wave = wave[idx]\n            thru = thru[idx]\n\n        # calcualte the rms width\n        integrand = thru * N.log(wave / avg_wave)**2 / wave\n        num = self.trapezoidIntegration(wave, integrand)\n\n        if num == 0 or den == 0:\n            return 0.0\n\n        return avg_wave * N.sqrt(num/den)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rectwidth(self):\n        mywaveunits = self.waveunits.name\n        self.convert('angstroms')\n\n        wave = self.wave\n        thru = self.throughput\n        self.convert(mywaveunits)\n\n        num = self.trapezoidIntegration(wave, thru)\n        den = thru.max()\n\n        if 0.0 in (num, den):\n            return 0.0\n        else:\n            return num/den", "response": "Calculate the rectangular width of the Bandpass."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef efficiency(self):\n        mywaveunits = self.waveunits.name\n        self.convert('angstroms')\n\n        wave = self.wave\n        thru = self.throughput\n        self.convert(mywaveunits)\n\n        ans = self.trapezoidIntegration(wave, thru/wave)\n        return ans", "response": "Calculate the bandpass dimensionless efficiency."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the two sources have the same throughput.", "response": "def check_sig(self, other):\n        \"\"\"Check overlap insignificance with another spectrum.\n        Also see :ref:`pysynphot-command-checko`.\n\n        .. note::\n\n            Only use when :meth:`check_overlap` returns \"partial\".\n\n        Parameters\n        ----------\n        other : `SourceSpectrum` or `SpectralElement`\n            The other spectrum.\n\n        Returns\n        -------\n        ans : bool\n            `True` means the *lack* of overlap is *insignificant*\n            (i.e., okay to proceed).\n\n        \"\"\"\n        swave = self.wave[N.where(self.throughput != 0)]\n        s1, s2 = swave.min(), swave.max()\n\n        owave = other.wave\n        o1, o2 = owave.min(), owave.max()\n\n        lorange = sorted([s1, o1])\n        hirange = sorted([s2, o2])\n\n        # Get the full throughput\n        total = self.integrate()\n\n        # Now get the other two pieces\n        # We cannot yet do\n        # low = self[slice(*lowrange)].integrate()\n        wave = self.wave\n        idxs = [N.searchsorted(wave, lorange, 'left'),\n                N.searchsorted(wave, hirange, 'left')]\n\n        excluded = 0.0\n        for idx in idxs:\n            try:\n                excluded += self.integrate(wave=wave[slice(*idx)])\n            except IndexError:\n                pass  # If the range is zero, do nothing\n\n        if excluded/total < 0.01:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_overlap(self, other):\n        if other.isAnalytic and not isinstance(other, Box):\n            # then it's defined everywhere, except for Box\n            return 'full'\n\n        swave = self.wave[N.where(self.throughput != 0)]\n        s1, s2 = swave.min(), swave.max()\n\n        owave = other.wave\n        o1, o2 = owave.min(), owave.max()\n\n        if (s1 >= o1 and s2 <= o2):\n            ans = 'full'\n\n        elif (s2 < o1) or (o2 < s1):\n            ans = 'none'\n\n        else:\n            ans = 'partial'\n\n        return ans", "response": "Check whether two sources have the same frequency as the given one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(self, targetunits):\n        nunits = units.Units(targetunits)\n        self.waveunits = nunits", "response": "Set new user unit for wavelength only."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, wave):\n        angwave = self.waveunits.ToAngstrom(wave)\n        return self.__call__(angwave)", "response": "Sample the frequency spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntapering the spectrum by adding zero throughput to each end.", "response": "def taper(self):\n        \"\"\"Taper the spectrum by adding zero throughput to each end.\n        This is similar to :meth:`TabularSourceSpectrum.taper`.\n\n        There is no check to see if the spectrum is already tapered.\n        Hence, calling this on a tapered spectrum will result in\n        multiple zero-throughput entries at both ends.\n\n        The wavelengths to use for the new first and last points are\n        calculated by using the same ratio as for the two interior points\n        used at each end.\n\n        Returns\n        -------\n        OutElement : `TabularSpectralElement`\n            Tapered spectrum.\n\n        \"\"\"\n        OutElement = TabularSpectralElement()\n\n        wcopy = N.zeros(self._wavetable.size + 2, dtype=N.float64)\n        fcopy = N.zeros(self._throughputtable.size + 2, dtype=N.float64)\n\n        wcopy[1:-1] = self._wavetable\n        fcopy[1:-1] = self._throughputtable\n\n        fcopy[0] = 0.0\n        fcopy[-1] = 0.0\n\n        # The wavelengths to use for the first and last points are\n        # calculated by using the same ratio as for the 2 interior points\n        wcopy[0] = wcopy[1]*wcopy[1]/wcopy[2]\n        wcopy[-1] = wcopy[-2]*wcopy[-2]/wcopy[-3]\n\n        OutElement._wavetable = wcopy\n        OutElement._throughputtable = fcopy\n\n        return OutElement"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unit_response(self):\n        hc = units.HC\n\n        if hasattr(self, 'primary_area'):\n            area = self.primary_area\n        else:\n            area = refs.PRIMARY_AREA\n\n        wave = self.GetWaveSet()\n        thru = self(wave)\n\n        return hc / (area * self.trapezoidIntegration(wave, thru*wave))", "response": "Calculate Bandpass unit response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetWaveSet(self):\n        wave1 = self.component1.GetWaveSet()\n        wave2 = self.component2.GetWaveSet()\n        return MergeWaveSets(wave1, wave2)", "response": "Obtain the wavelength set for the composite spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef writefits(self, *args, **kwargs):\n        old_wave = self.wave\n        self.wave = self._wavetable\n\n        try:\n            super(UniformTransmission, self).writefits(*args, **kwargs)\n        finally:\n            self.wave = old_wave", "response": "Write to file using default waveset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ToInternal(self):\n        self.validate_units()\n        savewunits = self.waveunits\n        angwave = self.waveunits.Convert(self._wavetable, 'angstrom')\n        self._wavetable = angwave.copy()\n        self.waveunits = savewunits", "response": "Convert wavelengths to the internal representation of angstroms."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the ASCII file and set the internal internal", "response": "def _readASCII(self, filename):\n        \"\"\"ASCII files have no headers. Following synphot, this\n        routine will assume the first column is wavelength in Angstroms,\n        and the second column is throughput (dimensionless).\"\"\"\n        self.waveunits = units.Units('angstrom')\n        self.throughputunits = 'none'\n        wlist, tlist = self._columnsFromASCII(filename)\n        self._wavetable = N.array(wlist, dtype=N.float64)\n        self._throughputtable = N.array(tlist, dtype=N.float64)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new time series from the given wavelengths.", "response": "def sample(self, wavelength):\n        \"\"\"Input wavelengths assumed to be in user unit.\"\"\"\n        wave = self.waveunits.Convert(wavelength, 'angstrom')\n        return self(wave)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resample(self, resampledWaveTab):\n        return ArraySpectralElement(\n            wave=resampledWaveTab.copy(), waveunits='angstrom',\n            throughput=self(resampledWaveTab).copy())", "response": "Resample the spectrum for the given wavelength set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bandWave(self):\n        if self.binset.startswith('('):\n            return self._computeBandwave(self.binset)\n        else:\n            return self._getBandwaveFomFile(self.binset)", "response": "Return the binned wavelength set most appropriate for the\n        observation mode as defined by pysynphot. locations. wavelengthcat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the combined throughput from multiplying all the components together.", "response": "def Throughput(self):\n        \"\"\"Combined throughput from multiplying all the components together.\n\n        Returns\n        -------\n        throughput : `~pysynphot.spectrum.TabularSpectralElement` or `None`\n            Combined throughput.\n\n        \"\"\"\n        try:\n            throughput = spectrum.TabularSpectralElement()\n\n            product = self._multiplyThroughputs(0)\n\n            throughput._wavetable = product.GetWaveSet()\n            throughput._throughputtable = product(throughput._wavetable)\n            throughput.waveunits = product.waveunits\n            throughput.name='*'.join([str(x) for x in self.components])\n\n##            throughput = throughput.resample(spectrum._default_waveset)\n\n            return throughput\n\n        except IndexError:   # graph table is broken.\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ThermalSpectrum(self):\n        try:\n            # delegate to subclass.\n            thom = _ThermalObservationMode(self._obsmode)\n            self.pixscale = thom.pixscale\n            return thom._getSpectrum()\n        except IndexError:   # graph table is broken.\n            raise IndexError(\"Cannot calculate thermal spectrum; graphtable may be broken\")", "response": "Calculate thermal spectrum.\n\n        Returns\n        -------\n        sp : `~pysynphot.spectrum.CompositeSourceSpectrum`\n            Thermal spectrum in ``photlam``.\n\n        Raises\n        ------\n        IndexError\n            Calculation failed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of opaque components that can be used to multiply the throughput of the opaque components.", "response": "def _multiplyThroughputs(self):\n        ''' Overrides base class in order to deal with opaque components.\n        '''\n        index = 0\n        for component in self.components:\n            if component.throughput != None:\n                break\n            index += 1\n\n        return BaseObservationMode._multiplyThroughputs(self, index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a reverse lookup dictionary for the input dictionary", "response": "def reverse(d):\n    \"\"\"Return a reverse lookup dictionary for the input dictionary\"\"\"\n    r={}\n    for k in d:\n        r[d[k]]=k\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_red_laws():\n    laws = {}\n\n    # start by converting the Cache.RedLaws file names to RedLaw objects\n    # if they aren't already\n    for k in Cache.RedLaws:\n        if isinstance(Cache.RedLaws[k],str):\n            Cache.RedLaws[k] = RedLaw(Cache.RedLaws[k])\n\n        laws[str(k)] = Cache.RedLaws[k].litref\n\n    # get the length of the longest name and litref\n    maxname = max([len(name) for name in laws.keys()])\n    maxref = max([len(ref) for ref in laws.values()])\n\n    s = '%-' + str(maxname) + 's   %-' + str(maxref) + 's'\n\n    print(s % ('name','reference'))\n    print(s % ('-'*maxname,'-'*maxref))\n\n    for k in sorted(laws.keys()):\n        print(s % (k, laws[k]))", "response": "Print available extinction laws to screen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Extinction(extval,name=None):\n   try:\n       ext=Cache.RedLaws[name].reddening(extval)\n   except AttributeError:\n       #The cache hasn't yet been filled.\n       Cache.RedLaws[name]=RedLaw(Cache.RedLaws[name])\n       ext=Cache.RedLaws[name].reddening(extval)\n   except KeyError:\n       #There's no reddening law by that name. See if we've been\n       #given a filename from which we can read one.\n       try:\n           Cache.RedLaws[name]=RedLaw(name)\n           ext=Cache.RedLaws[name].reddening(extval)\n       except IOError:\n           #If not, see if it's an old extinction law\n           try:\n               ext=extinction.DeprecatedExtinction(extval,name)\n           except KeyError:\n               raise ValueError('No extinction law has been defined for \"%s\", and no such file exists'%name)\n   return ext", "response": "Generate extinction curve for a given reddening law."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning wavelength in user units.", "response": "def _getWaveProp(self):\n        \"\"\"Return wavelength in user units.\"\"\"\n        wave = units.Angstrom().Convert(self._wavetable, self.waveunits.name)\n        return wave"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reddening(self,extval):\n        T = 10.0**(-0.4*extval*self.obscuration)\n        ans = ExtinctionSpectralElement(wave=self.wave,\n                                        waveunits=self.waveunits,\n                                        throughput=T,\n                                        name='%s(EBV=%g)'%(self.name, extval))\n        ans.citation = self.litref\n        return ans", "response": "Compute the reddening for the given extinction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetNextNode(self, modes, innode):\n        nodes = N.where(self.innodes == innode)\n\n        ## If there's no entry for the given innode, return -1\n        if nodes[0].size == 0:\n            return -1\n\n        ## If we don't match anything in the modes list, we find the\n        ## outnode corresponding the the string 'default'\n        defaultindex = N.where(self.keywords[nodes] == 'default')\n\n        if len(defaultindex[0]) != 0:\n            outnode = self.outnodes[nodes[0][defaultindex[0]]]\n\n        ## Now try and match one of the strings in the modes list with\n        ## the keywords corresponding to the list of entries with the given\n        ## innode\n        for mode in modes:\n            result = self.keywords[nodes].count(mode)\n            if result != 0:\n                index = N.where(self.keywords[nodes]==mode)\n                outnode = self.outnodes[nodes[0][index[0]]]\n\n\n        ## Return the outnode corresponding either to the matched mode,\n        ## or to 'default'\n        return outnode", "response": "Get the next outnode that matches an element from the modes list starting at the given innode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetComponentsFromGT(self, modes, innode):\n        components = []\n        thcomponents = []\n        outnode = 0\n        inmodes=set(modes)\n        used_modes=set()\n        count = 0\n        while outnode >= 0:\n            if (DEBUG and (outnode < 0)):\n                print(\"outnode == %d: stop condition\"%outnode)\n\n            previous_outnode = outnode\n\n            nodes = N.where(self.innodes == innode)\n\n            # If there are no entries with this innode, we're done\n            if nodes[0].size == 0:\n                if DEBUG:\n                    print(\"no such innode %d: stop condition\"%innode)\n                #return (components,thcomponents)\n                break\n\n            # Find the entry corresponding to the component named\n            # 'default', bacause thats the one we'll use if we don't\n            # match anything in the modes list\n            defaultindex = N.where(self.keywords[nodes] =='default')\n\n            if 'default' in self.keywords[nodes]:\n                dfi=N.where(self.keywords[nodes] == 'default')[0][0]\n                outnode = self.outnodes[nodes[0][dfi]]\n                component = self.compnames[nodes[0][dfi]]\n                thcomponent = self.thcompnames[nodes[0][dfi]]\n                used_default=True\n            else:\n                #There's no default, so fail if you don't match anything\n                # in the keyword matching step.\n                outnode = -2\n                component = thcomponent = None\n\n            # Now try and match something from the modes list\n            for mode in modes:\n\n                if mode in self.keywords[nodes]:\n                    used_modes.add(mode)\n                    index = N.where(self.keywords[nodes]==mode)\n                    if len(index[0])>1:\n                        raise KeyError('%d matches found for %s'%(len(index[0]),mode))\n                    idx=index[0][0]\n                    component = self.compnames[nodes[0][idx]]\n                    thcomponent = self.thcompnames[nodes[0][idx]]\n                    outnode = self.outnodes[nodes[0][idx]]\n                    used_default=False\n\n            if DEBUG:\n                print(\"Innode %d  Outnode %d  Compname %s\"%(innode, outnode, component))\n            components.append(component)\n            thcomponents.append(thcomponent)\n\n\n            innode = outnode\n\n            if outnode == previous_outnode:\n                if DEBUG:\n                    print(\"Innode: %d  Outnode:%d  Used default: %s\"%(innode, outnode,used_default))\n                count += 1\n                if count > 3:\n                    if DEBUG:\n                        print(\"same outnode %d > 3 times: stop condition\"%outnode)\n                    break\n\n        if (outnode < 0):\n            if DEBUG:\n                print(\"outnode == %d: stop condition\"%outnode)\n            raise ValueError(\"Incomplete obsmode %s\"%str(modes))\n\n\n        #Check for unused modes\n        if inmodes != used_modes:\n            unused=str(inmodes.difference(used_modes))\n            raise ValueError(\"Warning: unused keywords %s\"%unused)\n\n        return (components,thcomponents)", "response": "This function returns the components from the given observation mode keywords and starting node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexamine the column names to determine which type of file this is. Return a tuple of the type of file and whether it is non - parameterized or contains error column", "response": "def classify_file(f):\n    \"\"\"Examine the column names to determine which type of file\n    this is. Return a tuple:\n    retvalue[0] = \"file is non-parameterized\"\n    retvalue[1] = \"file contains error column\"\n    \"\"\"\n    cols=f[1].columns\n    if len(cols) == 2:\n        #Then we must have a simple file\n        return (True,False)\n    elif len(cols) == 3 and ('ERROR' in cols.names):\n        return (True,True)\n    elif len(cols) > 2 and ('ERROR' not in cols.names):\n        return (True,False)\n    else:\n        return (False,True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blue(fname,outdir=None):\n    if outdir is not None and not os.path.isdir(outdir):\n        os.mkdir(outdir)\n\n    dwave=0.001\n    f=pyfits.open(fname)\n    nonparam,error=classify_file(f)\n    #Check whether a fix is needed\n    if nonparam: #just check column 1 = throughput\n        t=f[1].data.field(1)\n        ok = (t[0]==0.0) and (t[-1]==0.0)\n    else:\n        ok=False\n        cols=f[1].columns\n        for k in range(1,len(cols)):\n            if 'err' not in cols[k].name.lower():\n                t=f[1].data.field(k)\n                ok = ok and ( (t[0]==0.0) and (t[-1]==0.0) )\n\n    if ok:\n        f.close()\n        return \"%s: no change necessary\"%fname\n\n\n    #If not ok, we fall through to here.\n    #We're going to have to make a new hdu\n    clist=[]\n    #And we need column info from the existing hdu\n    cols=f[1].columns\n    #Always make a new wavelength array\n    w=f[1].data.field(0)\n    nrows=len(w)\n    wave=N.zeros((nrows+1,),dtype=w.dtype)\n    wave[1:]=w\n    wave[0]=w[0]-dwave\n\n    wc=pyfits.Column(name=cols[0].name,\n                     format=cols[0].format,\n                     array=wave)\n    clist.append(wc)\n    #Stick zeros on the ends of all other columns, unless\n    #there's an \"ERR\" in their name\n\n    padval={True:100,False:0}\n    for k in range(1,len(cols)):\n        t=f[1].data.field(k)\n        thru=N.zeros((nrows+1,),dtype=t.dtype)\n        thru[1:]=t\n        pv=padval[(\"err\" in cols.names[k].lower())]\n        thru[0]=pv\n        tc=pyfits.Column(name=cols[k].name,\n                         format=cols[k].format,\n                         array=thru)\n        clist.append(tc)\n\n    #Now we have all the columns; put them in an HDU\n    tbhdu=pyfits.BinTableHDU.from_columns(clist)\n    #and copy in the header information we need\n    h=f[1].header.copy()\n    h.update('NAXIS1',tbhdu.header['NAXIS1'])\n    h.update('NAXIS2',tbhdu.header['NAXIS2'])\n    tbhdu.header=h\n\n\n    #Replace the HDU\n    f[1]=tbhdu\n\n    #Update the primary header\n    f[0].header.add_history(time.asctime())\n    f[0].header.add_history(\"..Added a point with throughput=0 at blue end of the\")\n    f[0].header.add_history(\"..throughput curves (dLambda=.001A) using the script\")\n    f[0].header.add_history(\"..extrap.py v%s. V. Laidler\"%__version__)\n\n    #And write out the new file\n    if outdir is not None:\n        newname=os.path.join(outdir,fincre(fname))\n    else:\n        newname=fincre(fname)\n    f.writeto(newname)\n\n    return(\"%s -> %s\"%(fname, newname))", "response": "Cut & pasted from run but to operate only on blue end."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fincre(oldname):\n    x=re.search('_(?P<version>[0-9][0-9][0-9])_syn',oldname)\n    version=x.group('version')\n    newversion=\"%03d\"%(int(version)+1)\n    ans=oldname.replace(version,newversion)\n    return ans", "response": "Increment the synphot version number from a filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_crbox(self,spstring,form,output=\"\",wavecat=\"INDEF\",\n                  lowave=0,hiwave=30000):\n        \"\"\"Calcspec has a bug. We will use countrate instead, and force it\n        to use a box function of uniform transmission as the obsmode.\"\"\"\n\n        range=hiwave-lowave\n        midwave=range/2.0\n        iraf.countrate(spectrum=spstring, magnitude=\"\",\n                       instrument=\"box(%f,%f)\"%(midwave,range),\n                       form=form,\n                       wavecat=wavecat,\n                       output=output)", "response": "Run the crbox command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef savepysyn(self,wave,flux,fname,units=None):\n        if units is None:\n            ytype='throughput'\n            units=' '\n        else:\n            ytype='flux'\n        col1=pyfits.Column(name='wavelength',format='D',array=wave)\n        col2=pyfits.Column(name=ytype,format='D',array=flux)\n        tbhdu=pyfits.BinTableHDU.from_columns(pyfits.ColDefs([col1,col2]))\n        tbhdu.header.update('tunit1','angstrom')\n        tbhdu.header.update('tunit2',units)\n        tbhdu.writeto(fname.replace('.fits','_pysyn.fits'))", "response": "Save the wave and flux to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_overlap(a, b):\n    if a.isAnalytic or b.isAnalytic:\n        #then it's defined everywhere\n        result = 'full'\n    else:\n        #get the wavelength arrays\n        waves = list()\n        for x in (a, b):\n            if hasattr(x,'throughput'):\n                wv = x.wave[np.where(x.throughput != 0)]\n            elif hasattr(x,'flux'):\n                wv = x.wave\n            else:\n                raise AttributeError(\"neither flux nor throughput in %s\"%x)\n            waves.append(wv)\n\n        #get the endpoints\n        a1,a2 = waves[0].min(), waves[0].max()\n        b1,b2 = waves[1].min(), waves[1].max()\n\n        #do the comparison\n        if (a1>=b1 and a2<=b2):\n            result = 'full'\n        elif (a2<b1) or (b2<a1):\n            result = 'none'\n        else:\n            result = 'partial'\n\n    return result", "response": "Check if wavelengths overlap between two spectra."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_overlap(comp1, comp2, force):\n    warnings = dict()\n    if force is None:\n        stat = comp2.check_overlap(comp1)\n        if stat=='full':\n            pass\n        elif stat == 'partial':\n            raise(exceptions.PartialOverlap('Spectrum and bandpass do not fully overlap. You may use force=[extrap|taper] to force this Observation anyway.'))\n        elif stat == 'none':\n            raise(exceptions.DisjointError('Spectrum and bandpass are disjoint'))\n\n    elif force.lower() == 'taper':\n        try:\n            comp1=comp1.taper()\n        except AttributeError:\n            comp1=comp1.tabulate().taper()\n            warnings['PartialOverlap']=force\n\n    elif force.lower().startswith('extrap'):\n        #default behavior works, but check the overlap so we can set the warning\n        stat=comp2.check_overlap(comp1)\n        if stat == 'partial':\n            warnings['PartialOverlap']=force\n\n    else:\n        raise(KeyError(\"Illegal value force=%s; legal values=('taper','extrap')\"%force))\n    return comp1, comp2, warnings", "response": "Validate the overlap between two components."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_overlap(self,force):\n\n        #Wrap the function for convenience\n        self.spectrum, self.bandpass, warn = validate_overlap(self.spectrum,\n                                                              self.bandpass, force)\n        self.warnings.update(warn)", "response": "Validate that spectrum and bandpass overlap."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets self. binwave to the binset of the source spectrum.", "response": "def initbinset(self,binset=None):\n        \"\"\"Set ``self.binwave``.\n\n        By default, wavelength values for binning are inherited\n        from bandpass. If the bandpass has no binning information,\n        then source spectrum wavelengths are used. However, if\n        user provides values, then those are used without question.\n\n        Parameters\n        ----------\n        binset : array_like or `None`\n            Wavelength values to be used for binning when converting to counts.\n\n        \"\"\"\n        if binset is None:\n            msg=\"(%s) does not have a defined binset in the wavecat table. The waveset of the spectrum will be used instead.\"%str(self.bandpass)\n\n            try:\n                self.binwave = self.bandpass.binset\n            except (KeyError, AttributeError):\n                self.binwave = self.spectrum.wave\n                print(msg)\n\n            if self.binwave is None:\n                self.binwave = self.spectrum.wave\n                print(msg)\n        else:\n            self.binwave=binset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initbinflux(self):\n        endpoints = binning.calculate_bin_edges(self.binwave)\n\n        # merge these endpoints in with the natural waveset\n        spwave = spectrum.MergeWaveSets(self.wave, endpoints)\n        spwave = spectrum.MergeWaveSets(spwave,self.binwave)\n\n        # compute indices associated to each endpoint.\n        indices = np.searchsorted(spwave, endpoints)\n        self._indices = indices[:-1]\n        self._indices_last = indices[1:]\n\n        # prepare integration variables.\n        flux = self(spwave)\n        avflux = (flux[1:] + flux[:-1]) / 2.0\n        self._deltaw = spwave[1:] - spwave[:-1]\n\n        # sum over each bin.\n        if utils_imported is True:\n            self._binflux, self._intwave = \\\n              pysynphot_utils.calcbinflux(len(self.binwave),\n                                          self._indices,\n                                          self._indices_last,\n                                          avflux,\n                                          self._deltaw)\n        else:\n            #Note that, like all Python striding, the range over which\n            #we integrate is [first:last).\n            self._binflux = np.empty(shape=self.binwave.shape,dtype=np.float64)\n            self._intwave = np.empty(shape=self.binwave.shape,dtype=np.float64)\n            for i in range(len(self._indices)):\n                first = self._indices[i]\n                last = self._indices_last[i]\n                self._binflux[i]=(avflux[first:last]*self._deltaw[first:last]).sum()/self._deltaw[first:last].sum()\n                self._intwave[i]=self._deltaw[first:last].sum()\n\n        #Save the endpoints for future use\n        self._bin_edges = endpoints", "response": "Initialize binned flux and edges for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes pysynphot. spectrum. SourceSpectrum. writefits but with clobber = True as default.", "response": "def writefits(self,fname,clobber=True, trimzero=True, binned=True,\n                  hkeys=None):\n        \"\"\"Like :meth:`pysynphot.spectrum.SourceSpectrum.writefits`\n        but with ``binned=True`` as default.\n\n        \"\"\"\n        spectrum.CompositeSourceSpectrum.writefits(self,fname,\n                                                   clobber=clobber,\n                                                   trimzero=trimzero,\n                                                   binned=binned,\n                                                   hkeys=hkeys)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef countrate(self,binned=True,range=None,force=False):\n\n        if self._binflux is None:\n          self.initbinflux()\n\n        myfluxunits = self.fluxunits.name\n        self.convert('counts')\n        warn=False\n        if binned:\n            #No range specified - use full range\n            if range is None:\n                lx,ux=(None,None)\n            #Range is disjoint from binwave\n            elif (range[0]>self.binwave[-1] or\n                  range[1]<self.binwave[0]):\n                raise exceptions.DisjointError(\"%s is disjoint from obs.binwave %s\"%(range,\n                                                                       [self.binwave[0],self.binwave[-1]]))\n            #Partial overlap\n            else:\n\n                if range[0] < self._bin_edges[0]:\n                    warn=True\n                    lx=None\n                else:\n                    lx=np.searchsorted(self._bin_edges,range[0])-1\n\n                if range[1] > self._bin_edges[-1]:\n                    warn=True\n                    ux=None\n                else:\n                    ux=np.searchsorted(self._bin_edges,range[1])\n\n\n            ans = math.fsum(self.binflux[lx:ux])\n            if warn and not force:\n                raise exceptions.PartialOverlap(\"%s does not fully overlap binwave range %s. Countrate in overlap area is %f\"%(range,[self.binwave[0],self.binwave[-1]],ans))\n\n        else:\n            if range is None:\n                ans = math.fsum(self.flux)\n            else:\n                raise NotImplementedError(\"Sorry, range+binned=False not yet implemented\")\n        self.convert(myfluxunits)\n        return ans", "response": "Calculates effective stimulus in count / s."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes effective stimulus for a given flux unit.", "response": "def effstim(self,fluxunits='photlam'):\n        \"\"\"Compute :ref:`effective stimulus <pysynphot-formula-effstim>`.\n\n        Calculations are done in given flux unit, and wavelengths\n        in Angstrom. Native dataset is used.\n\n        Parameters\n        ----------\n        fluxunits : str\n            Flux unit.\n\n        Returns\n        -------\n        ans : float\n            Effective stimulus.\n\n        Raises\n        ------\n        ValueError\n            Invalid integrated flux.\n\n        \"\"\"\n        oldunits=self.fluxunits\n        self.convert(fluxunits)\n        x=units.Units(fluxunits)\n        try:\n            if x.isDensity:\n                rate=self.integrate()\n                self._fluxcheck(rate)\n                if x.isMag:\n                    ans=x.unitResponse(self.bandpass) - 2.5*math.log10(rate)\n                else:\n                    ans=rate*x.unitResponse(self.bandpass)\n            else:\n                if x.isMag:\n                    #its linear unit must be counts\n                    self.convert('counts')\n                    total=self.flux.sum()\n                    self._fluxcheck(total)\n                    ans=-2.5*math.log10(total)\n                else:\n                    ans=self.flux.sum()\n                    self._fluxcheck(ans)\n        finally:\n            self.convert(oldunits)\n            del x\n\n        return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pivot(self,binned=True):\n        if binned:\n            wave = self.binwave\n        else:\n            wave = self.wave\n\n        countmulwave = self(wave)*wave\n        countdivwave = self(wave)/wave\n\n        num = self.trapezoidIntegration(wave,countmulwave)\n        den = self.trapezoidIntegration(wave,countdivwave)\n\n        if num == 0.0 or den == 0.0:\n            return 0.0\n\n        return math.sqrt(num/den)", "response": "Calculate the pivot wavelength of the observation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef efflam(self,binned=True):\n        myfluxunits=self.fluxunits.name\n        self.convert('flam')\n        if binned:\n            wave=self.binwave\n            flux=self.binflux\n        else:\n            wave=self.wave\n            flux=self.flux\n\n        num = self.trapezoidIntegration(wave,flux*wave*wave)\n        den = self.trapezoidIntegration(wave,flux*wave)\n        self.convert(myfluxunits)\n\n        if num == 0.0 or den == 0.0:\n            return 0.0\n\n        return num/den", "response": "Calculates the effective wavelength of the observation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample the observation at the given wavelength.", "response": "def sample(self, swave, binned=True, fluxunits='counts'):\n        \"\"\"Sample the observation at the given wavelength.\n        Also see :ref:`pysynphot-command-sample`.\n\n        Parameters\n        ----------\n        swave : float\n            Wavelength to sample.\n\n        binned : bool\n            Sample binned dataset (no interpolation).\n            Otherwise, native (perform interpolation).\n\n        fluxunits : {'counts'}\n            Only the unit of counts is supported for now.\n\n        Returns\n        -------\n        ans : float\n            Sampled flux in given unit.\n\n        Raises\n        ------\n        NotImplementedError\n            Flux unit is not supported or non-scalar wavelength is given.\n\n        ValueError\n            Given wavelength out of range.\n\n        \"\"\"\n        if self._binflux is None:\n          self.initbinflux()\n\n        if fluxunits != 'counts':\n            s = \"Sorry, only counts are supported at this time\"\n            raise NotImplementedError(s)\n        else:\n            #Save current fluxunits, in case they're different\n            saveunits = None\n            if not units.ismatch('counts', self.fluxunits):\n                saveunits = self.fluxunits\n                self.convert('counts')\n\n\n        if binned:\n            #Then we don't interpolate, just return the appropriate values\n            #from binflux\n            if np.isscalar(swave):\n                #Find the bin in which it belongs.\n                #_bin_edge[i] is the low edge of the bin centered\n                #at binwave[i].\n\n                idx = np.where(swave >= self._bin_edges)[0]\n                #idx[-1] is the largest edge that is still smaller\n                #than swave\n                try:\n                    ans = self.binflux[idx[-1]]\n                except IndexError:\n                    s = 'Value out of range: wavelength %g not contained in range [%g, %g]'\n                    s = s % (swave, self.binwave[0], self.binwave[-1])\n                    raise ValueError(s)\n\n            else:\n                #The logic for this case doesn't yet work on arrays\n                s = \"Sorry, only scalar samples are supported at this time\"\n                raise NotImplementedError(s)\n\n        else:\n            #Then we do interpolate on wave/flux\n            if np.isscalar(swave):\n                delta = 0.00001\n                wv = np.array([swave - delta, swave, swave + delta])\n                ans = np.interp(wv, self.wave, self.flux)[1]\n            else:\n                # This raises UnboundLocalError -- needs to be fixed!\n                ans = np.interp(wv, self.wave, self.flux)\n\n\n        #Change units back, if necessary, then return\n        if saveunits is not None:\n            self.convert(saveunits)\n        return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreduces the observation to a simple spectrum object.", "response": "def as_spectrum(self, binned=True):\n        \"\"\"Reduce the observation to a simple spectrum object.\n\n        An observation is a complex object with some restrictions on its\n        capabilities. At times, it would be useful to work with the\n        simulated observation as a simple object that is easier to\n        manipulate and takes up less memory.\n\n        Parameters\n        ----------\n        binned : bool\n            If `True` (default), export binned dataset. Otherwise, native.\n\n        Returns\n        -------\n        result : `~pysynphot.spectrum.ArraySourceSpectrum`\n            Observation dataset as a simple spectrum object.\n\n        \"\"\"\n        if binned:\n            wave, flux = self.binwave, self.binflux\n        else:\n            wave, flux = self.wave, self.flux\n\n        result = ArraySourceSpectrum(wave, flux,\n                                     self.waveunits,\n                                     self.fluxunits,\n                                     name = self.name,\n                                     keepneg = True)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef irafconvert(iraffilename):\n    convertdict = CONVERTDICT\n\n    # remove duplicate separators and extraneous relative paths\n    if not iraffilename.lower().startswith(('http', 'ftp')):\n        iraffilename = os.path.normpath(iraffilename)\n\n    # BUG: supports environment variables only as the leading element in the\n    #      filename\n    if iraffilename.startswith('$'):\n        # Then this is an environment variable.\n        # Use a regex to pull off the front piece.\n        pat = re.compile(r'\\$(\\w*)')\n        match = re.match(pat, iraffilename)\n        dirname = match.group(1)\n        unixdir = os.environ[dirname]\n        basename = iraffilename[match.end() + 1:]  # 1 to omit leading slash\n        unixfilename = os.path.join(unixdir, basename)\n        return unixfilename\n    elif '$' in iraffilename:\n        # Then it's an iraf-style variable\n        irafdir, basename = iraffilename.split('$')\n        if irafdir == 'synphot':\n            return get_data_filename(os.path.basename(basename))\n        unixdir = convertdict[irafdir]\n        unixfilename = os.path.join(unixdir, basename)\n        return unixfilename\n    else:\n        # If no $ sign found, just return the filename unchanged\n        return iraffilename", "response": "Convert the IRAF file name to its Unix equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap filename to its actual path.", "response": "def get_data_filename(filename):\n    \"\"\"Map filename to its actual path.\n\n    Parameters\n    ----------\n    filename : str\n        Filename to search.\n\n    Returns\n    -------\n    path : str\n        Full path to the file in data directory.\n\n    \"\"\"\n    global _data_map\n\n    if _data_map is None:\n        _data_map = {}\n        for root, dirs, files in os.walk(specdir):\n            for fname in files:\n                _data_map[fname] = os.path.join(root, fname)\n\n    if filename not in _data_map:\n        raise KeyError(filename + ' not found in ' + specdir)\n    return _data_map[filename]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef DefineStdSpectraForUnits():\n    # Linear flux-density units\n    units.Flam.StdSpectrum = FlatSpectrum(1, fluxunits='flam')\n    units.Fnu.StdSpectrum = FlatSpectrum(1, fluxunits='fnu')\n    units.Photlam.StdSpectrum = FlatSpectrum(1, fluxunits='photlam')\n    units.Photnu.StdSpectrum = FlatSpectrum(1, fluxunits='photnu')\n    units.Jy.StdSpectrum = FlatSpectrum(1, fluxunits='jy')\n    units.mJy.StdSpectrum = FlatSpectrum(1, fluxunits='mjy')\n\n    # Non-density units\n    scale = 1.0 / _default_waveset.size\n    units.Counts.StdSpectrum = FlatSpectrum(1, fluxunits='counts') * scale\n    units.OBMag.StdSpectrum = FlatSpectrum(1, fluxunits='counts') * scale\n\n    # Magnitude flux-density units\n    units.ABMag.StdSpectrum = FlatSpectrum(3.63e-20, fluxunits='fnu')\n    units.STMag.StdSpectrum = FlatSpectrum(3.63e-9, fluxunits='flam')\n    units.VegaMag.StdSpectrum = Vega", "response": "Define the standard spectrum for all the supported flux units."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef StdRenorm(spectrum, band, RNval, RNunitstring, force=False):\n    # Validate the overlap\n    if not force:\n        stat = band.check_overlap(spectrum)\n        if stat == 'full':\n            pass\n        elif stat == 'partial':\n            if band.check_sig(spectrum):\n                spectrum.warnings['PartialRenorm'] = True\n                print ('Warning: Spectrum is not defined everywhere in '\n                       'renormalization bandpass. At least 99% of the band '\n                       'throughput has data, therefore proceeding anyway. '\n                       'Spectrum will be extrapolated at constant value.')\n            else:\n                raise OverlapError('Spectrum and renormalization band do not '\n                                   'fully overlap. You may use force=True to '\n                                   'force the renormalization to proceed.')\n        elif stat == 'none':\n            raise DisjointError('Spectrum and renormalization band are '\n                                'disjoint.')\n\n    # Compute the flux of the spectrum through the bandpass and make sure\n    # the result makes sense.\n    sp = spectrum * band\n\n    totalflux = sp.integrate()\n    if totalflux <= 0.0:\n        raise ValueError('Integrated flux is <= 0')\n    if np.isnan(totalflux):\n        raise ValueError('Integrated flux is NaN')\n    if np.isinf(totalflux):\n        raise ValueError('Integrated flux is infinite')\n\n    # Get the standard unit spectrum in the renormalization units\n    RNunits = units.Units(RNunitstring)\n    if RNunits.isDensity:\n        up = RNunits.StdSpectrum * band\n    else:\n        up = RNunits.StdSpectrum\n\n    # Renormalize in magnitudes....\n    if RNunits.isMag:\n        ratio = totalflux / up.integrate()\n        dmag = RNval + 2.5 * math.log10(ratio)\n        newsp = spectrum.addmag(dmag)\n\n    #...or in linear flux units.\n    else:\n        const = RNval * (up.integrate() / totalflux)\n        newsp = spectrum * const\n\n    # Return the new spectrum\n    return newsp", "response": "Returns a new copy of the current object with the standard unit spectrum applied to the given spectrum and band."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run():\n    #Define the import statements\n    impheader = \"\"\"import sys\nimport conv_base\n\n\"\"\"\n    #Define the test case pattern\n    pattern=\"\"\"class Test%d(conv_base.%sCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.obsmode=\"%s\"\n        cls.spectrum=\"%s\"\n        cls.fname=\"C%s_%s.fits\"\n        cls.setup2()\\n\\n\"\"\"\n\n\n    #We're going to split these by obsmode.\n    outfiles = {'None':'sponly/sponly.py',\n                'acs':'acs/acs.py',\n                'nicmos':'nicmos/nicmos.py',\n                'stis':'stis/stis.py',\n                'wfc3,ir':'wfc3_ir/wfc3_ir.py',\n                'wfc3,uvis1':'wfc3_uvis1/wfc3_uvis1.py',\n                'wfc3,uvis2':'wfc3_uvis2/wfc3_uvis2.py',\n                'oops':'oops/oops.py'\n                }\n\n    #And subclass for spectrum-only and thermal cases based on obsmode.\n    casetype = {\"None\": \"Spec\",\n                \"nicmos\": \"Therm\",\n                \"wfc3,ir\": \"Therm\"}\n    #Use with default=Comm\n                \n    #Open the database\n    db = sqlite3.connect(\"/ssbwebv1/data1/pandokia/old/pdk/c3/db/pdk.db\")\n\n\n    #Open and initialize the output files\n    setdirs(outfiles)\n    for k in outfiles:\n        outfiles[k] = open(outfiles[k], 'w')\n        outfiles[k].write(impheader)\n\n    count=0\n\n    #Select unique (obsmode,spectrum) pairs from our database table of\n    #commissioning tests. Sort them by obsmode.\n\n    #Omit cases where spectrum is None, because we've verified that all\n    #those obsmodes are duplicated elsewhere.\n\n    #First the bp only. The purpose of these is thermal.\n    c = db.execute(\"\"\" SELECT DISTINCT obsmode, spectrum\n                       FROM synpysyn\n                       WHERE spectrum != \"None\"\n                       ORDER BY obsmode\"\"\")\n\n    #Now loop over them, and define test cases.\n    for x in c:\n       obsmode, spectrum = x\n       count+=1\n       outkey=pickname(obsmode,outfiles)\n       defn=pattern%(count,\n                     casetype.get(outkey, \"Comm\"),\n                     obsmode,spectrum,count,\"%s\")\n       outfiles[outkey].write(defn)\n\n    #Close all the files\n    for k in outfiles:\n        outfiles[k].close()", "response": "Generate the test case pattern for the given patternfile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setdirs(outfiles):\n    for k in outfiles:\n        fname=outfiles[k]\n        dname= os.path.dirname(fname)\n        if not os.path.isdir(dname):\n            os.mkdir(dname)", "response": "Create the directories if need be"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates Planck s law in photlam per steradian.", "response": "def bbfunc(wave, temperature):\n    \"\"\"Evaluate Planck's law in ``photlam`` (per steradian).\n\n    .. note::\n\n        Adapted from IRAF STSDAS SYNPHOT ``bbfunc`` task.\n\n    Parameters\n    ----------\n    wave : array_like\n        Wavelength values in Angstrom.\n\n    temperature : float\n        Blackbody temperature in Kelvin.\n\n    Returns\n    -------\n    result : array_like\n        Blackbody radiation in ``photlam`` per steradian.\n\n    \"\"\"\n    x = wave * temperature\n\n    mask = N.where(x > 0.0, 1, 0)\n    x = N.where(mask==1, 1.43883E8 / x, 0.0)\n\n    factor = N.zeros(wave.shape, dtype=N.float64)\n\n    mask1 = N.where(x < LOWER, 0, 1)\n    factor = N.where(mask1 == 0, 2.0 / (x * (x + 2.0)), factor)\n\n    mask2 = N.where(x < UPPER, 1, 0)\n    mask = mask1 * mask2\n    factor = N.where(mask == 1, 1.0 / (N.exp(x) - 1.0), factor)\n\n    x = x * temperature / 1.95722E5\n    x = factor * x * x * x\n\n    return x / (H * wave)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef llam_SI(wave, temperature):\n    exponent = C2 / (wave * temperature)\n\n    result = N.zeros(wave.shape, dtype=N.float64)\n\n    mask1 = N.where(exponent <= LOWER, 0, 1)\n    result = N.where(mask1==0, (2.0 * C1 * (wave**-5.0)) / (exponent * (exponent + 2.0)), result)\n\n    mask2 = N.where(exponent <= UPPER, 1, 0)\n    mask = mask1 * mask2\n    exponent = N.where(mask2==1, exponent, UPPER)\n    expfactor = N.exp(exponent)\n    result = N.where(mask==1, (C1 * (wave**-5.0) / (expfactor - 1.0)), result)\n\n    return result", "response": "Like bbfunc but in SI units."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bb_photlam_arcsec(wave, temperature):\n    lam = wave * 1.0E-10    # Angstrom -> meter\n\n    return F * llam_SI(lam, temperature) / (HS * C / lam)", "response": "Evaluate Planck s law in photlam per square arcsec."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef genstis(outname):\n    pattern=\"\"\"class stisS%d(countrateCase):\n    def setUp(self):\n        self.obsmode=\"%s\"\n        self.spectrum=\"%s\"\n        self.setglobal(__file__)\n        self.runpy()\\n\"\"\"\n\n    speclist=['/grp/hst/cdbs/calspec/gd71_mod_005.fits',\n              '/grp/hst/cdbs/calspec/gd153_mod_004.fits',\n              '/grp/hst/cdbs/calspec/g191b2b_mod_004.fits']\n\n    glist={'g140l':'fuvmama','g230l':'nuvmama','g430l':'ccd','g750l':'ccd',\n    'g230lb':'ccd'}\n\n    out=open(outname,'a')\n    out.write(\"\"\"from pytools import testutil\nimport sys\nfrom basecase import calcphotCase, calcspecCase, countrateCase,SpecSourcerateSpecCase\\n\n\"\"\")\n    count=0\n    for g in glist:\n        for sp in speclist:\n            obsmode='stis,%s,fuvmama,s52x2'%g\n            defn=pattern%(count,obsmode,sp)\n            out.write(defn)\n            count+=1\n\n    out.write(\"\"\"\\n\\n\nif __name__ == '__main__':\n    if 'debug' in sys.argv:\n        testutil.debug(__name__)\n    else:\n        testutil.testall(__name__,2)\n\"\"\")\n\n    out.close()", "response": "Generate TestCases from cmdfile according to the pattern in patternfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef t_integer(self, s):\r\n        r' \\d+ '\r\n        self.rv.append(Token(type='INTEGER', attr=s))", "response": "r \\ d +"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef t_identifier(self, s):\r\n        r' [$a-z_A-Z/\\//][\\w/\\.\\$:#]*'\r\n        self.rv.append(Token(type='IDENTIFIER', attr=s))", "response": "t_identifier - Ends a identifier"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef t_float(self, s):\r\n        r' ((\\d*\\.\\d+)|(\\d+\\.d*)|(\\d+)) ([eE][-+]?\\d+)?'\r\n        self.rv.append(Token(type='FLOAT', attr=s))", "response": "t_float - Parse a ISO - 8601 float"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_expr_plus(self, tree):\r\n        ''' V ::= expr ( V + V )'''\r\n        tree.value = convertstr(tree[0].value) + convertstr(tree[2].value)", "response": "A macro that adds the value of the key - value pair to the key - value pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nterming multiplication of the tree.", "response": "def p_term_mult(self, tree):\r\n        ''' V ::= term ( V * V )'''\r\n        tree.value = convertstr(tree[0].value) * convertstr(tree[2].value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_term_div(self, tree):\r\n        ''' V ::= term ( V / V )'''\r\n        tree.value = convertstr(tree[0].value) / tree[2].value", "response": "divides the term tree by the term"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the value of the key.", "response": "def p_value_paren(self, tree):\r\n        ''' V ::= value ( LPAREN V RPAREN )'''\r\n        tree.value = convertstr(tree[1].value)\r\n        tree.svalue = \"(%s)\"%str(tree[1].value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(cmdfile,subsetfile=None, lookupfile=None):\n    #Define the test case pattern\n    pattern=\"\"\"class %s(basecase.%sCase):\n    def setUp(self):\n        self.obsmode=\"%s\"\n        self.spectrum=\"%s\"\n        self.subset=%s\n        self.etcid=\"%s\"\n        self.setglobal(__file__)\n        self.runpy()\\n\"\"\"\n\n    #Open the (params:etcid) dictionary if it exists\n    if lookupfile is not None:\n        f=open(lookupfile)\n        lookup=pickle.load(f)\n        f.close()\n    else:\n        lookup={}\n        \n    #Process the subsetfile if present\n    subsetflag={}\n    if subsetfile is not None:\n        #Then this file contains a list of case names that should have a\n        #subset flag set.\n        f=open(subsetfile)\n        for line in f:\n            if not line.startswith('#') and len(line.strip())>0:\n                cols=line.strip().split()\n                subsetflag[cols[0]]=True\n        f.close()\n\n    #Open the main input and output files\n    f=open(cmdfile)\n    out=open(cmdfile.replace('.txt','.py'),'w')\n    out.write(\"\"\"from pytools import testutil\nimport sys\nimport basecase\n\"\"\")\n\n    #Set up some bookkeeping\n    count={'countrate':0,'calcspec':0,'calcphot':0,'SpecSourcerateSpec':0,'thermback':0}\n    dupcatcher={}\n    dupcounter={'countrate':0,'calcspec':0,'calcphot':0,'SpecSourcerateSpec':0,'thermback':0}\n    \n    #Now start the main loop\n    for line in f:\n    \n    #parse the line\n        ktuple=line2ktuple(line)\n        cmd,obsmode,spectrum=ktuple\n        count[cmd]+=1\n        casename=\"%sCase%d\"%(cmd,count[cmd])        \n        if ktuple in dupcatcher:\n            dupcounter[cmd]+=1\n            dupcatcher[ktuple].append(casename)\n        else:\n            dupcatcher[ktuple]=[casename]\n            etcid=lookup.get(ktuple,None)\n            if etcid is not None and len(etcid) == 1:\n                etcid=etcid[0]\n            defn=pattern%(casename,cmd,obsmode,spectrum,subsetflag.get(casename,False),etcid)\n            out.write(defn)\n\n    out.write(\"\"\"\\n\\n\nif __name__ == '__main__':\n    if 'debug' in sys.argv:\n        testutil.debug(__name__)\n    else:\n        testutil.testall(__name__,2)\n\"\"\")\n\n    f.close()\n\n    for k in count:\n        total= \"#%s:%d - %d dup =%d\\n\"%(k,count[k],dupcounter[k],count[k]-dupcounter[k])\n        sys.stdout.write(total)\n        out.write(total)\n    out.close()\n\n        \n    return dupcatcher", "response": "Generate test cases from the given commandfile."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a base unit object.", "response": "def Units(uname):\n    \"\"\"Generate a unit object.\n\n    Parameters\n    ----------\n    uname : str\n        Wavelength or flux unit name.\n\n    Returns\n    -------\n    unit : `BaseUnit` or `None`\n        Unit object. `None` means unitless.\n\n    Raises\n    ------\n    ValueError\n        Unknown unit name.\n\n    \"\"\"\n    if isinstance(uname,BaseUnit):\n        return uname\n    else:\n        try:\n            if issubclass(uname,BaseUnit):\n                return uname()\n        except TypeError:\n\n            try:\n                return factory(uname)\n            except KeyError:\n                if uname == str(None):\n                    return None\n                else:\n                    raise ValueError(\"Unknown units %s\"%uname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ismatch(a,b):\n    #Try the easy case\n    if a == b:\n        return True\n    else:\n        #Try isinstance in both orders\n        try:\n            if isinstance(a,b):\n                return True\n        except TypeError:\n            try:\n                if isinstance(b,a):\n                    return True\n            except TypeError:\n                #Try isinstance(a, type(b)) in both orders\n                try:\n                    if isinstance(a,type(b)):\n                        return True\n                except TypeError:\n                    try:\n                        if isinstance(b,type(a)):\n                            return True\n                    except TypeError:\n                        #Try the string representation\n                        if str(a).lower() == str(b).lower():\n                            return True\n                        else:\n                            return False", "response": "Method to allow smart comparisons between classes instances and units and give the right answer. For internal use only."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms unit conversion. Parameters ---------- wave : number or array_like Wavelength values to be used for conversion. target_units : str Unit to convert to. Returns ------- result : number or array_like Converted values. Raises ------ TypeError Conversion to given unit is not allowed.", "response": "def Convert(self,wave,target_units):\n        \"\"\"Perform unit conversion.\n\n        Parameters\n        ----------\n        wave : number or array_like\n            Wavelength values to be used for conversion.\n\n        target_units : str\n            Unit to convert to.\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        Raises\n        ------\n        TypeError\n            Conversion to given unit is not allowed.\n\n        \"\"\"\n        try:\n            return self.Dispatch[target_units.lower()](wave)\n        except KeyError:\n            raise TypeError(\"%s cannot be converted to %s\"%(self.name,\n                                                            target_units))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Convert(self, wave, flux, target_units, area=None):\n        try:\n            return self.Dispatch[target_units](wave, flux, area=area)\n        except KeyError:\n            raise TypeError(\"%s is not a valid flux unit\" % (target_units))", "response": "Perform unit conversion.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        target_units : str\n            Unit to convert to.\n\n        area : number or `None`\n            Telescope :ref:`area <pysynphot-area>`, if applicable.\n            This is only needed for conversions involving counts or ``obmag``.\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        Raises\n        ------\n        TypeError\n            Conversion to given unit is not allowed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unitResponse(self, band):\n        #sumfilt(wave,0,band)\n        # SUMFILT = Sum [ FILT(I) * WAVE(I) ** NPOW * DWAVE(I) ]\n        total = band.trapezoidIntegration(band.wave, band.throughput)\n        return 1.0/total", "response": "Put a flat spectrum of 1 photlam through this band and integrate."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ToPhotlam(self, wave, flux, **kwargs):\n        if hasattr(flux,'copy'):\n          return flux.copy()  # No conversion, just copy the array.\n        else:\n          return flux", "response": "Convert from Photlam to Photlam."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert from a wave and flux to an AB mag.", "response": "def ToABMag(self, wave, flux, **kwargs):\n        \"\"\"Convert to ``abmag``.\n\n        .. math::\n\n            \\\\textnormal{AB}_{\\\\nu} = -2.5 \\\\; \\\\log(h \\\\lambda \\\\; \\\\textnormal{photlam}) - 48.6\n\n        where :math:`h` is as defined in :ref:`pysynphot-constants`.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        kwargs : dict\n            Extra keywords (not used).\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        \"\"\"\n        arg = H * flux * wave\n        return -1.085736 * N.log(arg) + ABZERO"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting from HC to STMag.", "response": "def ToSTMag(self, wave, flux, **kwargs):\n        \"\"\"Convert to ``stmag``.\n\n        .. math::\n\n            \\\\textnormal{ST}_{\\\\lambda} = -2.5 \\\\; \\\\log(\\\\frac{hc}{\\\\lambda} \\\\; \\\\textnormal{photlam}) - 21.1\n\n        where :math:`h` and :math:`c` are as defined in\n        :ref:`pysynphot-constants`.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        kwargs : dict\n            Extra keywords (not used).\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        \"\"\"\n        arg = H * C * flux / wave\n        return -1.085736 * N.log(arg) + STZERO"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ToOBMag(self, wave, flux, area=None):\n        area = area if area else refs.PRIMARY_AREA\n        bin_widths = \\\n            binning.calculate_bin_widths(binning.calculate_bin_edges(wave))\n        arg = flux * bin_widths * area\n        return -1.085736 * N.log(arg)", "response": "Convert from Wavelength flux to OB magnitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts from VegaMag to VegaMag.", "response": "def ToVegaMag(self, wave, flux, **kwargs):\n        \"\"\"Convert to ``vegamag``.\n\n        .. math::\n\n            \\\\textnormal{vegamag} = -2.5 \\\\; \\\\log(\\\\frac{\\\\textnormal{photlam}}{f_{\\\\textnormal{Vega}}})\n\n        where :math:`f_{\\\\textnormal{Vega}}` is the flux of\n        :ref:`pysynphot-vega-spec` resampled at given wavelength values\n        and converted to ``photlam``.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        kwargs : dict\n            Extra keywords (not used).\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        \"\"\"\n        from . import spectrum\n        resampled = spectrum.Vega.resample(wave)\n        normalized = flux / resampled._fluxtable\n        return -2.5 * N.log10(normalized)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unitResponse(self,band):\n        wave=band.wave\n        total = band.trapezoidIntegration(wave,band.throughput/wave)\n        modtot = total * (1.0e-29/H)\n        return 1.0/modtot", "response": "This is used internally for : ref : pysynphot - formula - effstim.\n        calculations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unitResponse(self,band):\n        #sumfilt(wave,-1,band)\n        # SUMFILT = Sum [ FILT(I) * WAVE(I) ** NPOW * DWAVE(I) ]\n        wave=band.wave\n        total = band.trapezoidIntegration(wave,band.throughput/wave)\n        modtot = total/H\n        return 2.5*math.log10(modtot) + ABZERO", "response": "This is used internally for pysynphot - formula - effstim.\n        calculations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert from Hungarian - Marquardt to Photlam.", "response": "def ToPhotlam(self, wave, flux, **kwargs):\n        \"\"\"Convert to ``photlam``.\n\n        .. math::\n\n            m = -0.4 \\\\; (\\\\textnormal{ST}_{\\\\lambda} + 21.1)\n\n            \\\\textnormal{photlam} = \\\\frac{10^{m} \\\\lambda}{hc}\n\n        where :math:`h` and :math:`c` are as defined in\n        :ref:`pysynphot-constants`.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        kwargs : dict\n            Extra keywords (not used).\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        \"\"\"\n        return wave / H / C * 10.0**(-0.4 * (flux - STZERO))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unitResponse(self,band):\n        #sumfilt(wave,1,band)\n        # SUMFILT = Sum [ FILT(I) * WAVE(I) ** NPOW * DWAVE(I) ]\n        wave=band.wave\n        total = band.trapezoidIntegration(wave,band.throughput*wave)\n        modtot = total/(H*C)\n        return 2.5*math.log10(modtot) + STZERO", "response": "This is used internally for : ref:`pysynphot - formula - effstim`\n        calculations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting to ``photlam``. .. math:: \\\\textnormal{photlam} = 10^{-0.4 \\\\; \\\\textnormal{vegamag}} \\\\; f_{\\\\textnormal{Vega}} where :math:`f_{\\\\textnormal{Vega}}` is the flux of :ref:`pysynphot-vega-spec` resampled at given wavelength values and converted to ``photlam``. Parameters ---------- wave, flux : number or array_like Wavelength and flux values to be used for conversion. kwargs : dict Extra keywords (not used). Returns ------- result : number or array_like Converted values.", "response": "def ToPhotlam(self, wave, flux, **kwargs):\n        \"\"\"Convert to ``photlam``.\n\n        .. math::\n\n            \\\\textnormal{photlam} = 10^{-0.4 \\\\; \\\\textnormal{vegamag}} \\\\; f_{\\\\textnormal{Vega}}\n\n        where :math:`f_{\\\\textnormal{Vega}}` is the flux of\n        :ref:`pysynphot-vega-spec` resampled at given wavelength values\n        and converted to ``photlam``.\n\n        Parameters\n        ----------\n        wave, flux : number or array_like\n            Wavelength and flux values to be used for conversion.\n\n        kwargs : dict\n            Extra keywords (not used).\n\n        Returns\n        -------\n        result : number or array_like\n            Converted values.\n\n        \"\"\"\n        resampled = self.vegaspec.resample(wave)\n        return resampled.flux * 10.0**(-0.4 * flux)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ToPhotlam(self, wave, flux, area=None):\n        area = area if area else refs.PRIMARY_AREA\n        bin_widths = \\\n            binning.calculate_bin_widths(binning.calculate_bin_edges(wave))\n\n        return flux / (bin_widths * area)", "response": "Converts the internal wave and flux values to photlam."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef width_cal_preprocess(content):\n    ptn = re.compile(r'(\\033|\\x1b)\\[.*?m', re.I)\n    _content = re.sub(ptn, '', content) # remove ANSI escape code\n    return _content", "response": "remove ANSI escape code from content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the content to unicode and replace \\ r \\ t \\ n with space", "response": "def preprocess(content):\n    \"\"\"\n    \u5bf9\u8f93\u51fa\u5185\u5bb9\u8fdb\u884c\u9884\u5904\u7406\uff0c\u8f6c\u4e3astr\u7c7b\u578b (py3)\uff0c\u5e76\u66ff\u6362\u884c\u5185\\r\\t\\n\u7b49\u5b57\u7b26\u4e3a\u7a7a\u683c\n    do pre-process to the content, turn it into str (for py3), and replace \\r\\t\\n with space\n    \"\"\"\n\n    if six.PY2:\n        if not isinstance(content, unicode):\n            if isinstance(content, str):\n                _content = unicode(content, encoding=sys.stdin.encoding)\n            elif isinstance(content, int):\n                _content = unicode(content)\n        else:\n            _content = content\n        assert isinstance(_content, unicode)\n\n    elif six.PY3:\n        _content = str(content)\n\n    _content = re.sub(r'\\r|\\t|\\n', ' ', _content)\n    return _content"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the width of a line in terminal", "response": "def line_width(line):\n    \"\"\"\n    \u8ba1\u7b97\u672c\u884c\u5728\u8f93\u51fa\u5230\u547d\u4ee4\u884c\u540e\u6240\u5360\u7684\u5bbd\u5ea6\n    calculate the width of output in terminal\n    \"\"\"\n    if six.PY2:\n        assert isinstance(line, unicode)\n    _line = width_cal_preprocess(line)\n    result = sum(map(get_char_width, _line))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the actual rows with specific terminal width", "response": "def lines_of_content(content, width):\n    \"\"\"\n    \u8ba1\u7b97\u5185\u5bb9\u5728\u7279\u5b9a\u8f93\u51fa\u5bbd\u5ea6\u4e0b\u5b9e\u9645\u663e\u793a\u7684\u884c\u6570\n    calculate the actual rows with specific terminal width\n    \"\"\"\n    result = 0\n    if isinstance(content, list):\n        for line in content:\n            _line = preprocess(line)\n            result += ceil(line_width(_line) / width)\n    elif isinstance(content, dict):\n        for k, v in content.items():\n            # \u52a02\u662f\u7b97\u4e0a\u884c\u5185\u5192\u53f7\u548c\u7a7a\u683c\u7684\u5bbd\u5ea6\n            # adding 2 for the for the colon and space \": \"\n            _k, _v = map(preprocess, (k, v))\n            result += ceil((line_width(_k) + line_width(_v) + 2) / width)\n    return int(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_multi_line(content, force_single_line, sort_key):\n\n    global last_output_lines\n    global overflow_flag\n    global is_atty\n\n    if not is_atty:\n        if isinstance(content, list):\n            for line in content:\n                print(line)\n        elif isinstance(content, dict):\n            for k, v in sorted(content.items(), key=sort_key):\n                print(\"{}: {}\".format(k, v))\n        else:\n            raise TypeError(\"Excepting types: list, dict. Got: {}\".format(type(content)))\n        return\n\n    columns, rows = get_terminal_size()\n    lines = lines_of_content(content, columns)\n    if force_single_line is False and lines > rows:\n        overflow_flag = True\n    elif force_single_line is True and len(content) > rows:\n        overflow_flag = True\n\n    # \u786e\u4fdd\u521d\u59cb\u8f93\u51fa\u4f4d\u7f6e\u662f\u4f4d\u4e8e\u6700\u5de6\u5904\u7684\n    # to make sure the cursor is at the left most\n    print(\"\\b\" * columns, end=\"\")\n\n    if isinstance(content, list):\n        for line in content:\n            _line = preprocess(line)\n            print_line(_line, columns, force_single_line)\n    elif isinstance(content, dict):\n        for k, v in sorted(content.items(), key=sort_key):\n            _k, _v = map(preprocess, (k, v))\n            print_line(\"{}: {}\".format(_k, _v), columns, force_single_line)\n    else:\n        raise TypeError(\"Excepting types: list, dict. Got: {}\".format(type(content)))\n\n    # \u8f93\u51fa\u989d\u5916\u7684\u7a7a\u884c\u6765\u6e05\u9664\u4e0a\u4e00\u6b21\u8f93\u51fa\u7684\u5269\u4f59\u5185\u5bb9\n    # do extra blank lines to wipe the remaining of last output\n    print(\" \" * columns * (last_output_lines - lines), end=\"\")\n\n    # \u56de\u5230\u521d\u59cb\u8f93\u51fa\u4f4d\u7f6e\n    # back to the origin pos\n    print(magic_char * (max(last_output_lines, lines)-1), end=\"\")\n    sys.stdout.flush()\n    last_output_lines = lines", "response": "Print a multi - line list of content to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_consensus_strings(self, output):\n        '''\n        Writes the taxonomy of each leaf to a file. If the leaf has no\n        taxonomy, a taxonomy string will be created using the annotations\n        provided to the ancestor nodes of that leaf (meaning, it will be\n        decorated).\n\n        Parameters\n        ----------\n        output    : string\n            File to which the taxonomy strings for each leaf in the tree will\n            be written in Greengenes format, e.g.\n                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia\n                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia\n        '''\n\n        logging.info(\"Writing decorated taxonomy to file: %s\" % (output))\n\n        with open(output, 'w') as out:\n\n            for tip in self.tree.leaf_nodes():\n                tax_name = tip.taxon.label.replace(\" \", \"_\")\n\n                if tip.taxon.label in self.taxonomy:\n                    tax_string = '; '.join(self.taxonomy[tax_name])\n                else:\n                    ancestor_list = []\n                    for ancestor in tip.ancestor_iter():\n                        if ancestor.label:\n                            split_node_name = ancestor.label.split(':')\n                            if len(split_node_name) == 2:\n                                ancestor_list+=list(reversed(split_node_name[1].split('; ')))\n                            elif len(split_node_name) == 1:\n                                try:\n                                    float(split_node_name[0])\n                                except ValueError:\n                                    ancestor_list+=list(reversed(split_node_name[0].split('; ')))\n                            else:\n                                raise Exception(\"Malformed node name: %s\" % ancestor.label)\n                    tax_list = list(reversed(ancestor_list))\n\n                    if len(tax_list) < 1:\n                        logging.warning(\"No taxonomy found for species %s!\" % (tax_name))\n                        tax_string = \"Unknown\"\n                    else:\n                        tax_string = '; '.join(tax_list)\n\n                output_line = \"%s\\t%s\\n\" % (tax_name, tax_string)\n                out.write(output_line)", "response": "Writes the taxonomy of each leaf in the tree to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenames an internal node of the tree.", "response": "def _rename(self, node, name):\n        '''\n        Rename an internal node of the tree. If an annotation is already\n        present, append the new annotation to the end of it. If a bootstrap\n        value is present, add annotations are added after a \":\" as per standard\n        newick format.\n\n        Parameters\n        ----------\n        node: dendropy.Node\n            dendropy.Node object\n        name    : string\n            Annotation to rename the node with.\n        '''\n        if node.label:\n            try:\n                float(node.label)\n                new_label = \"%s:%s\" % (node.label,\n                                       name)\n            except ValueError:\n                new_label = \"%s; %s\" % (node.label,\n                                        name)\n            node.label = new_label\n        else:\n            node.label = name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecorates a tree with taxonomy. This code does not allow inconsistent taxonomy within a clade. If one sequence in a clade has a different annotation to the rest, it will split the clade. Paraphyletic group names are distinguished if unique_names = True using a simple tally of each group (see unique_names below). Parameters ---------- output_tree : string File to which the decorated tree will be written. output_tax : string File to which the taxonomy strings for each tip in the tree will be written. unique_names : boolean True indicating that a unique number will be appended to the end of a taxonomic rank if it is found more than once in the tree (i.e. it is paraphyletic in the tree). If false, multiple clades may be assigned with the same name.", "response": "def decorate(self, output_tree, output_tax, unique_names):\n        '''\n        Decorate a tree with taxonomy. This code does not allow inconsistent\n        taxonomy within a clade. If one sequence in a clade has a different\n        annotation to the rest, it will split the clade. Paraphyletic group\n        names are distinguished if unique_names = True using a simple tally of\n        each group (see unique_names below).\n\n        Parameters\n        ----------\n        output_tree        : string\n            File to which the decorated tree will be written.\n        output_tax         : string\n            File to which the taxonomy strings for each tip in the tree will be\n            written.\n        unique_names       : boolean\n            True indicating that a unique number will be appended to the end of\n            a taxonomic rank if it is found more than once in the tree\n            (i.e. it is paraphyletic in the tree). If false, multiple clades\n            may be assigned with the same name.\n        '''\n        logging.info(\"Decorating tree\")\n        encountered_taxonomies = {}\n        tc = TaxonomyCleaner()\n        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):\n\n            max_tax_string_length = 0\n            for tip in node.leaf_nodes():\n                tip_label=tip.taxon.label.replace(' ', '_')\n                if tip_label in self.taxonomy:\n                    tax_string_length \\\n                        = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])\n                    if tax_string_length > max_tax_string_length:\n                        max_tax_string_length = tax_string_length\n\n            logging.debug(\"Number of ranks found for node: %i\" % max_tax_string_length)\n            tax_string_array = []\n            for rank in range(max_tax_string_length):\n\n                rank_tax = []\n                for tip in node.leaf_nodes():\n\n                    tip_label = tip.taxon.label.replace(' ', '_')\n                    if tip_label in self.taxonomy:\n\n                        tip_tax = self.taxonomy[tip_label]\n                        if len(tip_tax) > rank:\n                            tip_rank = tip_tax[rank]\n                            if tip_rank not in rank_tax:\n                                rank_tax.append(tip_rank)\n                consistent_taxonomy = len(rank_tax) == 1\n\n                if consistent_taxonomy:\n                    tax=rank_tax.pop()\n                    logging.debug(\"Consistent taxonomy found for node: %s\" \\\n                                                                % tax)\n                    if tax not in tc.meaningless_taxonomic_names:\n                        if unique_names:\n                            if tax in encountered_taxonomies:\n                                encountered_taxonomies[tax]+=0\n                                tax = \"%s_%i\" \\\n                                        % (tax, encountered_taxonomies[tax])\n                            else:\n                                encountered_taxonomies[tax]=0\n                        tax_string_array.append(tax)\n\n            if any(tax_string_array):\n                index = 0\n                for anc in node.ancestor_iter():\n                    try:\n                        index+=anc.tax\n                    except:\n                        continue\n                tax_string_array = tax_string_array[index:]\n                if any(tax_string_array):\n                    self._rename(node, '; '.join(tax_string_array))\n                node.tax = len(tax_string_array)\n        logging.info(\"Writing decorated tree to file: %s\" % output_tree)\n        if output_tree:\n            self.tree.write(path=output_tree, schema=\"newick\")\n        if output_tax:\n            self._write_consensus_strings(output_tax)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an annotated dendropy tree return the taxonomy of each tip name to array of taxonomic affiliations", "response": "def taxonomy_from_annotated_tree(self, dendropy_tree):\n        '''Given an annotated dendropy tree, return the taxonomy of each tip\n        \n        Parameters\n        ---------\n        tree: dendropy.Tree\n            Decorated tree to extract taxonomy from       \n        Returns\n        -------\n        dictionary of tip name to array of taxonomic affiliations'''\n        tip_to_taxonomy = {}\n        for tip in dendropy_tree.leaf_node_iter():\n            tax = []\n            n = tip.parent_node\n            while n:\n                node_taxonomy = self.taxonomy_from_node_name(n.label)\n                if node_taxonomy and n.parent_node:\n                    tax = [node_taxonomy]+tax\n                n = n.parent_node\n            tip_name = tip.taxon.label.replace(' ','_')\n            if tip_name in tip_to_taxonomy:\n                raise Exception(\"Found duplicate tip name '%s'\" % tip_name)\n            if len(tax)==0:\n                tip_to_taxonomy[tip_name] = []\n            else:\n                tip_to_taxonomy[tip_name] = [t.strip() for t in '; '.join(tax).split(';')]\n        return tip_to_taxonomy"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the taxonomy incorporated at a particular node", "response": "def taxonomy_from_node_name(self, node_name):\n        '''return the taxonomy incorporated at a particular node, or None\n        if it does not encode any taxonomy\n        \n        Parameters\n        ----------\n        node_name: str\n            a node label.\n            \n        Returns\n        -------\n        Taxonomy as a string, or None if there is no taxonomy\n        '''\n        \n        def is_float(s):\n            try:\n                float(s)\n                return True\n            except ValueError:\n                return False\n        \n        if node_name is None:\n            return None\n        elif is_float(node_name):\n            # no name, just a bootstrap\n            return None\n        else:\n            bootstrap_regex = re.compile(r'[\\d\\.]+:(.*)')\n            reg = bootstrap_regex.match(node_name)\n            if reg:\n                # bootstrap in name\n                return reg.groups(0)[0]\n            else:\n                # bootstrap not in name\n                return node_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun input sequences in either blastp or blastx mode against the database specified in __init__.", "response": "def run(self, input_sequence_file, input_sequence_type, daa_file_basename=None):\n        '''Run input sequences in either blastp or blastx mode against the\n        database specified in __init__.\n            \n        Parameters\n        ----------\n        input_sequence_file: str\n            path to query sequences\n        input_sequence_type: either 'nucleotide' or 'protein'\n            the input_sequences are this kind of sequence\n            \n        Returns\n        -------\n        DiamondSearchResult\n        '''\n        \n        cmd_list = [\"diamond\"]\n        if input_sequence_type == UnpackRawReads.PROTEIN_SEQUENCE_TYPE:\n            cmd_list.append('blastp')\n        elif input_sequence_type == UnpackRawReads.NUCLEOTIDE_SEQUENCE_TYPE:\n            cmd_list.append('blastx')\n        else:\n            raise Exception(\"Programming error\")\n        \n        basename = daa_file_basename\n        if basename is None:\n            with tempfile.NamedTemporaryFile(prefix='graftm_diamond') as t:\n                # we are just stealing the name, don't need the file itself\n                basename = t.name\n            \n        for c in ['-k 1',\n                  \"-d\",\n                    self._database,\n                    \"-q\",\n                    \"%s\" % input_sequence_file,\n                    \"-a\",\n                    basename]:\n            cmd_list.append(c)\n        if self._threads:\n            cmd_list.append(\"--threads\")\n            cmd_list.append(str(self._threads))\n        if self._evalue:\n            cmd_list.append(\"--evalue\")\n            cmd_list.append(str(self._evalue))\n\n        cmd = ' '.join(cmd_list)\n        extern.run(cmd)\n        \n        daa_name = \"%s.daa\" % basename\n        res = DiamondSearchResult.import_from_daa_file(daa_name)\n        \n        if daa_file_basename is None:\n            # Diamond makes an extra file, need to remove this\n            os.remove(daa_name)\n            \n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nacquires a new graftm Package object", "response": "def acquire(graftm_package_path):\n        '''Acquire a new graftm Package\n\n        Parameters\n        ----------\n        graftm_output_path: str\n            path to base directory of graftm\n        '''\n        \n        contents_hash = json.load(\n                                   open(\n                                        os.path.join(\n                                                     graftm_package_path,\n                                                     GraftMPackage._CONTENTS_FILE_NAME\n                                                     ),\n                                         )\n                                   )\n        \n        \n        v=contents_hash[GraftMPackage.VERSION_KEY]\n        logging.debug(\"Loading version %i GraftM package: %s\" % (v, graftm_package_path))\n        if v == 2:\n            pkg = GraftMPackageVersion2()\n        elif v == 3:\n            pkg = GraftMPackageVersion3()\n        else:\n            raise InsufficientGraftMPackageException(\"Bad version: %s\" % v)\n        \n        pkg._contents_hash = contents_hash\n        pkg._base_directory = graftm_package_path\n        # check we are at current version otherwise choke\n        pkg.check_universal_keys(v)\n        pkg.check_required_keys(GraftMPackage._REQUIRED_KEYS[str(v)])\n        return pkg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_required_keys(self, required_keys):\n        '''raise InsufficientGraftMPackageException if this package does not\n        conform to the standard of the given package'''\n        h = self._contents_hash\n        for key in required_keys:\n            if key not in h:\n                raise InsufficientGraftMPackageException(\"Package missing key %s\" % key)", "response": "raise InsufficientGraftMPackageException if this package does not have the given keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new GraftM package with the given inputs.", "response": "def compile(output_package_path, refpkg_path, hmm_path, diamond_database_file, max_range, \n                trusted_cutoff=False, search_hmm_files=None):\n        '''Create a new GraftM package with the given inputs. Any files\n        specified as parameters are copied into the final package so can\n        be removed after calling this function.\n\n        Parameters\n        ----------\n        output_package_path: str\n            path to the package being created (must not exist)\n        refpkg_path: str\n            path to pplacer reference package\n        hmm_path: str\n            path to the align HMM. Used as the search HMM if search_hmm_files\n            is None\n        diamond_database_file: str\n            path to diamond DB file, or None for nucleotide packages\n        max_rage: str\n            as per maximum_range()\n        trusted_cutoff: boolean\n            set TC in search HMM\n\n        search_hmm_files: list of str or None\n            use these HMMs for search instead of the hmm_path. All\n            basenames of these paths must be unique, and not the same as\n            hmm_path.\n            \n        Returns\n        -------\n        Nothing\n        '''\n\n        if os.path.exists(output_package_path):\n            raise Exception(\"Not writing new GraftM package to already existing file/directory with name %s\" % output_package_path)\n        os.mkdir(output_package_path)\n\n        hmm_file_in_gpkg = os.path.basename(hmm_path)\n        shutil.copyfile(hmm_path, os.path.join(output_package_path, hmm_file_in_gpkg))\n\n        if diamond_database_file:\n            diamond_database_file_in_gpkg = os.path.basename(diamond_database_file)\n            shutil.copyfile(diamond_database_file, os.path.join(output_package_path, diamond_database_file_in_gpkg))\n\n        refpkg_in_gpkg = os.path.basename(refpkg_path)\n        shutil.copytree(refpkg_path, os.path.join(output_package_path, refpkg_in_gpkg))\n\n        \n        if search_hmm_files:\n            search_hmm_files_in_gpkg_names =\\\n                [os.path.basename(path) for path in search_hmm_files]\n            if hmm_file_in_gpkg in search_hmm_files_in_gpkg_names:\n                raise Exception(\"Search and align HMMs must all have different basenames to create a new gpkg\")\n            if len(set(search_hmm_files_in_gpkg_names)) != len(search_hmm_files_in_gpkg_names):\n                raise Exception(\"Search HMMs must have different basenames to create a new gpkg\")\n            for i, search_hmm in enumerate(search_hmm_files):\n                shutil.copyfile(search_hmm, os.path.join(output_package_path, search_hmm_files_in_gpkg_names[i]))\n        else:\n            search_hmm_files_in_gpkg_names = [hmm_file_in_gpkg]\n        \n        contents = {GraftMPackage.VERSION_KEY: GraftMPackageVersion2.version,\n                    GraftMPackage.ALIGNMENT_HMM_KEY: hmm_file_in_gpkg,\n                    GraftMPackage.SEARCH_HMM_KEY: search_hmm_files_in_gpkg_names,\n                    GraftMPackage.REFERENCE_PACKAGE_KEY: refpkg_in_gpkg,\n                    GraftMPackage.HMM_TRUSTED_CUTOFF_KEY: trusted_cutoff,\n                    GraftMPackage.RANGE_KEY: max_range}\n        if diamond_database_file:\n            contents[GraftMPackage.DIAMOND_DATABASE_KEY] = diamond_database_file_in_gpkg\n\n        json.dump(contents, open(os.path.join(output_package_path, GraftMPackage._CONTENTS_FILE_NAME), 'w'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a diamond database from the unaligned sequences in this package.", "response": "def create_diamond_db(self):\n        '''Create a diamond database from the unaligned sequences in this package.\n\n        Returns\n        -------\n        path to the created diamond db e.g. 'my_sequences.dmnd'\n        '''\n        base = self.unaligned_sequence_database_path()\n        cmd = \"diamond makedb --in '%s' -d '%s'\" % (self.unaligned_sequence_database_path(), base)\n        extern.run(cmd)\n        diamondb = '%s.dmnd' % base\n        # Mostly this moves a file to it's current location because Create\n        # follows this same logic, but there's a specially crafted\n        # test/data/mcrA.gpkg which is slightly different.\n        os.rename(diamondb, self.diamond_database_path())\n        return diamondb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef taxonomy_hash(self):\n        '''Read in the taxonomy and return as a hash of name: taxonomy,\n        where taxonomy is an array of strings.'''\n        gtns = Getaxnseq()\n        with open(self.taxtastic_taxonomy_path()) as tax:\n            with open(self.taxtastic_seqinfo_path()) as seqinfo:\n                return gtns.read_taxtastic_taxonomy_and_seqinfo(\n                    tax, seqinfo)", "response": "Read in the taxonomy and return as a hash of name taxonomy and seqinfo where taxonomy is an array of strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if this package is an amino acid alignment package otherwise it is a nucleotide alignment package.", "response": "def graftm_package_is_protein(graftm_package):\n        '''Return true if this package is an Amino Acid alignment package, otherwise\n        False i.e. it is a nucleotide package. In general it is best to use\n        'is_protein_package' instead.\n\n        '''\n        found = None\n        with open(graftm_package.alignment_hmm_path()) as f:\n            r = f.read().split(\"\\n\")\n        for line in r:\n            if line=='ALPH  DNA':\n                found = False\n                break\n            elif line=='ALPH  amino':\n                found = True\n                break\n        if found is None:\n            raise Exception(\"Unable to determine whether the HMM was amino acid or dna\")\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_protein_package(self):\n        '''Return true if this package is an Amino Acid alignment package, otherwise\n        False i.e. it is a nucleotide package. Cache the result for speed.\n\n        '''\n        if not hasattr(self, '_is_protein_package'):\n            self._is_protein_package = GraftMPackageVersion3.graftm_package_is_protein(self)\n        return self._is_protein_package", "response": "Return true if this package is an Amino Acid alignment package otherwise it is a nucleotide package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nguessing the sequence type from a string.", "response": "def _guess_sequence_type_from_string(self, seq):\n        '''Return 'protein' if there is >10% amino acid residues in the\n        (string) seq parameter, else 'nucleotide'. Raise Exception if a\n        non-standard character is encountered'''\n        # Define expected residues for each sequence type\n        aa_chars = ['P','V','L','I','M','F','Y','W','H','K','R','Q','N','E','D','S','X','*']\n        aas = set(itertools.chain(aa_chars, [lower(a) for a in aa_chars]))\n        na_chars = ['A', 'T', 'G', 'C', 'N', 'U']\n        nas = set(itertools.chain(na_chars, [lower(a) for a in na_chars]))\n\n        num_nucleotide = 0\n        num_protein = 0\n        count = 0\n        for residue in seq:\n            if residue in nas:\n                num_nucleotide += 1\n            elif residue in aas:\n                num_protein += 1\n            else:\n                raise Exception('Encountered unexpected character when attempting to guess sequence type: %s' % (residue))\n            count += 1\n            if count >300: break\n        if float(num_protein) / (num_protein+num_nucleotide) > 0.1:\n            return self.PROTEIN_SEQUENCE_TYPE\n        else:\n            return self.NUCLEOTIDE_SEQUENCE_TYPE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sequence_type(self):\n        '''Guess the type of input sequence provided to graftM (i.e. nucleotide\n        or amino acid) and return'''\n        if self.known_sequence_type is not None:\n            return self.known_sequence_type\n        else:\n            # If its Gzipped and fastq make a small sample of the sequence to be\n            # read\n            cmd='%s | head -n 2' % (self.command_line())\n            first_seq = subprocess.check_output(\n                                                cmd,\n                                                shell=True,\n                                                preexec_fn=lambda:signal(SIGPIPE, SIG_DFL)\n                                                )\n            _, seq = tuple(first_seq.strip().split('\\n'))\n            self.type = self._guess_sequence_type_from_string(seq)\n            logging.debug(\"Detected sequence type as %s\" % self.type)\n            return self.type", "response": "Guess the type of input sequence provided to graftM and return'''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef basename(self):\n        '''Return the name of the file with the '.fasta' or 'fq.gz' etc\n        removed'''\n        return os.path.basename(self.read_file)[:-len(self._get_extension(self.read_file))]", "response": "Return the name of the file with the. fasta or fq. gz etc\n        removed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string to open read files with", "response": "def command_line(self):\n        '''Return a string to open read files with'''\n        file_format=self.guess_sequence_input_file_format(self.read_file)\n        logging.debug(\"Detected file format %s\" % file_format)\n        if file_format == self.FORMAT_FASTA:\n            cmd=\"\"\"cat %s\"\"\" % (self.read_file)\n        elif file_format == self.FORMAT_FASTQ_GZ:\n            cmd=\"\"\"zcat %s | awk '{print \">\" substr($0,2);getline;print;getline;getline}' -\"\"\" % (self.read_file)\n        elif file_format == self.FORMAT_FASTA_GZ:\n            cmd=\"\"\"zcat %s\"\"\" % (self.read_file)\n        elif file_format == self.FORMAT_FASTQ:\n            cmd=\"\"\"awk '{print \">\" substr($0,2);getline;print;getline;getline}' %s\"\"\" % (self.read_file)\n        logging.debug(\"raw read unpacking command chunk: %s\" % cmd)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_fasttree_newick(self, tree, output_file_io):\n        '''Write the given dendropy tree to the open output file I/O, ensuring that is\n        acceptable for FastTree by e.g. removing quoted taxon names.\n\n        Parameters\n        ----------\n        tree: dendropy.Tree\n            tree to be written\n        output_file_io: stream\n            stream to write the newick formatted tree to\n\n        '''\n        newick = tree.as_string(schema='newick',\n                                suppress_internal_node_labels=True,\n                                suppress_internal_taxon_labels=True,\n                                preserve_spaces=False) # Labels must not be quoted.\n        output_file_io.write(newick.replace(\"'\",'').replace(' ','_'))", "response": "Write the given dendropy. Tree to the open output file I / O ensuring that is\n            acceptable for FastTree by eg. removing quoted taxon names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving sequences with in the given sequence_names array from the tree in place.", "response": "def remove_sequences(self, tree, sequence_names):\n        '''Remove sequences with in the given sequence_names array from the tree in\n        place. Assumes the sequences are found in the tree, and that they are\n        all unique.\n\n        Parameters\n        ----------\n        tree: dendropy.Tree\n            tree to remove from\n        sequence_names: list of str\n            list of tip names to remove\n\n        '''\n        tree.prune_taxa_with_labels(sequence_names)\n        tree.prune_taxa_with_labels([s.replace('_',' ') for s in sequence_names])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _hmmalign(self, input_path, directions, pipeline,\n                  forward_reads_output_path, reverse_reads_output_path):\n        '''\n        Align reads to the aln_hmm. Receives unaligned sequences and\n        aligns them.\n\n        Parameters\n        ----------\n        input_path : str\n            Filename of unaligned hits to be aligned\n        directions : dict\n            dictionary containing read names as keys, and complement\n            as the entry (True=Forward, False=Reverse)\n        pipeline: str\n            either PIPELINE_AA = \"P\" or PIPELINE_NT = \"D\"\n        forward_reads_output_fh: str\n            Where to write aligned forward reads\n        reverse_reads_output_fh: str\n            Where to write aligned reverse reads\n        Returns\n        -------\n        Nothing.\n        '''\n        if pipeline == PIPELINE_AA:\n            reverse_direction_reads_present=False\n        else:\n            reverse_direction_reads_present=False in directions.values()\n\n        with tempfile.NamedTemporaryFile(prefix='for_file', suffix='.fa') as for_file_fh:\n            for_file = for_file_fh.name\n            with tempfile.NamedTemporaryFile(prefix='rev_file', suffix='.fa') as rev_file_fh:\n                rev_file = rev_file_fh.name\n                # Align input reads to a specified hmm.\n                if reverse_direction_reads_present:  # Any that are in the reverse direction would be True\n                    reverse = []\n                    forward = []\n                    records = list(SeqIO.parse(open(input_path), 'fasta'))\n\n                    # Split the reads into reverse and forward lists\n                    for record in records:\n                        read_id = record.id\n\n                        if directions[read_id] == True:\n                            forward.append(record)\n                        elif directions[read_id] == False:\n                            reverse.append(record)\n                        else:\n                            raise Exception(logging.error('Programming error: hmmalign'))\n                            exit(1)\n\n                    logging.debug(\"Found %i forward direction reads\" % len(forward))\n                    logging.debug(\"Found %i reverse direction reads\" % len(reverse))\n\n                    # Write reverse complement and forward reads to files\n                    with open(for_file, 'w') as for_aln:\n                        logging.debug(\"Writing forward direction reads to %s\" % for_file)\n                        for record in forward:\n                            for_aln.write('>' + record.id + '\\n')\n                            for_aln.write(str(record.seq) + '\\n')\n                            # HMMalign and convert to fasta format\n                    if any(forward):\n                        self.hmmalign_sequences(self.aln_hmm, for_file, forward_reads_output_path)\n                    else:\n                        cmd = 'touch %s' % (forward_reads_output_path)\n                        extern.run(cmd)\n                    with open(rev_file, 'w') as rev_aln:\n                        logging.debug(\"Writing reverse direction reads to %s\" % rev_file)\n                        for record in reverse:\n                            if record.id and record.seq:\n                                rev_aln.write('>' + record.id + '\\n')\n                                rev_aln.write(str(record.seq.reverse_complement()) + '\\n')\n                    self.hmmalign_sequences(self.aln_hmm, rev_file, reverse_reads_output_path)\n                    conv_files = [forward_reads_output_path, reverse_reads_output_path]\n                    return conv_files\n\n                else:\n                    # If there are only forward reads, just hmmalign and be done with it.\n                    self.hmmalign_sequences(self.aln_hmm, input_path, forward_reads_output_path)\n                    conv_files = [forward_reads_output_path]\n\n                    return conv_files", "response": "Align reads to the aln_hmm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hmmalign_sequences(self, hmm, sequences, output_file):\n        '''Run hmmalign and convert output to aligned fasta format\n\n        Parameters\n        ----------\n        hmm: str\n            path to hmm file\n        sequences: str\n            path to file of sequences to be aligned\n        output_file: str\n            write sequences to this file\n\n        Returns\n        -------\n        nothing\n        '''\n\n        cmd = 'hmmalign --trim %s %s' % (hmm, sequences)\n        output = extern.run(cmd)\n        with open(output_file, 'w') as f:\n            SeqIO.write(SeqIO.parse(StringIO(output), 'stockholm'), f, 'fasta')", "response": "Run hmmalign and convert output to aligned fasta format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hmmsearch(self, output_path, input_path, unpack, seq_type, threads, cutoff, orfm):\n        '''\n        hmmsearch - Search raw reads for hits using search_hmm list\n\n        Parameters\n        ----------\n        output_path : str\n            path to output domtblout table\n        input_path : str\n            path to input sequences to search\n        unpack : UnpackRawReads\n            UnpackRawReads object, returns string command that will output\n            sequences to stdout when called on command line\n            (use: unpack.command_line())\n        seq_type : str\n            variable containing a string, either 'nucleotide' or 'aminoacid'.\n            Tells the pipeline whether or not to call ORFs on the sequence.\n            If sequence is 'nucleotide', ORFs are called. If not, no ORFs.\n        threads : Integer\n            Number of threads to use. Passed to HMMsearch command.\n        cutoff : str\n            cutoff for HMMsearch to use, either an evalue or --cut_tc, meaning\n            use the TC score cutoff specified within the HMM. Passed to \n            HMMsearch command.\n        orfm : OrfM\n            Object that builds the command chunk for calling ORFs on sequences\n            coming through as stdin. Outputs to stdout. Calls command_line\n            to construct final command line string.\n\n        Returns\n        -------\n        output_table_list : array of HMMSearchResult\n            Includes the name of the output domtblout table given by hmmer\n\n        Raises\n        ------\n        hmmsearcher.NoInputSequencesException\n            Raised if there are no sequences fed into the HMM.\n        '''\n\n        # Define the base hmmsearch command.\n        logging.debug(\"Using %i HMMs to search\" % (len(self.search_hmm)))\n        output_table_list = []\n        if len(self.search_hmm) > 1:\n            for hmm in self.search_hmm:\n                out = os.path.join(os.path.split(output_path)[0], os.path.basename(hmm).split('.')[0] + '_' + os.path.split(output_path)[1])\n                output_table_list.append(out)\n        elif len(self.search_hmm) == 1:\n            output_table_list.append(output_path)\n        else:\n            raise Exception(\"Programming error: expected 1 or more HMMs\")\n\n        # Choose an input to this base command based off the file format found.\n        if seq_type == 'nucleotide':  # If the input is nucleotide sequence\n            input_cmd = orfm.command_line(input_path)\n        elif seq_type == 'aminoacid':  # If the input is amino acid sequence\n            input_cmd = unpack.command_line()\n        else:\n            raise Exception('Programming Error: error guessing input sequence type')\n        \n        # Run the HMMsearches\n        if cutoff == \"--cut_tc\":\n            searcher = HmmSearcher(threads, cutoff) \n        else:\n            searcher = HmmSearcher(threads, '--domE %s' % cutoff)\n        searcher.hmmsearch(input_cmd, self.search_hmm, output_table_list)\n\n        hmmtables = [HMMSearchResult.import_from_hmmsearch_table(x) for x in output_table_list]\n        return hmmtables", "response": "This function searches raw reads for hits using the HMM search command line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nhmmer(self, output_path, unpack, threads, evalue):\n        '''\n        nhmmer - Search input path using nhmmer\n\n        Parameters\n        ----------\n        output_path : str\n            A string containing the path to the input sequences\n        unpack : obj\n            UnpackRawReads object, returns string command that will output\n            sequences to stdout when called on command line\n            (use: unpack.command_line())\n        threads : str\n            Number of threads to run. For compiling command line.\n        evalue : str\n            evalue to use. For compiling commmand line.\n\n        Returns\n        -------\n        output_table_list : array\n            Includes the name of the output domtblout table given by hmmer\n        '''\n        logging.debug(\"Using %i HMMs to search\" % (len(self.search_hmm)))\n        output_table_list = []\n        if len(self.search_hmm) > 1:\n            for hmm in self.search_hmm:\n                out = os.path.join(os.path.split(output_path)[0], os.path.basename(hmm).split('.')[0] + '_' + os.path.split(output_path)[1])\n                output_table_list.append(out)\n        elif len(self.search_hmm) == 1:\n            output_table_list.append(output_path)\n        else:\n            raise Exception(\"Programming error: Expected 1 or more HMMs\")\n        input_pipe = unpack.command_line()\n\n        searcher = NhmmerSearcher(threads, extra_args='--incE %s -E %s' % (evalue, evalue))\n        searcher.hmmsearch(input_pipe, self.search_hmm, output_table_list)\n\n        hmmtables = [HMMSearchResult.import_from_nhmmer_table(x) for x in output_table_list]\n\n        return hmmtables, output_table_list", "response": "This function searches the input path using nhmmer and returns the output path and the list of domtblout tables that are returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_euk_contamination(self, hmm_hit_tables):\n        '''\n        check_euk_contamination - Check output HMM tables hits reads that hit\n                                  the 18S HMM with a higher bit score.\n\n        Parameters\n        ----------\n        hmm_hit_tables : array\n            Array of paths to the output files produced by hmmsearch or\n            nhmmer.\n        run_stats : dict\n            A dictionary to updatewith the number of unique 18S reads and reads\n            detected by both 18S and non-18S HMMs\n\n        Returns\n        -------\n        euk_reads : set\n            Non-redundant set of all read names deemed to be eukaryotic\n        '''\n        \n        euk_hit_table = HMMreader(hmm_hit_tables.pop(-1))\n        other_hit_tables = [HMMreader(x) for x in hmm_hit_tables]\n        reads_unique_to_eukaryotes = []\n        reads_with_better_euk_hit = []\n\n        for hit in euk_hit_table.names():\n            bits = []\n            for hit_table in other_hit_tables:\n                if hit in hit_table.names():\n                    bits.append(hit_table.bit(hit))\n                else:\n                    reads_unique_to_eukaryotes.append(hit)\n            if bits:\n                if any([x for x in bits if x > euk_hit_table.bit(hit)]):\n                    continue\n                else:\n                    reads_with_better_euk_hit.append(hit)\n            else:\n                continue\n\n        if len(reads_with_better_euk_hit) == 0:\n            logging.info(\"No contaminating eukaryotic reads detected\")\n        else:\n            logging.info(\"Found %s read(s) that may be eukaryotic\" % len(reads_with_better_euk_hit + reads_unique_to_eukaryotes))\n\n        euk_reads = set(reads_with_better_euk_hit + reads_unique_to_eukaryotes)\n\n        return euk_reads", "response": "Check if the output HMM tables hits reads that hit the 18S HMM with a higher bit score."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_multiple_hits(self, hits, reads_path, output_path):\n        '''\n        splits out regions of a read that hit the HMM. For example when two of\n        same gene are identified within the same contig, The regions mapping to\n        the HMM will be split out and written out to a new file as a new record.\n\n        Parameters\n        ----------\n        hits : dict\n            A dictionary where the keys are the read names, the entry for each\n            is a list of lists, each containing the range within the contig\n            (or read) that mapped to the HMM. e.g.:\n\n            {'read1': [[3, 126], [305, 413]],\n             'read2': [[1, 125]],\n             ...\n\n        reads_path : str\n            path to reads file containing each read or contig in FASTA format to\n            be opened, and split.\n        output_path : str\n            path to file to which split reads will be written to in FASTA\n            format.\n\n        Returns\n        -------\n        Nothing, output path is known.\n        '''\n\n        complement_information = {}\n        try:\n            reads = SeqIO.to_dict(SeqIO.parse(reads_path, \"fasta\"))  # open up reads as dictionary\n        except:\n            logging.error(\"Multiple sequences found with the same ID. The input sequences are either ill formated or are interleaved. \\\nIf you provided GraftM with an interleaved sequence file, please split them into forward and reverse reads, and provide to the the appropriate \\\nflags (--forward, --reverse). Otherwise, it appears that you have provided sequences with redundant IDs. GraftM doesn't know how to \\\ndeal with these, so please remove/rename sequences with duplicate keys.\")\n            raise InterleavedFileError()\n        with open(output_path, 'w') as out:\n            for read_name, entry in hits.iteritems():  # For each contig\n                ranges = entry[\"entry\"]\n                complements = entry[\"strand\"]\n                index = 1\n                if len(ranges) > 1:  # if there are multiple hits in that contig\n                    for r, c in zip(ranges, complements):  # for each of those hits\n\n                        new_record = reads[read_name][r[0] - 1:r[1]]  # subset the record by the span of that hit\n                        new_record.id = new_record.id + '_split_%i' % index  # give that subset record a new header\n                        SeqIO.write(new_record, out, \"fasta\")  # and write it to output\n                        index += 1  # increment the split counter\n                        complement_information[new_record.id]=c\n\n                else:  # Otherwise, just write the read back to the file\n                    complement_information[read_name] = entry[\"strand\"][0] \n                    SeqIO.write(reads[read_name], out, \"fasta\")\n        \n        return complement_information", "response": "Extracts multiple contigs from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alignment_correcter(self, alignment_file_list, output_file_name,\n                            filter_minimum=None):\n        '''\n        Remove lower case insertions in alignment outputs from HMM align. Give\n        a list of alignments, and an output file name, and each alignment will\n        be corrected, and written to a single file, ready to be placed together\n        using pplacer.\n\n        Parameters\n        ----------\n        alignment_file_list : array\n            List of strings, each the path to different alignments from the\n            inputs provided to GraftM\n        output_file_name : str\n            The path and filename of the output file desired.\n        filter_minimum : int\n            minimum number of positions that must be aligned for each sequence\n        Returns\n        -------\n        True or False, depending if reads were written to file\n\n        '''\n\n        corrected_sequences = {}\n        for alignment_file in alignment_file_list:\n            insert_list = []  # Define list containing inserted positions to be removed (lower case characters)\n            sequence_list = list(SeqIO.parse(open(alignment_file, 'r'), 'fasta'))\n            for sequence in sequence_list:  # For each sequence in the alignment\n                for idx, nt in enumerate(list(sequence.seq)):  # For each nucleotide in the sequence\n                    if nt.islower():  # Check for lower case character\n                        insert_list.append(idx)  # Add to the insert list if it is\n            insert_list = list(OrderedDict.fromkeys(sorted(insert_list, reverse=True)))  # Reverse the list and remove duplicate positions\n            for sequence in sequence_list:  # For each sequence in the alignment\n                new_seq = list(sequence.seq)  # Define a list of sequences to be iterable list for writing\n                for position in insert_list:  # For each position in the removal list\n                    del new_seq[position]  # Delete that inserted position in every sequence\n                corrected_sequences['>' + sequence.id + '\\n'] = (''.join(new_seq) + '\\n').replace('~', '-')\n        \n        pre_filter_count=len(corrected_sequences)\n        \n        if filter_minimum:\n            # Use '>' not '>=' here because the sequence is on a single line, \n            # but also includes a newline character at the end of the sequence\n            corrected_sequences={key:item for key, item in corrected_sequences.iteritems() if len(item.replace('-', '')) > filter_minimum}\n        \n        post_filter_count=len(corrected_sequences)\n        logging.info(\"Filtered %i short sequences from the alignment\" % \\\n                        (pre_filter_count-post_filter_count)\n                    )\n        logging.info(\"%i sequences remaining\" % post_filter_count)\n        \n        if len(corrected_sequences) >= 1:\n            with open(output_file_name, 'w') as output_file:  # Create an open file to write the new sequences to\n                for fasta_id, fasta_seq in corrected_sequences.iteritems():\n                    output_file.write(fasta_id)\n                    output_file.write(fasta_seq)\n            return True\n        else:\n            return False", "response": "This function will remove lower case insertions in alignment outputs from HMM align and write to a single file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_orfs(self, input_path, orfm, hit_readnames, output_path, search_method, sequence_frame_info_list=None):\n        '''\n        Call ORFs on a file with nucleotide sequences and extract the proteins\n        whose name is in `hit_readnames`.\n\n        Parameters\n        ----------\n        input_path : str\n            Path to input nucleotide sequences in FASTA format.\n        orfm: obj\n            graftm.OrfM object with parameters already set\n        hit_readnames : str\n            path to a file containin the readnames of hits to the HMM, one per\n            line.\n        output_path : str\n            Path to output orfs into, in FASTA format.\n        search_method : str\n            The method for searching, either 'hmmsearch' or 'diamond'\n        sequence_frame_info : list\n            A dataframe (list of lists) containing readname, alignment direction\n            and alignment start point information\n        '''\n        \n        if search_method == \"hmmsearch\":\n            # Build and run command to extract ORF sequences:\n            orfm_cmd = orfm.command_line()\n            cmd = 'fxtract -H -X -f /dev/stdin <(%s %s) > %s' % (orfm_cmd, input_path, output_path)\n            process = subprocess.Popen([\"bash\", \"-c\", cmd],\n                                       stdin=subprocess.PIPE,\n                                       stdout=subprocess.PIPE)\n            process.communicate('\\n'.join(hit_readnames))\n\n        elif search_method == \"diamond\":\n            sequence_frame_info_dict = {x[0]:[x[1], x[2], x[3]] for x in sequence_frame_info_list}\n            records = SeqIO.parse(input_path, \"fasta\")\n            with open(output_path, 'w') as open_output_path:\n                for record in records:\n                    entry=sequence_frame_info_dict[record.id]\n                    indfrom=(min(entry[2], entry[1])-1)\n                    indto=max(entry[2], entry[1])\n                    if entry[0] == False:\n                        record.seq = max([x for x in record.seq[indfrom:indto].reverse_complement().translate().split(\"*\")], key=len)\n                    else:\n                        record.seq = max([x for x in record.seq[indfrom:indto].translate().split(\"*\")], key=len)\n                    SeqIO.write(record, open_output_path, \"fasta\")\n                open_output_path.flush()", "response": "This function will call ORFs on a file with nucleotide sequences and extract the proteins\n            whose name is in hit_readnames."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if any of the read names end in a slash.", "response": "def _check_for_slash_endings(self, readnames):\n        '''\n        Provide a list of read names to be checked for the /1 or /2 endings\n        which cause troubles downstream when working with paired reads. This\n        function checks the list for ANY read ending in /1 or /2. The reads are\n        first stripped of any comment lines, using the 'split' function\n\n        Parameters\n        ----------\n        readnames : list\n            list of strings, each a read name to be checked for the /1 or /2\n            ending\n\n        Returns\n        -------\n        Boolean. True if /1 or /2 endings are found, False if not.\n\n        '''\n\n        # Strip comment lines, if any\n        readnames = [x.split()[0] for x in readnames]\n\n        # Check for /1 and /2 endings:\n        if any(x for x in readnames if x.endswith('/1') or x.endswith('/2')):\n            result = True\n        else:\n            result = False\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nt_db_search(self, files, base, unpack, euk_check,\n                     search_method, maximum_range, threads, evalue):\n        '''\n        Nucleotide database search pipeline - pipeline where reads are searched\n        as nucleotides, and hits are identified using nhmmer searches\n\n        Parameters\n        ----------\n        files : obj\n            graftm_output_paths object.\n        base : str\n            The name of the input file, stripped of all suffixes, and paths.\n            Used for creating file names with 'files' object.\n        unpack : obj\n            UnpackRawReads object, returns string command that will output\n            sequences to stdout when called on command line\n            (use: unpack.command_line())\n        euk_check : bool\n            True indicates the sample will be checked for eukaryotic reads,\n            False indicates not.\n        search_method : str\n            The method for searching e.g. 'hmmsearch' or 'diamond'\n        maximum_range : int\n            Maximum range that a gene can extend within a contig. Any hits\n            that extend beyond this length cannot be linked. max_range is defined\n            as 1.5 X the average length of all full length genes used in the\n            search database. This is defined in the CONTENTS.json file within a\n            gpkg.\n        threads : str\n            Number of threads for hmmer to use\n        evalue : str\n            Evalue cutoff for hmmer to use\n        Returns\n        -------\n        String path to amino acid fasta file of reads that hit\n        '''\n\n        # Define outputs\n        hmmsearch_output_table = files.hmmsearch_output_path(base)\n        hit_reads_fasta = files.fa_output_path(base)\n        return \\\n            self.search_and_extract_nucleotides_matching_nucleotide_database(\\\n                                                      unpack,\n                                                      euk_check,\n                                                      search_method,\n                                                      maximum_range,\n                                                      threads,\n                                                      evalue,\n                                                      hmmsearch_output_table,\n                                                      hit_reads_fasta)", "response": "This function searches the nucleotide database for reads that match the input file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalign - Takes input path to fasta of unaligned reads aligns them to a HMM and returns the aligned reads in the output path.", "response": "def align(self, input_path, output_path, directions, pipeline, \n              filter_minimum):\n        '''align - Takes input path to fasta of unaligned reads, aligns them to\n        a HMM, and returns the aligned reads in the output path\n\n        Parameters\n        ----------\n        input_path : str\n        output_path : str\n        reverse_direction : dict\n            A dictionary of read names, with the entries being the complement\n            strand of the read (True = forward, False = reverse)\n        pipeline : str\n            Either \"P\" or \"D\" corresponding to the protein and nucleotide (DNA)\n            pipelines, respectively.\n\n\n        Returns\n        -------\n        N/A - output alignment path known.\n        '''\n\n        # HMMalign the forward reads, and reverse complement reads.\n        with tempfile.NamedTemporaryFile(prefix='for_conv_file', suffix='.fa') as fwd_fh:\n            fwd_conv_file = fwd_fh.name\n            with tempfile.NamedTemporaryFile(prefix='rev_conv_file', suffix='.fa') as rev_fh:\n                rev_conv_file = rev_fh.name\n                alignments = self._hmmalign(\n                    input_path,\n                    directions,\n                    pipeline,\n                    fwd_conv_file,\n                    rev_conv_file)\n                alignment_result = self.alignment_correcter(alignments,\n                                                            output_path,\n                                                            filter_minimum)\n                return alignment_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_longest_internal_edge(self, node):\n        '''return the node that has the longest branch length between the given\n        node and the root\n        \n        Parameters\n        ----------\n        node: dendropy.Tree\n            a node from the tree\n            \n        Returns\n        -------\n        The node that has the largest length between the node and the root_node\n        '''\n        max_length = -1\n        max_edge = None\n        while node and not node.edge.rootedge:\n            if node.edge.length > max_length:\n                max_edge = node.edge\n                max_length = max_edge.length\n            node = node.parent_node\n        return max_edge", "response": "return the node that has the longest branch length between the given node and the root_node\nCOOKIE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a collection of search_hmm_files and a list of contigs in generate a database from the contigs found in contig_files and output the results in output_database_file.", "response": "def generate_expand_search_database_from_contigs(self, contig_files, output_database_file, \n                                                 search_method):\n        '''Given a collection of search_hmm_files, search the contigs in \n        contig_files, and generate an HMM from the resulting hits, outputting\n        it as output_database_file.\n        \n        Parameters\n        ----------\n        contig_files: list of str\n            list of files to search\n        output_database_file: str\n            path to output file\n        search_method: str\n            \"diamond\" or \"hmmsearch\", to specify search method to use and what\n            type of database to build. \n        \n        Returns\n        -------\n        True if genes were recovered, else False'''\n        \n        ss = SequenceSearcher(self.search_hmm_files)\n        seqio = SequenceIO()\n        if search_method == self.DIAMOND_SEARCH_METHOD:\n            if self.diamond_database == None or self.unaligned_sequence_database == None:\n                logging.warning(\"Cannot expand_search continue with no diamond database or unaligned sequences.\") \n                return False\n        \n        with tempfile.NamedTemporaryFile(prefix='graftm_expand_search_orfs') as orfs:\n            logging.info(\"Finding expand_search hits in provided contigs..\")\n            for contig_file in contig_files:\n                logging.debug(\"Finding expand_search hits in %s..\" % contig_file)\n                unpack = UnpackRawReads(contig_file)\n                \n                with tempfile.NamedTemporaryFile(prefix='graftm_expand_search') as \\\n                                                        hit_reads_orfs_fasta:\n                    # search and extract matching ORFs\n                    with tempfile.NamedTemporaryFile(prefix='graftm_expand_search2') as \\\n                                                        hmmsearch_output_table:\n                        with tempfile.NamedTemporaryFile(prefix='graftm_expand_search3') as \\\n                                                        hit_reads_fasta:\n                            ss.search_and_extract_orfs_matching_protein_database(\\\n                                    unpack,\n                                    search_method,\n                                    self.maximum_range,\n                                    self.threads,\n                                    self.evalue,\n                                    self.min_orf_length,\n                                    None,\n                                    (self.diamond_database if self.diamond_database else None),\n                                    hmmsearch_output_table.name,\n                                    hit_reads_fasta.name,\n                                    hit_reads_orfs_fasta.name)\n                    # Append to the file\n                    shutil.copyfileobj(open(hit_reads_orfs_fasta.name), orfs)\n            \n            # Now have a fasta file of ORFs.\n            # Check to make sure the file is not zero-length\n            orfs.flush()\n            \n\n            \n            with tempfile.NamedTemporaryFile(prefix=\"graftm_expand_search_aln\") as aln:\n                    \n                if search_method == self.HMM_SEARCH_METHOD:\n                    \n                    # Check that there is more than one sequence to align.\n                    if len(seqio.read_fasta_file(orfs.name)) <= 1:# Just to build on this, you need to check if there is > 1 hit\n                                                                  # otherwise mafft will fail to align, causing a crash when hmmbuild is \n                                                                  # run on an empty file.\n                        logging.warn(\"Failed to find two or more matching ORFs in the expand_search contigs\")\n                        return False\n                    \n                    # Run mafft to align them\n                    cmd = \"mafft --auto %s >%s\" % (orfs.name, aln.name)\n                    logging.info(\"Aligning expand_search hits..\")\n                    extern.run(cmd)\n                \n                    # Run hmmbuild to create an HMM\n                    cmd = \"hmmbuild --amino %s %s >/dev/null\" % (output_database_file, aln.name)\n                    logging.info(\"Building HMM from expand_search hits..\")\n\n                    extern.run(cmd)\n                \n                elif search_method == self.DIAMOND_SEARCH_METHOD:\n\n                    # Concatenate database with existing database\n                    with tempfile.NamedTemporaryFile(prefix=\"concatenated_database\") as databasefile:\n                        for f in [orfs.name, self.unaligned_sequence_database]:\n                            for line in open(f):\n                                databasefile.write(line)\n                        databasefile.flush()\n                                \n                        # Run diamond make to create a diamond database\n                        cmd = \"diamond makedb --in '%s' -d '%s'\" % (databasefile.name, output_database_file)\n                        logging.info(\"Building a diamond database from expand_search hits..\")\n                        extern.run(cmd)\n                \n                else:\n                    raise Exception(\"Search method not recognised: %s\" % search_method)\n                    return False\n                \n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summarise(self, base_list, trusted_placements, reverse_pipe, times,\n                  hit_read_count_list, max_samples_for_krona):\n        '''\n        summarise - write summary information to file, including otu table, biom\n                    file, krona plot, and timing information\n\n        Parameters\n        ----------\n        base_list : array\n            list of each of the files processed by graftm, with the path and\n            and suffixed removed\n        trusted_placements : dict\n            dictionary of placements with entry as the key, a taxonomy string\n            as the value\n        reverse_pipe : bool\n            True = run reverse pipe, False = run normal pipeline\n        times : array\n            list of the recorded times for each step in the pipeline in the\n            format: [search_step_time, alignment_step_time, placement_step_time]\n        hit_read_count_list : array\n            list containing sublists, one for each file run through the GraftM\n            pipeline, each two entries, the first being the number of putative\n            eukaryotic reads (when searching 16S), the second being the number\n            of hits aligned and placed in the tree.\n        max_samples_for_krona: int\n            If the number of files processed is greater than this number, then\n            do not generate a krona diagram.\n        Returns\n        -------\n        '''\n\n        # Summary steps.\n        placements_list = []\n        for base in base_list:\n            # First assign the hash that contains all of the trusted placements\n            # to a variable to it can be passed to otu_builder, to be written\n            # to a file. :)\n            placements = trusted_placements[base]\n            self.s.readTax(placements, GraftMFiles(base, self.args.output_directory, False).read_tax_output_path(base))\n            placements_list.append(placements)\n\n        #Generate coverage table\n        #logging.info('Building coverage table for %s' % base)\n        #self.s.coverage_of_hmm(self.args.aln_hmm_file,\n        #                         self.gmf.summary_table_output_path(base),\n        #                         self.gmf.coverage_table_path(base),\n        #                         summary_dict[base]['read_length'])\n\n        logging.info('Writing summary table')\n        with open(self.gmf.combined_summary_table_output_path(), 'w') as f:\n            self.s.write_tabular_otu_table(base_list, placements_list, f)\n\n        logging.info('Writing biom file')\n        with biom_open(self.gmf.combined_biom_output_path(), 'w') as f:\n            biom_successful = self.s.write_biom(base_list, placements_list, f)\n        if not biom_successful:\n            os.remove(self.gmf.combined_biom_output_path())\n\n        logging.info('Building summary krona plot')\n        if len(base_list) > max_samples_for_krona:\n            logging.warn(\"Skipping creation of Krona diagram since there are too many input files. The maximum can be overridden using --max_samples_for_krona\")\n        else:\n            self.s.write_krona_plot(base_list, placements_list, self.gmf.krona_output_path())\n\n        # Basic statistics\n        placed_reads=[len(trusted_placements[base]) for base in base_list]\n        self.s.build_basic_statistics(times, hit_read_count_list, placed_reads, \\\n                                      base_list, self.gmf.basic_stats_path())\n\n        # Delete unnecessary files\n        logging.info('Cleaning up')\n        for base in base_list:\n            directions = ['forward', 'reverse']\n            if reverse_pipe:\n                for i in range(0,2):\n                    self.gmf = GraftMFiles(base, self.args.output_directory, directions[i])\n                    self.hk.delete([self.gmf.for_aln_path(base),\n                                    self.gmf.rev_aln_path(base),\n                                    self.gmf.conv_output_rev_path(base),\n                                    self.gmf.conv_output_for_path(base),\n                                    self.gmf.euk_free_path(base),\n                                    self.gmf.euk_contam_path(base),\n                                    self.gmf.readnames_output_path(base),\n                                    self.gmf.sto_output_path(base),\n                                    self.gmf.orf_titles_output_path(base),\n                                    self.gmf.orf_output_path(base),\n                                    self.gmf.output_for_path(base),\n                                    self.gmf.output_rev_path(base)])\n            else:\n                self.gmf = GraftMFiles(base, self.args.output_directory, False)\n                self.hk.delete([self.gmf.for_aln_path(base),\n                                self.gmf.rev_aln_path(base),\n                                self.gmf.conv_output_rev_path(base),\n                                self.gmf.conv_output_for_path(base),\n                                self.gmf.euk_free_path(base),\n                                self.gmf.euk_contam_path(base),\n                                self.gmf.readnames_output_path(base),\n                                self.gmf.sto_output_path(base),\n                                self.gmf.orf_titles_output_path(base),\n                                self.gmf.orf_output_path(base),\n                                self.gmf.output_for_path(base),\n                                self.gmf.output_rev_path(base)])\n\n        logging.info('Done, thanks for using graftM!\\n')", "response": "This function generates a summary of the krona diagram for each base in the list of base files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _assign_taxonomy_with_diamond(self, base_list, db_search_results,\n                                      graftm_package, graftm_files):\n        '''Run diamond to assign taxonomy\n\n        Parameters\n        ----------\n        base_list: list of str\n            list of sequence block names\n        db_search_results: list of DBSearchResult\n            the result of running hmmsearches\n        graftm_package: GraftMPackage object\n            Diamond is run against this database\n        graftm_files: GraftMFiles object\n            Result files are written here\n\n        Returns\n        -------\n        list of\n        1. time taken for assignment\n        2. assignments i.e. dict of base_list entry to dict of read names to\n            to taxonomies, or None if there was no hit detected.\n        '''\n        runner = Diamond(graftm_package.diamond_database_path(),\n                         self.args.threads,\n                         self.args.evalue)\n        taxonomy_definition = Getaxnseq().read_taxtastic_taxonomy_and_seqinfo\\\n                (open(graftm_package.taxtastic_taxonomy_path()),\n                 open(graftm_package.taxtastic_seqinfo_path()))\n        results = {}\n\n        # For each of the search results,\n        for i, search_result in enumerate(db_search_results):\n            sequence_id_to_hit = {}\n            # Run diamond\n            logging.debug(\"Running diamond on %s\" % search_result.hit_fasta())\n            diamond_result = runner.run(search_result.hit_fasta(),\n                                        UnpackRawReads.PROTEIN_SEQUENCE_TYPE,\n                                        daa_file_basename=graftm_files.diamond_assignment_output_basename(base_list[i]))\n            for res in diamond_result.each([SequenceSearchResult.QUERY_ID_FIELD,\n                                            SequenceSearchResult.HIT_ID_FIELD]):\n                if res[0] in sequence_id_to_hit:\n                    # do not accept duplicates\n                    if sequence_id_to_hit[res[0]] != res[1]:\n                        raise Exception(\"Diamond unexpectedly gave two hits for a single query sequence for %s\" % res[0])\n                else:\n                    sequence_id_to_hit[res[0]] = res[1]\n\n            # Extract taxonomy of the best hit, and add in the no hits\n            sequence_id_to_taxonomy = {}\n            for seqio in SequenceIO().read_fasta_file(search_result.hit_fasta()):\n                name = seqio.name\n                if name in sequence_id_to_hit:\n                    # Add Root; to be in line with pplacer assignment method\n                    sequence_id_to_taxonomy[name] = ['Root']+taxonomy_definition[sequence_id_to_hit[name]]\n                else:\n                    # picked up in the initial search (by hmmsearch, say), but diamond misses it\n                    sequence_id_to_taxonomy[name] = ['Root']\n\n            results[base_list[i]] = sequence_id_to_taxonomy\n        return results", "response": "Run Diamond to assign taxonomy to the base_list entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconcatenates a list of files into a single file.", "response": "def _concatenate_file(self, file_list, output):\n        '''\n        Call unix \"cat\" to concatenate a list of files\n\n        Parameters\n        ----------\n        file_list: list\n            List of strings, each leading to a file. These files are the ones to\n            be concatenate together. E.g.:\n                [\"/path/to/file1\", \"/path/to/file2\"]\n        output: str\n            Path to file to which to the files in file_list will be concatenated\n            into.\n\n        '''\n        to_cat = ' '.join(file_list)\n        logging.debug(\"Concatenating files: %s\" % (to_cat))\n        cmd = \"cat %s > %s\" % (to_cat, output)\n        extern.run(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate an existing GraftM package with new sequences and taxonomy.", "response": "def update(self, **kwargs):\n        '''\n        Update an existing GraftM package with new sequences and taxonomy. If no\n        taxonomy is provided, attempt to decorate the new sequences with\n        pre-existing taxonomy.\n\n        Parameters\n        ----------\n        input_sequence_path: str\n            Path to FASTA file containing sequences to add to the update GraftM\n            package\n        input_taxonomy_path: str\n            Taxonomy corresponding to the sequences in input_sequence_path. If None,\n            then attempt to assign taxonomy by decorating the tree made out of all\n            sequences.\n        input_graftm_package_path: str\n            Path to the directory of the GraftM package that is to be updated\n        output_graftm_package_path: str\n            Path to the directory to which the new GraftM package will be\n            written to\n        '''\n        input_sequence_path = kwargs.pop('input_sequence_path')\n        input_taxonomy_path = kwargs.pop('input_taxonomy_path', None)\n        input_graftm_package_path = kwargs.pop('input_graftm_package_path')\n        output_graftm_package_path = kwargs.pop('output_graftm_package_path')\n        threads = kwargs.pop('threads', UpdateDefaultOptions.threads) #TODO: add to user options\n        if len(kwargs) > 0:\n            raise Exception(\"Unexpected arguments detected: %s\" % kwargs)\n\n        logging.info(\"Reading previous GraftM package\")\n        old_gpkg = GraftMPackage.acquire(input_graftm_package_path)\n        min_input_version = 3\n        if old_gpkg.version < min_input_version:\n            raise InsufficientGraftMPackageVersion(\n                \"GraftM below version %s cannot be updated using the update function.\" % min_input_version +\n                \" Unaligned sequences are not included in these packages, therefore no new\"\n                \" alignment/HMM/Tree can be created\")\n\n        new_gpkg = UpdatedGraftMPackage()\n        new_gpkg.output = output_graftm_package_path\n        new_gpkg.name = output_graftm_package_path.replace(\".gpkg\", \"\")\n\n        #######################################\n        ### Collect all unaligned sequences ###\n        logging.info(\"Concatenating unaligned sequence files\")\n        new_gpkg.unaligned_sequences = \"%s_sequences.fa\" % (new_gpkg.name) #TODO: replace hard-coded paths like this with tempfiles\n        self._concatenate_file([old_gpkg.unaligned_sequence_database_path(),\n                                input_sequence_path],\n                               new_gpkg.unaligned_sequences)\n\n        #########################################################\n        ### Parse taxonomy info up front so errors come early ###\n        if input_taxonomy_path:\n            logging.info(\"Reading new taxonomy information\")\n            input_taxonomy = GreenGenesTaxonomy.read_file(input_taxonomy_path)\n            original_taxonomy_hash = old_gpkg.taxonomy_hash()\n            total_taxonomy_hash = original_taxonomy_hash.copy()\n            total_taxonomy_hash.update(input_taxonomy.taxonomy)\n            num_duplicate_taxonomies = len(total_taxonomy_hash) - \\\n                                       len(input_taxonomy.taxonomy) - \\\n                                       len(original_taxonomy_hash)\n            logging.debug(\"Found %i taxonomic definitions in common between the previous and updated taxonomies\" % num_duplicate_taxonomies)\n            if num_duplicate_taxonomies > 0:\n                logging.warn(\"Found %i taxonomic definitions in common between the previous and updated taxonomies. Using the updated taxonomy in each case.\" % num_duplicate_taxonomies)\n\n        ###############################\n        ### Re-construct alignments ###\n        logging.info(\"Multiple sequence aligning all sequences\")\n        new_gpkg.aligned_sequences = \"%s_mafft_alignment.fa\" % (new_gpkg.name)\n        self._align_sequences(new_gpkg.unaligned_sequences, new_gpkg.aligned_sequences, threads)\n\n        ########################\n        ### Re-construct HMM ###\n        logging.info(\"Creating HMM from alignment\")\n        new_gpkg.hmm = \"%s.hmm\" % (new_gpkg.name)\n        new_gpkg.hmm_alignment = \"%s_hmm_alignment.fa\" % (new_gpkg.name)\n        self._get_hmm_from_alignment(new_gpkg.aligned_sequences, new_gpkg.hmm, new_gpkg.hmm_alignment)\n\n        #########################\n        ### Re-construct tree ###\n        logging.info(\"Generating phylogenetic tree\")\n        new_gpkg.unrooted_tree = \"%s.tre\" % (new_gpkg.name)\n        new_gpkg.unrooted_tree_log = \"%s.tre.log\" % (new_gpkg.name)\n        new_gpkg.package_type, new_gpkg.hmm_length = self._pipe_type(old_gpkg.alignment_hmm_path())\n        new_gpkg.unrooted_gpkg_tree_log, new_gpkg.unrooted_gpkg_tree = \\\n            self._build_tree(new_gpkg.hmm_alignment, new_gpkg.name,\n                             new_gpkg.package_type, self.fasttree)\n\n        ##############################################\n        ### Re-root and decorate tree if necessary ###\n        if input_taxonomy_path:\n            new_gpkg.gpkg_tree_log = new_gpkg.unrooted_tree_log\n            new_gpkg.gpkg_tree = new_gpkg.unrooted_gpkg_tree\n        else:\n            logging.info(\"Finding taxonomy for new sequences\")\n            rerooter = Rerooter()\n            \n            old_tree = Tree.get(path=old_gpkg.reference_package_tree_path(),\n                                schema='newick')\n            new_tree = Tree.get(path=new_gpkg.unrooted_gpkg_tree,\n                                schema='newick')\n            old_tree = rerooter.reroot(old_tree)\n            new_tree = rerooter.reroot(new_tree)\n            # TODO: Shouldn't call an underscore method, eventually use\n            # Rerooter instead.\n            rerooted_tree = rerooter.reroot_by_tree(old_tree, new_tree)\n            new_gpkg.gpkg_tree = \"%s_gpkg.tree\" % new_gpkg.name\n            td = TreeDecorator(\n                rerooted_tree,\n                old_gpkg.taxtastic_taxonomy_path(),\n                old_gpkg.taxtastic_seqinfo_path())\n            \n            with tempfile.NamedTemporaryFile(suffix='tsv') as taxonomy:\n                td.decorate(new_gpkg.gpkg_tree, taxonomy.name, True) \n                total_taxonomy_hash = GreenGenesTaxonomy.read_file(taxonomy.name).taxonomy\n\n            ################################\n            ### Generating tree log file ###\n            logging.info(\"Generating phylogenetic tree log file\")\n            new_gpkg.gpkg_tree = \"%s_gpkg.tree\" % new_gpkg.name\n            new_gpkg.gpkg_tree_log = \"%s_gpkg.tree.log\" % new_gpkg.name\n            self._generate_tree_log_file(new_gpkg.unrooted_tree,\n                                         new_gpkg.hmm_alignment,\n                                         new_gpkg.gpkg_tree,\n                                         new_gpkg.gpkg_tree_log,\n                                         new_gpkg.package_type,\n                                         self.fasttree)\n\n        ################################\n        ### Creating taxtastic files ###\n        logging.info(\"Writing new taxonomy files\")\n        new_gpkg.tt_seqinfo = \"%s_seqinfo.csv\" % new_gpkg.name\n        new_gpkg.tt_taxonomy = \"%s_taxonomy.csv\" % new_gpkg.name\n        gtns = Getaxnseq()\n\n        gtns.write_taxonomy_and_seqinfo_files(\n            total_taxonomy_hash,\n            new_gpkg.tt_taxonomy,\n            new_gpkg.tt_seqinfo)\n        \n        ######################\n        ### Compile refpkg ###\n        logging.info(\"Compiling pplacer refpkg\")\n        new_gpkg.refpkg = \"%s.refpkg\" % (new_gpkg.name)\n        refpkg = self._taxit_create(new_gpkg.name,\n                                    new_gpkg.hmm_alignment,\n                                    new_gpkg.gpkg_tree,\n                                    new_gpkg.gpkg_tree_log,\n                                    new_gpkg.tt_taxonomy,\n                                    new_gpkg.tt_seqinfo,\n                                    new_gpkg.refpkg,\n                                    True)\n\n        #####################################\n        ### Re-construct diamond database ###\n        logging.info(\"Recreating DIAMOND DB\")\n        new_gpkg.diamond_database = \"%s.dmnd\" % (new_gpkg.name)\n        self._create_dmnd_database(new_gpkg.unaligned_sequences, new_gpkg.name)\n\n        ####################\n        ### Compile gpkg ###\n        logging.info(\"Compiling GraftM package\")\n        new_gpkg.name = \"%s.gpkg\" % new_gpkg.name\n        GraftMPackageVersion3.compile(new_gpkg.name, new_gpkg.refpkg,\n                                      new_gpkg.hmm, new_gpkg.diamond_database,\n                                      self._define_range(new_gpkg.unaligned_sequences),\n                                      new_gpkg.unaligned_sequences,\n                                      search_hmm_files=old_gpkg.search_hmm_paths())\n\n        ###################\n        ### Test it out ###\n        logging.info(\"Testing newly updated GraftM package works\")\n        self._test_package(new_gpkg.name)\n\n        logging.info(\"Finished\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo not include the. daa part that diamond creates", "response": "def diamond_search_output_basename(self, out_path):\n        '''Does not include the .daa part that diamond creates'''\n        return os.path.join(self.outdir, out_path, \"%s_diamond_search\" % self.basename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diamond_assignment_output_basename(self, out_path):\n        '''Does not include the .daa part that diamond creates'''\n        return os.path.join(self.outdir, out_path, \"%s_diamond_assignment\" % out_path)", "response": "Does not include the. daa part that diamond creates"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing in a taxonomy from the given I/O stream.", "response": "def read(input_taxonomy_io):\n        '''Parse in a taxonomy from the given I/O.\n\n        Parameters\n        ----------\n        input_taxonomy_io: io\n            an open io object that is iterable. This stream is neither opened\n            nor closed by this method\n\n        Returns\n        -------\n        A GreenGenesTaxonomy instance with the taxonomy parsed in\n\n        Raises\n        ------\n        DuplicateTaxonomyException: if there is duplicate taxonomy entries\n        MalformedGreenGenesTaxonomyException: if there is something else amiss'''\n        tax = {}\n        for line in input_taxonomy_io:\n            if len(line.strip()) == 0: continue #ignore empty lines\n            splits = line.split(\"\\t\")\n            if len(splits) != 2:\n                raise MalformedGreenGenesTaxonomyException(\"Unexpected number of tab-separated fields found in taxonomy file, on line %s\" % line)\n            name = splits[0].strip()\n            taxonomy = [t.strip() for t in splits[1].split(';')]\n            while len(taxonomy) > 0 and taxonomy[-1] == '':\n                taxonomy = taxonomy[:-1]\n            for lineage in taxonomy:\n                if lineage == '':\n                    raise MalformedGreenGenesTaxonomyException(\"Encountered a taxonomy string with the middle of the taxonomy string missing: %s\" % line)\n\n            if name in tax:\n                raise DuplicateTaxonomyException(\"Duplicate definition of taxonomy for %s\" % name)\n            tax[name] = taxonomy\n        return GreenGenesTaxonomy(tax)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nliking read except uses a file rather than an IO stream for convenience", "response": "def read_file(input_filename):\n        '''Like read() except uses a file rather than an IO stream, for convenience'''\n        with open(input_filename) as f:\n            g = GreenGenesTaxonomy.read(f)\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, output_io):\n        '''Write a taxonomy to an open stream out in GG format. Code calling this\n        function must open and close the io object.'''\n        for name, tax in self.taxonomy.items():\n            output_io.write(\"%s\\t%s\\n\" % (name, '; '.join(tax)))", "response": "Write a taxonomy to an open stream out in GG format. Code calling this\n            function must open and close the io object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the re - rooting algorithm in the Rerooter class.", "response": "def _reroot(self):\n        '''Run the re-rooting algorithm in the Rerooter class.'''\n        rerooter = Rerooter()\n        self.tree = rerooter.reroot_by_tree(self.reference_tree, \n                                                self.tree)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(self, taxonomy, output_tree, output_tax, no_unique_tax,\n             decorate, seqinfo):\n        '''Decorate and if necessary, re-root the tree. If an old reference tree\n        is provided it is assumed that re-rooting is desired\n        \n        Parameters\n        ----------\n        taxonomy: str\n            Path to a GreenGenes formatted, or taxtastic formatted taxonomy \n            file, containing the taxonomy of all or some if the sequences used \n            to construct the tree. This taxonomy will be used by the \n            tree_decorator.TreeDecorator class to decorate self.tree\n        output_tree: str\n            Path to file to which the decorated tree will be written to.\n        output_tax: str\n            Path to file to which the decorated taxonomy will be written to.\n        seqinfo: str\n            path to taxtastic seqinfo file to be used with taxonomy to annotate\n            the tree.'''\n        # Reroot\n        if self.reference_tree:\n            self._reroot()\n        # Decorate\n        td =  TreeDecorator(self.tree,\n                            taxonomy,\n                            seqinfo)\n        if decorate:\n            td.decorate(output_tree, output_tax, no_unique_tax)\n        else:\n            logging.info(\"Writing tree to file: %s\" % (output_tree))\n            self.tree.write(\n                            output_tree,\n                            format = \"newick\"   \n                            )", "response": "Decorate and if necessary re - root the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string to run OrfM with assuming sequences are incoming on the input path and printed to stdout", "response": "def command_line(self, input_path=None):\n        '''Return a string to run OrfM with, assuming sequences are incoming on\n        stdin and printed to stdout\n        \n        Parameters\n        ----------\n        input_path: str\n            path to the input path, or None for STDIN being the input\n        '''\n        \n        if self.min_orf_length:\n            orfm_arg_l = \" -m %d\" % self.min_orf_length\n        else:\n            orfm_arg_l = ''\n        \n        if self.restrict_read_length:\n            orfm_arg_l = \" -l %d\" % self.restrict_read_length\n            \n        cmd = 'orfm %s ' % orfm_arg_l\n        if input_path:\n            cmd += input_path\n        logging.debug(\"OrfM command chunk: %s\" % cmd)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef file_basename(self, file):\n        '''\n        Strips the path and last extension from the file variable. If the\n        extension is found to be valid, the basename will be returned. Otherwise\n        an error will be raise and graftM will exit\n        '''\n        valid_extensions = set(\".tree\",\n                               \".tre\")\n        split_file = os.path.basename(file).split('.')\n        base, suffix = '.'.join(split_file[:-1]), split_file[-1]\n\n        if suffix in valid_extensions:\n            return base\n        else:\n            logging.error(\"Invalid file extension found on file: %s\" % file)\n            logging.error(\"For trees, please provide a file with one of the \\\nfollowing extensions: %s\" % ' '.join(valid_extensions.keys()))\n            raise InvalidFileExtensionError", "response": "Returns the basename of the file variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the hmm used by graftM to cross check for euks.", "response": "def set_euk_hmm(self, args):\n        'Set the hmm used by graftM to cross check for euks.'\n        if hasattr(args, 'euk_hmm_file'):\n            pass\n        elif not hasattr(args, 'euk_hmm_file'):\n            # set to path based on the location of bin/graftM, which has\n            # a more stable relative path to the HMM when installed through\n            # pip.\n            setattr(args, 'euk_hmm_file', os.path.join(os.path.dirname(inspect.stack()[-1][1]),'..','share', '18S.hmm'))\n        else:\n            raise Exception('Programming Error: setting the euk HMM')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_file_existence(self, files):\n        '''Iterate through files and exit(1) if any do not pass\n        os.path.isfile'''\n        for f in files:\n            if not os.path.isfile(f):\n                logging.error(\"The file '%s' does not appear to exist, stopping\" % f)\n                exit(1)", "response": "Iterate through files and exit if any do not exist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the maximum length of the contigs that can be linked to the profile at the specified HMM.", "response": "def get_maximum_range(self, hmm):\n        '''\n        If no maximum range has been specified, and if using a hmm search, a\n        maximum range can be determined by using the length of the HMM\n\n        Parameters\n        ----------\n        hmm : str\n            path to hmm profile\n\n        Returns\n        -------\n        Length to search to when linking hits on a single contig\n        '''\n        length=int([x for x in open(hmm) if x.startswith(\"LENG\")][0].split()[1])\n        max_length=round(length*1.5, 0)\n\n        return max_length"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the fasta file by only keeping sequences that hit a proper sequence and only keeping sequences that hit a decoy sequence.", "response": "def filter(self, candidate_sequences_fasta_path, filtered_output_fasta_path):\n        '''Filter the fasta file by only keeping sequences that hit a proper sequence\n        better than the decoy database.\n            \n        Parameters\n        ----------\n        candidate_sequences_fasta_path: str\n            path to candidate sequences fasta file i.e. sequences to be filtered.\n        filtered_output_fasta_path: str\n            path to file which, after this method is run, will contain only\n            those sequences which pass the filter.\n            \n        Returns\n        -------\n        False if no sequences remain after filtering, else True.\n        '''\n        # Run query sequences against the proper database\n        seq_ids_and_bitscores = {}\n        logging.debug(\"Running diamond against the non-decoy sequences\")\n        pd = self._proper_hits_diamond.run(\n            candidate_sequences_fasta_path,\n            UnpackRawReads.PROTEIN_SEQUENCE_TYPE)\n        for res in pd.each([SequenceSearchResult.QUERY_ID_FIELD,\n                            SequenceSearchResult.ALIGNMENT_BIT_SCORE]):\n            seq = res[0]\n            score = float(res[1])\n            # Possible a single sequence gets 2 split up hits (maybe), so take\n            # the highest bitscore.\n            if seq not in seq_ids_and_bitscores or seq_ids_and_bitscores[seq] < score:\n                seq_ids_and_bitscores[seq] = score\n\n        num_before_decoy_removal = len(seq_ids_and_bitscores)\n        logging.info(\"Found %i sequences which hit the non-decoy sequences\" %\\\n                     num_before_decoy_removal)\n\n        if self._decoy_diamond is None:\n            logging.debug(\"Not running against the decoy database\")\n        else:\n            # Run the query sequences against the decoy database, removing from the\n            # list any sequences which hit better the decoy DB.\n            logging.debug(\"Running diamond against decoy sequences\")\n            pd = self._decoy_diamond.run(\n                candidate_sequences_fasta_path,\n                UnpackRawReads.PROTEIN_SEQUENCE_TYPE)\n            for res in pd.each([SequenceSearchResult.QUERY_ID_FIELD,\n                                SequenceSearchResult.ALIGNMENT_BIT_SCORE]):\n                seq = res[0]\n                score = float(res[1])\n                if seq in seq_ids_and_bitscores and seq_ids_and_bitscores[seq] < score:\n                    logging.debug(\"Removing sequence with better hit to the decoy database: %s\" % seq)\n                    del seq_ids_and_bitscores[seq]\n\n            logging.info(\"Removed %i\"\n                         \" sequences which hit the decoy sequences better\"\n                         \" than the non-decoy sequences\" %\\\n                         (num_before_decoy_removal-len(seq_ids_and_bitscores)))\n            if len(seq_ids_and_bitscores) == 0:\n                return False\n\n        # Extract the found sequences into the output file\n        logging.debug(\"Extracting query sequences\")\n        SequenceExtractor().extract(seq_ids_and_bitscores.keys(),\n                                    candidate_sequences_fasta_path,\n                                    filtered_output_fasta_path)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting reads that hit multiple HMMs to the databases to which they had the highest bit score. Return a dictionary containing the keys and the number of hits as the values.", "response": "def _interpret_hits(self, results_list, base_list):\n        '''Sort reads that hit multiple HMMs to the databases to which they had\n        the highest bit score. Return a dictionary containing HMMs as keys, and \n        number of hits as the values.\n        \n        This function is set up so that the read names could easily be returned\n        instead of numbers, for future development of GraftM\n        \n        Parameters\n        ----------\n        results_list: list\n            Iterable if SequenceSearchResult objects. e.g.\n                [SequenceSearchResult_1, SequenceSearchResult_2, ...]\n        \n        base_list: list\n            Iterable of the basenames for each sequence file provided to graftM\n            e.g.\n                [sample_1, sample_2, ...]\n                \n        Returns\n        -------\n        dictionary:\n            Contains samples as entries. The value for each sample is another \n            dictionary with HMM as the key, and number of hits as values:\n                {\"sample_1\":{HMM_1: 12\n                             HMM_2: 35\n                             HMM_3: 1258\n                             ...}\n                 ...\n                }\n            \n        '''\n        logging.debug(\"Sorting reads into HMMs by bit score\")\n        \n        run_results = {}\n\n        ########################################################################\n        ################## - Sort reads to best hit db - #######################\n        for base, results in zip(base_list, results_list): # For each sample\n            search_results = {}\n            for search in results():\n                search_list = list(\n                                   search.each([SequenceSearchResult.QUERY_ID_FIELD,\n                                                SequenceSearchResult.ALIGNMENT_BIT_SCORE,\n                                                SequenceSearchResult.HMM_NAME_FIELD])\n                                   )\n                for hit in search_list:\n                    if hit[0] in search_results:\n                        if float(hit[1]) > search_results[hit[0]][1]:\n                            search_results[hit[0]] = [float(hit[1]), hit[2]]\n                    else:\n                        search_results[hit[0]] = [float(hit[1]), hit[2]]\n            run_results[base] = search_results\n        \n        ########################################################################\n        ################## - Gather counts for each db - #######################\n        db_count = {}\n        for run in run_results.keys():\n            run_count = {}\n            for entry in run_results[run].values():\n                key = entry[1]\n                if key in run_count:\n                    run_count[key] += 1\n                else:\n                    run_count[key] = 1\n            db_count[run] = run_count\n        \n        return db_count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_results(self, db_count, output_path):\n        '''Write the table to the output_path directory\n        \n        db_count: dict\n            Contains samples as entries. The value for each sample is another \n            dictionary with HMM as the key, and number of hits as values:\n                {\"sample_1\":{HMM_1: 12\n                             HMM_2: 35\n                             HMM_3: 1258\n                             ...}\n                 ...\n                }\n        \n        output_path: str\n            Path to output file to which the resultant output file will be \n            written to.\n        '''\n        \n        logging.debug(\"Writing search otu table to file: %s\" % output_path)\n        \n        output_dict = {}\n        \n        for idx, value_dict in enumerate(db_count.values()):\n            for database, count in value_dict.iteritems():\n                if database in output_dict:\n                    output_dict[database].append(str(count))\n                else:\n                    output_dict[database] = ['0']*idx + [str(count)]\n            \n            for key, item in output_dict.iteritems():\n                if len(item) == idx:\n                    output_dict[key].append('0')\n        \n        with open(output_path, 'w') as out:\n            out.write('\\t'.join([\"#ID\"] + db_count.keys()) + '\\n')\n            for key, item in output_dict.iteritems():\n                out.write(\"%s\\t%s\" % (key, '\\t'.join(item)) + '\\n' )", "response": "Write the results to the output_path directory\n        \nMimeType output_path db_count dictionary containing the number of hits for each database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_search_otu_table(self, search_results_list, base_list, output_path):\n        '''\n        Build an OTU from SequenceSearchResult objects\n        \n        Parameters\n        ----------\n        search_results_list: list\n            Iterable if SequenceSearchResult objects. e.g.\n                [SequenceSearchResult_1, SequenceSearchResult_2, ...]\n        base_list: list\n            Iterable of the basenames for each sequence file provided to graftM\n            e.g.\n                [sample_1, sample_2, ...]\n        output_path: str\n            Path to output file to which the resultant output file will be \n            written to.\n        '''\n        \n        db_count = self._interpret_hits(search_results_list,\n                                        base_list)\n        \n        self._write_results(db_count, output_path)", "response": "Builds an OTU table from a list of SequenceSearchResult objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a dict of keys to aliases.", "response": "def convert_cluster_dict_keys_to_aliases(self, cluster_dict, alias_hash):\n        '''\n        Parameters\n        ----------\n        cluster_dict : dict\n            dictionary stores information on pre-placement clustering\n        alias_hash : dict\n            Stores information on each input read file given to GraftM, the\n            corresponding reads found within each file, and their taxonomy\n\n        Returns\n        --------\n        updated cluster_dict dict containing alias indexes for keys.\n        '''\n        output_dict = {}\n        directory_to_index_dict = {os.path.split(item[\"output_path\"])[0] : key\n                            for key, item in alias_hash.iteritems()}\n        for key, item in cluster_dict.iteritems():\n            cluster_file_directory = os.path.split(key)[0]\n            cluster_idx = directory_to_index_dict[cluster_file_directory]\n            output_dict[cluster_idx] = item\n        return output_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jplace_split(self, original_jplace, cluster_dict):\n        '''\n        To make GraftM more efficient, reads are dereplicated and merged into\n        one file prior to placement using pplacer. This function separates the\n        single jplace file produced by this process into the separate jplace\n        files, one per input file (if multiple were provided) and backfills\n        abundance (re-replicates?) into the placement file so analyses can be\n        done using the placement files.\n\n        Parameters\n        ----------\n        original_jplace : dict (json)\n            json .jplace file from the pplacer step.\n        cluster_dict : dict\n            dictionary stores information on pre-placement clustering\n\n        Returns\n        -------\n        A dict containing placement hashes to write to\n        new jplace file. Each key represents a file alias\n        '''\n        output_hash = {}\n\n        for placement in original_jplace['placements']: # for each placement\n            alias_placements_list = []\n            nm_dict = {}\n\n            p = placement['p']\n            if 'nm' in placement.keys():\n                nm = placement['nm']\n            elif 'n' in placement.keys():\n                nm = placement['n']\n            else:\n                raise Exception(\"Unexpected jplace format: Either 'nm' or 'n' are expected as keys in placement jplace .JSON file\")\n\n            for nm_entry in nm:\n                nm_list = []\n                placement_read_name, plval = nm_entry\n                read_alias_idx = placement_read_name.split('_')[-1] # Split the alias\n                                    # index out of the read name, which\n                                    # corresponds to the input file from\n                                    # which the read originated.\n                read_name = '_'.join(placement_read_name.split('_')[:-1])\n                read_cluster = cluster_dict[read_alias_idx][read_name]\n                for read in read_cluster:\n                    nm_list.append([read.name, plval])\n                if read_alias_idx not in nm_dict:\n                    nm_dict[read_alias_idx] = nm_list\n                else:\n                    nm_dict[read_alias_idx] += nm_entry\n\n            for alias_idx, nm_list in nm_dict.iteritems():\n                placement_hash = {'p': p,\n                                  'nm': nm_list}\n                if alias_idx not in output_hash:\n                    output_hash[alias_idx] = [placement_hash]\n                else:\n                    output_hash[alias_idx].append(placement_hash)\n        return output_hash", "response": "This function splits the original jplace file into two sets of jplace files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef place(self, reverse_pipe, seqs_list, resolve_placements, files, args,\n              slash_endings, tax_descr, clusterer):\n        '''\n        placement - This is the placement pipeline in GraftM, in aligned reads\n                    are placed into phylogenetic trees, and the results interpreted.\n                    If reverse reads are used, this is where the comparisons are made\n                    between placements, for the summary tables to be build in the\n                    next stage.\n\n        Parameters\n        ----------\n        reverse_pipe : bool\n            True: reverse reads are placed separately\n            False: no reverse reads to place.\n        seqs_list : list\n            list of paths to alignment fastas to be placed into the tree\n        resolve_placements : bool\n            True:resolve placements to their most trusted taxonomy\n            False: classify reads to their most trusted taxonomy, until the\n                   confidence cutoff is reached.\n        files : list\n            graftM output file name object\n        args : obj\n            argparse object\n        Returns\n        -------\n        trusted_placements : dict\n            dictionary of reads and their trusted placements\n        '''\n        trusted_placements = {}\n        files_to_delete = []\n        # Merge the alignments so they can all be placed at once.\n        alias_hash = self.alignment_merger(seqs_list, files.comb_aln_fa())\n        files_to_delete += seqs_list\n        files_to_delete.append(files.comb_aln_fa())\n        if os.path.getsize(files.comb_aln_fa()) == 0:\n            logging.debug(\"Combined alignment file has 0 size, not running pplacer\")\n            to_return = {}\n            for idx, file in enumerate(seqs_list):\n                base_file=os.path.basename(file).replace('_forward_hits.aln.fa', '')\n                to_return[base_file] = {}\n            return to_return\n\n        # Run pplacer on merged file\n        jplace = self.pplacer(files.jplace_output_path(), args.output_directory, files.comb_aln_fa(), args.threads)\n        files_to_delete.append(jplace)\n        logging.info(\"Placements finished\")\n\n        #Read the json of refpkg\n        logging.info(\"Reading classifications\")\n        classifications=Classify(tax_descr).assignPlacement(\n                                                           jplace,\n                                                           args.placements_cutoff,\n                                                           resolve_placements\n                                                           )\n        logging.info(\"Reads classified\")\n        # If the reverse pipe has been specified, run the comparisons between the two pipelines. If not then just return.\n\n        for idx, file in enumerate(seqs_list):\n\n            if reverse_pipe:\n                base_file=os.path.basename(file).replace('_forward_hits.aln.fa', '')\n                forward_gup=classifications.pop(sorted(classifications.keys())[0])\n                reverse_gup=classifications.pop(sorted(classifications.keys())[0])\n                seqs_list.pop(idx+1)\n                placements_hash = Compare().compare_placements(\n                                                               forward_gup,\n                                                               reverse_gup,\n                                                               args.placements_cutoff,\n                                                               slash_endings,\n                                                               base_file\n                                                               )\n                trusted_placements[base_file]=placements_hash['trusted_placements']\n\n            else: # Set the trusted placements as\n                base_file=os.path.basename(file).replace('_hits.aln.fa', '')\n                trusted_placements[base_file]={}\n                if str(idx) in classifications:\n                    for read, entry in classifications[str(idx)].items():\n                        trusted_placements[base_file][read] = entry['placement']\n        # Split the original jplace file\n        # and write split jplaces to separate file directories\n        with open(jplace) as f: jplace_json = json.load(f)\n        cluster_dict = self.convert_cluster_dict_keys_to_aliases(clusterer.seq_library,\n                                                                 alias_hash)\n        hash_with_placements = self.jplace_split(jplace_json,\n                                                 cluster_dict)\n\n        for file_alias, placement_entries_list in hash_with_placements.items():\n            alias_hash[file_alias]['place'] = placement_entries_list\n\n        for k in alias_hash.keys():\n            if 'place' not in alias_hash[k]:\n                alias_hash[k]['place'] = []\n        self.write_jplace(jplace_json,\n                          alias_hash)\n\n        self.hk.delete(files_to_delete)# Remove combined split, not really useful\n\n        return trusted_placements", "response": "This method will place fastas into tree and return the tree and the species that the tree is in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the reads_to_extract from the database_fasta_file and put them in output_file.", "response": "def extract(self, reads_to_extract, database_fasta_file, output_file):\n        '''Extract the reads_to_extract from the database_fasta_file and put them in\n        output_file.\n\n        Parameters\n        ----------\n        reads_to_extract: Iterable of str\n            IDs of reads to be extracted\n        database_fasta_file: str\n            path the fasta file that containing the reads\n        output_file: str\n            path to the file where they are put\n\n        Returns\n        -------\n        Nothing'''\n        cmd = \"fxtract -XH -f /dev/stdin '%s' > %s\" % (\n            database_fasta_file, output_file)\n\n        extern.run(cmd, stdin='\\n'.join(reads_to_extract))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_forward_and_reverse_complement(\n            self, forward_reads_to_extract, reverse_reads_to_extract, database_fasta_file,\n            output_file):\n        '''As per extract except also reverse complement the sequences.'''\n        self.extract(forward_reads_to_extract, database_fasta_file, output_file)\n        cmd_rev = \"fxtract -XH -f /dev/stdin '%s'\" % database_fasta_file\n\n        output = extern.run(cmd_rev, stdin='\\n'.join(reverse_reads_to_extract))\n\n        with open(output_file, 'a') as f:\n            for record in SeqIO.parse(StringIO(output), 'fasta'):\n                record.seq = record.reverse_complement().seq\n                SeqIO.write(record, f, 'fasta')", "response": "As per extract except also reverse complement the sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_empty_ranks(self, tax_list):\n        '''\n        Removes empty rank prefixes\n        \n        Parameters\n        ----------\n        tax_list    : list\n            A list of taxonomic ranks.\n        Returns\n        -------\n        A list of taxonomic ranks with empty prefixes removed.\n        '''\n        new_tax_list = []\n        for rank in tax_list:\n            if rank not in self.meaningless_taxonomic_names:\n                new_tax_list.append(rank)\n        return new_tax_list", "response": "Removes empty rank prefixes from a list of taxonomic ranks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the OTU table rows.", "response": "def _iterate_otu_table_rows(self, read_taxonomies):\n        '''yield that which is required for an OTU table: taxonomy, and\n        count of that taxonomy in each sample as an array\n        \n        Parameters\n        ---------\n        read_taxonomies:\n            a list of hashes, where the position in the list corresponds to the \n            sample list, the key is the read name, and the value is an array\n            of taxonomic info\n            \n        Yield\n        -----\n        OTU_ID: int, made up by this function\n        taxonomy: array where each element is a taxonomic level\n        count: number of observations of that taxonomy in each sample respectively\n            \n        Return\n        ------\n        Nothing, use this as an iterator'''\n        \n        taxonomy_string_to_taxonomy_array = {}\n        taxonomy_string_to_counts = {}\n        sample_index = 0\n        num_samples = len(read_taxonomies)\n        for read_to_taxonomy in read_taxonomies: # For each sample\n            for taxonomy_array in read_to_taxonomy.values(): # For each read\n                taxonomy_string = '; '.join(taxonomy_array)\n                if taxonomy_string_to_taxonomy_array.has_key(taxonomy_string):\n                    if taxonomy_string_to_taxonomy_array[taxonomy_string] != taxonomy_array:\n                        raise Exception(\"Programming error: two different taxonomies had same taxonomy string\")\n                    try:\n                        taxonomy_string_to_counts[taxonomy_string][sample_index] += 1\n                    except KeyError:\n                        taxonomy_string_to_counts[taxonomy_string][sample_index] = 1\n                else:\n                    taxonomy_string_to_taxonomy_array[taxonomy_string] = taxonomy_array\n                    taxonomy_string_to_counts[taxonomy_string] = [0]*num_samples\n                    taxonomy_string_to_counts[taxonomy_string][sample_index] = 1\n            sample_index += 1\n            \n        otu_id = 1\n        for tax_string, counts_array in taxonomy_string_to_counts.iteritems():\n            array = []\n            for i in range(num_samples):\n                try:\n                    array.append(counts_array[i])\n                except IndexError:\n                    array.append(0)\n            yield otu_id,\\\n                taxonomy_string_to_taxonomy_array[tax_string],\\\n                array\n            otu_id += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the OTU info to a BIOM file.", "response": "def write_biom(self, sample_names, read_taxonomies, biom_file_io):\n        '''Write the OTU info to a biom IO output stream\n        \n        Parameters\n        ----------\n        sample_names: String\n            names of each sample (sample_ids for biom)\n        read_taxonomies: Array of hashes as per _iterate_otu_table_rows()\n        biom_file_io: io\n            open writeable stream to write biom contents to\n            \n        Returns True if successful, else False'''\n        counts = []\n        observ_metadata = []\n        otu_ids = []\n        for otu_id, tax, count in self._iterate_otu_table_rows(read_taxonomies):\n            if len(count) != len(sample_names):\n                raise Exception(\"Programming error: mismatched sample names and counts\")\n            counts.append(count)\n            observ_metadata.append({'taxonomy': tax})\n            otu_ids.append(str(otu_id))\n        if len(counts) == 0:\n            logging.info(\"Not writing BIOM file since no sequences were assigned taxonomy\")\n            return True\n        table = Table(np.array(counts),\n                      otu_ids, sample_names, observ_metadata,\n                      [{}]*len(sample_names), table_id='GraftM Taxonomy Count Table')\n        try:\n            table.to_hdf5(biom_file_io, 'GraftM graft')\n            return True\n        except RuntimeError as e:\n            logging.warn(\"Error writing BIOM output, file not written. The specific error was: %s\" % e)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_tabular_otu_table(self, sample_names, read_taxonomies, combined_output_otu_table_io):\n        '''A function that takes a hash of trusted placements, and compiles them\n        into an OTU-esque table.'''\n        delim = u'\\t'\n        combined_output_otu_table_io.write(delim.join(['#ID', \n                                                       delim.join(sample_names),\n                                                       'ConsensusLineage']))\n        combined_output_otu_table_io.write(u\"\\n\")\n        for otu_id, tax, counts in self._iterate_otu_table_rows(read_taxonomies):\n            combined_output_otu_table_io.write(delim.join(\\\n                (str(otu_id),\n                 delim.join([str(c) for c in counts]),\n                 '; '.join(tax)))+\"\\n\")", "response": "A function that takes a hash of trusted placements and compiles them\nxid into an OTU - esque table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_krona_plot(self, sample_names, read_taxonomies, output_krona_filename):\n        '''Creates krona plot at the given location. Assumes the krona executable\n        ktImportText is available on the shell PATH'''\n        tempfiles = []\n        for n in sample_names:\n            tempfiles.append(tempfile.NamedTemporaryFile(prefix='GraftMkronaInput', suffix=n))\n        \n        delim=u'\\t'\n        for _, tax, counts in self._iterate_otu_table_rows(read_taxonomies):\n            for i, c in enumerate(counts):\n                if c != 0:\n                    tempfiles[i].write(delim.join((str(c),\n                                                  delim.join(tax)\n                                                  ))+\"\\n\")\n                    \n        for t in tempfiles:\n            t.flush()\n        \n        cmd = [\"ktImportText\",'-o',output_krona_filename]\n        for i, tmp in enumerate(tempfiles):\n            cmd.append(','.join([tmp.name,sample_names[i]]))\n\n        # run the actual krona\n        cmd = ' '.join(cmd)\n        extern.run(cmd)\n\n        # close tempfiles\n        for t in tempfiles:\n            t.close()", "response": "Creates krona plot at the given location."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the annotations hash provided by pplacer to include all reads from each cluster in the sequence.", "response": "def uncluster_annotations(self, input_annotations, reverse_pipe):\n        '''\n        Update the annotations hash provided by pplacer to include all\n        representatives within each cluster\n\n        Parameters\n        ----------\n        input_annotations : hash\n            Classifications for each representative sequence of the clusters.\n            each key being the sequence name, and the entry being the taxonomy\n            string as a list.\n        reverse_pipe : bool\n            True/False, whether the reverse reads pipeline is being followed.\n\n        Returns\n        -------\n        output_annotations : hash\n            An updated version of the above, which includes all reads from\n            each cluster\n        '''\n        output_annotations = {}\n        for placed_alignment_file_path, clusters in self.seq_library.iteritems():\n\n            if reverse_pipe and placed_alignment_file_path.endswith(\"_reverse_clustered.fa\"): continue\n            placed_alignment_file = os.path.basename(placed_alignment_file_path)\n            cluster_classifications = input_annotations[placed_alignment_file]\n\n            if reverse_pipe:\n                placed_alignment_base = placed_alignment_file.replace('_forward_clustered.fa', '')\n            else:\n                placed_alignment_base = placed_alignment_file.replace('_clustered.fa', '')\n            output_annotations[placed_alignment_base] = {}\n            for rep_read_name, rep_read_taxonomy in cluster_classifications.iteritems():\n\n                if reverse_pipe:\n                    orfm_regex = OrfM.regular_expression()\n                    clusters={(orfm_regex.match(key).groups(0)[0] if orfm_regex.match(key) else key): item for key, item in clusters.iteritems()}\n                for read in clusters[rep_read_name]:\n                    output_annotations[placed_alignment_base][read.name] = rep_read_taxonomy\n\n        return output_annotations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclustering reads at 100% identity level and writes them to the sequence library.", "response": "def cluster(self, input_fasta_list, reverse_pipe):\n        '''\n        cluster - Clusters reads at 100% identity level and  writes them to\n        file. Resets the input_fasta variable as the FASTA file containing the\n        clusters.\n\n        Parameters\n        ----------\n        input_fasta_list : list\n            list of strings, each a path to input fasta files to be clustered.\n        reverse_pipe : bool\n            True/False, whether the reverse reads pipeline is being followed.\n        Returns\n        -------\n        output_fasta_list : list\n            list of strings, each a path to the output fasta file to which\n            clusters were written to.\n        '''\n        output_fasta_list = []\n        for input_fasta in input_fasta_list:\n            output_path  = input_fasta.replace('_hits.aln.fa', '_clustered.fa')\n            cluster_dict = {}\n\n            logging.debug('Clustering reads')\n            if os.path.exists(input_fasta):\n                reads=self.seqio.read_fasta_file(input_fasta) # Read in FASTA records\n                logging.debug('Found %i reads' % len(reads)) # Report number found\n                clusters=self.clust.deduplicate(reads) # Cluster redundant sequences\n                logging.debug('Clustered to %s groups' % len(clusters)) # Report number of clusters\n                logging.debug('Writing representative sequences of each cluster to: %s' % output_path) # Report the name of the file\n            else:\n                logging.debug(\"Found no reads to be clustered\")\n                clusters = []\n\n            self.seqio.write_fasta_file(\n                                        [x[0] for x in clusters],\n                                        output_path\n                                        ) # Choose the first sequence to write to file as representative (all the same anyway)\n            for cluster in clusters:\n                cluster_dict[cluster[0].name]=cluster # assign the cluster to the dictionary\n            self.seq_library[output_path]= cluster_dict\n\n            output_fasta_list.append(output_path)\n\n        return output_fasta_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, input_package_path, output_package_path, **kwargs):\n        force = kwargs.pop('force',ArchiveDefaultOptions.force)\n        if len(kwargs) > 0:\n            raise Exception(\"Unexpected arguments detected: %s\" % kwargs)\n\n        logging.info(\"Archiving GraftM package '%s' as '%s'\" % (input_package_path, output_package_path))\n        gpkg = GraftMPackage.acquire(input_package_path)\n        if gpkg.version != 3:\n            raise Exception(\"Archiving GraftM packages only works with format 3 packages\")\n\n        self._setup_output(output_package_path, force)\n\n        logging.debug(\"Compressing contents for archive\")\n        with tarfile.open(output_package_path, 'w:gz') as tar:\n            for path in itertools.chain([gpkg.contents_file_path(),\n                                         gpkg.alignment_hmm_path(),\n                                         gpkg.reference_package_path(),\n                                         gpkg.unaligned_sequence_database_path()],\n                                        [hmm for hmm in gpkg.search_hmm_paths() if hmm != gpkg.alignment_hmm_path()]):\n                logging.debug(\"Compressing '%s'\" % path)\n                # Put the gpkg folder itself in the archive so as not to tar bomb.\n                tar.add(path,\n                        os.path.join(os.path.basename(os.path.abspath(gpkg._base_directory)),\n                                     os.path.basename(path)))\n            \n        logging.info(\"Archive successfully created\")", "response": "Create an archived GraftM package and add it to the output package path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(self, archive_path, output_package_path, **kwargs):\n        '''Extract an archived GraftM package.\n\n        Parameters\n        ----------\n        archive_path: str\n            path to archive\n        output_package_path: str\n            path to where to put the extracted file\n        kwargs:\n            force: bool\n                overwrite an existing directory\n        '''\n        force = kwargs.pop('force', ArchiveDefaultOptions.force)\n        if len(kwargs) > 0:\n            raise Exception(\"Unexpected arguments detected: %s\" % kwargs)\n        \n        logging.info(\"Un-archiving GraftM package '%s' from '%s'\" % (output_package_path, archive_path))\n        archive = os.path.abspath(archive_path)\n        output = os.path.abspath(output_package_path)\n        \n        self._setup_output(output, force)\n        \n        with tempdir.in_tempdir():\n            with tarfile.open(archive) as tar:\n                tar.extractall()\n                for tarinfo in tar:\n                    folder = os.path.dirname(tarinfo.name)\n                    break\n\n            # recreate diamond file\n            gpkg = GraftMPackage.acquire(folder)\n            if gpkg.version != 3:\n                raise Exception(\"Encountered an archived gpkg of unexpected version: %d\" % gpkg.version)\n            if gpkg.diamond_database_path():\n                logging.debug(\"Creating diamond DB\")\n                gpkg.create_diamond_db()\n\n            shutil.move(folder, output)\n\n        logging.info(\"Archive successfully extracted\")", "response": "Extract an archived GraftM package."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup_output(self, path, force):\n        '''Clear the way for an output to be placed at path'''\n        # Allow for special case of output being a pipe\n        if os.path.isdir(path) or os.path.isfile(path):\n            if force:\n                logging.warn(\"Deleting previous file/directory '%s'\" % path)\n                if os.path.isfile(path):\n                    os.remove(path)\n                else:\n                    shutil.rmtree(path)\n            else:\n                raise Exception(\"Cowardly refusing to overwrite already existing path at %s\" % path)", "response": "Clear the way for an output to be placed at path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the results yielding a list for each result where the field_name each element corresponds to the field given in the field_name.", "response": "def each(self, field_names):\n        \"\"\"Iterate over the results, yielding a list for each result, where\n        each element corresponds to the field given in the field_name parameters\n        \n        Parameters\n        ----------\n        field_names: list of str\n            The names of the fields to be returned during iteration\n            \n        Returns\n        -------\n        None\n        \n        Exceptions\n        ----------\n        raises something when a field name is not in self.fields\n        \"\"\"\n        field_ids = []\n        for f in field_names:\n            # below raises error if the field name is not found, so\n            # don't need to account for that.\n            field_ids.append(self.fields.index(f))\n        \n        for r in self.results:\n            yield([r[i] for i in field_ids])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_from_daa_file(daa_filename):\n        '''Generate new results object from the output of diamond blastx/p'''\n        \n        # blast m8 format is\n        # 'qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\n        res = DiamondSearchResult()\n        res.fields = [\n                       SequenceSearchResult.QUERY_ID_FIELD,\n                       SequenceSearchResult.HIT_ID_FIELD,\n                       SequenceSearchResult.PERCENT_ID_FIELD,\n                       SequenceSearchResult.ALIGNMENT_LENGTH_FIELD,\n                       SequenceSearchResult.MISMATCH_FIELD,\n                       #skip\n                       SequenceSearchResult.QUERY_FROM_FIELD,\n                       SequenceSearchResult.QUERY_TO_FIELD,\n                       SequenceSearchResult.HIT_FROM_FIELD,\n                       SequenceSearchResult.HIT_TO_FIELD,\n                       SequenceSearchResult.EVALUE_FIELD,\n                       SequenceSearchResult.ALIGNMENT_BIT_SCORE,\n                       # extras\n                       SequenceSearchResult.ALIGNMENT_DIRECTION,\n                       SequenceSearchResult.HMM_NAME_FIELD\n                       ]\n        \n        cmd = \"diamond view -a '%s'\" % daa_filename\n        logging.debug(\"Running cmd: %s\" % cmd)\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n        stdout, stderr = process.communicate()\n        \n        reader = csv.reader(stdout.decode('ascii').splitlines(),\n                                delimiter='\\t')\n        if process.returncode != 0:\n            raise Exception(\"Problem running diamond view with cmd: '%s',\" \n                            \"stderr was %s\" % (cmd, stderr))\n\n        for row in reader:\n            # 'qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\n            #    0       1     2      3        4        5      6     7    8      9    10     11\n            query_start = int(row[6])\n            query_end = int(row[7])\n            res.results.append([row[0],\n                                 row[1],\n                                 row[2],\n                                 row[3],\n                                 row[4],\n                                 query_start,\n                                 query_end,\n                                 int(row[8]),\n                                 int(row[9]),\n                                 row[10],\n                                 row[11],\n                                 query_start < query_end,\n                                 os.path.basename(daa_filename)\n                                 ])\n        return res", "response": "Import the results from a DAA file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates new results object from the output of nhmmer search", "response": "def import_from_nhmmer_table(hmmout_path):\n        '''Generate new results object from the output of nhmmer search'''\n        # nhmmer format is\n        # qseqid queryname hmmfrom hmmto alifrom alito envfrom envto sqlen strand evalue bitscore bias description\n        #   0        2        4      5      6      7      8      9    10     11     12      13     14     15     \n        res=HMMSearchResult()\n        res.fields = [\n                       SequenceSearchResult.QUERY_ID_FIELD,\n                       SequenceSearchResult.HMM_NAME_FIELD,\n                       SequenceSearchResult.ALIGNMENT_LENGTH_FIELD,\n                       SequenceSearchResult.QUERY_FROM_FIELD,\n                       SequenceSearchResult.QUERY_TO_FIELD,\n                       SequenceSearchResult.HIT_FROM_FIELD,\n                       SequenceSearchResult.HIT_TO_FIELD,\n                       SequenceSearchResult.ALIGNMENT_BIT_SCORE,\n                       SequenceSearchResult.ALIGNMENT_DIRECTION,\n                       ]\n        \n        for row in [x.rstrip().split() for x in open(hmmout_path) if not x.startswith('#')]:\n            alifrom    = int(row[6])\n            alito      = int(row[7])\n            aln_length = (alito-alifrom if alito-alifrom>0 else alifrom-alito)\n            res.results.append([row[0],\n                                row[2],\n                                aln_length,\n                                int(row[4]),\n                                int(row[5]),\n                                alifrom,\n                                alito,\n                                row[13],\n                                alito > alifrom\n                                ])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):\n        r'''Given a set of deduplicated sequences and a taxonomy hash,\n        return the respective LCAs of taxonomy\n        \n        Parameters\n        ----------\n        deduplicated_sequences: Array of arrays of Sequence objects\n            as output from deduplicate()\n        taxonomy_hash: dictionary \n            of sequence names to taxonomy array (i.e. array of str)\n        \n        Returns\n        -------\n        Array of taxonomy LCAs'''\n        \n        to_return = []\n        for dup_group in deduplicated_sequences:\n            lca = taxonomy_hash[dup_group[0].name]\n            for s in dup_group[1:]:\n                for i, tax in enumerate(taxonomy_hash[s.name]):\n                    if i >= len(lca) or tax != lca[i]:\n                        lca = lca[:i]\n                        break\n                if len(lca) > len(taxonomy_hash[s.name]):\n                    lca = lca[:len(taxonomy_hash[s.name])]\n            to_return.append(lca)\n        return to_return", "response": "r Returns the LCAs of the taxonomy that the sequence is in."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _munch_off_batch(self, queue):\n\n        # if the number of CPUs used == 1, just pop one off (which == below)\n        # elif the the number of things in the queue is 1, use all the CPUs and just that hmm\n        if len(queue) == 1 or self._num_cpus == 1:\n            pairs_to_run = [[queue.pop(0), self._num_cpus]]\n        else:\n            # else share out CPUs among hmmers\n            pairs_to_run = []\n            while len(queue) > 0 and len(pairs_to_run) < self._num_cpus:\n                pairs_to_run.append([queue.pop(0), 1])\n            # Share out any remaining CPUs\n            num_cpus_left = self._num_cpus - len(pairs_to_run)\n            while num_cpus_left > 0:\n                for i, _ in enumerate(pairs_to_run):\n                    pairs_to_run[i][1] += 1\n                    num_cpus_left -= 1\n                    if num_cpus_left == 0: break\n\n        return pairs_to_run", "response": "r Take a batch of sequences off the queue and return a list of lists of tuples where each element is a sequence of the same size and each element is a sequence of the same size."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _hmm_command(self, input_pipe, pairs_to_run):\n        element = pairs_to_run.pop()\n        hmmsearch_cmd = self._individual_hmm_command(element[0][0],\n                                                      element[0][1],\n                                                      element[1])\n        while len(pairs_to_run) > 0:\n            element = pairs_to_run.pop()\n            hmmsearch_cmd = \"tee >(%s) | %s\" % (self._individual_hmm_command(element[0][0],\n                                                                              element[0][1],\n                                                                              element[1]),\n                                                hmmsearch_cmd)\n\n        # Run the actual command\n        hmmsearch_cmd = \"%s | %s\" % (input_pipe, hmmsearch_cmd)\n        return hmmsearch_cmd", "response": "r This method returns the command to be run for a batch of HMMs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the contents. json file and return the dictionary containing the parsed. json file.", "response": "def _parse_contents(self, contents_file_path):\n        '''\n        Parse the contents .json file and return the dictionary\n\n        Parameters\n        ----------\n        contents_file_path: str\n            Path to file containing .json file to parse.\n\n        Returns\n        -------\n        contents_dict: dict\n            parsed .json file\n        '''\n        logging.debug(\"Parsing %s\" % (contents_file_path))\n        contents_dict = json.load(open(contents_file_path))\n        return contents_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving an alignment return a list of sequence names that are less than the min_aligned_fraction", "response": "def _check_reads_hit(self, alignment_io, min_aligned_fraction):\n        '''Given an alignment return a list of sequence names that are less\n        than the min_aligned_fraction'''\n        to_return = []\n        alignment_length = None\n        for s in SeqIO.parse(alignment_io, \"fasta\"):\n            if not alignment_length:\n                alignment_length = len(s.seq)\n                min_length = int(min_aligned_fraction * alignment_length)\n                logging.debug(\"Determined min number of aligned bases to be %s\" % min_length)\n            elif len(s.seq) != alignment_length:\n                raise Exception(\"Alignment file appears to not be of uniform length\")\n\n            num_unaligned = s.seq.count('-')\n            num_aligned = alignment_length-num_unaligned\n            logging.debug(\"Sequence %s has %d aligned positions\" % (s.name, alignment_length-num_unaligned))\n            if num_aligned <= min_length:\n                to_return.append(s.name)\n        return to_return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _align_sequences(self, input_sequences_path, output_alignment_path,\n                         threads):\n        '''Align sequences into alignment_file\n\n        Parameters\n        ----------\n        input_sequences_path: str\n            path to input sequences in fasta format\n        output_alignment_path: str\n            path to output alignment path\n        threads: str\n            number of threads to use\n        Returns\n        -------\n        Nothing\n        '''\n        logging.debug(\"Aligning sequences using mafft\")\n        cmd = \"mafft --anysymbol --thread %s --auto /dev/stdin > %s\" % (\n            threads,\n            output_alignment_path)\n        inputs = []\n        with open(input_sequences_path) as f:\n            for name,seq,_ in SequenceIO().each(f):\n                inputs.append('>%s' % name)\n                # Do not include * characters in the HMM, as this means tree\n                # insertion fails.\n                inputs.append(seq.replace('*',''))\n        extern.run(cmd, stdin=\"\\n\".join(inputs))", "response": "Align sequences into alignment_file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_hmm_from_alignment(self, alignment, hmm_filename, output_alignment_filename):\n        '''Return a HMM file and alignment of sequences to that HMM\n\n        Parameters\n        ----------\n        alignment: str\n            path to aligned proteins\n        hmm_filename: str\n            write the hmm to this file path\n        output_alignment_filename: str\n            write the output alignment to this file path\n\n        Returns\n        -------\n        Return the pipeline type of the HMM.\n        '''\n        logging.info(\"Building HMM from alignment\")\n\n        with tempfile.NamedTemporaryFile(suffix='.fasta',prefix='graftm') as tempaln:\n            cmd = \"hmmbuild -O /dev/stdout -o /dev/stderr '%s' '%s'\" % (\n                hmm_filename, alignment)\n            process = subprocess.Popen([\"bash\", \"-c\", cmd],\n                                       stdout=subprocess.PIPE,\n                                       stderr=subprocess.PIPE)\n            output, error = process.communicate()\n            logging.debug(\"Got STDERR from hmmbuild: %s\" % error)\n            if process.returncode != 0:\n                logging.error(\n                    \"hmmbuild exitstatus was non-zero, likely indicating an error of \"\n                    \"some description\")\n                logging.error(\"Got STDERR from hmmbuild: %s\" % error)\n\n            SeqIO.write(SeqIO.parse(StringIO(output), 'stockholm'), tempaln, 'fasta')\n            tempaln.flush()\n\n            ptype, _ = self._pipe_type(hmm_filename)\n            SequenceSearcher(hmm_filename).alignment_correcter([tempaln.name],\n                                                    output_alignment_filename)\n            return ptype", "response": "Returns a HMM file and alignment of sequences to that HMM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _align_sequences_to_hmm(self, hmm_file, sequences_file, output_alignment_file):\n        '''Align sequences to an HMM, and write an alignment of\n        these proteins after cleanup so that they can be used for tree-making\n\n        Parameters\n        ----------\n        sequences_file: str\n            path to file of unaligned protein sequences\n        hmm_file: str\n            path to hmm file\n        output_alignment_file: str\n            write alignment to this file\n\n        Returns\n        -------\n        nothing\n        '''\n\n        ss = SequenceSearcher(hmm_file)\n        with tempfile.NamedTemporaryFile(prefix='graftm', suffix='.aln.fasta') as tempalign:\n            ss.hmmalign_sequences(hmm_file, sequences_file, tempalign.name)\n            ss.alignment_correcter([tempalign.name], output_alignment_file)", "response": "Align sequences to an HMM and write an alignment of these proteins after cleanup so that they can be used for tree - making"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _define_range(self, sequences):\n        '''\n        define_range - define the maximum range within which two hits in a db\n        search can be linked. This is defined as 1.5X the average length of all\n        reads in the database.\n\n        Parameters\n        ----------\n        sequences : str\n            A path to the sequences in FASTA format. This fasta file is assumed\n            to be in the correct format. i.e. headers start with '>'\n\n        Returns\n        -------\n        max_range : int\n            As described above, 1.5X the size of the average length of genes\n            within the database\n        '''\n        sequence_count = 0\n        total_sequence = 0\n\n        for record in SeqIO.parse(open(sequences), 'fasta'): # For every sequence in the output\n            total_sequence+=1 # increment the total sequence count\n            sequence_count+=len(record.seq) # add the length of that sequence to the total string count\n\n        # Get the average, multiply by 1.5, and return\n        max_range = (sequence_count/total_sequence)*1.5\n        return max_range", "response": "Define the maximum range within which two hits in a db\n            search can be linked. This is defined as 1. 5X the average length of all the genes\n            in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the FastTree log file given a tree and the alignment that made that tree.", "response": "def _generate_tree_log_file(self, tree, alignment, output_tree_file_path,\n                               output_log_file_path, residue_type, fasttree):\n        '''Generate the FastTree log file given a tree and the alignment that\n        made that tree\n\n        Returns\n        -------\n        Nothing. The log file as parameter is written as the log file.\n        '''\n        if residue_type==Create._NUCLEOTIDE_PACKAGE_TYPE:\n            cmd = \"%s -quiet -gtr -nt -nome -mllen -intree '%s' -log %s -out %s %s\" %\\\n                                       (fasttree, tree, output_log_file_path,\n                                        output_tree_file_path, alignment)\n        elif residue_type==Create._PROTEIN_PACKAGE_TYPE:\n            cmd = \"%s -quiet -nome -mllen -intree '%s' -log %s -out %s %s\" %\\\n                                       (fasttree, tree, output_log_file_path,\n                                        output_tree_file_path, alignment)\n        extern.run(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves sequences from the alignment file that have names in sequence_names", "response": "def _remove_sequences_from_alignment(self, sequence_names, input_alignment_file, output_alignment_file):\n        '''Remove sequences from the alignment file that have names in\n        sequence_names\n\n        Parameters\n        ----------\n        sequence_names: list of str\n            names of sequences to remove\n        input_alignment_file: str\n            path to alignment file to remove from\n        output_alignment_file: str\n            path to alignment file to write to\n\n        Returns\n        -------\n        int: number of sequences written to file'''\n        nameset = set(sequence_names)\n        num_written = 0\n        with open(output_alignment_file, 'w') as f:\n            for s in SeqIO.parse(open(input_alignment_file), \"fasta\"):\n                if s.name not in nameset:\n                    SeqIO.write(s, f, \"fasta\")\n                    num_written += 1\n        logging.debug(\"After removing sequences from alignment, %i remain\" % num_written)\n        return num_written"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_dmnd_database(self, unaligned_sequences_path, daa_output):\n        '''\n        Build a diamond database using diamond makedb\n\n        Parameters\n        ----------\n        unaligned_sequences_path: str\n            path to a FASTA file containing unaligned sequences\n        daa_output: str\n            Name of output database.\n        '''\n        logging.debug(\"Building diamond database\")\n\n        cmd = \"diamond makedb --in '%s' -d '%s'\" % (unaligned_sequences_path, daa_output)\n        extern.run(cmd)", "response": "Create a diamond database using diamond makedb"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mask_strange_sequence_letters(self, sequences, package_type):\n        '''Replace strange characters like selenocysteine (U) with X or N for\n        protein and nucleotide sequences, respectively. Sequences are\n        modified in place.\n\n        Parameters\n        ----------\n        aligned_sequences: array of Sequence objects\n            sequences to mask\n        package_type: _PROTEIN_PACKAGE_TYPE or _NUCLEOTIDE_PACKAGE_TYPE\n            type of sequences these are\n\n        Returns\n        -------\n        None\n        '''\n        if package_type == Create._PROTEIN_PACKAGE_TYPE:\n            search_re = re.compile(r'([^ACDEFGHIKLMNPQRSTVWY\\-X])',re.IGNORECASE)\n            replace_char = 'X'\n        elif package_type == Create._NUCLEOTIDE_PACKAGE_TYPE:\n            search_re = re.compile(r'([^ATGC\\-N])',re.IGNORECASE)\n            replace_char = 'N'\n\n        for s in sequences:\n            if package_type == Create._NUCLEOTIDE_PACKAGE_TYPE:\n                s.seq = s.seq.replace('U','T')\n                s.seq = s.seq.replace('u','t')\n            newseq = s.seq\n            m = search_re.search(newseq)\n            while m:\n                logging.warn(\"Found a non-standard character in the sequence of %s: e.g. '%s'\"\\\n                             % (s.name, m.group()))\n                newseq = newseq[:m.start()] + replace_char + newseq[(m.start()+1):]\n                m = search_re.search(newseq)\n            s.seq = newseq", "response": "Masks strange characters like selenocysteine U or N for the given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_for_duplicate_sequence_names(self, fasta_file_path):\n        found_sequence_names = set()\n        for record in SeqIO.parse(fasta_file_path, 'fasta'):\n            name = record.name\n            if name in found_sequence_names:\n                return name\n            found_sequence_names.add(name)\n        return False", "response": "Test if the given fasta file contains sequences with duplicate\n            sequence names."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the taxonomy and seqinfo files into a dictionary of sequence_name => taxonomy where the taxonomy is an array of lineages containing lineages of lineages that are given to that sequence.", "response": "def read_taxtastic_taxonomy_and_seqinfo(self, taxonomy_io, seqinfo_io):\n        '''Read the taxonomy and seqinfo files into a dictionary of \n        sequence_name => taxonomy, where the taxonomy is an array of lineages\n        given to that sequence.\n        \n        Possibly this method is unable to handle the full definition of these\n        files? It doesn't return what each of the ranks are, for starters.\n        Doesn't deal with duplicate taxon names either.\n        '''\n        \n        # read in taxonomy file\n        lineages = [] #array of hashes where each taxon points to its parent taxon's name\n        taxon_to_lineage_index = {}\n        expected_number_of_fields = None\n        for line in taxonomy_io:\n            splits = line.strip().split(',')\n            if expected_number_of_fields is None:\n                expected_number_of_fields = len(splits)\n                lineages = [{}]* (expected_number_of_fields-4)\n                continue #this is the header line\n            elif len(splits) != expected_number_of_fields:\n                raise Exception(\"Encountered error parsing taxonomy file, expected %i fields but found %i on line: %s\" %\n                                (expected_number_of_fields, len(splits), line))\n            # e.g. 'tax_id,parent_id,rank,tax_name,root,kingdom,phylum,class,order,family,genus,species\n            tax_id = splits[0]\n            parent_id = splits[1]\n            \n            try:\n                lineage_index = splits.index('')-5\n            except ValueError:\n                lineage_index = len(splits)-5\n            taxon_to_lineage_index[tax_id] = lineage_index\n            lineages[lineage_index][tax_id] = parent_id\n        \n        taxonomy_dictionary = {}\n        for i, line in enumerate(seqinfo_io):\n            if i==0: continue #skip header line\n            \n            splits = line.strip().split(',')\n            if len(splits) != 2:\n                raise Exception(\"Bad formatting of seqinfo file on this line: %s\" % line)\n            \n            seq_name = splits[0]\n            taxon = splits[1]\n            lineage_index = taxon_to_lineage_index[taxon]\n            if lineage_index==0:\n                # Don't include Root in the taxonomy\n                taxonomy_dictionary[seq_name] = []\n            else:\n                full_taxonomy_rev = []\n                while lineage_index > 0:\n                    full_taxonomy_rev.append(taxon)\n                    taxon = lineages[lineage_index][taxon]\n                    lineage_index = lineage_index-1\n                taxonomy_dictionary[seq_name] = list(reversed(full_taxonomy_rev))\n            \n        return taxonomy_dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_taxonomy_and_seqinfo_files(self, taxonomies, output_taxonomy_file, output_seqinfo_file):\n        '''Write out taxonomy and seqinfo files as required by taxtastic\n        from known taxonomies\n        \n        Parameters\n        ----------\n        taxonomies:\n            hash of taxon_id to array of taxonomic information\n        output_taxonomy_file:\n            write taxtastic-compatible 'taxonomy' file here\n        output_seqinfo_file:\n            write taxtastic-compatible 'seqinfo' file here'''\n        \n        first_pass_id_and_taxonomies = []\n        tc=TaxonomyCleaner()\n        max_number_of_ranks = 0\n        \n        for taxon_id, tax_split in taxonomies.iteritems():\n            # Replace spaces with underscores e.g. 'Candidatus my_genus'\n            for idx, item in enumerate(tax_split):\n                tax_split[idx] = re.sub('\\s+', '_', item.strip())\n\n            # Remove 'empty' taxononomies e.g. 's__'\n            tax_split = tc.remove_empty_ranks(tax_split)\n\n            # Add this fixed up list to the list\n            first_pass_id_and_taxonomies.append([taxon_id]+tax_split)\n\n            if len(tax_split) > max_number_of_ranks:\n                max_number_of_ranks = len(tax_split)\n\n        # Find taxons that have multiple parents, building a hash of parents as we go (i.e. a tree of taxonomies embedded in a hash)\n        #\n        # Assumes that no two taxonomic labels are the same when they are from different\n        # taxonomic levels. When there are children with multiple parents at the\n        # same taxonomic label then these are warned about and worked around.\n        parents = {} #hash of taxon to immediate parent\n        known_duplicates = sets.Set([])\n        for j, array in enumerate(first_pass_id_and_taxonomies):\n            taxonomy = array[1:]\n            for i, tax in enumerate(taxonomy):\n                if i==0: continue #top levels don't have parents\n                ancestry = taxonomy[i-1]\n                if parents.has_key(tax):\n                    if parents[tax] != ancestry:\n\n                        dup = \"%s%s\" %(parents[tax], ancestry)\n                        # don't report the same problem several times\n                        if dup not in known_duplicates:\n                            print \" %s '%s' with multiple parents %s and %s\" % (array[0], tax, parents[tax], ancestry)\n                            known_duplicates.add(dup)\n\n                        # fix the current one\n                        new_name_id = 1\n                        new_name = \"%se%s\" % (tax, new_name_id)\n\n                        while parents.has_key(new_name) and parents[new_name] != ancestry:\n                            new_name_id += 1\n                            new_name = \"%se%s\" % (tax, new_name_id)\n\n                        first_pass_id_and_taxonomies[j][i+1] = new_name\n                        taxonomy[i] = new_name\n                        parents[new_name] = ancestry\n                else:\n                    # normal case, seeing a new taxon and parent for the first time\n\n                    parents[tax] = ancestry\n\n        # Write the sequence file\n        with open(output_seqinfo_file, 'w') as seqout:\n            # write header\n            seqout.write('seqname,tax_id\\n')\n            # write each taxonomic association\n            for array in first_pass_id_and_taxonomies:\n                sequence_id = array[0]\n                if len(array)==1:\n                    most_specific_taxonomic_affiliation = 'Root'\n                else:  \n                    most_specific_taxonomic_affiliation = array[-1]\n\n                seqout.write(\"%s,%s\\n\" % (array[0], \n                                          most_specific_taxonomic_affiliation))\n\n        # Write the taxonomy file\n        noted_taxonomies = sets.Set([])\n        taxonomic_level_names = [\"rank_%i\" % rank for rank in range(max_number_of_ranks)]\n        with open(output_taxonomy_file, 'w') as seqout:\n            # write header and root line\n            seqout.write(','.join(['tax_id','parent_id','rank','tax_name','root'] + taxonomic_level_names) +'\\n')\n            seqout.write(','.join(['Root','Root','root','Root','Root'])+ ''.join([',']*max_number_of_ranks) + '\\n')\n            # write all the taxonomies\n            for array in first_pass_id_and_taxonomies:\n                taxons = array[1:]\n                for i, tax in enumerate(taxons):\n                    line = self._taxonomy_line(i, taxons[:(i+1)], taxonomic_level_names)\n                    if line not in noted_taxonomies:\n                        seqout.write(line+\"\\n\")\n                        noted_taxonomies.add(line)", "response": "Write out taxonomy and seqinfo files as required by taxtastic\n            from known taxonomic information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef each_sequence(self, fp):\n        '''Like each except iterate over Sequence objects'''\n        for name, seq, _ in self.each(fp):\n            yield Sequence(name, seq)", "response": "Like each except iterate over Sequence objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunpacks a response from the database into a dictionary containing the response data.", "response": "def _unpack_response(response, cursor_id=None, as_class=dict, tz_aware=False):\n    \"\"\"Unpack a response from the database.\n\n    Check the response for errors and unpack, returning a dictionary\n    containing the response data.\n\n    :Parameters:\n      - `response`: byte string as returned from the database\n      - `cursor_id` (optional): cursor_id we sent to get this response -\n        used for raising an informative exception when we get cursor id not\n        valid at server response\n      - `as_class` (optional): class to use for resulting documents\n    \"\"\"\n    response_flag = struct.unpack(\"<i\", response[:4])[0]\n    if response_flag & 1:\n        # Shouldn't get this response if we aren't doing a getMore\n        assert cursor_id is not None\n\n        raise InterfaceError(\"cursor id '%s' not valid at server\" %\n                               cursor_id)\n    elif response_flag & 2:\n        error_object = bson.BSON(response[20:]).decode()\n        if error_object[\"$err\"] == \"not master\":\n            raise DatabaseError(\"master has changed\")\n        raise DatabaseError(\"database error: %s\" %\n                               error_object[\"$err\"])\n\n    result = {}\n    result[\"cursor_id\"] = struct.unpack(\"<q\", response[4:12])[0]\n    result[\"starting_from\"] = struct.unpack(\"<i\", response[12:16])[0]\n    result[\"number_returned\"] = struct.unpack(\"<i\", response[16:20])[0]\n    result[\"data\"] = bson.decode_all(response[20:], as_class, tz_aware)\n    assert len(result[\"data\"]) == result[\"number_returned\"]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fields_list_to_dict(fields):\n    for key in fields:\n        assert isinstance(key, (str,unicode))\n    return dict([[key, 1] for key in fields])", "response": "Takes a list of field names and returns a matching dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _password_digest(username, password):\n    if not isinstance(password, basestring):\n        raise TypeError(\"password must be an instance of basestring\")\n    if not isinstance(username, basestring):\n        raise TypeError(\"username must be an instance of basestring\")\n\n    md5hash = hashlib.md5()\n    md5hash.update(\"%s:mongo:%s\" % (username.encode('utf-8'),\n                                    password.encode('utf-8')))\n    return unicode(md5hash.hexdigest())", "response": "Get a password digest to use for authentication."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an auth key to use for authentication.", "response": "def _auth_key(nonce, username, password):\n    \"\"\"Get an auth key to use for authentication.\n    \"\"\"\n    digest = _password_digest(username, password)\n    md5hash = hashlib.md5()\n    md5hash.update(\"%s%s%s\" % (nonce, unicode(username), digest))\n    return unicode(md5hash.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _socket_connect(self):\n        self.usage_count = 0\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n            s.connect((self._host, self._port))\n            self.__stream = self.__backend.register_stream(s, **self.__kwargs)\n            self.__stream.set_close_callback(self._socket_close)\n            self.__alive = True\n        except socket.error, error:\n            raise InterfaceError(error)", "response": "create a socket connect and register a stream with the async backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _socket_close(self):\n        callback = self.__callback\n        self.__callback = None\n        try:\n            if callback:\n                callback(None, InterfaceError('connection closed'))\n        finally:\n            # Flush the job queue, don't call the callbacks associated with the remaining jobs\n            # since they have already been called as error callback on connection closing\n            self.__job_queue = []\n            self.__alive = False\n            self.__pool.cache(self)", "response": "cleanup after the socket is closed by the other end"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _close(self):\n        callback = self.__callback\n        self.__callback = None\n        try:\n            if callback:\n                callback(None, InterfaceError('connection closed'))\n        finally:\n            # Flush the job queue, don't call the callbacks associated with the remaining jobs\n            # since they have already been called as error callback on connection closing\n            self.__job_queue = []\n            self.__alive = False\n            self.__stream.close()", "response": "Close the socket and cleanup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_message(self, message, callback):\n        \n        if self.__callback is not None:\n            raise ProgrammingError('connection already in use')\n\n        if callback:\n            err_callback = functools.partial(callback, None)\n        else:\n            err_callback = None\n\n        # Go and update err_callback for async jobs in queue if any\n        for job in self.__job_queue:\n            # this is a dirty hack and I hate it, but there is no way of setting the correct\n            # err_callback during the connection time\n            if isinstance(job, asyncjobs.AsyncJob):\n                job.update_err_callback(err_callback)\n\n        if not self.__alive:\n            if self.__autoreconnect:\n                self.__connect(err_callback)\n            else:\n                raise InterfaceError('connection invalid. autoreconnect=False')\n        \n        # Put the current message on the bottom of the queue\n        self._put_job(asyncjobs.AsyncMessage(self, message, callback), 0)\n        self._next_job()", "response": "send a message over the wire"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _next_job(self):\n        if self.__job_queue:\n            # Produce message from the top of the queue\n            job = self.__job_queue.pop()\n            # logging.debug(\"queue = %s, popped %r\", self.__job_queue, job)\n            job.process()", "response": "execute the next job from the top of the queue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a connection pool transparently creating it if it doesn t already exist", "response": "def get_connection_pool(self, pool_id, *args, **kwargs):\n        \"\"\"get a connection pool, transparently creating it if it doesn't already exist\n\n        :Parameters:\n            - `pool_id`: unique id for a connection pool\n        \"\"\"\n        assert isinstance(pool_id, (str, unicode))\n        if not hasattr(self, '_pools'):\n            self._pools = {}\n        if pool_id not in self._pools:\n            self._pools[pool_id] = ConnectionPool(*args, **kwargs)\n        # logging.debug(\"%s: _connections = %d\", pool_id, self._pools[pool_id]._connections)\n        return self._pools[pool_id]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close_idle_connections(self, pool_id=None):\n        if not hasattr(self, '_pools'):\n            return\n\n        if pool_id:\n            if pool_id not in self._pools:\n                raise ProgrammingError(\"pool %r does not exist\" % pool_id)\n            else:\n                pool = self._pools[pool_id]\n                pool.close()\n        else:\n            for pool_id, pool in self._pools.items():\n                pool.close()", "response": "close idle connections to mongo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a cached connection from the pool", "response": "def connection(self):\n        \"\"\" get a cached connection from the pool \"\"\"\n        \n        self._condition.acquire()\n        try:\n            if (self._maxconnections and self._connections >= self._maxconnections):\n                raise TooManyConnections(\"%d connections are already equal to the max: %d\" % (self._connections, self._maxconnections))\n            # connection limit not reached, get a dedicated connection\n            try: # first try to get it from the idle cache\n                con = self._idle_cache.pop(0)\n            except IndexError: # else get a fresh connection\n                con = self.new_connection()\n            self._connections += 1\n        finally:\n            self._condition.release()\n        return con"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nput a dedicated connection back into the idle cache.", "response": "def cache(self, con):\n        \"\"\"Put a dedicated connection back into the idle cache.\"\"\"\n        if self._maxusage and con.usage_count > self._maxusage:\n            self._connections -=1\n            logging.debug('dropping connection %s uses past max usage %s' % (con.usage_count, self._maxusage))\n            con._close()\n            return\n        self._condition.acquire()\n        if con in self._idle_cache:\n            # called via socket close on a connection in the idle cache\n            self._condition.release()\n            return\n        try:\n            if not self._maxcached or len(self._idle_cache) < self._maxcached:\n                # the idle cache is not full, so put it there\n                self._idle_cache.append(con)\n            else: # if the idle cache is already full,\n                logging.debug('dropping connection. connection pool (%s) is full. maxcached %s' % (len(self._idle_cache), self._maxcached))\n                con._close() # then close the connection\n            self._condition.notify()\n        finally:\n            self._connections -= 1\n            self._condition.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose all connections in the pool.", "response": "def close(self):\n        \"\"\"Close all connections in the pool.\"\"\"\n        self._condition.acquire()\n        try:\n            while self._idle_cache: # close all idle connections\n                con = self._idle_cache.pop(0)\n                try:\n                    con._close()\n                except Exception:\n                    pass\n                self._connections -=1\n            self._condition.notifyAll()\n        finally:\n            self._condition.release()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert a document or list of documents into this collection.", "response": "def insert(self, doc_or_docs,\n               manipulate=True, safe=True, check_keys=True, callback=None, **kwargs):\n        \"\"\"Insert a document(s) into this collection.\n        \n        If `manipulate` is set, the document(s) are manipulated using\n        any :class:`~pymongo.son_manipulator.SONManipulator` instances\n        that have been added to this\n        :class:`~pymongo.database.Database`. Returns the ``\"_id\"`` of\n        the inserted document or a list of ``\"_id\"`` values of the\n        inserted documents.  If the document(s) does not already\n        contain an ``\"_id\"`` one will be added.\n        \n        If `safe` is ``True`` then the insert will be checked for\n        errors, raising :class:`~pymongo.errors.OperationFailure` if\n        one occurred. Safe inserts wait for a response from the\n        database, while normal inserts do not.\n        \n        Any additional keyword arguments imply ``safe=True``, and\n        will be used as options for the resultant `getLastError`\n        command. For example, to wait for replication to 3 nodes, pass\n        ``w=3``.\n        \n        :Parameters:\n          - `doc_or_docs`: a document or list of documents to be\n            inserted\n          - `manipulate` (optional): manipulate the documents before\n            inserting?\n          - `safe` (optional): check that the insert succeeded?\n          - `check_keys` (optional): check if keys start with '$' or\n            contain '.', raising :class:`~pymongo.errors.InvalidName`\n            in either case\n          - `**kwargs` (optional): any additional arguments imply\n            ``safe=True``, and will be used as options for the\n            `getLastError` command\n        \n        .. mongodoc:: insert\n        \"\"\"\n        if not isinstance(safe, bool):\n            raise TypeError(\"safe must be an instance of bool\")\n        \n        docs = doc_or_docs\n        # return_one = False\n        if isinstance(docs, dict):\n            # return_one = True\n            docs = [docs]\n        \n        # if manipulate:\n        #     docs = [self.__database._fix_incoming(doc, self) for doc in docs]\n        \n        self.__limit = None\n        if kwargs:\n            safe = True\n        \n        if safe and not callable(callback):\n            raise TypeError(\"callback must be callable\")\n        if not safe and callback is not None:\n            raise TypeError(\"callback can not be used with safe=False\")\n        \n        if callback:\n            callback = functools.partial(self._handle_response, orig_callback=callback)\n\n        connection = self.__pool.connection()\n        try:\n            connection.send_message(\n                message.insert(self.full_collection_name, docs,\n                    check_keys, safe, kwargs), callback=callback)\n        except:\n            connection.close()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate a document in this collection.", "response": "def update(self, spec, document, upsert=False, manipulate=False,\n               safe=True, multi=False, callback=None, **kwargs):\n        \"\"\"Update a document(s) in this collection.\n        \n        Raises :class:`TypeError` if either `spec` or `document` is\n        not an instance of ``dict`` or `upsert` is not an instance of\n        ``bool``. If `safe` is ``True`` then the update will be\n        checked for errors, raising\n        :class:`~pymongo.errors.OperationFailure` if one\n        occurred. Safe updates require a response from the database,\n        while normal updates do not - thus, setting `safe` to ``True``\n        will negatively impact performance.\n        \n        There are many useful `update modifiers`_ which can be used\n        when performing updates. For example, here we use the\n        ``\"$set\"`` modifier to modify some fields in a matching\n        document:\n        \n        .. doctest::\n          \n          >>> db.test.insert({\"x\": \"y\", \"a\": \"b\"})\n          ObjectId('...')\n          >>> list(db.test.find())\n          [{u'a': u'b', u'x': u'y', u'_id': ObjectId('...')}]\n          >>> db.test.update({\"x\": \"y\"}, {\"$set\": {\"a\": \"c\"}})\n          >>> list(db.test.find())\n          [{u'a': u'c', u'x': u'y', u'_id': ObjectId('...')}]\n        \n        If `safe` is ``True`` returns the response to the *lastError*\n        command. Otherwise, returns ``None``.\n        \n        # Any additional keyword arguments imply ``safe=True``, and will\n        # be used as options for the resultant `getLastError`\n        # command. For example, to wait for replication to 3 nodes, pass\n        # ``w=3``.\n        \n        :Parameters:\n          - `spec`: a ``dict`` or :class:`~bson.son.SON` instance\n            specifying elements which must be present for a document\n            to be updated\n          - `document`: a ``dict`` or :class:`~bson.son.SON`\n            instance specifying the document to be used for the update\n            or (in the case of an upsert) insert - see docs on MongoDB\n            `update modifiers`_\n          - `upsert` (optional): perform an upsert if ``True``\n          - `manipulate` (optional): manipulate the document before\n            updating? If ``True`` all instances of\n            :mod:`~pymongo.son_manipulator.SONManipulator` added to\n            this :class:`~pymongo.database.Database` will be applied\n            to the document before performing the update.\n          - `safe` (optional): check that the update succeeded?\n          - `multi` (optional): update all documents that match\n            `spec`, rather than just the first matching document. The\n            default value for `multi` is currently ``False``, but this\n            might eventually change to ``True``. It is recommended\n            that you specify this argument explicitly for all update\n            operations in order to prepare your code for that change.\n          - `**kwargs` (optional): any additional arguments imply\n            ``safe=True``, and will be used as options for the\n            `getLastError` command\n        \n        .. _update modifiers: http://www.mongodb.org/display/DOCS/Updating\n        \n        .. mongodoc:: update\n        \"\"\"\n        if not isinstance(spec, dict):\n            raise TypeError(\"spec must be an instance of dict\")\n        if not isinstance(document, dict):\n            raise TypeError(\"document must be an instance of dict\")\n        if not isinstance(upsert, bool):\n            raise TypeError(\"upsert must be an instance of bool\")\n        if not isinstance(safe, bool):\n            raise TypeError(\"safe must be an instance of bool\")\n        # TODO: apply SON manipulators\n        # if upsert and manipulate:\n        #     document = self.__database._fix_incoming(document, self)\n        \n        if kwargs:\n            safe = True\n        \n        if safe and not callable(callback):\n            raise TypeError(\"callback must be callable\")\n        if not safe and callback is not None:\n            raise TypeError(\"callback can not be used with safe=False\")\n        \n        if callback:\n            callback = functools.partial(self._handle_response, orig_callback=callback)\n\n        self.__limit = None\n        connection = self.__pool.connection()\n        try:\n            connection.send_message(\n                message.update(self.full_collection_name, upsert, multi,\n                    spec, document, safe, kwargs), callback=callback)\n        except:\n            connection.close()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_one(self, spec_or_id, **kwargs):\n        if spec_or_id is not None and not isinstance(spec_or_id, dict):\n            spec_or_id = {\"_id\": spec_or_id}\n        kwargs['limit'] = -1\n        self.find(spec_or_id, **kwargs)", "response": "Get a single document from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the database for the keys that match the specified criteria.", "response": "def find(self, spec=None, fields=None, skip=0, limit=0,\n                 timeout=True, snapshot=False, tailable=False, sort=None,\n                 max_scan=None, slave_okay=False,\n                 _must_use_master=False, _is_command=False, hint=None, debug=False,\n                 comment=None, callback=None):\n        \"\"\"Query the database.\n        \n        The `spec` argument is a prototype document that all results\n        must match. For example:\n        \n        >>> db.test.find({\"hello\": \"world\"}, callback=...)\n        \n        only matches documents that have a key \"hello\" with value\n        \"world\".  Matches can have other keys *in addition* to\n        \"hello\". The `fields` argument is used to specify a subset of\n        fields that should be included in the result documents. By\n        limiting results to a certain subset of fields you can cut\n        down on network traffic and decoding time.\n        \n        Raises :class:`TypeError` if any of the arguments are of\n        improper type.\n        \n        :Parameters:\n          - `spec` (optional): a SON object specifying elements which\n            must be present for a document to be included in the\n            result set\n          - `fields` (optional): a list of field names that should be\n            returned in the result set (\"_id\" will always be\n            included), or a dict specifying the fields to return\n          - `skip` (optional): the number of documents to omit (from\n            the start of the result set) when returning the results\n          - `limit` (optional): the maximum number of results to\n            return\n          - `timeout` (optional): if True, any returned cursor will be\n            subject to the normal timeout behavior of the mongod\n            process. Otherwise, the returned cursor will never timeout\n            at the server. Care should be taken to ensure that cursors\n            with timeout turned off are properly closed.\n          - `snapshot` (optional): if True, snapshot mode will be used\n            for this query. Snapshot mode assures no duplicates are\n            returned, or objects missed, which were present at both\n            the start and end of the query's execution. For details,\n            see the `snapshot documentation\n            <http://dochub.mongodb.org/core/snapshot>`_.\n          - `tailable` (optional): the result of this find call will\n            be a tailable cursor - tailable cursors aren't closed when\n            the last data is retrieved but are kept open and the\n            cursors location marks the final document's position. if\n            more data is received iteration of the cursor will\n            continue from the last document received. For details, see\n            the `tailable cursor documentation\n            <http://www.mongodb.org/display/DOCS/Tailable+Cursors>`_.\n          - `sort` (optional): a list of (key, direction) pairs\n            specifying the sort order for this query. See\n            :meth:`~pymongo.cursor.Cursor.sort` for details.\n          - `max_scan` (optional): limit the number of documents\n            examined when performing the query\n          - `slave_okay` (optional): is it okay to connect directly\n            to and perform queries on a slave instance\n        \n        .. mongodoc:: find\n        \"\"\"\n        \n        if spec is None:\n            spec = {}\n        \n        if limit is None:\n            limit = 0\n\n        if not isinstance(spec, dict):\n            raise TypeError(\"spec must be an instance of dict\")\n        if not isinstance(skip, int):\n            raise TypeError(\"skip must be an instance of int\")\n        if not isinstance(limit, int):\n            raise TypeError(\"limit must be an instance of int or None\")\n        if not isinstance(timeout, bool):\n            raise TypeError(\"timeout must be an instance of bool\")\n        if not isinstance(snapshot, bool):\n            raise TypeError(\"snapshot must be an instance of bool\")\n        if not isinstance(tailable, bool):\n            raise TypeError(\"tailable must be an instance of bool\")\n        if not callable(callback):\n            raise TypeError(\"callback must be callable\")\n        \n        if fields is not None:\n            if not fields:\n                fields = {\"_id\": 1}\n            if not isinstance(fields, dict):\n                fields = helpers._fields_list_to_dict(fields)\n        \n        self.__spec = spec\n        self.__fields = fields\n        self.__skip = skip\n        self.__limit = limit\n        self.__batch_size = 0\n        \n        self.__timeout = timeout\n        self.__tailable = tailable\n        self.__snapshot = snapshot\n        self.__ordering = sort and helpers._index_document(sort) or None\n        self.__max_scan = max_scan\n        self.__slave_okay = slave_okay\n        self.__explain = False\n        self.__hint = hint\n        self.__comment = comment\n        self.__debug = debug\n        # self.__as_class = as_class\n        self.__tz_aware = False #collection.database.connection.tz_aware\n        self.__must_use_master = _must_use_master\n        self.__is_command = _is_command\n        \n        connection = self.__pool.connection()\n        try:\n            if self.__debug:\n                logging.debug('QUERY_SPEC: %r' % self.__query_spec())\n\n            connection.send_message(\n                message.query(self.__query_options(),\n                              self.full_collection_name,\n                              self.__skip, \n                              self.__limit,\n                              self.__query_spec(),\n                              self.__fields), \n                callback=functools.partial(self._handle_response, orig_callback=callback))\n        except Exception, e:\n            logging.debug('Error sending query %s' % e)\n            connection.close()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __query_options(self):\n        options = 0\n        if self.__tailable:\n            options |= _QUERY_OPTIONS[\"tailable_cursor\"]\n        if self.__slave_okay or self.__pool._slave_okay:\n            options |= _QUERY_OPTIONS[\"slave_okay\"]\n        if not self.__timeout:\n            options |= _QUERY_OPTIONS[\"no_timeout\"]\n        return options", "response": "Get the query options string to use for this query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the spec to use for a query.", "response": "def __query_spec(self):\n        \"\"\"Get the spec to use for a query.\"\"\"\n        spec = self.__spec\n        if not self.__is_command and \"$query\" not in self.__spec:\n            spec = SON({\"$query\": self.__spec})\n        if self.__ordering:\n            spec[\"$orderby\"] = self.__ordering\n        if self.__explain:\n            spec[\"$explain\"] = True\n        if self.__hint:\n            spec[\"$hint\"] = self.__hint\n        if self.__comment:\n            spec[\"$comment\"] = self.__comment\n        if self.__snapshot:\n            spec[\"$snapshot\"] = True\n        if self.__max_scan:\n            spec[\"$maxScan\"] = self.__max_scan\n        return spec"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_context(app, pagename, templatename, context, doctree):\n    if doctree is None:\n        return\n    visitor = _FindTabsDirectiveVisitor(doctree)\n    doctree.walk(visitor)\n    if not visitor.found_tabs_directive:\n        paths = [posixpath.join('_static', 'sphinx_tabs/' + f) for f in FILES]\n        if 'css_files' in context:\n            context['css_files'] = context['css_files'][:]\n            for path in paths:\n                if path.endswith('.css'):\n                    context['css_files'].remove(path)\n        if 'script_files' in context:\n            context['script_files'] = context['script_files'][:]\n            for path in paths:\n                if path.endswith('.js'):\n                    context['script_files'].remove(path)", "response": "Update the context of a page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy assets to the output", "response": "def copy_assets(app, exception):\n    \"\"\" Copy asset files to the output \"\"\"\n    if 'getLogger' in dir(logging):\n        log = logging.getLogger(__name__).info  # pylint: disable=no-member\n    else:\n        log = app.info\n    builders = get_compatible_builders(app)\n    if exception:\n        return\n    if app.builder.name not in builders:\n        if not app.config['sphinx_tabs_nowarn']:\n            app.warn(\n                'Not copying tabs assets! Not compatible with %s builder' %\n                app.builder.name)\n        return\n\n    log('Copying tabs assets')\n\n    installdir = os.path.join(app.builder.outdir, '_static', 'sphinx_tabs')\n\n    for path in FILES:\n        source = resource_filename('sphinx_tabs', path)\n        dest = os.path.join(installdir, path)\n\n        destdir = os.path.dirname(dest)\n        if not os.path.exists(destdir):\n            os.makedirs(destdir)\n\n        copyfile(source, dest)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up the plugin", "response": "def setup(app):\n    \"\"\" Set up the plugin \"\"\"\n    app.add_config_value('sphinx_tabs_nowarn', False, '')\n    app.add_config_value('sphinx_tabs_valid_builders', [], '')\n    app.add_directive('tabs', TabsDirective)\n    app.add_directive('tab', TabDirective)\n    app.add_directive('group-tab', GroupTabDirective)\n    app.add_directive('code-tab', CodeTabDirective)\n    for path in ['sphinx_tabs/' + f for f in FILES]:\n        if path.endswith('.css'):\n            if 'add_css_file' in dir(app):\n                app.add_css_file(path)\n            else:\n                app.add_stylesheet(path)\n        if path.endswith('.js'):\n            if 'add_script_file' in dir(app):\n                app.add_script_file(path)\n            else:\n                app.add_javascript(path)\n    app.connect('html-page-context', update_context)\n    app.connect('build-finished', copy_assets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a tabs directive", "response": "def run(self):\n        \"\"\" Parse a tabs directive \"\"\"\n        self.assert_has_content()\n        env = self.state.document.settings.env\n\n        node = nodes.container()\n        node['classes'] = ['sphinx-tabs']\n\n        if 'next_tabs_id' not in env.temp_data:\n            env.temp_data['next_tabs_id'] = 0\n        if 'tabs_stack' not in env.temp_data:\n            env.temp_data['tabs_stack'] = []\n\n        tabs_id = env.temp_data['next_tabs_id']\n        tabs_key = 'tabs_%d' % tabs_id\n        env.temp_data['next_tabs_id'] += 1\n        env.temp_data['tabs_stack'].append(tabs_id)\n\n        env.temp_data[tabs_key] = {}\n        env.temp_data[tabs_key]['tab_ids'] = []\n        env.temp_data[tabs_key]['tab_titles'] = []\n        env.temp_data[tabs_key]['is_first_tab'] = True\n\n        self.state.nested_parse(self.content, self.content_offset, node)\n\n        if env.app.builder.name in get_compatible_builders(env.app):\n            tabs_node = nodes.container()\n            tabs_node.tagname = 'div'\n\n            classes = 'ui top attached tabular menu sphinx-menu'\n            tabs_node['classes'] = classes.split(' ')\n\n            tab_titles = env.temp_data[tabs_key]['tab_titles']\n            for idx, [data_tab, tab_name] in enumerate(tab_titles):\n                tab = nodes.container()\n                tab.tagname = 'a'\n                tab['classes'] = ['item'] if idx > 0 else ['active', 'item']\n                tab['classes'].append(data_tab)\n                tab += tab_name\n                tabs_node += tab\n\n            node.children.insert(0, tabs_node)\n\n        env.temp_data['tabs_stack'].pop()\n        return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        self.assert_has_content()\n        env = self.state.document.settings.env\n\n        tabs_id = env.temp_data['tabs_stack'][-1]\n        tabs_key = 'tabs_%d' % tabs_id\n\n        args = self.content[0].strip()\n        if args.startswith('{'):\n            try:\n                args = json.loads(args)\n                self.content.trim_start(1)\n            except ValueError:\n                args = {}\n        else:\n            args = {}\n\n        tab_name = nodes.container()\n        self.state.nested_parse(\n            self.content[:1], self.content_offset, tab_name)\n        args['tab_name'] = tab_name\n\n        include_tabs_id_in_data_tab = False\n        if 'tab_id' not in args:\n            args['tab_id'] = env.new_serialno(tabs_key)\n            include_tabs_id_in_data_tab = True\n        i = 1\n        while args['tab_id'] in env.temp_data[tabs_key]['tab_ids']:\n            args['tab_id'] = '%s-%d' % (args['tab_id'], i)\n            i += 1\n        env.temp_data[tabs_key]['tab_ids'].append(args['tab_id'])\n\n        data_tab = str(args['tab_id'])\n        if include_tabs_id_in_data_tab:\n            data_tab = '%d-%s' % (tabs_id, data_tab)\n        data_tab = \"sphinx-data-tab-{}\".format(data_tab)\n\n        env.temp_data[tabs_key]['tab_titles'].append(\n            (data_tab, args['tab_name']))\n\n        text = '\\n'.join(self.content)\n        node = nodes.container(text)\n\n        classes = 'ui bottom attached sphinx-tab tab segment'\n        node['classes'] = classes.split(' ')\n        node['classes'].extend(args.get('classes', []))\n        node['classes'].append(data_tab)\n\n        if env.temp_data[tabs_key]['is_first_tab']:\n            node['classes'].append('active')\n            env.temp_data[tabs_key]['is_first_tab'] = False\n\n        self.state.nested_parse(self.content[2:], self.content_offset, node)\n\n        if env.app.builder.name not in get_compatible_builders(env.app):\n            outer_node = nodes.container()\n            tab = nodes.container()\n            tab.tagname = 'a'\n            tab['classes'] = ['item']\n            tab += tab_name\n\n            outer_node.append(tab)\n            outer_node.append(node)\n            return [outer_node]\n\n        return [node]", "response": "Parse a tab directive and return a tree of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a tab directive", "response": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n\n        group_name = self.content[0]\n        self.content.trim_start(2)\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '   ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                group_name.encode('utf-8')).decode('utf-8'),\n            'group_tab': True\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(group_name),\n            '',\n        ]\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        self.assert_has_content()\n\n        args = self.content[0].strip().split()\n        self.content.trim_start(2)\n\n        lang = args[0]\n        tab_name = ' '.join(args[1:]) if len(args) > 1 else LEXER_MAP[lang]\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '      ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                tab_name.encode('utf-8')).decode('utf-8'),\n            'classes': ['code-tab'],\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(tab_name),\n            '',\n            '   .. code-block:: {}'.format(lang),\n        ]\n\n        if 'linenos' in self.options:\n            new_content.append('      :linenos:')\n\n        new_content.append('')\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children", "response": "Parse a tab directive"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __pack_message(operation, data):\n    request_id = random.randint(-2 ** 31 - 1, 2 ** 31)\n    message = struct.pack(\"<i\", 16 + len(data))\n    message += struct.pack(\"<i\", request_id)\n    message += __ZERO  # responseTo\n    message += struct.pack(\"<i\", operation)\n    return (request_id, message + data)", "response": "Takes message data and adds a message header based on the operation. Returns the resultant message string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(collection_name, docs, check_keys, safe, last_error_args):\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    bson_data = \"\".join([bson.BSON.encode(doc, check_keys) for doc in docs])\n    if not bson_data:\n        raise InvalidOperation(\"cannot do an empty bulk insert\")\n    data += bson_data\n    if safe:\n        (_, insert_message) = __pack_message(2002, data)\n        (request_id, error_message) = __last_error(last_error_args)\n        return (request_id, insert_message + error_message)\n    else:\n        return __pack_message(2002, data)", "response": "Create an insert message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an update message.", "response": "def update(collection_name, upsert, multi, spec, doc, safe, last_error_args):\n    \"\"\"Get an **update** message.\n    \"\"\"\n    options = 0\n    if upsert:\n        options += 1\n    if multi:\n        options += 2\n\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    data += struct.pack(\"<i\", options)\n    data += bson.BSON.encode(spec)\n    data += bson.BSON.encode(doc)\n    if safe:\n        (_, update_message) = __pack_message(2001, data)\n        (request_id, error_message) = __last_error(last_error_args)\n        return (request_id, update_message + error_message)\n    else:\n        return __pack_message(2001, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a **query** message.", "response": "def query(options, collection_name,\n          num_to_skip, num_to_return, query, field_selector=None):\n    \"\"\"Get a **query** message.\n    \"\"\"\n    data = struct.pack(\"<I\", options)\n    data += bson._make_c_string(collection_name)\n    data += struct.pack(\"<i\", num_to_skip)\n    data += struct.pack(\"<i\", num_to_return)\n    data += bson.BSON.encode(query)\n    if field_selector is not None:\n        data += bson.BSON.encode(field_selector)\n    return __pack_message(2004, data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_more(collection_name, num_to_return, cursor_id):\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    data += struct.pack(\"<i\", num_to_return)\n    data += struct.pack(\"<q\", cursor_id)\n    return __pack_message(2005, data)", "response": "Get a ** getMore ** message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(collection_name, spec, safe, last_error_args):\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    data += __ZERO\n    data += bson.BSON.encode(spec)\n    if safe:\n        (_, remove_message) = __pack_message(2006, data)\n        (request_id, error_message) = __last_error(last_error_args)\n        return (request_id, remove_message + error_message)\n    else:\n        return __pack_message(2006, data)", "response": "Get a **delete** message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection(self, collectionname, dbname=None):\n        if not collectionname or \"..\" in collectionname:\n            raise DataError(\"collection names cannot be empty\")\n        if \"$\" in collectionname and not (collectionname.startswith(\"oplog.$main\") or\n                                collectionname.startswith(\"$cmd\")):\n            raise DataError(\"collection names must not \"\n                              \"contain '$': %r\" % collectionname)\n        if collectionname.startswith(\".\") or collectionname.endswith(\".\"):\n            raise DataError(\"collecion names must not start \"\n                            \"or end with '.': %r\" % collectionname)\n        if \"\\x00\" in collectionname:\n            raise DataError(\"collection names must not contain the \"\n                              \"null character\")\n        return Cursor(dbname or self._pool._dbname, collectionname, self._pool)", "response": "Get a Cursor object for a specific collection by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of all the collection names in selected database", "response": "def collection_names(self, callback):\n        \"\"\"Get a list of all the collection names in selected database\"\"\"\n        callback = partial(self._collection_names_result, callback)\n        self[\"system.namespaces\"].find(_must_use_master=True, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collection_names_result(self, callback, results, error=None):\n        names = [r['name'] for r in results if r['name'].count('.') == 1]\n        assert error == None, repr(error)\n        strip = len(self._pool._dbname) + 1\n        callback([name[strip:] for name in names])", "response": "callback to for collection names query filters out collection names"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef command(self, command, value=1, callback=None,\n                check=True, allowable_errors=[], **kwargs):\n        \"\"\"Issue a MongoDB command.\n\n        Send command `command` to the database and return the\n        response. If `command` is an instance of :class:`basestring`\n        then the command {`command`: `value`} will be sent. Otherwise,\n        `command` must be an instance of :class:`dict` and will be\n        sent as is.\n\n        Any additional keyword arguments will be added to the final\n        command document before it is sent.\n\n        For example, a command like ``{buildinfo: 1}`` can be sent\n        using:\n\n        >>> db.command(\"buildinfo\")\n\n        For a command where the value matters, like ``{collstats:\n        collection_name}`` we can do:\n\n        >>> db.command(\"collstats\", collection_name)\n\n        For commands that take additional arguments we can use\n        kwargs. So ``{filemd5: object_id, root: file_root}`` becomes:\n\n        >>> db.command(\"filemd5\", object_id, root=file_root)\n\n        :Parameters:\n          - `command`: document representing the command to be issued,\n            or the name of the command (for simple commands only).\n\n            .. note:: the order of keys in the `command` document is\n               significant (the \"verb\" must come first), so commands\n               which require multiple keys (e.g. `findandmodify`)\n               should use an instance of :class:`~bson.son.SON` or\n               a string and kwargs instead of a Python `dict`.\n\n          - `value` (optional): value to use for the command verb when\n            `command` is passed as a string\n          - `**kwargs` (optional): additional keyword arguments will\n            be added to the command document before it is sent\n\n        .. mongodoc:: commands\n        \"\"\"\n\n        if isinstance(command, basestring):\n            command = SON([(command, value)])\n\n        command.update(kwargs)\n\n        self.connection(\"$cmd\").find_one(command,callback=callback,\n                                       _must_use_master=True,\n                                       _is_command=True)", "response": "Issue a MongoDB command to the database and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the image and stage name from a string like image : tag or image : tag AS name", "response": "def image_from(from_value):\n    \"\"\"\n    :param from_value: string like \"image:tag\" or \"image:tag AS name\"\n    :return: tuple of the image and stage name, e.g. (\"image:tag\", None)\n    \"\"\"\n    regex = re.compile(r\"\"\"(?xi)     # readable, case-insensitive regex\n        \\s*                          # ignore leading whitespace\n        (?P<image> \\S+ )             # image and optional tag\n        (?:                          # optional \"AS name\" clause for stage\n            \\s+ AS \\s+\n            (?P<name> \\S+ )\n        )?\n        \"\"\")\n    match = re.match(regex, from_value)\n    return match.group('image', 'name') if match else (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lines(self):\n        if self.cache_content and self.cached_content:\n            return self.cached_content.splitlines(True)\n\n        try:\n            with self._open_dockerfile('rb') as dockerfile:\n                lines = [b2u(l) for l in dockerfile.readlines()]\n                if self.cache_content:\n                    self.cached_content = ''.join(lines)\n                return lines\n        except (IOError, OSError) as ex:\n            logger.error(\"Couldn't retrieve lines from dockerfile: %r\", ex)\n            raise", "response": "Returns a list of lines from Dockerfile\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lines(self, lines):\n        if self.cache_content:\n            self.cached_content = ''.join([b2u(l) for l in lines])\n\n        try:\n            with self._open_dockerfile('wb') as dockerfile:\n                dockerfile.writelines([u2b(l) for l in lines])\n        except (IOError, OSError) as ex:\n            logger.error(\"Couldn't write lines to dockerfile: %r\", ex)\n            raise", "response": "Fill Dockerfile content with specified lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef content(self):\n        if self.cache_content and self.cached_content:\n            return self.cached_content\n\n        try:\n            with self._open_dockerfile('rb') as dockerfile:\n                content = b2u(dockerfile.read())\n                if self.cache_content:\n                    self.cached_content = content\n                return content\n        except (IOError, OSError) as ex:\n            logger.error(\"Couldn't retrieve content of dockerfile: %r\", ex)\n            raise", "response": "Returns the content of the image with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef content(self, content):\n        if self.cache_content:\n            self.cached_content = b2u(content)\n\n        try:\n            with self._open_dockerfile('wb') as dockerfile:\n                dockerfile.write(u2b(content))\n        except (IOError, OSError) as ex:\n            logger.error(\"Couldn't write content to dockerfile: %r\", ex)\n            raise", "response": "Write content to the Dockerfile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef structure(self):\n        def _rstrip_backslash(l):\n            l = l.rstrip()\n            if l.endswith('\\\\'):\n                return l[:-1]\n            return l\n\n        instructions = []\n        lineno = -1\n        insnre = re.compile(r'^\\s*(\\w+)\\s+(.*)$')  # matched group is insn\n        contre = re.compile(r'^.*\\\\\\s*$')          # line continues?\n        commentre = re.compile(r'^\\s*#')           # line is a comment?\n        in_continuation = False\n        current_instruction = None\n        for line in self.lines:\n            lineno += 1\n            if commentre.match(line):\n                continue\n            if not in_continuation:\n                m = insnre.match(line)\n                if not m:\n                    continue\n                current_instruction = {'instruction': m.groups()[0].upper(),\n                                       'startline': lineno,\n                                       'endline': lineno,\n                                       'content': line,\n                                       'value': _rstrip_backslash(m.groups()[1])}\n            else:\n                current_instruction['content'] += line\n                current_instruction['endline'] = lineno\n                if current_instruction['value']:\n                    current_instruction['value'] += _rstrip_backslash(line)\n                else:\n                    current_instruction['value'] = _rstrip_backslash(line.lstrip())\n\n            in_continuation = contre.match(line)\n            if not in_continuation and current_instruction is not None:\n                instructions.append(current_instruction)\n\n        return instructions", "response": "Returns a list of dicts describing the commands in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a JSON formatted string with instructions & values from Dockerfile", "response": "def json(self):\n        \"\"\"\n        :return: JSON formatted string with instructions & values from Dockerfile\n        \"\"\"\n        insndescs = [{insndesc['instruction']: insndesc['value']} for insndesc in self.structure]\n        return json.dumps(insndescs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of parent images", "response": "def parent_images(self):\n        \"\"\"\n        :return: list of parent images -- one image per each stage's FROM instruction\n        \"\"\"\n        parents = []\n        for instr in self.structure:\n            if instr['instruction'] != 'FROM':\n                continue\n            image, _ = image_from(instr['value'])\n            if image is not None:\n                parents.append(image)\n        return parents"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parent_images(self, parents):\n        parents = list(parents)\n        change_instrs = []\n        for instr in self.structure:\n            if instr['instruction'] != 'FROM':\n                continue\n\n            old_image, stage = image_from(instr['value'])\n            if not old_image:\n                continue  # broken FROM, fixing would just confuse things\n            if not parents:\n                raise RuntimeError(\"not enough parents to match build stages\")\n\n            image = parents.pop(0)\n            if image != old_image:\n                instr['value'] = '{0} AS {1}'.format(image, stage) if stage else image\n                instr['content'] = 'FROM {0}\\n'.format(instr['value'])\n                change_instrs.append(instr)\n\n        if parents:\n            raise RuntimeError(\"trying to update too many parents for build stages\")\n\n        lines = self.lines\n        for instr in reversed(change_instrs):\n            lines[instr['startline']:instr['endline']+1] = [instr['content']]\n\n        self.lines = lines", "response": "setter for images in FROM instructions. Images are updated per build stage with the given parents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange base image of final stage FROM instruction", "response": "def baseimage(self, new_image):\n        \"\"\"\n        change image of final stage FROM instruction\n        \"\"\"\n        images = self.parent_images or [None]\n        images[-1] = new_image\n        self.parent_images = images"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cmd(self):\n        value = None\n        for insndesc in self.structure:\n            if insndesc['instruction'] == 'FROM':  # new stage, reset\n                value = None\n            elif insndesc['instruction'] == 'CMD':\n                value = insndesc['value']\n        return value", "response": "Determine the final CMD instruction in the final build stage."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the CMD instruction in final build stage = value", "response": "def cmd(self, value):\n        \"\"\"\n        setter for final 'CMD' instruction in final build stage\n\n        \"\"\"\n        cmd = None\n        for insndesc in self.structure:\n            if insndesc['instruction'] == 'FROM':  # new stage, reset\n                cmd = None\n            elif insndesc['instruction'] == 'CMD':\n                cmd = insndesc\n\n        new_cmd = 'CMD ' + value\n        if cmd:\n            self.add_lines_at(cmd, new_cmd, replace=True)\n        else:\n            self.add_lines(new_cmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets LABEL or ENV instructions with environment replacement.", "response": "def _instruction_getter(self, name, env_replace):\n        \"\"\"\n        Get LABEL or ENV instructions with environment replacement\n\n        :param name: e.g. 'LABEL' or 'ENV'\n        :param env_replace: bool, whether to perform ENV substitution\n        :return: Labels instance or Envs instance\n        \"\"\"\n        if name != 'LABEL' and name != 'ENV':\n            raise ValueError(\"Unsupported instruction '%s'\", name)\n        instructions = {}\n        envs = {}\n\n        for instruction_desc in self.structure:\n            this_instruction = instruction_desc['instruction']\n            if this_instruction == 'FROM':\n                instructions.clear()\n                envs = self.parent_env.copy()\n            elif this_instruction in (name, 'ENV'):\n                logger.debug(\"%s value: %r\", name.lower(), instruction_desc['value'])\n                key_val_list = extract_labels_or_envs(env_replace=env_replace,\n                                                      envs=envs,\n                                                      instruction_value=instruction_desc['value'])\n                for key, value in key_val_list:\n                    if this_instruction == name:\n                        instructions[key] = value\n                        logger.debug(\"new %s %r=%r\", name.lower(), key, value)\n                    if this_instruction == 'ENV':\n                        envs[key] = value\n\n        logger.debug(\"instructions: %r\", instructions)\n        return Labels(instructions, self) if name == 'LABEL' else Envs(instructions, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _modify_instruction_label_env(self, instruction, instr_key, instr_value):\n        if instruction == 'LABEL':\n            instructions = self.labels\n        elif instruction == 'ENV':\n            instructions = self.envs\n        else:\n            raise ValueError(\"Unknown instruction '%s'\" % instruction)\n\n        if instr_key not in instructions:\n            raise KeyError('%s not in %ss' % (instr_key, instruction))\n\n        # extract target instructions from the final stage only\n        candidates = []\n        for insn in self.structure:\n            if insn['instruction'] == 'FROM':\n                candidates = []\n            if insn['instruction'] == instruction:\n                candidates.append(insn)\n\n        # Find where in the file to put the changes\n        content = startline = endline = None\n        for candidate in candidates:\n            words = list(WordSplitter(candidate['value']).split(dequote=False))\n\n            # LABEL/ENV syntax is one of two types:\n            if '=' not in words[0]:  # LABEL/ENV name value\n                # Remove quotes from key name and see if it's the one\n                # we're looking for.\n                if WordSplitter(words[0]).dequote() == instr_key:\n                    if instr_value is None:\n                        # Delete this line altogether\n                        content = None\n                    else:\n                        # Adjust label/env value\n                        words[1:] = [quote(instr_value)]\n\n                        # Now reconstruct the line\n                        content = \" \".join([instruction] + words) + '\\n'\n\n                    startline = candidate['startline']\n                    endline = candidate['endline']\n                    break\n            else:  # LABEL/ENV \"name\"=\"value\"\n                for index, token in enumerate(words):\n                    key, _ = token.split(\"=\", 1)\n                    if WordSplitter(key).dequote() == instr_key:\n                        if instr_value is None:\n                            # Delete this label\n                            del words[index]\n                        else:\n                            # Adjust label/env value\n                            words[index] = \"{0}={1}\".format(key,\n                                                            quote(instr_value))\n\n                        if len(words) == 0:\n                            # We removed the last label/env, delete the whole line\n                            content = None\n                        else:\n                            # Now reconstruct the line\n                            content = \" \".join([instruction] + words) + '\\n'\n\n                        startline = candidate['startline']\n                        endline = candidate['endline']\n                        break\n\n        # We know the label/env we're looking for is there\n        assert startline and endline\n\n        # Re-write the Dockerfile\n        lines = self.lines\n        del lines[startline:endline + 1]\n        if content:\n            lines.insert(startline, content)\n        self.lines = lines", "response": "Modify the label and env values of a given instruction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delete_instructions(self, instruction, value=None):\n        if instruction == 'LABEL' and value:\n            self._modify_instruction_label(value, None)\n            return\n        if instruction == 'ENV' and value:\n            self._modify_instruction_env(value, None)\n            return\n\n        lines = self.lines\n        deleted = False\n        for insn in reversed(self.structure):\n            if insn['instruction'] == instruction:\n                if value and insn['value'] != value:\n                    continue\n                deleted = True\n                del lines[insn['startline']:insn['endline'] + 1]\n        if deleted:\n            self.lines = lines", "response": "Delete the instruction with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an instruction to the log file.", "response": "def _add_instruction(self, instruction, value):\n        \"\"\"\n        :param instruction: instruction name to be added\n        :param value: instruction value\n        \"\"\"\n        if (instruction == 'LABEL' or instruction == 'ENV') and len(value) == 2:\n            new_line = instruction + ' ' + '='.join(map(quote, value)) + '\\n'\n        else:\n            new_line = '{0} {1}\\n'.format(instruction, value)\n        if new_line:\n            lines = self.lines\n            if not lines[len(lines) - 1].endswith('\\n'):\n                new_line = '\\n' + new_line\n            lines += new_line\n            self.lines = lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding lines to the beginning or end of the build.", "response": "def add_lines(self, *lines, **kwargs):\n        \"\"\"\n        Add lines to the beginning or end of the build.\n        :param lines: one or more lines to add to the content, by default at the end.\n        :param all_stages: bool for whether to add in all stages for a multistage build\n                           or (by default) only the last.\n        :param at_start: adds at the beginning (after FROM) of the stage(s) instead of the end.\n        :param skip_scratch: skip stages which use \"FROM scratch\"\n        \"\"\"\n        assert len(lines) > 0\n        lines = [_endline(line) for line in lines]\n        all_stages = kwargs.pop('all_stages', False)\n        at_start = kwargs.pop('at_start', False)\n        skip_scratch = kwargs.pop('skip_scratch', False)\n        assert not kwargs, \"Unknown keyword argument(s): {0}\".format(kwargs.keys())\n\n        froms = [\n            instr for instr in self.structure\n            if instr['instruction'] == 'FROM'\n        ] or [{'endline': -1}]  # no FROM? fake one before the beginning\n        if not all_stages:  # only modify the last\n            froms = [froms[-1]]\n\n        df_lines = self.lines\n        # make sure last line has a newline if lines are to be appended\n        if df_lines and not at_start:\n            df_lines[-1] = _endline(df_lines[-1])\n\n        # iterate through the stages in reverse order\n        # so adding lines doesn't invalidate line numbers from structure dicts.\n        # first add a bogus instruction to represent EOF in our iteration.\n        froms.append({'startline': len(df_lines) + 1})\n        for stage in range(len(froms)-2, -1, -1):  # e.g. 0 for single or 2, 1, 0 for 3 stages\n            start, finish = froms[stage], froms[stage+1]\n            linenum = start['endline'] + 1 if at_start else finish['startline']\n            if skip_scratch and froms[stage]['value'] == 'scratch':\n                continue\n            df_lines[linenum:linenum] = lines\n\n        self.lines = df_lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd lines at a specific location in the build file.", "response": "def add_lines_at(self, anchor, *lines, **kwargs):\n        \"\"\"\n        Add lines at a specific location in the file.\n        :param anchor: structure_dict|line_str|line_num a reference to where adds should occur\n        :param lines: one or more lines to add to the content\n        :param replace: if True -- replace the anchor\n        :param after: if True -- insert after the anchor (conflicts with \"replace\")\n        \"\"\"\n        assert len(lines) > 0\n        replace = kwargs.pop('replace', False)\n        after = kwargs.pop('after', False)\n        assert not (after and replace)\n        assert not kwargs, \"Unknown keyword argument(s): {0}\".format(kwargs.keys())\n\n        # find the line number for the insertion\n        df_lines = self.lines\n        if isinstance(anchor, int):  # line number, just validate\n            assert anchor in range(len(df_lines))\n            if replace:\n                del df_lines[anchor]\n        elif isinstance(anchor, dict):  # structure\n            assert anchor in self.structure, \"Current structure does not match: {0}\".format(anchor)\n            if replace:\n                df_lines[anchor['startline']:anchor['endline'] + 1] = []\n            if after:\n                anchor = anchor['endline']\n            else:\n                anchor = anchor['startline']\n        elif isinstance(anchor, string_types):  # line contents\n            matches = [index for index, text in enumerate(df_lines) if text == anchor]\n            if not matches:\n                raise RuntimeError(\"Cannot find line in the build file:\\n\" + anchor)\n            anchor = matches[-1]\n            if replace:\n                del df_lines[anchor]\n        else:\n            raise RuntimeError(\"Unknown anchor type {0}\".format(anchor))\n\n        if after:\n            # ensure there's a newline on final line\n            df_lines[anchor] = _endline(df_lines[anchor])\n            anchor += 1\n\n        df_lines[anchor:anchor] = [_endline(line) for line in lines]\n        self.lines = df_lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context_structure(self):\n        instructions = []\n        last_context = Context()\n        for instr in self.structure:\n            instruction_type = instr['instruction']\n            if instruction_type == \"FROM\":  # reset per stage\n                last_context = Context()\n\n            context = Context(envs=dict(last_context.envs),\n                              labels=dict(last_context.labels))\n\n            if instruction_type in [\"ENV\", \"LABEL\"]:\n                val = get_key_val_dictionary(instruction_value=instr['value'],\n                                             env_replace=self.env_replace,\n                                             envs=last_context.envs)\n                context.set_line_value(context_type=instruction_type, value=val)\n\n            instructions.append(context)\n            last_context = context\n        return instructions", "response": "Returns a list of Context objects that are in the structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate self. quotes and self. escaped to reflect the current character.", "response": "def _update_quoting_state(self, ch):\n        \"\"\"\n        Update self.quotes and self.escaped\n\n        :param ch: str, current character\n        :return: ch if it was not used to update quoting state, else ''\n        \"\"\"\n\n        # Set whether the next character is escaped\n        # Unquoted:\n        #   a backslash escapes the next character\n        # Double-quoted:\n        #   a backslash escapes the next character only if it is a double-quote\n        # Single-quoted:\n        #   a backslash is not special\n        is_escaped = self.escaped\n        self.escaped = (not self.escaped and\n                        ch == '\\\\' and\n                        self.quotes != self.SQUOTE)\n        if self.escaped:\n            return ''\n\n        if is_escaped:\n            if self.quotes == self.DQUOTE:\n                if ch == '\"':\n                    return ch\n                return \"{0}{1}\".format('\\\\', ch)\n\n            return ch\n\n        if self.quotes is None:\n            if ch in (self.SQUOTE, self.DQUOTE):\n                self.quotes = ch\n                return ''\n\n        elif self.quotes == ch:\n            self.quotes = None\n            return ''\n\n        return ch"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a generator for the words of the string.", "response": "def split(self, maxsplit=None, dequote=True):\n        \"\"\"\n        Generator for the words of the string\n\n        :param maxsplit: perform at most maxsplit splits;\n            if None, do not limit the number of splits\n        :param dequote: remove quotes and escape characters once consumed\n        \"\"\"\n\n        class Word(object):\n            \"\"\"\n            A None-or-str object which can always be appended to.\n            Similar to a defaultdict but with only a single value.\n            \"\"\"\n\n            def __init__(self):\n                self.value = None\n\n            @property\n            def valid(self):\n                return self.value is not None\n\n            def append(self, s):\n                if self.value is None:\n                    self.value = s\n                else:\n                    self.value += s\n\n        num_splits = 0\n        word = Word()\n        while True:\n            ch = self.stream.read(1)\n            if not ch:\n                # EOF\n                if word.valid:\n                    yield word.value\n\n                return\n\n            if (not self.escaped and\n                    self.envs is not None and\n                    ch == '$' and\n                    self.quotes != self.SQUOTE):\n                while True:\n                    # Substitute environment variable\n                    braced = False\n                    varname = ''\n                    while True:\n                        ch = self.stream.read(1)\n                        if varname == '' and ch == '{':\n                            braced = True\n                            continue\n\n                        if not ch:\n                            # EOF\n                            break\n\n                        if braced and ch == '}':\n                            break\n\n                        if not ch.isalnum() and ch != '_':\n                            break\n\n                        varname += ch\n\n                    try:\n                        word.append(self.envs[varname])\n                    except KeyError:\n                        pass\n\n                    # Check whether there is another envvar\n                    if ch != '$':\n                        break\n\n                if braced and ch == '}':\n                    continue\n\n                # ch now holds the next character\n\n            # Figure out what our quoting/escaping state will be\n            # after this character\n            is_escaped = self.escaped\n            ch_unless_consumed = self._update_quoting_state(ch)\n\n            if dequote:\n                # If we just processed a quote or escape character,\n                # and were asked to dequote the string, consume it now\n                ch = ch_unless_consumed\n\n            # If word-splitting has been requested, check whether we are\n            # at a whitespace character\n            may_split = maxsplit != 0 and (maxsplit is None or\n                                           num_splits < maxsplit)\n            at_split = may_split and (self.quotes is None and\n                                      not is_escaped and\n                                      ch.isspace())\n            if at_split:\n                # It is time to yield a word\n                if word.valid:\n                    num_splits += 1\n                    yield word.value\n\n                word = Word()\n            else:\n                word.append(ch)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset value defined on this line ('line_envs/' line_labels') and update 'envs/'/'labels.", "response": "def set_line_value(self, context_type, value):\n        \"\"\"\n        Set value defined on this line ('line_envs'/'line_labels')\n        and update 'envs'/'labels'.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :param value: new value for this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            self.line_envs = value\n            self.envs.update(value)\n        elif context_type.upper() == \"LABEL\":\n            self.line_labels = value\n            self.labels.update(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the values defined on this line.", "response": "def get_line_value(self, context_type):\n        \"\"\"\n        Get the values defined on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type defined on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.line_envs\n        elif context_type.upper() == \"LABEL\":\n            return self.line_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the values valid on this line.", "response": "def get_values(self, context_type):\n        \"\"\"\n        Get the values valid on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type valid on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.envs\n        elif context_type.upper() == \"LABEL\":\n            return self.labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uniqueStates(states,rates):\n    order     = np.lexsort(states.T)\n    states    = states[order]\n    diff      = np.ones(len(states), 'bool')\n    diff[1:]  = (states[1:] != states[:-1]).any(-1)\n    sums      = np.bincount(diff.cumsum() - 1, rates[order])\n    return states[diff], sums", "response": "Returns unique states and sums up the corresponding rates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef number_of_partitions(max_range, max_sum):\n    '''\n    Returns an array arr of the same shape as max_range, where\n    arr[j] = number of admissible partitions for \n             j summands bounded by max_range[j:] and with sum <= max_sum\n    '''\n    M = max_sum + 1\n    N = len(max_range) \n    arr = np.zeros(shape=(M,N), dtype = int)    \n    arr[:,-1] = np.where(np.arange(M) <= min(max_range[-1], max_sum), 1, 0)\n    for i in range(N-2,-1,-1):\n        for j in range(max_range[i]+1):\n            arr[j:,i] += arr[:M-j,i+1] \n    return arr.sum(axis = 0)", "response": "Returns the number of admissible partitions for \n            "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partition_zero(max_range, max_sum, out = None, n_part = None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `0,...,max_range[j]` that add up to at most `max_sum`.\n\n    Code due to ptrj, see http://stackoverflow.com/a/36563744/1479342.  \n   \n    Parameters\n    ----------\n    max_range : array or list of ints\n        Gives the ranges for each element in the output array. Element `j` has range `np.arange(max_range[j]+1)`. \n    max_sum : int\n        The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `0,...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> max_range=np.array([1,3,2])    \n    >>> max_sum = 3    \n    >>> partition_zero(max_range,max_sum)    \n    array([[0, 0, 0],\n           [0, 0, 1],\n           [0, 0, 2],\n           [0, 1, 0],\n           [0, 1, 1],\n           [0, 1, 2],\n           [0, 2, 0],\n           [0, 2, 1],\n           [0, 3, 0],\n           [1, 0, 0],\n           [1, 0, 1],\n           [1, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [1, 2, 0]])\n    '''\n    if out is None:\n        max_range = np.asarray(max_range, dtype = int).ravel()\n        n_part = number_of_partitions(max_range, max_sum)\n        out = np.zeros(shape = (n_part[0], max_range.size), dtype = int)\n\n    if(max_range.size == 1):\n        out[:] = np.arange(min(max_range[0],max_sum) + 1, dtype = int).reshape(-1,1)\n        return out\n\n    P = partition_zero(max_range[1:], max_sum, out=out[:n_part[1],1:], n_part = n_part[1:])        \n\n    S = np.minimum(max_sum - P.sum(axis = 1), max_range[0])\n    offset, sz  = 0, S.size\n    out[:sz,0] = 0\n    for i in range(1, max_range[0]+1):\n        ind, = np.nonzero(S)\n        offset, sz = offset + sz, ind.size\n        out[offset:offset+sz, 0] = i\n        out[offset:offset+sz, 1:] = P[ind]\n        S[ind] -= 1\n    return out", "response": "A function that can be helpful for obtaining the state space of a discrete Markov chain or decision processes. \n ArcGIS model is not yet implemented."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef partition(min_range,max_range,max_sum=None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `min_range[j],...,max_range[j]` for each j. \n    If the argument `max_sum` is specified, the numbers will add up to at most `max_sum`\n    \n    Parameters\n    ----------\n    min_range : array or list of ints\n        `min_range[j]` gives the minimum value for element `j` in the output array. \n    max_range : array or list of ints\n        `max_range[j]` gives the maximum value for element `j` in the output array. \n    max_sum : int\n        Optional argument. The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `min_range[j],...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> min_range=np.array([1,2,1])\n    >>> max_range=np.array([2,4,3])    \n    >>> max_sum = 6    \n    >>> partition(min_range,max_range,max_sum)    \n    array([[1, 2, 1],\n           [1, 2, 2],\n           [1, 2, 3],\n           [1, 3, 1],\n           [1, 3, 2],\n           [1, 4, 1],\n           [2, 2, 1],\n           [2, 2, 2],          \n           [2, 3, 1]])\n    '''\n    max_range = np.asarray(max_range, dtype = int).ravel()\n    min_range = np.asarray(min_range, dtype = int).ravel()\n    full_range = max_range-min_range\n    if any(full_range<0):\n        raise ValueError(\"max_range needs to be larger than min_range\")\n    if max_sum == None:\n        max_sum = np.sum(full_range)\n    else:\n        max_sum -= np.sum(min_range)\n    out = partition_zero(full_range,max_sum)\n    out += min_range\n    return out", "response": "A function that can be helpful for obtaining the state space of a discrete Markov chain or decision processes. This function returns a 2d array with on the rows all possible partitions of the given ranges and max_range for each element."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef checkInitialState(self,initialState):\n        assert initialState is not None, \"Initial state has not been specified.\"\n        assert isinstance(initialState,(int,list,tuple,np.ndarray,set)), \"initialState %r is not an int, tuple, list, set or numpy array\" % initialState\n\n         \n        if isinstance(initialState,list): \n            #Check whether all entries of the list are ints. Return an int if the len ==1, otherwise a tuple.\n            assert all(isinstance(i, int) for i in initialState), \"initialState %r is not integer\" % initialState \n            initialState = int(initialState) if len(initialState)==1 else tuple(initialState)          \n\n        elif isinstance(initialState,tuple): \n            #Check whether all entries are ints. Return an int if the len ==1, otherwise a tuple.\n            assert all(isinstance(i, int) for i in initialState), \"initialState %r is not integer\" % initialState\n            if len(initialState)==1:\n                initialState = int(initialState)     \n        \n        elif isinstance(initialState,np.ndarray):\n            #Check whether the state is a 1d numpy array. Return an int if it has length 1.\n            assert issubclass(initialState.dtype.type, np.integer) and initialState.ndim==1, \"initialState %r is not a one-dimensional integer numpy array\" % initialState \n            initialState = int(initialState) if len(initialState)==1 else tuple(initialState)\n            \n        elif isinstance(initialState,set):\n            #If we have a set, then check whether all elements are ints or tuples.\n            for state in initialState:                \n                assert isinstance(state,(tuple,int)), \"the set initialState %r should contain tuples or ints\" % initialState \n                if isinstance(state,tuple):\n                    assert all(isinstance(i,int) for i in state), \"the state %r should be integer\" % initialState \n                \n        return initialState", "response": "Checks whether the initial state of the assessment class is of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkTransitionType(self,state):\n        test = self.transition(state)\n        assert isinstance(test,(dict,tuple)), \"Transition function does not return a dict or tuple\"\n        \n        if isinstance(test,dict):\n            assert all(isinstance(states, (int,tuple)) for states in test.keys()), \"Transition function returns a dict, but states are not represented as tuples or integers\"\n            assert all(isinstance(rates, float) for rates in test.values()), \"Transition function returns a dict, but the rates should be floats.\"\n            usesNumpy=False\n            \n        if isinstance(test,tuple):\n            assert len(test)==2, \"The transition function should return two variables: states and rates.\"\n            states,rates = test\n            assert isinstance(states, np.ndarray) and states.ndim==2 and issubclass(states.dtype.type, np.integer), \"The states returned by the transition function need to be an integer 2d numpy array: %r\" %states\n            assert isinstance(rates, np.ndarray) and rates.ndim==1, \"The rates returned by the transition function need to be a 1d numpy array: %r\" % rates\n            usesNumpy = True\n            \n        return usesNumpy", "response": "Check whether the transition function returns output of the correct types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convertToTransitionDict(self,transitions):\n        states,rates = transitions\n        rateDict = defaultdict(float)\n        if states.shape[1] == 1:\n            for idx,state in enumerate(states):\n                rateDict[int(state)] += rates[idx]\n        else:\n            for idx,state in enumerate(states):\n                rateDict[tuple(state)] += rates[idx]        \n        return rateDict", "response": "Converts the output from transition() into a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive some initial state, this iteratively determines new states. We repeatedly call the transition function on unvisited states in the frontier set. Each newly visited state is put in a dictionary called 'mapping' and the rates are stored in a dictionary.", "response": "def indirectInitialMatrix(self, initialState):\n        \"\"\"\n        Given some initial state, this iteratively determines new states.\n        We repeatedly call the transition function on unvisited states in the frontier set.\n        Each newly visited state is put in a dictionary called 'mapping' and the rates are stored in a dictionary.\n        \"\"\"\n        mapping                 = {}   \n        rates                   = OrderedDict()\n        \n        #Check whether the initial state is defined and of the correct type, and convert to a tuple or int. \n        convertedState           = self.checkInitialState(initialState)        \n\n        if isinstance(convertedState,set):\n            #If initialstates is a set, include all states in the set in the mapping.\n            frontier = set( convertedState )\n            for idx,state in enumerate(convertedState):  \n                mapping[state] = idx\n                if idx == 0: #Test the return type of the transition function (dict or numpy).\n                    usesNumpy = self.checkTransitionType(initialState)                \n        else:\n            #Otherwise include only the single state.\n            frontier                = set( [convertedState] )\n            usesNumpy               = self.checkTransitionType(initialState)\n            mapping[convertedState] = 0       \n     \n\n        while len(frontier) > 0:\n            fromstate = frontier.pop()\n            fromindex = mapping[fromstate]    \n            \n            if usesNumpy: #If numpy is used, convert to a dictionary with tuples and rates.\n                transitions = self.transition(np.array(fromstate))\n                transitions = self.convertToTransitionDict(transitions) \n            else:\n                transitions = self.transition(fromstate)\n                \n            for tostate,rate in transitions.items():\n                if tostate not in mapping:\n                    frontier.add(tostate)\n                    mapping[tostate] = len(mapping)\n                toindex                     = mapping[tostate]\n                rates[(fromindex, toindex)] = rate\n\n        #Inverse the keys and values in mapping to get a dictionary with indices and states.\n        self.mapping = {value: key for key, value in list(mapping.items())}\n        \n        #Use the `rates` dictionary to fill a sparse dok matrix.\n        D = dok_matrix((self.size,self.size))\n        D.update(rates)\n        return D.tocsr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the state code for a specific state or set of states.", "response": "def getStateCode(self,state):\n        \"\"\"                \n        Calculates the state code for a specific state or set of states.\n        We transform the states so that they are nonnegative and take an inner product.\n        The resulting number is unique because we use numeral system with a large enough base.\n        \"\"\"\n        return np.dot(state-self.minvalues,self.statecode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setStateCodes(self):    \n\n        #calculate the statespace and determine the minima and maxima each element in the state vector     \n        statespace      = self.statespace()     \n        self.minvalues  = np.amin(statespace,axis=0)\n        self.maxvalues  = np.amax(statespace,axis=0)\n        \n        #calculate the largest number of values and create a state code        \n        statesize       = statespace.shape[1]  \n        largestRange    = 1+np.max(self.maxvalues-self.minvalues) \n        self.statecode  = np.power(largestRange, np.arange(statesize),dtype=int) \n   \n        #Calculate the codes, sort them, and store them in self.codes\n        codes           = self.getStateCode(statespace)         \n        sorted_indices  = np.argsort(codes)\n        self.codes      = codes[sorted_indices]  \n        if np.unique(self.codes).shape != self.codes.shape:\n            raise \"Non-unique coding of states, results are unreliable\"\n            \n        #For the end results, it is useful to put the indices and corresponding states in a dictionary        \n        mapping = OrderedDict()\n        for index,state in enumerate(statespace[sorted_indices]):\n            mapping[index]  = state    \n            \n        self.mapping        = mapping", "response": "Generates the codes for the states in the statespace and stores them in self. codes and self. mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getStateIndex(self,state):\n        statecodes = self.getStateCode(state)\n        return np.searchsorted(self.codes,statecodes).astype(int)", "response": "Returns the index of a state in the log table by calculating the state code and searching for this code a sorted list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the indices of new states and their rates.", "response": "def transitionStates(self,state):\n        \"\"\"\n        Return the indices of new states and their rates. \n        \"\"\"\n        newstates,rates         = self.transition(state)              \n        newindices              = self.getStateIndex(newstates)\n        return newindices,rates"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the initial matrix to a rate matrix.", "response": "def convertToRateMatrix(self, Q):\n        \"\"\"\n        Converts the initial matrix to a rate matrix.\n        We make all rows in Q sum to zero by subtracting the row sums from the diagonal.\n        \"\"\"\n        rowSums             = Q.sum(axis=1).getA1()\n        idxRange            = np.arange(Q.shape[0])\n        Qdiag               = coo_matrix((rowSums,(idxRange,idxRange)),shape=Q.shape).tocsr()\n        return Q-Qdiag"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convertToProbabilityMatrix(self, Q):\n        rowSums             = Q.sum(axis=1).getA1()\n        l                   = np.max(rowSums)*1.00001\n        diagonalElements    = 1.-rowSums/l\n        idxRange            = np.arange(Q.shape[0])\n        Qdiag               = coo_matrix((diagonalElements,(idxRange,idxRange)),shape=Q.shape).tocsr()\n        return Qdiag+Q.multiply(1./l)", "response": "Converts the initial matrix to a probability matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assertSingleClass(self,P):\n        components, _ = csgraph.connected_components(P, directed=True, connection='weak')   \n        assert components==1, \"The Markov chain has %r communicating classes. Make sure there is a single communicating class.\" %components", "response": "Check whether the rate matrix consists of a single connected class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the transition matrix for the current state of the user.", "response": "def getTransitionMatrix(self,probabilities=True):\n        \"\"\"\n        If self.P has been given already, we will reuse it and convert it to a sparse csr matrix if needed.\n        Otherwise, we will generate it using the direct or indirect method.         \n        Since most solution methods use a probability matrix, this is the default setting. \n        By setting probabilities=False we can also return a rate matrix.\n        \"\"\"\n        if self.P is not None:               \n            if isspmatrix(self.P): \n                if not isspmatrix_csr(self.P):\n                    self.P = self.P.tocsr() \n            else:\n                assert isinstance(self.P, np.ndarray) and self.P.ndim==2 and self.P.shape[0]==self.P.shape[1],'P needs to be a 2d numpy array with an equal number of columns and rows'                     \n                self.P = csr_matrix(self.P)   \n                \n        elif self.direct == True:\n            self.P = self.directInitialMatrix()\n            \n        else:\n            self.P = self.indirectInitialMatrix(self.initialState)   \n\n        if probabilities:    \n            P = self.convertToProbabilityMatrix(self.P)\n        else: \n            P = self.convertToRateMatrix(self.P)   \n        \n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef powerMethod(self, tol = 1e-8, maxiter = 1e5):\n        P = self.getIrreducibleTransitionMatrix().T #take transpose now to speed up dot product.\n        size = P.shape[0]        \n        pi = np.zeros(size);  pi1 = np.zeros(size)\n        pi[0] = 1;\n        n = norm(pi - pi1,1); i = 0;\n        while n > tol and i < maxiter:\n            pi1 = P.dot(pi)\n            pi = P.dot(pi1)\n            n = norm(pi - pi1,1); i += 1\n        self.pi = pi", "response": "Carry out the power method and store the result in the class attribute pi."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigenMethod(self, tol = 1e-8, maxiter = 1e5):\n        Q = self.getIrreducibleTransitionMatrix(probabilities=False)\n        \n        if Q.shape == (1, 1):\n            self.pi = np.array([1.0]) \n            return          \n        \n        if Q.shape == (2, 2):\n            self.pi= np.array([Q[1,0],Q[0,1]]/(Q[0,1]+Q[1,0]))\n            return                 \n        \n        size = Q.shape[0]\n        guess = np.ones(size,dtype=float)\n        w, v = eigs(Q.T, k=1, v0=guess, sigma=1e-6, which='LM',tol=tol, maxiter=maxiter)\n        pi = v[:, 0].real\n        pi /= pi.sum()\n        \n        self.pi = pi", "response": "This method calculates the initial eigenvalue of the class attribute pi."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining ``pi`` by solving a system of linear equations using :func:`spsolve`. The method has no parameters since it is an exact method. The result is stored in the class attribute ``pi``. Example ------- >>> P = np.array([[0.5,0.5],[0.6,0.4]]) >>> mc = markovChain(P) >>> mc.linearMethod() >>> print(mc.pi) [ 0.54545455 0.45454545] Remarks ------- For large state spaces, the linear algebra solver may not work well due to memory overflow. Code due to http://stackoverflow.com/questions/21308848/", "response": "def linearMethod(self): \n        \"\"\"\n        Determines ``pi`` by solving a system of linear equations using :func:`spsolve`. \n        The method has no parameters since it is an exact method. The result is stored in the class attribute ``pi``.   \n     \n        Example\n        -------\n        >>> P = np.array([[0.5,0.5],[0.6,0.4]])\n        >>> mc = markovChain(P)\n        >>> mc.linearMethod()\n        >>> print(mc.pi) \n        [ 0.54545455  0.45454545]\n        \n        Remarks\n        -------\n        For large state spaces, the linear algebra solver may not work well due to memory overflow.\n        Code due to http://stackoverflow.com/questions/21308848/\n        \"\"\"    \n        P       = self.getIrreducibleTransitionMatrix()        \n      \n        #if P consists of one element, then set self.pi = 1.0\n        if P.shape == (1, 1):\n            self.pi = np.array([1.0]) \n            return  \n     \n        size    = P.shape[0]\n        dP      = P - eye(size)\n        #Replace the first equation by the normalizing condition.\n        A       = vstack([np.ones(size), dP.T[1:,:]]).tocsr()  \n        rhs     = np.zeros((size,))\n        rhs[0]  = 1   \n        \n        self.pi = spsolve(A, rhs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the steady state distribution using your preferred method and store it in the attribute pi.", "response": "def computePi(self,method='power'):\n        \"\"\"\n        Calculate the steady state distribution using your preferred method and store it in the attribute `pi`. \n        By default uses the most robust method, 'power'. Other methods are 'eigen','linear', and 'krylov'\n        \n        Parameters\n        ----------\n        method : string, optional(default='power')\n            The method for obtaining ``pi``. The possible options are 'power','eigen','linear','krylov'. \n        \n        Example\n        -------\n        >>> P = np.array([[0.5,0.5],[0.6,0.4]])\n        >>> mc = markovChain(P)\n        >>> mc.computePi('power') \n        >>> print(mc.pi)\n        [ 0.54545455  0.45454545]        \n        \n        See Also\n        --------\n        For details about the specific methods see\n        :func:`powerMethod`,\n        :func:`eigenMethod`,\n        :func:`linearMethod`, and\n        :func:`krylovMethod` .       \n        \"\"\"\n        methodSet = ['power','eigen','linear','krylov']\n        assert method in methodSet, \"Incorrect method specified. Choose from %r\" % methodSet\n        method = method + 'Method'         \n        return getattr(self,method)()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef printPi(self):\n        assert self.pi is not None, \"Calculate pi before calling printPi()\"\n        assert len(self.mapping)>0, \"printPi() can only be used in combination with the direct or indirect method. Use print(mc.pi) if your subclass is called mc.\"        \n        for key,state in self.mapping.items():\n            print(state,self.pi[key])", "response": "Print all states state and steady state probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlaunches IPython under Windows", "response": "def launch_ipython(argv=None):\n    \"\"\"\n    Force usage of QtConsole under Windows\n    \"\"\"\n    from .linux import launch_ipython as _launch_ipython_linux\n    os.environ = {str(k): str(v) for k,v in os.environ.items()}\n    try:\n        from qtconsole.qtconsoleapp import JupyterQtConsoleApp\n    except ImportError:\n        sys.exit(\"ERROR: IPython QtConsole not installed in this environment. \"\n                 \"Try with `conda install jupyter ipython qtconsole`\")\n    else:\n        _launch_ipython_linux(ipython_app=JupyterQtConsoleApp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlaunches IPython from this interpreter with custom args if needed.", "response": "def launch_ipython(argv=None, ipython_app=None):\n    \"\"\"\n    Launch IPython from this interpreter with custom args if needed.\n    Chimera magic commands are also enabled automatically.\n    \"\"\"\n    try:\n        if ipython_app is None:\n            from IPython.terminal.ipapp import TerminalIPythonApp as ipython_app\n        from traitlets.config import Config\n    except ImportError:\n        sys.exit(\"ERROR: IPython not installed in this environment. \"\n                 \"Try with `conda install ipython`\")\n    else:\n        # launch_new_instance(argv)\n        app = ipython_app()\n        c = Config()\n        code = [\"from pychimera import enable_chimera_inline\",\n                \"enable_chimera_inline()\"]\n        c.InteractiveShellApp.exec_lines = code\n        app.update_config(c)\n        app.initialize(argv)\n        app.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef launch_notebook(argv=None):\n    try:\n        import nbformat.v4 as nbf\n        from notebook.notebookapp import NotebookApp\n        from notebook.services.contents import manager\n        from traitlets.config import Config\n    except ImportError:\n        sys.exit(\"ERROR: Jupyter Notebook not installed in this environment. \"\n                 \"Try with `conda install ipython jupyter notebook`\")\n    else:\n        nbf._original_new_notebook = nbf.new_notebook\n        def _prepopulate_nb_patch():\n            nb = nbf._original_new_notebook()\n            cell = nbf.new_code_cell(\"# Run this cell to complete Chimera initialization\\n\"\n                                     \"from pychimera import enable_chimera, enable_chimera_inline, chimera_view\\n\"\n                                     \"enable_chimera()\\nenable_chimera_inline()\\nimport chimera\")\n            nb['cells'].append(cell)\n            return nb\n        manager.new_notebook = _prepopulate_nb_patch\n        app = NotebookApp()\n        c = Config()\n        c.FileContentsManager.untitled_notebook = \"Untitled PyChimera Notebook\"\n        app.update_config(c)\n        app.initialize(argv)\n        app.start()", "response": "Launch a Jupyter Notebook with custom Untitled filenames and optional boilerplate code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_chimera_inline():\n\n    from IPython.display import IFrame\n    from IPython.core.magic import register_line_magic\n    import chimera\n    import Midas\n\n    @register_line_magic\n    def chimera_export_3D(line):\n        if chimera.viewer.__class__.__name__ == 'NoGuiViewer':\n            print('This magic requires a headless Chimera build. '\n                  'Check http://www.cgl.ucsf.edu/chimera/download.html#unsupported.',\n                  file=sys.stderr)\n            return\n        models = eval(line) if line else []\n\n        def html(*models):\n            if models:\n                for m in chimera.openModels.list():\n                    m.display = False\n                chimera.selection.clearCurrent()\n                for model in models:\n                    model.display = True\n                    chimera.selection.addCurrent(model)\n                    chimera.runCommand('focus sel')\n            chimera.viewer.windowSize = 800, 600\n            path = 'chimera_scene_export.html'\n            Midas.export(filename=path, format='WebGL')\n            return IFrame(path, *[x + 20 for x in chimera.viewer.windowSize])\n        return html(*models)\n    del chimera_export_3D\n\n    @register_line_magic\n    def chimera_run(line):\n        if not line:\n            print(\"Usage: %chimera_run <chimera command>\", file=sys.stderr)\n            return\n        chimera.runCommand(line)\n    del chimera_run", "response": "Enable IPython magic commands to run some Chimera actions in a single IPython notebook."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chimera_view(*molecules):\n    try:\n        import nglview as nv\n    except ImportError:\n        raise ImportError('You must install nglview!')\n    import chimera\n\n    class _ChimeraStructure(nv.Structure):\n        def __init__(self, *molecules):\n            if not molecules:\n                raise ValueError('Please supply at least one chimera.Molecule.')\n            self.ext = \"pdb\"\n            self.params = {}\n            self.id = str(id(molecules[0]))\n            self.molecules = molecules\n\n        def get_structure_string(self):\n            s = StringIO()\n            chimera.pdbWrite(self.molecules, self.molecules[0].openState.xform, s)\n            return s.getvalue()\n\n    if not molecules:\n        molecules = chimera.openModels.list(modelTypes=[chimera.Molecule])\n\n    structure = _ChimeraStructure(*molecules)\n    return nv.NGLWidget(structure)", "response": "Returns a Chimera widget for the requested molecules with NGLViewer in a Python notebook."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbypass script loading and initialize Chimera correctly, once the env has been properly patched. Parameters ---------- verbose : bool, optional, default=False If True, let Chimera speak freely. It can be _very_ verbose. nogui : bool, optional, default=True Don't start the GUI.", "response": "def enable_chimera(verbose=False, nogui=True):\n    \"\"\"\n    Bypass script loading and initialize Chimera correctly, once\n    the env has been properly patched.\n\n    Parameters\n    ----------\n    verbose : bool, optional, default=False\n        If True, let Chimera speak freely. It can be _very_ verbose.\n    nogui : bool, optional, default=True\n        Don't start the GUI.\n    \"\"\"\n    if os.getenv('CHIMERA_ENABLED'):\n        return\n    import chimera\n    _pre_gui_patches()\n    if not nogui:\n        chimera.registerPostGraphicsFunc(_post_gui_patches)\n    try:\n        import chimeraInit\n        if verbose:\n            chimera.replyobj.status('initializing pychimera')\n    except ImportError as e:\n        sys.exit(str(e) + \"\\nERROR: Chimera could not be loaded!\")\n    chimeraInit.init(['', '--script', NULL] + (sys.argv[1:] if not nogui else []),\n                     debug=verbose, silent=not verbose, nostatus=not verbose,\n                     nogui=nogui, eventloop=not nogui, exitonquit=not nogui,\n                     title=chimera.title+' (PyChimera)')\n    os.environ['CHIMERA_ENABLED'] = '1'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch_sys_version():\n    if '|' in sys.version:\n        sys_version = sys.version.split('|')\n        sys.version = ' '.join([sys_version[0].strip(), sys_version[-1].strip()])", "response": "Patch sys. version to include the version number of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch_environ(nogui=True):\n    if 'CHIMERA' in os.environ:\n        return\n\n    paths = guess_chimera_path(search_all=nogui)\n    CHIMERA_BASE = paths[0]\n    if nogui:  # try finding a headless version\n        try:\n            CHIMERA_BASE = next(p for p in paths if 'headless' in p)\n        except StopIteration:\n            pass\n\n    if not os.path.isdir(CHIMERA_BASE):\n        sys.exit(\"Could not find UCSF Chimera.\\n{}\".format(_INSTRUCTIONS))\n    os.environ['CHIMERA'] = CHIMERA_BASE\n    CHIMERA_LIB = os.path.join(CHIMERA_BASE, 'lib')\n\n    # Set Tcl/Tk for gui mode\n    if 'TCL_LIBRARY' in os.environ:\n        os.environ['CHIMERA_TCL_LIBRARY'] = os.environ['TCL_LIBRARY']\n    os.environ['TCL_LIBRARY'] = os.path.join(CHIMERA_LIB, 'tcl8.6')\n\n    if 'TCLLIBPATH' in os.environ:\n        os.environ['CHIMERA_TCLLIBPATH'] = os.environ['TCLLIBPATH']\n    os.environ['TCLLIBPATH'] = '{' + CHIMERA_LIB + '}'\n\n    if 'TK_LIBRARY' in os.environ:\n        os.environ['CHIMERA_TK_LIBRARY'] = os.environ['TK_LIBRARY']\n        del os.environ['TK_LIBRARY']\n\n    if 'TIX_LIBRARY' in os.environ:\n        os.environ['CHIMERA_TIX_LIBRARY'] = os.environ['TIX_LIBRARY']\n        del os.environ['TIX_LIBRARY']\n\n    if 'PYTHONNOUSERSITE' in os.environ:\n        os.environ['CHIMERA_PYTHONNOUSERSITE'] = os.environ['PYTHONNOUSERSITE']\n    os.environ['PYTHONNOUSERSITE'] = '1'\n\n    # Check interactive and IPython\n    if in_ipython() and hasattr(sys, 'ps1') and not sys.argv[0].endswith('ipython'):\n        sys.argv.insert(1, 'ipython')\n\n    # Platform-specific patches\n    patch_environ_for_platform(CHIMERA_BASE, CHIMERA_LIB, nogui=nogui)\n    os.execve(sys.executable, [sys.executable] + sys.argv, os.environ)", "response": "Patch the current environment variables so Chimera can start up and we can import its modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef guess_chimera_path(search_all=False):\n    paths = _search_chimera(CHIMERA_BINARY, CHIMERA_LOCATIONS, CHIMERA_PREFIX,\n                            search_all=search_all)\n    if not paths and search_all:  # try headless?\n        headless = '{0[0]}{1}{0[1]}'.format(os.path.split(CHIMERA_BINARY), '-headless')\n        paths = _search_chimera(headless, CHIMERA_LOCATIONS, CHIMERA_PREFIX,\n                                search_all=search_all)\n    if not paths:\n        sys.exit(\"Could not find UCSF Chimera.\\n{}\".format(_INSTRUCTIONS))\n    return paths", "response": "Try to guess Chimera installation path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _search_chimera(binary, directories, prefix, search_all=False):\n    # First, check if environment variable is already present\n    try:\n        return os.path.expanduser(os.environ['CHIMERADIR']),\n    except KeyError:\n        pass\n\n    paths = []\n    binary_path = find_executable(binary)\n    if binary_path is not None:\n        real_path = os.path.realpath(binary_path)  # follow symlinks\n        chimera_dir = os.path.sep + os.path.join(*real_path.split(os.path.sep)[1:-2])\n        paths.append(chimera_dir)\n    else:\n        search_all = True\n\n    if search_all:\n        for basedir in directories:\n            found_paths = filter(os.path.isdir, glob(os.path.join(basedir, prefix)))\n            if found_paths:\n                found_paths.sort()\n                found_paths.reverse()\n                paths.extend(found_paths)\n    seen = set()\n    paths = [p for p in paths if p not in seen and not seen.add(p)]\n    return paths", "response": "Search Chimera for Challenge files in a list of directories and prefix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_cli_options(args):\n    if _interactive_mode(args.interactive):\n        os.environ['PYTHONINSPECT'] = '1'\n    if in_ipython():\n        return\n    exclusive_choices = [[None, args.command], ['-c', args.string], ['-m', args.module]]\n    for flag_choice in exclusive_choices:\n        try:\n            a = sys.argv.index(flag_choice[0] or flag_choice[1])\n        except ValueError:\n            a = 1000\n        flag_choice.append(a)\n    exclusive_choices.sort(key=lambda v: v[2])\n    for i, (flag, choice, _) in enumerate(exclusive_choices):\n        if not choice:\n            continue\n        sys.argv = [choice] + sys.argv[sys.argv.index(choice)+1:]\n        if not flag:\n            if choice == 'ipython':\n                launch_ipython(argv=sys.argv[1:])\n            elif choice == 'notebook':\n                launch_notebook()\n            else:\n                globals().update(runpy.run_path(choice, run_name=\"__main__\"))\n        elif flag == '-m':\n            if '--' in sys.argv[1:2] :  # -m syntax needs '--' for extra args\n                sys.argv.pop(1)\n            globals().update(runpy.run_module(choice, run_name=\"__main__\"))\n        elif flag == '-c':\n            exec choice in globals(), locals()  # workaround\n        else:\n            continue\n        break", "response": "Quick implementation of Python interpreter s - m - c and file execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if we need to relaunch Python in interactive mode.", "response": "def _interactive_mode(interactive_flag=False):\n    \"\"\"\n    Check if we need to relaunch Python in interactive mode:\n    \"\"\"\n    return any([interactive_flag, sys.flags.interactive, len(sys.argv) <= 1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_version(filename):\n    init_py = open(filename).read()\n    return re.search(\"__version__ = ['\\\"]([^'\\\"]+)['\\\"]\", init_py).group(1)", "response": "Get package version as listed in __version__ in filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a QR Code from the input data.", "response": "def make(content, error=None, version=None, mode=None, mask=None, encoding=None,\n         eci=False, micro=None, boost_error=True):\n    \"\"\"\\\n    Creates a (Micro) QR Code.\n\n    This is main entry point to create QR Codes and Micro QR Codes.\n\n    Aside from `content`, all parameters are optional and an optimal (minimal)\n    (Micro) QR Code with a maximal error correction level (minimum \"M\") is\n    generated.\n\n    :param content: The data to encode. Either a Unicode string, an integer or\n            bytes. If bytes are provided, the `encoding` parameter should be\n            used to specify the used encoding.\n    :type content: str, int, bytes\n    :param error: Error correction level. If ``None`` (default), error\n            correction level ``L`` is used (note: Micro QR Code version M1 does\n            not support error correction. If an explicit error level is used,\n            a M1 QR Code won't be generated).\n            Valid values: ``None`` (allowing generation of M1 codes or use error\n            correction level \"L\" or better see ``boost_error``), \"L\", \"M\", \"Q\",\n            \"H\" (error correction level \"H\" isn't available for Micro QR Codes).\n\n            =====================================   ===========================\n            Error correction level                  Error correction capability\n            =====================================   ===========================\n            L (Segno's default unless version M1)   recovers  7% of data\n            M                                       recovers 15% of data\n            Q                                       recovers 25% of data\n            H (not available for Micro QR Codes)    recovers 30% of data\n            =====================================   ===========================\n\n            Higher error levels may require larger QR Codes (see also `version`\n            parameter).\n\n            The `error` parameter is case insensitive.\n\n            See also the `boost_error` parameter.\n    :type error: str, unicode or None\n    :param version: QR Code version. If the value is ``None`` (default), the\n            minimal version which fits for the input data will be used.\n            Valid values: \"M1\", \"M2\", \"M3\", \"M4\" (for Micro QR Codes) or an\n            integer between 1 and 40 (for QR Codes).\n            The `version` parameter is case insensitive.\n    :type version: int, str, unicode or None.\n    :param mode: \"numeric\", \"alphanumeric\", \"byte\", or \"kanji\". If the value is\n            ``None`` (default) the appropriate mode will automatically be\n            determined.\n            If `version` refers a to Micro QR Code, this function may raise a\n            :py:class:`ModeError` if the provided `mode` is not supported.\n\n            ============    =======================\n            Mode            (Micro) QR Code Version\n            ============    =======================\n            numeric         1 - 40, M1, M2, M3, M4\n            alphanumeric    1 - 40,     M2, M3, M4\n            byte            1 - 40,         M3, M4\n            kanji           1 - 40,         M3, M4\n            ============    =======================\n\n            The `mode` parameter is case insensitive.\n\n    :type mode: str, unicode, or None\n    :param mask: Data mask. If the value is ``None`` (default), the\n            appropriate data mask is choosen automatically. If the `mask`\n            parameter if provided, this function may raise a :py:exc:`MaskError`\n            if the mask is invalid.\n    :type mask: int or None\n    :param encoding: Indicates the encoding in mode \"byte\". By default\n            (`encoding` is ``None``) the implementation tries to use the\n            standard conform ISO/IEC 8859-1 encoding and if it does not fit, it\n            will use UTF-8. Note that no ECI mode indicator is inserted by\n            default (see `eci`).\n            The `encoding` parameter is case insensitive.\n    :type encoding: unicode|str|None\n    :param bool eci: Indicates if binary data which does not use the default\n            encoding (ISO/IEC 8859-1) should enforce the ECI mode. Since a lot\n            of QR Code readers do not support the ECI mode, this feature is\n            disabled by default and the data is encoded in the provided\n            `encoding` using the usual \"byte\" mode. Set `eci` to ``True`` if\n            an ECI header should be inserted into the QR Code. Note that\n            the implementation may not know the ECI designator for the provided\n            `encoding` and may raise an exception if the ECI designator cannot\n            be found.\n            The ECI mode is not supported by Micro QR Codes.\n    :param micro: If `version` is ``None`` this parameter can be used\n            to allow the creation of a Micro QR Code.\n            If set to ``False``, a QR Code is generated. If set to\n            ``None`` (default) a Micro QR Code may be generated if applicable.\n            If set to ``True`` the algorithm generates a Micro QR Code or\n            raises an exception if the `mode` is not compatible or the `content`\n            is too large for Micro QR Codes.\n    :type micro: bool or None\n    :param bool boost_error: Indicates if the error correction level may be\n            increased if it does not affect the version (default: ``True``).\n            If set to ``True``, the ``error`` parameter is interpreted as\n            minimum error level. If set to ``False``, the resulting (Micro) QR\n            Code uses the provided ``error`` level (or the default error\n            correction level, if error is ``None``)\n    :raises: :py:exc:`QRCodeError`: In case of a problem. In fact, it's more\n            likely that a derived exception is thrown:\n            :py:exc:`ModeError`: In case of problems with the mode (i.e. invalid\n            mode or invalid `mode` / `version` combination.\n            :py:exc:`VersionError`: In case the `version` is invalid or the\n            `micro` parameter contradicts the provided `version`.\n            :py:exc:`ErrorLevelError`: In case the error level is invalid or the\n            error level is not supported by the provided `version`.\n            :py:exc:`DataOverflowError`: In case the data does not fit into a\n            (Micro) QR Code or it does not fit into the provided `version`.\n            :py:exc:`MaskError`: In case an invalid data mask was specified.\n    :rtype: QRCode\n    \"\"\"\n    return QRCode(encoder.encode(content, error, version, mode, mask, encoding,\n                                 eci, micro, boost_error=boost_error))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_micro(content, error=None, version=None, mode=None, mask=None,\n               encoding=None, boost_error=True):\n    \"\"\"\\\n    Creates a Micro QR Code.\n\n    See :py:func:`make` for a description of the parameters.\n\n    Note: Error correction level \"H\" isn't available for Micro QR Codes. If\n    used, this function raises a :py:class:`segno.ErrorLevelError`.\n\n    :rtype: QRCode\n    \"\"\"\n    return make(content, error=error, version=version, mode=mode, mask=mask,\n                encoding=encoding, micro=True, boost_error=boost_error)", "response": "\\ Creates a Micro QR Code."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a sequence of QR Codes from a string.", "response": "def make_sequence(content, error=None, version=None, mode=None, mask=None,\n                  encoding=None, boost_error=True, symbol_count=None):\n    \"\"\"\\\n    Creates a sequence of QR Codes.\n\n    If the content fits into one QR Code and neither ``version`` nor\n    ``symbol_count`` is provided, this function may return a sequence with\n    one QR Code which does not use the Structured Append mode. Otherwise a\n    sequence of 2 .. n  (max. n = 16) QR Codes is returned which use the\n    Structured Append mode.\n\n    The Structured Append mode allows to split the content over a number\n    (max. 16) QR Codes.\n\n    The Structured Append mode isn't available for Micro QR Codes, therefor\n    the returned sequence contains QR Codes, only.\n\n    Since this function returns an iterable object, it may be used as follows:\n\n    .. code-block:: python\n\n        for i, qrcode in enumerate(segno.make_sequence(data, symbol_count=2)):\n             qrcode.save('seq-%d.svg' % i, scale=10, color='darkblue')\n\n    The returned number of QR Codes is determined by the `version` or\n    `symbol_count` parameter\n\n    See :py:func:`make` for a description of the other parameters.\n\n    :param int symbol_count: Number of symbols.\n    :rtype: QRCodeSequence\n    \"\"\"\n    return QRCodeSequence(map(QRCode,\n                              encoder.encode_sequence(content, error=error,\n                                                      version=version,\n                                                      mode=mode, mask=mask,\n                                                      encoding=encoding,\n                                                      boost_error=boost_error,\n                                                      symbol_count=symbol_count)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef symbol_size(self, scale=1, border=None):\n        return utils.get_symbol_size(self._version, scale=scale, border=border)", "response": "\\ Returns the size of the symbol in pixels with the provided scaling factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matrix_iter(self, scale=1, border=None):\n        return utils.matrix_iter(self.matrix, self._version, scale, border)", "response": "Returns an iterator over the matrix of values in the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show(self, delete_after=20, scale=10, border=None, color='#000',\n             background='#fff'):  # pragma: no cover\n        \"\"\"\\\n        Displays this QR code.\n\n        This method is mainly intended for debugging purposes.\n\n        This method saves the output of the :py:meth:`png` method (by default\n        with a scaling factor of 10) to a temporary file and opens it with the\n        standard PNG viewer application or within the standard webbrowser.\n        The temporary file is deleted afterwards (unless `delete_after` is set\n        to ``None``).\n\n        If this method does not show any result, try to increase the\n        `delete_after` value or set it to ``None``\n\n        :param delete_after: Time in seconds to wait till the temporary file is\n                deleted.\n        \"\"\"\n        import os\n        import time\n        import tempfile\n        import webbrowser\n        import threading\n        try:  # Python 2\n            from urlparse import urljoin\n            from urllib import pathname2url\n        except ImportError:  # Python 3\n            from urllib.parse import urljoin\n            from urllib.request import pathname2url\n\n        def delete_file(name):\n            time.sleep(delete_after)\n            try:\n                os.unlink(name)\n            except OSError:\n                pass\n\n        f = tempfile.NamedTemporaryFile('wb', suffix='.png', delete=False)\n        try:\n            self.save(f, scale=scale, color=color, background=background,\n                      border=border)\n        except:\n            f.close()\n            os.unlink(f.name)\n            raise\n        f.close()\n        webbrowser.open_new_tab(urljoin('file:', pathname2url(f.name)))\n        if delete_after is not None:\n            t = threading.Thread(target=delete_file, args=(f.name,))\n            t.start()", "response": "This method displays the QR code. It will automatically save the QR code to a temporary file and then open a web browser with the standard PNG viewer application or within the standard webbrowser application."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef svg_data_uri(self, xmldecl=False, encode_minimal=False,\n                     omit_charset=False, nl=False, **kw):\n        \"\"\"\\\n        Converts the QR Code into a SVG data URI.\n\n        The XML declaration is omitted by default (set ``xmldecl`` to ``True``\n        to enable it), further the newline is omitted by default (set ``nl`` to\n        ``True`` to enable it).\n\n        Aside from the missing ``out`` parameter and the different ``xmldecl``\n        and ``nl`` default values and the additional parameter ``encode_minimal``\n        and ``omit_charset`` this method uses the same parameters as the\n        usual SVG serializer.\n\n        :param bool xmldecl: Indicates if the XML declaration should be\n                        serialized (default: ``False``)\n        :param bool encode_minimal: Indicates if the resulting data URI should\n                        use minimal percent encoding (disabled by default).\n        :param bool omit_charset: Indicates if the ``;charset=...`` should be omitted\n                        (disabled by default)\n        :rtype: str\n        \"\"\"\n        return writers.as_svg_data_uri(self.matrix, self._version,\n                                       xmldecl=xmldecl, nl=nl,\n                                       encode_minimal=encode_minimal,\n                                       omit_charset=omit_charset, **kw)", "response": "\\ Converts the QR Code into a SVG data URI."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef png_data_uri(self, **kw):\n        return writers.as_png_data_uri(self.matrix, self._version, **kw)", "response": "\\ Converts the QR Code into a PNG data URI."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef terminal(self, out=None, border=None):\n        if out is None and sys.platform == 'win32':  # pragma: no cover\n            # Windows < 10 does not support ANSI escape sequences, try to\n            # call the a Windows specific terminal output which uses the\n            # Windows API.\n            try:\n                writers.write_terminal_win(self.matrix, self._version, border)\n            except OSError:\n                # Use the standard output even if it may print garbage\n                writers.write_terminal(self.matrix, self._version, sys.stdout,\n                                       border)\n        else:\n            writers.write_terminal(self.matrix, self._version, out or sys.stdout,\n                                   border)", "response": "Serializes the matrix as ANSI escape code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the QR Code to a file.", "response": "def save(self, out, kind=None, **kw):\n        \"\"\"\\\n        Serializes the QR Code in one of the supported formats.\n        The serialization format depends on the filename extension.\n\n        **Common keywords**\n\n\n        ==========    ==============================================================\n        Name          Description\n        ==========    ==============================================================\n        scale         Integer or float indicating the size of a single module.\n                      Default: 1. The interpretation of the scaling factor depends\n                      on the serializer. For pixel-based output (like PNG) the\n                      scaling factor is interepreted as pixel-size (1 = 1 pixel).\n                      EPS interprets ``1`` as 1 point (1/72 inch) per module.\n                      Some serializers (like SVG) accept float values. If the\n                      serializer does not accept float values, the value will be\n                      converted to an integer value (note: int(1.6) == 1).\n        border        Integer indicating the size of the quiet zone.\n                      If set to ``None`` (default), the recommended border size\n                      will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n        color         A string or tuple representing a color value for the dark\n                      modules. The default value is \"black\".  The color can be\n                      provided as ``(R, G, B)`` tuple, as web color name\n                      (like \"red\") or in hexadecimal format (``#RGB`` or\n                      ``#RRGGBB``). Some serializers (SVG and PNG) accept an alpha\n                      transparency value like ``#RRGGBBAA``.\n        background    A string or tuple representing a color for the light modules\n                      or background. See \"color\" for valid values.\n                      The default value depends on the serializer. SVG uses no\n                      background color (``None``) by default, other serializers\n                      use \"white\" as default background color.\n        ==========    ==============================================================\n\n\n        **Scalable Vector Graphics (SVG)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.BytesIO\n        kind             \"svg\" or \"svgz\" (to create a gzip compressed SVG)\n        scale            integer or float\n        color            Default: \"#000\" (black)\n                         ``None`` is a valid value. If set to ``None``, the resulting\n                         path won't have a \"stroke\" attribute. The \"stroke\" attribute\n                         may be defined via CSS (external).\n                         If an alpha channel is defined, the output depends of the\n                         used SVG version. For SVG versions >= 2.0, the \"stroke\"\n                         attribute will have a value like \"rgba(R, G, B, A)\", otherwise\n                         the path gets another attribute \"stroke-opacity\" to emulate\n                         the alpha channel.\n                         To minimize the document size, the SVG serializer uses\n                         automatically the shortest color representation: If\n                         a value like \"#000000\" is provided, the resulting\n                         document will have a color value of \"#000\". If the color\n                         is \"#FF0000\", the resulting color is not \"#F00\", but\n                         the web color name \"red\".\n        background       Default value ``None``. If this paramater is set to another\n                         value, the resulting image will have another path which\n                         is used to define the background color.\n                         If an alpha channel is used, the resulting path may\n                         have a \"fill-opacity\" attribute (for SVG version < 2.0)\n                         or the \"fill\" attribute has a \"rgba(R, G, B, A)\" value.\n                         See keyword \"color\" for further details.\n        xmldecl          Boolean value (default: ``True``) indicating whether the\n                         document should have an XML declaration header.\n                         Set to ``False`` to omit the header.\n        svgns            Boolean value (default: ``True``) indicating whether the\n                         document should have an explicit SVG namespace declaration.\n                         Set to ``False`` to omit the namespace declaration.\n                         The latter might be useful if the document should be\n                         embedded into a HTML 5 document where the SVG namespace\n                         is implicitly defined.\n        title            String (default: ``None``) Optional title of the generated\n                         SVG document.\n        desc             String (default: ``None``) Optional description of the\n                         generated SVG document.\n        svgid            A string indicating the ID of the SVG document\n                         (if set to ``None`` (default), the SVG element won't have\n                         an ID).\n        svgclass         Default: \"segno\". The CSS class of the SVG document\n                         (if set to ``None``, the SVG element won't have a class).\n        lineclass        Default: \"qrline\". The CSS class of the path element\n                         (which draws the dark modules (if set to ``None``, the path\n                         won't have a class).\n        omitsize         Indicates if width and height attributes should be\n                         omitted (default: ``False``). If these attributes are\n                         omitted, a ``viewBox`` attribute will be added to the\n                         document.\n        unit             Default: ``None``\n                         Inidctaes the unit for width / height and other coordinates.\n                         By default, the unit is unspecified and all values are\n                         in the user space.\n                         Valid values: em, ex, px, pt, pc, cm, mm, in, and percentages\n                         (any string is accepted, this parameter is not validated\n                         by the serializer)\n        encoding         Encoding of the XML document. \"utf-8\" by default.\n        svgversion       SVG version (default: ``None``). If specified (a float),\n                         the resulting document has an explicit \"version\" attribute.\n                         If set to ``None``, the document won't have a \"version\"\n                         attribute. This parameter is not validated.\n        compresslevel    Default: 9. This parameter is only valid, if a compressed\n                         SVG document should be created (file extension \"svgz\").\n                         1 is fastest and produces the least compression, 9 is slowest\n                         and produces the most. 0 is no compression.\n        =============    ==============================================================\n\n\n        **Portable Network Graphics (PNG)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.BytesIO\n        kind             \"png\"\n        scale            integer\n        color            Default: \"#000\" (black)\n                         ``None`` is a valid value iff background is not ``None``.\n        background       Default value ``#fff`` (white)\n                         See keyword \"color\" for further details.\n        compresslevel    Default: 9. Integer indicating the compression level\n                         for the ``IDAT`` (data) chunk.\n                         1 is fastest and produces the least compression, 9 is slowest\n                         and produces the most. 0 is no compression.\n        dpi              Default: None. Specifies the DPI value for the image.\n                         By default, the DPI value is unspecified. Please note\n                         that the DPI value is converted into meters (maybe with\n                         rounding errors) since PNG does not support the unit\n                         \"dots per inch\".\n        addad            Boolean value (default: True) to (dis-)allow a \"Software\"\n                         comment indicating that the file was created by Segno.\n        =============    ==============================================================\n\n\n        **Encapsulated PostScript (EPS)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.StringIO\n        kind             \"eps\"\n        scale            integer or float\n        color            Default: \"#000\" (black)\n        background       Default value: ``None`` (no background)\n        =============    ==============================================================\n\n\n        **Portable Document Format (PDF)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.BytesIO\n        kind             \"pdf\"\n        scale            integer or float\n        compresslevel    Default: 9. Integer indicating the compression level.\n                         1 is fastest and produces the least compression, 9 is slowest\n                         and produces the most. 0 is no compression.\n        =============    ==============================================================\n\n\n        **Text (TXT)**\n\n        Does not support the \"scale\" keyword!\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.StringIO\n        kind             \"txt\"\n        color            Default: \"1\"\n        background       Default: \"0\"\n        =============    ==============================================================\n\n\n        **ANSI escape code**\n\n        Supports the \"border\" keyword, only!\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        kind             \"ans\"\n        =============    ==============================================================\n\n\n        **Portable Bitmap (PBM)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.BytesIO\n        kind             \"pbm\"\n        scale            integer\n        plain            Default: False. Boolean to switch between the P4 and P1 format.\n                         If set to ``True``, the (outdated) P1 serialization format is\n                         used.\n        =============    ==============================================================\n\n\n        **Portable Arbitrary Map (PAM)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.BytesIO\n        kind             \"pam\"\n        scale            integer\n        color            Default: \"#000\" (black).\n        background       Default value ``#fff`` (white). Use ``None`` for a transparent\n                         background.\n        =============    ==============================================================\n\n\n        **LaTeX / PGF/TikZ**\n\n        To use the output of this serializer, the ``PGF/TikZ`` (and optionally\n        ``hyperref``) package is required in the LaTeX environment. The\n        serializer itself does not depend on any external packages.\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.StringIO\n        kind             \"tex\"\n        scale            integer or float\n        color            LaTeX color name (default: \"black\"). The color is written\n                         \"at it is\", so ensure that the color is a standard color or it\n                         has been defined in the enclosing LaTeX document.\n        url              Default: ``None``. Optional URL where the QR Code should\n                         point to. Requires the ``hyperref`` package in your LaTeX\n                         environment.\n        =============    ==============================================================\n\n\n        **X BitMap (XBM)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.StringIO\n        kind             \"xbm\"\n        scale            integer\n        name             Name of the variable (default: \"img\")\n        =============    ==============================================================\n\n\n        **X PixMap (XPM)**\n\n        =============    ==============================================================\n        Name             Description\n        =============    ==============================================================\n        out              Filename or io.StringIO\n        kind             \"xpm\"\n        scale            integer\n        color            Default: \"#000\" (black).\n        background       Default value ``#fff`` (white)\n                         ``None`` indicates a transparent background.\n        name             Name of the variable (default: \"img\")\n        =============    ==============================================================\n\n\n        :param out: A filename or a writable file-like object with a\n                ``name`` attribute. Use the `kind` parameter if `out` is\n                a :py:class:`io.ByteIO` or :py:class:`io.StringIO` stream which\n                don't have a ``name`` attribute.\n        :param kind: If the desired output format cannot be determined from\n                the ``out`` parameter, this parameter can be used to indicate the\n                serialization format (i.e. \"svg\" to enforce SVG output)\n        :param kw: Any of the supported keywords by the specific serialization\n                method.\n        \"\"\"\n        writers.save(self.matrix, self._version, out, kind, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef terminal(self, out=None, border=None):\n        for qrcode in self:\n            qrcode.terminal(out=out, border=border)", "response": "Serializes the sequence of QR Codes as ANSI escape codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, out, kind=None, **kw):\n        m = len(self)\n\n        def prepare_fn_noop(o, n):\n            \"\"\"\\\n            Function to enumerate file names, does nothing by default\n            \"\"\"\n            return o\n\n        def prepare_filename(o, n):\n            \"\"\"\\\n            Function to enumerate file names.\n            \"\"\"\n            return o.format(m, n)\n\n        prepare_fn = prepare_fn_noop\n        if m > 1 and isinstance(out, str_type):\n            dot_idx = out.rfind('.')\n            if dot_idx > -1:\n                out = out[:dot_idx] + '-{0:02d}-{1:02d}' + out[dot_idx:]\n                prepare_fn = prepare_filename\n        for n, qrcode in enumerate(self, start=1):\n            qrcode.save(prepare_fn(out, n), kind=kind, **kw)", "response": "Save the QR Code sequence to out."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize the QR Code as SVG.", "response": "def write_svg(matrix, version, out, scale=1, border=None, color='#000',\n              background=None, xmldecl=True, svgns=True, title=None, desc=None,\n              svgid=None, svgclass='segno', lineclass='qrline', omitsize=False,\n              unit=None, encoding='utf-8', svgversion=None, nl=True):\n    \"\"\"\\\n    Serializes the QR Code as SVG document.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write bytes.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 pixel per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: ``#000``). Any value\n            which is supported by SVG can be used. In addition, ``None``\n            is a valid value. The resulting path won't have a ``stroke``\n            attribute.\n    :param background: Optional background color (default: ``None`` = no\n            background color). See `color` for valid values.\n    :param bool xmldecl: Inidcates if the XML declaration header should be\n            written (default: ``True``)\n    :param bool svgns: Indicates if the SVG namespace should be written\n            (default: ``True``).\n    :param str title: Optional title of the generated SVG document.\n    :param str desc: Optional description of the generated SVG document.\n    :param svgid: The ID of the SVG document (if set to ``None`` (default),\n            the SVG element won't have an ID).\n    :param svgclass: The CSS class of the SVG document\n            (if set to ``None``, the SVG element won't have a class).\n    :param lineclass: The CSS class of the path element (which draws the\n            \"black\" modules (if set to ``None``, the path won't have a class).\n    :param bool omitsize: Indicates if width and height attributes should be\n            omitted (default: ``False``). If these attributes are omitted,\n            a ``viewBox`` attribute will be added to the document.\n    :param str unit: Unit for width / height and other coordinates.\n            By default, the unit is unspecified and all values are\n            in the user space.\n            Valid values: em, ex, px, pt, pc, cm, mm, in, and percentages\n    :param str encoding: Encoding of the XML document. \"utf-8\" by default.\n    :param float svgversion: SVG version (default: None)\n    :param bool nl: Indicates if the document should have a trailing newline\n            (default: ``True``)\n    \"\"\"\n    check_valid_scale(scale)\n    check_valid_border(border)\n    unit = unit or ''\n    if unit and omitsize:\n        raise ValueError('The unit \"{0}\" has no effect if the size '\n                         '(width and height) is omitted.'.format(unit))\n    with writable(out, 'wt', encoding=encoding) as f:\n        write = f.write\n        # Write the document header\n        if xmldecl:\n            write('<?xml version=\"1.0\" encoding=\"{0}\"?>\\n'.format(encoding))\n        write('<svg')\n        if svgns:\n            write(' xmlns=\"http://www.w3.org/2000/svg\"')\n        if svgversion is not None and svgversion < 2.0:\n            write(' version={0}'.format(quoteattr(str(svgversion))))\n        border = get_border(version, border)\n        width, height = get_symbol_size(version, scale, border)\n        if not omitsize:\n            write(' width=\"{0}{2}\" height=\"{1}{2}\"'.format(width, height, unit))\n        if omitsize or unit:\n            write(' viewBox=\"0 0 {0} {1}\"'.format(width, height))\n        if svgid:\n            write(' id={0}'.format(quoteattr(svgid)))\n        if svgclass:\n            write(' class={0}'.format(quoteattr(svgclass)))\n        write('>')\n        if title is not None:\n            write('<title>{0}</title>'.format(escape(title)))\n        if desc is not None:\n            write('<desc>{0}</desc>'.format(escape(desc)))\n        allow_css3_colors = svgversion is not None and svgversion >= 2.0\n        if background is not None:\n            bg_color = colors.color_to_webcolor(background, allow_css3_colors=allow_css3_colors)\n            fill_opacity = ''\n            if isinstance(bg_color, tuple):\n                bg_color, opacity = bg_color\n                fill_opacity = ' fill-opacity={0}'.format(quoteattr(str(opacity)))\n            write('<path fill=\"{0}\"{1} d=\"M0 0h{2}v{3}h-{2}z\"/>'\n                  .format(bg_color, fill_opacity, width, height))\n        write('<path')\n        if scale != 1:\n            write(' transform=\"scale({0})\"'.format(scale))\n        if color is not None:\n            opacity = None\n            stroke_color = colors.color_to_webcolor(color, allow_css3_colors=allow_css3_colors)\n            if isinstance(stroke_color, tuple):\n                stroke_color, opacity = stroke_color\n            write(' stroke={0}'.format(quoteattr(stroke_color)))\n            if opacity is not None:\n                write(' stroke-opacity={0}'.format(quoteattr(str(opacity))))\n        if lineclass:\n            write(' class={0}'.format(quoteattr(lineclass)))\n        write(' d=\"')\n        # Current pen pointer position\n        x, y = border, border + .5  # .5 == stroke-width / 2\n        line_iter = matrix_to_lines(matrix, x, y)\n        # 1st coord is absolute\n        (x1, y1), (x2, y2) = next(line_iter)\n        coord = ['M{0} {1}h{2}'.format(x1, y1, x2 - x1)]\n        append_coord = coord.append\n        x, y = x2, y2\n        for (x1, y1), (x2, y2) in line_iter:\n            append_coord('m{0} {1}h{2}'.format(x1 - x, int(y1 - y), x2 - x1))\n            x, y = x2, y2\n        write(''.join(coord))\n        # Close path and doc\n        write('\"/></svg>')\n        if nl:\n            write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_svg_data_uri(matrix, version, scale=1, border=None, color='#000',\n                    background=None, xmldecl=False, svgns=True, title=None,\n                    desc=None, svgid=None, svgclass='segno',\n                    lineclass='qrline', omitsize=False, unit='',\n                    encoding='utf-8', svgversion=None, nl=False,\n                    encode_minimal=False, omit_charset=False):\n    \"\"\"\\\n    Converts the matrix to a SVG data URI.\n\n    The XML declaration is omitted by default (set ``xmldecl`` to ``True``\n    to enable it), further the newline is omitted by default (set ``nl`` to\n    ``True`` to enable it).\n\n    Aside from the missing ``out`` parameter and the different ``xmldecl``\n    and ``nl`` default values and the additional parameter ``encode_minimal``\n    and ``omit_charset`` this function uses the same parameters as the\n    usual SVG serializer.\n\n    :param bool encode_minimal: Indicates if the resulting data URI should\n                    use minimal percent encoding (disabled by default).\n    :param bool omit_charset: Indicates if the ``;charset=...`` should be omitted\n                    (disabled by default)\n    :rtype: str\n    \"\"\"\n    encode = partial(quote, safe=b\"\") if not encode_minimal else partial(quote, safe=b\" :/='\")\n    buff = io.BytesIO()\n    write_svg(matrix, version, buff, scale=scale, color=color, background=background,\n              border=border, xmldecl=xmldecl, svgns=svgns, title=title,\n              desc=desc, svgclass=svgclass, lineclass=lineclass,\n              omitsize=omitsize, encoding=encoding, svgid=svgid, unit=unit,\n              svgversion=svgversion, nl=nl)\n    return 'data:image/svg+xml{0},{1}' \\\n                .format(';charset=' + encoding if not omit_charset else '',\n                        # Replace \" quotes with ' and URL encode the result\n                        # See also https://codepen.io/tigt/post/optimizing-svgs-in-data-uris\n                        encode(_replace_quotes(buff.getvalue())))", "response": "Converts a matrix to a SVG data URI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite an SVG document to a binary file.", "response": "def write_svg_debug(matrix, version, out, scale=15, border=None,\n                    fallback_color='fuchsia', color_mapping=None,\n                    add_legend=True):\n    \"\"\"\\\n    Internal SVG serializer which is useful to debugging purposes.\n\n    This function is not exposed to the QRCode class by intention and the\n    resulting SVG document is very inefficient (lots of <rect/>s).\n    Dark modules are black and light modules are white by default. Provide\n    a custom `color_mapping` to override these defaults.\n    Unknown modules are red by default.\n\n    :param matrix: The matrix\n    :param version: Version constant\n    :param out: binary file-like object or file name\n    :param scale: Scaling factor\n    :param border: Quiet zone\n    :param fallback_color: Color which is used for modules which are not 0x0 or 0x1\n                and for which no entry in `color_mapping` is defined.\n    :param color_mapping: dict of module values to color mapping (optional)\n    :param bool add_legend: Indicates if the bit values should be added to the\n                matrix (default: True)\n    \"\"\"\n    clr_mapping = {\n        0x0: '#fff',\n        0x1: '#000',\n        0x2: 'red',\n        0x3: 'orange',\n        0x4: 'gold',\n        0x5: 'green',\n    }\n    if color_mapping is not None:\n        clr_mapping.update(color_mapping)\n    border = get_border(version, border)\n    width, height = get_symbol_size(version, scale, border)\n    matrix_size = get_symbol_size(version, scale=1, border=0)[0]\n    with writable(out, 'wt', encoding='utf-8') as f:\n        legend = []\n        write = f.write\n        write('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n')\n        write('<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {0} {1}\">'.format(width, height))\n        write('<style type=\"text/css\"><![CDATA[ text { font-size: 1px; font-family: Helvetica, Arial, sans; } ]]></style>')\n        write('<g transform=\"scale({0})\">'.format(scale))\n        for i in range(matrix_size):\n            y = i + border\n            for j in range(matrix_size):\n                x = j + border\n                bit = matrix[i][j]\n                if add_legend and bit not in (0x0, 0x1):\n                    legend.append((x, y, bit))\n                fill = clr_mapping.get(bit, fallback_color)\n                write('<rect x=\"{0}\" y=\"{1}\" width=\"1\" height=\"1\" fill=\"{2}\"/>'.format(x, y, fill))\n        # legend may be empty if add_legend == False\n        for x, y, val in legend:\n            write('<text x=\"{0}\" y=\"{1}\">{2}</text>'.format(x+.2, y+.9, val))\n        write('</g></svg>\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_eps(matrix, version, out, scale=1, border=None, color='#000',\n              background=None):\n    \"\"\"\\\n    Serializes the QR Code as EPS document.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write strings.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 point (1/72 inch) per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: black). The\n            color can be provided as ``(R, G, B)`` tuple (this method\n            acceppts floats as R, G, B values), as web color name (like\n            \"red\") or in hexadecimal format (``#RGB`` or ``#RRGGBB``).\n    :param background: Optional background color (default: ``None`` = no\n            background color). See `color` for valid values.\n    \"\"\"\n    import textwrap\n\n    def write_line(writemeth, content):\n        \"\"\"\\\n        Writes `content` and ``LF``.\n        \"\"\"\n        # Postscript: Max. 255 characters per line\n        for line in textwrap.wrap(content, 254):\n            writemeth(line)\n            writemeth('\\n')\n\n    def rgb_to_floats(clr):\n        \"\"\"\\\n        Converts the provided color into an acceptable format for Postscript's\n         ``setrgbcolor``\n        \"\"\"\n        def to_float(c):\n            if isinstance(c, float):\n                if not 0.0 <= c <= 1.0:\n                    raise ValueError('Invalid color \"{0}\". Not in range 0 .. 1'\n                                     .format(c))\n                return c\n            return 1 / 255.0 * c if c != 1 else c\n\n        return tuple([to_float(i) for i in colors.color_to_rgb(clr)])\n\n    check_valid_scale(scale)\n    check_valid_border(border)\n    with writable(out, 'wt') as f:\n        writeline = partial(write_line, f.write)\n        border = get_border(version, border)\n        width, height = get_symbol_size(version, scale, border)\n        # Write common header\n        writeline('%!PS-Adobe-3.0 EPSF-3.0')\n        writeline('%%Creator: {0}'.format(CREATOR))\n        writeline('%%CreationDate: {0}'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n        writeline('%%DocumentData: Clean7Bit')\n        writeline('%%BoundingBox: 0 0 {0} {1}'.format(width, height))\n        # Write the shortcuts\n        writeline('/m { rmoveto } bind def')\n        writeline('/l { rlineto } bind def')\n        stroke_color_is_black = colors.color_is_black(color)\n        stroke_color = color if stroke_color_is_black else rgb_to_floats(color)\n        if background is not None:\n            writeline('{0:f} {1:f} {2:f} setrgbcolor clippath fill'\n                      .format(*rgb_to_floats(background)))\n            if stroke_color_is_black:\n                # Reset RGB color back to black iff stroke color is black\n                # In case stroke color != black set the RGB color later\n                writeline('0 0 0 setrgbcolor')\n        if not stroke_color_is_black:\n            writeline('{0:f} {1:f} {2:f} setrgbcolor'.format(*stroke_color))\n        if scale != 1:\n            writeline('{0} {0} scale'.format(scale))\n        writeline('newpath')\n        # Current pen position y-axis\n        # Note: 0, 0 = lower left corner in PS coordinate system\n        y = get_symbol_size(version, scale=1, border=0)[1] + border - .5  # .5 = linewidth / 2\n        line_iter = matrix_to_lines(matrix, border, y, incby=-1)\n        # EPS supports absolute coordinates as well, but relative coordinates\n        # are more compact and IMO nicer; so the 1st coordinate is absolute, all\n        # other coordinates are relative\n        (x1, y1), (x2, y2) = next(line_iter)\n        coord = ['{0} {1} moveto {2} 0 l'.format(x1, y1, x2 - x1)]\n        append_coord = coord.append\n        x = x2\n        for (x1, y1), (x2, y2) in line_iter:\n            append_coord(' {0} {1} m {2} 0 l'.format(x1 - x, int(y1 - y), x2 - x1))\n            x, y = x2, y2\n        writeline(''.join(coord))\n        writeline('stroke')\n        writeline('%%EOF')", "response": "Serializes the QR Code as EPS document."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_png(matrix, version, out, scale=1, border=None, color='#000',\n              background='#fff', compresslevel=9, dpi=None, addad=True):\n    \"\"\"\\\n    Serializes the QR Code as PNG image.\n\n    By default, the generated PNG will be a greyscale image with a bitdepth\n    of 1. If different colors are provided, an indexed-color image with\n    the same bitdepth is generated.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write bytes.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 pixel per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: black). The\n            color can be provided as ``(R, G, B)`` tuple, as web color name\n            (like \"red\") or in hexadecimal format (``#RGB`` or ``#RRGGBB``).\n    :param background: Optional background color (default: white).\n            See `color` for valid values. In addition, ``None`` is\n            accepted which indicates a transparent background.\n    :param int dpi: Optional DPI setting. By default (``None``), the PNG won't\n            have any DPI information. Note that the DPI value is converted into\n            meters since PNG does not support any DPI information.\n    :param int compresslevel: Integer indicating the compression level\n            (default: 9). 1 is fastest and produces the least\n            compression, 9 is slowest and produces the most.\n            0 is no compression.\n    \"\"\"\n\n    def png_color(clr):\n        return colors.color_to_rgb_or_rgba(clr, alpha_float=False)\n\n    def chunk(name, data):\n        \"\"\"\\\n        Returns a PNG chunk with checksum.\n        \"\"\"\n        chunk_head = name + data\n        # See <https://docs.python.org/2/library/zlib.html#zlib.crc32>\n        # why crc32() & 0xFFFFFFFF is necessary\n        return pack(b'>I', len(data)) + chunk_head \\\n               + pack(b'>I', zlib.crc32(chunk_head) & 0xFFFFFFFF)\n\n    def scale_row_x_axis(row):\n        \"\"\"\\\n        Returns each pixel `scale` times.\n        \"\"\"\n        for b in row:\n            for i in range(scale):\n                yield b\n\n    def scanline(row, filter_type=b'\\0'):\n        \"\"\"\\\n        Returns a single scanline.\n        \"\"\"\n        return bytearray(chain(filter_type, _pack_bits_into_byte(row)))\n\n    def invert_row_bits(row):\n        \"\"\"\\\n        Inverts the row bits 0 -> 1, 1 -> 0\n        \"\"\"\n        return (b ^ 0x1 for b in row)\n\n    scale = int(scale)\n    check_valid_scale(scale)\n    check_valid_border(border)\n    # Background color index\n    bg_color_idx = 0\n    trans_color = 1  # white\n    stroke_is_transparent, bg_is_transparent = color is None, background is None\n    stroke_color = png_color(color) if not stroke_is_transparent else None\n    bg_color = png_color(background) if not bg_is_transparent else None\n    if stroke_color == bg_color:\n        raise ValueError('The stroke color and background color must not be the same')\n    stroke_is_black, stroke_is_white = False, False\n    bg_is_white, bg_is_black = False, False\n    if not stroke_is_transparent:\n        stroke_is_black = colors.color_is_black(stroke_color)\n        if not stroke_is_black:\n            stroke_is_white = colors.color_is_white(stroke_color)\n    if not bg_is_transparent:\n        bg_is_white = colors.color_is_white(bg_color)\n        if not bg_is_white:\n            bg_is_black = colors.color_is_black(bg_color)\n    transparency = stroke_is_transparent or bg_is_transparent\n    is_greyscale = False\n    invert_row = False\n    if bg_is_white:\n        is_greyscale = stroke_is_black or stroke_is_transparent\n        invert_row = is_greyscale\n        trans_color = int(not is_greyscale)\n    elif bg_is_black:\n        is_greyscale = stroke_is_transparent or stroke_is_white\n    elif bg_is_transparent:\n        is_greyscale = stroke_is_black or stroke_is_white\n        invert_row = is_greyscale\n    palette = None\n    if not is_greyscale:\n        # PLTE image\n        if bg_is_transparent:\n            bg_color = colors.invert_color(stroke_color[:3])\n            if len(stroke_color) == 4:\n                bg_color += (0,)\n        elif stroke_is_transparent:\n            stroke_color = colors.invert_color(bg_color[:3])\n            if len(bg_color) == 4:\n                stroke_color += (0,)\n        palette = sorted([bg_color, stroke_color], key=len, reverse=True)\n        bg_color_idx = palette.index(bg_color)\n        # Usually, the background color is the first entry in the PLTE so\n        # no bit inverting should be necessary\n        invert_row = bg_color_idx > 0\n    border = get_border(version, border)\n    width, height = get_symbol_size(version, scale, border)\n    if dpi:\n        dpi = int(dpi)\n        if dpi < 0:\n            raise ValueError('DPI value must not be negative')\n        dpi = int(dpi // 0.0254)\n    with writable(out, 'wb') as f:\n        write = f.write\n        # PNG writing by \"hand\" since this lib should not rely on PIL/Pillow\n        # and PyPNG does not support PNG filters which leads to a suboptimal\n        # performance (and file size) if the PNG image should be scaled.\n        # I.e. a (unrealistic) scaling factor of 600 would require a 17400 x 17400\n        # matrix as input for PyPNG for a 21 x 21 QR Code (+ border = 4) while\n        # this algorithm works upon the primary 21 x 21 matrix.\n        write(b'\\211PNG\\r\\n\\032\\n')  # Magic number\n        colortype = 3 if not is_greyscale else 0\n        if is_greyscale:\n            bg_color_idx = int(invert_row)\n        # Header:\n        # width, height, bitdepth, colortype, compression meth., filter, interlance\n        write(chunk(b'IHDR', pack(b'>2I5B', width, height, 1, colortype, 0, 0, 0)))\n        if dpi:\n            write(chunk(b'pHYs', pack(b'>LLB', dpi, dpi, 1)))\n        if colortype == 3:  # Palette\n            write(chunk(b'PLTE', b''.join(pack(b'>3B', *clr[:3]) for clr in palette)))\n            # <https://www.w3.org/TR/PNG/#11tRNS>\n            if len(palette[0]) > 3:  # Color with alpha is the first in the palette\n                f.write(chunk(b'tRNS', b''.join(pack(b'>B', clr[3]) for clr in palette if len(clr) > 3)))\n            elif transparency:\n                f.write(chunk(b'tRNS', pack(b'>B', bg_color_idx)))\n        elif is_greyscale and transparency:  # Greyscale with alpha\n            # Greyscale with alpha channel\n            # <https://www.w3.org/TR/PNG/#11tRNS>\n            # 2 bytes for color type == 0 (greyscale)\n            write(chunk(b'tRNS', pack(b'>1H', trans_color)))\n        # <https://www.w3.org/TR/PNG/#9Filters>\n        # This variable holds the \"Up\" filter which indicates that this scanline\n        # is equal to the above scanline (since it is filled with null bytes)\n        same_as_above = b''\n        row_filters = []\n        if invert_row:\n            row_filters.append(invert_row_bits)\n        if scale > 1:\n            # 2 == PNG Filter \"Up\"  <https://www.w3.org/TR/PNG/#9-table91>\n            same_as_above = scanline([0] * width, filter_type=b'\\2') * (scale - 1)\n            row_filters.append(scale_row_x_axis)\n        horizontal_border, vertical_border = b'', b''\n        if border > 0:\n            # Calculate horizontal and vertical border\n            horizontal_border = scanline([bg_color_idx] * width) * border * scale\n            vertical_border = [bg_color_idx] * border * scale\n        res = bytearray(horizontal_border)\n        for row in matrix:\n            row = reduce(lambda r, fn: fn(r), row_filters, row)\n            # Chain precalculated left border with row and right border\n            res += scanline(chain(vertical_border, row, vertical_border))\n            res += same_as_above  # This is b'' if no scaling factor was provided\n        res += horizontal_border\n        if _PY2:\n            res = bytes(res)\n        write(chunk(b'IDAT', zlib.compress(res, compresslevel)))\n        if addad:\n            write(chunk(b'tEXt', b'Software\\x00' + CREATOR.encode('ascii')))\n        write(chunk(b'IEND', b''))", "response": "Serializes the QR Code as PNG image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the provided matrix into a PNG data URI.", "response": "def as_png_data_uri(matrix, version, scale=1, border=None, color='#000',\n                    background='#fff', compresslevel=9, addad=True):\n    \"\"\"\\\n    Converts the provided matrix into a PNG data URI.\n\n    :rtype: str\n    \"\"\"\n    buff = io.BytesIO()\n    write_png(matrix, version, buff, scale=scale, border=border, color=color,\n              background=background, compresslevel=compresslevel, addad=addad)\n    return 'data:image/png;base64,{0}' \\\n                .format(base64.b64encode(buff.getvalue()).decode('ascii'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a QR Code matrix to a PDF file.", "response": "def write_pdf(matrix, version, out, scale=1, border=None, color='#000',\n              background=None, compresslevel=9):\n    \"\"\"\\\n    Serializes the QR Code as PDF document.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write bytes.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 pixel per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: black). The\n            color can be provided as ``(R, G, B)`` tuple, as web color name\n            (like \"red\") or in hexadecimal format (``#RGB`` or ``#RRGGBB``).\n    :param background: Optional background color (default: ``None`` = no\n            background color). See `color` for valid values.\n    :param int compresslevel: Integer indicating the compression level\n            (default: 9). 1 is fastest and produces the least\n            compression, 9 is slowest and produces the most.\n            0 is no compression.\n    \"\"\"\n\n    def write_string(writemeth, s):\n        writemeth(s.encode('ascii'))\n\n    def to_pdf_color(clr):\n        \"\"\"\\\n        Converts the provided color into an acceptable format for PDF's\n        \"DeviceRGB\" color space.\n         \"\"\"\n        def to_float(c):\n            if isinstance(c, float):\n                if not 0.0 <= c <= 1.0:\n                    raise ValueError('Invalid color \"{0}\". Not in range 0 .. 1'\n                                     .format(c))\n                return c\n            return 1 / 255.0 * c if c != 1 else c\n        return tuple([to_float(i) for i in colors.color_to_rgb(clr)])\n\n    check_valid_scale(scale)\n    check_valid_border(border)\n    width, height = get_symbol_size(version, scale, border)\n    border = get_border(version, border)\n    creation_date = \"{0}{1:+03d}'{2:02d}'\".format(time.strftime('%Y%m%d%H%M%S'),\n                                                  time.timezone // 3600,\n                                                  abs(time.timezone) % 60)\n    cmds = []\n    append_cmd = cmds.append\n    if scale > 1:\n        append_cmd('{0} 0 0 {0} 0 0 cm'.format(scale))\n    if background is not None:\n        # If the background color is defined, a rect is drawn in the background\n        append_cmd('{} {} {} rg'.format(*to_pdf_color(background)))\n        append_cmd('0 0 {} {} re'.format(width, height))\n        append_cmd('f q')\n    # Set the stroke color only iff it is not black (default)\n    if not colors.color_is_black(color):\n        append_cmd('{} {} {} RG'.format(*to_pdf_color(color)))\n    # Current pen position y-axis\n    # Note: 0, 0 = lower left corner in PDF coordinate system\n    y = get_symbol_size(version, scale=1, border=0)[1] + border - .5\n    # Set the origin in the upper left corner\n    append_cmd('1 0 0 1 {0} {1} cm'.format(border, y))\n    # PDF supports absolute coordinates, only\n    for (x1, y1), (x2, y2) in matrix_to_lines(matrix, 0, 0, incby=-1):\n        append_cmd('{0} {1} m {2} {1} l'.format(x1, y1, x2, y2))\n    append_cmd('S')\n    graphic = zlib.compress((' '.join(cmds)).encode('ascii'), compresslevel)\n    with writable(out, 'wb') as f:\n        write = f.write\n        writestr = partial(write_string, write)\n        object_pos = []\n        write(b'%PDF-1.4\\r%\\xE2\\xE3\\xCF\\xD3\\r\\n')\n        for obj in ('obj <</Type /Catalog /Pages 2 0 R>>\\r\\nendobj\\r\\n',\n                    'obj <</Type /Pages /Kids [3 0 R] /Count 1>>\\r\\nendobj\\r\\n',\n                    'obj <</Type /Page /Parent 2 0 R /MediaBox [0 0 {0} {1}] /Contents 4 0 R>>\\r\\nendobj\\r\\n'.format(width, height),\n                    'obj <</Length {0} /Filter /FlateDecode>>\\r\\nstream\\r\\n'.format(len(graphic))):\n            object_pos.append(f.tell())\n            writestr('{0} 0 {1}'.format(len(object_pos), obj))\n        write(graphic)\n        write(b'\\r\\nendstream\\r\\nendobj\\r\\n')\n        object_pos.append(f.tell())\n        writestr('{0} 0 obj <</CreationDate(D:{1})/Producer({2})/Creator({2})\\r\\n>>\\r\\nendofbj\\r\\n' \\\n                 .format(len(object_pos), creation_date, CREATOR))\n        object_pos.append(f.tell())\n        xref_location = f.tell()\n        writestr('xref\\r\\n0 {0}\\r\\n0000000000 65535 f\\r\\n'.format(len(object_pos) + 1))\n        for pos in object_pos:\n            writestr('{0:010d} {1:05d} n\\r\\n'.format(pos, 0))\n        writestr('trailer <</Size {0}/Root 1 0 R/Info 5 0 R>>\\r\\n'.format(len(object_pos) + 1))\n        writestr('startxref\\r\\n{0}\\r\\n%%EOF\\r\\n'.format(xref_location))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize QR code in a text format.", "response": "def write_txt(matrix, version, out, border=None, color='1', background='0'):\n    \"\"\"\\\n    Serializes QR code in a text format.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write text.\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Character to use for the black modules (default: '1')\n    :param background: Character to use for the white modules (default: '0')\n    \"\"\"\n    row_iter = matrix_iter(matrix, version, scale=1, border=border)\n    colours = (str(background), str(color))\n    with writable(out, 'wt') as f:\n        write = f.write\n        for row in row_iter:\n            write(''.join([colours[i] for i in row]))\n            write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_pbm(matrix, version, out, scale=1, border=None, plain=False):\n    row_iter = matrix_iter(matrix, version, scale, border)\n    width, height = get_symbol_size(version, scale=scale, border=border)\n    with writable(out, 'wb') as f:\n        write = f.write\n        write('{0}\\n'\n              '# Created by {1}\\n'\n              '{2} {3}\\n'\\\n              .format(('P4' if not plain else 'P1'), CREATOR, width, height).encode('ascii'))\n        if not plain:\n            for row in row_iter:\n                write(bytearray(_pack_bits_into_byte(row)))\n        else:\n            for row in row_iter:\n                write(b''.join(str(i).encode('ascii') for i in row))\n                write(b'\\n')", "response": "Serializes the matrix as PBM."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_pam(matrix, version, out, scale=1, border=None, color='#000',\n              background='#fff'):\n    \"\"\"\\\n    Serializes the matrix as `PAM <http://netpbm.sourceforge.net/doc/pam.html>`_\n    image.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write binary data.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 pixel per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: black). The\n            color can be provided as ``(R, G, B)`` tuple, as web color name\n            (like \"red\") or in hexadecimal format (``#RGB`` or ``#RRGGBB``).\n    :param background: Optional background color (default: white).\n            See `color` for valid values. In addition, ``None`` is\n            accepted which indicates a transparent background.\n    \"\"\"\n    def invert_row_bits(row):\n        \"\"\"\\\n        Inverts the row bits 0 -> 1, 1 -> 0\n        \"\"\"\n        return bytearray([b ^ 0x1 for b in row])\n\n    def row_to_color_values(row, colours):\n        return b''.join([colours[b] for b in row])\n\n    if not color:\n        raise ValueError('Invalid stroke color \"{0}\"'.format(color))\n    row_iter = matrix_iter(matrix, version, scale, border)\n    width, height = get_symbol_size(version, scale=scale, border=border)\n    depth, maxval, tuple_type = 1, 1, 'BLACKANDWHITE'\n    transparency = False\n    stroke_color = colors.color_to_rgb_or_rgba(color, alpha_float=False)\n    bg_color = colors.color_to_rgb_or_rgba(background, alpha_float=False) if background is not None else None\n    colored_stroke = not (colors.color_is_black(stroke_color) or colors.color_is_white(stroke_color))\n    if bg_color is None:\n        tuple_type = 'GRAYSCALE_ALPHA' if not colored_stroke else 'RGB_ALPHA'\n        transparency = True\n        bg_color = colors.invert_color(stroke_color[:3])\n        bg_color += (0,)\n        if len(stroke_color) != 4:\n            stroke_color += (255,)\n    elif colored_stroke or not (colors.color_is_black(bg_color) or colors.color_is_white(bg_color)):\n        tuple_type = 'RGB'\n    is_rgb = tuple_type.startswith('RGB')\n    colours = None\n    if not is_rgb and transparency:\n        depth = 2\n        colours = (b'\\x01\\x00', b'\\x00\\x01')\n    elif is_rgb:\n        maxval = max(chain(stroke_color, bg_color))\n        depth = 3 if not transparency else 4\n        fmt = '>{0}B'.format(depth).encode('ascii')\n        colours=(pack(fmt, *bg_color), pack(fmt, *stroke_color))\n    row_filter = invert_row_bits if colours is None else partial(row_to_color_values, colours=colours)\n    with writable(out, 'wb') as f:\n        write = f.write\n        write('P7\\n'\n              '# Created by {0}\\n'\n              'WIDTH {1}\\n'\n              'HEIGHT {2}\\n'\n              'DEPTH {3}\\n'\n              'MAXVAL {4}\\n'\n              'TUPLTYPE {5}\\n'\n              'ENDHDR\\n'.format(CREATOR, width, height, depth, maxval, tuple_type).encode('ascii'))\n        for row in row_iter:\n            write(row_filter(row))", "response": "Serializes the matrix as PAM."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_xpm(matrix, version, out, scale=1, border=None, color='#000',\n              background='#fff', name='img'):\n    \"\"\"\\\n    Serializes the matrix as `XPM <https://en.wikipedia.org/wiki/X_PixMap>`_ image.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write binary data.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 pixel per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param color: Color of the modules (default: black). The\n            color can be provided as ``(R, G, B)`` tuple, as web color name\n            (like \"red\") or in hexadecimal format (``#RGB`` or ``#RRGGBB``).\n    :param background: Optional background color (default: white).\n            See `color` for valid values. ``None`` indicates a transparent\n            background.\n    :param str name: Name of the image (must be a valid C-identifier).\n            Default: \"img\".\n    \"\"\"\n    row_iter = matrix_iter(matrix, version, scale, border)\n    width, height = get_symbol_size(version, scale=scale, border=border)\n    stroke_color = colors.color_to_rgb_hex(color)\n    bg_color = colors.color_to_rgb_hex(background) if background is not None else 'None'\n    with writable(out, 'wt') as f:\n        write = f.write\n        write('/* XPM */\\n'\n              'static char *{0}[] = {{\\n'\n              '\"{1} {2} 2 1\",\\n'\n              '\"  c {3}\",\\n'\n              '\"X c {4}\",\\n'.format(name, width, height, bg_color, stroke_color))\n        for i, row in enumerate(row_iter):\n            write(''.join(chain(['\"'],  (' ' if not b else 'X' for b in row),\n                                ['\"{0}\\n'.format(',' if i < height - 1 else '')])))\n        write('};\\n')", "response": "Serializes the matrix as XPM."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_xbm(matrix, version, out, scale=1, border=None, name='img'):\n    row_iter = matrix_iter(matrix, version, scale, border)\n    border = get_border(version, border)\n    width, height = get_symbol_size(version, scale=scale, border=border)\n    with writable(out, 'wt') as f:\n        write = f.write\n        write('#define {0}_width {1}\\n'\n              '#define {0}_height {2}\\n'\n              'static unsigned char {0}_bits[] = {{\\n'.format(name, width, height))\n        for i, row in enumerate(row_iter, start=1):\n            iter_ = zip_longest(*[iter(row)] * 8, fillvalue=0x0)\n            # Reverse bits since XBM uses little endian\n            bits = ['0x{0:02x}'.format(reduce(lambda x, y: (x << 1) + y, bits[::-1])) for bits in iter_]\n            write('    ')\n            write(', '.join(bits))\n            write(',\\n' if i < height else '\\n')\n        write('};\\n')", "response": "Serializes the QR code matrix as XBM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize the matrix as LaTeX PGF picture.", "response": "def write_tex(matrix, version, out, scale=1, border=None, color='black', unit='pt', url=None):\n    \"\"\"\\\n    Serializes the matrix as LaTeX PGF picture.\n\n    Requires the `PGF/TikZ <https://en.wikipedia.org/wiki/PGF/TikZ>`_ package\n    (i.e. ``\\\\usepackage{pgf}``) in the LaTeX source.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: Filename or a file-like object supporting to write text data.\n    :param scale: Indicates the size of a single module (default: 1 which\n            corresponds to 1 x 1 in the provided unit per module).\n    :param int border: Integer indicating the size of the quiet zone.\n            If set to ``None`` (default), the recommended border size\n            will be used (``4`` for QR Codes, ``2`` for a Micro QR Codes).\n    :param str color: LaTeX color name. The color name is taken at it is, so\n            ensure that it refers either to a default color name or that the\n            color was defined previously.\n    :param unit: Unit of the drawing (default: ``pt``)\n    :param url: Optional URL where the QR Code should point to. Requires the\n            \"hyperref\" package. Default: ``None``.\n    \"\"\"\n    def point(x, y):\n        return '\\\\pgfqpoint{{{0}{2}}}{{{1}{2}}}'.format(x, y, unit)\n\n    check_valid_scale(scale)\n    check_valid_border(border)\n    border = get_border(version, border)\n    with writable(out, 'wt') as f:\n        write = f.write\n        write('% Creator:  {0}\\n'.format(CREATOR))\n        write('% Date:     {0}\\n'.format(time.strftime('%Y-%m-%dT%H:%M:%S')))\n        if url:\n            write('\\\\href{{{0}}}{{'.format(url))\n        write('\\\\begin{pgfpicture}\\n')\n        write('  \\\\pgfsetlinewidth{{{0}{1}}}\\n'.format(scale, unit))\n        if color and color != 'black':\n            write('  \\\\color{{{0}}}\\n'.format(color))\n        x, y = border, -border\n        for (x1, y1), (x2, y2) in matrix_to_lines(matrix, x, y, incby=-1):\n            write('  \\\\pgfpathmoveto{{{0}}}\\n'.format(point(x1 * scale, y1 * scale)))\n            write('  \\\\pgfpathlineto{{{0}}}\\n'.format(point(x2 * scale, y2 * scale)))\n        write('  \\\\pgfusepath{stroke}\\n')\n        write('\\\\end{{pgfpicture}}{0}\\n'.format('' if not url else '}'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_terminal(matrix, version, out, border=None):\n    with writable(out, 'wt') as f:\n        write = f.write\n        colours = ['\\033[{0}m'.format(i) for i in (7, 49)]\n        for row in matrix_iter(matrix, version, scale=1, border=border):\n            prev_bit = -1\n            cnt = 0\n            for bit in row:\n                if bit == prev_bit:\n                    cnt += 1\n                else:\n                    if cnt:\n                        write(colours[prev_bit])\n                        write('  ' * cnt)\n                        write('\\033[0m')  # reset color\n                    prev_bit = bit\n                    cnt = 1\n            if cnt:\n                write(colours[prev_bit])\n                write('  ' * cnt)\n                write('\\033[0m')  # reset color\n            write('\\n')", "response": "Function to write to a terminal which supports ANSI escape codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_terminal_win(matrix, version, border=None):  # pragma: no cover\n    import sys\n    import struct\n    import ctypes\n    write = sys.stdout.write\n    std_out = ctypes.windll.kernel32.GetStdHandle(-11)\n    csbi = ctypes.create_string_buffer(22)\n    res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(std_out, csbi)\n    if not res:\n        raise OSError('Cannot find information about the console. '\n                      'Not running on the command line?')\n    default_color = struct.unpack(b'hhhhHhhhhhh', csbi.raw)[4]\n    set_color = partial(ctypes.windll.kernel32.SetConsoleTextAttribute, std_out)\n    colours = (240, default_color)\n    for row in matrix_iter(matrix, version, scale=1, border=border):\n        prev_bit = -1\n        cnt = 0\n        for bit in row:\n            if bit == prev_bit:\n                cnt += 1\n            else:\n                if cnt:\n                    set_color(colours[prev_bit])\n                    write('  ' * cnt)\n                prev_bit = bit\n                cnt = 1\n        if cnt:\n            set_color(colours[prev_bit])\n            write('  ' * cnt)\n        set_color(default_color)  # reset color\n        write('\\n')", "response": "Function to write a QR Code to a MS Windows terminal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pack_bits_into_byte(iterable):\n    return (reduce(lambda x, y: (x << 1) + y, e)\n            for e in zip_longest(*[iter(iterable)] * 8, fillvalue=0x0))", "response": "\\ Packs eight bits into one byte."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing the matrix in any of the supported formats.", "response": "def save(matrix, version, out, kind=None, **kw):\n    \"\"\"\\\n    Serializes the matrix in any of the supported formats.\n\n    :param matrix: The matrix to serialize.\n    :param int version: The (Micro) QR code version\n    :param out: A filename or a writable file-like object with a\n            ``name`` attribute. If a stream like :py:class:`io.ByteIO` or\n            :py:class:`io.StringIO` object without a ``name`` attribute is\n            provided, use the `kind` parameter to specify the serialization\n            format.\n    :param kind: If the desired output format cannot be extracted from\n            the filename, this parameter can be used to indicate the\n            serialization format (i.e. \"svg\" to enforce SVG output)\n    :param kw: Any of the supported keywords by the specific serialization\n            method.\n    \"\"\"\n    is_stream = False\n    if kind is None:\n        try:\n            fname = out.name\n            is_stream = True\n        except AttributeError:\n            fname = out\n        ext = fname[fname.rfind('.') + 1:].lower()\n    else:\n        ext = kind.lower()\n    if not is_stream and ext == 'svgz':\n        f = gzip.open(out, 'wb', compresslevel=kw.pop('compresslevel', 9))\n        try:\n            _VALID_SERIALISERS['svg'](matrix, version, f, **kw)\n        finally:\n            f.close()\n    else:\n        if kw.pop('debug', False) and ext == 'svg':\n            ext = 'svg_debug'\n        try:\n            _VALID_SERIALISERS[ext](matrix, version, out, **kw)\n        except KeyError:\n            raise ValueError('Unknown file extension \".{0}\"'.format(ext))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_parser():\n\n    def _convert_scale(val):\n        val = float(val)\n        return val if val != int(val) else int(val)\n\n    parser = argparse.ArgumentParser(prog='segno',\n                                     description='Segno QR Code and Micro QR Code generator version {0}'.format(segno.__version__))\n    parser.add_argument('--version', '-v', help='(Micro) QR Code version: 1 .. 40 or \"M1\", \"M2\", \"M3\", \"M4\"',\n                        required=False,)\n    parser.add_argument('--error', '-e', help='Error correction level: \"L\": 7%% (default), \"M\": 15%%, \"Q\": 25%%, \"H\": 30%%, \"-\": no error correction (used for M1 symbols)',\n                        choices=('L', 'M', 'Q', 'H', '-'),\n                        default=None,\n                        type=lambda x: x.upper())\n    parser.add_argument('--mode', '-m', help='Mode',\n                        choices=('numeric', 'alphanumeric', 'byte', 'kanji'),\n                        default=None,\n                        type=lambda x: x.lower())\n    parser.add_argument('--pattern', '-p', help='Mask pattern to use',\n                        required=False,\n                        default=None,\n                        type=int)\n    parser.add_argument('--scale', '-s', help='Scaling factor',\n                        default=1,\n                        type=_convert_scale)\n    parser.add_argument('--border', '-b', help='Size of the border / quiet zone',\n                        default=None,\n                        type=int)\n    parser.add_argument('--micro', help='Allow the creation of Micro QR Codes',\n                        dest='micro', action='store_true')\n    parser.add_argument('--no-micro', help='Disallow creation of Micro QR Codes (default)',\n                        dest='micro', action='store_false')\n    parser.add_argument('--color', help='Color of the dark modules. Use \"transparent\" to set the color to None (not supported by all serializers)')\n    parser.add_argument('--background', help='Color of the light modules. Use \"transparent\" to set the background to None (not supported by all serializers)')\n    parser.add_argument('--output', '-o', help='Output file. If not specified, the QR Code is printed to the terminal',\n                        required=False,\n                        )\n    parser.add_argument('--no-error-boost', help='Disables the automatic error incrementation if a higher error correction level is possible',\n                        dest='boost_error', action='store_false')\n    parser.add_argument('--seq', help='Creates a sequence of QR Codes (Structured Append mode). Version or symbol count must be provided',\n                        dest='seq', action='store_true')\n    parser.add_argument('--symbol-count', '-sc', help='Number of symbols to create',\n                        default=None,\n                        type=int)\n    # SVG\n    svg_group = parser.add_argument_group('SVG', 'SVG specific options')\n    svg_group.add_argument('--no-classes', help='Omits the (default) SVG classes',\n                           action='store_true')\n    svg_group.add_argument('--no-xmldecl', help='Omits the XML declaration header',\n                           dest='xmldecl',\n                           action='store_false')\n    svg_group.add_argument('--no-namespace', help='Indicates that the SVG document should have no SVG namespace declaration',\n                           dest='svgns',\n                           action='store_false')\n    svg_group.add_argument('--no-newline', help='Indicates that the SVG document should have no trailing newline',\n                           dest='nl',\n                           action='store_false')\n    svg_group.add_argument('--title', help='Specifies the title of the SVG document')\n    svg_group.add_argument('--desc', help='Specifies the description of the SVG document')\n    svg_group.add_argument('--svgid', help='Indicates the ID of the <svg/> element')\n    svg_group.add_argument('--svgclass', help='Indicates the CSS class of the <svg/> element')\n    svg_group.add_argument('--lineclass', help='Indicates the CSS class of the <path/> element (the dark modules)')\n    svg_group.add_argument('--no-size', help='Indicates that the SVG document should not have \"width\" and \"height\" attributes',\n                           dest='omitsize',\n                           action='store_true')\n    svg_group.add_argument('--unit', help='Indicates SVG coordinate system unit')\n    svg_group.add_argument('--svgversion', help='Indicates the SVG version',\n                           type=float)\n    svg_group.add_argument('--encoding', help='Specifies the encoding of the document',\n                           default='utf-8')\n    # PNG\n    png_group = parser.add_argument_group('PNG', 'PNG specific options')\n    png_group.add_argument('--dpi', help='Sets the DPI value of the PNG file',\n                           type=int)\n    png_group.add_argument('--no-ad', help='Omits the \"Software\" comment in the PNG file',\n                           dest='addad',\n                           action='store_false')\n    parser.add_mutually_exclusive_group().add_argument('--ver', '-V', help=\"Shows Segno's version\",\n                                                       action='version',\n                                                       version='Segno {0}'.format(segno.__version__))\n    parser.add_argument('content', nargs='+', help='The content to encode')\n    return parser", "response": "\\ Returns the command line parser."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(args):\n    parser = make_parser()\n    if not len(args):\n        parser.print_help()\n        sys.exit(1)\n    parsed_args = parser.parse_args(args)\n    if parsed_args.error == '-':\n        parsed_args.error = None\n    # 'micro' is False by default. If version is set to a Micro QR Code version,\n    # encoder.encode raises a VersionError.\n    # Small problem: --version=M4 --no-micro is allowed\n    version = parsed_args.version\n    if version is not None:\n        version = str(version).upper()\n    if not parsed_args.micro and version in ('M1', 'M2', 'M3', 'M4'):\n        parsed_args.micro = None\n    return _AttrDict(vars(parsed_args))", "response": "\\ Parses the arguments and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a configuration and returns it.", "response": "def build_config(config, filename=None):\n    \"\"\"\\\n    Builds a configuration and returns it. The config contains only keywords,\n    which are supported by the serializer. Unsupported values are ignored.\n    \"\"\"\n    # Done here since it seems not to be possible to detect if an argument\n    # was supplied by the user or if it's the default argument.\n    # If using type=lambda v: None if v in ('transparent', 'trans') else v\n    # we cannot detect if \"None\" comes from \"transparent\" or the default value\n    for clr in ('color', 'background'):\n        val = config.pop(clr, None)\n        if val in ('transparent', 'trans'):\n            config[clr] = None\n        elif val:\n            config[clr] = val\n    # SVG\n    for name in ('svgid', 'svgclass', 'lineclass'):\n        if config.get(name, None) is None:\n            config.pop(name, None)\n    if config.pop('no_classes', False):\n        config['svgclass'] = None\n        config['lineclass'] = None\n    if filename is not None:\n        ext = filename[filename.rfind('.') + 1:].lower()\n        if ext == 'svgz':  # There is no svgz serializer, use same config as svg\n            ext = 'svg'\n        supported_args = _EXT_TO_KW_MAPPING.get(ext, ())\n        # Drop unsupported arguments from config rather than getting a\n        # \"unsupported keyword\" exception\n        for k in list(config):\n            if k not in supported_args:\n                del config[k]\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_wifi_data(ssid, password, security, hidden=False):\n    def quotation_mark(x):\n        \"\"\"\\\n        Returns '\"' if x could be interpreted as hexadecimal value, otherwise\n        an empty string.\n\n        See: <https://github.com/zxing/zxing/wiki/Barcode-Contents>\n        [...] Enclose in double quotes if it is an ASCII name, but could be\n        interpreted as hex (i.e. \"ABCD\") [...]\n        \"\"\"\n        try:\n            int(x, 16)\n        except ValueError:\n            return ''\n        return '\"'\n\n    escape = _escape_mecard\n    data = 'WIFI:'\n    if security:\n        data += 'T:{0};'.format(security.upper() if security != 'nopass' else security)\n    data += 'S:{1}{0}{1};'.format(escape(ssid), quotation_mark(ssid))\n    if password:\n        data += 'P:{1}{0}{1};'.format(escape(password), quotation_mark(password))\n    data += 'H:true;' if hidden else ';'\n    return data", "response": "Creates the WIFI configuration string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a WIFI configuration QR Code.", "response": "def make_wifi(ssid, password, security, hidden=False):\n    \"\"\"\\\n    Creates a WIFI configuration QR Code.\n\n    :param str ssid: The SSID of the network.\n    :param str|None password: The password.\n    :param str|None security: Authentication type; the value should\n            be \"WEP\" or \"WPA\". Set to ``None`` to omit the value.\n            \"nopass\" is equivalent to setting the value to ``None`` but in\n            the former case, the value is not omitted.\n    :param bool hidden: Indicates if the network is hidden (default: ``False``)\n    :rtype: segno.QRCode\n    \"\"\"\n    return segno.make_qr(make_wifi_data(ssid, password, security, hidden))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_mecard_data(name, reading=None, email=None, phone=None, videophone=None,\n                     memo=None, nickname=None, birthday=None, url=None,\n                     pobox=None, roomno=None, houseno=None, city=None,\n                     prefecture=None, zipcode=None, country=None):\n    \"\"\"\\\n    Creates a string encoding the contact information as MeCard.\n\n    :param str name: Name. If it contains a comma, the first part\n            is treated as lastname and the second part is treated as forename.\n    :param str|None reading: Designates a text string to be set as the\n            kana name in the phonebook\n    :param str|iterable email: E-mail address. Multiple values are\n            allowed.\n    :param str|iterable phone: Phone number. Multiple values are\n            allowed.\n    :param str|iterable videophone: Phone number for video calls.\n            Multiple values are allowed.\n    :param str memo: A notice for the contact.\n    :param str nickname: Nickname.\n    :param (str|int|date) birthday: Birthday. If a string is provided,\n            it should encode the date as YYYYMMDD value.\n    :param str|iterable url: Homepage. Multiple values are allowed.\n    :param str|None pobox: P.O. box (address information).\n    :param str|None roomno: Room number (address information).\n    :param str|None houseno: House number (address information).\n    :param str|None city: City (address information).\n    :param str|None prefecture: Prefecture (address information).\n    :param str|None zipcode: Zip code (address information).\n    :param str|None country: Country (address information).\n    :rtype: str\n    \"\"\"\n    def make_multifield(name, val):\n        if val is None:\n            return ()\n        if isinstance(val, str_type):\n            val = (val,)\n        return ['{0}:{1};'.format(name, escape(i)) for i in val]\n\n    escape = _escape_mecard\n    data = ['MECARD:N:{0};'.format(escape(name))]\n    if reading:\n        data.append('SOUND:{0};'.format(escape(reading)))\n    data.extend(make_multifield('TEL', phone))\n    data.extend(make_multifield('TELAV', videophone))\n    data.extend(make_multifield('EMAIL', email))\n    if nickname:\n        data.append('NICKNAME:{0};'.format(escape(nickname)))\n    if birthday:\n        try:\n            birthday = birthday.strftime('%Y%m%d')\n        except AttributeError:\n            pass\n        data.append('BDAY:{0};'.format(birthday))\n    data.extend(make_multifield('URL', url))\n    adr_properties = (pobox, roomno, houseno, city, prefecture, zipcode, country)\n    if any(adr_properties):\n        adr_data = [escape(i or '') for i in adr_properties]\n        data.append('ADR:{0},{1},{2},{3},{4},{5},{6};'.format(*adr_data))\n    if memo:\n        data.append('MEMO:{0};'.format(escape(memo)))\n    data.append(';')\n    return ''.join(data)", "response": "\\ Creates a string encoding the contact information as MeCard."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_vcard_data(name, displayname, email=None, phone=None, fax=None,\n                    videophone=None, memo=None, nickname=None, birthday=None,\n                    url=None, pobox=None, street=None, city=None, region=None,\n                    zipcode=None, country=None, org=None, lat=None, lng=None,\n                    source=None, rev=None, title=None, photo_uri=None):\n    \"\"\"\\\n    Creates a string encoding the contact information as vCard 3.0.\n\n    Only a subset of available vCard properties is supported.\n\n    :param str name: The name. If it contains a semicolon, , the first part\n            is treated as lastname and the second part is treated as forename.\n    :param str displayname: Common name.\n    :param str|iterable email: E-mail address. Multiple values are allowed.\n    :param str|iterable phone: Phone number. Multiple values are allowed.\n    :param str|iterable fax: Fax number. Multiple values are allowed.\n    :param str|iterable videophone: Phone number for video calls.\n            Multiple values are allowed.\n    :param str memo: A notice for the contact.\n    :param str nickname: Nickname.\n    :param str|date birthday: Birthday. If a string is provided,\n            it should encode the date as YYYY-MM-DD value.\n    :param str|iterable url: Homepage. Multiple values are allowed.\n    :param str|None pobox: P.O. box (address information).\n    :param str|None street: Street address.\n    :param str|None city: City (address information).\n    :param str|None region: Region (address information).\n    :param str|None zipcode: Zip code (address information).\n    :param str|None country: Country (address information).\n    :param str org: Company / organization name.\n    :param float lat: Latitude.\n    :param float lng: Longitude.\n    :param str source: URL where to obtain the vCard.\n    :param str|date rev: Revision of the vCard / last modification date.\n    :param str|iterable|None title: Job Title. Multiple values are allowed.\n    :param str|iterable|None photo_uri: Photo URI. Multiple values are allowed.\n    :rtype: str\n    \"\"\"\n    def make_multifield(name, val):\n        if val is None:\n            return ()\n        if isinstance(val, str_type):\n            val = (val,)\n        return ['{0}:{1}'.format(name, escape(i)) for i in val]\n\n    escape = _escape_vcard\n    data = ['BEGIN:VCARD', 'VERSION:3.0',\n            'N:{0}'.format(name),\n            'FN:{0}'.format(escape(displayname))]\n    if org:\n        data.append('ORG:{0}'.format(escape(org)))\n    data.extend(make_multifield('EMAIL', email))\n    data.extend(make_multifield('TEL', phone))\n    data.extend(make_multifield('TEL;TYPE=FAX', fax))\n    data.extend(make_multifield('TEL;TYPE=VIDEO', videophone))\n    data.extend(make_multifield('URL', url))\n    data.extend(make_multifield('TITLE', title))\n    data.extend(make_multifield('PHOTO;VALUE=uri', photo_uri))\n    if nickname:\n        data.append('NICKNAME:{0}'.format(escape(nickname)))\n    adr_properties = (pobox, street, city, region, zipcode, country)\n    if any(adr_properties):\n        adr_data = [escape(i or '') for i in adr_properties]\n        data.append('ADR:{0};;{1};{2};{3};{4};{5}'.format(*adr_data))\n    if birthday:\n        try:\n            birthday = birthday.strftime('%Y-%m-%d')\n        except AttributeError:\n            pass\n        if not _looks_like_datetime(birthday):\n            raise ValueError('\"birthday\" does not seem to be a valid date or date/time representation')\n        data.append('BDAY:{0};'.format(birthday))\n    if lat or lng and (not(all((lat, lng)))):\n        raise ValueError('Incomplete geo information, please specify latitude and longitude.')\n    if lat and lng:\n        data.append('GEO:{0};{1}'.format(lat, lng))\n    if source:\n        data.append('SOURCE:{0}'.format(escape(url)))\n    if memo:\n        data.append('NOTE:{0}'.format(escape(memo)))\n    if rev:\n        if not _looks_like_datetime(rev):\n            raise ValueError('\"rev\" does not seem to be a valid date or date/time representation')\n        data.append('REV:{0}'.format(rev))\n    data.append('END:VCARD')\n    data.append('')\n    return '\\r\\n'.join(data)", "response": "Creates a vCard data string from the contact information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_vcard(name, displayname, email=None, phone=None, fax=None,\n               videophone=None, memo=None, nickname=None, birthday=None,\n               url=None, pobox=None, street=None, city=None, region=None,\n               zipcode=None, country=None, org=None, lat=None, lng=None,\n               source=None, rev=None, title=None):\n    \"\"\"\\\n    Creates a QR Code which encodes a `vCard <https://en.wikipedia.org/wiki/VCard>`_\n    version 3.0.\n\n    Only a subset of available vCard properties is supported.\n\n    :param str name: The name. If it contains a semicolon, , the first part\n            is treated as lastname and the second part is treated as forename.\n    :param str displayname: Common name.\n    :param str|iterable email: E-mail address. Multiple values are allowed.\n    :param str|iterable phone: Phone number. Multiple values are allowed.\n    :param str|iterable fax: Fax number. Multiple values are allowed.\n    :param str|iterable videophone: Phone number for video calls.\n            Multiple values are allowed.\n    :param str memo: A notice for the contact.\n    :param str nickname: Nickname.\n    :param str|date birthday: Birthday. If a string is provided,\n            it should encode the date as YYYY-MM-DD value.\n    :param str|iterable url: Homepage. Multiple values are allowed.\n    :param str|None pobox: P.O. box (address information).\n    :param str|None street: Street address.\n    :param str|None city: City (address information).\n    :param str|None region: Region (address information).\n    :param str|None zipcode: Zip code (address information).\n    :param str|None country: Country (address information).\n    :param str org: Company / organization name.\n    :param float lat: Latitude.\n    :param float lng: Longitude.\n    :param str source: URL where to obtain the vCard.\n    :param str|date rev: Revision of the vCard / last modification date.\n    :param str|iterable|None title: Job Title. Multiple values are allowed.\n\n    :rtype: segno.QRCode\n    \"\"\"\n    return segno.make_qr(make_vcard_data(name, displayname, email=email,\n                                         phone=phone, fax=fax,\n                                         videophone=videophone, memo=memo,\n                                         nickname=nickname, birthday=birthday,\n                                         url=url, pobox=pobox, street=street,\n                                         city=city, region=region,\n                                         zipcode=zipcode, country=country,\n                                         org=org, lat=lat, lng=lng,\n                                         source=source, rev=rev, title=title))", "response": "Creates a QR Code which encodes a vCard."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_geo_data(lat, lng):\n    def float_to_str(f):\n        return '{0:.8f}'.format(f).rstrip('0')\n\n    return 'geo:{0},{1}'.format(float_to_str(lat), float_to_str(lng))", "response": "\\ Creates a geo location URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_make_email_data(to, cc=None, bcc=None, subject=None, body=None):\n    def multi(val):\n        if not val:\n            return ()\n        if isinstance(val, str_type):\n            return (val,)\n        return tuple(val)\n\n    delim = '?'\n    data = ['mailto:']\n    if not to:\n        raise ValueError('\"to\" must not be empty or None')\n    data.append(','.join(multi(to)))\n    for key, val in (('cc', cc), ('bcc', bcc)):\n        vals = multi(val)\n        if vals:\n            data.append('{0}{1}={2}'.format(delim, key, ','.join(vals)))\n            delim = '&'\n    for key, val in (('subject', subject), ('body', body)):\n        if val is not None:\n            data.append('{0}{1}={2}'.format(delim, key, quote(val.encode('utf-8'))))\n        delim = '&'\n    return ''.join(data)", "response": "\\ Creates an e - mail message with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an e - mail address from a list of carbon copies and a subject and body.", "response": "def make_email(to, cc=None, bcc=None, subject=None, body=None):\n    \"\"\"\\\n    Encodes either a simple e-mail address or a complete message with\n    (blind) carbon copies and a subject and a body.\n\n    :param str|iterable to: The email address (recipient). Multiple\n            values are allowed.\n    :param str|iterable|None cc: The carbon copy recipient. Multiple\n            values are allowed.\n    :param str|iterable|None bcc: The blind carbon copy recipient.\n            Multiple values are allowed.\n    :param str|None subject: The subject.\n    :param str|None body: The message body.\n    \"\"\"\n    return segno.make_qr(make_make_email_data(to=to, cc=cc, bcc=bcc,\n                                              subject=subject, body=body))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode the given content into a new QR Code.", "response": "def encode(content, error=None, version=None, mode=None, mask=None,\n           encoding=None, eci=False, micro=None, boost_error=True):\n    \"\"\"\\\n    Creates a (Micro) QR Code.\n\n    See :py:func:`segno.make` for a detailed description of the parameters.\n\n    Contrary to ``make`` this function returns a named tuple:\n    ``(matrix, version, error, mask, segments)``\n\n    Note that ``version`` is always an integer referring\n    to the values of the :py:mod:`segno.consts` constants. ``error`` is ``None``\n    iff a M1 QR Code was generated, otherwise it is always an integer.\n\n    :rtype: namedtuple\n    \"\"\"\n    version = normalize_version(version)\n    if not micro and micro is not None and version in consts.MICRO_VERSIONS:\n        raise VersionError('A Micro QR Code version (\"{0}\") is provided but '\n                           'parameter \"micro\" is False'\n                           .format(get_version_name(version)))\n    if micro and version is not None and version not in consts.MICRO_VERSIONS:\n        raise VersionError('Illegal Micro QR Code version \"{0}\"'\n                           .format(get_version_name(version)))\n    error = normalize_errorlevel(error, accept_none=True)\n    mode = normalize_mode(mode)\n    if mode is not None and version is not None \\\n            and not is_mode_supported(mode, version):\n        raise ModeError('Mode \"{0}\" is not available in version \"{1}\"'\n                        .format(get_mode_name(mode), get_version_name(version)))\n    if error == consts.ERROR_LEVEL_H and (micro or version in consts.MICRO_VERSIONS):\n        raise ErrorLevelError('Error correction level \"H\" is not available for '\n                              'Micro QR Codes')\n    if eci and (micro or version in consts.MICRO_VERSIONS):\n        raise VersionError('The ECI mode is not available for Micro QR Codes')\n    segments = prepare_data(content, mode, encoding, version)\n    guessed_version = find_version(segments, error, eci=eci, micro=micro)\n    if version is None:\n        version = guessed_version\n    elif guessed_version > version:\n        raise DataOverflowError('The provided data does not fit into version '\n                                '\"{0}\". Proposal: version {1}'\n                                .format(get_version_name(version),\n                                        get_version_name(guessed_version)))\n    if error is None and version != consts.VERSION_M1:\n        error = consts.ERROR_LEVEL_L\n    is_micro = version < 1\n    mask = normalize_mask(mask, is_micro)\n    return _encode(segments, error, version, mask, eci, boost_error)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode a string into a sequence of named tuples.", "response": "def encode_sequence(content, error=None, version=None, mode=None,\n                    mask=None, encoding=None, eci=False, boost_error=True,\n                    symbol_count=None):\n    \"\"\"\\\n    EXPERIMENTAL: Creates a sequence of QR Codes in Structured Append mode.\n\n    :return: Iterable of named tuples, see :py:func:`encode` for details.\n    \"\"\"\n    def one_item_segments(chunk, mode):\n        \"\"\"\\\n        Creates a Segments sequence with one item.\n        \"\"\"\n        segs = Segments()\n        segs.add_segment(make_segment(chunk, mode=mode, encoding=encoding))\n        return segs\n\n    def divide_into_chunks(data, num):\n        k, m = divmod(len(data), num)\n        return [data[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(num)]\n\n    def calc_qrcode_bit_length(char_count, ver_range, mode, encoding=None,\n                               is_eci=False, is_sa=False):\n        overhead = 4  # Mode indicator for QR Codes, only\n        # Number of bits in character count indicator\n        overhead += consts.CHAR_COUNT_INDICATOR_LENGTH[mode][ver_range]\n        if is_eci and mode == consts.MODE_BYTE and encoding != consts.DEFAULT_BYTE_ENCODING:\n            overhead += 4  # ECI indicator\n            overhead += 8  # ECI assignment no\n        if is_sa:\n            # 4 bit for mode, 4 bit for the position, 4 bit for total number of symbols\n            # 8 bit for parity data\n            overhead += 5 * 4\n        bits = 0\n        if mode == consts.MODE_NUMERIC:\n            num, remainder = divmod(char_count, 3)\n            bits += num * 10 + (4 if remainder == 1 else 7)\n        elif mode == consts.MODE_ALPHANUMERIC:\n            num, remainder = divmod(char_count, 2)\n            bits += num * 11 + (6 if remainder else 0)\n        elif mode == consts.MODE_BYTE:\n            bits += char_count * 8\n        elif mode == consts.MODE_KANJI:\n            bits += char_count * 13\n        return overhead + bits\n\n    def number_of_symbols_by_version(content, version, error, mode):\n        \"\"\"\\\n        Returns the number of symbols for the provided version.\n        \"\"\"\n        length = len(content)\n        ver_range = version_range(version)\n        bit_length = calc_qrcode_bit_length(length, ver_range, mode, encoding,\n                                            is_eci=eci, is_sa=True)\n        capacity = consts.SYMBOL_CAPACITY[version][error]\n        # Initial result does not contain the overhead of SA mode for all QR Codes\n        cnt = int(math.ceil(bit_length / capacity))\n        # Overhead of SA mode for all QR Codes\n        bit_length += 5 * 4 * (cnt - 1) + (12 * (cnt - 1) if eci else 0)\n        return int(math.ceil(bit_length / capacity))\n\n    version = normalize_version(version)\n    if version is not None:\n        if version < 1:\n            raise VersionError('This function does not accept Micro QR Code versions. '\n                               'Provided: \"{0}\"'.format(get_version_name(version)))\n    elif symbol_count is None:\n        raise ValueError('Please provide a QR Code version or the symbol count')\n    if symbol_count is not None and not 1 <= symbol_count <= 16:\n        raise ValueError('The symbol count must be in range 1 .. 16')\n    error = normalize_errorlevel(error, accept_none=True)\n    if error is None:\n        error = consts.ERROR_LEVEL_L\n    mode = normalize_mode(mode)\n    mask = normalize_mask(mask, is_micro=False)\n    segments = prepare_data(content, mode, encoding, version)\n    guessed_version = None\n    if symbol_count is None:\n        try:\n            # Try to find a version which fits without using Structured Append\n            guessed_version = find_version(segments, error, eci=eci, micro=False)\n        except DataOverflowError:\n            # Data does fit into a usual QR Code but ignore the error silently,\n            # guessed_version is None\n            pass\n        if guessed_version and guessed_version <= (version or guessed_version):\n            # Return iterable of size 1\n            return [_encode(segments, error=error, version=(version or guessed_version),\n                            mask=mask, eci=eci, boost_error=boost_error)]\n    if len(segments.modes) > 1:\n        raise ValueError('This function cannot handle more than one mode (yet). Sorry.')\n    mode = segments.modes[0]  # CHANGE iff more than one mode is supported!\n    # Creating one QR code failed or max_no is not None\n    if mode == consts.MODE_NUMERIC:\n        content = str(content)\n    if symbol_count is not None and len(content) < symbol_count:\n        raise ValueError('The content is not long enough to be divided into {0} symbols'.format(symbol_count))\n    sa_parity_data = calc_structured_append_parity(content)\n    num_symbols = symbol_count or 16\n    if version is not None:\n        num_symbols = number_of_symbols_by_version(content, version, error, mode)\n    if num_symbols > 16:\n        raise DataOverflowError('The data does not fit into Structured Append version {0}'.format(version))\n    chunks = divide_into_chunks(content, num_symbols)\n    if symbol_count is not None:\n        segments = one_item_segments(max(chunks, key=len), mode)\n        version = find_version(segments, error, eci=eci, micro=False, is_sa=True)\n    sa_info = partial(_StructuredAppendInfo, total=len(chunks) - 1,\n                      parity=sa_parity_data)\n    return [_encode(one_item_segments(chunk, mode), error=error, version=version,\n                    mask=mask, eci=eci, boost_error=boost_error,\n                    sa_info=sa_info(i)) for i, chunk in enumerate(chunks)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes a sequence of segments into a new ISO - IEC 18004 codeword sequence.", "response": "def _encode(segments, error, version, mask, eci, boost_error, sa_info=None):\n    \"\"\"\\\n    Creates a (Micro) QR Code.\n\n    NOTE: This function does not check if the input is valid and does not belong\n    to the public API.\n    \"\"\"\n    is_micro = version < 1\n    sa_mode = sa_info is not None\n    buff = Buffer()\n    ver = version\n    ver_range = version\n    if not is_micro:\n        ver = None\n        ver_range = version_range(version)\n    if boost_error:\n        error = boost_error_level(version, error, segments, eci, is_sa=sa_mode)\n    if sa_mode:\n        # ISO/IEC 18004:2015(E) -- 8 Structured Append (page 59)\n        for i in sa_info[:3]:\n            buff.append_bits(i, 4)\n        buff.append_bits(sa_info.parity, 8)\n    # ISO/IEC 18004:2015(E) -- 7.4 Data encoding (page 22)\n    for segment in segments:\n        write_segment(buff, segment, ver, ver_range, eci)\n    capacity = consts.SYMBOL_CAPACITY[version][error]\n    # ISO/IEC 18004:2015(E) -- 7.4.9 Terminator (page 32)\n    write_terminator(buff, capacity, ver, len(buff))\n    #  ISO/IEC 18004:2015(E) -- 7.4.10 Bit stream to codeword conversion (page 34)\n    write_padding_bits(buff, version, len(buff))\n    # ISO/IEC 18004:2015(E) -- 7.4.10 Bit stream to codeword conversion (page 34)\n    write_pad_codewords(buff, version, capacity, len(buff))\n    # ISO/IEC 18004:2015(E) -- 7.6 Constructing the final message codeword sequence (page 45)\n    buff = make_final_message(version, error, buff.toints())\n    # Matrix with timing pattern and reserved format / version regions\n    matrix = make_matrix(version)\n    # ISO/IEC 18004:2015 -- 6.3.3 Finder pattern (page 16)\n    add_finder_patterns(matrix, is_micro)\n    # ISO/IEC 18004:2015 -- 6.3.6 Alignment patterns (page 17)\n    add_alignment_patterns(matrix, version)\n    # ISO/IEC 18004:2015 -- 7.7 Codeword placement in matrix (page 46)\n    add_codewords(matrix, buff, version)\n    # ISO/IEC 18004:2015(E) -- 7.8.2 Data mask patterns (page 50)\n    # ISO/IEC 18004:2015(E) -- 7.8.3 Evaluation of data masking results (page 53)\n    mask = find_and_apply_best_mask(matrix, version, is_micro, mask)\n    # ISO/IEC 18004:2015(E) -- 7.9 Format information (page 55)\n    add_format_info(matrix, version, error, mask)\n    # ISO/IEC 18004:2015(E) -- 7.10 Version information (page 58)\n    add_version_info(matrix, version)\n    return Code(matrix, version, error, mask, segments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nboosts the error level of the given error level.", "response": "def boost_error_level(version, error, segments, eci, is_sa=False):\n    \"\"\"\\\n    Increases the error level if possible.\n\n    :param int version: Version constant.\n    :param int|None error: Error level constant or ``None``\n    :param Segments segments: Instance of :py:class:`Segments`\n    :param bool eci: Indicates if ECI designator should be written.\n    :param bool is_sa: Indicates if Structured Append mode ist used.\n    \"\"\"\n    if error not in (consts.ERROR_LEVEL_H, None) and len(segments) == 1:\n        levels = [consts.ERROR_LEVEL_L, consts.ERROR_LEVEL_M,\n                  consts.ERROR_LEVEL_Q, consts.ERROR_LEVEL_H]\n        if version < 1:\n            levels.pop()  # H isn't support by Micro QR Codes\n            if version < consts.VERSION_M4:\n                levels.pop()  # Error level Q isn't supported by M2 and M3\n        data_length = segments.bit_length_with_overhead(version, eci, is_sa=is_sa)\n        for level in levels[levels.index(error)+1:]:\n            try:\n                found = consts.SYMBOL_CAPACITY[version][level] >= data_length\n            except KeyError:\n                break\n            if found:\n                error = level\n    return error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_segment(buff, segment, ver, ver_range, eci=False):\n    mode = segment.mode\n    append_bits = buff.append_bits\n    # Write ECI header if requested\n    if eci and mode == consts.MODE_BYTE \\\n            and segment.encoding != consts.DEFAULT_BYTE_ENCODING:\n        append_bits(consts.MODE_ECI, 4)\n        append_bits(get_eci_assignment_number(segment.encoding), 8)\n    if ver is None:  # QR Code\n        append_bits(mode, 4)\n    elif ver > consts.VERSION_M1:  # Micro QR Code (M1 has no mode indicator)\n        append_bits(consts.MODE_TO_MICRO_MODE_MAPPING[mode], ver + 3)\n    # Character count indicator\n    append_bits(segment.char_count,\n                consts.CHAR_COUNT_INDICATOR_LENGTH[mode][ver_range])\n    buff.extend(segment.bits)", "response": "Writes a _Segment to the byte buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_padding_bits(buff, version, length):\n    # ISO/IEC 18004:2015(E) - 7.4.10 Bit stream to codeword conversion -- page 32\n    # [...]\n    # All codewords are 8 bits in length, except for the final data symbol\n    # character in Micro QR Code versions M1 and M3 symbols, which is 4 bits\n    # in length. If the bit stream length is such that it does not end at a\n    # codeword boundary, padding bits with binary value 0 shall be added after\n    # the final bit (least significant bit) of the data stream to extend it\n    # to the codeword boundary. [...]\n    if version not in (consts.VERSION_M1, consts.VERSION_M3):\n        buff.extend([0] * (8 - (length % 8)))", "response": "Writes padding bits to the data stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_pad_codewords(buff, version, capacity, length):\n    # ISO/IEC 18004:2015(E) -- 7.4.10 Bit stream to codeword conversion (page 32)\n    # The message bit stream shall then be extended to fill the data capacity\n    # of the symbol corresponding to the Version and Error Correction Level, as\n    # defined in Table 8, by adding the Pad Codewords 11101100 and 00010001\n    # alternately. For Micro QR Code versions M1 and M3 symbols, the final data\n    # codeword is 4 bits long. The Pad Codeword used in the final data symbol\n    # character position in Micro QR Code versions M1 and M3 symbols shall be\n    # represented as 0000.\n    write = buff.extend\n    if version in (consts.VERSION_M1, consts.VERSION_M3):\n        write([0] * (capacity - length))\n    else:\n        pad_codewords = ((1, 1, 1, 0, 1, 1, 0, 0), (0, 0, 0, 1, 0, 0, 0, 1))\n        for i in range(capacity // 8 - length // 8):\n            write(pad_codewords[i % 2])", "response": "Writes the pad codewords iff the data does not fill the capacity of the current QR Code symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_finder_patterns(matrix, is_micro):\n    add_finder_pattern(matrix, 0, 0)  # Upper left corner\n    if not is_micro:\n        add_finder_pattern(matrix, 0, -7)  # Upper right corner\n        add_finder_pattern(matrix, -7, 0)", "response": "Adds the finder patterns to the matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_finder_pattern(matrix, x, y):\n    fidx1, fidx2 = (0, 8) if y != 0 else (1, 9)\n    fyoff = 1 if x == 0 else 0\n    idx1, idx2 = (0, 8) if y > -1 else (y - 1, len(matrix))\n    x -= fyoff ^ 0x1\n    for i in range(8):\n        matrix[x + i][idx1:idx2] = _FINDER_PATTERN[fyoff + i][fidx1:fidx2]", "response": "Adds the finder pattern to the specified matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_timing_pattern(matrix, is_micro):\n    bit = 0x1\n    y, stop = (0, len(matrix)) if is_micro else (6, len(matrix) - 8)\n    for x in range(8, stop):\n        matrix[x][y] = bit\n        matrix[y][x] = bit\n        bit ^= 0x1", "response": "Adds the horizontal and vertical timinig pattern to the provided matrix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the codewords to the current codeword in the given matrix.", "response": "def add_codewords(matrix, codewords, version):\n    \"\"\"\\\n    Adds the codewords (data and error correction) to the provided matrix.\n\n    ISO/IEC 18004:2015(E) -- 7.7.3 Symbol character placement (page 46)\n\n    :param matrix: The matrix to add the codewords into.\n    :param codewords: Sequence of ints\n    \"\"\"\n    matrix_size = len(matrix)\n    is_micro = version < 1\n    # Necessary for M1 and M3: The algorithm would start at the upper right\n    # corner, see <https://github.com/heuer/segno/issues/36>\n    inc = 0 if version not in (consts.VERSION_M1, consts.VERSION_M3) else 2\n    idx = 0  # Pointer to the current codeword\n    # ISO/IEC 18004:2015(E) - page 48\n    # [...] An alternative method for placement in the symbol [...] is to regard\n    # the interleaved codeword sequence as a single bit stream, which is placed\n    # (starting with the most significant bit) in the two-module wide columns\n    # alternately upwards and downwards from the right to left of the symbol.\n    # [...]\n    for right in range(matrix_size - 1, 0, -2):\n        if not is_micro and right <= 6:\n            right -= 1\n        for vertical in range(matrix_size):\n            for z in range(2):\n                j = right - z\n                upwards = ((right + inc) & 2) == 0\n                if not is_micro:\n                    upwards ^= j < 6\n                i = (matrix_size - 1 - vertical) if upwards else vertical\n                if matrix[i][j] == 0x2 and idx < len(codewords):\n                    matrix[i][j] = codewords[idx]\n                    idx += 1\n    if idx != len(codewords):  # pragma: no cover\n        raise QRCodeError('Internal error: Adding codewords to matrix failed. '\n                          'Added {0} of {1} codewords'.format(idx, len(codewords)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing the final message codeword sequence.", "response": "def make_final_message(version, error, codewords):\n    \"\"\"\\\n    Constructs the final message (codewords incl. error correction).\n\n    ISO/IEC 18004:2015(E) -- 7.6 Constructing the final message codeword sequence (page 45)\n\n    :param int version: (Micro) QR Code version constant.\n    :param int error: Error level constant.\n    :param codewords: An iterable sequence of codewords (ints)\n    :return: Byte buffer representing the final message.\n    \"\"\"\n    ec_infos = consts.ECC[version][error]\n    last_cw_is_four = version in (consts.VERSION_M1, consts.VERSION_M3)\n    data_blocks, error_blocks = make_blocks(ec_infos, codewords)\n    if last_cw_is_four:\n        # All codewords are 8 bit by default, M1 and M3 symbols use 4 bits\n        # to represent the last last codeword\n        # datablocks[0] is save since Micro QR Codes use just one datablock and\n        # one error block\n        data_blocks[0][-1] >>= 4\n    buff = Buffer()\n    append_int = partial(buff.append_bits, length=8)\n    # Write codewords\n    for i in range(max(info.num_data for info in ec_infos)):\n        for block in data_blocks:\n            if i >= len(block):\n                continue\n            if last_cw_is_four and i + 1 == len(block):\n                buff.append_bits(block[i], 4)\n            else:\n                append_int(block[i])\n    # Write error codewords\n    for i in range(max(info.num_total - info.num_data for info in ec_infos)):\n        for block in error_blocks:\n            if i >= len(block):\n                continue\n            append_int(block[i])\n    # ISO/IEC 18004:2015(E) -- 7.6 Constructing the final message codeword sequence\n    # [...] In certain QR Code versions, however, where the number of modules\n    # available for data and error correction codewords is not an exact multiple\n    # of 8, there may be a need for 3, 4 or 7 Remainder Bits to be appended to\n    # the final message bit stream in order to fill exactly the number of\n    # modules in the encoding region\n    remainder = 0  # Calculation: Number of Data modules - number of bits\n    if version in (2, 3, 4, 5, 6):\n        remainder = 7\n    elif version in (14, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 33, 34):\n        remainder = 3\n    elif version in (21, 22, 23, 24, 25, 26, 27):\n        remainder = 4\n    buff.extend(b'\\0' * remainder)\n    return buff"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_blocks(ec_infos, codewords):\n    data_blocks, error_blocks = [], []\n    offset = 0\n    for ec_info in ec_infos:\n        for i in range(ec_info.num_blocks):\n            block = codewords[offset:offset + ec_info.num_data]\n            data_blocks.append(block)\n            error_blocks.append(make_error_block(ec_info, block))\n            offset += ec_info.num_data\n    return data_blocks, error_blocks", "response": "\\ Returns the data and error blocks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evaluate_mask(matrix, matrix_size):\n    return score_n1(matrix, matrix_size) + score_n2(matrix, matrix_size) \\\n           + score_n3(matrix, matrix_size) + score_n4(matrix, matrix_size)", "response": "Evaluates the provided matrix of a QR code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the penalty score for a single element of the matrix.", "response": "def score_n1(matrix, matrix_size):\n    \"\"\"\\\n    Implements the penalty score feature 1.\n\n    ISO/IEC 18004:2015(E) -- 7.8.3 Evaluation of data masking results - Table 11 (page 54)\n\n    ============================================   ========================    ======\n    Feature                                        Evaluation condition        Points\n    ============================================   ========================    ======\n    Adjacent modules in row/column in same color   No. of modules = (5 + i)    N1 + i\n    ============================================   ========================    ======\n\n    N1 = 3\n\n    :param matrix: The matrix to evaluate\n    :param matrix_size: The width (or height) of the matrix.\n    :return int: The penalty score (feature 1) of the matrix.\n    \"\"\"\n    score = 0\n    for i in range(matrix_size):\n        prev_bit_row, prev_bit_col = -1, -1\n        row_counter, col_counter = 0, 0\n        for j in range(matrix_size):\n            # Row-wise\n            bit = matrix[i][j]\n            if bit == prev_bit_row:\n                row_counter += 1\n            else:\n                if row_counter >= 5:\n                    score += row_counter - 2  # N1 == 3\n                row_counter = 1\n                prev_bit_row = bit\n            # Col-wise\n            bit = matrix[j][i]\n            if bit == prev_bit_col:\n                col_counter += 1\n            else:\n                if col_counter >= 5:\n                    score += col_counter - 2  # N1 == 3\n                col_counter = 1\n                prev_bit_col = bit\n        if row_counter >= 5:\n            score += row_counter - 2  # N1 == 3\n        if col_counter >= 5:\n            score += col_counter - 2  # N1 == 3\n    return score"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef score_n2(matrix, matrix_size):\n    score = 0\n    for i in range(matrix_size - 1):\n        for j in range(matrix_size - 1):\n            bit = matrix[i][j]\n            if bit == matrix[i][j + 1] and bit == matrix[i + 1][j] \\\n                and bit == matrix[i + 1][j + 1]:\n                score += 1\n    return score * 3", "response": "Computes the penalty score for a single N2 block."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef score_n3(matrix, matrix_size):\n    def is_match(seq, start, end):\n        start = max(start, 0)\n        end = min(end, matrix_size)\n        for i in range(start, end):\n            if seq[i] == 0x1:\n                return False\n        return True\n\n    def find_occurrences(seq):\n        count = 0\n        idx = seq.find(_N3_PATTERN)\n        while idx != -1:\n            offset = idx + 7\n            if is_match(seq, idx - 4, idx) or is_match(seq, offset, offset + 4):\n                count += 1\n            else:\n                # Found no / not enough light patterns, start at next possible\n                # match:\n                #                   v\n                # dark light dark dark dark light dark\n                #                   ^\n                offset = idx + 4\n            idx = seq.find(_N3_PATTERN, offset)\n        return count\n\n    score = 0\n    for i in range(matrix_size):\n        score += find_occurrences(matrix[i])\n        score += find_occurrences(bytearray([matrix[y][i] for y in range(matrix_size)]))\n    return score * 40", "response": "Computes the penalty score for a pattern in the N3 matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef score_n4(matrix, matrix_size):\n    dark_modules = sum(map(sum, matrix))\n    total_modules = matrix_size ** 2\n    k = int(abs(dark_modules * 2 - total_modules) * 10 // total_modules)\n    return 10 * k", "response": "Determines the penalty score of the matrix with the N4 feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate_micro_mask(matrix, matrix_size):\n    sum1 = sum(matrix[i][-1] == 0x1 for i in range(1, matrix_size))\n    sum2 = sum(matrix[-1][i] == 0x1 for i in range(1, matrix_size))\n    return sum1 * 16 + sum2 if sum1 <= sum2 else sum2 * 16 + sum1", "response": "Evaluates the provided matrix of a Micro QR code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the format information for the provided error level and mask pattern.", "response": "def calc_format_info(version, error, mask_pattern):\n    \"\"\"\\\n    Returns the format information for the provided error level and mask patttern.\n\n    ISO/IEC 18004:2015(E) -- 7.9 Format information (page 55)\n    ISO/IEC 18004:2015(E) -- Table C.1 \u2014 Valid format information bit sequences (page 80)\n\n    :param int version: Version constant\n    :param int error: Error level constant.\n    :param int mask_pattern: Mask pattern number.\n    \"\"\"\n    fmt = mask_pattern\n    if version > 0:\n        if error == consts.ERROR_LEVEL_L:\n            fmt += 0x08\n        elif error == consts.ERROR_LEVEL_H:\n            fmt += 0x10\n        elif error == consts.ERROR_LEVEL_Q:\n            fmt += 0x18\n        format_info = consts.FORMAT_INFO[fmt]\n    else:\n        fmt += consts.ERROR_LEVEL_TO_MICRO_MAPPING[version][error] << 2\n        format_info = consts.FORMAT_INFO_MICRO[fmt]\n    return format_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_format_info(matrix, version, error, mask_pattern):\n    # 14: most significant bit\n    #  0: least significant bit\n    #\n    # QR Code format info:                                          Micro QR format info\n    # col 0                col 7              col matrix[-1]      col 1\n    #                        0                       |               |                [ ]\n    #                        1                                                         0\n    #                        2                                                         1\n    #                        3                                                         2\n    #                        4                                                         3\n    #                        5                                                         4\n    #                       [ ]                                                        5\n    #                        6                                                         6\n    # 14 13 12 11 10 9 [ ] 8 7    ...  7 6 5 4 3 2 1 0              14 13 12 11 10 9 8 7\n    #\n    # ...\n    #                       [ ] (dark module)\n    #                        8\n    #                        9\n    #                       10\n    #                       11\n    #                       12\n    #                       13\n    #                       14\n    is_micro = version < 1\n    format_info = calc_format_info(version, error, mask_pattern)\n    offset = int(is_micro)\n    for i in range(8):\n        bit = (format_info >> i) & 0x01\n        if i == 6 and not is_micro:  # Timing pattern\n            offset += 1\n        # vertical row, upper left corner\n        matrix[i + offset][8] = bit\n        if not is_micro:\n            # horizontal row, upper right corner\n            matrix[8][-1 - i] = bit\n    offset = int(is_micro)\n    for i in range(8):\n        bit = (format_info >> (14 - i)) & 0x01\n        if i == 6 and not is_micro:  # Timing pattern\n            offset = 1\n        # horizontal row, upper left corner\n        matrix[8][i + offset] = bit\n        if not is_micro:\n            # vertical row, bottom left corner\n            matrix[-1 - i][8] = bit\n    if not is_micro:\n        # Dark module\n        matrix[-8][8] = 0x1", "response": "Adds the format information to the provided matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the version information to the matrix.", "response": "def add_version_info(matrix, version):\n    \"\"\"\\\n    Adds the version information to the matrix, for versions < 7 this is a no-op.\n\n    ISO/IEC 18004:2015(E) -- 7.10 Version information (page 58)\n    \"\"\"\n    #\n    # module  0 = least significant bit\n    # module 17 = most significant bit\n    #\n    # Figure 27 \u2014 Version information positioning (page 58)\n    #\n    # Lower left                    Upper right\n    # ----------                    -----------\n    #  0  3  6  9 12 15               0  1  2\n    #  1  4  7 10 13 16               3  4  5\n    #  2  5  8 11 14 17               6  7  8\n    #                                 9 10 11\n    #                                12 13 14\n    #                                15 16 17\n    #\n    if version < 7:\n        return\n    version_info = consts.VERSION_INFO[version - 7]\n    for i in range(6):\n        bit1 = (version_info >> (i * 3)) & 0x01\n        bit2 = (version_info >> ((i * 3) + 1)) & 0x01\n        bit3 = (version_info >> ((i * 3) + 2)) & 0x01\n        # Lower left\n        matrix[-11][i] = bit1\n        matrix[-10][i] = bit2\n        matrix[-9][i] = bit3\n        # Upper right\n        matrix[i][-11] = bit1\n        matrix[i][-10] = bit2\n        matrix[i][-9] = bit3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_to_bytes(data, encoding):\n    if isinstance(data, bytes):\n        return data, len(data), encoding or consts.DEFAULT_BYTE_ENCODING\n    data = str(data)\n    if encoding is not None:\n        # Use the provided encoding; could raise an exception by intention\n        data = data.encode(encoding)\n    else:\n        try:\n            # Try to use the default byte encoding\n            encoding = consts.DEFAULT_BYTE_ENCODING\n            data = data.encode(encoding)\n        except UnicodeError:\n            try:\n                # Try Kanji / Shift_JIS\n                encoding = consts.KANJI_ENCODING\n                data = data.encode(encoding)\n            except UnicodeError:\n                # Use UTF-8\n                encoding = 'utf-8'\n                data = data.encode(encoding)\n    return data, len(data), encoding", "response": "\\ Converts the provided data into bytes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_segment(data, mode, encoding=None):\n    segment_data, segment_length, segment_encoding = data_to_bytes(data, encoding)\n    segment_mode = mode\n    # If the user prefers BYTE, use BYTE and do not try to find a better mode\n    # Necessary since BYTE < KANJI and find_mode may return KANJI as (more)\n    # appropriate mode and the encoder throws an exception\n    # if \"user provided mode\" < \"found mode\"\n    guessed_mode = find_mode(segment_data) if segment_mode != consts.MODE_BYTE else consts.MODE_BYTE\n    if segment_mode is not None:\n        # Check if user provided mode is applicable for the given segment_data\n        if segment_mode < guessed_mode:\n            raise ModeError('The provided mode \"{0}\" is not applicable '\n                            'for {1}. Proposal: {2}'\n                            .format(get_mode_name(segment_mode),\n                                    repr(segment_data),\n                                    get_mode_name(guessed_mode)))\n    else:\n        segment_mode = guessed_mode\n    if segment_mode != consts.MODE_BYTE:\n        segment_encoding = None\n    buff = Buffer()\n    append_bits = buff.append_bits\n    if segment_mode == consts.MODE_NUMERIC:\n        # ISO/IEC 18004:2015(E) -- 7.4.3 Numeric mode (page 25)\n        # The input data string is divided into groups of three digits, and\n        # each group is converted to its 10-bit binary equivalent. If the number\n        # of input digits is not an exact multiple of three, the final one or\n        # two digits are converted to 4 or 7 bits respectively.\n        for i in range(0, segment_length, 3):\n            chunk = segment_data[i:i + 3]\n            append_bits(int(chunk), len(chunk) * 3 + 1)\n    elif segment_mode == consts.MODE_ALPHANUMERIC:\n        # ISO/IEC 18004:2015(E) -- 7.4.4 Alphanumeric mode (page 26)\n        to_byte = consts.ALPHANUMERIC_CHARS.find\n        for i in range(0, segment_length, 2):\n            chunk = segment_data[i:i + 2]\n            # Input data characters are divided into groups of two characters\n            # which are encoded as 11-bit binary codes. The character value of\n            # the first character is multiplied by 45 and the character value\n            # of the second digit is added to the product. The sum is then\n            # converted to an 11-bit binary number.\n            if len(chunk) > 1:\n                append_bits(to_byte(chunk[0]) * 45 + to_byte(chunk[1]), 11)\n            else:\n                # If the number of input data characters is not a multiple of\n                # two, the character value of the final character is encoded\n                # as a 6-bit binary number.\n                append_bits(to_byte(chunk), 6)\n    elif segment_mode == consts.MODE_BYTE:\n        # ISO/IEC 18004:2015(E) -- 7.4.5 Byte mode (page 27)\n        if _PY2:\n            segment_data = (ord(b) for b in segment_data)\n        for b in segment_data:\n            append_bits(b, 8)\n    else:\n        # ISO/IEC 18004:2015(E) -- 7.4.6 Kanji mode (page 29)\n        if _PY2:\n            segment_data = [ord(b) for b in segment_data]\n        # Note: len(segment.data)! segment.data_length = len(segment.data) / 2!!\n        for i in range(0, len(segment_data), 2):\n            code = (segment_data[i] << 8) | segment_data[i + 1]\n            if 0x8140 <= code <= 0x9ffc:\n                # For characters with Shift JIS values from 8140HEX to 9FFCHEX:\n                # a) Subtract 8140HEX from Shift JIS value;\n                diff = code - 0x8140\n            elif 0xe040 <= code <= 0xebbf:\n                # For characters with Shift JIS values from E040HEX to EBBFHEX:\n                # a) Subtract C140HEX from Shift JIS value;\n                diff = code - 0xc140\n            else:  # pragma: no cover\n                raise QRCodeError('Invalid Kanji bytes: {0}'.format(code))\n            # b) Multiply most significant byte of result by C0HEX;\n            # c) Add least significant byte to product from b);\n            # d) Convert result to a 13-bit binary string.\n            append_bits(((diff >> 8) * 0xc0) + (diff & 0xff), 13)\n    return _Segment(buff.getbits(), segment_length, segment_mode, segment_encoding)", "response": "Creates a segment from the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_matrix(version, reserve_regions=True, add_timing=True):\n    size = calc_matrix_size(version)\n    row = [0x2] * size\n    matrix = tuple([bytearray(row) for i in range(size)])\n    if reserve_regions:\n        if version > 6:\n            # Reserve version pattern areas\n            for i in range(6):\n                # Upper right\n                matrix[i][-11] = 0x0\n                matrix[i][-10] = 0x0\n                matrix[i][-9] = 0x0\n                # Lower left\n                matrix[-11][i] = 0x0\n                matrix[-10][i] = 0x0\n                matrix[-9][i] = 0x0\n        # Reserve format pattern areas\n        for i in range(9):\n            matrix[i][8] = 0x0  # Upper left\n            matrix[8][i] = 0x0  # Upper bottom\n            if version > 0:\n                matrix[-i][8] = 0x0  # Bottom left\n                matrix[8][- i] = 0x0  # Upper right\n    if add_timing:\n        # ISO/IEC 18004:2015 -- 6.3.5 Timing pattern (page 17)\n        add_timing_pattern(matrix, version < 1)\n    return matrix", "response": "\\ Creates a matrix of the provided size w x h."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize the provided version.", "response": "def normalize_version(version):\n    \"\"\"\\\n    Canonicalizes the provided `version`.\n\n    If the `version` is ``None``, this function returns ``None``. Otherwise\n    this function checks if `version` is an integer or a Micro QR Code version.\n    In case the string represents a Micro QR Code version, an uppercased\n    string identifier is returned.\n\n    If the `version` does not represent a valid version identifier (aside of\n    ``None``, a VersionError is raised.\n\n    :param version: An integer, a string or ``None``.\n    :raises: VersionError: In case the version is not ``None`` and does not\n                represent a valid (Micro) QR Code version.\n    :rtype: int, str or ``None``\n    \"\"\"\n    if version is None:\n        return None\n    error = False\n    try:\n        version = int(version)\n        # Don't want Micro QR Code constants as input\n        error = version < 1\n    except (ValueError, TypeError):\n        try:\n            version = consts.MICRO_VERSION_MAPPING[version.upper()]\n        except (KeyError, AttributeError):\n            error = True\n    if error or not 0 < version < 41 and version not in consts.MICRO_VERSIONS:\n        raise VersionError('Unsupported version \"{0}\". '\n                           'Supported: {1} and 1 .. 40'\n                           .format(version, ', '.join(sorted(consts.MICRO_VERSION_MAPPING.keys()))))\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes the mode parameter.", "response": "def normalize_mode(mode):\n    \"\"\"\\\n    Returns a (Micro) QR Code mode constant which is equivalent to the\n    provided `mode`.\n\n    In case the provided `mode` is ``None``, this function returns ``None``.\n    Otherwise a mode constant is returned unless the provided parameter cannot\n    be mapped to a valid mode. In the latter case, a ModeError is raised.\n\n    :param mode: An integer or string or ``None``.\n    :raises: ModeError: In case the provided `mode` does not represent a valid\n             QR Code mode.\n    :rtype: int or None\n    \"\"\"\n    if mode is None or (isinstance(mode, int)\n                        and mode in consts.MODE_MAPPING.values()):\n        return mode\n    try:\n        return consts.MODE_MAPPING[mode.lower()]\n    except:  # KeyError or mode.lower() fails\n        raise ModeError('Illegal mode \"{0}\". Supported values: {1}'\n                        .format(mode, ', '.join(sorted(consts.MODE_MAPPING.keys()))))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_errorlevel(error, accept_none=False):\n    if error is None:\n        if not accept_none:\n            raise ErrorLevelError('The error level must be provided')\n        return error\n    try:\n        return consts.ERROR_MAPPING[error.upper()]\n    except:  # KeyError or error.upper() fails\n        if error in consts.ERROR_MAPPING.values():\n            return error\n        raise ErrorLevelError('Illegal error correction level: \"{0}\".'\n                              'Supported levels: {1}'\n                              .format(error, ', '.join(sorted(consts.ERROR_MAPPING.keys()))))", "response": "Normalizes an error level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_version(segments, error, eci, micro, is_sa=False):\n    assert not (eci and micro)\n    min_version = 1 if micro is False else consts.VERSION_M1\n    max_version = 40 if micro is not True else consts.VERSION_M4\n    if min_version < 1:\n        min_version = max([find_minimum_version_for_mode(mode) for mode in segments.modes])\n    if error is not None and micro is not False:\n        min_version = consts.VERSION_M2\n    for version in range(min_version, max_version + 1):\n        if error is None and version != consts.VERSION_M1:\n            error = consts.ERROR_LEVEL_L\n        found = False\n        try:\n            found = consts.SYMBOL_CAPACITY[version][error] >= segments.bit_length_with_overhead(version, eci, is_sa)\n        except KeyError:\n            pass\n        if found:\n            return version\n    help_txt = ''\n    if micro is None:\n        help_txt = '(Micro) '\n    elif micro:\n        help_txt = 'Micro '\n    raise DataOverflowError('Data too large. No {0}QR Code can handle the '\n                            'provided data'.format(help_txt))", "response": "\\ Returns the version number of the QR Code that can handle the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the parity data for the Structured Append mode.", "response": "def calc_structured_append_parity(content):\n    \"\"\"\\\n    Calculates the parity data for the Structured Append mode.\n\n    :param str content: The content.\n    :rtype: int\n    \"\"\"\n    if not isinstance(content, str_type):\n        content = str(content)\n    try:\n        data = content.encode('iso-8859-1')\n    except UnicodeError:\n        try:\n            data = content.encode('shift-jis')\n        except (LookupError, UnicodeError):\n            data = content.encode('utf-8')\n    if _PY2:\n        data = (ord(c) for c in data)\n    return reduce(xor, data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the data mask functions for the current QR Code pattern and condition.", "response": "def get_data_mask_functions(is_micro):\n    \"\"\"\n    Returns the data mask functions.\n\n    ISO/IEC 18004:2015(E) -- 7.8.2 Data mask patterns (page 50)\n    Table 10 \u2014 Data mask pattern generation conditions (page 50)\n\n    ===============     =====================   =====================================\n    QR Code Pattern     Micro QR Code Pattern\ufffc  Condition\n    ===============     =====================   =====================================\n    000                                         (i + j) mod 2 = 0\n    001                 00                      i mod 2 = 0\n    010                                         j mod 3 = 0\n    011                                         (i + j) mod 3 = 0\n    100                 01                      ((i div 2) + (j div 3)) mod 2 = 0\n    101                                         (i j) mod 2 + (i j) mod 3 = 0\n    110                 10                      ((i j) mod 2 + (i j) mod 3) mod 2 = 0\n    111                 11                      ((i+j) mod 2 + (i j) mod 3) mod 2 = 0\n    ===============     =====================   =====================================\n\n    :param is_micro: Indicates if data mask functions for a Micro QR Code\n            should be returned\n    :return: A tuple of functions\n    \"\"\"\n    # i = row position, j = col position; (i, j) = (0, 0) = upper left corner\n    def fn0(i, j):\n        return (i + j) & 0x1 == 0\n\n    def fn1(i, j):\n        return i & 0x1 == 0\n\n    def fn2(i, j):\n        return j % 3 == 0\n\n    def fn3(i, j):\n        return (i + j) % 3 == 0\n\n    def fn4(i, j):\n        return (i // 2 + j // 3) & 0x1 == 0\n\n    def fn5(i, j):\n        tmp = i * j\n        return (tmp & 0x1) + (tmp % 3) == 0\n\n    def fn6(i, j):\n        tmp = i * j\n        return ((tmp & 0x1) + (tmp % 3)) & 0x1 == 0\n\n    def fn7(i, j):\n        return (((i + j) & 0x1) + (i * j) % 3) & 0x1 == 0\n\n    if is_micro:\n        return fn1, fn4, fn6, fn7\n    return fn0, fn1, fn2, fn3, fn4, fn5, fn6, fn7"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a segment to the list of available segments.", "response": "def add_segment(self, segment):\n        \"\"\"\\\n\n        :param _Segment segment: Segment to add.\n        \"\"\"\n        if self.segments:\n            prev_seg = self.segments[-1]\n            if prev_seg.mode == segment.mode and prev_seg.encoding == segment.encoding:\n                # Merge segment with previous segment\n                segment = _Segment(prev_seg.bits + segment.bits,\n                                   prev_seg.char_count + segment.char_count,\n                                   segment.mode, segment.encoding)\n                self.bit_length -= len(prev_seg.bits)\n                del self.segments[-1]\n                del self.modes[-1]\n        self.segments.append(segment)\n        self.bit_length += len(segment.bits)\n        self.modes.append(segment.mode)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toints(self):\n        def grouper(iterable, n, fillvalue=None):\n            \"Collect data into fixed-length chunks or blocks\"\n            # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n            return zip_longest(*[iter(iterable)] * n, fillvalue=fillvalue)\n        return [int(''.join(map(str, group)), 2) for group in grouper(self._data, 8, 0)]", "response": "\\ Returns an iterable of integers interpreting the content of seq as sequence of binary numbers of length 8."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef color_to_rgb_or_rgba(color, alpha_float=True):\n    rgba = color_to_rgba(color, alpha_float=alpha_float)\n    if rgba[3] in (1.0, 255):\n        return rgba[:3]\n    return rgba", "response": "\\ Returns the provided color as RGB or RGBA tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a color value to a web color name.", "response": "def color_to_webcolor(color, allow_css3_colors=True, optimize=True):\n    \"\"\"\\\n    Returns either a hexadecimal code or a color name.\n\n    :param color: A web color name (i.e. ``darkblue``) or a hexadecimal value\n            (``#RGB`` or ``#RRGGBB``) or a RGB(A) tuple (i.e. ``(R, G, B)`` or\n            ``(R, G, B, A)``)\n    :param bool allow_css3_colors: Indicates if a CSS3 color value like\n            rgba(R G, B, A) is an acceptable result.\n    :param bool optimize: Inidcates if the shortest possible color value should\n            be returned (default: ``True``).\n    :rtype: str\n    :return: The provided color as web color: ``#RGB``, ``#RRGGBB``,\n            ``rgba(R, G, B, A)``, or web color name.\n    \"\"\"\n    if color_is_black(color):\n        return '#000'\n    elif color_is_white(color):\n        return '#fff'\n    clr = color_to_rgb_or_rgba(color)\n    alpha_channel = None\n    if len(clr) == 4:\n        if allow_css3_colors:\n            return 'rgba({0},{1},{2},{3})'.format(*clr)\n        alpha_channel = clr[3]\n        clr = clr[:3]\n    hx = '#{0:02x}{1:02x}{2:02x}'.format(*clr)\n    if optimize:\n        if hx == '#d2b48c':\n            hx = 'tan'  # shorter\n        elif hx == '#ff0000':\n            hx = 'red'  # shorter\n        elif hx[1] == hx[2] and hx[3] == hx[4] and hx[5] == hx[6]:\n            hx = '#{0}{1}{2}'.format(hx[1], hx[3], hx[5])\n    return hx if alpha_channel is None else (hx, alpha_channel)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a web color name or hexadecimal value to RGB tuple.", "response": "def color_to_rgb(color):\n    \"\"\"\\\n    Converts web color names like \"red\" or hexadecimal values like \"#36c\",\n    \"#FFFFFF\" and RGB tuples like ``(255, 255 255)`` into a (R, G, B) tuple.\n\n    :param color: A web color name (i.e. ``darkblue``) or a hexadecimal value\n            (``#RGB`` or ``#RRGGBB``) or a RGB tuple (i.e. ``(R, G, B)``))\n    :return: ``(R, G, B)`` tuple.\n    \"\"\"\n    rgb = color_to_rgb_or_rgba(color)\n    if len(rgb) != 3:\n        raise ValueError('The alpha channel {0} in color \"{1}\" cannot be '\n                         'converted to RGB'.format(rgb[3], color))\n    return rgb"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef color_to_rgba(color, alpha_float=True):\n    res = []\n    alpha_channel = (1.0,) if alpha_float else (255,)\n    if isinstance(color, tuple):\n        col_length = len(color)\n        is_valid = False\n        if 3 <= col_length <= 4:\n            for i, part in enumerate(color[:3]):\n                is_valid = 0 <= part <= 255\n                res.append(part)\n                if not is_valid or i == 2:\n                    break\n            if is_valid:\n                if col_length == 4:\n                    res.append(_alpha_value(color[3], alpha_float))\n                else:\n                    res.append(alpha_channel[0])\n        if is_valid:\n            return tuple(res)\n        raise ValueError('Unsupported color \"{0}\"'.format(color))\n    try:\n        return _NAME2RGB[color.lower()] + alpha_channel\n    except KeyError:\n        try:\n            clr = _hex_to_rgb_or_rgba(color, alpha_float=alpha_float)\n            if len(clr) == 4:\n                return clr\n            else:\n                return clr + alpha_channel\n        except ValueError:\n            raise ValueError('Unsupported color \"{0}\". Neither a known web '\n                             'color name nor a color in hexadecimal format.'\n                             .format(color))", "response": "\\ Returns a tuple of rgb or rgba values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_valid_border(border):\n    if border is not None and (int(border) != border or border < 0):\n        raise ValueError('The border must not a non-negative integer value. '\n                         'Got: \"{0}\"'.format(border))", "response": "Raises a ValueError iff border is negative."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matrix_to_lines(matrix, x, y, incby=1):\n    y -= incby  # Move along y-axis so we can simply increment y in the loop\n    last_bit = 0x1\n    for row in matrix:\n        x1 = x\n        x2 = x\n        y += incby\n        for bit in row:\n            if last_bit != bit and not bit:\n                yield (x1, y), (x2, y)\n                x1 = x2\n            x2 += 1\n            if not bit:\n                x1 += 1\n            last_bit = bit\n        if last_bit:\n            yield (x1, y), (x2, y)\n            last_bit = 0x0", "response": "\\ Returns an iterable of tuples each containing the horizontal lines of dark modules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef matrix_iter(matrix, version, scale=1, border=None):\n    check_valid_border(border)\n    scale = int(scale)\n    check_valid_scale(scale)\n    border = get_border(version, border)\n    width, height = get_symbol_size(version, scale=1, border=0)\n\n    def get_bit(i, j):\n        return 0x1 if (0 <= i < height and 0 <= j < width and matrix[i][j]) else 0x0\n\n    for i in range(-border, height + border):\n        for s in range(scale):\n            yield chain.from_iterable(([get_bit(i, j)] * scale for j in range(-border, width + border)))", "response": "\\ Returns an iterator over the provided matrix which includes the border and the scaling factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef onStart(self):\n        curses.mousemask(0)\n        self.paths.host_config()\n        version = Version()\n\n        # setup initial runtime stuff\n        if self.first_time[0] and self.first_time[1] != 'exists':\n            system = System()\n            thr = Thread(target=system.start, args=(), kwargs={})\n            thr.start()\n            countdown = 60\n            while thr.is_alive():\n                npyscreen.notify_wait('Completing initialization:...' + str(countdown),\n                                      title='Setting up things...')\n                time.sleep(1)\n                countdown -= 1\n            thr.join()\n\n        quit_s = '\\t'*4 + '^Q to quit'\n        tab_esc = '\\t'*4 + 'ESC to close menu popup'\n        self.addForm('MAIN',\n                     MainForm,\n                     name='Vent ' + version +\n                     '\\t\\t\\t\\t\\t^T for help' + quit_s + tab_esc,\n                     color='IMPORTANT')\n        self.addForm('HELP',\n                     HelpForm,\n                     name='Help\\t\\t\\t\\t\\t\\t\\t\\t^T to toggle previous' +\n                     quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALINTRO',\n                     TutorialIntroForm,\n                     name='Vent Tutorial' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALBACKGROUND',\n                     TutorialBackgroundForm,\n                     name='About Vent' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALTERMINOLOGY',\n                     TutorialTerminologyForm,\n                     name='About Vent' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALGETTINGSETUP',\n                     TutorialGettingSetupForm,\n                     name='About Vent' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALSTARTINGCORES',\n                     TutorialStartingCoresForm,\n                     name='Working with Cores' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALADDINGPLUGINS',\n                     TutorialAddingPluginsForm,\n                     name='Working with Plugins' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALADDINGFILES',\n                     TutorialAddingFilesForm,\n                     name='Files' + quit_s,\n                     color='DANGER')\n        self.addForm('TUTORIALTROUBLESHOOTING',\n                     TutorialTroubleshootingForm,\n                     name='Troubleshooting' + quit_s,\n                     color='DANGER')", "response": "Override onStart method for npyscreen"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setGroups(self, groups, kerningGroupConversionRenameMaps=None):\n        skipping = []\n        for name, members in groups.items():\n            checked = []\n            for m in members:\n                if m in self.font:\n                    checked.append(m)\n                else:\n                    skipping.append(m)\n            if checked:\n                self.font.groups[name] = checked\n        if skipping:\n            if self.verbose and self.logger:\n                self.logger.info(\"\\tNote: some glyphnames were removed from groups: %s (unavailable in the font)\", \", \".join(skipping))\n        if kerningGroupConversionRenameMaps:\n            # in case the sources were UFO2, \n            # and defcon upconverted them to UFO3\n            # and now we have to down convert them again,\n            # we don't want the UFO3 public prefixes in the group names\n            self.font.kerningGroupConversionRenameMaps = kerningGroupConversionRenameMaps", "response": "Copy the groups into our font."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setLib(self, lib):\n        for name, item in lib.items():\n            self.font.lib[name] = item", "response": "Copy the lib items into our font."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copyFeatures(self, featureSource):\n        if featureSource in self.sources:\n            src, loc = self.sources[featureSource]\n            if isinstance(src.features.text, str):\n                self.font.features.text = u\"\"+src.features.text\n            elif isinstance(src.features.text, unicode):\n                self.font.features.text = src.features.text", "response": "Copy the features from this source to the current font."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makeUnicodeMapFromSources(self):\n        values = {}\n        for locationName, (source, loc) in self.sources.items():\n            # this will be expensive in large fonts\n            for glyph in source:\n                if glyph.unicodes is not None:\n                    if glyph.name not in values:\n                        values[glyph.name] = {}\n                for u in glyph.unicodes:\n                    values[glyph.name][u] = 1\n        for name, u in values.items():\n            if len(u) == 0:\n                # only report missing unicodes if the name has no extension\n                if \".\" not in name:\n                    self._missingUnicodes.append(name)\n                continue\n            k = list(u.keys())\n            self.unicodeValues[name] = k\n        return self.unicodeValues", "response": "Create a dictionary with glyphName - > unicode value pairs\n            using the data in the sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all glyphnames we have masters for.", "response": "def getAvailableGlyphnames(self):\n        \"\"\" Return a list of all glyphnames we have masters for.\"\"\"\n        glyphNames = {}\n        for locationName, (source, loc) in self.sources.items():\n            for glyph in source:\n                glyphNames[glyph.name] = 1\n        names = sorted(glyphNames.keys())\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds font info data.", "response": "def addInfo(self, instanceLocation=None, sources=None, copySourceName=None):\n        \"\"\" Add font info data. \"\"\"\n        if instanceLocation is None:\n            instanceLocation = self.locationObject\n        infoObject = self.font.info\n        infoMasters = []\n        if sources is None:\n            sources = self.sources\n        items = []\n        for sourceName, (source, sourceLocation) in sources.items():\n            if sourceName in self.muted['info']:\n                # info in this master was muted, so do not add.\n                continue\n            items.append((sourceLocation, MathInfo(source.info)))\n        try:\n            bias, m = buildMutator(items, axes=self.axes)\n        except:\n            if self.logger:\n                self.logger.exception(\"Error processing font info. %s\", items)\n            return\n        instanceObject = m.makeInstance(instanceLocation)\n        if self.roundGeometry:\n            try:\n                instanceObject = instanceObject.round()\n            except AttributeError:\n                warnings.warn(\"MathInfo object missing round() method.\")\n        instanceObject.extractInfo(self.font.info)\n\n        # handle the copyable info fields\n        if copySourceName is not None:\n            if not copySourceName in sources:\n                if self.verbose and self.logger:\n                    self.logger.info(\"Copy info source %s not found, skipping.\", copySourceName)\n                    return\n            copySourceObject, loc = sources[copySourceName]\n            self._copyFontInfo(self.font.info, copySourceObject.info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying the non - calculator fields from the source info to the target info.", "response": "def _copyFontInfo(self, targetInfo, sourceInfo):\n        \"\"\" Copy the non-calculating fields from the source info.\n        \"\"\"\n        infoAttributes = [\n            \"versionMajor\",\n            \"versionMinor\",\n            \"copyright\",\n            \"trademark\",\n            \"note\",\n            \"openTypeGaspRangeRecords\",\n            \"openTypeHeadCreated\",\n            \"openTypeHeadFlags\",\n            \"openTypeNameDesigner\",\n            \"openTypeNameDesignerURL\",\n            \"openTypeNameManufacturer\",\n            \"openTypeNameManufacturerURL\",\n            \"openTypeNameLicense\",\n            \"openTypeNameLicenseURL\",\n            \"openTypeNameVersion\",\n            \"openTypeNameUniqueID\",\n            \"openTypeNameDescription\",\n            \"#openTypeNamePreferredFamilyName\",\n            \"#openTypeNamePreferredSubfamilyName\",\n            \"#openTypeNameCompatibleFullName\",\n            \"openTypeNameSampleText\",\n            \"openTypeNameWWSFamilyName\",\n            \"openTypeNameWWSSubfamilyName\",\n            \"openTypeNameRecords\",\n            \"openTypeOS2Selection\",\n            \"openTypeOS2VendorID\",\n            \"openTypeOS2Panose\",\n            \"openTypeOS2FamilyClass\",\n            \"openTypeOS2UnicodeRanges\",\n            \"openTypeOS2CodePageRanges\",\n            \"openTypeOS2Type\",\n            \"postscriptIsFixedPitch\",\n            \"postscriptForceBold\",\n            \"postscriptDefaultCharacter\",\n            \"postscriptWindowsCharacterSet\"\n        ]\n        for infoAttribute in infoAttributes:\n            copy = False\n            if self.ufoVersion == 1 and infoAttribute in fontInfoAttributesVersion1:\n                copy = True\n            elif self.ufoVersion == 2 and infoAttribute in fontInfoAttributesVersion2:\n                copy = True\n            elif self.ufoVersion == 3 and infoAttribute in fontInfoAttributesVersion3:\n                copy = True\n            if copy:\n                value = getattr(sourceInfo, infoAttribute)\n                setattr(targetInfo, infoAttribute, value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addKerning(self, instanceLocation=None, sources=None):\n        items = []\n        kerningObject = self.font.kerning\n        kerningMasters = []\n        if instanceLocation is None:\n            instanceLocation = self.locationObject\n\n        if sources is None:\n            # kerning has no special requests, add the default sources\n            sources = self.sources\n        for sourceName, (source, sourceLocation) in sources.items():\n            if sourceName in self.muted['kerning']:\n                # kerning in this master was muted, so do not add.\n                if self.verbose and self.logger:\n                    self.logger.info(\"\\tMuting kerning data for %s\", instanceLocation)\n                continue\n            if len(source.kerning.keys())>0:\n                items.append((sourceLocation, MathKerning(source.kerning, source.groups)))\n        if items:\n            m = None\n            try:\n                bias, m = buildMutator(items, axes=self.axes)\n            except:\n                if self.logger:\n                    self.logger.exception(\"\\tError processing kerning data. %s\", items)\n                return\n            instanceObject = m.makeInstance(instanceLocation)\n            if self.roundGeometry:\n                instanceObject.round()\n            instanceObject.extractKerning(self.font)", "response": "Add kerning data to this instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a glyph to the master.", "response": "def addGlyph(self, glyphName, unicodes=None, instanceLocation=None, sources=None, note=None):\n        \"\"\"\n        Calculate a new glyph and add it to this instance.\n        \n        *   glyphName:   The name of the glyph\n        *   unicodes:   The unicode values for this glyph (optional)\n        *   instanceLocation:   Location for this glyph\n        *   sources:    List of sources for this glyph.\n        *   note:   Note for this glyph.\n        \"\"\"\n        self.font.newGlyph(glyphName)\n        glyphObject = self.font[glyphName]\n        if note is not None:\n            glyphObject.note = note\n            # why does this not save?\n        if unicodes is not None:\n            glyphObject.unicodes = unicodes\n        if instanceLocation is None:\n            instanceLocation = self.locationObject\n        glyphMasters = []\n        if sources is None:\n            # glyph has no special requests, add the default sources\n            for sourceName, (source, sourceLocation) in self.sources.items():\n                if glyphName in self.muted['glyphs'].get(sourceName, []):\n                    # this glyph in this master was muted, so do not add.\n                    continue\n                d = dict(   font=source,\n                            location=sourceLocation,\n                            glyphName=glyphName)\n                glyphMasters.append(d)\n        else:\n            # use the glyph sources provided\n            # if self.verbose and self.logger:\n            #     self.logger.info(\"\\tGlyph %s has special masters %s\", glyphName, sources)\n            glyphMasters = sources\n        # make the glyphs\n        try:\n            self._calculateGlyph(glyphObject, instanceLocation, glyphMasters)\n        except:\n            self._failed.append(glyphName)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calculateGlyph(self, targetGlyphObject, instanceLocationObject, glyphMasters):\n        sources = None\n        items = []\n\n        for item in glyphMasters:\n            locationObject = item['location']\n            fontObject = item['font']\n            glyphName = item['glyphName']\n            if not glyphName in fontObject:\n                continue\n            glyphObject = MathGlyph(fontObject[glyphName])\n            items.append((locationObject, glyphObject))\n        bias, m = buildMutator(items, axes=self.axes)\n        instanceObject = m.makeInstance(instanceLocationObject)\n        if self.roundGeometry:\n            try:\n                instanceObject = instanceObject.round()\n            except AttributeError:\n                if self.verbose and self.logger:\n                    self.logger.info(\"MathGlyph object missing round() method.\")\n\n\n        try:\n            instanceObject.extractGlyph(targetGlyphObject, onlyGeometry=True)\n        except TypeError:\n            # this causes ruled glyphs to end up in the wrong glyphname\n            # but defcon2 objects don't support it\n            pPen = targetGlyphObject.getPointPen()\n            targetGlyphObject.clear()\n            instanceObject.drawPoints(pPen)\n            targetGlyphObject.width = instanceObject.width", "response": "Calculate a glyph for this glyph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the xml. Make pretty if necessary.", "response": "def save(self, pretty=True):\n        \"\"\" Save the xml. Make pretty if necessary. \"\"\"\n        self.endInstance()\n        if pretty:\n            _indent(self.root, whitespace=self._whiteSpace)\n        tree = ET.ElementTree(self.root)\n        tree.write(self.path, encoding=\"utf-8\", method='xml', xml_declaration=True)\n        if self.logger:\n            self.logger.info(\"Writing %s\", self.path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a Location object to an locationElement.", "response": "def _makeLocationElement(self, locationObject, name=None):\n        \"\"\" Convert Location object to an locationElement.\"\"\"\n\n        locElement = ET.Element(\"location\")\n        if name is not None:\n           locElement.attrib['name'] = name\n        for dimensionName, dimensionValue in locationObject.items():\n           dimElement = ET.Element('dimension')\n           dimElement.attrib['name'] = dimensionName\n           if type(dimensionValue)==tuple:\n               dimElement.attrib['xvalue'] = \"%f\"%dimensionValue[0]\n               dimElement.attrib['yvalue'] = \"%f\"%dimensionValue[1]\n           else:\n               dimElement.attrib['xvalue'] = \"%f\"%dimensionValue\n           locElement.append(dimElement)\n        return locElement"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addSource(self,\n            path,\n            name,\n            location,\n            copyLib=False,\n            copyGroups=False,\n            copyInfo=False,\n            copyFeatures=False,\n            muteKerning=False,\n            muteInfo=False,\n            mutedGlyphNames=None,\n            familyName=None,\n            styleName=None,\n        ):\n        \"\"\"\n        Add a new UFO source to the document.\n        *   path:           path to this UFO, will be written as a relative path to the document path.\n        *   name:           reference name for this source\n        *   location:       name of the location for this UFO\n        *   copyLib:        copy the contents of this source to instances\n        *   copyGroups:     copy the groups of this source to instances\n        *   copyInfo:       copy the non-numerical fields from this source.info to instances.\n        *   copyFeatures:   copy the feature text from this source to instances\n        *   muteKerning:    mute the kerning data from this source\n        *   muteInfo:       mute the font info data from this source\n        *   familyName:     family name for this UFO (to be able to work on the names without reading the whole UFO)\n        *   styleName:      style name for this UFO (to be able to work on the names without reading the whole UFO)\n\n        Note: no separate flag for mute font: the source is just not added.\n        \"\"\"\n        sourceElement = ET.Element(\"source\")\n        sourceElement.attrib['filename'] = self._posixPathRelativeToDocument(path)\n        sourceElement.attrib['name'] = name\n        if copyLib:\n            libElement = ET.Element('lib')\n            libElement.attrib['copy'] = \"1\"\n            sourceElement.append(libElement)\n\n        if copyGroups:\n            groupsElement = ET.Element('groups')\n            groupsElement.attrib['copy'] = \"1\"\n            sourceElement.append(groupsElement)\n\n        if copyFeatures:\n            featuresElement = ET.Element('features')\n            featuresElement.attrib['copy'] = \"1\"\n            sourceElement.append(featuresElement)\n\n        if copyInfo or muteInfo:\n            # copy info:\n            infoElement = ET.Element('info')\n            if copyInfo:\n                infoElement.attrib['copy'] = \"1\"\n            if muteInfo:\n                infoElement.attrib['mute'] = \"1\"\n            sourceElement.append(infoElement)\n\n        if muteKerning:\n            # add kerning element to the source\n            kerningElement = ET.Element(\"kerning\")\n            kerningElement.attrib[\"mute\"] = '1'\n            sourceElement.append(kerningElement)\n\n        if mutedGlyphNames:\n            # add muted glyphnames to the source\n            for name in mutedGlyphNames:\n                glyphElement = ET.Element(\"glyph\")\n                glyphElement.attrib[\"name\"] = name\n                glyphElement.attrib[\"mute\"] = '1'\n                sourceElement.append(glyphElement)\n\n        if familyName is not None:\n            sourceElement.attrib['familyname'] = familyName\n        if styleName is not None:\n            sourceElement.attrib['stylename'] = styleName\n\n\n        locationElement = self._makeLocationElement(location)\n        sourceElement.append(locationElement)\n        self.root.findall('.sources')[0].append(sourceElement)", "response": "Add a new source to the document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef startInstance(self, name=None,\n            location=None,\n            familyName=None,\n            styleName=None,\n            fileName=None,\n            postScriptFontName=None,\n            styleMapFamilyName=None,\n            styleMapStyleName=None,\n\n            ):\n        \"\"\" Start a new instance.\n            Instances can need a lot of configuration.\n            So this method starts a new instance element. Use endInstance() to finish it.\n\n            *   name: the name of this instance\n            *   familyName: name for the font.info.familyName field. Required.\n            *   styleName: name fot the font.info.styleName field. Required.\n            *   fileName: filename for the instance UFO file. Required.\n            *   postScriptFontName: name for the font.info.postScriptFontName field. Optional.\n            *   styleMapFamilyName: name for the font.info.styleMapFamilyName field. Optional.\n            *   styleMapStyleName: name for the font.info.styleMapStyleName field. Optional.\n        \"\"\"\n        if self.currentInstance is not None:\n            # We still have the previous one open\n            self.endInstance()\n        instanceElement = ET.Element('instance')\n        if name is not None:\n            instanceElement.attrib['name'] = name\n        if location is not None:\n            locationElement = self._makeLocationElement(location)\n            instanceElement.append(locationElement)\n        if familyName is not None:\n            instanceElement.attrib['familyname'] = familyName\n        if styleName is not None:\n            instanceElement.attrib['stylename'] = styleName\n        if fileName is not None:\n            instanceElement.attrib['filename'] = self._posixPathRelativeToDocument(fileName)\n        if postScriptFontName is not None:\n            instanceElement.attrib['postscriptfontname'] = postScriptFontName\n        if styleMapFamilyName is not None:\n            instanceElement.attrib['stylemapfamilyname'] = styleMapFamilyName\n        if styleMapStyleName is not None:\n            instanceElement.attrib['stylemapstylename'] = styleMapStyleName\n\n        self.currentInstance = instanceElement", "response": "Start a new instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfinalise the instance definition started by startInstance.", "response": "def endInstance(self):\n        \"\"\"\n            Finalise the instance definition started by startInstance().\n        \"\"\"\n        if self.currentInstance is None:\n            return\n        allInstances = self.root.findall('.instances')[0].append(self.currentInstance)\n        self.currentInstance = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writeGlyph(self,\n            name,\n            unicodes=None,\n            location=None,\n            masters=None,\n            note=None,\n            mute=False,\n            ):\n        \"\"\" Add a new glyph to the current instance.\n            * name: the glyph name. Required.\n            * unicodes: unicode values for this glyph if it needs to be different from the unicode values associated with this glyph name in the masters.\n            * location: a design space location for this glyph if it needs to be different from the instance location.\n            * masters: a list of masters and locations for this glyph if they need to be different from the masters specified for this instance.\n            * note: a note for this glyph\n            * mute: if this glyph is muted. None of the other attributes matter if this one is true.\n        \"\"\"\n        if self.currentInstance is None:\n            return\n        glyphElement = ET.Element('glyph')\n        if mute:\n            glyphElement.attrib['mute'] = \"1\"\n        if unicodes is not None:\n            glyphElement.attrib['unicode'] = \" \".join([hex(u) for u in unicodes])\n        if location is not None:\n            locationElement = self._makeLocationElement(location)\n            glyphElement.append(locationElement)\n        if name is not None:\n            glyphElement.attrib['name'] = name\n        if note is not None:\n            noteElement = ET.Element('note')\n            noteElement.text = note\n            glyphElement.append(noteElement)\n        if masters is not None:\n            mastersElement = ET.Element(\"masters\")\n            for glyphName, masterName, location in masters:\n                masterElement = ET.Element(\"master\")\n                if glyphName is not None:\n                    masterElement.attrib['glyphname'] = glyphName\n                masterElement.attrib['source'] = masterName\n                if location is not None:\n                    locationElement = self._makeLocationElement(location)\n                    masterElement.append(locationElement)\n                mastersElement.append(masterElement)\n            glyphElement.append(mastersElement)\n        if self.currentInstance.findall('.glyphs') == []:\n            glyphsElement = ET.Element('glyphs')\n            self.currentInstance.append(glyphsElement)\n        else:\n            glyphsElement = self.currentInstance.findall('.glyphs')[0]\n        glyphsElement.append(glyphElement)", "response": "Writes a new glyph to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting font into the current instance.", "response": "def writeInfo(self, location=None, masters=None):\n        \"\"\" Write font into the current instance.\n            Note: the masters attribute is ignored at the moment.\n        \"\"\"\n        if self.currentInstance is None:\n            return\n        infoElement = ET.Element(\"info\")\n        if location is not None:\n            locationElement = self._makeLocationElement(location)\n            infoElement.append(locationElement)\n        self.currentInstance.append(infoElement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writeKerning(self, location=None, masters=None):\n        if self.currentInstance is None:\n            return\n        kerningElement = ET.Element(\"kerning\")\n        if location is not None:\n            locationElement = self._makeLocationElement(location)\n            kerningElement.append(locationElement)\n        self.currentInstance.append(kerningElement)", "response": "Writes kerning into the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a list of in out values for a warpmap.", "response": "def writeWarp(self, warpDict):\n        \"\"\" Write a list of (in, out) values for a warpmap \"\"\"\n        warpElement = ET.Element(\"warp\")\n        axisNames = sorted(warpDict.keys())\n        for name in axisNames:\n            axisElement = ET.Element(\"axis\")\n            axisElement.attrib['name'] = name\n            for a, b in warpDict[name]:\n                warpPt = ET.Element(\"map\")\n                warpPt.attrib['input'] = str(a)\n                warpPt.attrib['output'] = str(b)\n                axisElement.append(warpPt)\n            warpElement.append(axisElement)\n        self.root.append(warpElement)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addAxis(self, tag, name, minimum, maximum, default, warpMap=None):\n        axisElement = ET.Element(\"axis\")\n        axisElement.attrib['name'] = name\n        axisElement.attrib['tag'] = tag\n        axisElement.attrib['minimum'] = str(minimum)\n        axisElement.attrib['maximum'] = str(maximum)\n        axisElement.attrib['default'] = str(default)\n        if warpMap is not None:\n            for a, b in warpMap:\n                warpPt = ET.Element(\"map\")\n                warpPt.attrib['input'] = str(a)\n                warpPt.attrib['output'] = str(b)\n                axisElement.append(warpPt)\n        self.root.findall('.axes')[0].append(axisElement)", "response": "Write an axis element. This will be added to the axes element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getSourcePaths(self, makeGlyphs=True, makeKerning=True, makeInfo=True):\n        paths = []\n        for name in self.sources.keys():\n            paths.append(self.sources[name][0].path)\n        return paths", "response": "Return a list of paths referenced in the document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the input file and generate the instances.", "response": "def process(self, makeGlyphs=True, makeKerning=True, makeInfo=True):\n        \"\"\" Process the input file and generate the instances. \"\"\"\n        if self.logger:\n            self.logger.info(\"Reading %s\", self.path)\n        self.readInstances(makeGlyphs=makeGlyphs, makeKerning=makeKerning, makeInfo=makeInfo)\n        self.reportProgress(\"done\", 'stop')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readVersion(self):\n        ds = self.root.findall(\"[@format]\")[0]\n        raw_format = ds.attrib['format']\n        try:\n            self.documentFormatVersion = int(raw_format)\n        except ValueError:\n            # as of fontTools >= 3.27 'format' is formatted as a float \"4.0\"\n            self.documentFormatVersion = float(raw_format)", "response": "Read the document version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the warp element and store it in self. warpDict.", "response": "def readWarp(self):\n        \"\"\" Read the warp element\n\n        ::\n            <warp>\n                <axis name=\"weight\">\n                    <map input=\"0\" output=\"0\" />\n                    <map input=\"500\" output=\"200\" />\n                    <map input=\"1000\" output=\"1000\" />\n                </axis>\n            </warp>\n\n        \"\"\"\n        warpDict = {}\n        for warpAxisElement in self.root.findall(\".warp/axis\"):\n            axisName = warpAxisElement.attrib.get(\"name\")\n            warpDict[axisName] = []\n            for warpPoint in warpAxisElement.findall(\".map\"):\n                inputValue = float(warpPoint.attrib.get(\"input\"))\n                outputValue = float(warpPoint.attrib.get(\"output\"))\n                warpDict[axisName].append((inputValue, outputValue))\n        self.warpDict = warpDict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the axes element.", "response": "def readAxes(self):\n        \"\"\" Read the axes element.\n        \"\"\"\n        for axisElement in self.root.findall(\".axes/axis\"):\n            axis = {}\n            axis['name'] = name = axisElement.attrib.get(\"name\")\n            axis['tag'] = axisElement.attrib.get(\"tag\")\n            axis['minimum'] = float(axisElement.attrib.get(\"minimum\"))\n            axis['maximum'] = float(axisElement.attrib.get(\"maximum\"))\n            axis['default'] = float(axisElement.attrib.get(\"default\"))\n            # we're not using the map for anything.\n            axis['map'] = []\n            for warpPoint in axisElement.findall(\".map\"):\n                inputValue = float(warpPoint.attrib.get(\"input\"))\n                outputValue = float(warpPoint.attrib.get(\"output\"))\n                axis['map'].append((inputValue, outputValue))\n            # there are labelnames in the element\n            # but we don't need them for building the fonts.\n            self.axes[name] = axis\n            self.axesOrder.append(axis['name'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readSources(self):\n        for sourceCount, sourceElement in enumerate(self.root.findall(\".sources/source\")):\n            # shall we just read the UFO here?\n            filename = sourceElement.attrib.get('filename')\n            # filename is a path relaive to the documentpath. resolve first.\n            sourcePath = os.path.abspath(os.path.join(os.path.dirname(self.path), filename))\n            sourceName = sourceElement.attrib.get('name')\n            if sourceName is None:\n                # if the source element has no name attribute\n                # (some authoring tools do not need them)\n                # then we should make a temporary one. We still need it for reference.\n                sourceName = \"temp_master.%d\"%(sourceCount)\n            self.reportProgress(\"prep\", 'load', sourcePath)\n            if not os.path.exists(sourcePath):\n                raise MutatorError(\"Source not found at %s\"%sourcePath)\n            sourceObject = self._instantiateFont(sourcePath)\n            # read the locations\n            sourceLocationObject = None\n            sourceLocationObject = self.locationFromElement(sourceElement)\n\n            if sourceLocationObject is None:\n               raise MutatorError(\"No location defined for source %s\"%sourceName)\n\n            # read lib flag\n            for libElement in sourceElement.findall('.lib'):\n                if libElement.attrib.get('copy') == '1':\n                    self.libSource = sourceName\n\n            # read the groups flag\n            for groupsElement in sourceElement.findall('.groups'):\n                if groupsElement.attrib.get('copy') == '1':\n                    self.groupsSource = sourceName\n\n            # read the info flag\n            for infoElement in sourceElement.findall(\".info\"):\n                if infoElement.attrib.get('copy') == '1':\n                    self.infoSource = sourceName\n                if infoElement.attrib.get('mute') == '1':\n                    self.muted['info'].append(sourceName)\n\n            # read the features flag\n            for featuresElement in sourceElement.findall(\".features\"):\n                if featuresElement.attrib.get('copy') == '1':\n                    if self.featuresSource is not None:\n                        self.featuresSource = None\n                    else:\n                        self.featuresSource = sourceName\n\n            mutedGlyphs = []\n            for glyphElement in sourceElement.findall(\".glyph\"):\n                glyphName = glyphElement.attrib.get('name')\n                if glyphName is None:\n                    continue\n                if glyphElement.attrib.get('mute') == '1':\n                    if not sourceName in self.muted['glyphs']:\n                        self.muted['glyphs'][sourceName] = []\n                    self.muted['glyphs'][sourceName].append(glyphName)\n\n            for kerningElement in sourceElement.findall(\".kerning\"):\n                if kerningElement.attrib.get('mute') == '1':\n                    self.muted['kerning'].append(sourceName)\n\n            # store\n            self.sources[sourceName] = sourceObject, sourceLocationObject\n            self.reportProgress(\"prep\", 'done')", "response": "Read the sources of the master."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the MutatorMath location of this element either by name or from a child element.", "response": "def locationFromElement(self, element):\n        \"\"\"\n            Find the MutatorMath location of this element, either by name or from a child element.\n        \"\"\"\n        elementLocation = None\n        for locationElement in element.findall('.location'):\n            elementLocation = self.readLocationElement(locationElement)\n            break\n        return elementLocation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readLocationElement(self, locationElement):\n        loc = Location()\n        for dimensionElement in locationElement.findall(\".dimension\"):\n            dimName = dimensionElement.attrib.get(\"name\")\n            xValue = yValue = None\n            try:\n                xValue = dimensionElement.attrib.get('xvalue')\n                xValue = float(xValue)\n            except ValueError:\n                if self.logger:\n                    self.logger.info(\"KeyError in readLocation xValue %3.3f\", xValue)\n            try:\n                yValue = dimensionElement.attrib.get('yvalue')\n                if yValue is not None:\n                    yValue = float(yValue)\n            except ValueError:\n                pass\n            if yValue is not None:\n                loc[dimName] = (xValue, yValue)\n            else:\n                loc[dimName] = xValue\n        return loc", "response": "Read a location element from the XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single instance element.", "response": "def readInstance(self, key, makeGlyphs=True, makeKerning=True, makeInfo=True):\n        \"\"\" Read a single instance element.\n\n            key: an (attribute, value) tuple used to find the requested instance.\n\n        ::\n\n            <instance familyname=\"SuperFamily\" filename=\"OutputNameInstance1.ufo\" location=\"location-token-aaa\" stylename=\"Regular\">\n\n        \"\"\"\n        attrib, value = key\n        for instanceElement in self.root.findall('.instances/instance'):\n            if instanceElement.attrib.get(attrib) == value:\n                self._readSingleInstanceElement(instanceElement, makeGlyphs=makeGlyphs, makeKerning=makeKerning, makeInfo=makeInfo)\n                return\n        raise MutatorError(\"No instance found with key: (%s, %s).\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readInstances(self, makeGlyphs=True, makeKerning=True, makeInfo=True):\n        for instanceElement in self.root.findall('.instances/instance'):\n            self._readSingleInstanceElement(instanceElement, makeGlyphs=makeGlyphs, makeKerning=makeKerning, makeInfo=makeInfo)", "response": "Read all instance elements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a single instance element.", "response": "def _readSingleInstanceElement(self, instanceElement, makeGlyphs=True, makeKerning=True, makeInfo=True):\n        \"\"\" Read a single instance element.\n            If we have glyph specifications, only make those.\n            Otherwise make all available glyphs.\n        \"\"\"\n        # get the data from the instanceElement itself\n        filename = instanceElement.attrib.get('filename')\n\n        instancePath = os.path.join(os.path.dirname(self.path), filename)\n        self.reportProgress(\"generate\", 'start', instancePath)\n        if self.verbose and self.logger:\n            self.logger.info(\"\\tGenerating instance %s\", os.path.basename(instancePath))\n        filenameTokenForResults = os.path.basename(filename)\n\n        instanceObject = self._instanceWriterClass(instancePath,\n            ufoVersion=self.ufoVersion,\n            roundGeometry=self.roundGeometry,\n            axes = self.axes,\n            verbose=self.verbose,\n            logger=self.logger\n            )\n        self.results[filenameTokenForResults] = instancePath\n\n        # set the masters\n        instanceObject.setSources(self.sources)\n        self.unicodeMap = instanceObject.makeUnicodeMapFromSources()\n        instanceObject.setMuted(self.muted)\n        familyname = instanceElement.attrib.get('familyname')\n        if familyname is not None:\n            instanceObject.setFamilyName(familyname)\n        stylename = instanceElement.attrib.get('stylename')\n        if stylename is not None:\n            instanceObject.setStyleName(stylename)\n        postScriptFontName = instanceElement.attrib.get('postscriptfontname')\n        if postScriptFontName is not None:\n            instanceObject.setPostScriptFontName(postScriptFontName)\n        styleMapFamilyName = instanceElement.attrib.get('stylemapfamilyname')\n        if styleMapFamilyName is not None:\n            instanceObject.setStyleMapFamilyName(styleMapFamilyName)\n        styleMapStyleName = instanceElement.attrib.get('stylemapstylename')\n        if styleMapStyleName is not None:\n            instanceObject.setStyleMapStyleName(styleMapStyleName)\n\n        # location\n        instanceLocation = self.locationFromElement(instanceElement)\n\n        if instanceLocation is not None:\n            instanceObject.setLocation(instanceLocation)\n\n        if makeGlyphs:\n\n            # step 1: generate all glyphs we have mutators for.\n            names = instanceObject.getAvailableGlyphnames()\n            for n in names:\n                unicodes = self.unicodeMap.get(n, None)\n                try:\n                    instanceObject.addGlyph(n, unicodes)\n                except AssertionError:\n                    if self.verbose and self.logger:\n                        self.logger.info(\"Problem making glyph %s, skipping.\", n)\n            # step 2: generate all the glyphs that have special definitions.\n            for glyphElement in instanceElement.findall('.glyphs/glyph'):\n                self.readGlyphElement(glyphElement, instanceObject)\n\n        # read the kerning\n        if makeKerning:\n            for kerningElement in instanceElement.findall('.kerning'):\n                self.readKerningElement(kerningElement, instanceObject)\n                break\n\n        # read the fontinfo\n        if makeInfo:\n            for infoElement in instanceElement.findall('.info'):\n                self.readInfoElement(infoElement, instanceObject)\n\n        # copy the features\n        if self.featuresSource is not None:\n            instanceObject.copyFeatures(self.featuresSource)\n\n        # copy the groups\n        if self.groupsSource is not None:\n            if self.groupsSource in self.sources:\n                groupSourceObject, loc = self.sources[self.groupsSource]\n                # copy the groups from the designated source to the new instance\n                # note: setGroups will filter the group members\n                # only glyphs present in the font will be added to the group.\n                # Depending on the ufoversion we might or might not expect the kerningGroupConversionRenameMaps attribute.\n                if hasattr(groupSourceObject, \"kerningGroupConversionRenameMaps\"):\n                    renameMap = groupSourceObject.kerningGroupConversionRenameMaps\n                else:\n                    renameMap = {}\n                instanceObject.setGroups(groupSourceObject.groups, kerningGroupConversionRenameMaps=renameMap)\n\n        # lib items\n        if self.libSource is not None:\n            if self.libSource in self.sources:\n                libSourceObject, loc = self.sources[self.libSource]\n                instanceObject.setLib(libSourceObject.lib)\n\n        # save the instance. Done.\n        success, report = instanceObject.save()\n        if not success and self.logger:\n            # report problems other than validation errors and failed glyphs\n            self.logger.info(\"%s:\\nErrors generating: %s\", filename, report)\n\n        # report failed glyphs\n        failed = instanceObject.getFailed()\n        if failed:\n            failed.sort()\n            msg = \"%s:\\nErrors calculating %s glyphs: \\n%s\"%(filename, len(failed),\"\\t\"+\"\\n\\t\".join(failed))\n            self.reportProgress('error', 'glyphs', msg)\n            if self.verbose and self.logger:\n                self.logger.info(msg)\n\n        # report missing unicodes\n        missing = instanceObject.getMissingUnicodes()\n        if missing:\n            missing.sort()\n            msg = \"%s:\\nPossibly missing unicodes for %s glyphs: \\n%s\"%(filename, len(missing),\"\\t\"+\"\\n\\t\".join(missing))\n            self.reportProgress('error', 'unicodes', msg)\n\n        # store\n        self.instances[postScriptFontName] = instanceObject\n        self.reportProgress(\"generate\", 'stop', filenameTokenForResults)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the info element and add it to the info object.", "response": "def readInfoElement(self, infoElement, instanceObject):\n        \"\"\" Read the info element.\n\n            ::\n\n                <info/>\n\n                <info\">\n                <location/>\n                </info>\n\n            \"\"\"\n        infoLocation = self.locationFromElement(infoElement)\n        instanceObject.addInfo(infoLocation, copySourceName=self.infoSource)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the kerning element and add it to the instance object.", "response": "def readKerningElement(self, kerningElement, instanceObject):\n        \"\"\" Read the kerning element.\n\n        ::\n\n                Make kerning at the location and with the masters specified at the instance level.\n                <kerning/>\n\n        \"\"\"\n        kerningLocation = self.locationFromElement(kerningElement)\n        instanceObject.addKerning(kerningLocation)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the glyph element and add it to the glyph object.", "response": "def readGlyphElement(self, glyphElement, instanceObject):\n        \"\"\"\n        Read the glyph element.\n\n        ::\n\n            <glyph name=\"b\" unicode=\"0x62\"/>\n\n            <glyph name=\"b\"/>\n\n            <glyph name=\"b\">\n                <master location=\"location-token-bbb\" source=\"master-token-aaa2\"/>\n                <master glyphname=\"b.alt1\" location=\"location-token-ccc\" source=\"master-token-aaa3\"/>\n\n                <note>\n                    This is an instance from an anisotropic interpolation.\n                </note>\n            </glyph>\n\n        \"\"\"\n        # name\n        glyphName = glyphElement.attrib.get('name')\n        if glyphName is None:\n            raise MutatorError(\"Glyph object without name attribute.\")\n\n        # mute\n        mute = glyphElement.attrib.get(\"mute\")\n        if mute == \"1\":\n            instanceObject.muteGlyph(glyphName)\n            # we do not need to stick around after this\n            return\n\n        # unicode\n        unicodes = glyphElement.attrib.get('unicode')\n        if unicodes == None:\n            unicodes = self.unicodeMap.get(glyphName, None)\n        else:\n            try:\n                unicodes = [int(u, 16) for u in unicodes.split(\" \")]\n            except ValueError:\n                raise MutatorError(\"unicode values %s are not integers\" % unicodes)\n\n        # note\n        note = None\n        for noteElement in glyphElement.findall('.note'):\n            note = noteElement.text\n            break\n\n        # location\n        instanceLocation = self.locationFromElement(glyphElement)\n\n        # masters\n        glyphSources = None\n        for masterElement in glyphElement.findall('.masters/master'):\n            fontSourceName = masterElement.attrib.get('source')\n            fontSource, fontLocation = self.sources.get(fontSourceName)\n            if fontSource is None:\n                raise MutatorError(\"Unknown glyph master: %s\"%masterElement)\n            sourceLocation = self.locationFromElement(masterElement)\n            if sourceLocation is None:\n                # if we don't read a location, use the instance location\n                sourceLocation = fontLocation\n            masterGlyphName = masterElement.attrib.get('glyphname')\n            if masterGlyphName is None:\n                # if we don't read a glyphname, use the one we have\n                masterGlyphName = glyphName\n            d = dict(   font=fontSource,\n                        location=sourceLocation,\n                        glyphName=masterGlyphName)\n            if glyphSources is None:\n                glyphSources = []\n            glyphSources.append(d)\n        # calculate the glyph\n        instanceObject.addGlyph(glyphName, unicodes, instanceLocation, glyphSources, note=note)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a instance of a font object with all the given subclasses", "response": "def _instantiateFont(self, path):\n        \"\"\"\n        Return a instance of a font object\n        with all the given subclasses\n        \"\"\"\n        return self._fontClass(path,\n            libClass=self._libClass,\n            kerningClass=self._kerningClass,\n            groupsClass=self._groupsClass,\n            infoClass=self._infoClass,\n            featuresClass=self._featuresClass,\n            glyphClass=self._glyphClass,\n            glyphContourClass=self._glyphContourClass,\n            glyphPointClass=self._glyphPointClass,\n            glyphComponentClass=self._glyphComponentClass,\n            glyphAnchorClass=self._glyphAnchorClass)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nqueue up containers waiting for GPU resources", "response": "def gpu_queue(options):\n    \"\"\"\n    Queued up containers waiting for GPU resources\n    \"\"\"\n    import docker\n    import json\n    import os\n    import time\n\n    from vent.helpers.meta import GpuUsage\n\n    status = (False, None)\n    if (os.path.isfile('/root/.vent/vent.cfg') and os.path.isfile('/root/.vent/plugin_manifest.cfg')):\n        path_dir = '/root/.vent'\n    else:\n        path_dir = '/vent'\n\n    print('gpu queue', str(options))\n    print('gpu queue', str(GpuUsage(base_dir=path_dir+'/',\n                                    meta_dir=path_dir)))\n\n    options = json.loads(options)\n    configs = options['configs']\n    gpu_options = configs['gpu_options']\n    devices = []\n    options['remove'] = True\n\n    # device specified, remove all other devices\n    if 'device' in gpu_options:\n        dev = '/dev/nvidia' + gpu_options['device'] + ':/dev/nvidia'\n        dev += gpu_options['device'] + ':rwm'\n        if 'devices' in configs:\n            d = list(configs['devices'])\n            for device in d:\n                if any(str.isdigit(str(char)) for char in device):\n                    if dev == device:\n                        devices.append(device)\n                    else:\n                        configs['devices'].remove(device)\n    else:\n        d = configs['devices']\n        for device in d:\n            if any(str.isdigit(str(char)) for char in device):\n                devices.append(device)\n\n    # check if devices is still an empty list\n    if not devices:\n        status = (False, 'no valid devices match the requested device')\n        print(str(status))\n        return status\n\n    mem_needed = 0\n    dedicated = False\n    # need a gpu to itself\n    if ('dedicated' in configs['gpu_options'] and\n            configs['gpu_options']['dedicated'] == 'yes'):\n        dedicated = True\n    if 'mem_mb' in configs['gpu_options']:\n        # TODO input error checking\n        mem_needed = int(configs['gpu_options']['mem_mb'])\n\n    print('mem_needed: ', mem_needed)\n    print('dedicated: ', dedicated)\n    device = None\n    while not device:\n        usage = GpuUsage(base_dir=path_dir+'/', meta_dir=path_dir)\n\n        if usage[0]:\n            usage = usage[1]\n        else:\n            return usage\n        print(usage)\n        # {\"device\": \"0\",\n        #  \"mem_mb\": \"1024\",\n        #  \"dedicated\": \"yes\",\n        #  \"enabled\": \"yes\"}\n        for d in devices:\n            dev = str(d.split(':')[0].split('nvidia')[1])\n            print(dev)\n            # if the device is already dedicated, can't be used\n            dedicated_gpus = usage['vent_usage']['dedicated']\n            is_dedicated = False\n            for gpu in dedicated_gpus:\n                if dev in gpu:\n                    is_dedicated = True\n            print('is_dedicated: ', is_dedicated)\n            if not is_dedicated:\n                ram_used = 0\n                if dev in usage['vent_usage']['mem_mb']:\n                    ram_used = usage['vent_usage']['mem_mb'][dev]\n                # check for vent usage/processes running\n                if (dedicated and\n                    dev not in usage['vent_usage']['mem_mb'] and\n                    mem_needed <= usage[int(dev)]['global_memory'] and\n                        not usage[int(dev)]['processes']):\n                    device = dev\n                # check for ram constraints\n                elif mem_needed <= (usage[int(dev)]['global_memory'] - ram_used):\n                    device = dev\n\n        # TODO make this sleep incremental up to a point, potentially kill\n        #      after a set time configured from vent.cfg, outputting as it goes\n        time.sleep(1)\n\n    # lock jobs to a specific gpu (no shared GPUs for a single process) this is\n    # needed to calculate if memory requested (but not necessarily in use)\n    # would become oversubscribed\n\n    # store which device was mapped\n    options['labels']['vent.gpu.device'] = device\n    gpu_device = '/dev/nvidia' + device + ':/dev/nvidia' + device + ':rwm'\n    if 'devices' in configs:\n        d = configs['devices']\n        for dev in d:\n            if any(str.isdigit(str(char)) for char in dev):\n                if gpu_device != dev:\n                    configs['devices'].remove(dev)\n\n    try:\n        d_client = docker.from_env()\n        del options['configs']\n        del configs['gpu_options']\n        params = options.copy()\n        params.update(configs)\n        d_client.containers.run(**params)\n        status = (True, None)\n    except Exception as e:  # pragma: no cover\n        status = (False, str(e))\n\n    print(str(status))\n    return status"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef file_queue(path, template_path='/vent/', r_host='redis'):\n    import configparser\n    import ast\n    import docker\n    import json\n    import requests\n    import os\n    import sys\n    import time\n    import uuid\n\n    from redis import Redis\n    from rq import Queue\n    from subprocess import check_output, Popen, PIPE\n    from string import punctuation\n    from vent.helpers.logs import Logger\n\n    status = (True, None)\n    images = []\n    configs = {}\n    logger = Logger(__name__)\n\n    if (os.path.isfile('/root/.vent/vent.cfg') and os.path.isfile('/root/.vent/plugin_manifest.cfg')):\n        template_path = '/root/.vent/'\n\n    try:\n        d_client = docker.from_env()\n\n        # get the correct path for binding\n        vent_config = configparser.ConfigParser(interpolation=None)\n        vent_config.optionxform = str\n        vent_config.read(template_path+'vent.cfg')\n        if (vent_config.has_section('main') and\n                vent_config.has_option('main', 'files')):\n            files = vent_config.get('main', 'files')\n        else:\n            files = '/'\n\n        # deal with ~\n        files = os.path.expanduser(files)\n\n        chars = set(punctuation)\n        chars.discard('/')\n        chars.discard('_')\n        chars.discard('-')\n        file_name = ''\n        # escape any funky symbols to allow users FREEDOM of directory name\n        for char in files:\n            if char in chars:\n                if char == '\\\\':\n                    file_name += '\\\\' + char\n                else:\n                    file_name += '\\\\\\\\' + char\n            else:\n                file_name += char\n\n        files = file_name\n        _, path = path.split('_', 1)\n        directory = path.rsplit('/', 1)[0]\n        path = path.replace('/files', files, 1)\n        path_copy = path\n\n        # read in configuration of plugins to get the ones that should run\n        # against the path.\n        # keep track of images that failed getting configurations for\n        failed_images = set()\n        config = configparser.ConfigParser(interpolation=None)\n        config.optionxform = str\n        print('Path to manifest: ' + template_path+'plugin_manifest.cfg')\n        config.read(template_path+'plugin_manifest.cfg')\n        sections = config.sections()\n        name_maps = {}\n        orig_path_d = {}\n        path_cmd = {}\n        labels_d = {}\n\n        # get all name maps\n        for section in sections:\n            link_name = config.get(section, 'link_name')\n            image_name = config.get(section, 'image_name')\n            name_maps[link_name] = image_name.replace(\n                ':', '-').replace('/', '-')\n\n        for section in sections:\n            path = path_copy\n            orig_path = ''\n            repo = config.get(section, 'repo')\n            t_type = config.get(section, 'type')\n            labels = {'vent-plugin': '', 'file': path,\n                      'vent.section': section, 'vent.repo': repo, 'vent.type': t_type}\n            image_name = config.get(section, 'image_name')\n            link_name = config.get(section, 'link_name')\n            # doesn't matter if it's a repository or registry because both in manifest\n            if config.has_option(section, 'groups'):\n                if 'replay' in config.get(section, 'groups'):\n                    try:\n                        # read the vent.cfg file to grab the network-mapping\n                        # specified. For replay_pcap\n                        n_name = 'network-mapping'\n                        n_map = []\n                        if vent_config.has_section(n_name):\n                            # make sure that the options aren't empty\n                            if vent_config.options(n_name):\n                                options = vent_config.options(n_name)\n                                for option in options:\n                                    if vent_config.get(n_name, option):\n                                        n_map.append(vent_config.get(\n                                            n_name, option))\n                                orig_path = path\n                                path = str(n_map[0]) + ' ' + path\n                    except Exception as e:  # pragma: no cover\n                        failed_images.add(image_name)\n                        status = (False, str(e))\n            if config.has_option(section, 'service'):\n                try:\n                    options_dict = json.loads(config.get(section, 'service'))\n                    for option in options_dict:\n                        value = options_dict[option]\n                        labels[option] = value\n                except Exception as e:   # pragma: no cover\n                    failed_images.add(image_name)\n                    status = (False, str(e))\n            if config.has_option(section, 'settings'):\n                try:\n                    options_dict = json.loads(config.get(section, 'settings'))\n                    in_base = directory == '/files'\n                    # process base by default\n                    process_file = in_base\n                    # check if this tool shouldn't process the base by default\n                    if 'process_base' in options_dict:\n                        if options_dict['process_base'] == 'no':\n                            process_file = False\n                    # check if this tool should look at subdirs created by\n                    # other tools' output\n                    if 'process_from_tool' in options_dict and not in_base:\n                        for tool in options_dict['process_from_tool'].split(','):\n                            dir_pieces = directory.split('/')\n                            dir_check = dir_pieces\n                            for dir_piece in dir_pieces:\n                                if 'UTC' in dir_piece:\n                                    dir_check = dir_piece\n                            if tool.replace(' ', '-') in dir_check:\n                                process_file = True\n                    if 'ext_types' in options_dict and process_file:\n                        ext_types = options_dict['ext_types'].split(',')\n                        for ext_type in ext_types:\n                            if path.endswith(ext_type):\n                                images.append(image_name)\n                                configs[image_name] = {}\n\n                except Exception as e:   # pragma: no cover\n                    failed_images.add(image_name)\n                    status = (False, str(e))\n            if image_name in configs:\n                if config.has_option(section, 'docker'):\n                    try:\n                        options_dict = ast.literal_eval(\n                            config.get(section, 'docker'))\n                        for option in options_dict:\n                            try:\n                                configs[image_name][option] = ast.literal_eval(\n                                    options_dict[option])\n                            except Exception as e:  # pragma: no cover\n                                configs[image_name][option] = options_dict[option]\n                        if 'links' in configs[image_name]:\n                            for link in configs[image_name]['links']:\n                                if link in name_maps:\n                                    configs[image_name]['links'][name_maps[link]\n                                                                 ] = configs[image_name]['links'].pop(link)\n                        # TODO network_mode\n                        # TODO volumes_from\n                        # TODO external services\n                    except Exception as e:   # pragma: no cover\n                        failed_images.add(image_name)\n                        status = (False, str(e))\n\n            if config.has_option(section, 'gpu') and image_name in configs:\n                try:\n                    options_dict = json.loads(config.get(section, 'gpu'))\n                    if 'enabled' in options_dict:\n                        enabled = options_dict['enabled']\n                        if enabled == 'yes':\n                            configs[image_name]['gpu_options'] = options_dict\n                            labels['vent.gpu'] = 'yes'\n                            if 'dedicated' in options_dict:\n                                labels['vent.gpu.dedicated'] = options_dict['dedicated']\n                            if 'device' in options_dict:\n                                labels['vent.gpu.device'] = options_dict['device']\n                            if 'mem_mb' in options_dict:\n                                labels['vent.gpu.mem_mb'] = options_dict['mem_mb']\n                            port = ''\n                            host = ''\n                            if (vent_config.has_section('nvidia-docker-plugin') and\n                                    vent_config.has_option('nvidia-docker-plugin', 'port')):\n                                port = vent_config.get(\n                                    'nvidia-docker-plugin', 'port')\n                            else:\n                                port = '3476'\n                            if (vent_config.has_section('nvidia-docker-plugin') and\n                                    vent_config.has_option('nvidia-docker-plugin', 'host')):\n                                host = vent_config.get(\n                                    'nvidia-docker-plugin', 'host')\n                            else:\n                                # grab the default gateway\n                                try:\n                                    route = Popen(('/sbin/ip', 'route'),\n                                                  stdout=PIPE)\n                                    host = check_output(('awk', '/default/ {print$3}'),\n                                                        stdin=route.stdout).strip().decode('utf-8')\n                                    route.wait()\n                                except Exception as e:  # pragma no cover\n                                    logger.error('Default gateway '\n                                                 'went wrong ' + str(e))\n                            nd_url = 'http://' + host + ':' + port + '/v1.0/docker/cli'\n                            params = {'vol': 'nvidia_driver'}\n                            try:\n                                r = requests.get(nd_url, params=params)\n                                if r.status_code == 200:\n                                    options = r.text.split()\n                                    for option in options:\n                                        if option.startswith('--volume-driver='):\n                                            configs[image_name]['volume_driver'] = option.split('=', 1)[\n                                                1]\n                                        elif option.startswith('--volume='):\n                                            vol = option.split('=', 1)[\n                                                1].split(':')\n                                            if 'volumes' in configs[image_name]:\n                                                # !! TODO handle if volumes is a list\n                                                configs[image_name]['volumes'][vol[0]] = {'bind': vol[1],\n                                                                                          'mode': vol[2]}\n                                            else:\n                                                configs[image_name]['volumes'] = {vol[0]:\n                                                                                  {'bind': vol[1],\n                                                                                   'mode': vol[2]}}\n                                        elif option.startswith('--device='):\n                                            dev = option.split('=', 1)[1]\n                                            if 'devices' in configs[image_name]:\n                                                configs[image_name]['devices'].append(dev +\n                                                                                      ':' +\n                                                                                      dev +\n                                                                                      ':rwm')\n                                            else:\n                                                configs[image_name]['devices'] = [\n                                                    dev + ':' + dev + ':rwm']\n                                        else:\n                                            # unable to parse option provided by\n                                            # nvidia-docker-plugin\n                                            pass\n                            except Exception as e:  # pragma: no cover\n                                failed_images.add(image_name)\n                                status = (False, str(e))\n                                print('Failure with nvidia-docker-plugin: ' +\n                                      str(e))\n                except Exception as e:   # pragma: no cover\n                    failed_images.add(image_name)\n                    status = (False, str(e))\n                    print('Unable to process gpu options: ' + str(e))\n            path_cmd[image_name] = path\n            orig_path_d[image_name] = orig_path\n            labels_d[image_name] = labels\n\n        # TODO get syslog address rather than hardcode\n        # TODO add group label\n        # TODO get group and name for syslog tag\n        log_config = {'type': 'syslog',\n                      'config': {'syslog-address': 'tcp://0.0.0.0:514',\n                                 'syslog-facility': 'daemon',\n                                 'tag': '{{.Name}}'}}\n\n        # setup gpu queue\n        can_queue_gpu = True\n        try:\n            q = Queue(connection=Redis(host=r_host), default_timeout=86400)\n        except Exception as e:  # pragma: no cover\n            can_queue_gpu = False\n            print('Unable to connect to redis: ' + str(e))\n\n        # start containers\n        for image in images:\n            if image not in failed_images:\n                orig_path = orig_path_d[image]\n                labels = labels_d[image]\n                configs[image]['remove'] = True\n                name = image.replace('/', '-').replace(':', '-') + '_' + \\\n                    str(int(time.time()))+'_'+str(uuid.uuid4())[:4]\n\n                if orig_path:\n                    # replay_pcap is special so we can't bind it like normal\n                    # since the plugin takes in an additional argument\n                    dir_path = orig_path.rsplit('/', 1)[0]\n                else:\n                    dir_path = path.rsplit('/', 1)[0]\n                volumes = {dir_path: {'bind': dir_path, 'mode': 'rw'}}\n\n                if 'volumes' in configs[image]:\n                    for volume in volumes:\n                        configs[image]['volumes'][volume] = volumes[volume]\n                else:\n                    configs[image]['volumes'] = volumes\n\n                command = path_cmd[image]\n                if 'command' in configs[image]:\n                    command = configs[image]['command'] + ' ' + command\n                    del configs[image]['command']\n\n                if 'vent.gpu' in labels and labels['vent.gpu'] == 'yes':\n                    if can_queue_gpu:\n                        # queue up containers requiring a gpu\n                        q_str = json.dumps({'image': image,\n                                            'command': command,\n                                            'labels': labels,\n                                            'detach': True,\n                                            'name': name,\n                                            'log_config': log_config,\n                                            'configs': configs[image]})\n                        q.enqueue('watch.gpu_queue', q_str, ttl=2592000)\n                    else:\n                        failed_images.add(image)\n                else:\n                    if 'gpu_options' in configs[image]:\n                        del configs[image]['gpu_options']\n                    d_client.containers.run(image=image,\n                                            command=command,\n                                            labels=labels,\n                                            detach=True,\n                                            name=name,\n                                            log_config=log_config,\n                                            **configs[image])\n        if failed_images:\n            status = (False, failed_images)\n        else:\n            status = (True, images)\n    except Exception as e:  # pragma: no cover\n        status = (False, str(e))\n        print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno))\n        print('Failed to process job: ' + str(e))\n\n    print(str(status))\n    return status", "response": "Process files that have been added from the rq - worker and starts plugins and queues them for processing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(self, func=lambda x: x):\n        return Enumerable(itertools.imap(func, self))", "response": "Transforms data into different form\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the minimum value of data elements", "response": "def min(self, func=lambda x: x):\n        \"\"\"\n        Returns the min value of data elements\n        :param func: lambda expression to transform data\n        :return: minimum value\n        \"\"\"\n        if self.count() == 0:\n            raise NoElementsError(u\"Iterable contains no elements\")\n        return min(self.select(func))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef max(self, func=lambda x: x):\n        if self.count() == 0:\n            raise NoElementsError(u\"Iterable contains no elements\")\n        return max(self.select(func))", "response": "Returns the max value of data elements\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avg(self, func=lambda x: x):\n        count = self.count()\n        if count == 0:\n            raise NoElementsError(u\"Iterable contains no elements\")\n        return float(self.sum(func)) / float(count)", "response": "Returns the average value of data elements in the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the median value of data elements in this collection.", "response": "def median(self, func=lambda x: x):\n        \"\"\"\n        Return the median value of data elements\n        :param func: lambda expression to project and sort data\n        :return: median value\n        \"\"\"\n        if self.count() == 0:\n            raise NoElementsError(u\"Iterable contains no elements\")\n        result = self.order_by(func).select(func).to_list()\n        length = len(result)\n        i = int(length / 2)\n        return result[i] if length % 2 == 1 else (float(result[i - 1]) + float(result[i])) / float(2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef element_at(self, n):\n        result = list(itertools.islice(self.to_list(), max(0, n), n + 1, 1))\n        if len(result) == 0:\n            raise NoElementsError(u\"No element found at index {0}\".format(n))\n        return result[0]", "response": "Returns element at given index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn new Enumerable sorted in ascending order by given key", "response": "def order_by(self, key):\n        \"\"\"\n        Returns new Enumerable sorted in ascending order by given key\n        :param key: key to sort by as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        if key is None:\n            raise NullArgumentError(u\"No key for sorting given\")\n        kf = [OrderingDirection(key, reverse=False)]\n        return SortedEnumerable(key_funcs=kf, data=self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef order_by_descending(self, key):\n        if key is None:\n            raise NullArgumentError(u\"No key for sorting given\")\n        kf = [OrderingDirection(key, reverse=True)]\n        return SortedEnumerable(key_funcs=kf, data=self._data)", "response": "Returns new Enumerable sorted in descending order by given key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef where(self, predicate):\n        if predicate is None:\n            raise NullArgumentError(\"No predicate given for where clause\")\n        return Enumerable(itertools.ifilter(predicate, self))", "response": "Returns new Enumerable where elements matching predicate are selected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn single element that matches given predicate.", "response": "def single(self, predicate):\n        \"\"\"\n        Returns single element that matches given predicate.\n        Raises:\n            * NoMatchingElement error if no matching elements are found\n            * MoreThanOneMatchingElement error if more than one matching\n            element is found\n        :param predicate: predicate as a lambda expression\n        :return: Matching element as object\n        \"\"\"\n        result = self.where(predicate).to_list()\n        count = len(result)\n        if count == 0:\n            raise NoMatchingElement(\"No matching element found\")\n        if count > 1:\n            raise MoreThanOneMatchingElement(\n                \"More than one matching element found. Use where instead\"\n            )\n        return result[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd enumerable to an enumerable object.", "response": "def concat(self, enumerable):\n        \"\"\"\n        Adds enumerable to an enumerable\n        :param enumerable: An iterable object\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(enumerable, Enumerable):\n            raise TypeError(\n                u\"enumerable argument must be an instance of Enumerable\"\n            )\n        return Enumerable(itertools.chain(self._data, enumerable.data))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngrouping an enumerable on given key selector. Index of key name corresponds to index of key lambda function.", "response": "def group_by(self, key_names=[], key=lambda x: x, result_func=lambda x: x):\n        \"\"\"\n        Groups an enumerable on given key selector. Index of key name\n        corresponds to index of key lambda function.\n\n        Usage:\n            Enumerable([1,2,3]).group_by(key_names=['id'], key=lambda x: x) _\n                .to_list() -->\n                Enumerable object [\n                    Grouping object {\n                        key.id: 1,\n                        _data: [1]\n                    },\n                    Grouping object {\n                        key.id: 2,\n                        _data: [2]\n                    },\n                    Grouping object {\n                        key.id: 3,\n                        _data: [3]\n                    }\n                ]\n            Thus the key names for each grouping object can be referenced\n            through the key property. Using the above example:\n\n            Enumerable([1,2,3]).group_by(key_names=['id'], key=lambda x: x) _\n            .select(lambda g: { 'key': g.key.id, 'count': g.count() }\n\n        :param key_names: list of key names\n        :param key: key selector as lambda expression\n        :param result_func: transformation function as lambda expression\n        :return: Enumerable of grouping objects\n        \"\"\"\n        result = []\n        ordered = sorted(self, key=key)\n        grouped = itertools.groupby(ordered, key)\n        for k, g in grouped:\n            can_enumerate = isinstance(k, list) or isinstance(k, tuple) \\\n                and len(k) > 0\n            key_prop = {}\n            for i, prop in enumerate(key_names):\n                key_prop.setdefault(prop, k[i] if can_enumerate else k)\n            key_object = Key(key_prop)\n            result.append(Grouping(key_object, list(g)))\n        return Enumerable(result).select(result_func)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an enumerable containing elements that are distinct based on given key selector", "response": "def distinct(self, key=lambda x: x):\n        \"\"\"\n        Returns enumerable containing elements that are distinct based on\n        given key selector\n        :param key: key selector as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        return Enumerable(self.group_by(key=key).select(lambda g: g.first()).to_list())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join(\n            self,\n            inner_enumerable,\n            outer_key=lambda x: x,\n            inner_key=lambda x: x,\n            result_func=lambda x: x\n    ):\n        \"\"\"\n        Return enumerable of inner equi-join between two enumerables\n        :param inner_enumerable: inner enumerable to join to self\n        :param outer_key: key selector of outer enumerable as lambda expression\n        :param inner_key: key selector of inner enumerable as lambda expression\n        :param result_func: lambda expression to transform result of join\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(inner_enumerable, Enumerable):\n            raise TypeError(\n                u\"inner_enumerable parameter must be an instance of Enumerable\"\n            )\n        return Enumerable(\n            itertools.product(\n                itertools.ifilter(\n                    lambda x: outer_key(x) in itertools.imap(\n                        inner_key,\n                        inner_enumerable\n                    ), self\n                ),\n                itertools.ifilter(\n                    lambda y: inner_key(y) in itertools.imap(\n                        outer_key,\n                        self),\n                    inner_enumerable\n                )\n            )\n        )\\\n            .where(lambda x: outer_key(x[0]) == inner_key(x[1])) \\\n            .select(\n            result_func)", "response": "Return enumerable of inner equi - join between two enumerables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_join(\n            self,\n            inner_enumerable,\n            outer_key=lambda x: x,\n            inner_key=lambda x: x,\n            result_func=lambda x: x\n    ):\n        \"\"\"\n        Return enumerable of group join between two enumerables\n        :param inner_enumerable: inner enumerable to join to self\n        :param outer_key: key selector of outer enumerable as lambda expression\n        :param inner_key: key selector of inner enumerable as lambda expression\n        :param result_func: lambda expression to transform the result of group\n        join\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(inner_enumerable, Enumerable):\n            raise TypeError(\n                u\"inner enumerable parameter must be an instance of Enumerable\"\n            )\n        return Enumerable(\n            itertools.product(\n                self,\n                inner_enumerable.default_if_empty()\n            )\n        ).group_by(\n            key_names=['id'],\n            key=lambda x: outer_key(x[0]),\n            result_func=lambda g: (\n                g.first()[0],\n                g.where(\n                    lambda x: inner_key(x[1]) == g.key.id).select(\n                        lambda x: x[1]\n                )\n            )\n        ).select(result_func)", "response": "Return enumerable of group join between two enumerables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns true if any elements that satisfy predicate are found", "response": "def any(self, predicate):\n        \"\"\"\n        Returns true if any elements that satisfy predicate are found\n        :param predicate: condition to satisfy as lambda expression\n        :return: boolean True or False\n        \"\"\"\n        if predicate is None:\n            raise NullArgumentError(\n                u\"predicate lambda expression is necessary\")\n        return self.where(predicate).count() > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggregate(self, func, seed=None):\n        if self.count() == 0:\n            raise NoElementsError(\"No elements perform aggregation\")\n        result = seed if seed is not None else self.first()\n        for i, e in enumerate(self):\n            if i == 0 and seed is None:\n                continue\n            result = func(result, e)\n        return result", "response": "Perform a function over every element of the enumerable and return the result of the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an enumerable that is a union of elements between self and given", "response": "def union(self, enumerable, key=lambda x: x):\n        \"\"\"\n        Returns enumerable that is a union of elements between self and given\n        enumerable\n        :param enumerable: enumerable to union self to\n        :param key: key selector used to determine uniqueness\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(enumerable, Enumerable):\n            raise TypeError(\n                u\"enumerable parameter must be an instance of Enumerable\")\n        return self.concat(enumerable).distinct(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contains(self, element, key=lambda x: x):\n        return self.select(key).any(lambda x: x == key(element))", "response": "Returns True if element is found in enumerable otherwise False\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zip(self, enumerable, func):\n        if not isinstance(enumerable, Enumerable):\n            raise TypeError()\n        return Enumerable(itertools.izip(self, enumerable)).select(lambda x: func(x))", "response": "Returns a new Enumerable with the elements from the given enumerable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef then_by(self, func):\n        if func is None:\n            raise NullArgumentError(u\"then by requires a lambda function arg\")\n        self._key_funcs.append(OrderingDirection(key=func, reverse=False))\n        return SortedEnumerable(self._key_funcs, self._data)", "response": "Return a new SortedEnumerable instance with the elements sorted by the given function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new instance with the elements in descending order.", "response": "def then_by_descending(self, func):\n        \"\"\"\n        Subsequent sorting function in descending order\n        :param func: lambda function for secondary sort key\n        :return: SortedEnumerable instance\n        \"\"\"\n        if func is None:\n            raise NullArgumentError(\n                u\"then_by_descending requires a lambda function arg\")\n        self._key_funcs.append(OrderingDirection(key=func, reverse=True))\n        return SortedEnumerable(self._key_funcs, self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inventory(self, choices=None):\n        status = (True, None)\n        if not choices:\n            return (False, 'No choices made')\n        try:\n            # choices: repos, tools, images, built, running, enabled\n            items = {'repos': [], 'tools': {}, 'images': {},\n                     'built': {}, 'running': {}, 'enabled': {}}\n\n            tools = Template(self.manifest).list_tools()\n            for choice in choices:\n                for tool in tools:\n                    try:\n                        if choice == 'repos':\n                            if 'repo' in tool:\n                                if (tool['repo'] and\n                                        tool['repo'] not in items[choice]):\n                                    items[choice].append(tool['repo'])\n                        elif choice == 'tools':\n                            items[choice][tool['section']] = tool['name']\n                        elif choice == 'images':\n                            # TODO also check against docker\n                            items[choice][tool['section']] = tool['image_name']\n                        elif choice == 'built':\n                            items[choice][tool['section']] = tool['built']\n                        elif choice == 'running':\n                            containers = Containers()\n                            status = 'not running'\n                            for container in containers:\n                                image_name = tool['image_name'] \\\n                                    .rsplit(':' +\n                                            tool['version'], 1)[0]\n                                image_name = image_name.replace(':', '-')\n                                image_name = image_name.replace('/', '-')\n                                self.logger.info('image_name: ' + image_name)\n                                if container[0] == image_name:\n                                    status = container[1]\n                                elif container[0] == image_name + \\\n                                        '-' + tool['version']:\n                                    status = container[1]\n                            items[choice][tool['section']] = status\n                        elif choice == 'enabled':\n                            items[choice][tool['section']] = tool['enabled']\n                        else:\n                            # unknown choice\n                            pass\n                    except Exception as e:  # pragma: no cover\n                        self.logger.error('Unable to grab info about tool: ' +\n                                          str(tool) + ' because: ' + str(e))\n            status = (True, items)\n        except Exception as e:  # pragma: no cover\n            self.logger.error(\n                'Inventory failed with error: {0}'.format(str(e)))\n            status = (False, str(e))\n        return status", "response": "Return a dictionary of the inventory items and status"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _start_priority_containers(self, groups, group_orders, tool_d):\n        vent_cfg = Template(self.path_dirs.cfg_file)\n        cfg_groups = vent_cfg.option('groups', 'start_order')\n        if cfg_groups[0]:\n            cfg_groups = cfg_groups[1].split(',')\n        else:\n            cfg_groups = []\n        all_groups = sorted(set(groups))\n        s_conts = []\n        f_conts = []\n        # start tools in order of group defined in vent.cfg\n        for group in cfg_groups:\n            # remove from all_groups because already checked out\n            if group in all_groups:\n                all_groups.remove(group)\n            if group in group_orders:\n                for cont_t in sorted(group_orders[group]):\n                    if cont_t[1] not in s_conts:\n                        s_conts, f_conts = self._start_container(cont_t[1],\n                                                                 tool_d,\n                                                                 s_conts,\n                                                                 f_conts)\n        # start tools that haven't been specified in the vent.cfg, if any\n        for group in all_groups:\n            if group in group_orders:\n                for cont_t in sorted(group_orders[group]):\n                    if cont_t[1] not in s_conts:\n                        s_conts, f_conts = self._start_container(cont_t[1],\n                                                                 tool_d,\n                                                                 s_conts,\n                                                                 f_conts)\n        return (s_conts, f_conts)", "response": "Select containers based on priorities to start"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect remaining containers that didn t have priorities to start", "response": "def _start_remaining_containers(self, containers_remaining, tool_d):\n        \"\"\"\n        Select remaining containers that didn't have priorities to start\n        \"\"\"\n        s_containers = []\n        f_containers = []\n        for container in containers_remaining:\n            s_containers, f_containers = self._start_container(container,\n                                                               tool_d,\n                                                               s_containers,\n                                                               f_containers)\n        return (s_containers, f_containers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a container that was passed in and return status", "response": "def _start_container(self, container, tool_d, s_containers, f_containers):\n        \"\"\" Start container that was passed in and return status \"\"\"\n        # use section to add info to manifest\n        section = tool_d[container]['section']\n        del tool_d[container]['section']\n        manifest = Template(self.manifest)\n        try:\n            # try to start an existing container first\n            c = self.d_client.containers.get(container)\n            c.start()\n            s_containers.append(container)\n            manifest.set_option(section, 'running', 'yes')\n            self.logger.info('started ' + str(container) +\n                             ' with ID: ' + str(c.short_id))\n        except Exception as err:\n            s_containers, f_containers = self._run_container(\n                container, tool_d, section, s_containers, f_containers)\n\n        # save changes made to manifest\n        manifest.write_config()\n        return s_containers, f_containers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the commit IDs for all of the branches of a repository", "response": "def repo_commits(self, repo):\n        \"\"\" Get the commit IDs for all of the branches of a repository \"\"\"\n        commits = []\n        try:\n            status = self.path_dirs.apply_path(repo)\n            # switch to directory where repo will be cloned to\n            if status[0]:\n                cwd = status[1]\n            else:\n                self.logger.error('apply_path failed. Exiting repo_commits with'\n                                  ' status: ' + str(status))\n                return status\n\n            status = self.repo_branches(repo)\n            if status[0]:\n                branches = status[1]\n                for branch in branches:\n                    try:\n                        branch_output = check_output(shlex\n                                                     .split('git rev-list origin/' +\n                                                            branch),\n                                                     stderr=STDOUT,\n                                                     close_fds=True).decode('utf-8')\n                        branch_output = branch_output.split('\\n')[:-1]\n                        branch_output += ['HEAD']\n                        commits.append((branch, branch_output))\n                    except Exception as e:  # pragma: no cover\n                        self.logger.error('repo_commits failed with error: ' +\n                                          str(e) + ' on branch: ' +\n                                          str(branch))\n                        status = (False, e)\n                        return status\n            else:\n                self.logger.error('repo_branches failed. Exiting repo_commits'\n                                  ' with status: ' + str(status))\n                return status\n\n            chdir(cwd)\n            status = (True, commits)\n        except Exception as e:  # pragma: no cover\n            self.logger.error('repo_commits failed with error: ' + str(e))\n            status = (False, e)\n\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the branches of a repository", "response": "def repo_branches(self, repo):\n        \"\"\" Get the branches of a repository \"\"\"\n        branches = []\n        try:\n            # switch to directory where repo will be cloned to\n            status = self.path_dirs.apply_path(repo)\n            if status[0]:\n                cwd = status[1]\n            else:\n                self.logger.error('apply_path failed. Exiting repo_branches'\n                                  ' with status ' + str(status))\n                return status\n\n            branch_output = check_output(shlex.split('git branch -a'),\n                                         stderr=STDOUT,\n                                         close_fds=True)\n            branch_output = branch_output.split(b'\\n')\n            for branch in branch_output:\n                br = branch.strip()\n                if br.startswith(b'*'):\n                    br = br[2:]\n                if b'/' in br:\n                    branches.append(br.rsplit(b'/', 1)[1].decode('utf-8'))\n                elif br:\n                    branches.append(br.decode('utf-8'))\n\n            branches = list(set(branches))\n            for branch in branches:\n                try:\n                    check_output(shlex.split('git checkout ' + branch),\n                                 stderr=STDOUT,\n                                 close_fds=True)\n                except Exception as e:  # pragma: no cover\n                    self.logger.error('repo_branches failed with error: ' +\n                                      str(e) + ' on branch: ' + str(branch))\n                    status = (False, e)\n                    return status\n\n            chdir(cwd)\n            status = (True, branches)\n        except Exception as e:  # pragma: no cover\n            self.logger.error('repo_branches failed with error: ' + str(e))\n            status = (False, e)\n\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting available tools for a repository branch at a version", "response": "def repo_tools(self, repo, branch, version):\n        \"\"\" Get available tools for a repository branch at a version \"\"\"\n        try:\n            tools = []\n            status = self.path_dirs.apply_path(repo)\n            # switch to directory where repo will be cloned to\n            if status[0]:\n                cwd = status[1]\n            else:\n                self.logger.error('apply_path failed. Exiting repo_tools with'\n                                  ' status: ' + str(status))\n                return status\n\n            # TODO commenting out for now, should use update_repo\n            #status = self.p_helper.checkout(branch=branch, version=version)\n            status = (True, None)\n\n            if status[0]:\n                path, _, _ = self.path_dirs.get_path(repo)\n                tools = AvailableTools(path, version=version)\n            else:\n                self.logger.error('checkout failed. Exiting repo_tools with'\n                                  ' status: ' + str(status))\n                return status\n\n            chdir(cwd)\n            status = (True, tools)\n        except Exception as e:  # pragma: no cover\n            self.logger.error('repo_tools failed with error: ' + str(e))\n            status = (False, e)\n\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self):\n        self.add_handlers({'^T': self.change_forms, '^Q': self.exit})\n        self.addfield = self.add(npyscreen.TitleFixedText, name='Vent',\n                                 labelColor='DEFAULT', editable=False)\n        self.multifield1 = self.add(npyscreen.MultiLineEdit, editable=False,\n                                    value=\"\"\"\n        About Vent\n\n        Vent is a library that includes a CLI designed to serve as a general\n        platform for analyzing network traffic. Built with some basic\n        functionality, Vent serves as a user-friendly platform to build custom\n        plugins on to perform user-defined processing on incoming network data.\n        Vent supports any filetype, but only processes ones based on the types\n        of plugins installed for that instance of Vent. Simply create your\n        plugins, point vent to them & install them, and drop a file in vent to\n        begin processing!\n\n        For a detailed explanation of Vent Concepts, check out the General\n        section in our Help Menu. Topics include: Vent Plugins, Tools,\n        Filetypes, and Statuses! Use ^X to access the menu and ESC to\n        close it.\n\n        Select CANCEL or ^Q to return to the Main Menu. Select OK or ^T to\n        return to your previous menu.\n\n        PRO TIP: You can use TAB to cycle through options.\n        \"\"\")\n        self.m2 = self.add_menu(name='Vent Basics', shortcut='b')\n        self.m2.addItem(text='Menu Interactions', onSelect=HelpForm.switch,\n                        arguments=['Menu'], shortcut='m')\n        self.m2.addItem(text='Plugins', onSelect=HelpForm.switch,\n                        arguments=['Plugins'], shortcut='p')\n        self.m2.addItem(text='Tools', onSelect=HelpForm.switch,\n                        arguments=['Tools'], shortcut='t')\n        self.m2.addItem(text='Filetypes', onSelect=HelpForm.switch,\n                        arguments=['Filetypes'], shortcut='f')\n        self.m2.addItem(text='Statuses', onSelect=HelpForm.switch,\n                        arguments=['Status'], shortcut='s')\n        self.m3 = self.add_menu(name='Working with Plugins', shortcut='p')\n        self.m3.addItem(text='Adding a Plugin', onSelect=HelpForm.switch,\n                        arguments=['Plugin Adding'], shortcut='a')\n        self.m3.addItem(text='Building a Plugin', onSelect=HelpForm.switch,\n                        arguments=['Plugin Building'], shortcut='b')", "response": "Override method for creating FormBaseNew form"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nswitch the current forms to the other one.", "response": "def change_forms(self, *args, **keywords):\n        \"\"\"\n        Checks which form is currently displayed and toggles to the other one\n        \"\"\"\n        # Returns to previous Form in history if there is a previous Form\n        try:\n            self.parentApp.switchFormPrevious()\n        except Exception as e:  # pragma: no cover\n            self.parentApp.switchForm('MAIN')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bumpversion(self, part, commit=True, tag=False, message=None,\n                    allow_dirty=False):\n        \"\"\" Run bumpversion.main() with the specified arguments, and return the\n        new computed version string.\n        \"\"\"\n        import bumpversion\n\n        args = (\n            (['--verbose'] if self.verbose > 1 else []) +\n            (['--allow-dirty'] if allow_dirty else []) +\n            (['--commit'] if commit else ['--no-commit']) +\n            (['--tag'] if tag else ['--no-tag']) +\n            (['--message', message] if message is not None else []) +\n            ['--list', part]\n        )\n        log.debug(\n            \"$ bumpversion %s\" % \" \".join(a.replace(\" \", \"\\\\ \") for a in args))\n\n        with capture_logger(\"bumpversion.list\") as out:\n            bumpversion.main(args)\n\n        last_line = out.getvalue().splitlines()[-1]\n        new_version = last_line.replace(\"new_version=\", \"\")\n        return new_version", "response": "Run bumpversion. main and return the new version string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef buildMutator(items, axes=None, bias=None):\n    from mutatorMath.objects.bender import Bender\n    items = [(Location(loc),obj) for loc, obj in items]\n    m = Mutator()\n    if axes is not None:\n        bender = Bender(axes)\n        m.setBender(bender)\n    else:\n        bender = noBend\n    # the order itself does not matter, but we should always build in the same order.\n    items = sorted(items)\n    if not bias:\n        bias = biasFromLocations([bender(loc) for loc, obj in items], True)\n    else:\n        # note: this means that the actual bias might be different from the initial value. \n        bias = bender(bias)\n    m.setBias(bias)\n    n = None\n    ofx = []\n    onx = []\n    for loc, obj in items:\n        loc = bender(loc)\n        if (loc-bias).isOrigin():\n            m.setNeutral(obj)\n            break\n    if m.getNeutral() is None:\n        raise MutatorError(\"Did not find a neutral for this system\", m)\n    for loc, obj in items:\n        locbent = bender(loc)\n        lb = locbent-bias\n        if lb.isOrigin(): continue\n        if lb.isOnAxis():\n            onx.append((lb, obj-m.getNeutral()))\n        else:\n            ofx.append((lb, obj-m.getNeutral()))\n    for loc, obj in onx:\n        m.addDelta(loc, obj, punch=False,  axisOnly=True)\n    for loc, obj in ofx:\n        m.addDelta(loc, obj, punch=True,  axisOnly=True)\n    return bias, m", "response": "Builds a mutator for the given set of items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getLimits(locations, current, sortResults=True, verbose=False):\n    limit = {}\n    for l in locations:\n        a, b = current.common(l)\n        if a is None:\n            continue\n        for name, value in b.items():\n            f = a[name]\n            if name not in limit:\n                limit[name] = {}\n                limit[name]['<'] = {}\n                limit[name]['='] = {}\n                limit[name]['>'] = {}\n                if f > 0:\n                    limit[name]['>'] = {0: [Location()]}\n                elif f<0:\n                    limit[name]['<'] = {0: [Location()]}\n                else:\n                    limit[name]['='] = {0: [Location()]}\n            if current[name] < value - _EPSILON:\n                if value not in limit[name][\"<\"]:\n                    limit[name][\"<\"][value] = []\n                limit[name][\"<\"][value].append(l)\n            elif current[name] > value + _EPSILON:\n                if value not in limit[name][\">\"]:\n                    limit[name][\">\"][value] = []\n                limit[name][\">\"][value].append(l)\n            else:\n                if value not in limit[name][\"=\"]:\n                    limit[name][\"=\"][value] = []\n                limit[name][\"=\"][value].append(l)\n    if not sortResults:\n        return limit\n    # now we have all the data, let's sort to the relevant values\n    l = {}\n    for name, lims in  limit.items():\n        less = []\n        more = []\n        if lims[\">\"].keys():\n            less = sorted(lims[\">\"].keys())\n            lim_min = less[-1]\n        else:\n            lim_min = None\n        if lims[\"<\"].keys():\n            more = sorted(lims[\"<\"].keys())\n            lim_max = more[0]\n        else:\n            lim_max = None\n        if lim_min is None and lim_max is not None:\n            # extrapolation < min\n            if len(limit[name]['='])>0:\n                l[name] = (None, list(limit[name]['='].keys())[0], None)\n            elif len(more) > 1 and len(limit[name]['='])==0:\n                # extrapolation\n                l[name] = (None,  more[0], more[1])\n        elif lim_min is not None and lim_max is None:\n            # extrapolation < max\n            if len(limit[name]['='])>0:\n                # less > 0, M > 0, more = None\n                # -> end of a limit\n                l[name] = (None, limit[name]['='], None)\n            elif len(less) > 1 and len(limit[name]['='])==0:\n                # less > 0, M = None, more = None\n                # extrapolation\n                l[name] = (less[-2], less[-1], None)\n        else:\n            if len(limit[name]['=']) > 0:\n                l[name] = (None, list(limit[name]['='].keys())[0], None)\n            else:\n                l[name] = (lim_min,  None, lim_max)\n    return l", "response": "Get the limits for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setNeutral(self, aMathObject, deltaName=\"origin\"):\n        self._neutral = aMathObject\n        self.addDelta(Location(), aMathObject-aMathObject, deltaName, punch=False, axisOnly=True)", "response": "Set the neutral object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a delta at a specific location.", "response": "def addDelta(self, location, aMathObject, deltaName = None, punch=False,  axisOnly=True):\n        \"\"\" Add a delta at this location.\n            *   location:   a Location object\n            *   mathObject: a math-sensitive object\n            *   deltaName: optional string/token\n            *   punch:\n                *   True: add the difference with the instance value at that location and the delta\n                *   False: just add the delta.\n        \"\"\"\n        #location = self._bender(location)\n        if punch:\n            r = self.getInstance(location, axisOnly=axisOnly)\n            if r is not None:\n                self[location.asTuple()] = aMathObject-r, deltaName\n            else:\n                raise MutatorError(\"Could not get instance.\")\n        else:\n            self[location.asTuple()] = aMathObject, deltaName"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getAxisNames(self):\n        s = {}\n        for l, x in self.items():\n            s.update(dict.fromkeys([k for k, v in l], None))\n        return set(s.keys())", "response": "Collect a set of axis names from all deltas."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting all on - axis locations.", "response": "def _collectAxisPoints(self):\n        \"\"\"\n            Return a dictionary with all on-axis locations.\n        \"\"\"\n        for l, (value, deltaName) in self.items():\n            location = Location(l)\n            name = location.isOnAxis()\n            if name is not None and name is not False:\n                if name not in self._axes:\n                    self._axes[name] = []\n                if l not in self._axes[name]:\n                    self._axes[name].append(l)\n        return self._axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary with all off - axis locations.", "response": "def _collectOffAxisPoints(self):\n        \"\"\"\n            Return a dictionary with all off-axis locations.\n        \"\"\"\n        offAxis = {}\n        for l, (value, deltaName) in self.items():\n            location = Location(l)\n            name = location.isOnAxis()\n            if name is None or name is False:\n                offAxis[l] = 1\n        return list(offAxis.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collectLocations(self):\n        pts = []\n        for l, (value, deltaName) in self.items():\n            pts.append(Location(l))\n        return pts", "response": "Return a dictionary with all objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all locations of all objects.", "response": "def _allLocations(self):\n        \"\"\"\n            Return a list of all locations of all objects.\n        \"\"\"\n        l = []\n        for locationTuple in self.keys():\n            l.append(Location(locationTuple))\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the delta at aLocation.", "response": "def getInstance(self, aLocation, axisOnly=False, getFactors=False):\n\n        \"\"\" Calculate the delta at aLocation.\n            *   aLocation:  a Location object, expected to be in bent space\n            *   axisOnly:\n                *   True: calculate an instance only with the on-axis masters.\n                *   False: calculate an instance with on-axis and off-axis masters.\n            *   getFactors:\n                *   True: return a list of the calculated factors.\n        \"\"\"\n        self._collectAxisPoints()\n        factors = self.getFactors(aLocation, axisOnly)\n        total = None\n        for f, item, name in factors:\n            if total is None:\n                total = f * item\n                continue\n            total += f * item\n        if total is None:\n            total = 0 * self._neutral\n        if getFactors:\n            return total, factors\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makeInstance(self, aLocation, bend=True):\n        if bend:\n            aLocation = self._bender(Location(aLocation))\n        if not aLocation.isAmbivalent():\n            instanceObject = self.getInstance(aLocation-self._bias)\n        else:\n            locX, locY = aLocation.split()\n            instanceObject = self.getInstance(locX-self._bias)*(1,0)+self.getInstance(locY-self._bias)*(0,1)\n        return instanceObject+self._neutral", "response": "Calculate an instance of the class with the right bias and add the neutral."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getFactors(self, aLocation, axisOnly=False, allFactors=False):\n        deltas = []\n        aLocation.expand(self.getAxisNames())\n        limits = getLimits(self._allLocations(), aLocation)\n        for deltaLocationTuple, (mathItem, deltaName) in sorted(self.items()):\n            deltaLocation = Location(deltaLocationTuple)\n            deltaLocation.expand( self.getAxisNames())\n            factor = self._accumulateFactors(aLocation, deltaLocation, limits, axisOnly)\n            if not (factor-_EPSILON < 0 < factor+_EPSILON) or allFactors:\n                # only add non-zero deltas.\n                deltas.append((factor, mathItem, deltaName))\n        deltas = sorted(deltas, key=itemgetter(0), reverse=True)\n        return deltas", "response": "Return a list of all factors and math items at aLocation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _accumulateFactors(self, aLocation, deltaLocation, limits, axisOnly):\n        relative = []\n        deltaAxis = deltaLocation.isOnAxis()\n        if deltaAxis is None:\n            relative.append(1)\n        elif deltaAxis:\n            deltasOnSameAxis = self._axes.get(deltaAxis, [])\n            d = ((deltaAxis, 0),)\n            if d not in deltasOnSameAxis:\n                deltasOnSameAxis.append(d)\n            if len(deltasOnSameAxis) == 1:\n                relative.append(aLocation[deltaAxis] * deltaLocation[deltaAxis])\n            else:\n                factor =  self._calcOnAxisFactor(aLocation, deltaAxis, deltasOnSameAxis, deltaLocation)\n                relative.append(factor)\n        elif not axisOnly:\n            factor = self._calcOffAxisFactor(aLocation, deltaLocation, limits)\n            relative.append(factor)\n        if not relative:\n            return 0\n        f = None\n        for v in relative:\n            if f is None: f = v\n            else:\n                f *= v\n        return f", "response": "Calculates the factors of deltaLocation towards aLocation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the on - axis factors.", "response": "def _calcOnAxisFactor(self, aLocation, deltaAxis, deltasOnSameAxis, deltaLocation):\n        \"\"\"\n            Calculate the on-axis factors.\n        \"\"\"\n        if deltaAxis == \"origin\":\n            f = 0\n            v = 0\n        else:\n            f = aLocation[deltaAxis]\n            v = deltaLocation[deltaAxis]\n        i = []\n        iv = {}\n        for value in deltasOnSameAxis:\n            iv[Location(value)[deltaAxis]]=1\n        i = sorted(iv.keys())\n        r = 0\n        B, M, A = [], [], []\n        mA, mB, mM = None, None, None\n        for value in i:\n            if value < f: B.append(value)\n            elif value > f: A.append(value)\n            else: M.append(value)\n        if len(B) > 0:\n            mB = max(B)\n            B.sort()\n        if len(A) > 0:\n            mA = min(A)\n            A.sort()\n        if len(M) > 0:\n            mM = min(M)\n            M.sort()\n        if mM is not None:\n            if ((f-_EPSILON <  v) and (f+_EPSILON > v)) or f==v: r = 1\n            else: r = 0\n        elif mB is not None and mA is not None:\n            if v < mB or v > mA: r = 0\n            else:\n                if v == mA:\n                    r = float(f-mB)/(mA-mB)\n                else:\n                    r = float(f-mA)/(mB-mA)\n        elif mB is None and mA is not None:\n            if v==A[1]:\n                r = float(f-A[0])/(A[1]-A[0])\n            elif v == A[0]:\n                r = float(f-A[1])/(A[0]-A[1])\n            else:\n                r = 0\n        elif  mB is not None and mA is None:\n            if v == B[-2]:\n                r = float(f-B[-1])/(B[-2]-B[-1])\n            elif v == mB:\n                r = float(f-B[-2])/(B[-1]-B[-2])\n            else:\n                r = 0\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calcOffAxisFactor(self, aLocation, deltaLocation, limits):\n        relative = []\n        for dim in limits.keys():\n            f = aLocation[dim]\n            v = deltaLocation[dim]\n            mB, M, mA = limits[dim]\n            r = 0\n            if mA is not None and v > mA:\n                relative.append(0)\n                continue\n            elif mB is not None and v < mB:\n                relative.append(0)\n                continue\n            if f < v-_EPSILON:\n                if mB is None:\n                    if M is not None and mA is not None:\n                        if v == M:\n                            r = (float(max(f,mA)-min(f, mA))/float(max(M,mA)-min(M, mA)))\n                        else:\n                            r = -(float(max(f,mA)-min(f, mA))/float(max(M,mA)-min(M, mA)) -1)\n                    else: r = 0\n                elif mA is None: r = 0\n                else: r = float(f-mB)/(mA-mB)\n            elif f > v+_EPSILON:\n                if mB is None: r = 0\n                elif mA is None:\n                    if M is not None and mB is not None:\n                        if v == M:\n                            r = (float(max(f,mB)-min(f, mB))/(max(mB, M)-min(mB, M)))\n                        else:\n                            r = -(float(max(f,mB)-min(f, mB))/(max(mB, M)-min(mB, M)) - 1)\n                    else: r = 0\n                else: r = float(mA-f)/(mA-mB)\n            else: r = 1\n            relative.append(r)\n        f = 1\n        for i in relative:\n            f *= i\n        return f", "response": "Calculate the off - axis factors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the locations by ranking.", "response": "def sortLocations(locations):\n    \"\"\" Sort the locations by ranking:\n            1.  all on-axis points\n            2.  all off-axis points which project onto on-axis points\n                these would be involved in master to master interpolations\n                necessary for patching. Projecting off-axis masters have\n                at least one coordinate in common with an on-axis master.\n            3.  non-projecting off-axis points, 'wild' off axis points\n                These would be involved in projecting limits and need to be patched.\n    \"\"\"\n    onAxis = []\n    onAxisValues = {}\n    offAxis = []\n    offAxis_projecting = []\n    offAxis_wild = []\n    # first get the on-axis points\n    for l in locations:\n        if l.isOrigin():\n            continue\n        if l.isOnAxis():\n            onAxis.append(l)\n            for axis in l.keys():\n                if axis not in onAxisValues:\n                    onAxisValues[axis] = []\n                onAxisValues[axis].append(l[axis])\n        else:\n            offAxis.append(l)\n    for l in offAxis:\n        ok = False\n        for axis in l.keys():\n            if axis not in onAxisValues:\n                continue\n            if l[axis] in onAxisValues[axis]:\n                ok = True\n        if ok:\n            offAxis_projecting.append(l)\n        else:\n            offAxis_wild.append(l)\n    return onAxis, offAxis_projecting, offAxis_wild"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the vector that translates the whole system to the origin.", "response": "def biasFromLocations(locs, preferOrigin=True):\n    \"\"\"\n        Find the vector that translates the whole system to the origin. \n    \"\"\"\n    dims = {}\n    locs.sort()\n    for l in locs:\n        for d in l.keys():\n            if not d in dims:\n                dims[d] = []\n            v = l[d]\n            if type(v)==tuple:\n                dims[d].append(v[0])\n                dims[d].append(v[1])\n            else:\n                dims[d].append(v)\n    candidate = Location()\n    for k in dims.keys():\n        dims[k].sort()\n        v = mostCommon(dims[k])\n        if dims[k].count(v) > 1:\n            # add the dimension with two or more hits\n            candidate[k] = mostCommon(dims[k])\n    matches = []\n    # 1. do we have an exact match?\n    for l in locs:\n        if candidate == l:\n            return l\n    # 2. find a location that matches candidate (but has more dimensions)\n    for l in locs:\n        ok = True\n        for k, v in candidate.items():\n            if l.get(k)!=v:\n                ok = False\n                break\n        if ok:\n            if not l in matches:\n                matches.append(l)\n    matches.sort()\n    if len(matches)>0:\n        if preferOrigin:\n            for c in matches:\n                if c.isOrigin():\n                    return c\n        return matches[0]\n    # 3. no matches. Find the best from the available locations\n    results = {}\n    for bias in locs:\n        rel = []\n        for l in locs:\n            rel.append((l - bias).isOnAxis())\n        c = rel.count(False)\n        if not c in results:\n            results[c] = []\n        results[c].append(bias)\n    if results:\n        candidates = results[min(results.keys())]\n        if preferOrigin:\n            for c in candidates:\n                if c.isOrigin():\n                    return c\n        candidates.sort()\n        return candidates[0]\n    return Location()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mostCommon(L):\n    # get an iterable of (item, iterable) pairs\n    SL = sorted((x, i) for i, x in enumerate(L))\n    # print 'SL:', SL\n    groups = itertools.groupby(SL, key=operator.itemgetter(0))\n    # auxiliary function to get \"quality\" for an item\n    def _auxfun(g):\n        item, iterable = g\n        count = 0\n        min_index = len(L)\n        for _, where in iterable:\n            count += 1\n            min_index = min(min_index, where)\n        # print 'item %r, count %r, minind %r' % (item, count, min_index)\n        return count, -min_index\n    # pick the highest-count/earliest item\n    return max(groups, key=_auxfun)[0]", "response": "Return the most common element of a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the coordinates from a tuple.", "response": "def fromTuple(self, locationTuple):\n        \"\"\"\n        Read the coordinates from a tuple.\n        ::\n            \n            >>> t = (('pop', 1), ('snap', -100))\n            >>> l = Location()\n            >>> l.fromTuple(t)\n            >>> print(l)\n            <Location pop:1, snap:-100 >\n\n        \"\"\"\n        for key, value in locationTuple:\n            try:\n                self[key] = float(value)\n            except TypeError:\n                self[key] = tuple([float(v) for v in value])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef asTuple(self):\n        t = []\n        k = sorted(self.keys())\n        for key in k:\n            t.append((key, self[key]))\n        return tuple(t)", "response": "Return the location as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getType(self, short=False):\n        if self.isOrigin():\n            return \"origin\"\n        t = []\n        onAxis = self.isOnAxis()\n        if onAxis is False:\n            if short:\n                t.append(\"off-axis\")\n            else:\n                t.append(\"off-axis, \"+ \" \".join(self.getActiveAxes()))\n        else:\n            if short:\n                t.append(\"on-axis\")\n            else:\n                t.append(\"on-axis, %s\"%onAxis)\n        if self.isAmbivalent():\n            t.append(\"split\")\n        return ', '.join(t)", "response": "Return a string describing the type of the location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getActiveAxes(self):\n        names = sorted(k for k in self.keys() if self[k]!=0)\n        return names", "response": "Return a list of names of axes which are not zero\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asString(self, strict=False):\n        if len(self.keys())==0:\n            return \"origin\"\n        v = []\n        n = []\n        try:\n            for name, value in self.asTuple():\n                s = ''\n                if value is None:\n                    s = \"None\"\n                elif type(value) == tuple or type(value) == list:\n                    s = \"(%.3f,%.3f)\"%(value[0], value[1])\n                elif int(value) == value:\n                    s = \"%d\"%(int(value))\n                else:\n                    s = \"%.3f\"%(value)\n                if s != '':\n                    n.append(\"%s:%s\"%(name, s))\n            return \", \".join(n)\n        except TypeError:\n            import traceback\n            print(\"Location value error:\", name, value)\n            for key, value in self.items():\n                print(\"\\t\\tkey:\", key)\n                print(\"\\t\\tvalue:\", value)\n            traceback.print_exc()\n            return \"error\"", "response": "Return the location as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data in a dict with sorted names and column titles.", "response": "def asSortedStringDict(self, roundValue=False):\n        \"\"\" Return the data in a dict with sorted names and column titles.\n        ::\n\n            >>> l = Location(pop=1, snap=(1,10))\n            >>> l.asSortedStringDict()[0]['value']\n            '1'\n            >>> l.asSortedStringDict()[0]['axis']\n            'pop'\n            >>> l.asSortedStringDict()[1]['axis']\n            'snap'\n            >>> l.asSortedStringDict()[1]['value']\n            '(1,10)'\n            \n        \"\"\"\n        data = []\n        names = sorted(self.keys())\n        for n in names:\n            data.append({'axis':n, 'value':numberToString(self[n])})\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef strip(self):\n        result = []\n        for k, v in self.items():\n            if isinstance(v, tuple):\n                if v > (_EPSILON, ) * len(v) or v < (-_EPSILON, ) * len(v):\n                    result.append((k, v))\n            elif v > _EPSILON or v < -_EPSILON:\n                result.append((k, v))\n        return self.__class__(result)", "response": "Remove coordinates that are zero the opposite of expand."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef common(self, other):\n        selfDim = set(self.keys())\n        otherDim = set(other.keys())\n        dims = selfDim | otherDim\n        newSelf = None\n        newOther = None\n        for dim in dims:\n            sd = self.get(dim, None)\n            od = other.get(dim, None)\n            if sd is None or od is None:\n                # axis is missing in one or the other\n                continue\n            if -_EPSILON < sd < _EPSILON and -_EPSILON < od < _EPSILON:\n                # values are both zero\n                continue\n            if newSelf is None:\n                newSelf = self.__class__()\n            if newOther is None:\n                newOther = self.__class__()\n            newSelf[dim] = self[dim]\n            newOther[dim] = other[dim]\n        return newSelf, newOther", "response": "Return two objects with the same dimensions if they lie in the same orthogonal plane."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the location is at the origin.", "response": "def isOrigin(self):\n        \"\"\"\n        Return True if the location is at the origin.\n        ::\n            >>> l = Location(pop=1)\n            >>> l.isOrigin()\n            False\n            >>> l = Location()\n            >>> l.isOrigin()\n            True\n        \"\"\"\n        for name, value in self.items():\n            if isinstance(value, tuple):\n                if (value < (-_EPSILON,) * len(value)\n                        or value > (_EPSILON,) * len(value)):\n                    return False\n            if value < -_EPSILON or value > _EPSILON:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isOnAxis(self):\n        new = self.__class__()\n        new.update(self)\n        s = new.strip()\n        dims = list(s.keys())\n        if len(dims)> 1:\n            return False\n        elif len(dims)==1:\n            return dims[0]\n        return None", "response": "Returns statements about this location and its axis if it is on - axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if any of the factors are in fact tuples.", "response": "def isAmbivalent(self, dim=None):\n        \"\"\"\n        Return True if any of the factors are in fact tuples.\n        If a dimension name is given only that dimension is tested.\n        ::\n            >>> l = Location(pop=1)\n            >>> l.isAmbivalent()\n            False\n            >>> l = Location(pop=1, snap=(100, -100))\n            >>> l.isAmbivalent()\n            True\n        \"\"\"\n        if dim is not None:\n            try:\n                return isinstance(self[dim], tuple)\n            except KeyError:\n                # dimension is not present, it should be 0, so not ambivalent\n                return False\n        for dim, val in self.items():\n            if isinstance(val, tuple):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(self):\n        x = self.__class__()\n        y = self.__class__()\n        for dim, val in self.items():\n            if isinstance(val, tuple):\n                x[dim] = val[0]\n                y[dim] = val[1]\n            else:\n                x[dim] = val\n                y[dim] = val\n        return x, y", "response": "Split an ambivalent location into two locations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a copy with the x values preferred for ambivalent locations.", "response": "def spliceX(self):\n        \"\"\"\n        Return a copy with the x values preferred for ambivalent locations.\n        ::\n\n            >>> l = Location(pop=(-5,5))\n            >>> l.spliceX()\n            <Location pop:-5 >\n        \"\"\"\n        new = self.__class__()\n        for dim, val in self.items():\n            if isinstance(val, tuple):\n                new[dim] = val[0]\n            else:\n                new[dim] = val\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the geometric distance to the origin location.", "response": "def distance(self, other=None):\n        \"\"\"Return the geometric distance to the other location.\n        If no object is provided, this will calculate the distance to the origin.\n\n        ::\n\n            >>> l = Location(pop=100)\n            >>> m = Location(pop=200)\n            >>> l.distance(m)\n            100.0\n            >>> l = Location()\n            >>> m = Location(pop=200)\n            >>> l.distance(m)\n            200.0\n            >>> l = Location(pop=3, snap=5)\n            >>> m = Location(pop=7, snap=8)\n            >>> l.distance(m)\n            5.0\n        \"\"\"\n        t = 0\n        if other is None:\n            other = self.__class__()\n        for axisName in set(self.keys()) | set(other.keys()):\n            t += (other.get(axisName,0)-self.get(axisName,0))**2\n        return math.sqrt(t)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if this object is the same location.", "response": "def sameAs(self, other):\n        \"\"\"\n        Check if this is the same location.\n        ::\n        \n            >>> l = Location(pop=5, snap=100)\n            >>> m = Location(pop=5.0, snap=100.0)\n            >>> l.sameAs(m)\n            0\n            >>> l = Location(pop=5, snap=100)\n            >>> m = Location(pop=5.0, snap=100.0001)\n            >>> l.sameAs(m)\n            -1\n        \"\"\"\n        if not hasattr(other, \"get\"):\n            return -1\n        d = self.distance(other)\n        if d < _EPSILON:\n            return 0\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the rabbitmq and elasticsearch and wait for connections to both rabbitmq and elasticsearch to be made wait for connections to both rabbitmq and elasticsearch to be made", "response": "def connections(self, wait):\n        \"\"\"\n        wait for connections to both rabbitmq and elasticsearch to be made\n        before binding a routing key to a channel and sending messages to\n        elasticsearch\n        \"\"\"\n        while wait:\n            try:\n                params = pika.ConnectionParameters(host=self.rmq_host,\n                                                   port=self.rmq_port)\n                connection = pika.BlockingConnection(params)\n                self.channel = connection.channel()\n                self.channel.exchange_declare(exchange='topic_recs',\n                                              exchange_type='topic')\n\n                result = self.channel.queue_declare()\n                self.queue_name = result.method.queue\n                self.es_conn = Elasticsearch([{'host': self.es_host,\n                                               'port': self.es_port}])\n                wait = False\n                print('connected to rabbitmq and elasticsearch...')\n            except Exception as e:  # pragma: no cover\n                print(str(e))\n                print('waiting for connection to rabbitmq...' + str(e))\n                time.sleep(2)\n                wait = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef callback(self, ch, method, properties, body):\n        index = method.routing_key.split('.')[1]\n        index = index.replace('/', '-')\n        failed = False\n        body = str(body)\n        try:\n            doc = json.loads(body)\n        except Exception as e:  # pragma: no cover\n            try:\n                body = body.strip().replace('\"', '\\\"')\n                body = '{\"log\":\"' + body + '\"}'\n                doc = json.loads(body)\n            except Exception as e:  # pragma: no cover\n                failed = True\n\n        if not failed:\n            try:\n                self.es_conn.index(index=index,\n                                   doc_type=method.routing_key.split('.')[1],\n                                   id=method.routing_key + '.' +\n                                   str(uuid.uuid4()),\n                                   body=doc)\n            except Exception as e:  # pragma: no cover\n                print('Failed to send document to elasticsearch because: ' + str(e))", "response": "This callback is triggered when a rabiitmq message is received and sends it to elasticsearch index\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the channel listener and start consuming messages", "response": "def start(self):\n        \"\"\" start the channel listener and start consuming messages \"\"\"\n        self.connections(True)\n\n        binding_keys = sys.argv[1:]\n        if not binding_keys:\n            print(sys.stderr,\n                  'Usage: {0!s} [binding_key]...'.format(sys.argv[0]))\n            sys.exit(0)\n\n        for binding_key in binding_keys:\n            self.channel.queue_bind(exchange='topic_recs',\n                                    queue=self.queue_name,\n                                    routing_key=binding_key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting consuming rabbitmq messages", "response": "def consume(self):  # pragma: no cover\n        \"\"\" start consuming rabbitmq messages \"\"\"\n        print(' [*] Waiting for logs. To exit press CTRL+C')\n        self.channel.basic_consume(self.queue_name, self.callback)\n        self.channel.start_consuming()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _auto_install(self):\n        template = Template(template=self.manifest)\n        sections = template.sections()\n        images = self.d_client.images.list(filters={'label': 'vent'})\n        add_sections = []\n        status = (True, None)\n        for image in images:\n            ignore = False\n            if ('Labels' in image.attrs['Config'] and\n                'vent.section' in image.attrs['Config']['Labels'] and\n                    not image.attrs['Config']['Labels']['vent.section'] in sections[1]):\n                section = image.attrs['Config']['Labels']['vent.section']\n                section_str = image.attrs['Config']['Labels']['vent.section'].split(\n                    ':')\n                template.add_section(section)\n                if 'vent.name' in image.attrs['Config']['Labels']:\n                    template.set_option(section,\n                                        'name',\n                                        image.attrs['Config']['Labels']['vent.name'])\n                if 'vent.repo' in image.attrs['Config']['Labels']:\n                    template.set_option(section,\n                                        'repo',\n                                        image.attrs['Config']['Labels']['vent.repo'])\n                    git_path = join(self.path_dirs.plugins_dir,\n                                    '/'.join(section_str[:2]))\n                    # TODO clone it down\n                    template.set_option(section, 'path', join(\n                        git_path, section_str[-3][1:]))\n                    # get template settings\n                    # TODO account for template files not named vent.template\n                    v_template = Template(template=join(\n                        git_path, section_str[-3][1:], 'vent.template'))\n                    tool_sections = v_template.sections()\n                    if tool_sections[0]:\n                        for s in tool_sections[1]:\n                            section_dict = {}\n                            options = v_template.options(s)\n                            if options[0]:\n                                for option in options[1]:\n                                    option_name = option\n                                    if option == 'name':\n                                        # get link name\n                                        template.set_option(section,\n                                                            'link_name',\n                                                            v_template.option(s, option)[1])\n                                        option_name = 'link_name'\n                                    opt_val = v_template.option(s, option)[1]\n                                    section_dict[option_name] = opt_val\n                            if section_dict:\n                                template.set_option(section, s,\n                                                    json.dumps(section_dict))\n                if ('vent.type' in image.attrs['Config']['Labels'] and\n                        image.attrs['Config']['Labels']['vent.type'] == 'repository'):\n                    template.set_option(\n                        section, 'namespace', '/'.join(section_str[:2]))\n                    template.set_option(section, 'branch', section_str[-2])\n                    template.set_option(section, 'version', section_str[-1])\n                    template.set_option(section, 'last_updated', str(\n                        datetime.utcnow()) + ' UTC')\n                    if image.attrs['RepoTags']:\n                        template.set_option(\n                            section, 'image_name', image.attrs['RepoTags'][0])\n                    else:\n                        # image with none tag is outdated, don't add it\n                        ignore = True\n                    template.set_option(section, 'type', 'repository')\n                if 'vent.groups' in image.attrs['Config']['Labels']:\n                    template.set_option(section,\n                                        'groups',\n                                        image.attrs['Config']['Labels']['vent.groups'])\n                template.set_option(section, 'built', 'yes')\n                template.set_option(section, 'image_id',\n                                    image.attrs['Id'].split(':')[1][:12])\n                template.set_option(section, 'running', 'no')\n                # check if image is running as a container\n                containers = self.d_client.containers.list(\n                    filters={'label': 'vent'})\n                for container in containers:\n                    if container.attrs['Image'] == image.attrs['Id']:\n                        template.set_option(section, 'running', 'yes')\n                if not ignore:\n                    add_sections.append(section)\n                    template.write_config()\n        # TODO this check will always be true, need to actually validate the above logic\n        if status[0]:\n            status = (True, add_sections)\n        return status", "response": "Auto - installs the images in the manifest if they are not there already"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef backup(self):\n        status = (True, None)\n        # initialize all needed variables (names for backup files, etc.)\n        backup_name = ('.vent-backup-' + '-'.join(Timestamp().split(' ')))\n        backup_dir = join(expanduser('~'), backup_name)\n        backup_manifest = join(backup_dir, 'backup_manifest.cfg')\n        backup_vcfg = join(backup_dir, 'backup_vcfg.cfg')\n        manifest = self.manifest\n\n        # create new backup directory\n        try:\n            mkdir(backup_dir)\n        except Exception as e:  # pragma: no cover\n            self.logger.error(str(e))\n            return (False, str(e))\n        # create new files in backup directory\n        try:\n            # backup manifest\n            with open(backup_manifest, 'w') as bmanifest:\n                with open(manifest, 'r') as manifest_file:\n                    bmanifest.write(manifest_file.read())\n            # backup vent.cfg\n            with open(backup_vcfg, 'w') as bvcfg:\n                with open(self.vent_config, 'r') as vcfg_file:\n                    bvcfg.write(vcfg_file.read())\n            self.logger.info('Backup information written to ' + backup_dir)\n            status = (True, backup_dir)\n        except Exception as e:  # pragma: no cover\n            self.logger.error(\"Couldn't backup vent: \" + str(e))\n            status = (False, str(e))\n        # TODO #266\n        return status", "response": "Saves the current running vent s configuration information to be used for restoring at a later time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _startup(self):\n        status = (True, None)\n        try:\n            s_dict = {}\n            # rewrite the yml file to exclusively lowercase\n            with open(self.startup_file, 'r') as sup:\n                vent_startup = sup.read()\n            with open(self.startup_file, 'w') as sup:\n                for line in vent_startup:\n                    sup.write(line.lower())\n            with open(self.startup_file, 'r') as sup:\n                s_dict = yaml.safe_load(sup.read())\n            if 'vent.cfg' in s_dict:\n                v_cfg = Template(self.vent_config)\n                for section in s_dict['vent.cfg']:\n                    for option in s_dict['vent.cfg'][section]:\n                        val = ('no', 'yes')[\n                            s_dict['vent.cfg'][section][option]]\n                        v_status = v_cfg.add_option(section, option, value=val)\n                        if not v_status[0]:\n                            v_cfg.set_option(section, option, val)\n                v_cfg.write_config()\n                del s_dict['vent.cfg']\n            tool_d = {}\n            extra_options = ['info', 'service', 'settings', 'docker', 'gpu']\n            s_dict_c = copy.deepcopy(s_dict)\n            # TODO check for repo or image type\n            for repo in s_dict_c:\n                repository = Repository(System().manifest)\n                repository.repo = repo\n                repository._clone()\n                repo_path, org, r_name = self.path_dirs.get_path(repo)\n                get_tools = []\n                for tool in s_dict_c[repo]:\n                    t_branch = 'master'\n                    t_version = 'HEAD'\n                    if 'branch' in s_dict[repo][tool]:\n                        t_branch = s_dict[repo][tool]['branch']\n                    if 'version' in s_dict[repo][tool]:\n                        t_version = s_dict[repo][tool]['version']\n                    get_tools.append((tool, t_branch, t_version))\n\n                available_tools = AvailableTools(repo_path, tools=get_tools)\n                for tool in s_dict_c[repo]:\n                    # if we can't find the tool in that repo, skip over this\n                    # tool and notify in the logs\n                    t_path, t_path_cased = PathDirs.rel_path(\n                        tool, available_tools)\n                    if t_path is None:\n                        self.logger.error(\"Couldn't find tool \" + tool + ' in'\n                                          ' repo ' + repo)\n                        continue\n                    # ensure no NoneType iteration errors\n                    if s_dict_c[repo][tool] is None:\n                        s_dict[repo][tool] = {}\n                    # check if we need to configure instances along the way\n                    instances = 1\n                    if 'settings' in s_dict[repo][tool]:\n                        if 'instances' in s_dict[repo][tool]['settings']:\n                            instances = int(s_dict[repo][tool]\n                                            ['settings']['instances'])\n                    # add the tool\n                    t_branch = 'master'\n                    t_version = 'HEAD'\n                    t_image = None\n                    add_tools = None\n                    add_tools = [(t_path_cased, '')]\n                    if 'branch' in s_dict[repo][tool]:\n                        t_branch = s_dict[repo][tool]['branch']\n                    if 'version' in s_dict[repo][tool]:\n                        t_version = s_dict[repo][tool]['version']\n                    if 'image' in s_dict[repo][tool]:\n                        t_image = s_dict[repo][tool]['image']\n                    repository.add(\n                        repo, add_tools, branch=t_branch, version=t_version, image_name=t_image)\n                    manifest = Template(self.manifest)\n                    # update the manifest with extra defined runtime settings\n                    base_section = ':'.join([org, r_name, t_path,\n                                             t_branch, t_version])\n                    for option in extra_options:\n                        if option in s_dict[repo][tool]:\n                            opt_dict = manifest.option(base_section, option)\n                            # add new values defined into default options for\n                            # that tool, don't overwrite them\n                            if opt_dict[0]:\n                                opt_dict = json.loads(opt_dict[1])\n                            else:\n                                opt_dict = {}\n                            # stringify values for vent\n                            for v in s_dict[repo][tool][option]:\n                                pval = s_dict[repo][tool][option][v]\n                                s_dict[repo][tool][option][v] = json.dumps(\n                                    pval)\n                            opt_dict.update(s_dict[repo][tool][option])\n                            manifest.set_option(base_section, option,\n                                                json.dumps(opt_dict))\n                    # copy manifest info into new sections if necessary\n                    if instances > 1:\n                        for i in range(2, instances + 1):\n                            i_section = base_section.rsplit(':', 2)\n                            i_section[0] += str(i)\n                            i_section = ':'.join(i_section)\n                            manifest.add_section(i_section)\n                            for opt_val in manifest.section(base_section)[1]:\n                                if opt_val[0] == 'name':\n                                    manifest.set_option(i_section, opt_val[0],\n                                                        opt_val[1] + str(i))\n                                else:\n                                    manifest.set_option(i_section, opt_val[0],\n                                                        opt_val[1])\n                    manifest.write_config()\n\n            tool_d = {}\n            tools = Tools()\n            # start tools, if necessary\n            for repo in s_dict:\n                for tool in s_dict[repo]:\n                    if 'start' in s_dict[repo][tool]:\n                        if s_dict[repo][tool]['start']:\n                            local_instances = 1\n                            if 'settings' in s_dict[repo][tool] and 'instances' in s_dict[repo][tool]['settings']:\n                                local_instances = int(\n                                    s_dict[repo][tool]['settings']['instances'])\n                            t_branch = 'master'\n                            t_version = 'HEAD'\n                            if 'branch' in s_dict[repo][tool]:\n                                t_branch = s_dict[repo][tool]['branch']\n                            if 'version' in s_dict[repo][tool]:\n                                t_version = s_dict[repo][tool]['version']\n                            for i in range(1, local_instances + 1):\n                                i_name = tool + str(i) if i != 1 else tool\n                                i_name = i_name.replace('@', '')\n                                tool_d.update(\n                                    tools._prep_start(repo, i_name)[1])\n\n            if tool_d:\n                tools.start(tool_d, None, is_tool_d=True)\n        except Exception as e:  # pragma: no cover\n            self.logger.error('Startup failed because: {0}'.format(str(e)))\n            status = (False, str(e))\n        return status", "response": "Detects if a startup file is specified and stand up a vent\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_configure(self,\n                      repo=None,\n                      name=None,\n                      groups=None,\n                      main_cfg=False):\n        \"\"\"\n        Get the vent.template settings for a given tool by looking at the\n        plugin_manifest\n        \"\"\"\n        constraints = locals()\n        del constraints['main_cfg']\n        status = (True, None)\n        template_dict = {}\n        return_str = ''\n        if main_cfg:\n            vent_cfg = Template(self.vent_config)\n            for section in vent_cfg.sections()[1]:\n                template_dict[section] = {}\n                for vals in vent_cfg.section(section)[1]:\n                    template_dict[section][vals[0]] = vals[1]\n        else:\n            # all possible vent.template options stored in plugin_manifest\n            options = ['info', 'service', 'settings', 'docker', 'gpu']\n            tools = Template(System().manifest).constrain_opts(\n                constraints, options)[0]\n            if tools:\n                # should only be one tool\n                tool = list(tools.keys())[0]\n                # load all vent.template options into dict\n                for section in tools[tool]:\n                    template_dict[section] = json.loads(tools[tool][section])\n            else:\n                status = (False, \"Couldn't get vent.template information\")\n        if status[0]:\n            # display all those options as they would in the file\n            for section in template_dict:\n                return_str += '[' + section + ']\\n'\n                # ensure instances shows up in configuration\n                for option in template_dict[section]:\n                    if option.startswith('#'):\n                        return_str += option + '\\n'\n                    else:\n                        return_str += option + ' = '\n                        return_str += template_dict[section][option] + '\\n'\n                return_str += '\\n'\n            # only one newline at end of file\n            status = (True, return_str[:-1])\n        return status", "response": "Get the vent. template settings for a given tool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving configuration changes made to vent. template through npyscreen to the plugin_manifest and the template.", "response": "def save_configure(self,\n                       repo=None,\n                       name=None,\n                       groups=None,\n                       config_val='',\n                       from_registry=False,\n                       main_cfg=False,\n                       instances=1):\n        \"\"\"\n        Save changes made to vent.template through npyscreen to the template\n        and to plugin_manifest\n        \"\"\"\n        def template_to_manifest(vent_template, manifest, tool, instances):\n            \"\"\"\n            Helper function to transfer information from vent.template\n            to plugin_manifest\n            \"\"\"\n            sections = vent_template.sections()\n            if sections[0]:\n                for section in sections[1]:\n                    section_dict = {}\n                    if section == 'settings':\n                        section_dict.update({'instances': str(instances)})\n                    options = vent_template.options(section)\n                    if options[0]:\n                        for option in options[1]:\n                            option_name = option\n                            if option == 'name':\n                                option_name = 'link_name'\n                            opt_val = vent_template.option(section,\n                                                           option)[1]\n                            section_dict[option_name] = opt_val\n                    if section_dict:\n                        manifest.set_option(tool, section,\n                                            json.dumps(section_dict))\n                    elif manifest.option(tool, section)[0]:\n                        manifest.del_option(tool, section)\n\n        constraints = locals()\n        del constraints['config_val']\n        del constraints['from_registry']\n        del constraints['main_cfg']\n        del constraints['instances']\n        del constraints['template_to_manifest']\n        status = (True, None)\n        fd = None\n        # ensure instances is an int and remove instances from config_val to\n        # ensure correct info\n        instances = int(instances)\n        config_val = re.sub(r'instances\\ *=\\ *\\d+\\n', '', config_val)\n        api_system = System()\n        manifest = api_system.manifest\n        if not main_cfg:\n            if not from_registry:\n                # creating new instances\n                if instances > 1:\n                    fd, template_path = tempfile.mkstemp(suffix='.template')\n                    # scrub name for clean section name\n                    if re.search(r'\\d+$', name):\n                        name = re.sub(r'\\d+$', '', name)\n                    t_identifier = {'name': name,\n                                    'branch': branch,\n                                    'version': version}\n                    result = Template(manifest).constrain_opts(\n                        t_identifier, [])\n                    tools = result[0]\n                    tool = list(tools.keys())[0]\n                else:\n                    options = ['path', 'multi_tool', 'name']\n                    tools, _ = Template(manifest).constrain_opts(\n                        constraints, options)\n                    # only one tool in tools because perform this function for\n                    # every tool\n                    if tools:\n                        tool = list(tools.keys())[0]\n                        if ('multi_tool' in tools[tool] and\n                                tools[tool]['multi_tool'] == 'yes'):\n                            name = tools[tool]['name']\n                            if name == 'unspecified':\n                                name = 'vent'\n                            template_path = join(tools[tool]['path'],\n                                                 name+'.template')\n                        else:\n                            template_path = join(tools[tool]['path'],\n                                                 'vent.template')\n                    else:\n                        status = (False, \"Couldn't save configuration\")\n            else:\n                fd, template_path = tempfile.mkstemp(suffix='.template')\n                options = ['namespace']\n                constraints.update({'type': 'registry'})\n                tools, _ = Template(manifest).constrain_opts(constraints,\n                                                             options)\n                if tools:\n                    tool = list(tools.keys())[0]\n                else:\n                    status = (False, \"Couldn't save configuration\")\n            if status[0]:\n                try:\n                    # save in vent.template\n                    with open(template_path, 'w') as f:\n                        f.write(config_val)\n                    # save in plugin_manifest\n                    vent_template = Template(template_path)\n                    manifest = Template(manifest)\n                    if instances > 1:\n                        # add instances as needed\n                        for i in range(1, instances + 1):\n                            i_section = tool.rsplit(':', 2)\n                            i_section[0] += str(i) if i != 1 else ''\n                            i_section = ':'.join(i_section)\n                            if not manifest.section(i_section)[0]:\n                                manifest.add_section(i_section)\n                                for val_pair in manifest.section(tool)[1]:\n                                    name = val_pair[0]\n                                    val = val_pair[1]\n                                    if name == 'name':\n                                        val += str(i)\n                                    elif name == 'last_updated':\n                                        val = Timestamp()\n                                    elif name == 'running':\n                                        val = 'no'\n                                    manifest.set_option(i_section, name, val)\n                                template_to_manifest(vent_template, manifest,\n                                                     i_section, instances)\n                            else:\n                                settings = manifest.option(i_section,\n                                                           'settings')\n                                if settings[0]:\n                                    settings_dict = json.loads(settings[1])\n                                    settings_dict['instances'] = str(instances)\n                                    manifest.set_option(i_section, 'settings',\n                                                        json.dumps(\n                                                            settings_dict))\n                                else:\n                                    inst = str(instances)\n                                    settings_dict = {'instances': inst}\n                                    manifest.set_option(i_section, 'settings',\n                                                        json.dumps(\n                                                            settings_dict))\n                    else:\n                        try:\n                            settings_str = manifest.option(tool, 'settings')[1]\n                            settings_dict = json.loads(settings_str)\n                            old_instances = int(settings_dict['instances'])\n                        except Exception:\n                            old_instances = 1\n                        template_to_manifest(vent_template, manifest,\n                                             tool, old_instances)\n                    manifest.write_config()\n                    status = (True, manifest)\n                except Exception as e:  # pragma: no cover\n                    self.logger.error('save_configure error: ' + str(e))\n                    status = (False, str(e))\n            # close os file handle and remove temp file\n            if from_registry or instances > 1:\n                try:\n                    close(fd)\n                    remove(template_path)\n                except Exception as e:  # pragma: no cover\n                    self.logger.error('save_configure error: ' + str(e))\n        else:\n            with open(self.vent_config, 'w') as f:\n                f.write(config_val)\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestart necessary tools based on changes made to vent. cfg or vent. template.", "response": "def restart_tools(self,\n                      repo=None,\n                      name=None,\n                      groups=None,\n                      branch='master',\n                      version='HEAD',\n                      main_cfg=False,\n                      old_val='',\n                      new_val=''):\n        \"\"\"\n        Restart necessary tools based on changes that have been made either to\n        vent.cfg or to vent.template. This includes tools that need to be\n        restarted because they depend on other tools that were changed.\n        \"\"\"\n        status = (True, None)\n        if not main_cfg:\n            try:\n                t_identifier = {'name': name,\n                                'branch': branch,\n                                'version': version}\n                result = Template(System().manifest).constrain_opts(t_identifier,\n                                                                    ['running',\n                                                                     'link_name'])\n                tools = result[0]\n                tool = list(tools.keys())[0]\n                if ('running' in tools[tool] and\n                        tools[tool]['running'] == 'yes'):\n                    start_tools = [t_identifier]\n                    dependent_tools = [tools[tool]['link_name']]\n                    start_tools += Dependencies(dependent_tools)\n                    # TODO\n                    start_d = {}\n                    for tool_identifier in start_tools:\n                        self.clean(**tool_identifier)\n                        start_d.update(self.prep_start(**tool_identifier)[1])\n                    if start_d:\n                        Tools().start(start_d, '', is_tool_d=True)\n            except Exception as e:  # pragma: no cover\n                self.logger.error('Trouble restarting tool ' + name +\n                                  ' because: ' + str(e))\n                status = (False, str(e))\n        else:\n            try:\n                # string manipulation to get tools into arrays\n                ext_start = old_val.find('[external-services]')\n                if ext_start >= 0:\n                    ot_str = old_val[old_val.find('[external-services]') + 20:]\n                else:\n                    ot_str = ''\n                old_tools = []\n                for old_tool in ot_str.split('\\n'):\n                    if old_tool != '':\n                        old_tools.append(old_tool.split('=')[0].strip())\n                ext_start = new_val.find('[external-services]')\n                if ext_start >= 0:\n                    nt_str = new_val[new_val.find('[external-services]') + 20:]\n                else:\n                    nt_str = ''\n                new_tools = []\n                for new_tool in nt_str.split('\\n'):\n                    if new_tool != '':\n                        new_tools.append(new_tool.split('=')[0].strip())\n                # find tools changed\n                tool_changes = []\n                for old_tool in old_tools:\n                    if old_tool not in new_tools:\n                        tool_changes.append(old_tool)\n                for new_tool in new_tools:\n                    if new_tool not in old_tools:\n                        tool_changes.append(new_tool)\n                    else:\n                        # tool name will be the same\n                        oconf = old_val[old_val.find(new_tool):].split('\\n')[0]\n                        nconf = new_val[new_val.find(new_tool):].split('\\n')[0]\n                        if oconf != nconf:\n                            tool_changes.append(new_tool)\n                # put link names in a dictionary for finding dependencies\n                dependent_tools = []\n                for i, entry in enumerate(tool_changes):\n                    dependent_tools.append(entry)\n                    # change names to lowercase for use in clean, prep_start\n                    tool_changes[i] = {'name': entry.lower().replace('-', '_')}\n                dependencies = Dependencies(dependent_tools)\n                # restart tools\n                restart = tool_changes + dependencies\n                tool_d = {}\n                for tool in restart:\n                    self.clean(**tool)\n                    tool_d.update(self.prep_start(**tool)[1])\n                if tool_d:\n                    # TODO fix the arguments\n                    Tools().start(tool_d)\n            except Exception as e:  # pragma: no cover\n                self.logger.error('Problem restarting tools: ' + str(e))\n                status = (False, str(e))\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef repo_values(self):\n        branches = []\n        commits = {}\n        m_helper = Tools()\n        status = m_helper.repo_branches(self.parentApp.repo_value['repo'])\n        # branches and commits must both be retrieved successfully\n        if status[0]:\n            branches = status[1]\n            status = m_helper.repo_commits(self.parentApp.repo_value['repo'])\n            if status[0]:\n                r_commits = status[1]\n                for commit in r_commits:\n                    commits[commit[0]] = commit[1]\n            else:\n                # if commits failed, return commit errors\n                return status\n        else:\n            # if branch failed, return branch errors\n            return status\n        # if everything is good, return branches with commits\n        return branches, commits", "response": "Get the branches and commits of it\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the current branch and commit list and display the error and branch list.", "response": "def create(self):\n        \"\"\" Update with current branches and commits \"\"\"\n        self.add_handlers({'^Q': self.quit})\n        self.add(npyscreen.TitleText, name='Branches:', editable=False)\n\n        if not self.branches or not self.commits:\n            repo_vals = self.repo_values()\n            i = 3\n            # check if repo_values returned successfully\n            if (isinstance(repo_vals[0], list) and\n                    isinstance(repo_vals[1], dict)):\n                self.branches, self.commits = repo_vals\n                for branch in self.branches:\n                    self.branch_cb[branch] = self.add(npyscreen.CheckBox,\n                                                      name=branch, rely=i,\n                                                      relx=5, max_width=25)\n                    self.commit_tc[branch] = self.add(npyscreen.TitleCombo,\n                                                      value=0, rely=i+1,\n                                                      relx=10, max_width=30,\n                                                      name='Commit:',\n                                                      values=self.commits[branch])\n                    i += 3\n            else:\n                self.error = self.add(npyscreen.MultiLineEdit,\n                                      name='Errors',\n                                      editable=False, labelColor='DANGER',\n                                      rely=i, relx=5, color='DANGER', value=\"\"\"\n                Errors were found...\n                \"\"\" + str(repo_vals[1]) + \"\"\"\n\n                Please confirm the repo url and credentials are valid!\n                Vent will return to the main screen.\n                \"\"\")\n                self.error.display()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_ok(self):\n        self.parentApp.repo_value['versions'] = {}\n        self.parentApp.repo_value['build'] = {}\n        for branch in self.branch_cb:\n            if self.branch_cb[branch].value:\n                # process checkboxes\n                self.parentApp.repo_value['versions'][branch] = self.commit_tc[branch].values[self.commit_tc[branch].value]\n                self.parentApp.repo_value['build'][branch] = True\n        if self.error:\n            self.quit()\n        self.parentApp.addForm('CHOOSETOOLS',\n                               ChooseToolsForm,\n                               name='Choose tools to add for new plugin'\n                               '\\t\\t\\t\\t\\t\\t^Q to quit',\n                               color='CONTROL')\n        self.parentApp.change_form('CHOOSETOOLS')", "response": "Add the branch commit and build selection and plugins as plugins"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the widgets for AddForm", "response": "def create(self):\n        \"\"\" Create widgets for AddForm \"\"\"\n        self.add_handlers({'^T': self.quit, '^Q': self.quit})\n        self.add(npyscreen.Textfield,\n                 value='Add a plugin from a Git repository or an image from a '\n                       'Docker registry.',\n                 editable=False,\n                 color='STANDOUT')\n        self.add(npyscreen.Textfield,\n                 value='For Git repositories, you can optionally specify a '\n                       'username and password',\n                 editable=False,\n                 color='STANDOUT')\n        self.add(npyscreen.Textfield,\n                 value='for private repositories.',\n                 editable=False,\n                 color='STANDOUT')\n        self.add(npyscreen.Textfield,\n                 value='For Docker images, specify a name for referencing the '\n                       'image that is being',\n                 editable=False,\n                 color='STANDOUT')\n        self.add(npyscreen.Textfield,\n                 value='added and optionally override the tag and/or the '\n                       'registry and specify',\n                 editable=False,\n                 color='STANDOUT')\n        self.add(npyscreen.Textfield,\n                 value='comma-separated groups this image should belong to.',\n                 editable=False,\n                 color='STANDOUT')\n        self.nextrely += 1\n        self.repo = self.add(npyscreen.TitleText,\n                             name='Repository',\n                             value=self.default_repo)\n        self.user = self.add(npyscreen.TitleText, name='Username')\n        self.pw = self.add(npyscreen.TitlePassword, name='Password')\n        self.nextrely += 1\n        self.add(npyscreen.TitleText,\n                 name='OR',\n                 editable=False,\n                 labelColor='STANDOUT')\n        self.nextrely += 1\n        self.image = self.add(npyscreen.TitleText, name='Image')\n        self.link_name = self.add(npyscreen.TitleText,\n                                  name='Name')\n        self.tag = self.add(npyscreen.TitleText, name='Tag', value='latest')\n        self.registry = self.add(npyscreen.TitleText,\n                                 name='Registry',\n                                 value='docker.io')\n        self.groups = self.add(npyscreen.TitleText, name='Groups')\n        self.repo.when_value_edited()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding method for creating FormBaseNew form", "response": "def create(self):\n        \"\"\" Override method for creating FormBaseNew form \"\"\"\n        self.add_handlers({'^T': self.quit, '^Q': self.quit,\n                           '^V': self.toggle_view})\n        self.add(npyscreen.TitleFixedText, name=self.action['title'], value='')\n        response = self.action['api_action'].inventory(choices=['repos',\n                                                                'tools',\n                                                                'images',\n                                                                'built',\n                                                                'running'])\n        if response[0]:\n            inventory = response[1]\n            if len(inventory['repos']) == 0:\n                value = 'No tools were found.\\n'\n            else:\n                value = 'Tools for all groups found:\\n'\n            tools = None\n            if inventory['tools']:\n                tools = inventory['tools']\n\n            for repo in inventory['repos']:\n                s_value = ''\n                repo_name = repo.rsplit('/', 2)[1:]\n                if len(repo_name) == 1:\n                    repo_name = repo.split('/')\n                if tools:\n                    p_value = '\\n  Plugin: ' + repo + '\\n'\n                    for tool in tools:\n                        t_name = tool.split(':')\n                        if (t_name[0] == repo_name[0] and\n                                t_name[1] == repo_name[1]):\n                            s_value += '    ' + tools[tool] + '\\n      Built: '\n                            s_value += inventory['built'][tool] + '\\n'\n                            s_value += '      Image name: '\n                            s_value += inventory['images'][tool] + '\\n'\n                            s_value += '      Status: '\n                            s_value += inventory['running'][tool] + '\\n'\n                if s_value:\n                    value += p_value + s_value\n        else:\n            value = 'There was an issue with ' + self.action['name']\n            value += ' retrieval:\\n' + str(response[1])\n            value += '\\nPlease see vent.log for more details.'\n        self.all_tools = value.split('\\n')\n        self.display_val = self.add(npyscreen.Pager, values=value.split('\\n'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toggle_view(self, *args, **kwargs):\n        group_to_display = self.views.popleft()\n        self.cur_view.value = group_to_display\n        for repo in self.tools_tc:\n            for tool in self.tools_tc[repo]:\n                t_groups = self.manifest.option(tool, 'groups')[1]\n                if group_to_display not in t_groups and \\\n                        group_to_display != 'all groups':\n                    self.tools_tc[repo][tool].value = False\n                    self.tools_tc[repo][tool].hidden = True\n                else:\n                    self.tools_tc[repo][tool].value = True\n                    self.tools_tc[repo][tool].hidden = False\n        # redraw elements\n        self.display()\n        # add view back to queue\n        self.views.append(group_to_display)", "response": "Toggles the view between different groups"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, group_view=False):\n        self.add_handlers({'^T': self.quit, '^Q': self.quit})\n        self.add(npyscreen.TitleText,\n                 name='Select which tools to ' + self.action['action'] + ':',\n                 editable=False)\n        togglable = ['remove']\n        if self.action['action_name'] in togglable:\n            self.cur_view = self.add(npyscreen.TitleText,\n                                     name='Group view:',\n                                     value='all groups', editable=False,\n                                     rely=3)\n            self.add_handlers({'^V': self.toggle_view})\n            i = 5\n        else:\n            i = 4\n\n        if self.action['action_name'] == 'start':\n            response = self.tools_inst.inventory(choices=['repos',\n                                                          'tools',\n                                                          'built',\n                                                          'running'])\n        else:\n            response = self.tools_inst.inventory(choices=['repos', 'tools'])\n        if response[0]:\n            inventory = response[1]\n\n            repos = inventory['repos']\n\n            # dict has repo as key and list of core/non-core tools as values\n            has_core = {}\n            has_non_core = {}\n\n            # find all tools that are in this repo\n            # and list them if they are core\n            for repo in repos:\n                core_list = []\n                ncore_list = []\n\n                # splice the repo names for processing\n                if (repo.startswith('http')):\n                    repo_name = repo.rsplit('/', 2)[1:]\n                else:\n                    repo_name = repo.split('/')\n\n                for tool in inventory['tools']:\n                    tool_repo_name = tool.split(':')\n\n                    # cross reference repo names\n                    if (repo_name[0] == tool_repo_name[0] and\n                            repo_name[1] == tool_repo_name[1]):\n                        # check to ensure tool not set to locally active = no\n                        # in vent.cfg\n                        externally_active = False\n                        vent_cfg_file = self.api_action.vent_config\n                        vent_cfg = Template(vent_cfg_file)\n                        tool_pairs = vent_cfg.section('external-services')[1]\n                        for ext_tool in tool_pairs:\n                            if ext_tool[0].lower() == inventory['tools'][tool]:\n                                try:\n                                    ext_tool_options = json.loads(ext_tool[1])\n                                    loc = 'locally_active'\n                                    if (loc in ext_tool_options and\n                                            ext_tool_options[loc] == 'no'):\n                                        externally_active = True\n                                except Exception as e:\n                                    self.logger.error(\"Couldn't check ext\"\n                                                      ' because: ' + str(e))\n                                    externally_active = False\n                        manifest = Template(self.api_action.manifest)\n                        if not externally_active:\n                            instance_num = re.search(r'\\d+$',\n                                                     manifest.option(\n                                                         tool, 'name')[1])\n                            if not instance_num:\n                                ncore_list.append(tool)\n                            # multiple instances share same image\n                            elif self.action['action_name'] not in self.no_instance:\n                                ncore_list.append(tool)\n\n                has_core[repo] = core_list\n                has_non_core[repo] = ncore_list\n\n            for repo in repos:\n                self.tools_tc[repo] = {}\n\n                if self.action['cores']:\n                    # make sure only repos with core tools are displayed\n                    if has_core.get(repo):\n                        self.repo_widgets[repo] = self.add(npyscreen.TitleText,\n                                                           name='Plugin: '+repo,\n                                                           editable=False,\n                                                           rely=i, relx=5)\n\n                        for tool in has_core[repo]:\n                            tool_name = tool.split(':', 2)[2].split('/')[-1]\n                            if tool_name == '':\n                                tool_name = '/'\n                            self.tools_tc[repo][tool] = self.add(\n                                npyscreen.CheckBox, name=tool_name,\n                                value=True, relx=10)\n                            i += 1\n                        i += 3\n                else:\n                    # make sure only repos with non-core tools are displayed\n                    if has_non_core.get(repo):\n                        self.repo_widgets[repo] = self.add(npyscreen.TitleText,\n                                                           name='Plugin: '+repo,\n                                                           editable=False,\n                                                           rely=i, relx=5)\n\n                        for tool in has_non_core[repo]:\n                            tool_name = tool.split(':', 2)[2].split('/')[-1]\n                            if tool_name == '':\n                                tool_name = '/'\n                            self.tools_tc[repo][tool] = self.add(\n                                npyscreen.CheckBox, name=tool_name,\n                                value=True, relx=10)\n                            i += 1\n                        i += 3\n        return", "response": "Create a new version of the current tools."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_ok(self):\n        def diff(first, second):\n            \"\"\"\n            Get the elements that exist in the first list and not in the second\n            \"\"\"\n            second = set(second)\n            return [item for item in first if item not in second]\n\n        def popup(original, orig_type, thr, title):\n            \"\"\"\n            Start the thread and display a popup of info\n            until the thread is finished\n            \"\"\"\n            thr.start()\n            info_str = ''\n            while thr.is_alive():\n                if orig_type == 'containers':\n                    info = diff(Containers(), original)\n                elif orig_type == 'images':\n                    info = diff(Images(), original)\n                if info:\n                    info_str = ''\n                for entry in info:\n                    info_str = entry[0] + ': ' + entry[1] + '\\n' + info_str\n                if self.action['action_name'] != 'configure':\n                    npyscreen.notify_wait(info_str, title=title)\n                    time.sleep(1)\n\n            thr.join()\n            try:\n                result = self.api_action.queue.get(False)\n                if isinstance(result, tuple) and isinstance(result[1], tuple):\n                    running, failed = result[1]\n                    r_str = ''\n                    for container in running:\n                        r_str += container + ': successful\\n'\n                    for container in failed:\n                        r_str += container + ': failed\\n'\n                    npyscreen.notify_confirm(r_str)\n            except Exception as e:  # pragma: no cover\n                pass\n            return\n\n        if self.action['type'] == 'images':\n            originals = Images()\n        else:\n            originals = Containers()\n\n        tool_d = {}\n        if self.action['action_name'] in ['remove', 'stop', 'update']:\n            reconfirmation_str = ''\n            if self.action['cores']:\n                reconfirmation_str = 'Are you sure you want to '\n                reconfirmation_str += self.action['action_name']\n                reconfirmation_str += ' core containers?'\n            else:\n                reconfirmation_str = 'Are you sure you want to '\n                reconfirmation_str += self.action['action_name']\n                reconfirmation_str += ' plugin containers?'\n\n            perform = npyscreen.notify_ok_cancel(reconfirmation_str,\n                                                 title='Confirm command')\n            if not perform:\n                return\n\n        tools_to_configure = []\n        for repo in self.tools_tc:\n            for tool in self.tools_tc[repo]:\n                if self.tools_tc[repo][tool].value:\n                    t = tool.split(':', 2)[2].split('/')[-1]\n                    if t.startswith('/:'):\n                        t = ' '+t[1:]\n                    t = t.split(':')\n                    if self.action['action_name'] in ['start', 'stop']:\n                        status = self.action['action_object1'](repo, t[0])\n                    elif self.action['action_name'] == 'configure':\n                        constraints = {'name': t[0],\n                                       'branch': t[1],\n                                       'version': t[2],\n                                       'repo': repo}\n                        options = ['type']\n                        action = self.action['api_action']\n                        tool = self.manifest.constrain_opts(constraints,\n                                                            options)[0]\n                        # only one tool should be returned\n                        name = list(tool.keys())[0]\n                        if tool[name]['type'] == 'registry':\n                            registry_image = True\n                        else:\n                            registry_image = False\n                        kargs = {'name': 'Configure ' + t[0],\n                                 'tool_name': t[0],\n                                 'branch': t[1],\n                                 'version': t[2],\n                                 'repo': repo,\n                                 'next_tool': None,\n                                 'get_configure': self.api_action.get_configure,\n                                 'save_configure': self.api_action.save_configure,\n                                 'restart_tools': self.api_action.restart_tools,\n                                 'start_tools': action.start,\n                                 'from_registry': registry_image}\n                        if tools_to_configure:\n                            kargs['next_tool'] = tools_to_configure[-1]\n                        self.parentApp.addForm('EDITOR' + t[0], EditorForm,\n                                               **kargs)\n                        tools_to_configure.append('EDITOR' + t[0])\n                    elif self.action['action_name'] == 'remove':\n                        status = self.action['action_object1'](repo, t[0])\n                    else:\n                        kargs = {'name': t[0],\n                                 'branch': t[1],\n                                 'version': t[2]}\n                        # add core recognition\n                        if self.action['cores']:\n                            kargs.update({'groups': 'core'})\n                        # use latest version for update, not necessarily\n                        # version in manifest\n                        if self.action['action_name'] == 'update':\n                            if t[2] != 'HEAD':\n                                repo_commits = self.tools_inst.repo_commits(repo)[\n                                    1]\n                                for branch in repo_commits:\n                                    if branch[0] == t[1]:\n                                        kargs.update(\n                                            {'new_version': branch[1][0]})\n                            else:\n                                kargs.update({'new_version': 'HEAD'})\n                        thr = Thread(target=self.action['action_object1'],\n                                     args=(),\n                                     kwargs=kargs)\n                        popup(originals, self.action['type'], thr,\n                              'Please wait, ' + self.action['present_t'] +\n                              '...')\n\n        if self.action['action_name'] != 'configure':\n            npyscreen.notify_confirm('Done ' + self.action['present_t'] + '.',\n                                     title=self.action['past_t'])\n            self.quit()\n        else:\n            if len(tools_to_configure) > 0:\n                self.parentApp.change_form(tools_to_configure[-1])\n            else:\n                npyscreen.notify_confirm('No tools selected, returning to'\n                                         ' main menu',\n                                         title='No action taken')\n                self.quit()", "response": "This function is called when the user is OK with the action of the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess an event and send it to the appropriate Rabbit channel.", "response": "def process(self, event):\n        \"\"\"\n        event.event_type\n            'modified' | 'created' | 'moved' | 'deleted'\n        event.is_directory\n            True | False\n        event.src_path\n            path/to/observed/file\n        \"\"\"\n        uid = str(uuid.uuid4())\n        hostname = os.environ.get('VENT_HOST')\n        if not hostname:\n            hostname = ''\n\n        try:\n            # TODO should directories be treated as bulk paths to send to a\n            #      plugin?\n            if not event.is_directory:\n                spath = event.src_path\n                # wait for long copies to finish\n                historicalSize = -1\n                while (historicalSize != os.path.getsize(spath)):\n                    historicalSize = os.path.getsize(spath)\n                    time.sleep(0.1)\n\n                if os.path.getsize(spath) == 0:\n                    spath = str(spath)\n                    print('file drop ignoring empty file: {0}'.format(spath))\n                    if spath.startswith('/files/trace_'):\n                        key = spath.split('_')[1]\n                        # Rabbit settings\n                        exchange = 'topic-poseidon-internal'\n                        exchange_type = 'topic'\n                        routing_key = 'poseidon.algos.decider'\n\n                        message = {}\n                        message[key] = {'valid': False, 'source': 'file_drop'}\n                        message = json.dumps(message)\n\n                        # Send Rabbit message\n                        try:\n                            connection = pika.BlockingConnection(\n                                pika.ConnectionParameters(host='rabbit')\n                            )\n\n                            channel = connection.channel()\n                            channel.exchange_declare(\n                                exchange=exchange, exchange_type=exchange_type\n                            )\n                            channel.basic_publish(exchange=exchange,\n                                                  routing_key=routing_key,\n                                                  body=message)\n                            connection.close()\n                        except Exception as e:\n                            print('failed to send rabbit message because: ' +\n                                  str(e))\n\n                    return\n\n                # check if the file was already queued and ignore\n                exists = False\n                print(uid + ' started ' + spath)\n                jobs = self.r.keys(pattern='rq:job*')\n                for job in jobs:\n                    print(uid + ' ***')\n                    description = self.r.hget(\n                        job, 'description').decode('utf-8')\n                    print(uid + ' ' + description)\n                    if description.startswith(\"watch.file_queue('\"):\n                        print(uid + ' ' +\n                              description.split(\"watch.file_queue('\" +\n                                                hostname + '_')[1][:-2])\n                        print(uid + ' ' + spath)\n                        if description.split(\"watch.file_queue('\" +\n                                             hostname +\n                                             '_')[1][:-2] == spath:\n                            print(uid + ' true')\n                            exists = True\n                    elif description.startswith(\"watch.gpu_queue('\"):\n                        print(uid + ' ' +\n                              description.split('\"file\": \"')[1].split('\"')[0])\n                        print(uid + ' ' + spath)\n                        if description.split('\"file\": \"')[1].split('\"')[0] == spath:\n                            print(uid + ' true')\n                            exists = True\n                    print(uid + ' ***')\n\n                if not exists:\n                    # !! TODO this should be a configuration option in the\n                    #         vent.template\n                    print(uid + \" let's queue it \" + spath)\n                    # let jobs be queued for up to 30 days\n                    self.q.enqueue('watch.file_queue',\n                                   hostname + '_' + spath,\n                                   ttl=2592000)\n                print(uid + ' end ' + spath)\n        except Exception as e:  # pragma: no cover\n            print('file drop error: ' + str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenProgressFunc(state=\"update\", action=None, text=None, tick=0):\n\tprint(\"tokenProgressFunc %s: %s\\n%s (%s)\"%(state, str(action), str(text), str(tick)))", "response": "Print a progress message for the current state of reading sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef section(self, section):\n        # check if the named section exists\n        if self.config.has_section(section):\n            return (True, self.config.items(section))\n        return (False, 'Section: ' + section + ' does not exist')", "response": "Returns a list of tuples of option value for the section"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef options(self, section):\n        if self.config.has_section(section):\n            return (True, self.config.options(section))\n        return (False, 'Section: ' + section + ' does not exist')", "response": "Returns a list of options for a section"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the option in the specified section", "response": "def option(self, section, option):\n        \"\"\" Returns the value of the option \"\"\"\n        if self.config.has_section(section):\n            if self.config.has_option(section, option):\n                return (True, self.config.get(section, option))\n            return (False, 'Option: ' + option + ' does not exist')\n        return (False, 'Section: ' + section + ' does not exist')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a section to the configuration file. Returns True if the section already exists returns list of sections.", "response": "def add_section(self, section):\n        \"\"\"\n        If section exists, returns log,\n        otherwise adds section and returns list of sections.\n        \"\"\"\n        # check if section already exists\n        if not self.config.has_section(section):\n            self.config.add_section(section)\n            # return updated sections\n            return (True, self.config.sections())\n        return (False, 'Section: ' + section + ' already exists')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an option to the config file.", "response": "def add_option(self, section, option, value=None):\n        \"\"\"\n        Creates an option for a section. If the section does\n        not exist, it will create the section.\n        \"\"\"\n        # check if section exists; create if not\n        if not self.config.has_section(section):\n            message = self.add_section(section)\n            if not message[0]:\n                return message\n\n        if not self.config.has_option(section, option):\n            if value:\n                self.config.set(section, option, value)\n            else:\n                self.config.set(section, option)\n            return(True, self.config.options(section))\n        return(False, 'Option: {} already exists @ {}'.format(option, section))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef del_section(self, section):\n        if self.config.has_section(section):\n            self.config.remove_section(section)\n            return (True, self.config.sections())\n        return (False, 'Section: ' + section + ' does not exist')", "response": "Deletes a section from the config file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an option from the config file", "response": "def del_option(self, section, option):\n        \"\"\" Deletes an option if the section and option exist \"\"\"\n        if self.config.has_section(section):\n            if self.config.has_option(section, option):\n                self.config.remove_option(section, option)\n                return (True, self.config.options(section))\n            return (False, 'Option: ' + option + ' does not exist')\n        return (False, 'Section: ' + section + ' does not exist')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting an option to a value in the given section.", "response": "def set_option(self, section, option, value):\n        \"\"\"\n        Sets an option to a value in the given section. Option is created if it\n        does not already exist\n        \"\"\"\n        if self.config.has_section(section):\n            self.config.set(section, option, value)\n            return (True, self.config.options(section))\n        return (False, 'Section: ' + section + ' does not exist')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef constrained_sections(self, constraints=None, options=None):\n        sections = {}\n        if not constraints:\n            constraints = {}\n        if not options:\n            options = []\n        all_sections = self.sections()\n        for a_section in all_sections[1]:\n            include = True\n            for constraint in constraints:\n                result = self.option(a_section, constraint)\n                if not result[0] or result[1] != constraints[constraint]:\n                    include = False\n                # handle group membership\n                if (result[0] and\n                    constraint == 'groups' and\n                        constraints[constraint] in result[1]):\n                    include = True\n            if include:\n                sections[a_section] = {}\n                for option in options:\n                    result = self.option(a_section, option)\n                    if result[0]:\n                        sections[a_section][option] = result[1]\n        return sections", "response": "Returns a dictionary of sections that must be present\n        in a section and optionally a dictionary of option values that must be present\n        in a section and optionally a dictionary of option values that must be present\n        in a section."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstrain the options against a template", "response": "def constrain_opts(self, constraint_dict, options):\n        \"\"\" Return result of constraints and options against a template \"\"\"\n        constraints = {}\n        for constraint in constraint_dict:\n            if constraint != 'self':\n                if (constraint_dict[constraint] or\n                        constraint_dict[constraint] == ''):\n                    constraints[constraint] = constraint_dict[constraint]\n        results = self.constrained_sections(constraints=constraints,\n                                            options=options)\n        return results, self.template"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of tuples of all tools", "response": "def list_tools(self):\n        \"\"\"\n        Return list of tuples of all tools\n        \"\"\"\n        tools = []\n        exists, sections = self.sections()\n        if exists:\n            for section in sections:\n                options = {'section': section,\n                           'built': None,\n                           'version': None,\n                           'repo': None,\n                           'branch': None,\n                           'name': None,\n                           'groups': None,\n                           'image_name': None}\n                for option in list(options.keys()):\n                    exists, value = self.option(section, option)\n                    if exists:\n                        options[option] = value\n                if 'core' not in options['groups'] and 'hidden' not in options['groups']:\n                    tools.append(options)\n        return tools"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride to prevent user from selecting too many instances", "response": "def when_value_edited(self, *args, **kargs):\n        \"\"\" Overrided to prevent user from selecting too many instances \"\"\"\n        if len(self.value) > self.instance_num:\n            self.value.pop(-2)\n            self.display()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_to_exit(self, *args, **kargs):\n        if len(self.value) == self.instance_num:\n            return True\n        return False", "response": "Overrided to prevent user from exiting selection until the user has selected the right amount of instances"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self):\n        self.add_handlers({'^E': self.quit, '^Q': self.quit})\n        to_delete = self.old_instances - self.new_instances\n        self.add(npyscreen.Textfield, value='Select which instances to delete'\n                 ' (you must select ' + str(to_delete) +\n                 ' instance(s) to delete):', editable=False, color='GOOD')\n        self.del_instances = self.add(InstanceSelect,\n                                      values=self.cur_instances,\n                                      scroll_exit=True, rely=3,\n                                      instance_num=to_delete)", "response": "Creates the necessary display for this form"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange to the next tool to edit or back to MAIN form", "response": "def change_screens(self):\n        \"\"\" Change to the next tool to edit or back to MAIN form \"\"\"\n        if self.next_tool:\n            self.parentApp.change_form(self.next_tool)\n        else:\n            self.parentApp.change_form('MAIN')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the instances that the user chose to delete", "response": "def on_ok(self):\n        \"\"\" Delete the instances that the user chose to delete \"\"\"\n        npyscreen.notify_wait('Deleting instances given...',\n                              title='In progress')\n        # keep track of number for shifting instances down for alignment\n        shift_num = 1\n        to_update = []\n        for i, val in enumerate(self.del_instances.values):\n            # clean all tools for renmaing and relabeling purposes\n            t = val.split(':')\n            section = self.display_to_section[val]\n            prev_running = self.manifest.option(section, 'running')\n            run = prev_running[0] and prev_running[1] == 'yes'\n            if i in self.del_instances.value:\n                if run:\n                    # grab dependencies of tools that linked to previous one\n                    if i == 0:\n                        dependent_tools = [self.manifest.option(\n                            section, 'link_name')[1]]\n                        for dependency in Dependencies(dependent_tools):\n                            self.clean(**dependency)\n                            to_update.append(dependency)\n                    self.clean(name=t[0], branch=t[1], version=t[2])\n                self.manifest.del_section(section)\n            else:\n                try:\n                    # update instances for tools remaining\n                    section = self.display_to_section[val]\n                    settings_dict = json.loads(self.manifest.option\n                                               (section, 'settings')[1])\n                    settings_dict['instances'] = self.new_instances\n                    self.manifest.set_option(section, 'settings',\n                                             json.dumps(settings_dict))\n                    # check if tool name doesn't need to be shifted because\n                    # it's already correct\n                    identifier = str(shift_num) if shift_num != 1 else ''\n                    new_section = section.rsplit(':', 2)\n                    new_section[0] = re.sub(r'\\d+$', identifier,\n                                            new_section[0])\n                    new_section = ':'.join(new_section)\n                    if section != new_section:\n                        # clean tool so that we can rename its container and\n                        # labels with new information\n                        self.clean(name=t[0], branch=t[1], version=t[2])\n                        prev_name = self.manifest.option(section, 'name')[1]\n                        new_name = re.split(r'[0-9]', prev_name)[0] + \\\n                            identifier\n                        self.manifest.set_option(section, 'name', new_name)\n                        # copy new contents into shifted version\n                        self.manifest.add_section(new_section)\n                        for val_pair in self.manifest.section(section)[1]:\n                            self.manifest.set_option(new_section, val_pair[0],\n                                                     val_pair[1])\n                        self.manifest.del_section(section)\n                        if run:\n                            to_update.append({'name': new_name,\n                                              'branch': t[1],\n                                              'version': t[2]})\n                    shift_num += 1\n                except Exception as e:\n                    npyscreen.notify_confirm('Trouble deleting tools'\n                                             ' because ' + str(e))\n        self.manifest.write_config()\n        tool_d = {}\n        for tool in to_update:\n            tool_d.update(self.prep_start(**tool)[1])\n        if tool_d:\n            self.start_tools(tool_d)\n        npyscreen.notify_confirm('Done deleting instances.',\n                                 title='Finished')\n        self.change_screens()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a POST request with id/nic/interval/filter/iters and it will start a container for collection with those specifications", "response": "def on_post(self, req, resp):\n        \"\"\"\n        Send a POST request with id/nic/interval/filter/iters and it will start\n        a container for collection with those specifications\n        \"\"\"\n        resp.content_type = falcon.MEDIA_TEXT\n        resp.status = falcon.HTTP_200\n\n        # verify payload is in the correct format\n        # default to no filter\n        payload = {}\n        if req.content_length:\n            try:\n                payload = json.load(req.stream)\n            except Exception as e:  # pragma: no cover\n                resp.body = \"(False, 'malformed payload')\"\n                return\n        else:\n            resp.body = \"(False, 'malformed payload')\"\n            return\n\n        if 'filter' not in payload:\n            payload['filter'] = ''\n\n        # payload should have the following fields:\n        # - id\n        # - nic\n        # - interval\n        # - filter\n        # - iters\n        # should spin up a tcpdump container that writes out pcap files based\n        # on the filter needs to be attached to the nic specified, if iters is\n        # -1 then loops until killed, otherwise completes iters number of\n        # captures (and creates that many pcap files) should keep track of\n        # container id, container name, and id of filter and filter + whatever\n\n        # verify payload has necessary information\n        if 'nic' not in payload:\n            resp.body = \"(False, 'payload missing nic')\"\n            return\n        if 'id' not in payload:\n            resp.body = \"(False, 'payload missing id')\"\n            return\n        if 'interval' not in payload:\n            resp.body = \"(False, 'payload missing interval')\"\n            return\n        if 'iters' not in payload:\n            resp.body = \"(False, 'payload missing iters')\"\n            return\n\n        # connect to docker\n        c = None\n        try:\n            c = docker.from_env()\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'unable to connect to docker because: \" + str(e) + \"')\"\n            return\n\n        # spin up container with payload specifications\n        if c:\n            tool_d = {'network_mode': 'host',\n                      'volumes_from': [socket.gethostname()]}\n\n            cmd = '/tmp/run.sh ' + payload['nic'] + ' ' + payload['interval']\n            cmd += ' ' + payload['id'] + ' ' + payload['iters'] + ' \"'\n            cmd += payload['filter'] + '\"'\n            try:\n                container = c.containers.run(image='cyberreboot/vent-ncapture:master',\n                                             command=cmd, remove=True, detach=True, **tool_d)\n                resp.body = \"(True, 'successfully created and started filter: \" + \\\n                    str(payload['id']) + ' on container: ' + \\\n                    str(container.id) + \"')\"\n            except Exception as e:  # pragma: no cover\n                resp.body = \"(False, 'unable to start container because: \" + str(e) + \"')\"\n                return\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a POST request with a docker container ID and it will be deleted.", "response": "def on_post(self, req, resp):\n        \"\"\"\n        Send a POST request with a docker container ID and it will be deleted.\n\n        Example input: {'id': \"12345\"}, {'id': [\"123\", \"456\"]}\n        \"\"\"\n        resp.content_type = falcon.MEDIA_TEXT\n        resp.status = falcon.HTTP_200\n\n        # verify user input\n        payload = {}\n        if req.content_length:\n            try:\n                payload = json.load(req.stream)\n            except Exception as e:  # pragma: no cover\n                resp.body = \"(False, 'malformed payload')\"\n                return\n        else:\n            resp.body = \"(False, 'malformed payload')\"\n            return\n\n        # verify payload has a container ID\n        if 'id' not in payload:\n            resp.body = \"(False, 'payload missing id')\"\n            return\n\n        # connect to docker and stop the given container\n        c = None\n        try:\n            c = docker.from_env()\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'unable to connect to docker because: \" + str(e) + \"')\"\n            return\n\n        # delete containers chosen from CLI\n        try:\n            for container_id in payload['id']:\n                c.containers.get(container_id).remove()\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'unable to delete containers because: \" + str(e) + \"')\"\n            return\n\n        resp.body = '(True, ' + str(payload['id']) + ')'\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a GET request to get the list of all of the filter containers", "response": "def on_get(self, req, resp):\n        \"\"\"\n        Send a GET request to get the list of all of the filter containers\n        \"\"\"\n        resp.content_type = falcon.MEDIA_TEXT\n        resp.status = falcon.HTTP_200\n\n        # connect to docker\n        try:\n            containers = docker.from_env()\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'unable to connect to docker because: \" + str(e) + \"')\"\n            return\n\n        # search for all docker containers and grab ncapture containers\n        container_list = []\n        try:\n            for c in containers.containers.list(all=True):\n                # TODO: maybe find a way to not have to hard code image name\n                if c.attrs['Config']['Image'] == \\\n                        'cyberreboot/vent-ncapture:master':\n                    # the core container is not what we want\n                    if 'core' not in c.attrs['Config']['Labels']['vent.groups']:\n                        lst = {}\n                        lst['id'] = c.attrs['Id'][:12]\n                        lst['status'] = c.attrs['State']['Status']\n                        lst['args'] = c.attrs['Args']\n                        container_list.append(lst)\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'Failure because: \" + str(e) + \"')\"\n            return\n\n        resp.body = json.dumps(container_list)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_get(self, req, resp):\n        resp.content_type = falcon.MEDIA_TEXT\n        resp.status = falcon.HTTP_200\n\n        # connect to docker\n        try:\n            d_client = docker.from_env()\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'unable to connect to docker because: \" + str(e) + \"')\"\n            return\n\n        # start container to get network interfaces\n        nics = ''\n        try:\n            nics = d_client.containers.run('cyberreboot/gonet',\n                                           network_mode='host', remove=True)\n            resp.body = '(True, ' + str(nics.id) + ')'\n        except Exception as e:  # pragma: no cover\n            resp.body = \"(False, 'Failure because: \" + str(e) + \"')\"\n            return\n\n        return", "response": "Send a GET request to get the list of all available network interfaces"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the base menu for the current locale.", "response": "def create(self):\n        \"\"\" Overridden to add handlers and content \"\"\"\n        self.add_handlers({'^Q': self.quit})\n        self.add(npyscreen.TitleText, name=self.title, editable=False)\n        self.add(npyscreen.MultiLineEdit, editable=False, value=self.text,\n                 max_width=75, slow_scroll=True)\n        self.m2 = self.add_menu(name='About Vent', shortcut='v')\n        self.m2.addItem(text='Background', onSelect=self.switch,\n                        arguments=['TUTORIALBACKGROUND'], shortcut='b')\n        self.m2.addItem(text='Terminology', onSelect=self.switch,\n                        arguments=['TUTORIALTERMINOLOGY'], shortcut='t')\n        self.m2.addItem(text='Getting Setup', onSelect=self.switch,\n                        arguments=['TUTORIALGETTINGSETUP'], shortcut='s')\n        self.m3 = self.add_menu(name='Working with Cores', shortcut='c')\n        self.m3.addItem(text='Starting Cores', onSelect=self.switch,\n                        arguments=['TUTORIALSTARTINGCORES'], shortcut='c')\n        self.m4 = self.add_menu(name='Working with Plugins', shortcut='p')\n        self.m4.addItem(text='Adding Plugins', onSelect=self.switch,\n                        arguments=['TUTORIALADDINGPLUGINS'], shortcut='a')\n        self.m5 = self.add_menu(name='Files', shortcut='f')\n        self.m5.addItem(text='Adding Files', onSelect=self.switch,\n                        arguments=['TUTORIALADDINGFILES'], shortcut='a')\n        self.m6 = self.add_menu(name='Help', shortcut='s')\n        self.m6.addItem(text='Basic Troubleshooting', onSelect=self.switch,\n                        arguments=['TUTORIALTROUBLESHOOTING'], shortcut='t')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the backup files to select from", "response": "def create(self):\n        \"\"\" Add backup files to select from \"\"\"\n        self.add_handlers({'^T': self.quit})\n        self.add(npyscreen.Textfield, value='Pick a version to restore from: ',\n                 editable=False, color='STANDOUT')\n        self.dir_select = self.add(npyscreen.SelectOne,\n                                   values=self.display_vals,\n                                   scroll_exit=True, rely=4)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_ok(self):\n        if self.dir_select.value:\n            npyscreen.notify_wait('In the process of restoring',\n                                  title='Restoring...')\n            status = self.restore(self.dirs[self.dir_select.value[0]])\n            if status[0]:\n                npyscreen.notify_confirm('Status of restore:\\n' +\n                                         status[1])\n            else:\n                npyscreen.notify_confirm(status[1])\n            self.quit()\n        else:\n            npyscreen.notify_confirm('Choose a version to restore from')", "response": "Perform restoration on the backup file selected"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_dir(path):\n        try:\n            makedirs(path)\n        except OSError as e:  # pragma: no cover\n            if e.errno == errno.EEXIST and isdir(path):\n                return (True, 'exists')\n            else:\n                return (False, e)\n        return (True, path)", "response": "Ensures that the directory path exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if file exists and creates it if not", "response": "def ensure_file(path):\n        \"\"\" Checks if file exists, if fails, tries to create file \"\"\"\n        try:\n            exists = isfile(path)\n            if not exists:\n                with open(path, 'w+') as fname:\n                    fname.write('initialized')\n                return (True, path)\n            return (True, 'exists')\n        except OSError as e:  # pragma: no cover\n            return (False, e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts relative path to a tool from the main cloned directory", "response": "def rel_path(name, available_tools):\n        \"\"\"\n        Extracts relative path to a tool (from the main cloned directory) out\n        of available_tools based on the name it is given\n        \"\"\"\n        if name == '@' or name == '.' or name == '/':\n            name = ''\n        multi_tool = '@' in name\n        for tool in available_tools:\n            t_name = tool[0].lower()\n            if multi_tool:\n                if name.split('@')[-1] == t_name.split('@')[-1]:\n                    return t_name, t_name\n            else:\n                if name == t_name.split('/')[-1]:\n                    return t_name, tool[0]\n                elif name == '' and t_name.split('@')[-1] == 'unspecified':\n                    return '', ''\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef host_config(self):\n        if platform.system() == 'Darwin':\n            default_file_dir = join(expanduser('~'),\n                                    'vent_files')\n        else:\n            default_file_dir = '/opt/vent_files'\n        status = self.ensure_dir(default_file_dir)\n        if not isfile(self.cfg_file):\n            config = Template(template=self.cfg_file)\n            sections = {'main': {'files': default_file_dir},\n                        'network-mapping': {},\n                        'nvidia-docker-plugin': {'port': '3476'}}\n            for s in sections:\n                if sections[s]:\n                    for option in sections[s]:\n                        config.add_option(s, option, sections[s][option])\n                else:\n                    config.add_section(s)\n            config.write_config()\n        return status", "response": "Ensure the host configuration file exists"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_path(self, repo):\n        try:\n            # rewrite repo for consistency\n            if repo.endswith('.git'):\n                repo = repo.split('.git')[0]\n\n            # get org and repo name and path repo will be cloned to\n            org, name = repo.split('/')[-2:]\n            path = join(self.plugins_dir, org, name)\n\n            # save current path\n            cwd = getcwd()\n\n            # set to new repo path\n            self.ensure_dir(path)\n            chdir(path)\n            status = (True, cwd, path)\n        except Exception as e:  # pragma: no cover\n            status = (False, str(e))\n        return status", "response": "Set path to where the repo is and return original path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the path for the repo", "response": "def get_path(self, repo):\n        \"\"\" Return the path for the repo \"\"\"\n        if repo.endswith('.git'):\n            repo = repo.split('.git')[0]\n        org, name = repo.split('/')[-2:]\n        path = self.plugins_dir\n        path = join(path, org, name)\n        return path, org, name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef override_config(self, path):\n        status = (True, None)\n        config_override = False\n\n        try:\n            # parse the yml file\n            c_dict = {}\n            if exists(self.plugin_config_file):\n                with open(self.plugin_config_file, 'r') as config_file:\n                    c_dict = yaml.safe_load(config_file.read())\n\n            # check for environment variable overrides\n            check_c_dict = c_dict.copy()\n            for tool in check_c_dict:\n                for section in check_c_dict[tool]:\n                    for key in check_c_dict[tool][section]:\n                        if key in environ:\n                            c_dict[tool][section][key] = getenv(key)\n\n            # assume the name of the plugin is its directory\n            plugin_name = path.split('/')[-1]\n            if plugin_name == '':\n                plugin_name = path.split('/')[-2]\n            plugin_config_path = path + '/config/' + plugin_name + '.config'\n\n            if exists(plugin_config_path):\n                plugin_template = Template(plugin_config_path)\n                plugin_options = c_dict[plugin_name]\n                for section in plugin_options:\n                    for option in plugin_options[section]:\n                        plugin_template.set_option(section, option,\n                                                   str(plugin_options[section][option]))\n                plugin_template.write_config()\n                config_override = True\n        except Exception as e:  # pragma: no cover\n            status = (False, str(e))\n\n        return status, config_override", "response": "This function will take a yml located in home directory titled. plugin_config. yml. It will then override the plugin s config file with the values from the environment variable overrides\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of log entries for all the log containers in the system.", "response": "def Logs(c_type=None, grep_list=None):\n    \"\"\" Generically filter logs stored in log containers \"\"\"\n    def get_logs(logs, log_entries):\n        try:\n            for log in logs:\n                if str(container.name) in log_entries:\n                    log_entries[str(container.name)].append(log)\n                else:\n                    log_entries[str(container.name)] = [log]\n        except Exception as e:  # pragma: no cover\n            logger.error('Unable to get logs for ' +\n                         str(container.name) +\n                         ' because: ' + str(e))\n        return log_entries\n\n    status = (True, None)\n    log_entries = {}\n    d_client = docker.from_env()\n    containers = d_client.containers.list(all=True,\n                                          filters={'label': 'vent'})\n    logger.debug('containers found: ' + str(containers))\n    comp_c = containers\n    if c_type:\n        try:\n            comp_c = [c for c in containers\n                      if (c_type\n                          in c.attrs['Config']['Labels']['vent.groups'])]\n        except Exception as e:  # pragma: no cover\n            logger.error('Unable to limit containers by: ' +\n                         str(c_type) + ' because: ' +\n                         str(e))\n\n    if grep_list:\n        for expression in grep_list:\n            for container in comp_c:\n                try:\n                    # 'logs' stores each line containing the expression\n                    logs = [log for log in container.logs().split('\\n')\n                            if expression in log]\n                    log_entries = get_logs(logs, log_entries)\n                except Exception as e:  # pragma: no cover\n                    logger.info('Unable to get logs for ' +\n                                str(container) +\n                                ' because: ' + str(e))\n    else:\n        for container in comp_c:\n            try:\n                logs = container.logs().split('\\n')\n                log_entries = get_logs(logs, log_entries)\n            except Exception as e:  # pragma: no cover\n                logger.info('Unabled to get logs for ' +\n                            str(container) +\n                            ' because: ' + str(e))\n\n    status = (True, log_entries)\n    return status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting Docker setup information", "response": "def Docker():\n    \"\"\" Get Docker setup information \"\"\"\n    docker_info = {'server': {}, 'env': '', 'type': '', 'os': ''}\n\n    # get docker server version\n    try:\n        d_client = docker.from_env()\n        docker_info['server'] = d_client.version()\n    except Exception as e:  # pragma: no cover\n        logger.error(\"Can't get docker info \" + str(e))\n\n    # get operating system\n    system = System()\n    docker_info['os'] = system\n\n    # check if native or using docker-machine\n    if 'DOCKER_MACHINE_NAME' in environ:\n        # using docker-machine\n        docker_info['env'] = environ['DOCKER_MACHINE_NAME']\n        docker_info['type'] = 'docker-machine'\n    elif 'DOCKER_HOST' in environ:\n        # not native\n        docker_info['env'] = environ['DOCKER_HOST']\n        docker_info['type'] = 'remote'\n    else:\n        # using \"local\" server\n        docker_info['type'] = 'native'\n    return docker_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Containers(vent=True, running=True, exclude_labels=None):\n    containers = []\n\n    try:\n        d_client = docker.from_env()\n        if vent:\n            c = d_client.containers.list(all=not running,\n                                         filters={'label': 'vent'})\n        else:\n            c = d_client.containers.list(all=not running)\n        for container in c:\n            include = True\n            if exclude_labels:\n                for label in exclude_labels:\n                    if 'vent.groups' in container.labels and label in container.labels['vent.groups']:\n                        include = False\n            if include:\n                containers.append((container.name, container.status))\n    except Exception as e:  # pragma: no cover\n        logger.error('Docker problem ' + str(e))\n\n    return containers", "response": "Get containers that are created by default limit to vent containers that are running"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting number of available CPUs", "response": "def Cpu():\n    \"\"\" Get number of available CPUs \"\"\"\n    cpu = 'Unknown'\n\n    try:\n        cpu = str(multiprocessing.cpu_count())\n    except Exception as e:  # pragma: no cover\n        logger.error(\"Can't access CPU count' \" + str(e))\n    return cpu"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking for support of GPUs and return what s available", "response": "def Gpu(pull=False):\n    \"\"\" Check for support of GPUs, and return what's available \"\"\"\n    gpu = (False, '')\n\n    try:\n        image = 'nvidia/cuda:8.0-runtime'\n        image_name, tag = image.split(':')\n        d_client = docker.from_env()\n        nvidia_image = d_client.images.list(name=image)\n\n        if pull and len(nvidia_image) == 0:\n            try:\n                d_client.images.pull(image_name, tag=tag)\n                nvidia_image = d_client.images.list(name=image)\n            except Exception as e:  # pragma: no cover\n                logger.error('Something with the GPU went wrong ' + str(e))\n\n        if len(nvidia_image) > 0:\n            cmd = 'nvidia-docker run --rm ' + image + ' nvidia-smi -L'\n            proc = Popen([cmd],\n                         stdout=PIPE,\n                         stderr=PIPE,\n                         shell=True,\n                         close_fds=True)\n            gpus = proc.stdout.read()\n            err = proc.stderr.read()\n            if gpus:\n                gpu_str = ''\n                for line in gpus.strip().split('\\n'):\n                    gpu_str += line.split(' (UUID: ')[0] + ', '\n                gpu = (True, gpu_str[:-2])\n            else:\n                if err:\n                    gpu = (False, 'Unknown', str(err))\n                else:\n                    gpu = (False, 'None')\n        else:\n            gpu = (False, 'None')\n    except Exception as e:  # pragma: no cover\n        gpu = (False, 'Unknown', str(e))\n    return gpu"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GpuUsage(**kargs):\n    usage = (False, None)\n    gpu_status = {'vent_usage': {'dedicated': [], 'mem_mb': {}}}\n\n    path_dirs = PathDirs(**kargs)\n    path_dirs.host_config()\n    template = Template(template=path_dirs.cfg_file)\n\n    # get running jobs using gpus\n    try:\n        d_client = docker.from_env()\n        c = d_client.containers.list(all=False,\n                                     filters={'label': 'vent-plugin'})\n        for container in c:\n            if ('vent.gpu' in container.attrs['Config']['Labels'] and\n                    container.attrs['Config']['Labels']['vent.gpu'] == 'yes'):\n                device = container.attrs['Config']['Labels']['vent.gpu.device']\n                if ('vent.gpu.dedicated' in container.attrs['Config']['Labels'] and\n                        container.attrs['Config']['Labels']['vent.gpu.dedicated'] == 'yes'):\n                    gpu_status['vent_usage']['dedicated'].append(device)\n                elif 'vent.gpu.mem_mb' in container.attrs['Config']['Labels']:\n                    if device not in gpu_status['vent_usage']['mem_mb']:\n                        gpu_status['vent_usage']['mem_mb'][device] = 0\n                    gpu_status['vent_usage']['mem_mb'][device] += int(\n                        container.attrs['Config']['Labels']['vent.gpu.mem_mb'])\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get running jobs ' + str(e))\n\n    port = '3476'\n    # default docker gateway\n    host = '172.17.0.1'\n    result = template.option('nvidia-docker-plugin', 'port')\n    if result[0]:\n        port = result[1]\n    result = template.option('nvidia-docker-plugin', 'host')\n    if result[0]:\n        host = result[1]\n    else:\n        try:\n            # now just requires ip, ifconfig\n            route = check_output(('ip', 'route')).decode('utf-8').split('\\n')\n            default = ''\n            # grab the default network device.\n            for device in route:\n                if 'default' in device:\n                    default = device.split()[4]\n                    break\n\n            # grab the IP address for the default device\n            ip_addr = check_output(('ifconfig', default)).decode('utf-8')\n            ip_addr = ip_addr.split('\\n')[1].split()[1]\n            host = ip_addr\n        except Exception as e:  # pragma: no cover\n            logger.error('Something with the ip addresses'\n                         'went wrong ' + str(e))\n\n    # have to get the info separately to determine how much memory is availabe\n    nd_url = 'http://' + host + ':' + port + '/v1.0/gpu/info/json'\n    try:\n        r = requests.get(nd_url)\n        if r.status_code == 200:\n            status = r.json()\n            for i, device in enumerate(status['Devices']):\n                gm = int(round(math.log(int(device['Memory']['Global']), 2)))\n                gpu_status[i] = {'global_memory': 2**gm,\n                                 'cores': device['Cores']}\n        else:\n            usage = (False, 'Unable to get GPU usage request error code: ' +\n                     str(r.status_code))\n    except Exception as e:  # pragma: no cover\n        usage = (False, 'Error: ' + str(e))\n\n    # get actual status of each gpu\n    nd_url = 'http://' + host + ':' + port + '/v1.0/gpu/status/json'\n    try:\n        r = requests.get(nd_url)\n        if r.status_code == 200:\n            status = r.json()\n            for i, device in enumerate(status['Devices']):\n                if i not in gpu_status:\n                    gpu_status[i] = {}\n                gpu_status[i]['utilization'] = device['Utilization']\n                gpu_status[i]['memory'] = device['Memory']\n                gpu_status[i]['processes'] = device['Processes']\n            usage = (True, gpu_status)\n        else:\n            usage = (False, 'Unable to get GPU usage request error code: ' +\n                     str(r.status_code))\n    except Exception as e:  # pragma: no cover\n        usage = (False, 'Error: ' + str(e))\n\n    return usage", "response": "Get the current GPU usage of available GPUs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets images that are build by default limit to vent images", "response": "def Images(vent=True):\n    \"\"\" Get images that are build, by default limit to vent images \"\"\"\n    images = []\n    # TODO needs to also check images in the manifest that couldn't have the\n    #      label added\n    try:\n        d_client = docker.from_env()\n        if vent:\n            i = d_client.images.list(filters={'label': 'vent'})\n        else:\n            i = d_client.images.list()\n        for image in i:\n            images.append((image.tags[0], image.short_id))\n    except Exception as e:  # pragma: no cover\n        logger.error('Something with the Images went wrong ' + str(e))\n\n    return images"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Jobs():\n    jobs = [0, 0, 0, 0]\n\n    # get running jobs\n    try:\n        d_client = docker.from_env()\n        c = d_client.containers.list(all=False,\n                                     filters={'label': 'vent-plugin'})\n        files = []\n        for container in c:\n            jobs[1] += 1\n            if 'file' in container.attrs['Config']['Labels']:\n                if container.attrs['Config']['Labels']['file'] not in files:\n                    files.append(container.attrs['Config']['Labels']['file'])\n        jobs[0] = len(files)\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get running jobs ' + str(e))\n\n    # get finished jobs\n    try:\n        d_client = docker.from_env()\n        c = d_client.containers.list(all=True,\n                                     filters={'label': 'vent-plugin',\n                                              'status': 'exited'})\n\n        file_names = []\n        tool_names = []\n        finished_jobs = []\n        path_dirs = PathDirs()\n        manifest = join(path_dirs.meta_dir, 'status.json')\n\n        if exists(manifest):\n            file_status = 'a'\n        else:\n            file_status = 'w'\n\n        # get a list of past jobs' file names if status.json exists\n        if file_status == 'a':\n            with open(manifest, 'r') as infile:\n                for line in infile:\n                    finished_jobs.append(json.loads(line))\n\n            # get a list of file names so we can check against each container\n            file_names = [d['FileName'] for d in finished_jobs]\n\n            # multiple tools can run on 1 file. Use a tuple to status check\n            tool_names = [(d['FileName'], d['VentPlugin'])\n                          for d in finished_jobs]\n\n        for container in c:\n            jobs[3] += 1\n\n            if 'file' in container.attrs['Config']['Labels']:\n                # make sure the file name and the tool tup exists because\n                # multiple tools can run on 1 file.\n                if (container.attrs['Config']['Labels']['file'],\n                    container.attrs['Config']['Labels']['vent.name']) not in \\\n                        tool_names:\n                    # TODO figure out a nicer way of getting desired values\n                    # from containers.attrs.\n                    new_file = {}\n                    new_file['FileName'] = \\\n                        container.attrs['Config']['Labels']['file']\n                    new_file['VentPlugin'] = \\\n                        container.attrs['Config']['Labels']['vent.name']\n                    new_file['StartedAt'] = \\\n                        container.attrs['State']['StartedAt']\n                    new_file['FinishedAt'] = \\\n                        container.attrs['State']['FinishedAt']\n                    new_file['ID'] = \\\n                        container.attrs['Id'][:12]\n\n                    # create/append a json file with all wanted information\n                    with open(manifest, file_status) as outfile:\n                        json.dump(new_file, outfile)\n                        outfile.write('\\n')\n\n                # delete any containers with 'vent-plugin' in the groups\n                if 'vent-plugin' in container.attrs['Config']['Labels']:\n                    container.remove()\n\n        # add extra one to account for file that just finished if the file was\n        # just created since file_names is processed near the beginning\n        if file_status == 'w' and len(file_names) == 1:\n            jobs[2] = len(set(file_names)) + 1\n        else:\n            jobs[2] = len(set(file_names))\n\n        jobs[3] = jobs[3] - jobs[1]\n\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get finished jobs ' + str(e))\n\n    return tuple(jobs)", "response": "Get the number of running and finished jobs for the given node"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets tools that exist in the manifest", "response": "def ManifestTools(**kargs):\n    \"\"\" Get tools that exist in the manifest \"\"\"\n    path_dirs = PathDirs(**kargs)\n    manifest = join(path_dirs.meta_dir, 'plugin_manifest.cfg')\n    template = Template(template=manifest)\n    tools = template.sections()\n    return tools[1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of possible tools in repo for the given version and branch.", "response": "def AvailableTools(path, tools=None, branch='master', version='HEAD', core=False):\n    \"\"\"\n    Return list of possible tools in repo for the given version and branch or\n    take a list of tools to get paths for specific version and branch\n    tools is a list of tuples: (tool name, branch, version)\n    \"\"\"\n    def get_dockerfiles(matches, path, tool=None):\n        for root, _, filenames in walk(path):\n            files = fnmatch.filter(filenames, 'Dockerfile*')\n            # append additional identifiers to tools if multiple in same\n            # directory\n            add_info = len(files) > 1\n            for f in files:\n                if core and 'core' not in root:\n                    pass\n                else:\n                    addtl_info = ''\n                    if add_info:\n                        # @ will be delimiter symbol for multi-tools\n                        try:\n                            addtl_info = '@' + f.split('.')[1]\n                        except Exception as e:\n                            addtl_info = '@unspecified'\n                    if tool:\n                        if root.split(path)[1].rsplit('/', 1)[-1].lower() == tool:\n                            matches.append((root.split(path)[1] + addtl_info,\n                                            version))\n                        elif tool == '@':\n                            matches.append(('' + addtl_info, version))\n                    else:\n                        matches.append(\n                            (root.split(path)[1] + addtl_info, version))\n        return matches\n\n    matches = []\n    if tools:\n        for tool in tools:\n            name = tool[0]\n            branch = tool[1]\n            version = tool[2]\n            status = Checkout(path, branch=branch, version=version)\n            if not status[0]:\n                logger.error('Unable to checkout: {0} {1} {2} because: {3}'.format(\n                    path, branch, version, status[1]))\n            matches = get_dockerfiles(matches, path, tool=name)\n    else:\n        status = Checkout(path, branch=branch, version=version)\n        if not status[0]:\n            logger.error('Unable to checkout: {0} {1} {2} because: {3}'.format(\n                path, branch, version, status[1]))\n        matches = get_dockerfiles(matches, path)\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of paths and versions that were specified.", "response": "def ToolMatches(tools=None, version='HEAD'):\n    \"\"\" Get the tools paths and versions that were specified \"\"\"\n    matches = []\n    if tools:\n        for tool in tools:\n            match_version = version\n            if tool[1] != '':\n                match_version = tool[1]\n            match = ''\n            if tool[0].endswith('/'):\n                match = tool[0][:-1]\n            elif tool[0] != '.':\n                match = tool[0]\n            if not match.startswith('/') and match != '':\n                match = '/'+match\n            matches.append((match, match_version))\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget services that have exposed ports", "response": "def Services(core, vent=True, external=False, **kargs):\n    \"\"\"\n    Get services that have exposed ports, expects param core to be True or\n    False based on which type of services to return, by default limit to vent\n    containers and processes not running externally, if not limited by vent\n    containers, then core is ignored.\n    \"\"\"\n    services = []\n    path_dirs = PathDirs(**kargs)\n    template = Template(template=path_dirs.cfg_file)\n    services_uri = template.option('main', 'services_uri')\n    try:\n        # look for internal services\n        if not external:\n            d_client = docker.from_env()\n            if vent:\n                c_filter = {'label': 'vent'}\n                containers = d_client.containers.list(filters=c_filter)\n            else:\n                containers = d_client.containers.list()\n            for c in containers:\n                uris = {}\n                name = None\n                if vent and 'vent.name' in c.attrs['Config']['Labels']:\n                    if ((core and\n                         'vent.groups' in c.attrs['Config']['Labels'] and\n                         'core' in c.attrs['Config']['Labels']['vent.groups']) or\n                        (not core and\n                         'vent.groups' in c.attrs['Config']['Labels'] and\n                         'core' not in c.attrs['Config']['Labels']['vent.groups'])):\n                        name = c.attrs['Config']['Labels']['vent.name']\n                        if name == '':\n                            name = c.attrs['Config']['Labels']['vent.namespace'].split(\n                                '/')[1]\n                        for label in c.attrs['Config']['Labels']:\n                            if label.startswith('uri'):\n                                try:\n                                    val = int(label[-1])\n                                    if val not in uris:\n                                        uris[val] = {}\n                                    uris[val][label[:-1]\n                                              ] = c.attrs['Config']['Labels'][label]\n                                except Exception as e:  # pragma: no cover\n                                    logger.error('Malformed services section'\n                                                 ' in the template file '\n                                                 + str(e))\n                else:\n                    name = c.name\n                if name and 'vent.repo' in c.attrs['Config']['Labels']:\n                    name = c.attrs['Config']['Labels']['vent.repo'].split(\n                        '/')[-1] + ': ' + name\n                ports = c.attrs['NetworkSettings']['Ports']\n                p = []\n                port_num = 1\n                for port in ports:\n                    if ports[port]:\n                        try:\n                            service_str = ''\n                            if 'uri_prefix' in uris[port_num]:\n                                service_str += uris[port_num]['uri_prefix']\n                            host = ports[port][0]['HostIp']\n                            if services_uri[0] and host == '0.0.0.0':\n                                host = services_uri[1]\n                            service_str += host + ':'\n                            service_str += ports[port][0]['HostPort']\n                            if 'uri_postfix' in uris[port_num]:\n                                service_str += uris[port_num]['uri_postfix']\n                            uri_creds = ''\n                            if 'uri_user' in uris[port_num]:\n                                uri_creds += ' user:'\n                                uri_creds += uris[port_num]['uri_user']\n                            if 'uri_pw' in uris[port_num]:\n                                uri_creds += ' pw:'\n                                uri_creds += uris[port_num]['uri_pw']\n                            if uri_creds:\n                                service_str += ' - (' + uri_creds + ' )'\n                            p.append(service_str)\n                        except Exception as e:  # pragma: no cover\n                            logger.info('No services defined for ' + str(name) + ' with exposed port ' +\n                                        str(port_num) + ' because: ' + str(e))\n                        port_num += 1\n                if p and name:\n                    services.append((name, p))\n                    logger.info(services)\n        # look for external services\n        else:\n            ext_tools = template.section('external-services')[1]\n            for ext_tool in ext_tools:\n                try:\n                    name = ext_tool[0].lower()\n                    p = []\n                    settings_dict = json.loads(ext_tool[1])\n                    if ('locally_active' in settings_dict and\n                            settings_dict['locally_active'] == 'no'):\n                        # default protocol to display will be http\n                        protocol = 'http'\n                        ip_address = ''\n                        port = ''\n                        for setting in settings_dict:\n                            if setting == 'ip_address':\n                                ip_address = settings_dict[setting]\n                            if setting == 'port':\n                                port = settings_dict[setting]\n                            if setting == 'protocol':\n                                protocol = settings_dict[setting]\n                        p.append(protocol + '://' + ip_address + ':' + port)\n                    if p and name:\n                        services.append((name, p))\n                except Exception:  # pragma: no cover\n                    p = None\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get services ' + str(e))\n    return services"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the current datetime in UTC", "response": "def Timestamp():\n    \"\"\" Get the current datetime in UTC \"\"\"\n    timestamp = ''\n    try:\n        timestamp = str(datetime.datetime.now())+' UTC'\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get current time ' + str(e))\n    return timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the current uptime information", "response": "def Uptime():\n    \"\"\" Get the current uptime information \"\"\"\n    uptime = ''\n    try:\n        uptime = check_output(['uptime'], close_fds=True).decode('utf-8')[1:]\n    except Exception as e:  # pragma: no cover\n        logger.error('Could not get current uptime ' + str(e))\n    return uptime"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the location of the file drop", "response": "def DropLocation():\n    \"\"\" Get the directory that file drop is watching \"\"\"\n    template = Template(template=PathDirs().cfg_file)\n    drop_loc = template.option('main', 'files')[1]\n    drop_loc = expanduser(drop_loc)\n    drop_loc = abspath(drop_loc)\n    return (True, drop_loc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParsedSections(file_val):\n    try:\n        template_dict = {}\n        cur_section = ''\n        for val in file_val.split('\\n'):\n            val = val.strip()\n            if val != '':\n                section_match = re.match(r'\\[.+\\]', val)\n                if section_match:\n                    cur_section = section_match.group()[1:-1]\n                    template_dict[cur_section] = {}\n                else:\n                    option, value = val.split('=', 1)\n                    option = option.strip()\n                    value = value.strip()\n                    if option.startswith('#'):\n                        template_dict[cur_section][val] = ''\n                    else:\n                        template_dict[cur_section][option] = value\n    except Exception:  # pragma: no cover\n        template_dict = {}\n    return template_dict", "response": "Parses the file contents into a dictionary of sections and options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Dependencies(tools):\n    dependencies = []\n    if tools:\n        path_dirs = PathDirs()\n        man = Template(join(path_dirs.meta_dir, 'plugin_manifest.cfg'))\n        for section in man.sections()[1]:\n            # don't worry about dealing with tool if it's not running\n            running = man.option(section, 'running')\n            if not running[0] or running[1] != 'yes':\n                continue\n            t_name = man.option(section, 'name')[1]\n            t_branch = man.option(section, 'branch')[1]\n            t_version = man.option(section, 'version')[1]\n            t_identifier = {'name': t_name,\n                            'branch': t_branch,\n                            'version': t_version}\n            options = man.options(section)[1]\n            if 'docker' in options:\n                d_settings = json.loads(man.option(section,\n                                                   'docker')[1])\n                if 'links' in d_settings:\n                    for link in json.loads(d_settings['links']):\n                        if link in tools:\n                            dependencies.append(t_identifier)\n    return dependencies", "response": "Takes in a list of tools that are being updated and returns any tools that depend on linking to them"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn new Enumerable sorted in ascending order by given key", "response": "def order_by(self, key):\n        \"\"\"\n        Returns new Enumerable sorted in ascending order by given key\n        :param key: key to sort by as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        if key is None:\n            raise NullArgumentError(u\"No key for sorting given\")\n        kf = [OrderingDirection(key, reverse=False)]\n        return SortedEnumerable3(kf, self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn new Enumerable sorted in descending order by given key", "response": "def order_by_descending(self, key):\n        \"\"\"\n        Returns new Enumerable sorted in descending order by given key\n        :param key: key to sort by as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        if key is None:\n            raise NullArgumentError(u\"No key for sorting given\")\n        kf = [OrderingDirection(key, reverse=True)]\n        return SortedEnumerable3(kf, self._data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns new Enumerable where elements matching predicate are selected.", "response": "def where(self, predicate):\n        \"\"\"\n        Returns new Enumerable where elements matching predicate are selected\n        :param predicate: predicate as a lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        if predicate is None:\n            raise NullArgumentError(u\"No predicate given for where clause\")\n        return Enumerable3(filter(predicate, self))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nflattening an iterable of iterables returning a new Enumerable object containing only the elements that satisfy the given selector function.", "response": "def select_many(self, func=lambda x: x):\n        \"\"\"\n        Flattens an iterable of iterables returning a new Enumerable\n        :param func: selector as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        return Enumerable3(itertools.chain.from_iterable(self.select(func)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef join(\n            self,\n            inner_enumerable,\n            outer_key=lambda x: x,\n            inner_key=lambda x: x,\n            result_func=lambda x: x\n    ):\n        \"\"\"\n        Return enumerable of inner equi-join between two enumerables\n        :param inner_enumerable: inner enumerable to join to self\n        :param outer_key: key selector of outer enumerable as lambda expression\n        :param inner_key: key selector of inner enumerable as lambda expression\n        :param result_func: lambda expression to transform result of join\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(inner_enumerable, Enumerable3):\n            raise TypeError(\n                u\"inner_enumerable parameter must be an instance of Enumerable\"\n            )\n        return Enumerable3(\n            itertools.product(\n                filter(\n                    lambda x: outer_key(x) in map(inner_key, inner_enumerable),\n                    self\n                ),\n                filter(\n                    lambda y: inner_key(y) in map(outer_key, self),\n                    inner_enumerable\n                )\n            )\n        )\\\n            .where(lambda x: outer_key(x[0]) == inner_key(x[1]))\\\n            .select(result_func)", "response": "Return enumerable of inner equi - join between two enumerables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersect(self, enumerable, key=lambda x: x):\n        if not isinstance(enumerable, Enumerable3):\n            raise TypeError(\n                u\"enumerable parameter must be an instance of Enumerable\")\n        return self.join(enumerable, key, key).select(lambda x: x[0])", "response": "Returns enumerable that is the intersection between self and enumerable"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef union(self, enumerable, key=lambda x: x):\n        if not isinstance(enumerable, Enumerable3):\n            raise TypeError(\n                u\"enumerable parameter must be an instance of Enumerable\")\n        if self.count() == 0:\n            return enumerable\n        if enumerable.count() == 0:\n            return self\n        return self.concat(enumerable).distinct(key)", "response": "Returns an enumerable that is a union of elements between self and given\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an enumerable that subtracts given enumerable elements from self.", "response": "def except_(self, enumerable, key=lambda x: x):\n        \"\"\"\n        Returns enumerable that subtracts given enumerable elements from self\n        :param enumerable: enumerable object\n        :param key: key selector as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n        if not isinstance(enumerable, Enumerable3):\n            raise TypeError(\n                u\"enumerable parameter must be an instance of Enumerable\")\n        membership = (\n            0 if key(element) in enumerable.intersect(self, key).select(key) else 1\n            for element in self\n        )\n        return Enumerable3(\n            itertools.compress(\n                self,\n                membership\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new Enumerable3 containing the elements of the given enumerable.", "response": "def zip(self, enumerable, func):\n        \"\"\"\n        Merges 2 Enumerables using the given function. If the 2 collections are of unequal length, then\n        merging continues until the end of one of the collections is reached\n        :param enumerable: Enumerable collection to merge with\n        :param func: a function to perform the merging\n        :return: Enumerable\n        \"\"\"\n        if not isinstance(enumerable, Enumerable3):\n            raise TypeError()\n        return Enumerable3(itertools.zip_longest(self, enumerable)) \\\n            .where(lambda x: x[0] is not None and x[1] is not None) \\\n            .select(lambda x: func(x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the tools available for the branch", "response": "def repo_tools(self, branch):\n        \"\"\" Set the appropriate repo dir and get the tools available of it \"\"\"\n        tools = []\n        m_helper = Tools()\n        repo = self.parentApp.repo_value['repo']\n        version = self.parentApp.repo_value['versions'][branch]\n        status = m_helper.repo_tools(repo, branch, version)\n        if status[0]:\n            r_tools = status[1]\n            for tool in r_tools:\n                tools.append(tool[0])\n        return tools"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self):\n        self.add_handlers({'^Q': self.quit})\n        self.add(npyscreen.TitleText,\n                 name='Select which tools to add from each branch selected:',\n                 editable=False)\n        self.add(npyscreen.Textfield,\n                 value='NOTE tools you have already installed will be ignored',\n                 color='STANDOUT',\n                 editable=False)\n\n        i = 6\n        for branch in self.parentApp.repo_value['versions']:\n            self.tools_tc[branch] = {}\n            self.add(npyscreen.TitleText,\n                     name='Branch: ' + branch,\n                     editable=False,\n                     rely=i,\n                     relx=5,\n                     max_width=25)\n            tools = self.repo_tools(branch)\n            i += 1\n            for tool in tools:\n                value = True\n                if tool.startswith('/dev'):\n                    value = False\n                # tool in base directory\n                if tool == '' or tool.startswith(':'):\n                    tool = '/' + tool\n                self.tools_tc[branch][tool] = self.add(npyscreen.CheckBox,\n                                                       name=tool,\n                                                       value=value,\n                                                       relx=10)\n                i += 1\n            i += 2", "response": "Create the current tools for each branch at the version chosen"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_ok(self):\n        def diff(first, second):\n            \"\"\"\n            Get the elements that exist in the first list and not in the second\n            \"\"\"\n            second = set(second)\n            return [item for item in first if item not in second]\n\n        def popup(original_tools, branch, thr, title):\n            \"\"\"\n            Start the thread and display a popup of the tools being added until\n            the thread is finished\n            \"\"\"\n            thr.start()\n            tool_str = 'Adding tools...'\n            npyscreen.notify_wait(tool_str, title=title)\n            while thr.is_alive():\n                tools = diff(ManifestTools(), original_tools)\n                if tools:\n                    tool_str = ''\n                for tool in tools:\n                    pre_tool = 'Added: ' + branch + '/' + tool + '\\n'\n                    tool_str = pre_tool + tool_str\n                npyscreen.notify_wait(tool_str, title=title)\n                time.sleep(1)\n            return\n\n        original_tools = ManifestTools()\n        for branch in self.tools_tc:\n            tools = []\n            for tool in self.tools_tc[branch]:\n                if self.tools_tc[branch][tool].value:\n                    # get rid of temporary show for multiple tools in same\n                    # directory\n                    if tool == '/':\n                        tools.append(('.', ''))\n                    else:\n                        tools.append((tool, ''))\n            repo = self.parentApp.repo_value['repo']\n            version = self.parentApp.repo_value['versions'][branch]\n            api_action = Tools(version=version, branch=branch)\n            thr = threading.Thread(target=api_action.new, args=(),\n                                   kwargs={'tool_type': 'repo',\n                                           'uri': repo,\n                                           'tools': tools})\n            popup(original_tools, branch, thr,\n                  'Please wait, adding tools for the ' + branch + ' branch...')\n        npyscreen.notify_confirm('Done adding repository: ' +\n                                 self.parentApp.repo_value['repo'],\n                                 title='Added Repository')\n        self.quit()", "response": "Add tools to the plugins and return the new version of the tools"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef while_waiting(self):\n        # give a little extra time for file descriptors to close\n        time.sleep(0.1)\n\n        self.addfield.value = Timestamp()\n        self.addfield.display()\n        self.addfield2.value = Uptime()\n        self.addfield2.display()\n        self.addfield3.value = str(len(Containers()))+' running'\n        if len(Containers()) > 0:\n            self.addfield3.labelColor = 'GOOD'\n        else:\n            self.addfield3.labelColor = 'DEFAULT'\n        self.addfield3.display()\n\n        # if file drop location changes deal with it\n        logger = Logger(__name__)\n        if self.file_drop.value != DropLocation()[1]:\n            logger.info('Starting: file drop restart')\n            try:\n                self.file_drop.value = DropLocation()[1]\n                logger.info('Path given: ' + str(self.file_drop.value))\n            except Exception as e:  # pragma no cover\n                logger.error('file drop restart failed with error: ' + str(e))\n            logger.info('Finished: file drop restart')\n        self.file_drop.display()\n        return", "response": "Update the fields periodically if nothing is happening"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a form and switch to it", "response": "def add_form(self, form, form_name, form_args):\n        \"\"\" Add new form and switch to it \"\"\"\n        self.parentApp.addForm(form_name, form, **form_args)\n        self.parentApp.change_form(form_name)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_forms(self, form_names):\n        for form in form_names:\n            try:\n                self.parentApp.removeForm(form)\n            except Exception as e:  # pragma: no cover\n                pass\n        return", "response": "Remove all forms supplied"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef perform_action(self, action):\n        form = ToolForm\n        s_action = form_action = action.split('_')[0]\n        form_name = s_action.title() + ' tools'\n        cores = False\n        a_type = 'containers'\n        forms = [action.upper() + 'TOOLS']\n        form_args = {'color': 'CONTROL',\n                     'names': [s_action],\n                     'name': form_name,\n                     'action_dict': {'action_name': s_action,\n                                     'present_t': s_action + 'ing ' + a_type,\n                                     'past_t': s_action.title() + ' ' + a_type,\n                                     'action': form_action,\n                                     'type': a_type,\n                                     'cores': cores}}\n        # grammar rules\n        vowels = ['a', 'e', 'i', 'o', 'u']\n\n        # consonant-vowel-consonant ending\n        # Eg: stop -> stopping\n        if s_action[-1] not in vowels and \\\n           s_action[-2] in vowels and \\\n           s_action[-3] not in vowels:\n            form_args['action_dict']['present_t'] = s_action + \\\n                s_action[-1] + 'ing ' + a_type\n\n        # word ends with a 'e'\n        # eg: remove -> removing\n        if s_action[-1] == 'e':\n            form_args['action_dict']['present_t'] = s_action[:-1] \\\n                + 'ing ' + a_type\n\n        if s_action == 'configure':\n            form_args['names'].pop()\n            form_args['names'].append('get_configure')\n            form_args['names'].append('save_configure')\n            form_args['names'].append('restart_tools')\n        if action == 'add':\n            form = AddForm\n            forms = ['ADD', 'ADDOPTIONS', 'CHOOSETOOLS']\n            form_args['name'] = 'Add plugins'\n            form_args['name'] += '\\t'*6 + '^Q to quit'\n        elif action == 'inventory':\n            form = InventoryToolsForm\n            forms = ['INVENTORY']\n            form_args = {'color': 'STANDOUT', 'name': 'Inventory of tools'}\n        elif action == 'services':\n            form = ServicesForm\n            forms = ['SERVICES']\n            form_args = {'color': 'STANDOUT',\n                         'name': 'Plugin Services',\n                         'core': True}\n        elif action == 'services_external':\n            form = ServicesForm\n            forms = ['SERVICES']\n            form_args = {'color': 'STANDOUT',\n                         'name': 'External Services',\n                         'core': True,\n                         'external': True}\n        form_args['name'] += '\\t'*8 + '^T to toggle main'\n        if s_action in self.view_togglable:\n            form_args['name'] += '\\t'*8 + '^V to toggle group view'\n        try:\n            self.remove_forms(forms)\n            thr = Thread(target=self.add_form, args=(),\n                         kwargs={'form': form,\n                                 'form_name': forms[0],\n                                 'form_args': form_args})\n            thr.start()\n            while thr.is_alive():\n                npyscreen.notify('Please wait, loading form...',\n                                 title='Loading')\n                time.sleep(1)\n        except Exception as e:  # pragma: no cover\n            pass\n        return", "response": "Perform the action in the API from the CLI"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nswitches the tutorials of the current user.", "response": "def switch_tutorial(self, action):\n        \"\"\" Tutorial forms \"\"\"\n        if action == 'background':\n            self.parentApp.change_form('TUTORIALBACKGROUND')\n        elif action == 'terminology':\n            self.parentApp.change_form('TUTORIALTERMINOLOGY')\n        elif action == 'setup':\n            self.parentApp.change_form('TUTORIALGETTINGSETUP')\n        elif action == 'starting_tools':\n            self.parentApp.change_form('TUTORIALSTARTINGCORES')\n        elif action == 'adding_tools':\n            self.parentApp.change_form('TUTORIALADDINGPLUGINS')\n        elif action == 'adding_files':\n            self.parentApp.change_form('TUTORIALADDINGFILES')\n        elif action == 'basic_troubleshooting':\n            self.parentApp.change_form('TUTORIALTROUBLESHOOTING')\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding method for creating FormBaseNewWithMenu form", "response": "def create(self):\n        \"\"\" Override method for creating FormBaseNewWithMenu form \"\"\"\n        try:\n            self.api_action = System()\n        except DockerException as de:  # pragma: no cover\n            notify_confirm(str(de),\n                           title='Docker Error',\n                           form_color='DANGER',\n                           wrap=True)\n            MainForm.exit()\n\n        self.add_handlers({'^T': self.help_form, '^Q': MainForm.exit})\n        # all forms that can toggle view by group\n        self.view_togglable = ['inventory', 'remove']\n\n        #######################\n        # MAIN SCREEN WIDGETS #\n        #######################\n\n        self.addfield = self.add(npyscreen.TitleFixedText, name='Date:',\n                                 labelColor='DEFAULT', value=Timestamp())\n        self.addfield2 = self.add(npyscreen.TitleFixedText, name='Uptime:',\n                                  labelColor='DEFAULT', value=Uptime())\n        self.cpufield = self.add(npyscreen.TitleFixedText,\n                                 name='Logical CPUs:',\n                                 labelColor='DEFAULT', value=Cpu())\n        self.gpufield = self.add(npyscreen.TitleFixedText, name='GPUs:',\n                                 labelColor='DEFAULT', value=Gpu()[1])\n        self.location = self.add(npyscreen.TitleFixedText,\n                                 name='User Data:',\n                                 value=PathDirs().meta_dir,\n                                 labelColor='DEFAULT')\n        self.file_drop = self.add(npyscreen.TitleFixedText,\n                                  name='File Drop:',\n                                  value=DropLocation()[1],\n                                  labelColor='DEFAULT')\n        self.addfield3 = self.add(npyscreen.TitleFixedText, name='Containers:',\n                                  labelColor='DEFAULT',\n                                  value='0 '+' running')\n        self.multifield1 = self.add(npyscreen.MultiLineEdit, max_height=22,\n                                    editable=False, value=\"\"\"\n\n            '.,\n              'b      *\n               '$    #.\n                $:   #:\n                *#  @):\n                :@,@):   ,.**:'\n      ,         :@@*: ..**'\n       '#o.    .:(@'.@*\"'\n          'bq,..:,@@*'   ,*\n          ,p$q8,:@)'  .p*'\n         '    '@@Pp@@*'\n               Y7'.'\n              :@):.\n             .:@:'.\n           .::(@:.\n                       _\n      __   _____ _ __ | |_\n      \\ \\ / / _ \\ '_ \\| __|\n       \\ V /  __/ | | | |_\n        \\_/ \\___|_| |_|\\__|\n                           \"\"\")\n\n        ################\n        # MENU OPTIONS #\n        ################\n\n        # Tool Menu Items\n        self.m3 = self.add_menu(name='Tools', shortcut='p')\n        self.m3.addItem(text='Add New Tool',\n                        onSelect=self.perform_action,\n                        arguments=['add'], shortcut='a')\n        self.m3.addItem(text='Configure Tools',\n                        onSelect=self.perform_action,\n                        arguments=['configure'], shortcut='t')\n        self.m3.addItem(text='Inventory',\n                        onSelect=self.perform_action,\n                        arguments=['inventory'], shortcut='i')\n        self.m3.addItem(text='Remove Tools',\n                        onSelect=self.perform_action,\n                        arguments=['remove'], shortcut='r')\n        self.m3.addItem(text='Start Tools',\n                        onSelect=self.perform_action,\n                        arguments=['start'], shortcut='s')\n        self.m3.addItem(text='Stop Tools',\n                        onSelect=self.perform_action,\n                        arguments=['stop'], shortcut='p')\n\n        # Services Menu Items\n        self.m5 = self.add_menu(name='Services Running', shortcut='s')\n        self.m5.addItem(text='External Services', onSelect=self.perform_action,\n                        arguments=['services_external'], shortcut='e')\n        self.m5.addItem(text='Tool Services',\n                        onSelect=self.perform_action,\n                        arguments=['services'], shortcut='t')\n\n        # System Commands Menu Items\n        self.m6 = self.add_menu(name='System Commands', shortcut='y')\n        self.m6.addItem(text='Backup', onSelect=self.system_commands,\n                        arguments=['backup'], shortcut='b')\n        self.m6.addItem(text='Change Vent Configuration',\n                        onSelect=self.system_commands, arguments=['configure'],\n                        shortcut='c')\n        self.m6.addItem(text='Detect GPUs', onSelect=self.system_commands,\n                        arguments=['gpu'], shortcut='g')\n        self.m6.addItem(text='Factory Reset', onSelect=self.system_commands,\n                        arguments=['reset'], shortcut='r')\n        self.m6.addItem(text='Restore (To Be Implemented...', onSelect=self.system_commands,\n                        arguments=['restore'], shortcut='t')\n\n        # TODO this should be either or depending on whether or not it's running already\n        self.m6.addItem(text='Start', onSelect=self.system_commands,\n                        arguments=['start'], shortcut='s')\n        self.m6.addItem(text='Stop', onSelect=self.system_commands,\n                        arguments=['stop'], shortcut='o')\n\n        self.m6.addItem(text='Upgrade (To Be Implemented...)',\n                        onSelect=self.system_commands,\n                        arguments=['upgrade'], shortcut='u')\n\n        # Tutorial Menu Items\n        self.m7 = self.add_menu(name='Tutorials', shortcut='t')\n        self.s1 = self.m7.addNewSubmenu(name='About Vent', shortcut='v')\n        self.s1.addItem(text='Background', onSelect=self.switch_tutorial,\n                        arguments=['background'], shortcut='b')\n        self.s1.addItem(text='Terminology', onSelect=self.switch_tutorial,\n                        arguments=['terminology'], shortcut='t')\n        self.s1.addItem(text='Getting Setup', onSelect=self.switch_tutorial,\n                        arguments=['setup'], shortcut='s')\n        self.s2 = self.m7.addNewSubmenu(name='Working with Tools',\n                                        shortcut='c')\n        self.s2.addItem(text='Starting Tools', onSelect=self.switch_tutorial,\n                        arguments=['starting_tools'], shortcut='s')\n        self.s3 = self.m7.addNewSubmenu(name='Working with Plugins',\n                                        shortcut='p')\n        self.s3.addItem(text='Adding Tools', onSelect=self.switch_tutorial,\n                        arguments=['adding_tools'], shortcut='a')\n        self.s4 = self.m7.addNewSubmenu(name='Files', shortcut='f')\n        self.s4.addItem(text='Adding Files', onSelect=self.switch_tutorial,\n                        arguments=['adding_files'], shortcut='a')\n        self.s5 = self.m7.addNewSubmenu(name='Help', shortcut='s')\n        self.s5.addItem(text='Basic Troubleshooting',\n                        onSelect=self.switch_tutorial,\n                        arguments=['basic_troubleshooting'], shortcut='t')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self):\n        self.add_handlers({'^T': self.quit, '^Q': self.quit})\n        self.services_tft = self.add(npyscreen.TitleFixedText,\n                                     name='No services running.',\n                                     value='')\n        services = Services(self.core, external=self.external)\n        if services:\n            self.services_tft.hidden = True\n            for service in services:\n                value = ''\n                for val in service[1]:\n                    value += val+', '\n                self.add(npyscreen.TitleFixedText,\n                         name=service[0],\n                         value=value[:-2])", "response": "Override method for creating FormBaseNew form"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Logger(name, **kargs):\n    path_dirs = PathDirs(**kargs)\n    logging.captureWarnings(True)\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    handler = logging.handlers.WatchedFileHandler(os.path.join(\n        path_dirs.meta_dir, 'vent.log'))\n    handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s:%(lineno)-4d - '\n                                  '%(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    if not len(logger.handlers):\n        logger.addHandler(handler)\n\n    return logger", "response": "Create and return a logger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating multi - line edit for editing", "response": "def create(self):\n        \"\"\" Create multi-line widget for editing \"\"\"\n        # add various pointers to those editing vent_cfg\n        if self.vent_cfg:\n            self.add(npyscreen.Textfield,\n                     value='# when configuring external'\n                           ' services make sure to do so',\n                     editable=False)\n            self.add(npyscreen.Textfield,\n                     value='# in the form of Service = {\"setting\": \"value\"}',\n                     editable=False)\n            self.add(npyscreen.Textfield,\n                     value='# make sure to capitalize your service correctly'\n                           ' (i.e. Elasticsearch vs. elasticsearch)',\n                     editable=False)\n            self.add(npyscreen.Textfield,\n                     value='# and make sure to enclose all dict keys and'\n                           ' values in double quotes (\"\")',\n                     editable=False)\n            self.add(npyscreen.Textfield,\n                     value='',\n                     editable=False)\n        elif self.instance_cfg:\n            self.add(npyscreen.Textfield,\n                     value='# these settings will be used'\n                           ' to configure the new instances',\n                     editable=False)\n        self.edit_space = self.add(npyscreen.MultiLineEdit,\n                                   value=self.config_val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges to the next tool to edit or back to MAIN form", "response": "def change_screens(self):\n        \"\"\" Change to the next tool to edit or back to MAIN form \"\"\"\n        if self.settings['next_tool']:\n            self.parentApp.change_form(self.settings['next_tool'])\n        else:\n            self.parentApp.change_form('MAIN')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid_input(val):\n        # looks for 3 nums followed by a dot 3 times and then ending with\n        # 3 nums, can be proceeded by any number of spaces\n        ip_value = re.compile(r'(\\d{1,3}\\.){3}\\d{1,3}$')\n        # looks for only numbers and commas (because priorities can have commas\n        # between them), can be proceeded by any number of spaces\n        all_num = re.compile(r'(\\d,?\\ *)+$')\n        sections_comments = re.compile(r\"\"\"\n        \\ *\\#.*             # comments (any number of whitespace, then #\n                            # followed by anything)\n\n        | \\[[\\w-]+\\]$       # section headers (any combination of chars, nums,\n                            # underscores, and dashes between brackets)\n        \"\"\", re.VERBOSE)\n        # can't can be a comment on option side and value side can't have\n        # [, ], {, or } otherwise it is turned over to literal_eval for\n        # checkout\n        options_values = re.compile(r'[^# ]+\\ *=[^[\\]{}]*$')\n        line_num = 0\n        warning_str = ''\n        error_str = ''\n        trimmed_val = []\n        for entry in val.split('\\n'):\n            line_num += 1\n            # get rid of any extraneous commas at the end of a dict and remove\n            # extra whitespace from input\n            trimmed_val.append(re.sub(r',\\ *}', '}', entry).strip())\n            # empty line\n            if entry.strip() == '':\n                continue\n            # look at regular (non dictionary or list) option-value pairs\n            if options_values.match(entry):\n                value = entry.split('=', 1)[1]\n                # deal with potentially more equals signs\n                for val in value.split('='):\n                    val = val.strip()\n                    # empty val means malformed equals signs\n                    if val == '':\n                        error_str += '-You have a misplaced equals sign on' \\\n                            ' line ' + str(line_num) + '\\n'\n                    # starts with a num; look for bad ip input or warn user\n                    # about having extraneous characters in number input\n                    if re.match('\\ *\\d', val):\n                        # bad ip syntax\n                        if val.find('.') >= 0 and not ip_value.match(val):\n                            error_str += '-You have an incorrectly' \\\n                                ' formatted ip address (bad syntax) at' \\\n                                ' line ' + str(line_num) + '\\n'\n                        # possibly malformed numbers\n                        elif val.find('.') < 0 and not all_num.match(val):\n                            warning_str += '-Line starting with a number has' \\\n                                ' characters mixed in at line ' + \\\n                                str(line_num) + '\\n'\n                        # bad ip values\n                        elif val.find('.') >= 0:\n                            for num in val.strip().split('.'):\n                                num = int(num)\n                                if num > 255 or num < 0:\n                                    error_str += '-You have an incorrectly' \\\n                                        ' formatted ip address (values' \\\n                                        ' exceeding 255 or below 0) at' \\\n                                        ' line ' + str(line_num) + '\\n'\n                    # ensure no lines end with a comma (most likely extraneous\n                    # commas from groups or priorities)\n                    if re.search(',$', val):\n                        error_str += '-You have an incorrect comma at the' \\\n                            ' end of line ' + str(line_num) + '\\n'\n            # see if input is a header or comment, otherwise try to\n            # literal_eval it to ensure correct structure\n            elif not sections_comments.match(entry):\n                lit_val = ''\n                try:\n                    opt_val = entry.split('=', 1)\n                    if opt_val[0].strip() == '':\n                        error_str += '-You have nothing preceeding an' \\\n                            ' equals sign at line ' + str(line_num) + '\\n'\n                    else:\n                        lit_val = opt_val[1].strip()\n                except IndexError:\n                    lit_val = ''\n                    error_str += '-You have an incorrectly formatted' \\\n                        ' section header at line ' + str(line_num) + '\\n'\n                if lit_val:\n                    try:\n                        ast.literal_eval(lit_val)\n                    except SyntaxError:\n                        error_str += '-You have an incorrectly formatted' \\\n                            ' list/dictionary at line ' + str(line_num) + \\\n                            '\\n'\n\n        if error_str:\n            npyscreen.notify_confirm('You have the following error(s) and'\n                                     \" can't proceed until they are fixed:\" +\n                                     '\\n' + '-'*50 + '\\n' + error_str,\n                                     title='Error in input')\n            return (False, '')\n        elif warning_str:\n            res = npyscreen.notify_yes_no('You have may have some error(s)'\n                                          ' that you want to check before'\n                                          ' proceeding:' + '\\n' + '-'*50 +\n                                          '\\n' + warning_str + '\\n' + '-'*50 +\n                                          '\\n' + 'Do you want to continue?',\n                                          title='Double check')\n            return (res, '\\n'.join(trimmed_val))\n        return (True, '\\n'.join(trimmed_val))", "response": "Ensures that the input is of a valid format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_ok(self):\n        # ensure user didn't have any syntactical errors\n        input_is_good, trimmed_input = self.valid_input(self.edit_space.value)\n        if not input_is_good:\n            return\n        self.edit_space.value = trimmed_input\n\n        # get the number of instances and ensure user didn't malform that\n        if re.search(r'instances\\ *=', self.edit_space.value):\n            try:\n                # split out spaces\n                instances_val = re.split(r'instances\\ *=\\ *',\n                                         self.edit_space.value)[1]\n                instances_val = instances_val.split('\\n')[0]\n                new_instances = int(re.match(r'\\d+$', instances_val).group())\n            except AttributeError:\n                npyscreen.notify_confirm(\"You didn't specify a valid number\"\n                                         ' for instances.', title='Invalid'\n                                         ' instance number')\n                return\n            # user can't change instances when configuring new instnaces\n            if (self.instance_cfg and\n                    self.settings['new_instances'] != new_instances):\n                npyscreen.notify_confirm(\"You can't change the number of\"\n                                         ' instnaces while configuring new'\n                                         ' instances!', title='Illegal change')\n                return\n            # get old number of instances\n            try:\n                if 'old_instances' in self.settings:\n                    old_instances = self.settings['old_instances']\n                else:\n                    settings_dict = json.loads(\n                        self.manifest.option(self.section,\n                                             'settings')[1])\n                    old_instances = int(settings_dict['instances'])\n            except Exception:\n                old_instances = 1\n        else:\n            new_instances = 1\n            old_instances = 1\n\n        # save changes and update manifest we're looking at with changes\n        if self.vent_cfg:\n            save_args = {'main_cfg': True,\n                         'config_val': self.edit_space.value}\n            self.manifest = self.settings['save_configure'](**save_args)[1]\n        else:\n            save_args = copy.deepcopy(self.tool_identifier)\n            save_args.update({'config_val': self.edit_space.value})\n            if self.registry_tool:\n                save_args.update({'from_registry': True})\n            if self.instance_cfg:\n                save_args.update({'instances': new_instances})\n            self.manifest = self.settings['save_configure'](**save_args)[1]\n\n        # restart tools, if necessary\n        if not self.just_downloaded and not self.instance_cfg:\n            restart_kargs = {'main_cfg': self.vent_cfg,\n                             'old_val': self.config_val,\n                             'new_val': self.edit_space.value}\n            if self.vent_cfg:\n                wait_str = 'Restarting tools affected by changes...'\n            else:\n                wait_str = 'Restarting this tool with new settings...'\n                restart_kargs.update(self.tool_identifier)\n            npyscreen.notify_wait(wait_str,\n                                  title='Restarting with changes')\n            self.settings['restart_tools'](**restart_kargs)\n\n        # start new instances if user wanted to\n        if self.instance_cfg and self.settings['start_new']:\n            npyscreen.notify_wait('Starting new instances...',\n                                  title='Start')\n            tool_d = {}\n            for i in range(self.settings['old_instances'] + 1,\n                           self.settings['new_instances'] + 1):\n                # create section by scrubbing instance number out of names\n                # and adding new instance number\n                i_section = self.section.rsplit(':', 2)\n                i_section[0] = re.sub(r'[0-9]', '', i_section[0]) + str(i)\n                i_section = ':'.join(i_section)\n                t_name = self.manifest.option(i_section, 'name')[1]\n                t_id = {'name': t_name}\n                tool_d.update(self.settings['prep_start'](**t_id)[1])\n            if tool_d:\n                self.settings['start_tools'](tool_d)\n\n        # prompt user for instance changes, as necessary\n        if not self.instance_cfg and not self.vent_cfg:\n            if new_instances > old_instances:\n                try:\n                    diff = str(new_instances - old_instances)\n                    res = npyscreen.notify_yes_no('You will be creating ' +\n                                                  diff + ' additional'\n                                                  ' instance(s) is that okay?',\n                                                  title='Confirm new'\n                                                  ' instance(s)')\n                    if res:\n                        if self.manifest.option(self.section,\n                                                'built')[1] == 'yes':\n                            run = npyscreen.notify_yes_no('Do you want to'\n                                                          ' start these new'\n                                                          ' tools upon'\n                                                          ' creation?',\n                                                          title='Run new'\n                                                          ' instance(s)')\n                        else:\n                            run = False\n                        # get clean name (no instance numbers in it)\n                        new_name = self.settings['tool_name']\n                        new_name = re.sub(r'[0-9]+$', '', new_name)\n                        self.settings['tool_name'] = new_name\n                        npyscreen.notify_wait('Pulling up default settings'\n                                              ' for ' +\n                                              self.settings['tool_name'] +\n                                              '...',\n                                              title='Gathering settings')\n                        Repository(System().manifest)._clone(\n                            self.settings['repo'])\n                        self.settings['new_instances'] = new_instances\n                        self.settings['old_instances'] = old_instances\n                        self.settings['start_new'] = run\n                        self.settings['new_instance'] = True\n                        self.settings['name'] = 'Configure new instance(s)' + \\\n                            ' for ' + self.settings['tool_name']\n                        self.parentApp.addForm('INSTANCEEDITOR' +\n                                               self.settings['tool_name'],\n                                               EditorForm, **self.settings)\n                        self.parentApp.change_form('INSTANCEEDITOR' +\n                                                   self.settings['tool_name'])\n                    else:\n                        return\n                except Exception:\n                    npyscreen.notify_confirm('Trouble finding tools to add,'\n                                             ' exiting', title='Error')\n                    self.on_cancel()\n            elif new_instances < old_instances:\n                try:\n                    diff = str(old_instances - new_instances)\n                    res = npyscreen.notify_yes_no('You will be deleting ' +\n                                                  diff + ' instance(s), is'\n                                                  ' that okay?',\n                                                  title='Confirm delete'\n                                                  ' instance(s)')\n                    if res:\n                        form_name = 'Delete instances for ' + \\\n                            re.sub(r'\\d+$', '',\n                                   self.settings['tool_name']) + '\\t'*8 + \\\n                            '^E to exit configuration process'\n                        clean_section = self.section.rsplit(':', 2)\n                        clean_section[0] = re.sub(r'\\d+$', '',\n                                                  clean_section[0])\n                        clean_section = ':'.join(clean_section)\n                        d_args = {'name': form_name,\n                                  'new_instances': new_instances,\n                                  'old_instances': old_instances,\n                                  'next_tool': self.settings['next_tool'],\n                                  'manifest': self.manifest,\n                                  'section': clean_section,\n                                  'clean': self.settings['clean'],\n                                  'prep_start': self.settings['prep_start'],\n                                  'start_tools': self.settings['start_tools']}\n                        self.parentApp.addForm('DELETER' +\n                                               self.settings['tool_name'],\n                                               DeleteForm, **d_args)\n                        self.parentApp.change_form('DELETER' +\n                                                   self.settings['tool_name'])\n                except Exception:\n                    npyscreen.notify_confirm('Trouble finding instances to'\n                                             ' delete, exiting', title='Error')\n                    self.on_cancel()\n\n        if (new_instances == old_instances or\n                self.instance_cfg or self.vent_cfg):\n            npyscreen.notify_confirm('Done configuring ' +\n                                     self.settings['tool_name'],\n                                     title='Configurations saved')\n            self.change_screens()", "response": "Save changes made to vent. template"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlocating the ArcGIS installation on this machine.", "response": "def locate_arcgis():\n  '''\n  Find the path to the ArcGIS Desktop installation.\n\n  Keys to check:\n\n  HLKM/SOFTWARE/ESRI/ArcGIS 'RealVersion' - will give the version, then we can use\n  that to go to\n  HKLM/SOFTWARE/ESRI/DesktopXX.X 'InstallDir'. Where XX.X is the version\n\n  We may need to check HKLM/SOFTWARE/Wow6432Node/ESRI instead\n  '''\n  try:\n    key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,\n                          'SOFTWARE\\\\Wow6432Node\\\\ESRI\\\\ArcGIS', 0)\n\n    version = _winreg.QueryValueEx(key, \"RealVersion\")[0][:4]\n\n    key_string = \"SOFTWARE\\\\Wow6432Node\\\\ESRI\\\\Desktop{0}\".format(version)\n    desktop_key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,\n                                  key_string, 0)\n\n    install_dir = _winreg.QueryValueEx(desktop_key, \"InstallDir\")[0]\n    return install_dir\n  except WindowsError:\n    raise ImportError(\"Could not locate the ArcGIS directory on this machine\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_arcpy():  \n  '''\n  Allows arcpy to imported on 'unmanaged' python installations (i.e. python installations\n  arcgis is not aware of).\n  Gets the location of arcpy and related libs and adds it to sys.path\n  '''\n  install_dir = locate_arcgis()  \n  arcpy = path.join(install_dir, \"arcpy\")\n  # Check we have the arcpy directory.\n  if not path.exists(arcpy):\n    raise ImportError(\"Could not find arcpy directory in {0}\".format(install_dir))\n\n  # First check if we have a bin64 directory - this exists when arcgis is 64bit\n  bin_dir = path.join(install_dir, \"bin64\")\n  \n  # check if we are using a 64-bit version of Python\n  is_64bits = sys.maxsize > 2**32\n  \n  if not path.exists(bin_dir) or is_64bits == False:\n    # Fall back to regular 'bin' dir otherwise.\n    bin_dir = path.join(install_dir, \"bin\")\n\n  scripts = path.join(install_dir, \"ArcToolbox\", \"Scripts\")  \n  sys.path.extend([arcpy, bin_dir, scripts])", "response": "Gets the location of arcpy and related libs and adds it to sys. path\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the options to a option - parser instance or a .", "response": "def __add_options(parser):\n    \"\"\"\n    Add the `Makespec` options to a option-parser instance or a\n    option group.\n    \"\"\"\n    g = parser.add_option_group('What to generate')\n    g.add_option(\"-F\", \"--onefile\", dest=\"onefile\",\n                 action=\"store_true\", default=False,\n                 help=\"create a single file deployment\")\n    g.add_option(\"-D\", \"--onedir\", dest=\"onefile\",\n                 action=\"store_false\",\n                 help=\"create a single directory deployment (default)\")\n    g.add_option(\"-o\", \"--out\",\n                 dest=\"workdir\", metavar=\"DIR\",\n                 help=\"generate the spec file in the specified directory \"\n                      \"(default: current directory)\")\n    g.add_option(\"-n\", \"--name\",\n                 help=\"name to assign to the project \"\n                      \"(default: first script's basename)\")\n\n    g = parser.add_option_group('What to bundle, where to search')\n    g.add_option(\"-p\", \"--paths\", default=[], dest=\"pathex\",\n                 metavar=\"DIR\", action=\"append\",\n                 help=\"set base path for import (like using PYTHONPATH). \"\n                      \"Multiple directories are allowed, separating them \"\n                      \"with %s, or using this option multiple times\"\n                      % repr(os.pathsep))\n    g.add_option(\"--additional-hooks-dir\", action=\"append\", dest=\"hookspath\",\n                 help=\"additional path to search for hooks \"\n                      \"(may be given several times)\")    \n    g.add_option(\"-K\", \"--tk\", action=\"store_true\",\n                 help=\"include TCL/TK in the deployment\")\n    g.add_option(\"-a\", \"--ascii\", action=\"store_true\",\n                 help=\"do NOT include unicode encodings \"\n                      \"(default: included if available)\")\n\n    g = parser.add_option_group('How to generate')\n    g.add_option(\"-d\", \"--debug\", action=\"store_true\", default=False,\n                 help=(\"use the debug (verbose) build of the executable for \"\n                       \"packaging. This will make the packaged executable be \"\n                       \"more verbose when run.\"))\n    g.add_option(\"-s\", \"--strip\", action=\"store_true\",\n                 help=\"strip the exe and shared libs \"\n                      \"(don't try this on Windows)\")\n    g.add_option(\"--noupx\", action=\"store_true\", default=False,\n                 help=\"do not use UPX even if available (works differently \"\n                      \"between Windows and *nix)\")\n    #p.add_option(\"-Y\", \"--crypt\", metavar=\"FILE\",\n    #             help=\"encrypt pyc/pyo files\")\n\n    g = parser.add_option_group('Windows and Mac OS X specific options')\n    g.add_option(\"-c\", \"--console\", \"--nowindowed\", dest=\"console\",\n                 action=\"store_true\", default=True,\n                 help=\"use a console subsystem executable (default)\")\n    g.add_option(\"-w\", \"--windowed\", \"--noconsole\", dest=\"console\",\n                 action=\"store_false\",\n                 help=\"use a windowed subsystem executable, which on Windows \"\n                      \"does not open the console when the program is launched.\"\n                      'Mandatory whe creating .app bundle on Mac OS X')\n    g.add_option(\"-i\", \"--icon\", dest=\"icon_file\",\n                 metavar=\"FILE.ICO or FILE.EXE,ID or FILE.ICNS\",\n                 help=\"If FILE is an .ico file, add the icon to the final \"\n                      \"executable. Otherwise, the syntax 'file.exe,id' to \"\n                      \"extract the icon with the specified id \"\n                      \"from file.exe and add it to the final executable. \"\n                      \"If FILE is an .icns file, add the icon to the final \"\n                      \".app bundle on Mac OS X (for Mac not yet implemented)\")\n\n    g = parser.add_option_group('Windows specific options')\n    g.add_option(\"--version-file\",\n                 dest=\"version_file\", metavar=\"FILE\",\n                 help=\"add a version resource from FILE to the exe\")\n    g.add_option(\"-m\", \"--manifest\", metavar=\"FILE or XML\",\n                 help=\"add manifest FILE or XML to the exe\")\n    g.add_option(\"-r\", \"--resource\", default=[], dest=\"resources\",\n                 metavar=\"FILE[,TYPE[,NAME[,LANGUAGE]]]\", action=\"append\",\n                 help=\"add/update resource of the given type, name and language \"\n                      \"from FILE to the final executable. FILE can be a \"\n                      \"data file or an exe/dll. For data files, atleast \"\n                      \"TYPE and NAME need to be specified, LANGUAGE defaults \"\n                      \"to 0 or may be specified as wildcard * to update all \"\n                      \"resources of the given TYPE and NAME. For exe/dll \"\n                      \"files, all resources from FILE will be added/updated \"\n                      \"to the final executable if TYPE, NAME and LANGUAGE \"\n                      \"are omitted or specified as wildcard *.\"\n                      \"Multiple resources are allowed, using this option \"\n                      \"multiple times.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes a request object", "response": "def _encode_request(self, request):\n        \"\"\"Encode a request object\"\"\"\n        return pickle.dumps(request_to_dict(request, self.spider), protocol=-1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npush a request to the broker.", "response": "def push(self, request):\n        \"\"\"Push a request\"\"\"\n        self.server.basic_publish(\n            exchange='',\n            routing_key=self.key,\n            body=self._encode_request(request)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npop a request from the queue.", "response": "def pop(self):\n        \"\"\"Pop a request\"\"\"\n\n        method_frame, header, body = self.server.basic_get(queue=self.key)\n\n        if body:\n            return self._decode_request(body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps running python script in a subprocess. Return stdout of the invoked command.", "response": "def exec_python(*args, **kwargs):\n    \"\"\"\n    Wrap running python script in a subprocess.\n\n    Return stdout of the invoked command.\n    \"\"\"\n    cmdargs, kwargs = __wrap_python(args, kwargs)\n    return exec_command(*cmdargs, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping running python script in a subprocess. Return exit code of the invoked command.", "response": "def exec_python_rc(*args, **kwargs):\n    \"\"\"\n    Wrap running python script in a subprocess.\n\n    Return exit code of the invoked command.\n    \"\"\"\n    cmdargs, kwargs = __wrap_python(args, kwargs)\n    return exec_command_rc(*cmdargs, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_dir():\n    try:\n        d = os.environ['HADOOPY_CHDIR']\n        sys.stderr.write('HADOOPY: Trying to chdir to [%s]\\n' % d)\n    except KeyError:\n        pass\n    else:\n        try:\n            os.chdir(d)\n        except OSError:\n            sys.stderr.write('HADOOPY: Failed to chdir to [%s]\\n' % d)", "response": "Change the local directory if the HADOOPY_CHDIR environmental variable is provided"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_stdout_buffering():\n    stdout_orig = sys.stdout\n    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n    # NOTE(brandyn): This removes the original stdout\n    return stdout_orig", "response": "This turns off stdout buffering so that outputs are immediately\n    materialized and log messages show up before the program exits."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(mapper=None, reducer=None, combiner=None, script_path=None, jobconfs=(), **kw):\n    if script_path is None:\n        script_path = inspect.getfile(inspect.currentframe().f_back)\n    script_path = os.path.abspath(script_path)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--log', help='Default log level to use', choices=('debug', 'info', 'warning', 'error', 'critical'), default='info')\n    subparsers = parser.add_subparsers(help='Job Commands (additional help available inside each)')\n\n    parser_freeze = subparsers.add_parser('freeze', help='Freeze the script to a tar file.')\n    parser_freeze.add_argument('tar_path', help='Path to .tar or .tar.gz file.')\n    parser_freeze.add_argument('-Z', help='Path to a file to be placed in the tar root (may be repeated for many files).', action='append')\n    parser_freeze.set_defaults(func=run_freeze)\n\n    parser_info = subparsers.add_parser('info', help='Display job info as JSON')\n    parser_info.set_defaults(func=lambda : run_info(mapper, reducer, combiner, jobconfs, kw))\n\n    parser_launch_frozen = subparsers.add_parser('launch_frozen', help='Run Hadoop job (freezes script)')\n    parser_launch_frozen.add_argument('in_name', help='Input HDFS path')\n    parser_launch_frozen.add_argument('out_name', help='Output HDFS path')\n    parser_launch_frozen.add_argument('-jobconf', help='Jobconf', action='append', dest='jobconfs', default=[])\n    parser_launch_frozen.set_defaults(func=lambda *args, **kw: hadoopy.launch_frozen(*args, script_path=script_path, **kw))\n\n    parser_pipe = subparsers.add_parser('pipe', help='Internal: Run map/combine/reduce task using \"pipe hopping\" to make stdout redirect to stderr.')\n    parser_pipe.add_argument('command', help='Command to run', choices=('map', 'reduce', 'combine'))\n    parser_pipe.set_defaults(func=run_pipe)\n\n    parser_map = subparsers.add_parser('map', help='Internal: Run map task.')\n    parser_map.add_argument('read_fd', type=int, help='Read file descriptor', nargs='?')\n    parser_map.add_argument('write_fd', type=int, help='Write file descriptor', nargs='?')\n    parser_map.set_defaults(func=lambda **y: run_task(mapper, reducer, combiner, command='map', **y))\n\n    parser_combine = subparsers.add_parser('combine', help='Internal: Run combine task.')\n    parser_combine.add_argument('read_fd', type=int, help='Read file descriptor', nargs='?')\n    parser_combine.add_argument('write_fd', type=int, help='Write file descriptor', nargs='?')\n    parser_combine.set_defaults(func=lambda **y: run_task(mapper, reducer, combiner, command='combine', **y))\n\n    parser_reduce = subparsers.add_parser('reduce', help='Internal: Run reduce task.')\n    parser_reduce.add_argument('read_fd', type=int, help='Read file descriptor', nargs='?')\n    parser_reduce.add_argument('write_fd', type=int, help='Write file descriptor', nargs='?')\n    parser_reduce.set_defaults(func=lambda **y: run_task(mapper, reducer, combiner, command='reduce', **y))\n\n    args = vars(parser.parse_args())\n    # Handle logging arguments\n    if 'log' in args:\n        numeric_level = getattr(logging, args['log'].upper(), None)\n        logging.basicConfig(level=numeric_level)\n        del args['log']\n    # Call function with all arguments except for itself\n    func = args['func']\n    del args['func']\n    func(**args)", "response": "Hadoopy entrance function\n\n    This is to be called in all Hadoopy job's.  Handles arguments passed in,\n    calls the provided functions with input, and stores the output.\n\n    TypedBytes are used if the following is True\n    os.environ['stream_map_input'] == 'typedbytes'\n\n    It is *highly* recommended that TypedBytes be used for all non-trivial\n    tasks.  Keep in mind that the semantics of what you can safely emit from\n    your functions is limited when using Text (i.e., no \\\\t or \\\\n).  You can use\n    the base64 module to ensure that your output is clean.\n\n    If the HADOOPY_CHDIR environmental variable is set, this will immediately\n    change the working directory to the one specified.  This is useful if your\n    data is provided in an archive but your program assumes it is in that\n    directory.\n\n    As hadoop streaming relies on stdin/stdout/stderr for communication,\n    anything that outputs on them in an unexpected way (especially stdout) will\n    break the pipe on the Java side and can potentially cause data errors.  To\n    fix this problem, hadoopy allows file descriptors (integers) to be provided\n    to each task.  These will be used instead of stdin/stdout by hadoopy.  This\n    is designed to combine with the 'pipe' command.\n\n    To use the pipe functionality, instead of using\n    `your_script.py map` use `your_script.py pipe map`\n    which will call the script as a subprocess and use the read_fd/write_fd\n    command line arguments for communication.  This isolates your script and\n    eliminates the largest source of errors when using hadoop streaming.\n\n    The pipe functionality has the following semantics\n    stdin: Always an empty file\n    stdout: Redirected to stderr (which is visible in the hadoop log)\n    stderr: Kept as stderr\n    read_fd: File descriptor that points to the true stdin\n    write_fd: File descriptor that points to the true stdout\n\n    | **Command Interface**\n    | The command line switches added to your script (e.g., script.py) are\n\n    python script.py *map* (read_fd) (write_fd)\n        Use the provided mapper, optional read_fd/write_fd.\n    python script.py *reduce* (read_fd) (write_fd)\n        Use the provided reducer, optional read_fd/write_fd.\n    python script.py *combine* (read_fd) (write_fd)\n        Use the provided combiner, optional read_fd/write_fd.\n    python script.py *freeze* <tar_path> <-Z add_file0 -Z add_file1...>\n        Freeze the script to a tar file specified by <tar_path>.  The extension\n        may be .tar or .tar.gz.  All files are placed in the root of the tar.\n        Files specified with -Z will be added to the tar root.\n    python script.py info\n        Prints a json object containing 'tasks' which is a list of tasks which\n        can include 'map', 'combine', and 'reduce'.  Also contains 'doc' which is\n        the provided documentation through the doc argument to the run function.\n        The tasks correspond to provided inputs to the run function.\n\n    | **Specification of mapper/reducer/combiner**\n    | Input Key/Value Types\n    |     For TypedBytes/SequenceFileInputFormat, the Key/Value are the decoded TypedBytes\n    |     For TextInputFormat, the Key is a byte offset (int) and the Value is a line without the newline (string)\n    |\n    | Output Key/Value Types\n    |     For TypedBytes, anything Pickle-able can be used\n    |     For Text, types are converted to string.  Note that neither may contain \\\\t or \\\\n as these are used in the encoding.  Output is key\\\\tvalue\\\\n\n    |\n    | Expected arguments\n    |     mapper(key, value) or mapper.map(key, value)\n    |     reducer(key, values) or reducer.reduce(key, values)\n    |     combiner(key, values) or combiner.reduce(key, values)\n    |\n    | Optional methods\n    |     func.configure(): Called before any input read.  Returns None.\n    |     func.close():  Called after all input read.  Returns None or Iterator of (key, value)\n    |\n    | Expected return\n    |     None or Iterator of (key, value)\n\n    :param mapper: Function or class following the above spec\n    :param reducer: Function or class following the above spec\n    :param combiner: Function or class following the above spec\n    :param doc: If specified, on error print this and call sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map(self, key, value):\n        try:\n            image = imfeat.resize_image(imfeat.image_fromstring(value, {'type': 'numpy', 'mode': 'bgr', 'dtype': 'uint8'}),\n                                        100, 100)\n        except:\n            hadoopy.counter('DATA_ERRORS', 'ImageLoadError')\n            return\n\n        # Distance metric\n        diff = image - self.target_image\n        dist = np.sum(diff * diff)\n\n        # Output\n        if dist < self.min_dist:\n            self.min_dist = dist\n            self.min_key = key\n            self.min_value = value", "response": "Map image to a new image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noutput a counter update that is displayed in the Hadoop web interface.", "response": "def counter(group, counter, amount=1, err=None):\n    \"\"\"Output a counter update that is displayed in the Hadoop web interface\n\n    Counters are useful for quickly identifying the number of times an error\n    occurred, current progress, or coarse statistics.\n\n    :param group: Counter group\n    :param counter: Counter name\n    :param amount: Value to add (default 1)\n    :param err: Func that outputs a string, if None then sys.stderr.write is used (default None)\n    \"\"\"\n    if not err:\n        err = _err\n    err(\"reporter:counter:%s,%s,%s\\n\" % (group, counter, str(amount)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_random_graph(node_num, edge_num, self_loops=False, multi_edges=False):\n    '''\n    Generates and returns a :py:class:`~altgraph.Graph.Graph` instance with *node_num* nodes\n    randomly connected by *edge_num* edges.\n    '''\n    g = Graph.Graph()\n\n    if not multi_edges:\n        if self_loops:\n            max_edges = node_num * node_num\n        else:\n            max_edges = node_num * (node_num-1)\n\n        if edge_num > max_edges:\n            raise GraphError(\"inconsistent arguments to 'generate_random_graph'\")\n\n    nodes = range(node_num)\n\n    for node in nodes:\n        g.add_node(node)\n\n    while 1:\n        head = random.choice(nodes)\n        tail = random.choice(nodes)\n\n        # loop defense\n        if head == tail and not self_loops:\n            continue\n\n        # multiple edge defense\n        if g.edge_by_node(head,tail) is not None and not multi_edges:\n            continue\n\n        # add the edge\n        g.add_edge(head, tail)\n        if g.number_of_edges() >= edge_num:\n            break\n\n    return g", "response": "Generates and returns a random graph with node_num and edge_num edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_scale_free_graph(steps, growth_num, self_loops=False, multi_edges=False):\n    '''\n    Generates and returns a :py:class:`~altgraph.Graph.Graph` instance that will have *steps* \\* *growth_num* nodes\n    and a scale free (powerlaw) connectivity. Starting with a fully connected graph with *growth_num* nodes\n    at every step *growth_num* nodes are added to the graph and are connected to existing nodes with\n    a probability proportional to the degree of these existing nodes.\n    '''\n    # FIXME: The code doesn't seem to do what the documentation claims.\n    graph = Graph.Graph()\n\n    # initialize the graph\n    store = []\n    for i in range(growth_num):\n        #store   += [ i ] * (growth_num - 1)\n        for j in range(i + 1, growth_num):\n            store.append(i)\n            store.append(j)\n            graph.add_edge(i,j)\n\n    # generate\n    for node in range(growth_num, steps * growth_num):\n        graph.add_node(node)\n        while ( graph.out_degree(node) < growth_num ):\n            nbr = random.choice(store)\n\n            # loop defense\n            if node == nbr and not self_loops:\n                continue\n\n            # multi edge defense\n            if graph.edge_by_node(node, nbr) and not multi_edges:\n                continue\n\n            graph.add_edge(node, nbr)\n\n\n        for nbr in graph.out_nbrs(node):\n            store.append(node)\n            store.append(nbr)\n\n    return graph", "response": "Generates a graph that is a fully connected graph with growth_num nodes and a scale free connectivity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a depth - first walk in a depth - first order starting at head.", "response": "def filter_stack(graph, head, filters):\n    \"\"\"\n    Perform a walk in a depth-first order starting\n    at *head*.\n\n    Returns (visited, removes, orphans).\n\n    * visited: the set of visited nodes\n    * removes: the list of nodes where the node\n      data does not all *filters*\n    * orphans: tuples of (last_good, node),\n      where node is not in removes, is directly \n      reachable from a node in *removes* and \n      *last_good* is the closest upstream node that is not\n      in *removes*.\n    \"\"\"\n\n    visited, removes, orphans = set([head]), set(), set()\n    stack = deque([(head, head)])\n    get_data = graph.node_data\n    get_edges = graph.out_edges\n    get_tail = graph.tail\n\n    while stack:\n        last_good, node = stack.pop()\n        data = get_data(node)\n        if data is not None:\n            for filtfunc in filters:\n                if not filtfunc(data):\n                    removes.add(node)\n                    break\n            else:\n                last_good = node\n        for edge in get_edges(node):\n            tail = get_tail(edge)\n            if last_good is not node:\n                orphans.add((last_good, tail))\n            if tail not in visited:\n                visited.add(tail)\n                stack.append((last_good, tail))\n\n    orphans = [(last_good, tail) for (last_good, tail) in orphans if tail not in removes]\n    #orphans.sort()\n\n    return visited, removes, orphans"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef walkRelocatables(self, shouldRelocateCommand=_shouldRelocateCommand):\n        for (idx, (lc, cmd, data)) in enumerate(self.commands):\n            if shouldRelocateCommand(lc.cmd):\n                name = _RELOCATABLE_NAMES[lc.cmd]\n                ofs = cmd.name - sizeof(lc.__class__) - sizeof(cmd.__class__)\n                yield idx, name, data[ofs:data.find(B('\\x00'), ofs)].decode(\n                        sys.getfilesystemencoding())", "response": "Generator that yields all relocatable commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rewriteInstallNameCommand(self, loadcmd):\n        if self.id_cmd is not None:\n            self.rewriteDataForCommand(self.id_cmd, loadcmd)\n            return True\n        return False", "response": "Rewrite the load command of this dylib"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrewrites the load commands based upon a change dictionary.", "response": "def rewriteLoadCommands(self, changefunc):\n        \"\"\"\n        Rewrite the load commands based upon a change dictionary\n        \"\"\"\n        data = changefunc(self.parent.filename)\n        changed = False\n        if data is not None:\n            if self.rewriteInstallNameCommand(\n                    data.encode(sys.getfilesystemencoding())):\n                changed = True\n        for idx, name, filename in self.walkRelocatables():\n            data = changefunc(filename)\n            if data is not None:\n                if self.rewriteDataForCommand(idx, data.encode(\n                        sys.getfilesystemencoding())):\n                    changed = True\n        return changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sizeof(s):\n    if hasattr(s, '_size_'):\n        return s._size_\n\n    elif isinstance(s, bytes):\n        return len(s)\n\n    raise ValueError(s)", "response": "Return the size of an object when packed\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pypackable(name, pytype, format):\n    size, items = _formatinfo(format)\n    return type(Packable)(name, (pytype, Packable), {\n        '_format_': format,\n        '_size_': size,\n        '_items_': items,\n    })", "response": "Create a mix - in class with a python type and a\n    Packable with the given struct format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the size and number of items in a struct format.", "response": "def _formatinfo(format):\n    \"\"\"\n    Calculate the size and number of items in a struct format.\n    \"\"\"\n    size = struct.calcsize(format)\n    return size, len(struct.unpack(format, B('\\x00') * size))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compile_pycos(toc):\n    global BUILDPATH\n\n    # For those modules that need to be rebuilt, use the build directory\n    # PyInstaller creates during the build process.\n    basepath = os.path.join(BUILDPATH, \"localpycos\")\n\n    new_toc = []\n    for (nm, fnm, typ) in toc:\n\n        # Trim the terminal \"c\" or \"o\"\n        source_fnm = fnm[:-1]\n\n        # If the source is newer than the compiled, or the compiled doesn't\n        # exist, we need to perform a build ourselves.\n        if mtime(source_fnm) > mtime(fnm):\n            try:\n                py_compile.compile(source_fnm)\n            except IOError:\n                # If we're compiling on a system directory, probably we don't\n                # have write permissions; thus we compile to a local directory\n                # and change the TOC entry accordingly.\n                ext = os.path.splitext(fnm)[1]\n\n                if \"__init__\" not in fnm:\n                    # If it's a normal module, use last part of the qualified\n                    # name as module name and the first as leading path\n                    leading, mod_name = nm.split(\".\")[:-1], nm.split(\".\")[-1]\n                else:\n                    # In case of a __init__ module, use all the qualified name\n                    # as leading path and use \"__init__\" as the module name\n                    leading, mod_name = nm.split(\".\"), \"__init__\"\n\n                leading = os.path.join(basepath, *leading)\n\n                if not os.path.exists(leading):\n                    os.makedirs(leading)\n\n                fnm = os.path.join(leading, mod_name + ext)\n                py_compile.compile(source_fnm, fnm)\n\n        new_toc.append((nm, fnm, typ))\n\n    return new_toc", "response": "This function compiles all the required ArcGIS files and returns a list of tuples with the updated pathnames."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new TOC with proper library suffix for EXTENSION items.", "response": "def addSuffixToExtensions(toc):\n    \"\"\"\n    Returns a new TOC with proper library suffix for EXTENSION items.\n    \"\"\"\n    new_toc = TOC()\n    for inm, fnm, typ in toc:\n        if typ in ('EXTENSION', 'DEPENDENCY'):\n            binext = os.path.splitext(fnm)[1]\n            if not os.path.splitext(inm)[1] == binext:\n                inm = inm + binext\n        new_toc.append((inm, fnm, typ))\n    return new_toc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if guts are equal", "response": "def _check_guts_eq(attr, old, new, last_build):\n    \"\"\"\n    rebuild is required if values differ\n    \"\"\"\n    if old != new:\n        logger.info(\"building because %s changed\", attr)\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if mtimes of files listed in old toc are newer than ast_build Taxonomy", "response": "def _check_guts_toc_mtime(attr, old, toc, last_build, pyc=0):\n    \"\"\"\n    rebuild is required if mtimes of files listed in old toc are newer\n    than ast_build\n\n    if pyc=1, check for .py files, too\n    \"\"\"\n    for (nm, fnm, typ) in old:\n        if mtime(fnm) > last_build:\n            logger.info(\"building because %s changed\", fnm)\n            return True\n        elif pyc and mtime(fnm[:-1]) > last_build:\n            logger.info(\"building because %s changed\", fnm[:-1])\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_guts_toc(attr, old, toc, last_build, pyc=0):\n    return (_check_guts_eq(attr, old, toc, last_build)\n            or _check_guts_toc_mtime(attr, old, toc, last_build, pyc=pyc))", "response": "Check if toc content changed since last build."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rmdir(path):\n    if not os.path.abspath(path):\n        path = os.path.abspath(path)\n    if not path.startswith(BUILDPATH) and os.path.isdir(path) and os.listdir(path):\n        specerr = 0\n        if BUILDPATH.startswith(path):\n            logger.error('specfile error: The output path \"%s\" contains '\n                         'BUILDPATH (%s)', path, BUILDPATH)\n            specerr += 1\n        if SPECPATH.startswith(path):\n            logger.error('Specfile error: The output path \"%s\" contains '\n                         'SPECPATH (%s)', path, SPECPATH)\n            specerr += 1\n        if specerr:\n            raise SystemExit('Error: Please edit/recreate the specfile (%s) '\n                             'and set a different output name (e.g. \"dist\").'\n                             % SPEC)\n        if NOCONFIRM:\n            choice = 'y'\n        elif sys.stdout.isatty():\n            choice = raw_input('WARNING: The output directory \"%s\" and ALL ITS '\n                               'CONTENTS will be REMOVED! Continue? (y/n)' % path)\n        else:\n            raise SystemExit('Error: The output directory \"%s\" is not empty. '\n                             'Please remove all its contents or use the '\n                             '-y option (remove output directory without '\n                             'confirmation).' % path)\n        if choice.strip().lower() == 'y':\n            logger.info('Removing %s', path)\n            shutil.rmtree(path)\n        else:\n            raise SystemExit('User aborted')", "response": "Remove a directory from the build directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if path points to a file inside a python egg file or to an egg directly.", "response": "def check_egg(pth):\n    \"\"\"Check if path points to a file inside a python egg file (or to an egg\n       directly).\"\"\"\n    if is_py23:\n        if os.path.altsep:\n            pth = pth.replace(os.path.altsep, os.path.sep)\n        components = pth.split(os.path.sep)\n        sep = os.path.sep\n    else:\n        components = pth.replace(\"\\\\\", \"/\").split(\"/\")\n        sep = \"/\"\n        if is_win:\n            sep = \"\\\\\"\n    for i, name in zip(range(0, len(components)), components):\n        if name.lower().endswith(\".egg\"):\n            eggpth = sep.join(components[:i + 1])\n            if os.path.isfile(eggpth):\n                # eggs can also be directories!\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checkCache(fnm, strip=0, upx=0):\n    # On darwin a cache is required anyway to keep the libaries\n    # with relative install names. Caching on darwin does not work\n    # since we need to modify binary headers to use relative paths\n    # to dll depencies and starting with '@executable_path'.\n\n    if ((not strip and not upx and not is_darwin and not is_win)\n        or fnm.lower().endswith(\".manifest\")):\n        return fnm\n    if strip:\n        strip = 1\n    else:\n        strip = 0\n    if upx:\n        upx = 1\n    else:\n        upx = 0\n\n    # Load cache index\n    cachedir = os.path.join(CONFIGDIR, 'bincache%d%d' % (strip, upx))\n    if not os.path.exists(cachedir):\n        os.makedirs(cachedir)\n    cacheindexfn = os.path.join(cachedir, \"index.dat\")\n    if os.path.exists(cacheindexfn):\n        cache_index = _load_data(cacheindexfn)\n    else:\n        cache_index = {}\n\n    # Verify if the file we're looking for is present in the cache.\n    basenm = os.path.normcase(os.path.basename(fnm))\n    digest = cacheDigest(fnm)\n    cachedfile = os.path.join(cachedir, basenm)\n    cmd = None\n    if basenm in cache_index:\n        if digest != cache_index[basenm]:\n            os.remove(cachedfile)\n        else:\n            # On Mac OS X we need relative paths to dll dependencies\n            # starting with @executable_path\n            if is_darwin:\n                dylib.mac_set_relative_dylib_deps(cachedfile)\n            return cachedfile\n    if upx:\n        if strip:\n            fnm = checkCache(fnm, 1, 0)\n        bestopt = \"--best\"\n        # FIXME: Linux builds of UPX do not seem to contain LZMA (they assert out)\n        # A better configure-time check is due.\n        if config[\"hasUPX\"] >= (3,) and os.name == \"nt\":\n            bestopt = \"--lzma\"\n\n        upx_executable = \"upx\"\n        if config.get('upx_dir'):\n            upx_executable = os.path.join(config['upx_dir'], upx_executable)\n        cmd = [upx_executable, bestopt, \"-q\", cachedfile]\n    else:\n        if strip:\n            # -S = strip only debug symbols.\n            # The default strip behaviour breaks some shared libraries\n            # under Mac OSX\n            cmd = [\"strip\", \"-S\", cachedfile]\n    shutil.copy2(fnm, cachedfile)\n    os.chmod(cachedfile, 0755)\n\n    if pyasm and fnm.lower().endswith(\".pyd\"):\n        # If python.exe has dependent assemblies, check for embedded manifest\n        # of cached pyd file because we may need to 'fix it' for pyinstaller\n        try:\n            res = winmanifest.GetManifestResources(os.path.abspath(cachedfile))\n        except winresource.pywintypes.error, e:\n            if e.args[0] == winresource.ERROR_BAD_EXE_FORMAT:\n                # Not a win32 PE file\n                pass\n            else:\n                logger.error(os.path.abspath(cachedfile))\n                raise\n        else:\n            if winmanifest.RT_MANIFEST in res and len(res[winmanifest.RT_MANIFEST]):\n                for name in res[winmanifest.RT_MANIFEST]:\n                    for language in res[winmanifest.RT_MANIFEST][name]:\n                        try:\n                            manifest = winmanifest.Manifest()\n                            manifest.filename = \":\".join([cachedfile,\n                                                          str(winmanifest.RT_MANIFEST),\n                                                          str(name),\n                                                          str(language)])\n                            manifest.parse_string(res[winmanifest.RT_MANIFEST][name][language],\n                                                  False)\n                        except Exception, exc:\n                            logger.error(\"Cannot parse manifest resource %s, \"\n                                         \"%s from\", name, language)\n                            logger.error(cachedfile)\n                            logger.exception(exc)\n                        else:\n                            # Fix the embedded manifest (if any):\n                            # Extension modules built with Python 2.6.5 have\n                            # an empty <dependency> element, we need to add\n                            # dependentAssemblies from python.exe for\n                            # pyinstaller\n                            olen = len(manifest.dependentAssemblies)\n                            _depNames = set([dep.name for dep in\n                                             manifest.dependentAssemblies])\n                            for pydep in pyasm:\n                                if not pydep.name in _depNames:\n                                    logger.info(\"Adding %r to dependent \"\n                                                \"assemblies of %r\",\n                                                pydep.name, cachedfile)\n                                    manifest.dependentAssemblies.append(pydep)\n                                    _depNames.update(pydep.name)\n                            if len(manifest.dependentAssemblies) > olen:\n                                try:\n                                    manifest.update_resources(os.path.abspath(cachedfile),\n                                                              [name],\n                                                              [language])\n                                except Exception, e:\n                                    logger.error(os.path.abspath(cachedfile))\n                                    raise\n\n    if cmd:\n        try:\n            compat.exec_command(*cmd)\n        except OSError, e:\n            raise SystemExit(\"Execution failed: %s\" % e)\n\n    # update cache index\n    cache_index[basenm] = digest\n    _save_data(cacheindexfn, cache_index)\n\n    # On Mac OS X we need relative paths to dll dependencies\n    # starting with @executable_path\n    if is_darwin:\n        dylib.mac_set_relative_dylib_deps(cachedfile)\n    return cachedfile", "response": "Check if a file is in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging repeated dependencies from a list of Analysis objects supplied as argument.", "response": "def MERGE(*args):\n    \"\"\"\n    Wipe repeated dependencies from a list of (Analysis, id, filename) tuples,\n    supplied as argument. Replace id with the correct filename.\n    \"\"\"\n\n    # Get the longest common path\n    common_prefix = os.path.dirname(os.path.commonprefix([os.path.abspath(a.scripts[-1][1]) for a, _, _ in args]))\n    if common_prefix[-1] != os.sep:\n        common_prefix += os.sep\n    logger.info(\"Common prefix: %s\", common_prefix)\n    # Adjust dependencies for each Analysis object; the first Analysis in the\n    # list will include all dependencies.\n    id_to_path = {}\n    for _, i, p in args:\n        id_to_path[i] = p\n    dependencies = {}\n    for analysis, _, _ in args:\n        path = os.path.abspath(analysis.scripts[-1][1]).replace(common_prefix, \"\", 1)\n        path = os.path.splitext(path)[0]\n        if path in id_to_path:\n            path = id_to_path[path]\n        set_dependencies(analysis, dependencies, path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn None if guts have changed", "response": "def get_guts(self, last_build, missing='missing or bad'):\n        \"\"\"\n        returns None if guts have changed\n        \"\"\"\n        try:\n            data = _load_data(self.out)\n        except:\n            logger.info(\"building because %s %s\", os.path.basename(self.out), missing)\n            return None\n\n        if len(data) != len(self.GUTS):\n            logger.info(\"building because %s is bad\", self.outnm)\n            return None\n        for i, (attr, func) in enumerate(self.GUTS):\n            if func is None:\n                # no check for this value\n                continue\n            if func(attr, data[i], getattr(self, attr), last_build):\n                return None\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fixMissingPythonLib(self, binaries):\n\n        if is_aix:\n            # Shared libs on AIX are archives with shared object members, thus the \".a\" suffix.\n            names = ('libpython%d.%d.a' % sys.version_info[:2],) \n        elif is_unix:\n            # Other *nix platforms.\n            names = ('libpython%d.%d.so' % sys.version_info[:2],)\n        elif is_darwin:\n            names = ('Python', 'libpython%d.%d.dylib' % sys.version_info[:2])\n        else:\n            return\n\n        for (nm, fnm, typ) in binaries:\n            for name in names:\n                if typ == 'BINARY' and name in fnm:\n                    # lib found\n                    return\n\n        # resume search using the first item in names\n        name = names[0]\n\n        if is_unix:\n            lib = bindepend.findLibrary(name)\n            if lib is None:\n                raise IOError(\"Python library not found!\")\n\n        elif is_darwin:\n            # On MacPython, Analysis.assemble is able to find the libpython with\n            # no additional help, asking for config['python'] dependencies.\n            # However, this fails on system python, because the shared library\n            # is not listed as a dependency of the binary (most probably it's\n            # opened at runtime using some dlopen trickery).\n            lib = os.path.join(sys.exec_prefix, 'Python')\n            if not os.path.exists(lib):\n                raise IOError(\"Python library not found!\")\n\n        binaries.append((os.path.split(lib)[1], lib, 'BINARY'))", "response": "Add the Python library if missing from the binaries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all DLLs in given directories and subdirectories.", "response": "def dlls_in_subdirs(directory):\n    \"\"\"Returns *.dll, *.so, *.dylib in given directories and subdirectories.\"\"\"\n    files = []\n    for root, dirs, files in os.walk(directory):\n        files.extend(dlls_in_dir(root))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dlls_in_dir(directory):\n    files = []\n    files.extend(glob.glob(os.path.join(directory, '*.so')))\n    files.extend(glob.glob(os.path.join(directory, '*.dll')))\n    files.extend(glob.glob(os.path.join(directory, '*.dylib')))\n    return files", "response": "Returns a list of all DLLs in given directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_repo_revision():\n    '''\n    Returns git revision string somelike `git rev-parse --short HEAD`\n    does.\n\n    Returns an empty string if anything goes wrong, such as missing\n    .hg files or an unexpected format of internal HG files or no\n    mercurial repository found.\n    '''\n    repopath = _findrepo()\n    if not repopath:\n        return ''\n    try:\n        head = open(os.path.join(repopath, 'HEAD'), 'rU').read()\n        for l in head.splitlines():\n            l = l.split()\n            if l[0] == 'ref:':\n                ref = l[1]\n                break\n        else:\n            ref = None\n        if ref:\n            rev = open(os.path.join(repopath, ref), 'rU').read()\n            rev = rev[:7]\n            if rev:\n                return rev\n    except IOError:\n        pass\n    try:\n        rev = compat.exec_command('git', 'rev-parse', '--short', 'HEAD').strip()\n        if rev:\n            return rev\n    except:\n        pass\n    return ''", "response": "Returns git revision string somelike git rev - parse HEAD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_random_key(size=32):\n    import os\n    if hasattr(os, \"urandom\"): # Python 2.4+\n        return os.urandom(size)\n\n    # Try using PyCrypto if available\n    try:\n        from Crypto.Util.randpool import RandomPool\n        from Crypto.Hash import SHA256\n        return RandomPool(hash=SHA256).get_bytes(size)\n\n    except ImportError:\n        print >>sys.stderr, \"WARNING: The generated key will not be cryptographically-secure key. Consider using Python 2.4+ to generate the key, or install PyCrypto.\"\n\n        # Stupid random generation\n        import random\n        L = []\n        for i in range(size):\n            L.append(chr(random.randint(0, 255)))\n        return \"\".join(L)", "response": "Generate a cryptographically - secure random key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef display(self, mode='dot'):\n        '''\n        Displays the current graph via dotty\n        '''\n\n        if  mode == 'neato':\n            self.save_dot(self.temp_neo)\n            neato_cmd = \"%s -o %s %s\" % (self.neato, self.temp_dot, self.temp_neo)\n            os.system(neato_cmd)\n        else:\n            self.save_dot(self.temp_dot)\n\n        plot_cmd = \"%s %s\" % (self.dotty, self.temp_dot)\n        os.system(plot_cmd)", "response": "Displays the current graph via dotty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef node_style(self, node, **kwargs):\n        '''\n        Modifies a node style to the dot representation.\n        '''\n        if node not in self.edges:\n            self.edges[node] = {}\n        self.nodes[node] = kwargs", "response": "Modifies a node style to the dot representation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all_node_style(self, **kwargs):\n        '''\n        Modifies all node styles\n        '''\n        for node in self.nodes:\n            self.node_style(node, **kwargs)", "response": "Modifies all node styles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying an edge style to the dot representation.", "response": "def edge_style(self, head, tail, **kwargs):\n        '''\n        Modifies an edge style to the dot representation.\n        '''\n        if tail not in self.nodes:\n            raise GraphError(\"invalid node %s\" % (tail,))\n\n        try:\n            if tail not in self.edges[head]:\n                self.edges[head][tail]= {}\n            self.edges[head][tail] = kwargs\n        except KeyError:\n            raise GraphError(\"invalid edge  %s -> %s \" % (head, tail) )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the current graph representation into a file_name.", "response": "def save_dot(self, file_name=None):\n        '''\n        Saves the current graph representation into a file\n        '''\n\n        if not file_name:\n            warnings.warn(DeprecationWarning, \"always pass a file_name\")\n            file_name = self.temp_dot\n\n        fp   = open(file_name, \"w\")\n        try:\n            for chunk in self.iterdot():\n                fp.write(chunk)\n        finally:\n            fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the dot file as an image file file", "response": "def save_img(self, file_name=None, file_type=\"gif\", mode='dot'):\n        '''\n        Saves the dot file as an image file\n        '''\n\n        if not file_name:\n            warnings.warn(DeprecationWarning, \"always pass a file_name\")\n            file_name = \"out\"\n\n        if  mode == 'neato':\n            self.save_dot(self.temp_neo)\n            neato_cmd = \"%s -o %s %s\" % (self.neato, self.temp_dot, self.temp_neo)\n            os.system(neato_cmd)\n            plot_cmd = self.dot\n        else:\n            self.save_dot(self.temp_dot)\n            plot_cmd = self.dot\n\n        file_name  = \"%s.%s\" % (file_name, file_type)\n        create_cmd = \"%s -T%s %s -o %s\" % (plot_cmd, file_type, self.temp_dot, file_name)\n        os.system(create_cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the revision of the mercurial repository.", "response": "def get_repo_revision():\n    '''\n    Returns mercurial revision string somelike `hg identify` does.\n\n    Format is rev1:short-id1+;rev2:short-id2+\n\n    Returns an empty string if anything goes wrong, such as missing\n    .hg files or an unexpected format of internal HG files or no\n    mercurial repository found.\n    '''\n    repopath = _findrepo()\n    if not repopath:\n        return ''\n    # first try to use mercurial itself\n    try:\n        import mercurial.hg, mercurial.ui, mercurial.scmutil\n        from mercurial.node import short as hexfunc\n    except ImportError:\n        pass\n    else:\n        ui = mercurial.ui.ui()\n        repo = mercurial.hg.repository(ui, repopath)\n        parents = repo[None].parents()\n        changed = filter(None, repo.status()) and \"+\" or \"\"\n        return ';'.join(['%s:%s%s' % (p.rev(), hexfunc(p.node()), changed)\n                         for p in parents])\n\n    # todo: mercurial not found, try to retrieve the information\n    # ourselves\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cython_version():\n    import Cython.Compiler.Main\n    match = re.search('^([0-9]+)\\.([0-9]+)',\n                      Cython.Compiler.Main.Version.version)\n    try:\n        return map(int, match.groups())\n    except AttributeError:\n        raise ImportError", "response": "Returns the version of the Cython package as a pair of ints"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_glibc_version():\n    # TODO: Look into a nicer way to get the version\n    try:\n        out = subprocess.Popen(['ldd', '--version'],\n                               stdout=subprocess.PIPE).communicate()[0]\n    except OSError:\n        return\n    match = re.search('([0-9]+)\\.([0-9]+)\\.?[0-9]*', out)\n    try:\n        return map(int, match.groups())\n    except AttributeError:\n        return", "response": "Returns the version of the glibc version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect the image with the minimum distance", "response": "def reduce(self, key, values):\n        \"\"\"Select the image with the minimum distance\n\n        Args:\n            key: (see mapper)\n            values: (see mapper)\n\n        Yields:\n            A tuple in the form of (key, value)\n            key: Image name\n            value: Image as jpeg byte data\n        \"\"\"\n        \n        dist, key, value = min(values, key=lambda x: x[0])\n        print('MinDist[%f]' % dist)\n        yield key, value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking to see if the file object self. lib actually has a file we understand.", "response": "def checkmagic(self):\n        \"\"\"\n        Overridable.\n        Check to see if the file object self.lib actually has a file\n        we understand.\n        \"\"\"\n        self.lib.seek(self.start)  # default - magic is at start of file\n\n        if self.lib.read(len(self.MAGIC)) != self.MAGIC:\n            raise ArchiveReadError(\"%s is not a valid %s archive file\"\n                                   % (self.path, self.__class__.__name__))\n\n        if self.lib.read(len(self.pymagic)) != self.pymagic:\n            raise ArchiveReadError(\"%s has version mismatch to dll\" %\n                (self.path))\n\n        self.lib.read(4)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the TOC from the current file.", "response": "def loadtoc(self):\n        \"\"\"\n        Overridable.\n        Default: After magic comes an int (4 byte native) giving the\n        position of the TOC within self.lib.\n        Default: The TOC is a marshal-able string.\n        \"\"\"\n        self.lib.seek(self.start + self.TOCPOS)\n        (offset,) = struct.unpack('!i', self.lib.read(4))\n        self.lib.seek(self.start + offset)\n        self.toc = marshal.load(self.lib)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(self, name):\n        ispkg, pos = self.toc.get(name, (0, None))\n        if pos is None:\n            return None\n        self.lib.seek(self.start + pos)\n        return ispkg, marshal.load(self.lib)", "response": "Get the object corresponding to name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, path, lTOC):\n        self.path = path\n        self.lib = open(path, 'wb')\n        #reserve space for the header\n        if self.HDRLEN:\n            self.lib.write('\\0' * self.HDRLEN)\n\n            #create an empty toc\n\n        if type(self.TOCTMPLT) == type({}):\n            self.toc = {}\n        else:       # assume callable\n            self.toc = self.TOCTMPLT()\n\n        for tocentry in lTOC:\n            self.add(tocentry)   # the guts of the archive\n\n        tocpos = self.lib.tell()\n        self.save_toc(tocpos)\n        if self.TRLLEN:\n            self.save_trailer(tocpos)\n        if self.HDRLEN:\n            self.update_headers(tocpos)\n        self.lib.close()", "response": "Build an archive file of name path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an entry to the archive.", "response": "def add(self, entry):\n        \"\"\"\n        Override this to influence the mechanics of the Archive.\n        Assumes entry is a seq beginning with (nm, pth, ...) where\n        nm is the key by which we'll be asked for the object.\n        pth is the name of where we find the object. Overrides of\n        get_obj_from can make use of further elements in entry.\n        \"\"\"\n        if self.os is None:\n            import os\n            self.os = os\n        nm = entry[0]\n        pth = entry[1]\n        pynm, ext = self.os.path.splitext(self.os.path.basename(pth))\n        ispkg = pynm == '__init__'\n        assert ext in ('.pyc', '.pyo')\n        self.toc[nm] = (ispkg, self.lib.tell())\n        f = open(entry[1], 'rb')\n        f.seek(8)  # skip magic and timestamp\n        self.lib.write(f.read())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_headers(self, tocpos):\n        self.lib.seek(self.start)\n        self.lib.write(self.MAGIC)\n        self.lib.write(self.pymagic)\n        self.lib.write(struct.pack('!i', tocpos))", "response": "Update the internal internal"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_headers(self, tocpos):\n        Archive.update_headers(self, tocpos)\n        self.lib.write(struct.pack('!iB', self.LEVEL, self.crypted))", "response": "Update the header of the archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures the given argument is in filesystem encoding", "response": "def fsencoding(s, encoding=sys.getfilesystemencoding()):\n    \"\"\"\n    Ensure the given argument is in filesystem encoding (not unicode)\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(encoding)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flipwritable(fn, mode=None):\n    if os.access(fn, os.W_OK):\n        return None\n    old_mode = os.stat(fn).st_mode\n    os.chmod(fn, stat.S_IWRITE | old_mode)\n    return old_mode", "response": "Flip the writability of a file and return the old mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mergecopy(src, dest):\n    if os.path.exists(dest) and os.stat(dest).st_mtime >= os.stat(src).st_mtime:\n        return\n    copy2(src, dest)", "response": "A recursive version of the copy2 function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sdk_normalize(filename):\n    if filename.startswith('/Developer/SDKs/'):\n        pathcomp = filename.split('/')\n        del pathcomp[1:4]\n        filename = '/'.join(pathcomp)\n    return filename", "response": "Normalize a path to strip out the SDK portion from the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the file is in a system path", "response": "def in_system_path(filename):\n    \"\"\"\n    Return True if the file is in a system path\n    \"\"\"\n    fn = sdk_normalize(os.path.realpath(filename))\n    if fn.startswith('/usr/local/'):\n        return False\n    elif fn.startswith('/System/') or fn.startswith('/usr/'):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_platform_file(path):\n    if not os.path.exists(path) or os.path.islink(path):\n        return False\n    # If the header is fat, we need to read into the first arch\n    fileobj = open(path, 'rb')\n    bytes = fileobj.read(MAGIC_LEN)\n    if bytes == FAT_MAGIC_BYTES:\n        # Read in the fat header\n        fileobj.seek(0)\n        header = mach_o.fat_header.from_fileobj(fileobj, _endian_='>')\n        if header.nfat_arch < 1:\n            return False\n        # Read in the first fat arch header\n        arch = mach_o.fat_arch.from_fileobj(fileobj, _endian_='>')\n        fileobj.seek(arch.offset)\n        # Read magic off the first header\n        bytes = fileobj.read(MAGIC_LEN)\n    fileobj.close()\n    for magic in MAGIC:\n        if bytes == magic:\n            return True\n    return False", "response": "Return True if the file is Mach - O or Mach - O."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_platform_files(dst):\n    for root, dirs, files in os.walk(dst):\n        for fn in files:\n            fn = os.path.join(root, fn)\n            if is_platform_file(fn):\n                yield fn", "response": "Walk a directory and yield each full path that is a Mach - O file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef strip_files(files, argv_max=(256 * 1024)):\n    tostrip = [(fn, flipwritable(fn)) for fn in files]\n    while tostrip:\n        cmd = list(STRIPCMD)\n        flips = []\n        pathlen = reduce(operator.add, [len(s) + 1 for s in cmd])\n        while pathlen < argv_max:\n            if not tostrip:\n                break\n            added, flip = tostrip.pop()\n            pathlen += len(added) + 1\n            cmd.append(added)\n            flips.append((added, flip))\n        else:\n            cmd.pop()\n            tostrip.append(flips.pop())\n        os.spawnv(os.P_WAIT, cmd[0], cmd)\n        for args in flips:\n            flipwritable(*args)", "response": "Strip a list of files from the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun command with arguments.", "response": "def check_call(*popenargs, **kwargs):\n    \"\"\"Run command with arguments.  Wait for command to complete.  If\n    the exit code was zero then return, otherwise raise\n    CalledProcessError.  The CalledProcessError object will have the\n    return code in the returncode attribute.\n\n    The arguments are the same as for the Popen constructor.  Example:\n\n    check_call([\"ls\", \"-l\"])\n    \"\"\"\n    retcode = call(*popenargs, **kwargs)\n    cmd = kwargs.get(\"args\")\n    if cmd is None:\n        cmd = popenargs[0]\n    if retcode:\n        raise CalledProcessError(retcode, cmd)\n    return retcode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntranslate a sequence of arguments into a command line string.", "response": "def list2cmdline(seq):\n    \"\"\"\n    Translate a sequence of arguments into a command line\n    string, using the same rules as the MS C runtime:\n\n    1) Arguments are delimited by white space, which is either a\n       space or a tab.\n\n    2) A string surrounded by double quotation marks is\n       interpreted as a single argument, regardless of white space\n       or pipe characters contained within.  A quoted string can be\n       embedded in an argument.\n\n    3) A double quotation mark preceded by a backslash is\n       interpreted as a literal double quotation mark.\n\n    4) Backslashes are interpreted literally, unless they\n       immediately precede a double quotation mark.\n\n    5) If backslashes immediately precede a double quotation mark,\n       every pair of backslashes is interpreted as a literal\n       backslash.  If the number of backslashes is odd, the last\n       backslash escapes the next double quotation mark as\n       described in rule 3.\n    \"\"\"\n\n    # See\n    # http://msdn.microsoft.com/en-us/library/17w5ykft.aspx\n    # or search http://msdn.microsoft.com for\n    # \"Parsing C++ Command-Line Arguments\"\n    result = []\n    needquote = False\n    for arg in seq:\n        bs_buf = []\n\n        # Add a space to separate this argument from the others\n        if result:\n            result.append(' ')\n\n        needquote = (\" \" in arg) or (\"\\t\" in arg) or (\"|\" in arg) or not arg\n        if needquote:\n            result.append('\"')\n\n        for c in arg:\n            if c == '\\\\':\n                # Don't know if we need to double yet.\n                bs_buf.append(c)\n            elif c == '\"':\n                # Double backslashes.\n                result.append('\\\\' * len(bs_buf)*2)\n                bs_buf = []\n                result.append('\\\\\"')\n            else:\n                # Normal char\n                if bs_buf:\n                    result.extend(bs_buf)\n                    bs_buf = []\n                result.append(c)\n\n        # Add remaining backslashes, if any.\n        if bs_buf:\n            result.extend(bs_buf)\n\n        if needquote:\n            result.extend(bs_buf)\n            result.append('\"')\n\n    return ''.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninteract with process Send data to stdin and read data from stdout and stderr until end - of - file is reached.", "response": "def communicate(self, input=None):\n        \"\"\"Interact with process: Send data to stdin.  Read data from\n        stdout and stderr, until end-of-file is reached.  Wait for\n        process to terminate.  The optional input argument should be a\n        string to be sent to the child process, or None, if no data\n        should be sent to the child.\n\n        communicate() returns a tuple (stdout, stderr).\"\"\"\n\n        # Optimization: If we are only using one pipe, or no pipe at\n        # all, using select() or threads is unnecessary.\n        if [self.stdin, self.stdout, self.stderr].count(None) >= 2:\n            stdout = None\n            stderr = None\n            if self.stdin:\n                if input:\n                    self.stdin.write(input)\n                self.stdin.close()\n            elif self.stdout:\n                stdout = self.stdout.read()\n                self.stdout.close()\n            elif self.stderr:\n                stderr = self.stderr.read()\n                self.stderr.close()\n            self.wait()\n            return (stdout, stderr)\n\n        return self._communicate(input)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a command and returns a single .", "response": "def __exec_python_cmd(cmd):\n    \"\"\"\n    Executes an externally spawned Python interpreter and returns\n    anything that was emitted in the standard output as a single\n    string.\n    \"\"\"\n    # Prepend PYTHONPATH with pathex\n    pp = os.pathsep.join(PyInstaller.__pathex__)\n    old_pp = compat.getenv('PYTHONPATH')\n    if old_pp:\n        pp = os.pathsep.join([pp, old_pp])\n    compat.setenv(\"PYTHONPATH\", pp)\n    try:\n        try:\n            txt = compat.exec_python(*cmd)\n        except OSError, e:\n            raise SystemExit(\"Execution failed: %s\" % e)\n    finally:\n        if old_pp is not None:\n            compat.setenv(\"PYTHONPATH\", old_pp)\n        else:\n            compat.unsetenv(\"PYTHONPATH\")\n    return txt.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exec_script(scriptfilename, *args):\n\n    if scriptfilename != os.path.basename(scriptfilename):\n        raise SystemError(\"To prevent missuse, the script passed to \"\n                          \"hookutils.exec-script must be located in \"\n                          \"the `hooks` directory.\")\n\n    cmd = [os.path.join(os.path.dirname(__file__), scriptfilename)]\n    cmd.extend(args)\n    return __exec_python_cmd(cmd)", "response": "Executes a Python script in an externally spawned interpreter and returns the output of the standard output as a\n    single string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qt4_plugins_binaries(plugin_type):\n    binaries = []\n    pdir = qt4_plugins_dir()\n    files = misc.dlls_in_dir(os.path.join(pdir, plugin_type))\n    for f in files:\n        binaries.append((\n            os.path.join('qt4_plugins', plugin_type, os.path.basename(f)),\n            f, 'BINARY'))\n    return binaries", "response": "Return list of dynamic libraries formated for mod. binaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qt4_menu_nib_dir():\n    menu_dir = ''\n    # Detect MacPorts prefix (usually /opt/local).\n    # Suppose that PyInstaller is using python from macports.\n    macports_prefix = sys.executable.split('/Library')[0]\n    # list of directories where to look for qt_menu.nib\n    dirs = [\n        # Qt4 from MacPorts not compiled as framework.\n        os.path.join(macports_prefix, 'lib', 'Resources'),\n        # Qt4 from MacPorts compiled as framework.\n        os.path.join(macports_prefix, 'libexec', 'qt4-mac', 'lib',\n            'QtGui.framework', 'Versions', '4', 'Resources'),\n        # Qt4 installed into default location.\n        '/Library/Frameworks/QtGui.framework/Resources',\n        '/Library/Frameworks/QtGui.framework/Versions/4/Resources',\n        '/Library/Frameworks/QtGui.Framework/Versions/Current/Resources',\n    ]\n    # Check directory existence\n    for d in dirs:\n        d = os.path.join(d, 'qt_menu.nib')\n        if os.path.exists(d):\n            menu_dir = d\n            break\n\n    if not menu_dir:\n        logger.error('Cannont find qt_menu.nib directory')\n    return menu_dir", "response": "Return path to Qt resource dir qt_menu. nib."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding a library or framework using dyld semantics", "response": "def dyld_find(name, executable_path=None, env=None):\n    \"\"\"\n    Find a library or framework using dyld semantics\n    \"\"\"\n    name = _ensure_utf8(name)\n    executable_path = _ensure_utf8(executable_path)\n    for path in dyld_image_suffix_search(chain(\n                dyld_override_search(name, env),\n                dyld_executable_path_search(name, executable_path),\n                dyld_default_search(name, env),\n            ), env):\n        if os.path.isfile(path):\n            return path\n    raise ValueError, \"dylib %s could not be found\" % (name,)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a framework using dyld semantics.", "response": "def framework_find(fn, executable_path=None, env=None):\n    \"\"\"\n    Find a framework using dyld semantics in a very loose manner.\n\n    Will take input such as:\n        Python\n        Python.framework\n        Python.framework/Versions/Current\n    \"\"\"\n    try:\n        return dyld_find(fn, executable_path=executable_path, env=env)\n    except ValueError:\n        pass\n    fmwk_index = fn.rfind('.framework')\n    if fmwk_index == -1:\n        fmwk_index = len(fn)\n        fn += '.framework'\n    fn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))\n    return dyld_find(fn, executable_path=executable_path, env=env)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the SVN revision number of the current working copy.", "response": "def get_repo_revision():\n    '''\n    Returns SVN revision number.\n\n    Returns 0 if anything goes wrong, such as missing .svn files or\n    an unexpected format of internal SVN files or folder is not\n    a svn working copy.\n\n    See http://stackoverflow.com/questions/1449935/getting-svn-revision-number-into-a-program-automatically\n    '''\n    rev = 0\n    path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    entries_path = os.path.join(path, '.svn', 'entries')\n\n    try:\n        entries = open(entries_path, 'rU').read()\n    except IOError:\n        pass\n    else:\n        # Versions >= 7 of the entries file are flat text.  The first line is\n        # the version number. The next set of digits after 'dir' is the revision.\n        if re.match('(\\d+)', entries):\n            rev_match = re.search('\\d+\\s+dir\\s+(\\d+)', entries)\n            if rev_match:\n                rev = rev_match.groups()[0]\n        # Older XML versions of the file specify revision as an attribute of\n        # the first entries node.\n        else:\n            from xml.dom import minidom\n            dom = minidom.parse(entries_path)\n            rev = dom.getElementsByTagName('entry')[0].getAttribute('revision')\n\n    return rev"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_windows_dir():\n    try:\n        import win32api\n    except ImportError:\n        windir = compat.getenv('SystemRoot', compat.getenv('WINDIR'))\n    else:\n        windir = win32api.GetWindowsDirectory()\n    if not windir:\n        raise SystemExit(\"Error: Can not determine your Windows directory\")\n    return windir", "response": "Return the Windows directory e. g. C:\\\\Windows"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path that Windows will search for dlls.", "response": "def get_system_path():\n    \"\"\"Return the path that Windows will search for dlls.\"\"\"\n    _bpath = []\n    if is_win:\n        try:\n            import win32api\n        except ImportError:\n            logger.warn(\"Cannot determine your Windows or System directories\")\n            logger.warn(\"Please add them to your PATH if .dlls are not found\")\n            logger.warn(\"or install http://sourceforge.net/projects/pywin32/\")\n        else:\n            sysdir = win32api.GetSystemDirectory()\n            sysdir2 = os.path.normpath(os.path.join(sysdir, '..', 'SYSTEM'))\n            windir = win32api.GetWindowsDirectory()\n            _bpath = [sysdir, sysdir2, windir]\n    _bpath.extend(compat.getenv('PATH', '').split(os.pathsep))\n    return _bpath"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getChildElementsByTagName(self, tagName):\n    result = []\n    for child in self.childNodes:\n        if isinstance(child, Element):\n            if child.tagName == tagName:\n                result.append(child)\n    return result", "response": "Return child elements of type tagName if found else []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the first element of type tagName if found else None", "response": "def getFirstChildElementByTagName(self, tagName):\n    \"\"\" Return the first element of type tagName if found, else None \"\"\"\n    for child in self.childNodes:\n        if isinstance(child, Element):\n            if child.tagName == tagName:\n                return child\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates and return a manifest instance from a resource in dll. exe file.", "response": "def ManifestFromResFile(filename, names=None, languages=None):\n    \"\"\" Create and return manifest instance from resource in dll/exe file \"\"\"\n    res = GetManifestResources(filename, names, languages)\n    pth = []\n    if res and res[RT_MANIFEST]:\n        while isinstance(res, dict) and res.keys():\n            key = res.keys()[0]\n            pth.append(str(key))\n            res = res[key]\n    if isinstance(res, dict):\n        raise InvalidManifestError(\"No matching manifest resource found in '%s'\" % \n                                   filename)\n    manifest = Manifest()\n    manifest.filename = \":\".join([filename] + pth)\n    manifest.parse_string(res, False)\n    return manifest"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget manifest resources from file", "response": "def GetManifestResources(filename, names=None, languages=None):\n    \"\"\" Get manifest resources from file \"\"\"\n    return winresource.GetResources(filename, [RT_MANIFEST], names, languages)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate manifest XML as resource in dstpath.", "response": "def UpdateManifestResourcesFromXML(dstpath, xmlstr, names=None, \n                                   languages=None):\n    \"\"\" Update or add manifest XML as resource in dstpath \"\"\"\n    logger.info(\"Updating manifest in %s\", dstpath)\n    if dstpath.lower().endswith(\".exe\"):\n        name = 1 \n    else:\n        name = 2\n    winresource.UpdateResources(dstpath, xmlstr, RT_MANIFEST, names or [name], \n                             languages or [0, \"*\"])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating or add manifest XML from srcpath as resource in dstpath.", "response": "def UpdateManifestResourcesFromXMLFile(dstpath, srcpath, names=None, \n                                       languages=None):\n    \"\"\" Update or add manifest XML from srcpath as resource in dstpath \"\"\"\n    logger.info(\"Updating manifest from %s in %s\", srcpath, dstpath)\n    if dstpath.lower().endswith(\".exe\"):\n        name = 1 \n    else:\n        name = 2\n    winresource.UpdateResourcesFromDataFile(dstpath, srcpath, RT_MANIFEST, \n                                         names or [name], \n                                         languages or [0, \"*\"])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_manifest(filename, manifest, console):\n    if not manifest:\n        manifest = ManifestFromXMLFile(filename)\n        # /path/NAME.exe.manifest - split extension twice to get NAME.\n        name = os.path.basename(filename)\n        manifest.name = os.path.splitext(os.path.splitext(name)[0])[0]\n    elif isinstance(manifest, basestring) and \"<\" in manifest:\n        # Assume XML string\n        manifest = ManifestFromXML(manifest)\n    elif not isinstance(manifest, Manifest):\n        # Assume filename\n        manifest = ManifestFromXMLFile(manifest)\n    dep_names = set([dep.name for dep in manifest.dependentAssemblies])\n    if manifest.filename != filename:\n        # Update dependent assemblies\n        depmanifest = ManifestFromXMLFile(filename)\n        for assembly in depmanifest.dependentAssemblies:\n            if not assembly.name in dep_names:\n                manifest.dependentAssemblies.append(assembly)\n                dep_names.add(assembly.name)\n    if (not console and\n        not \"Microsoft.Windows.Common-Controls\" in dep_names):\n        # Add Microsoft.Windows.Common-Controls to dependent assemblies\n        manifest.dependentAssemblies.append(\n            Manifest(type_=\"win32\",\n                 name=\"Microsoft.Windows.Common-Controls\",\n                 language=\"*\",\n                 processorArchitecture=processor_architecture(),\n                 version=(6, 0, 0, 0),\n                 publicKeyToken=\"6595b64144ccf1df\")\n            )\n    manifest.writeprettyxml(filename)\n    return manifest", "response": "Create a manifest from an XML file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the hash of the file.", "response": "def calc_hash(self, hashalg=None):\n        \"\"\"\n        Calculate the hash of the file.\n        \n        Will be called automatically from the constructor if the file exists \n        and hashalg is given (and supported), but may also be called manually \n        e.g. to update the hash if the file has changed.\n        \n        \"\"\"\n        fd = open(self.filename, \"rb\")\n        buf = fd.read()\n        fd.close()\n        if hashalg:\n            self.hashalg = hashalg.upper()\n        self.hash = getattr(hashlib, self.hashalg.lower())(buf).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new dependent assembly to the list of dependent Assembly objects.", "response": "def add_dependent_assembly(self, manifestVersion=None, noInheritable=False, \n                 noInherit=False, type_=None, name=None, language=None, \n                 processorArchitecture=None, version=None, \n                 publicKeyToken=None, description=None, \n                 requestedExecutionLevel=None, uiAccess=None, \n                 dependentAssemblies=None, files=None, \n                 comInterfaceExternalProxyStubs=None):\n        \"\"\"\n        Shortcut for self.dependentAssemblies.append(Manifest(*args, **kwargs))\n        \"\"\"\n        self.dependentAssemblies.append(Manifest(manifestVersion, \n                                        noInheritable, noInherit, type_, name, \n                                        language, processorArchitecture, \n                                        version, publicKeyToken, description, \n                                        requestedExecutionLevel, uiAccess, \n                                        dependentAssemblies, files, \n                                        comInterfaceExternalProxyStubs))\n        if self.filename:\n            # Enable search for private assembly by assigning bogus filename\n            # (only the directory has to be correct)\n            self.dependentAssemblies[-1].filename = \":\".join((self.filename, \n                                                              name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_file(self, name=\"\", hashalg=\"\", hash=\"\", comClasses=None, \n                 typelibs=None, comInterfaceProxyStubs=None, \n                 windowClasses=None):\n        \"\"\" Shortcut for manifest.files.append \"\"\"\n        self.files.append(File(name, hashalg, hash, comClasses, \n                          typelibs, comInterfaceProxyStubs, windowClasses))", "response": "Shortcut for manifest. files. append"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_files(self, ignore_policies=True):\n        \n        # Shared Assemblies:\n        # http://msdn.microsoft.com/en-us/library/aa375996%28VS.85%29.aspx\n        #\n        # Private Assemblies:\n        # http://msdn.microsoft.com/en-us/library/aa375674%28VS.85%29.aspx\n        #\n        # Assembly Searching Sequence:\n        # http://msdn.microsoft.com/en-us/library/aa374224%28VS.85%29.aspx\n        #\n        # NOTE:\n        # Multilanguage User Interface (MUI) support not yet implemented\n        \n        files = []\n        \n        languages = []\n        if self.language not in (None, \"\", \"*\", \"neutral\"):\n            languages.append(self.getlanguage())\n            if \"-\" in self.language:\n                # language-culture syntax, e.g. en-us\n                # Add only the language part\n                languages.append(self.language.split(\"-\")[0])\n            if self.language not in (\"en-us\", \"en\"):\n                languages.append(\"en-us\")\n            if self.language != \"en\":\n                languages.append(\"en\")\n        languages.append(self.getlanguage(\"*\"))\n        \n        winsxs = os.path.join(compat.getenv(\"SystemRoot\"), \"WinSxS\")\n        if not os.path.isdir(winsxs):\n            logger.warn(\"No such dir %s\", winsxs)\n        manifests = os.path.join(winsxs, \"Manifests\")\n        if not os.path.isdir(manifests):\n            logger.warn(\"No such dir %s\", manifests)\n        if not ignore_policies and self.version:\n            if sys.getwindowsversion() < (6, ):\n                # Windows XP\n                pcfiles = os.path.join(winsxs, \"Policies\")\n                if not os.path.isdir(pcfiles):\n                    logger.warn(\"No such dir %s\", pcfiles)\n            else:\n                # Vista or later\n                pcfiles = manifests\n        \n        for language in languages:\n            version = self.version\n            \n            # Search for publisher configuration\n            if not ignore_policies and version:\n                # Publisher Configuration (aka policy)\n                # A publisher configuration file globally redirects \n                # applications and assemblies having a dependence on one \n                # version of a side-by-side assembly to use another version of \n                # the same assembly. This enables applications and assemblies \n                # to use the updated assembly without having to rebuild all of \n                # the affected applications.\n                # http://msdn.microsoft.com/en-us/library/aa375680%28VS.85%29.aspx\n                #\n                # Under Windows XP and 2003, policies are stored as \n                # <version>.policy files inside \n                # %SystemRoot%\\WinSxS\\Policies\\<name>\n                # Under Vista and later, policies are stored as \n                # <name>.manifest files inside %SystemRoot%\\winsxs\\Manifests\n                redirected = False\n                if os.path.isdir(pcfiles):\n                    logger.info(\"Searching for publisher configuration %s ...\",\n                                self.getpolicyid(True, language=language))\n                    if sys.getwindowsversion() < (6, ):\n                        # Windows XP\n                        policies = os.path.join(pcfiles, \n                                                self.getpolicyid(True,\n                                                                 language=language) + \n                                                \".policy\")\n                    else:\n                        # Vista or later\n                        policies = os.path.join(pcfiles, \n                                                self.getpolicyid(True,\n                                                                 language=language) + \n                                                \".manifest\")\n                    for manifestpth in glob(policies):\n                        if not os.path.isfile(manifestpth):\n                            logger.warn(\"Not a file %s\", manifestpth)\n                            continue\n                        logger.info(\"Found %s\", manifestpth)\n                        try:\n                            policy = ManifestFromXMLFile(manifestpth)\n                        except Exception, exc:\n                            logger.error(\"Could not parse file %s\", manifestpth)\n                            logger.exception(exc)\n                        else:\n                            logger.info(\"Checking publisher policy for \"\n                                        \"binding redirects\")\n                            for assembly in policy.dependentAssemblies:\n                                if (not assembly.same_id(self, True) or\n                                    assembly.optional):\n                                    continue\n                                for redirect in assembly.bindingRedirects:\n                                    if logger.isEnabledFor(logging.INFO):\n                                        old = \"-\".join([\".\".join([str(i) \n                                                                  for i in \n                                                                  part]) \n                                                        for part in \n                                                        redirect[0]])\n                                        new = \".\".join([str(i) \n                                                        for i in\n                                                        redirect[1]])\n                                        logger.info(\"Found redirect for \"\n                                                    \"version(s) %s -> %n\",\n                                                    old, new)\n                                    if (version >= redirect[0][0] and\n                                        version <= redirect[0][-1] and\n                                        version != redirect[1]):\n                                        logger.info(\"Applying redirect \"\n                                                    \"%s -> %s\",\n                                                    \".\".join([str(i) \n                                                              for i in\n                                                              version]),\n                                                    new)\n                                        version = redirect[1]\n                                        redirected = True\n                    if not redirected:\n                        logger.info(\"Publisher configuration not used\")\n            \n            # Search for assemblies according to assembly searching sequence\n            paths = []\n            if os.path.isdir(manifests):\n                # Add winsxs search paths\n                paths.extend(glob(os.path.join(manifests, \n                                               self.getid(language=language, \n                                                          version=version) + \n                                               \"_*.manifest\")))\n            if self.filename:\n                # Add private assembly search paths\n                dirnm = os.path.dirname(self.filename)\n                if language in (LANGUAGE_NEUTRAL_NT5, \n                                LANGUAGE_NEUTRAL_NT6):\n                    for ext in (\".dll\", \".manifest\"):\n                        paths.extend(glob(os.path.join(dirnm, self.name + ext)))\n                        paths.extend(glob(os.path.join(dirnm, self.name, \n                                                       self.name + ext)))\n                else:\n                    for ext in (\".dll\", \".manifest\"):\n                        paths.extend(glob(os.path.join(dirnm, language, \n                                                       self.name + ext)))\n                    for ext in (\".dll\", \".manifest\"):\n                        paths.extend(glob(os.path.join(dirnm, language, \n                                                       self.name, \n                                                       self.name + ext)))\n            logger.info(\"Searching for assembly %s ...\",\n                        self.getid(language=language, version=version))\n            for manifestpth in paths:\n                if not os.path.isfile(manifestpth):\n                    logger.warn(\"Not a file %s\", manifestpth)\n                    continue\n                assemblynm = os.path.basename(\n                    os.path.splitext(manifestpth)[0])\n                try:\n                    if manifestpth.endswith(\".dll\"):\n                        logger.info(\"Found manifest in %s\", manifestpth)\n                        manifest = ManifestFromResFile(manifestpth, [1])\n                    else:\n                        logger.info(\"Found manifest %s\", manifestpth)\n                        manifest = ManifestFromXMLFile(manifestpth)\n                except Exception, exc:\n                    logger.error(\"Could not parse manifest %s\", manifestpth)\n                    logger.exception(exc)\n                else:\n                    if manifestpth.startswith(winsxs):\n                        assemblydir = os.path.join(winsxs, assemblynm)\n                        if not os.path.isdir(assemblydir):\n                            logger.warn(\"No such dir %s\", assemblydir)\n                            logger.warn(\"Assembly incomplete\")\n                            return []\n                    else:\n                        assemblydir = os.path.dirname(manifestpth)\n                    files.append(manifestpth)\n                    for file_ in self.files or manifest.files:\n                        fn = file_.find(assemblydir)\n                        if fn:\n                            files.append(fn)\n                        else:\n                            # If any of our files does not exist,\n                            # the assembly is incomplete\n                            logger.warn(\"Assembly incomplete\")\n                            return []\n                return files\n\n        logger.warn(\"Assembly not found\")\n        return []", "response": "Search shared and private assemblies and return a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getid(self, language=None, version=None):\n        if not self.name:\n            logger.warn(\"Assembly metadata incomplete\")\n            return \"\"\n        id = []\n        if self.processorArchitecture:\n            id.append(self.processorArchitecture)\n        id.append(self.name)\n        if self.publicKeyToken:\n            id.append(self.publicKeyToken)\n        if version or self.version:\n            id.append(\".\".join([str(i) for i in version or self.version]))\n        if not language:\n            language = self.getlanguage()\n        if language:\n            id.append(language)\n        return \"_\".join(id)", "response": "Returns an identification string which uniquely names a manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getlanguage(self, language=None, windowsversion=None):\n        if not language:\n            language = self.language\n        if language in (None, \"\", \"*\", \"neutral\"):\n            return (LANGUAGE_NEUTRAL_NT5,\n                    LANGUAGE_NEUTRAL_NT6)[(windowsversion or \n                                           sys.getwindowsversion()) >= (6, )]\n        return language", "response": "Get and return the manifest s language as string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getpolicyid(self, fuzzy=True, language=None, windowsversion=None):\n        if not self.name:\n            logger.warn(\"Assembly metadata incomplete\")\n            return \"\"\n        id = []\n        if self.processorArchitecture:\n            id.append(self.processorArchitecture)\n        name = []\n        name.append(\"policy\")\n        if self.version:\n            name.append(str(self.version[0]))\n            name.append(str(self.version[1]))\n        name.append(self.name)\n        id.append(\".\".join(name))\n        if self.publicKeyToken:\n            id.append(self.publicKeyToken)\n        if self.version and (windowsversion or sys.getwindowsversion()) >= (6, ):\n            # Vista and later\n            if fuzzy:\n                id.append(\"*\")\n            else:\n                id.append(\".\".join([str(i) for i in self.version]))\n        if not language:\n            language = self.getlanguage(windowsversion=windowsversion)\n        if language:\n            id.append(language)\n        id.append(\"*\")\n        id = \"_\".join(id)\n        if self.version and (windowsversion or sys.getwindowsversion()) < (6, ):\n            # Windows XP\n            if fuzzy:\n                id = os.path.join(id, \"*\")\n            else:\n                id = os.path.join(id, \".\".join([str(i) for i in self.version]))\n        return id", "response": "Returns an identification string which can be used to find a policy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_dom(self, domtree, initialize=True):\n        if domtree.nodeType == Node.DOCUMENT_NODE:\n            rootElement = domtree.documentElement\n        elif domtree.nodeType == Node.ELEMENT_NODE:\n            rootElement = domtree\n        else:\n            raise InvalidManifestError(\"Invalid root element node type \" + \n                                       str(rootElement.nodeType) + \n                                       \" - has to be one of (DOCUMENT_NODE, \"\n                                       \"ELEMENT_NODE)\")\n        allowed_names = (\"assembly\", \"assemblyBinding\", \"configuration\", \n                         \"dependentAssembly\")\n        if rootElement.tagName not in allowed_names:\n            raise InvalidManifestError(\n                \"Invalid root element <%s> - has to be one of <%s>\" % \n                (rootElement.tagName, \">, <\".join(allowed_names)))\n        # logger.info(\"loading manifest metadata from element <%s>\", rootElement.tagName)\n        if rootElement.tagName == \"configuration\":\n            for windows in rootElement.getCEByTN(\"windows\"):\n                for assemblyBinding in windows.getCEByTN(\"assemblyBinding\"):\n                    self.load_dom(assemblyBinding, initialize)\n        else:\n            if initialize:\n                self.__init__()\n            self.manifestType = rootElement.tagName\n            self.manifestVersion = [int(i) \n                                    for i in \n                                    (rootElement.getA(\"manifestVersion\") or \n                                     \"1.0\").split(\".\")]\n            self.noInheritable = bool(rootElement.getFCEByTN(\"noInheritable\"))\n            self.noInherit = bool(rootElement.getFCEByTN(\"noInherit\"))\n            for assemblyIdentity in rootElement.getCEByTN(\"assemblyIdentity\"):\n                self.type = assemblyIdentity.getA(\"type\") or None\n                self.name = assemblyIdentity.getA(\"name\") or None\n                self.language = assemblyIdentity.getA(\"language\") or None\n                self.processorArchitecture = assemblyIdentity.getA(\n                    \"processorArchitecture\") or None\n                version = assemblyIdentity.getA(\"version\")\n                if version:\n                    self.version = [int(i) for i in version.split(\".\")]\n                self.publicKeyToken = assemblyIdentity.getA(\"publicKeyToken\") or None\n            for publisherPolicy in rootElement.getCEByTN(\"publisherPolicy\"):\n                self.applyPublisherPolicy = (publisherPolicy.getA(\"apply\") or \n                                             \"\").lower() == \"yes\"\n            for description in rootElement.getCEByTN(\"description\"):\n                if description.firstChild:\n                    self.description = description.firstChild.wholeText\n            for trustInfo in rootElement.getCEByTN(\"trustInfo\"):\n                for security in trustInfo.getCEByTN(\"security\"):\n                    for reqPriv in security.getCEByTN(\"requestedPrivileges\"):\n                        for reqExeLev in reqPriv.getCEByTN(\"requestedExecutionLevel\"):\n                            self.requestedExecutionLevel = reqExeLev.getA(\"level\")\n                            self.uiAccess = (reqExeLev.getA(\"uiAccess\") or \n                                             \"\").lower() == \"true\"\n            if rootElement.tagName == \"assemblyBinding\":\n                dependencies = [rootElement]\n            else:\n                dependencies = rootElement.getCEByTN(\"dependency\")\n            for dependency in dependencies:\n                for dependentAssembly in dependency.getCEByTN(\n                    \"dependentAssembly\"):\n                    manifest = ManifestFromDOM(dependentAssembly)\n                    if not manifest.name:\n                        # invalid, skip\n                        continue\n                    manifest.optional = (dependency.getA(\"optional\") or \n                                         \"\").lower() == \"yes\"\n                    self.dependentAssemblies.append(manifest)\n                    if self.filename:\n                        # Enable search for private assembly by assigning bogus \n                        # filename (only the directory has to be correct)\n                        self.dependentAssemblies[-1].filename = \":\".join(\n                            (self.filename, manifest.name))\n            for bindingRedirect in rootElement.getCEByTN(\"bindingRedirect\"):\n                oldVersion = [[int(i) for i in part.split(\".\")] \n                              for part in \n                              bindingRedirect.getA(\"oldVersion\").split(\"-\")]\n                newVersion = [int(i) \n                              for i in \n                              bindingRedirect.getA(\"newVersion\").split(\".\")]\n                self.bindingRedirects.append((oldVersion, newVersion))\n            for file_ in rootElement.getCEByTN(\"file\"):\n                self.add_file(name=file_.getA(\"name\"),\n                              hashalg=file_.getA(\"hashalg\"),\n                              hash=file_.getA(\"hash\"))", "response": "Load manifest metadata from DOM tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload manifest from file or file object", "response": "def parse(self, filename_or_file, initialize=True):\n        \"\"\" Load manifest from file or file object \"\"\"\n        if isinstance(filename_or_file, (str, unicode)):\n            filename = filename_or_file\n        else:\n            filename = filename_or_file.name\n        try:\n            domtree = minidom.parse(filename_or_file)\n        except xml.parsers.expat.ExpatError, e:\n            args = [e.args[0]]\n            if isinstance(filename, unicode):\n                filename = filename.encode(sys.getdefaultencoding(), \"replace\")\n            args.insert(0, '\\n  File \"%s\"\\n   ' % filename)\n            raise ManifestXMLParseError(\" \".join([str(arg) for arg in args]))\n        if initialize:\n            self.__init__()\n        self.filename = filename\n        self.load_dom(domtree, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads manifest from XML string.", "response": "def parse_string(self, xmlstr, initialize=True):\n        \"\"\" Load manifest from XML string \"\"\"\n        try:\n            domtree = minidom.parseString(xmlstr)\n        except xml.parsers.expat.ExpatError, e:\n            raise ManifestXMLParseError(e)\n        self.load_dom(domtree, initialize)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef same_id(self, manifest, skip_version_check=False):\n        if skip_version_check:\n            version_check = True\n        else:\n            version_check = self.version == manifest.version\n        return (self.language == manifest.language and\n                self.name == manifest.name and\n                self.processorArchitecture == manifest.processorArchitecture and\n                self.publicKeyToken == manifest.publicKeyToken and\n                self.type == manifest.type and\n                version_check)", "response": "Return a bool indicating if this entry and another entry have the same identitiy."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the manifest as DOM tree.", "response": "def todom(self):\n        \"\"\" Return the manifest as DOM tree \"\"\"\n        doc = Document()\n        docE = doc.cE(self.manifestType)\n        if self.manifestType == \"assemblyBinding\":\n            cfg = doc.cE(\"configuration\")\n            win = doc.cE(\"windows\")\n            win.aChild(docE)\n            cfg.aChild(win)\n            doc.aChild(cfg)\n        else:\n            doc.aChild(docE)\n        if self.manifestType != \"dependentAssembly\":\n            docE.setA(\"xmlns\", \"urn:schemas-microsoft-com:asm.v1\")\n            if self.manifestType != \"assemblyBinding\":\n                docE.setA(\"manifestVersion\", \n                          \".\".join([str(i) for i in self.manifestVersion]))\n        if self.noInheritable:\n            docE.aChild(doc.cE(\"noInheritable\"))\n        if self.noInherit:\n            docE.aChild(doc.cE(\"noInherit\"))\n        aId = doc.cE(\"assemblyIdentity\")\n        if self.type:\n            aId.setAttribute(\"type\", self.type)\n        if self.name:\n            aId.setAttribute(\"name\", self.name)\n        if self.language:\n            aId.setAttribute(\"language\", self.language)\n        if self.processorArchitecture:\n            aId.setAttribute(\"processorArchitecture\", \n                             self.processorArchitecture)\n        if self.version:\n            aId.setAttribute(\"version\", \n                             \".\".join([str(i) for i in self.version]))\n        if self.publicKeyToken:\n            aId.setAttribute(\"publicKeyToken\", self.publicKeyToken)\n        if aId.hasAttributes():\n            docE.aChild(aId)\n        else:\n            aId.unlink()\n        if self.applyPublisherPolicy != None:\n            ppE = doc.cE(\"publisherPolicy\")\n            if self.applyPublisherPolicy:\n                ppE.setA(\"apply\", \"yes\")\n            else:\n                ppE.setA(\"apply\", \"no\")\n            docE.aChild(ppE)\n        if self.description:\n            descE = doc.cE(\"description\")\n            descE.aChild(doc.cT(self.description))\n            docE.aChild(descE)\n        if self.requestedExecutionLevel in (\"asInvoker\", \"highestAvailable\", \n                                            \"requireAdministrator\"):\n            tE = doc.cE(\"trustInfo\")\n            tE.setA(\"xmlns\", \"urn:schemas-microsoft-com:asm.v3\")\n            sE = doc.cE(\"security\")\n            rpE = doc.cE(\"requestedPrivileges\")\n            relE = doc.cE(\"requestedExecutionLevel\")\n            relE.setA(\"level\", self.requestedExecutionLevel)\n            if self.uiAccess:\n                relE.setA(\"uiAccess\", \"true\")\n            else:\n                relE.setA(\"uiAccess\", \"false\")\n            rpE.aChild(relE)\n            sE.aChild(rpE)\n            tE.aChild(sE)\n            docE.aChild(tE)\n        if self.dependentAssemblies:\n            for assembly in self.dependentAssemblies:\n                if self.manifestType != \"assemblyBinding\":\n                    dE = doc.cE(\"dependency\")\n                    if assembly.optional:\n                        dE.setAttribute(\"optional\", \"yes\")\n                daE = doc.cE(\"dependentAssembly\")\n                adom = assembly.todom()\n                for child in adom.documentElement.childNodes:\n                    daE.aChild(child.cloneNode(False))\n                adom.unlink()\n                if self.manifestType != \"assemblyBinding\":\n                    dE.aChild(daE)\n                    docE.aChild(dE)\n                else:\n                    docE.aChild(daE)\n        if self.bindingRedirects:\n            for bindingRedirect in self.bindingRedirects:\n                brE = doc.cE(\"bindingRedirect\")\n                brE.setAttribute(\"oldVersion\", \n                                 \"-\".join([\".\".join([str(i) \n                                                     for i in \n                                                     part]) \n                                           for part in \n                                           bindingRedirect[0]]))\n                brE.setAttribute(\"newVersion\", \n                                 \".\".join([str(i) for i in bindingRedirect[1]]))\n                docE.aChild(brE)\n        if self.files:\n            for file_ in self.files:\n                fE = doc.cE(\"file\")\n                for attr in (\"name\", \"hashalg\", \"hash\"):\n                    val = getattr(file_, attr)\n                    if val:\n                        fE.setA(attr, val)\n                docE.aChild(fE)\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the manifest as pretty - printed XML.", "response": "def toprettyxml(self, indent=\"  \", newl=os.linesep, encoding=\"UTF-8\"):\n        \"\"\" Return the manifest as pretty-printed XML \"\"\"\n        domtree = self.todom()\n        # WARNING: The XML declaration has to follow the order \n        # version-encoding-standalone (standalone being optional), otherwise \n        # if it is embedded in an exe the exe will fail to launch! \n        # ('application configuration incorrect')\n        if sys.version_info >= (2,3):\n            xmlstr = domtree.toprettyxml(indent, newl, encoding)\n        else:\n            xmlstr = domtree.toprettyxml(indent, newl)\n        xmlstr = xmlstr.strip(os.linesep).replace(\n                '<?xml version=\"1.0\" encoding=\"%s\"?>' % encoding, \n                '<?xml version=\"1.0\" encoding=\"%s\" standalone=\"yes\"?>' % \n                encoding)\n        domtree.unlink()\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the manifest as XML", "response": "def toxml(self, encoding=\"UTF-8\"):\n        \"\"\" Return the manifest as XML \"\"\"\n        domtree = self.todom()\n        # WARNING: The XML declaration has to follow the order \n        # version-encoding-standalone (standalone being optional), otherwise \n        # if it is embedded in an exe the exe will fail to launch! \n        # ('application configuration incorrect')\n        xmlstr = domtree.toxml(encoding).replace(\n            '<?xml version=\"1.0\" encoding=\"%s\"?>' % encoding, \n            '<?xml version=\"1.0\" encoding=\"%s\" standalone=\"yes\"?>' % encoding)\n        domtree.unlink()\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates or add manifest resource in dll file dstpath", "response": "def update_resources(self, dstpath, names=None, languages=None):\n        \"\"\" Update or add manifest resource in dll/exe file dstpath \"\"\"\n        UpdateManifestResourcesFromXML(dstpath, self.toprettyxml(), names, \n                                       languages)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the manifest as XML to a file or file object.", "response": "def writeprettyxml(self, filename_or_file=None, indent=\"  \", newl=os.linesep, \n                       encoding=\"UTF-8\"):\n        \"\"\" Write the manifest as XML to a file or file object \"\"\"\n        if not filename_or_file:\n            filename_or_file = self.filename\n        if isinstance(filename_or_file, (str, unicode)):\n            filename_or_file = open(filename_or_file, \"wb\")\n        xmlstr = self.toprettyxml(indent, newl, encoding)\n        filename_or_file.write(xmlstr)\n        filename_or_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef writexml(self, filename_or_file=None, indent=\"  \", newl=os.linesep, \n                 encoding=\"UTF-8\"):\n        \"\"\" Write the manifest as XML to a file or file object \"\"\"\n        if not filename_or_file:\n            filename_or_file = self.filename\n        if isinstance(filename_or_file, (str, unicode)):\n            filename_or_file = open(filename_or_file, \"wb\")\n        xmlstr = self.toxml(indent, newl, encoding)\n        filename_or_file.write(xmlstr)\n        filename_or_file.close()", "response": "Writes the manifest as XML to a file or file object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch_local(in_name, out_name, script_path, poll=None, max_input=None,\n                 files=(), cmdenvs=(), pipe=True, python_cmd='python', remove_tempdir=True,\n                 identity_mapper=False, num_reducers=None,\n                 **kw):\n    \"\"\"A simple local emulation of hadoop\n\n    This doesn't run hadoop and it doesn't support many advanced features, it\n    is intended for simple debugging.  The input/output uses HDFS if an\n    HDFS path is given. This allows for small tasks to be run locally\n    (primarily while debugging). A temporary working directory is used and\n    removed.\n\n    Support\n\n    * Environmental variables\n    * Map-only tasks\n    * Combiner\n    * Files\n    * Pipe (see below)\n    * Display of stdout/stderr\n    * Iterator of KV pairs as input or output (bypassing HDFS)\n\n    :param in_name: Input path (string or list of strings) or Iterator of (key, value).  If it is an iterator then no input is taken from HDFS.\n    :param out_name: Output path or None.  If None then output is not placed on HDFS, it is available through the 'output' key of the return value.\n    :param script_path: Path to the script (e.g., script.py)\n    :param poll: If not None, then only attempt to get a kv pair from kvs if when called, poll returns True.\n    :param max_input: Maximum number of Mapper inputs, None (default) then unlimited.\n    :param files: Extra files (other than the script) (iterator).  NOTE: Hadoop copies the files into working directory\n    :param cmdenvs: Extra cmdenv parameters (iterator)\n    :param pipe: If true (default) then call user code through a pipe to isolate it and stop bugs when printing to stdout.  See project docs.\n    :param python_cmd: The python command to use. The default is \"python\".  Can be used to override the system default python, e.g. python_cmd = \"python2.6\"\n    :param remove_tempdir: If True (default), then rmtree the temporary dir, else print its location.  Useful if you need to see temporary files or how input files are copied.\n    :param identity_mapper: If True, use an identity mapper, regardless of what is in the script.\n    :param num_reducers: If 0, don't run the reducer even if one exists, else obey what is in the script.\n    :rtype: Dictionary with some of the following entries (depending on options)\n    :returns: freeze_cmds: Freeze command(s) ran\n    :returns: frozen_tar_path: HDFS path to frozen file\n    :returns: hadoop_cmds: Hadoopy command(s) ran\n    :returns: process: subprocess.Popen object\n    :returns: output: Iterator of (key, value) pairs\n    :raises: subprocess.CalledProcessError: Hadoop error.\n    :raises: OSError: Hadoop streaming not found.\n    :raises: TypeError: Input types are not correct.\n    :raises: ValueError: Script not found\n    \"\"\"\n    if isinstance(files, (str, unicode)) or isinstance(cmdenvs, (str, unicode)) or ('cmdenvs' in kw and isinstance(kw['cmdenvs'], (str, unicode))):\n        raise TypeError('files and cmdenvs must be iterators of strings and not strings!')\n    logging.info('Local[%s]' % script_path)\n    script_info = hadoopy._runner._parse_info(script_path, python_cmd)\n    if isinstance(in_name, (str, unicode)) or (in_name and isinstance(in_name, (list, tuple)) and isinstance(in_name[0], (str, unicode))):\n        in_kvs = hadoopy.readtb(in_name)\n    else:\n        in_kvs = in_name\n    if 'reduce' in script_info['tasks'] and num_reducers != 0:\n        if identity_mapper:\n            kvs = in_kvs\n        else:\n            kvs = list(LocalTask(script_path, 'map', files, max_input, pipe,\n                                 python_cmd, remove_tempdir).run_task(in_kvs, cmdenvs, poll))\n        if 'combine' in script_info['tasks']:\n            kvs = hadoopy.Test.sort_kv(kvs)\n            kvs = list(LocalTask(script_path, 'combine', files, max_input, pipe,\n                                 python_cmd, remove_tempdir).run_task(kvs, cmdenvs))\n        kvs = hadoopy.Test.sort_kv(kvs)\n        kvs = LocalTask(script_path, 'reduce', files, max_input, pipe,\n                        python_cmd, remove_tempdir).run_task(kvs, cmdenvs)\n    else:\n        if identity_mapper:\n            kvs = in_kvs\n        else:\n            kvs = LocalTask(script_path, 'map', files, max_input, pipe,\n                            python_cmd, remove_tempdir).run_task(in_kvs, cmdenvs, poll)\n    out = {}\n    if out_name is not None:\n        hadoopy.writetb(out_name, kvs)\n        out['output'] = hadoopy.readtb(out_name)\n    else:\n        out['output'] = kvs\n    return out", "response": "This function is used to launch a local task on HDFS."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full path name of a mod.", "response": "def getfullnameof(mod, xtrapath=None):\n    \"\"\"\n    Return the full path name of MOD.\n\n    MOD is the basename of a dll or pyd.\n    XTRAPATH is a path or list of paths to search first.\n    Return the full path name of MOD.\n    Will search the full Windows search path, as well as sys.path\n    \"\"\"\n    # Search sys.path first!\n    epath = sys.path + winutils.get_system_path()\n    if xtrapath is not None:\n        if type(xtrapath) == type(''):\n            epath.insert(0, xtrapath)\n        else:\n            epath = xtrapath + epath\n    for p in epath:\n        npth = os.path.join(p, mod)\n        if os.path.exists(npth):\n            return npth\n        # second try: lower case filename\n        for p in epath:\n            npth = os.path.join(p, mod.lower())\n            if os.path.exists(npth):\n                return npth\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the binary dependencies of the PTH.", "response": "def _getImports_pe(pth):\n    \"\"\"\n    Find the binary dependencies of PTH.\n\n    This implementation walks through the PE header\n    and uses library pefile for that and supports\n    32/64bit Windows\n    \"\"\"\n    import PyInstaller.lib.pefile as pefile\n    pe = pefile.PE(pth)\n    dlls = set()\n    # Some libraries have no other binary dependencies.\n    # e.g. C:\\windows\\system32\\kernel32.dll on Wine\n    for entry in getattr(pe, 'DIRECTORY_ENTRY_IMPORT', []):\n        dlls.add(entry.dll)\n    return dlls"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding LTOC to include all the binary dependencies of all dependent assemblies and imports in the module.", "response": "def Dependencies(lTOC, xtrapath=None, manifest=None):\n    \"\"\"\n    Expand LTOC to include all the closure of binary dependencies.\n\n    LTOC is a logical table of contents, ie, a seq of tuples (name, path).\n    Return LTOC expanded by all the binary dependencies of the entries\n    in LTOC, except those listed in the module global EXCLUDES\n\n    manifest should be a winmanifest.Manifest instance on Windows, so\n    that all dependent assemblies can be added\n    \"\"\"\n    for nm, pth, typ in lTOC:\n        if seen.get(nm.upper(), 0):\n            continue\n        logger.info(\"Analyzing %s\", pth)\n        seen[nm.upper()] = 1\n        if is_win:\n            for ftocnm, fn in selectAssemblies(pth, manifest):\n                lTOC.append((ftocnm, fn, 'BINARY'))\n        for lib, npth in selectImports(pth, xtrapath):\n            if seen.get(lib.upper(), 0) or seen.get(npth.upper(), 0):\n                continue\n            seen[npth.upper()] = 1\n            lTOC.append((lib, npth, 'BINARY'))\n\n    return lTOC"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the default cache location for the current package", "response": "def pkg_resouces_get_default_cache():\n    \"\"\"\n    Determine the default cache location\n\n    This returns the ``PYTHON_EGG_CACHE`` environment variable, if set.\n    Otherwise, on Windows, it returns a 'Python-Eggs' subdirectory of the\n    'Application Data' directory.  On all other systems, it's '~/.python-eggs'.\n    \"\"\"\n    # This function borrowed from setuptools/pkg_resources\n    egg_cache = compat.getenv('PYTHON_EGG_CACHE')\n    if egg_cache is not None:\n        return egg_cache\n\n    if os.name != 'nt':\n        return os.path.expanduser('~/.python-eggs')\n\n    app_data = 'Application Data'   # XXX this may be locale-specific!\n    app_homes = [\n        (('APPDATA',), None),       # best option, should be locale-safe\n        (('USERPROFILE',), app_data),\n        (('HOMEDRIVE', 'HOMEPATH'), app_data),\n        (('HOMEPATH',), app_data),\n        (('HOME',), None),\n        (('WINDIR',), app_data),    # 95/98/ME\n    ]\n\n    for keys, subdir in app_homes:\n        dirname = ''\n        for key in keys:\n            if key in os.environ:\n                dirname = os.path.join(dirname, compat.getenv(key))\n            else:\n                break\n        else:\n            if subdir:\n                dirname = os.path.join(dirname, subdir)\n            return os.path.join(dirname, 'Python-Eggs')\n    else:\n        raise RuntimeError(\n            \"Please set the PYTHON_EGG_CACHE enviroment variable\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getAssemblies(pth):\n    if not os.path.isfile(pth):\n        pth = check_extract_from_egg(pth)[0][0]\n    if pth.lower().endswith(\".manifest\"):\n        return []\n    # check for manifest file\n    manifestnm = pth + \".manifest\"\n    if os.path.isfile(manifestnm):\n        fd = open(manifestnm, \"rb\")\n        res = {RT_MANIFEST: {1: {0: fd.read()}}}\n        fd.close()\n    elif not winresource:\n        # resource access unavailable (needs pywin32)\n        return []\n    else:\n        # check the binary for embedded manifest\n        try:\n            res = GetManifestResources(pth)\n        except winresource.pywintypes.error, exc:\n            if exc.args[0] == winresource.ERROR_BAD_EXE_FORMAT:\n                logger.info('Cannot get manifest resource from non-PE '\n                            'file %s', pth)\n                return []\n            raise\n    rv = []\n    if RT_MANIFEST in res and len(res[RT_MANIFEST]):\n        for name in res[RT_MANIFEST]:\n            for language in res[RT_MANIFEST][name]:\n                # check the manifest for dependent assemblies\n                try:\n                    manifest = Manifest()\n                    manifest.filename = \":\".join([pth, str(RT_MANIFEST),\n                                                  str(name), str(language)])\n                    manifest.parse_string(res[RT_MANIFEST][name][language],\n                                          False)\n                except Exception, exc:\n                    logger.error(\"Can not parse manifest resource %s, %s\"\n                                 \"from %s\", name, language, pth)\n                    logger.exception(exc)\n                else:\n                    if manifest.dependentAssemblies:\n                        logger.debug(\"Dependent assemblies of %s:\", pth)\n                        logger.debug(\", \".join([assembly.getid()\n                                               for assembly in\n                                               manifest.dependentAssemblies]))\n                    rv.extend(manifest.dependentAssemblies)\n    return rv", "response": "Return the dependent assemblies of a binary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef selectAssemblies(pth, manifest=None):\n    rv = []\n    if not os.path.isfile(pth):\n        pth = check_extract_from_egg(pth)[0][0]\n    if manifest:\n        _depNames = set([dep.name for dep in manifest.dependentAssemblies])\n    for assembly in getAssemblies(pth):\n        if seen.get(assembly.getid().upper(), 0):\n            continue\n        if manifest and not assembly.name in _depNames:\n            # Add assembly as dependency to our final output exe's manifest\n            logger.info(\"Adding %s to dependent assemblies \"\n                        \"of final executable\", assembly.name)\n            manifest.dependentAssemblies.append(assembly)\n            _depNames.add(assembly.name)\n        if not dylib.include_library(assembly.name):\n            logger.debug(\"Skipping assembly %s\", assembly.getid())\n            continue\n        if assembly.optional:\n            logger.debug(\"Skipping optional assembly %s\", assembly.getid())\n            continue\n        files = assembly.find_files()\n        if files:\n            seen[assembly.getid().upper()] = 1\n            for fn in files:\n                fname, fext = os.path.splitext(fn)\n                if fext.lower() == \".manifest\":\n                    nm = assembly.name + fext\n                else:\n                    nm = os.path.basename(fn)\n                ftocnm = nm\n                if assembly.language not in (None, \"\", \"*\", \"neutral\"):\n                    ftocnm = os.path.join(assembly.getlanguage(),\n                                          ftocnm)\n                nm, ftocnm, fn = [item.encode(sys.getfilesystemencoding())\n                                  for item in\n                                  (nm,\n                                   ftocnm,\n                                   fn)]\n                if not seen.get(fn.upper(), 0):\n                    logger.debug(\"Adding %s\", ftocnm)\n                    seen[nm.upper()] = 1\n                    seen[fn.upper()] = 1\n                    rv.append((ftocnm, fn))\n                else:\n                    #logger.info(\"skipping %s part of assembly %s dependency of %s\",\n                    #            ftocnm, assembly.name, pth)\n                    pass\n        else:\n            logger.error(\"Assembly %s not found\", assembly.getid())\n    return rv", "response": "Select a set of assemblies that should be included in the current binary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef selectImports(pth, xtrapath=None):\n    rv = []\n    if xtrapath is None:\n        xtrapath = [os.path.dirname(pth)]\n    else:\n        assert isinstance(xtrapath, list)\n        xtrapath = [os.path.dirname(pth)] + xtrapath  # make a copy\n    dlls = getImports(pth)\n    for lib in dlls:\n        if seen.get(lib.upper(), 0):\n            continue\n        if not is_win and not is_cygwin:\n            # all other platforms\n            npth = lib\n            dir, lib = os.path.split(lib)\n        else:\n            # plain win case\n            npth = getfullnameof(lib, xtrapath)\n\n        # now npth is a candidate lib if found\n        # check again for excludes but with regex FIXME: split the list\n        if npth:\n            candidatelib = npth\n        else:\n            candidatelib = lib\n\n        if not dylib.include_library(candidatelib):\n            if (candidatelib.find('libpython') < 0 and\n               candidatelib.find('Python.framework') < 0):\n                # skip libs not containing (libpython or Python.framework)\n                if not seen.get(npth.upper(), 0):\n                    logger.debug(\"Skipping %s dependency of %s\",\n                                 lib, os.path.basename(pth))\n                continue\n            else:\n                pass\n\n        if npth:\n            if not seen.get(npth.upper(), 0):\n                logger.debug(\"Adding %s dependency of %s\",\n                             lib, os.path.basename(pth))\n                rv.append((lib, npth))\n        else:\n            logger.error(\"lib not found: %s dependency of %s\", lib, pth)\n\n    return rv", "response": "Select the dependencies of a binary that should be included."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the binary dependencies of the given path.", "response": "def _getImports_ldd(pth):\n    \"\"\"\n    Find the binary dependencies of PTH.\n\n    This implementation is for ldd platforms (mostly unix).\n    \"\"\"\n    rslt = set()\n    if is_aix:\n        # Match libs of the form 'archive.a(sharedobject.so)'\n        # Will not match the fake lib '/unix'\n        lddPattern = re.compile(r\"\\s+(.*?)(\\(.*\\))\")\n    else:\n        lddPattern = re.compile(r\"\\s+(.*?)\\s+=>\\s+(.*?)\\s+\\(.*\\)\")\n\n    for line in compat.exec_command('ldd', pth).strip().splitlines():\n        m = lddPattern.search(line)\n        if m:\n            if is_aix:\n                lib = m.group(1)\n                name = os.path.basename(lib) + m.group(2)\n            else:\n                name, lib = m.group(1), m.group(2)\n            if name[:10] in ('linux-gate', 'linux-vdso'):\n                # linux-gate is a fake library which does not exist and\n                # should be ignored. See also:\n                # http://www.trilithium.com/johan/2005/08/linux-gate/\n                continue\n\n            if os.path.exists(lib):\n                # Add lib if it is not already found.\n                if lib not in rslt:\n                    rslt.add(lib)\n            else:\n                logger.error('Can not find %s in path %s (needed by %s)',\n                             name, lib, pth)\n    return rslt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the binary dependencies of PTH.", "response": "def _getImports_macholib(pth):\n    \"\"\"\n    Find the binary dependencies of PTH.\n\n    This implementation is for Mac OS X and uses library macholib.\n    \"\"\"\n    from PyInstaller.lib.macholib.MachO import MachO\n    from PyInstaller.lib.macholib.dyld import dyld_find\n    rslt = set()\n    seen = set()  # Libraries read from binary headers.\n    # Walk through mach binary headers.\n    m = MachO(pth)\n    for header in m.headers:\n        for idx, name, lib in header.walkRelocatables():\n            # Sometimes some libraries are present multiple times.\n            if lib not in seen:\n                seen.add(lib)\n\n    ## Try to find files in file system.\n\n    # In cases with @loader_path or @executable_path\n    # try to look in the same directory as the checked binary is.\n    # This seems to work in most cases.\n    exec_path = os.path.abspath(os.path.dirname(pth))\n\n    for lib in seen:\n        if lib.startswith('@loader_path'):\n            # macholib can't handle @loader_path. It has to be\n            # handled the same way as @executable_path.\n            # It is also replaced by 'exec_path'.\n            lib = lib.replace('@loader_path', '@executable_path')\n        try:\n            lib = dyld_find(lib, executable_path=exec_path)\n            rslt.add(lib)\n        except ValueError:\n            logger.error('Can not find path %s (needed by %s)', lib, pth)\n\n    return rslt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nforward to the correct getImports implementation for the platform.", "response": "def getImports(pth):\n    \"\"\"Forwards to the correct getImports implementation for the platform.\n    \"\"\"\n    if not os.path.isfile(pth):\n        pth = check_extract_from_egg(pth)[0][0]\n    if is_win or is_cygwin:\n        if pth.lower().endswith(\".manifest\"):\n            return []\n        try:\n            return _getImports_pe(pth)\n        except Exception, exception:\n            # Assemblies can pull in files which aren't necessarily PE,\n            # but are still needed by the assembly. Any additional binary\n            # dependencies should already have been handled by\n            # selectAssemblies in that case, so just warn, return an empty\n            # list and continue.\n            if logger.isEnabledFor(logging.WARN):\n                 # logg excaption only if level >= warn\n                logger.warn('Can not get binary dependencies for file: %s', pth)\n                logger.exception(exception)\n            return []\n    elif is_darwin:\n        return _getImports_macholib(pth)\n    else:\n        return _getImports_ldd(pth)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findLibrary(name):\n    assert is_unix, \"Current implementation for Unix only (Linux, Solaris, AIX)\"\n\n    lib = None\n\n    # Look in the LD_LIBRARY_PATH\n    lp = compat.getenv('LD_LIBRARY_PATH', '')\n    for path in lp.split(os.pathsep):\n        libs = glob(os.path.join(path, name + '*'))\n        if libs:\n            lib = libs[0]\n            break\n\n    # Look in /etc/ld.so.cache\n    if lib is None:\n        expr = r'/[^\\(\\)\\s]*%s\\.[^\\(\\)\\s]*' % re.escape(name)\n        m = re.search(expr, compat.exec_command('/sbin/ldconfig', '-p'))\n        if m:\n            lib = m.group(0)\n\n    # Look in the known safe paths\n    if lib is None:\n        paths = ['/lib', '/usr/lib']\n        if is_aix:\n            paths.append('/opt/freeware/lib')\n        for path in paths:\n            libs = glob(os.path.join(path, name + '*'))\n            if libs:\n                lib = libs[0]\n                break\n\n    # give up :(\n    if lib is None:\n        return None\n\n    # Resolve the file name into the soname\n    dir, file = os.path.split(lib)\n    return os.path.join(dir, getSoname(lib))", "response": "Find a library in the system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getSoname(filename):\n    cmd = [\"objdump\", \"-p\", \"-j\", \".dynamic\", filename]\n    m = re.search(r'\\s+SONAME\\s+([^\\s]+)', compat.exec_command(*cmd))\n    if m:\n        return m.group(1)", "response": "Return the soname of a library."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _os_bootstrap():\n\n    global _os_stat, _os_getcwd, _os_environ, _os_listdir\n    global _os_path_join, _os_path_dirname, _os_path_basename\n    global _os_sep\n\n    names = sys.builtin_module_names\n\n    join = dirname = environ = listdir = basename = None\n    mindirlen = 0\n    # Only 'posix' and 'nt' os specific modules are supported.\n    # 'dos', 'os2' and 'mac' (MacOS 9) are not supported.\n    if 'posix' in names:\n        from posix import stat, getcwd, environ, listdir\n        sep = _os_sep = '/'\n        mindirlen = 1\n    elif 'nt' in names:\n        from nt import stat, getcwd, environ, listdir\n        sep = _os_sep = '\\\\'\n        mindirlen = 3\n    else:\n        raise ImportError('no os specific module found')\n\n    if join is None:\n        def join(a, b, sep=sep):\n            if a == '':\n                return b\n            lastchar = a[-1:]\n            if lastchar == '/' or lastchar == sep:\n                return a + b\n            return a + sep + b\n\n    if dirname is None:\n        def dirname(a, sep=sep, mindirlen=mindirlen):\n            for i in range(len(a) - 1, -1, -1):\n                c = a[i]\n                if c == '/' or c == sep:\n                    if i < mindirlen:\n                        return a[:i + 1]\n                    return a[:i]\n            return ''\n\n    if basename is None:\n        def basename(p):\n            i = p.rfind(sep)\n            if i == -1:\n                return p\n            else:\n                return p[i + len(sep):]\n\n    def _listdir(dir, cache={}):\n        # since this function is only used by caseOk, it's fine to cache the\n        # results and avoid reading the whole contents of a directory each time\n        # we just want to check the case of a filename.\n        if not dir in cache:\n            cache[dir] = listdir(dir)\n        return cache[dir]\n\n    _os_stat = stat\n    _os_getcwd = getcwd\n    _os_path_join = join\n    _os_path_dirname = dirname\n    _os_environ = environ\n    _os_listdir = _listdir\n    _os_path_basename = basename", "response": "Set up os module replacement functions for use during import bootstrap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan_code_for_ctypes(co, instrs, i):\n\n    def _libFromConst(i):\n        \"\"\"Extracts library name from an expected LOAD_CONST instruction and\n        appends it to local binaries list.\n        \"\"\"\n        op, oparg, conditional, curline = instrs[i]\n        if op == LOAD_CONST:\n            soname = co.co_consts[oparg]\n            b.append(soname)\n\n    b = []\n\n    op, oparg, conditional, curline = instrs[i]\n\n    if op in (LOAD_GLOBAL, LOAD_NAME):\n        name = co.co_names[oparg]\n\n        if name in (\"CDLL\", \"WinDLL\"):\n            # Guesses ctypes imports of this type: CDLL(\"library.so\")\n\n            # LOAD_GLOBAL 0 (CDLL) <--- we \"are\" here right now\n            # LOAD_CONST 1 ('library.so')\n\n            _libFromConst(i+1)\n\n        elif name == \"ctypes\":\n            # Guesses ctypes imports of this type: ctypes.DLL(\"library.so\")\n\n            # LOAD_GLOBAL 0 (ctypes) <--- we \"are\" here right now\n            # LOAD_ATTR 1 (CDLL)\n            # LOAD_CONST 1 ('library.so')\n\n            op2, oparg2, conditional2, curline2 = instrs[i+1]\n            if op2 == LOAD_ATTR:\n                if co.co_names[oparg2] in (\"CDLL\", \"WinDLL\"):\n                    # Fetch next, and finally get the library name\n                    _libFromConst(i+2)\n\n        elif name in (\"cdll\", \"windll\"):\n            # Guesses ctypes imports of these types:\n\n            #  * cdll.library (only valid on Windows)\n\n            #     LOAD_GLOBAL 0 (cdll) <--- we \"are\" here right now\n            #     LOAD_ATTR 1 (library)\n\n            #  * cdll.LoadLibrary(\"library.so\")\n\n            #     LOAD_GLOBAL              0 (cdll) <--- we \"are\" here right now\n            #     LOAD_ATTR                1 (LoadLibrary)\n            #     LOAD_CONST               1 ('library.so')\n\n            op2, oparg2, conditional2, curline2 = instrs[i+1]\n            if op2 == LOAD_ATTR:\n                if co.co_names[oparg2] != \"LoadLibrary\":\n                    # First type\n                    soname = co.co_names[oparg2] + \".dll\"\n                    b.append(soname)\n                else:\n                    # Second type, needs to fetch one more instruction\n                    _libFromConst(i+2)\n\n    # If any of the libraries has been requested with anything different from\n    # the bare filename, drop that entry and warn the user - pyinstaller would\n    # need to patch the compiled pyc file to make it work correctly!\n\n    w = []\n    for bin in list(b):\n        if bin != os.path.basename(bin):\n            b.remove(bin)\n            w.append(\"W: ignoring %s - ctypes imports only supported using bare filenames\" % (bin,))\n\n    return b, w", "response": "Scan code for ctypes dependencies."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve ctypes BINARY entries for modules with their full path.", "response": "def _resolveCtypesImports(cbinaries):\n    \"\"\"Completes ctypes BINARY entries for modules with their full path.\n    \"\"\"\n    if is_unix:\n        envvar = \"LD_LIBRARY_PATH\"\n    elif is_darwin:\n        envvar = \"DYLD_LIBRARY_PATH\"\n    else:\n        envvar = \"PATH\"\n\n    def _setPaths():\n        path = os.pathsep.join(PyInstaller.__pathex__)\n        old = compat.getenv(envvar)\n        if old is not None:\n            path = os.pathsep.join((path, old))\n        compat.setenv(envvar, path)\n        return old\n\n    def _restorePaths(old):\n        if old is None:\n            compat.unsetenv(envvar)\n        else:\n            compat.setenv(envvar, old)\n\n    ret = []\n\n    # Try to locate the shared library on disk. This is done by\n    # executing ctypes.utile.find_library prepending ImportTracker's\n    # local paths to library search paths, then replaces original values.\n    old = _setPaths()\n    for cbin in cbinaries:\n        # Ignore annoying warnings like:\n        # 'W: library kernel32.dll required via ctypes not found'\n        # 'W: library coredll.dll required via ctypes not found'\n        if cbin in ['coredll.dll', 'kernel32.dll']:\n            continue\n        ext = os.path.splitext(cbin)[1]\n        # On Windows, only .dll files can be loaded.\n        if os.name == \"nt\" and ext.lower() in [\".so\", \".dylib\"]:\n            continue\n        cpath = find_library(os.path.splitext(cbin)[0])\n        if is_unix:\n            # CAVEAT: find_library() is not the correct function. Ctype's\n            # documentation says that it is meant to resolve only the filename\n            # (as a *compiler* does) not the full path. Anyway, it works well\n            # enough on Windows and Mac. On Linux, we need to implement\n            # more code to find out the full path.\n            if cpath is None:\n                cpath = cbin\n            # \"man ld.so\" says that we should first search LD_LIBRARY_PATH\n            # and then the ldcache\n            for d in compat.getenv(envvar, '').split(os.pathsep):\n                if os.path.isfile(os.path.join(d, cpath)):\n                    cpath = os.path.join(d, cpath)\n                    break\n            else:\n                text = compat.exec_command(\"/sbin/ldconfig\", \"-p\")\n                for L in text.strip().splitlines():\n                    if cpath in L:\n                        cpath = L.split(\"=>\", 1)[1].strip()\n                        assert os.path.isfile(cpath)\n                        break\n                else:\n                    cpath = None\n        if cpath is None:\n            logger.warn(\"library %s required via ctypes not found\", cbin)\n        else:\n            ret.append((cbin, cpath, \"BINARY\"))\n    _restorePaths(old)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nanalyze a name and return the unique ID for the one entry in the database.", "response": "def analyze_one(self, nm, importernm=None, imptyp=0, level=-1):\n        \"\"\"\n        break the name being imported up so we get:\n        a.b.c -> [a, b, c] ; ..z -> ['', '', z]\n        \"\"\"\n        #print '## analyze_one', nm, importernm, imptyp, level\n        if not nm:\n            nm = importernm\n            importernm = None\n            level = 0\n        nmparts = nm.split('.')\n\n        if level < 0:\n            # behaviour up to Python 2.4 (and default in Python 2.5)\n            # first see if we could be importing a relative name\n            contexts = [None]\n            if importernm:\n                if self.ispackage(importernm):\n                    contexts.insert(0, importernm)\n                else:\n                    pkgnm = \".\".join(importernm.split(\".\")[:-1])\n                    if pkgnm:\n                        contexts.insert(0, pkgnm)\n        elif level == 0:\n            # absolute import, do not try relative\n            importernm = None\n            contexts = [None]\n        elif level > 0:\n            # relative import, do not try absolute\n            if self.ispackage(importernm):\n                level -= 1\n            if level > 0:\n                importernm = \".\".join(importernm.split('.')[:-level])\n            contexts = [importernm, None]\n            importernm = None\n\n        _all = None\n\n        assert contexts\n\n        # so contexts is [pkgnm, None] or just [None]\n        if nmparts[-1] == '*':\n            del nmparts[-1]\n            _all = []\n        nms = []\n        for context in contexts:\n            ctx = context\n            for i, nm in enumerate(nmparts):\n                if ctx:\n                    fqname = ctx + '.' + nm\n                else:\n                    fqname = nm\n                mod = self.modules.get(fqname, UNTRIED)\n                if mod is UNTRIED:\n                    logger.debug('Analyzing %s', fqname)\n                    mod = self.doimport(nm, ctx, fqname)\n                if mod:\n                    nms.append(mod.__name__)\n                    ctx = fqname\n                else:\n                    break\n            else:\n                # no break, point i beyond end\n                i = i + 1\n            if i:\n                break\n        # now nms is the list of modules that went into sys.modules\n        # just as result of the structure of the name being imported\n        # however, each mod has been scanned and that list is in mod.imports\n        if i<len(nmparts):\n            if ctx:\n                if hasattr(self.modules[ctx], nmparts[i]):\n                    return nms\n                if not self.ispackage(ctx):\n                    return nms\n            self.warnings[\"W: no module named %s (%s import by %s)\" % (fqname, imptyps[imptyp], importernm or \"__main__\")] = 1\n            if fqname in self.modules:\n                del self.modules[fqname]\n            return nms\n        if _all is None:\n            return nms\n        bottommod = self.modules[ctx]\n        if bottommod.ispackage():\n            for nm in bottommod._all:\n                if not hasattr(bottommod, nm):\n                    mod = self.doimport(nm, ctx, ctx+'.'+nm)\n                    if mod:\n                        nms.append(mod.__name__)\n                    else:\n                        bottommod.warnings.append(\"W: name %s not found\" % nm)\n        return nms"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the degree distribution of a graph.", "response": "def degree_dist(graph, limits=(0,0), bin_num=10, mode='out'):\n    '''\n    Computes the degree distribution for a graph.\n\n    Returns a list of tuples where the first element of the tuple is the center of the bin\n    representing a range of degrees and the second element of the tuple are the number of nodes\n    with the degree falling in the range.\n\n    Example::\n\n        ....\n    '''\n\n    deg = []\n    if mode == 'inc':\n        get_deg = graph.inc_degree\n    else:\n        get_deg = graph.out_degree\n\n    for node in graph:\n        deg.append( get_deg(node) )\n\n    if not deg:\n        return []\n\n    results = _binning(values=deg, limits=limits, bin_num=bin_num)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbin data that falls between certain limits and bin_num bins.", "response": "def _binning(values, limits=(0,0), bin_num=10):\n    '''\n    Bins data that falls between certain limits, if the limits are (0, 0) the\n    minimum and maximum values are used.\n\n    Returns a list of tuples where the first element of the tuple is the center of the bin\n    and the second element of the tuple are the counts.\n    '''\n    if limits == (0, 0):\n        eps = 1.0/sys.maxint\n        min_val, max_val = min(values) - eps, max(values) + eps\n    else:\n        min_val, max_val = limits\n\n    # get bin size\n    bin_size = (max_val - min_val)/float(bin_num)\n    bins = [0] * (bin_num)\n\n    # will ignore these outliers for now\n    out_points = 0\n    for value in values:\n        try:\n            if (value - min_val) < 0:\n                out_points += 1\n            else:\n                index = int((value - min_val)/float(bin_size))\n                bins[index] += 1\n        except IndexError:\n            out_points += 1\n\n    # make it ready for an x,y plot\n    result = []\n    center = (bin_size/2) + min_val\n    for i, y in enumerate(bins):\n        x = center + bin_size * i\n        result.append( (x,y) )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetResources(hsrc, types=None, names=None, languages=None):\n    if types: types = set(types)\n    if names: names = set(names)\n    if languages: languages = set(languages)\n    res = {}\n    try:\n        # logger.debug(\"Enumerating resource types\")\n        enum_types = win32api.EnumResourceTypes(hsrc)\n        if types and not \"*\" in types:\n            enum_types = filter(lambda type_: \n                                type_ in types, \n                                enum_types)\n        for type_ in enum_types:\n            # logger.debug(\"Enumerating resources of type %s\", type_)\n            enum_names = win32api.EnumResourceNames(hsrc, type_)\n            if names and not \"*\" in names:\n                enum_names = filter(lambda name: \n                                    name in names, \n                                    enum_names)\n            for name in enum_names:\n                # logger.debug(\"Enumerating resources of type %s name %s\", type_, name)\n                enum_languages = win32api.EnumResourceLanguages(hsrc, \n                                                                type_, \n                                                                name)\n                if languages and not \"*\" in languages:\n                    enum_languages = filter(lambda language: \n                                            language in languages, \n                                            enum_languages)\n                for language in enum_languages:\n                    data = win32api.LoadResource(hsrc, type_, name, language)\n                    if not type_ in res:\n                        res[type_] = {}\n                    if not name in res[type_]:\n                        res[type_][name] = {}\n                    res[type_][name][language] = data\n    except pywintypes.error, exception:\n        if exception.args[0] in (ERROR_RESOURCE_DATA_NOT_FOUND,\n                                 ERROR_RESOURCE_TYPE_NOT_FOUND,\n                                 ERROR_RESOURCE_NAME_NOT_FOUND,\n                                 ERROR_RESOURCE_LANG_NOT_FOUND):\n            # logger.info('%s: %s', exception.args[1:3])\n            pass\n        else:\n            raise exception\n    return res", "response": "Get resources from a resource table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget resources from dll or exe file.", "response": "def GetResources(filename, types=None, names=None, languages=None):\n    \"\"\"\n    Get resources from dll/exe file.\n    \n    types = a list of resource types to search for (None = all)\n    names = a list of resource names to search for (None = all)\n    languages = a list of resource languages to search for (None = all)\n    Return a dict of the form {type_: {name: {language: data}}} which \n    might also be empty if no matching resources were found.\n    \n    \"\"\"\n    hsrc = win32api.LoadLibraryEx(filename, 0, LOAD_LIBRARY_AS_DATAFILE)\n    res = _GetResources(hsrc, types, names, languages)\n    win32api.FreeLibrary(hsrc)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates or add resource data in dll file dstpath.", "response": "def UpdateResources(dstpath, data, type_, names=None, languages=None):\n    \"\"\"\n    Update or add resource data in dll/exe file dstpath.\n    \n    type_ = resource type to update\n    names = a list of resource names to update (None = all)\n    languages = a list of resource languages to update (None = all)\n    \n    \"\"\"\n    # look for existing resources\n    res = GetResources(dstpath, [type_], names, languages)\n    # add type_, names and languages not already present in existing resources\n    if not type_ in res and type_ != \"*\":\n        res[type_] = {}\n    if names:\n        for name in names:\n            if not name in res[type_] and name != \"*\":\n                res[type_][name] = []\n                if languages:\n                    for language in languages:\n                        if not language in res[type_][name] and language != \"*\":\n                            res[type_][name].append(language)\n    # add resource to destination, overwriting existing resources\n    hdst = win32api.BeginUpdateResource(dstpath, 0)\n    for type_ in res:\n        for name in res[type_]:\n            for language in res[type_][name]:\n                logger.info(\"Updating resource type %s name %s language %s\",\n                            type_, name, language)\n                win32api.UpdateResource(hdst, type_, name, data, language)\n    win32api.EndUpdateResource(hdst, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating or add resource data from file srcpath in dll / exe file dstpath.", "response": "def UpdateResourcesFromDataFile(dstpath, srcpath, type_, names=None, \n                                languages=None):\n    \"\"\"\n    Update or add resource data from file srcpath in dll/exe file dstpath.\n    \n    type_ = resource type to update\n    names = a list of resource names to update (None = all)\n    languages = a list of resource languages to update (None = all)\n    \n    \"\"\"\n    src = open(srcpath, \"rb\")\n    data = src.read()\n    src.close()\n    UpdateResources(dstpath, data, type_, names, languages)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef UpdateResourcesFromDict(dstpath, res, types=None, names=None, \n                            languages=None):\n    \"\"\"\n    Update or add resources from resource dict in dll/exe file dstpath.\n    \n    types = a list of resource types to update (None = all)\n    names = a list of resource names to update (None = all)\n    languages = a list of resource languages to update (None = all)\n    \n    \"\"\"\n    if types: types = set(types)\n    if names: names = set(names)\n    if langauges: languages = set(languages)\n    for type_ in res:\n        if not types or type_ in types:\n            for name in res[type_]:\n                if not names or name in names:\n                    for language in res[type_][name]:\n                        if not languages or language in languages:\n                            UpdateResources(dstpath, \n                                            res[type_][name][language], \n                                            [type_], [name], [language])", "response": "Update or add resources from a resource dict in dll / exe file dstpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef UpdateResourcesFromResFile(dstpath, srcpath, types=None, names=None, \n                               languages=None):\n    \"\"\"\n    Update or add resources from dll/exe file srcpath in dll/exe file dstpath.\n    \n    types = a list of resource types to update (None = all)\n    names = a list of resource names to update (None = all)\n    languages = a list of resource languages to update (None = all)\n    \n    \"\"\"\n    res = GetResources(srcpath, types, names, languages)\n    UpdateResourcesFromDict(dstpath, res)", "response": "Update or add resources from dll. exe file srcpath in dll. exe file dstpath."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget resources. types = a list of resource types to search for (None = all) names = a list of resource names to search for (None = all) languages = a list of resource languages to search for (None = all) Return a dict of the form {type_: {name: {language: data}}} which might also be empty if no matching resources were found.", "response": "def get_resources(self, types=None, names=None, languages=None):\n        \"\"\"\n        Get resources.\n        \n        types = a list of resource types to search for (None = all)\n        names = a list of resource names to search for (None = all)\n        languages = a list of resource languages to search for (None = all)\n        Return a dict of the form {type_: {name: {language: data}}} which \n        might also be empty if no matching resources were found.\n        \n        \"\"\"\n        return GetResources(self.filename, types, names, languages)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_resources(self, data, type_, names=None, languages=None):\n        UpdateResources(self.filename, data, type_, names, languages)", "response": "Update or add resource data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_resources_from_datafile(self, srcpath, type_, names=None, \n                                       languages=None):\n        \"\"\"\n        Update or add resource data from file srcpath.\n        \n        type_ = resource type to update\n        names = a list of resource names to update (None = all)\n        languages = a list of resource languages to update (None = all)\n        \n        \"\"\"\n        UpdateResourcesFromDataFile(self.filename, srcpath, type_, names, \n                                    languages)", "response": "Update or add resource data from file srcpath."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate or add resources from resource dict.", "response": "def update_resources_from_dict(self, res, types=None, names=None, \n                                   languages=None):\n        \"\"\"\n        Update or add resources from resource dict.\n        \n        types = a list of resource types to update (None = all)\n        names = a list of resource names to update (None = all)\n        languages = a list of resource languages to update (None = all)\n        \n        \"\"\"\n        UpdateResourcesFromDict(self.filename, res, types, names, \n                                languages)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates or add resources from dll / exe file srcpath.", "response": "def update_resources_from_resfile(self, srcpath, types=None, names=None, \n                                      languages=None):\n        \"\"\"\n        Update or add resources from dll/exe file srcpath.\n        \n        types = a list of resource types to update (None = all)\n        names = a list of resource names to update (None = all)\n        languages = a list of resource languages to update (None = all)\n        \n        \"\"\"\n        UpdateResourcesFromResFile(self.filename, srcpath, types, names, \n                                   languages)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates over the subgraph that is entirely reachable by condition starting from the given start node or the object graph root.", "response": "def flatten(self, condition=None, start=None):\n        \"\"\"\n        Iterate over the subgraph that is entirely reachable by condition\n        starting from the given start node or the ObjectGraph root\n        \"\"\"\n        if start is None:\n            start = self\n        start = self.getRawIdent(start)\n        return self.graph.iterdata(start=start, condition=condition)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the object graph in - place by removing all edges to nodes that do not match every filter in the given list.", "response": "def filterStack(self, filters):\n        \"\"\"\n        Filter the ObjectGraph in-place by removing all edges to nodes that\n        do not match every filter in the given filter list\n\n        Returns a tuple containing the number of: (nodes_visited, nodes_removed, nodes_orphaned)\n        \"\"\"\n        visited, removes, orphans = filter_stack(self.graph, self, filters)\n\n        for last_good, tail in orphans:\n            self.graph.add_edge(last_good, tail, edge_data='orphan')\n\n        for node in removes:\n            self.graph.hide_node(node)\n\n        return len(visited)-1, len(removes), len(orphans)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef removeNode(self, node):\n        ident = self.getIdent(node)\n        if ident is not None:\n            self.graph.hide_node(ident)", "response": "Remove the given node from the graph if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all edges from fromnode to tonode", "response": "def removeReference(self, fromnode, tonode):\n        \"\"\"\n        Remove all edges from fromnode to tonode\n        \"\"\"\n        if fromnode is None:\n            fromnode = self\n        fromident = self.getIdent(fromnode)\n        toident = self.getIdent(tonode)\n        if fromident is not None and toident is not None:\n            while True:\n                edge = self.graph.edge_by_node(fromident, toident)\n                if edge is None:\n                    break\n                self.graph.hide_edge(edge)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getIdent(self, node):\n        ident = self.getRawIdent(node)\n        if ident is not None:\n            return ident\n        node = self.findNode(node)\n        if node is None:\n            return None\n        return node.graphident", "response": "Get the graph identifier for a node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getRawIdent(self, node):\n        if node is self:\n            return node\n        ident = getattr(node, 'graphident', None)\n        return ident", "response": "Get the identifier for a node object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findNode(self, node):\n        ident = self.getRawIdent(node)\n        if ident is None:\n            ident = node\n        try:\n            return self.graph.node_data(ident)\n        except KeyError:\n            return None", "response": "Find the node on the graph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a node to the graph referenced by the root node.", "response": "def addNode(self, node):\n        \"\"\"\n        Add a node to the graph referenced by the root\n        \"\"\"\n        self.msg(4, \"addNode\", node)\n\n        try:\n            self.graph.restore_node(node.graphident)\n        except GraphError:\n            self.graph.add_node(node.graphident, node)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createReference(self, fromnode, tonode, edge_data=None):\n        if fromnode is None:\n            fromnode = self\n        fromident, toident = self.getIdent(fromnode), self.getIdent(tonode)\n        if fromident is None or toident is None:\n            return\n        self.msg(4, \"createReference\", fromnode, tonode, edge_data)\n        self.graph.add_edge(fromident, toident, edge_data=edge_data)", "response": "Create a reference from fromnode to tonode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef createNode(self, cls, name, *args, **kw):\n        m = self.findNode(name)\n        if m is None:\n            m = cls(name, *args, **kw)\n            self.addNode(m)\n        return m", "response": "Add a node of type cls to the graph if it does not already exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef msg(self, level, s, *args):\n        if s and level <= self.debug:\n            print \"%s%s %s\" % (\"  \" * self.indent, s, ' '.join(map(repr, args)))", "response": "Print a debug message with the given level"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smallest(self):\n        '''\n        Find smallest item after removing deleted items from front of heap.\n        '''\n        if len(self) == 0:\n            raise IndexError, \"smallest of empty priorityDictionary\"\n        heap = self.__heap\n        while heap[0][1] not in self or self[heap[0][1]] != heap[0][0]:\n            lastItem = heap.pop()\n            insertionPoint = 0\n            while 1:\n                smallChild = 2*insertionPoint+1\n                if smallChild+1 < len(heap) and heap[smallChild] > heap[smallChild+1] :\n                    smallChild += 1\n                if smallChild >= len(heap) or lastItem <= heap[smallChild]:\n                    heap[insertionPoint] = lastItem\n                    break\n                heap[insertionPoint] = heap[smallChild]\n                insertionPoint = smallChild\n        return heap[0][1]", "response": "Find the smallest item in priorityDictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite text into file filename.", "response": "def _write_textfile(filename, text):\n    \"\"\"\n    Write `text` into file `filename`. If the target directory does\n    not exist, create it.\n    \"\"\"\n    dirname = os.path.dirname(filename)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n    outf = open(filename, 'w')\n    outf.write(text)\n    outf.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the Configure options to an option - parser instance or a option group.", "response": "def __add_options(parser):\n    \"\"\"\n    Add the `Configure` options to a option-parser instance or a\n    option group.\n    \"\"\"\n    parser.add_option('--upx-dir', default=None,\n                      help='Directory containing UPX.')\n    parser.add_option('-C', '--configfile',\n                      default=DEFAULT_CONFIGFILE,\n                      dest='configfilename',\n                      help='Name of generated configfile (default: %default)')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a path has zero length.", "response": "def isempty(path):\n    \"\"\"Check if a path has zero length (also true if it's a directory)\n\n    :param path: A string for the path.  This should not have any wildcards.\n    :returns: True if the path has zero length, False otherwise.\n    \"\"\"\n    cmd = \"hadoop fs -test -z %s\"\n    p = _hadoop_fs_command(cmd % (path))\n    p.communicate()\n    rcode = p.returncode\n    return bool(int(rcode == 0))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef abspath(path):\n    global _USER_HOME_DIR\n    # FIXME(brandyn): User's home directory must exist\n    # FIXME(brandyn): Requires something to be in home dir\n    if path[0] == '/':\n        return os.path.abspath(path)\n    if _USER_HOME_DIR is None:\n        try:\n            _USER_HOME_DIR = _get_home_dir()\n        except IOError, e:\n            if not exists('.'):\n                raise IOError(\"Home directory doesn't exist\")\n            raise e\n    return os.path.abspath(os.path.join(_USER_HOME_DIR, path))", "response": "Return the absolute path to a file and canonicalize it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cp(hdfs_src, hdfs_dst):\n    cmd = \"hadoop fs -cp %s %s\" % (hdfs_src, hdfs_dst)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)", "response": "Copy a file\n    to hdfs_dst"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls stat on HDFS Path", "response": "def stat(path, format):\n    \"\"\"Call stat on file\n\n    :param path: HDFS Path\n    :param format: Stat format\n    :returns: Stat output\n    :raises: IOError: If unsuccessful\n    \"\"\"\n    cmd = \"hadoop fs -stat %s %s\" % (format, path)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)\n    return stdout.rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove a file on hdfs", "response": "def mv(hdfs_src, hdfs_dst):\n    \"\"\"Move a file on hdfs\n\n    :param hdfs_src: Source (str)\n    :param hdfs_dst: Destination (str)\n    :raises: IOError: If unsuccessful\n    \"\"\"\n    cmd = \"hadoop fs -mv %s %s\" % (hdfs_src, hdfs_dst)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(local_path, hdfs_path):\n    cmd = \"hadoop fs -put %s %s\" % (local_path, hdfs_path)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)", "response": "Put a file on hdfs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a file from hdfs", "response": "def get(hdfs_path, local_path):\n    \"\"\"Get a file from hdfs\n\n    :param hdfs_path: Destination (str)\n    :param local_path: Source (str)\n    :raises: IOError: If unsuccessful\n    \"\"\"\n    cmd = \"hadoop fs -get %s %s\" % (hdfs_path, local_path)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists files on HDFS.", "response": "def ls(path):\n    \"\"\"List files on HDFS.\n\n    :param path: A string (potentially with wildcards).\n    :rtype: A list of strings representing HDFS paths.\n    :raises: IOError: An error occurred listing the directory (e.g., not available).\n    \"\"\"\n    rcode, stdout, stderr = _checked_hadoop_fs_command('hadoop fs -ls %s' % path)\n    found_line = lambda x: re.search('Found [0-9]+ items$', x)\n    out = [x.split(' ')[-1] for x in stdout.split('\\n')\n           if x and not found_line(x)]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite typedbytes sequence file to HDFS given an iterator of KeyValue pairs", "response": "def writetb(path, kvs, java_mem_mb=256):\n    \"\"\"Write typedbytes sequence file to HDFS given an iterator of KeyValue pairs\n\n    :param path: HDFS path (string)\n    :param kvs: Iterator of (key, value)\n    :param java_mem_mb: Integer of java heap size in MB (default 256)\n    :raises: IOError: An error occurred while saving the data.\n    \"\"\"\n    read_fd, write_fd = os.pipe()\n    read_fp = os.fdopen(read_fd, 'r')\n    hstreaming = _find_hstreaming()\n    cmd = 'hadoop jar %s loadtb %s' % (hstreaming, path)\n    p = _hadoop_fs_command(cmd, stdin=read_fp, java_mem_mb=java_mem_mb)\n    read_fp.close()\n    with hadoopy.TypedBytesFile(write_fd=write_fd) as tb_fp:\n        for kv in kvs:\n            if p.poll() is not None:\n                raise IOError('writetb: Hadoop process quit while we were sending it data.  Hadoop output below...\\nstdout\\n%s\\nstderr\\n%s' % p.communicate())\n            tb_fp.write(kv)\n        tb_fp.flush()\n    p.wait()\n    if p.returncode is not 0:\n        raise IOError('writetb: Hadoop process returned [%d]. Hadoop output below...\\nstderr\\n%s' % (p.returncode, p.stderr.read()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writetb_parts(path, kvs, num_per_file, **kw):\n    out = []\n    part_num = 0\n\n    def _flush(out, part_num):\n        hadoopy.writetb('%s/part-%.5d' % (path, part_num), out, **kw)\n        return [], part_num + 1\n    for kv in kvs:\n        out.append(kv)\n        if len(out) >= num_per_file:\n            out, part_num = _flush(out, part_num)\n    if out:\n        out, part_num = _flush(out, part_num)", "response": "Write typedbytes sequence files to HDFS given an iterator of KeyValue pairs num_per_file and java_mem_mb."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads typedbytes sequence files on HDFS.", "response": "def readtb(paths, num_procs=1, java_mem_mb=256, ignore_logs=True):\n    \"\"\"Read typedbytes sequence files on HDFS (with optional compression).\n\n    By default, ignores files who's names start with an underscore '_' as they\n    are log files.  This allows you to cat a directory that may be a variety of\n    outputs from hadoop (e.g., _SUCCESS, _logs).  This works on directories and\n    files.  The KV pairs may be interleaved between files\n    (they are read in parallel).\n\n    :param paths: HDFS path (str) or paths (iterator)\n    :param num_procs: Number of reading procs to open (default 1)\n    :param java_mem_mb: Integer of java heap size in MB (default 256)\n    :param ignore_logs: If True, ignore all files who's name starts with an underscore.  Defaults to True.\n    :returns: An iterator of key, value pairs.\n    :raises: IOError: An error occurred reading the directory (e.g., not available).\n    \"\"\"\n    import select\n    hstreaming = _find_hstreaming()\n    if isinstance(paths, (str, unicode)):\n        paths = [paths]\n    read_fds = set()\n    procs = {}\n    tb_fps = {}\n\n    def _open_tb(cur_path):\n        cmd = 'hadoop jar %s dumptb %s' % (hstreaming, cur_path)\n        read_fd, write_fd = os.pipe()\n        write_fp = os.fdopen(write_fd, 'w')\n        p = _hadoop_fs_command(cmd, stdout=write_fp, java_mem_mb=java_mem_mb)\n        write_fp.close()\n        read_fds.add(read_fd)\n        procs[read_fd] = p\n        tb_fps[read_fd] = hadoopy.TypedBytesFile(read_fd=read_fd)\n\n    def _path_gen():\n        for root_path in paths:\n            try:\n                all_paths = ls(root_path)\n            except IOError:\n                raise IOError(\"No such file or directory: '%s'\" % root_path)\n            if ignore_logs:\n                # Ignore any files that start with an underscore\n                keep_file = lambda x: os.path.basename(x)[0] != '_'\n                all_paths = filter(keep_file, all_paths)\n            for cur_path in all_paths:\n                yield _open_tb(cur_path)\n    try:\n        path_gen = _path_gen()\n        for x in range(num_procs):\n            try:\n                path_gen.next()\n            except (AttributeError, StopIteration):\n                path_gen = None\n        while read_fds:\n            cur_fds = select.select(read_fds, [], [])[0]\n            for read_fd in cur_fds:\n                p = procs[read_fd]\n                tp_fp = tb_fps[read_fd]\n                try:\n                    yield tp_fp.next()\n                except StopIteration:\n                    p.wait()\n                    del procs[read_fd]\n                    del tb_fps[read_fd]\n                    del p\n                    os.close(read_fd)\n                    read_fds.remove(read_fd)\n                    try:\n                        path_gen.next()\n                    except (AttributeError, StopIteration):\n                        path_gen = None\n    finally:\n        # Cleanup outstanding procs\n        for p in procs.values():\n            p.kill()\n            p.wait()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_node(self, node, node_data=None):\n        #\n        # the nodes will contain tuples that will store incoming edges,\n        # outgoing edges and data\n        #\n        # index 0 -> incoming edges\n        # index 1 -> outgoing edges\n\n        if node in self.hidden_nodes:\n            # Node is present, but hidden\n            return\n\n        if node not in self.nodes:\n            self.nodes[node] = ([], [], node_data)", "response": "Adds a new node to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a directed edge going from head_id to tail_id.", "response": "def add_edge(self, head_id, tail_id, edge_data=1, create_nodes=True):\n        \"\"\"\n        Adds a directed edge going from head_id to tail_id.\n        Arbitrary data can be attached to the edge via edge_data.\n        It may create the nodes if adding edges between nonexisting ones.\n\n        :param head_id: head node\n        :param tail_id: tail node\n        :param edge_data: (optional) data attached to the edge\n        :param create_nodes: (optional) creates the head_id or tail_id node in case they did not exist\n        \"\"\"\n        # shorcut\n        edge = self.next_edge\n\n        # add nodes if on automatic node creation\n        if create_nodes:\n            self.add_node(head_id)\n            self.add_node(tail_id)\n\n        # update the corresponding incoming and outgoing lists in the nodes\n        # index 0 -> incoming edges\n        # index 1 -> outgoing edges\n\n        try:\n            self.nodes[tail_id][0].append(edge)\n            self.nodes[head_id][1].append(edge)\n        except KeyError:\n            raise GraphError('Invalid nodes %s -> %s' % (head_id, tail_id))\n\n        # store edge information\n        self.edges[edge] = (head_id, tail_id, edge_data)\n\n\n        self.next_edge += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hide_edge(self, edge):\n        try:\n            head_id, tail_id, edge_data = self.hidden_edges[edge] = self.edges[edge]\n            self.nodes[tail_id][0].remove(edge)\n            self.nodes[head_id][1].remove(edge)\n            del self.edges[edge]\n        except KeyError:\n            raise GraphError('Invalid edge %s' % edge)", "response": "Hides an edge from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhiding a node from the graph.", "response": "def hide_node(self, node):\n        \"\"\"\n        Hides a node from the graph.  The incoming and outgoing edges of the\n        node will also be hidden.  The node may be unhidden at some later time.\n        \"\"\"\n        try:\n            all_edges = self.all_edges(node)\n            self.hidden_nodes[node] = (self.nodes[node], all_edges)\n            for edge in all_edges:\n                self.hide_edge(edge)\n            del self.nodes[node]\n        except KeyError:\n            raise GraphError('Invalid node %s' % node)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrestoring a previously hidden node into the graph and restores all of its incoming and outgoing edges.", "response": "def restore_node(self, node):\n        \"\"\"\n        Restores a previously hidden node back into the graph and restores\n        all of its incoming and outgoing edges.\n        \"\"\"\n        try:\n            self.nodes[node], all_edges = self.hidden_nodes[node]\n            for edge in all_edges:\n                self.restore_edge(edge)\n            del self.hidden_nodes[node]\n        except KeyError:\n            raise GraphError('Invalid node %s' % node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore_edge(self, edge):\n        try:\n            head_id, tail_id, data = self.hidden_edges[edge]\n            self.nodes[tail_id][0].append(edge)\n            self.nodes[head_id][1].append(edge)\n            self.edges[edge] = head_id, tail_id, data\n            del self.hidden_edges[edge]\n        except KeyError:\n            raise GraphError('Invalid edge %s' % edge)", "response": "Restores a previously hidden edge into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_all_edges(self):\n        for edge in self.hidden_edges.keys():\n            try:\n                self.restore_edge(edge)\n            except GraphError:\n                pass", "response": "Restores all hidden edges."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef edge_by_id(self, edge):\n        try:\n            head, tail, data =  self.edges[edge]\n        except KeyError:\n            head, tail = None, None\n            raise GraphError('Invalid edge %s' % edge)\n\n        return (head, tail)", "response": "Returns the head and tail node ID of an edge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edge_by_node(self, head, tail):\n        for edge in self.out_edges(head):\n            if self.tail(edge) == tail:\n                return edge\n        return None", "response": "Returns the edge that connects the head_id and tail_id nodes\n        Returns None if no edge is found"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn node data outgoing edges incoming edges for node", "response": "def describe_node(self, node):\n        \"\"\"\n        return node, node data, outgoing edges, incoming edges for node\n        \"\"\"\n        incoming, outgoing, data = self.nodes[node]\n        return node, data, outgoing, incoming"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning edge head tail data for edge", "response": "def describe_edge(self, edge):\n        \"\"\"\n        return edge, edge data, head, tail for edge\n        \"\"\"\n        head, tail, data = self.edges[edge]\n        return edge, data, head, tail"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of nodes connected by outgoing edges", "response": "def out_nbrs(self, node):\n        \"\"\"\n        List of nodes connected by outgoing edges\n        \"\"\"\n        l = map(self.tail, self.out_edges(node))\n        #l.sort()\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of nodes connected by incoming edges", "response": "def inc_nbrs(self, node):\n        \"\"\"\n        List of nodes connected by incoming edges\n        \"\"\"\n        l = map(self.head, self.inc_edges(node))\n        #l.sort()\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all nodes connected by incoming and outgoing edges.", "response": "def all_nbrs(self, node):\n        \"\"\"\n        List of nodes connected by incoming and outgoing edges\n        \"\"\"\n        l = dict.fromkeys( self.inc_nbrs(node) + self.out_nbrs(node) )\n        return list(l)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of the outgoing edges of a node", "response": "def out_edges(self, node):\n        \"\"\"\n        Returns a list of the outgoing edges\n        \"\"\"\n        try:\n            return list(self.nodes[node][1])\n        except KeyError:\n            raise GraphError('Invalid node %s' % node)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inc_edges(self, node):\n        try:\n            return list(self.nodes[node][0])\n        except KeyError:\n            raise GraphError('Invalid node %s' % node)\n\n        return None", "response": "Returns a list of incoming edges for a node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_edges(self, node):\n        return set(self.inc_edges(node) + self.out_edges(node))", "response": "Returns a list of incoming and outging edges."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bfs_subgraph(self, start_id, forward=True):\n        if forward:\n            get_bfs  = self.forw_bfs\n            get_nbrs = self.out_nbrs\n        else:\n            get_bfs  = self.back_bfs\n            get_nbrs = self.inc_nbrs\n\n        g = Graph()\n        bfs_list = get_bfs(start_id)\n        for node in bfs_list:\n            g.add_node(node)\n\n        for node in bfs_list:\n            for nbr_id in get_nbrs(node):\n                g.add_edge(node, nbr_id)\n\n        return g", "response": "Private method creates a subgraph of the bfs entries in the bfs tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over nodes in some depth first traversal.", "response": "def iterdfs(self, start, end=None, forward=True):\n        \"\"\"\n        Collecting nodes in some depth first traversal.\n\n        The forward parameter specifies whether it is a forward or backward\n        traversal.\n        \"\"\"\n        visited, stack = set([start]), deque([start])\n\n        if forward:\n            get_edges = self.out_edges\n            get_next = self.tail\n        else:\n            get_edges = self.inc_edges\n            get_next = self.head\n\n        while stack:\n            curr_node = stack.pop()\n            yield curr_node\n            if curr_node == end:\n                break\n            for edge in sorted(get_edges(curr_node)):\n                tail = get_next(edge)\n                if tail not in visited:\n                    visited.add(tail)\n                    stack.append(tail)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iterdata(self, start, end=None, forward=True, condition=None):\n\n        visited, stack = set([start]), deque([start])\n\n        if forward:\n            get_edges = self.out_edges\n            get_next = self.tail\n        else:\n            get_edges = self.inc_edges\n            get_next = self.head\n\n        get_data = self.node_data\n\n        while stack:\n            curr_node = stack.pop()\n            curr_data = get_data(curr_node)\n            if curr_data is not None:\n                if condition is not None and not condition(curr_data):\n                    continue\n                yield curr_data\n            if curr_node == end:\n                break\n            for edge in get_edges(curr_node):\n                tail = get_next(edge)\n                if tail not in visited:\n                    visited.add(tail)\n                    stack.append(tail)", "response": "Iterate over the item data of the node with the given condition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _iterbfs(self, start, end=None, forward=True):\n        queue, visited = deque([(start, 0)]), set([start])\n\n        # the direction of the bfs depends on the edges that are sampled\n        if forward:\n            get_edges = self.out_edges\n            get_next = self.tail\n        else:\n            get_edges = self.inc_edges\n            get_next = self.head\n\n        while queue:\n            curr_node, curr_step = queue.popleft()\n            yield (curr_node, curr_step)\n            if curr_node == end:\n                break\n            for edge in get_edges(curr_node):\n                tail = get_next(edge)\n                if tail not in visited:\n                    visited.add(tail)\n                    queue.append((tail, curr_step + 1))", "response": "Iterate over the breadth - first traversal of the node - level tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forw_bfs(self, start, end=None):\n        return [node for node, step in self._iterbfs(start, end, forward=True)]", "response": "Returns a list of nodes in some forward BFS order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef back_bfs(self, start, end=None):\n        return [node for node, step in self._iterbfs(start, end, forward=False)]", "response": "Returns a list of nodes in some backward BFS order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forw_dfs(self, start, end=None):\n        return list(self.iterdfs(start, end, forward=True))", "response": "Returns a list of nodes in some forward DFS order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef back_dfs(self, start, end=None):\n        return list(self.iterdfs(start, end, forward=False))", "response": "Returns a list of nodes in some backward DFS order."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the graph s every node can be reached from every other node.", "response": "def connected(self):\n        \"\"\"\n        Returns :py:data:`True` if the graph's every node can be reached from every\n        other node.\n        \"\"\"\n        node_list = self.node_list()\n        for node in node_list:\n            bfs_list = self.forw_bfs(node)\n            if len(bfs_list) != len(node_list):\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing and returns the local clustering coefficient of a node.", "response": "def clust_coef(self, node):\n        \"\"\"\n        Computes and returns the local clustering coefficient of node.  The\n        local cluster coefficient is proportion of the actual number of edges between\n        neighbours of node and the maximum number of edges between those neighbours.\n\n        See <http://en.wikipedia.org/wiki/Clustering_coefficient#Local_clustering_coefficient>\n        for a formal definition.\n        \"\"\"\n        num = 0\n        nbr_set = set(self.out_nbrs(node))\n\n        if node in nbr_set:\n            nbr_set.remove(node) # loop defense\n\n        for nbr in nbr_set:\n            sec_set = set(self.out_nbrs(nbr))\n            if nbr in sec_set:\n                sec_set.remove(nbr) # loop defense\n            num += len(nbr_set & sec_set)\n\n        nbr_num = len(nbr_set)\n        if nbr_num:\n            clust_coef = float(num) / (nbr_num * (nbr_num - 1))\n        else:\n            clust_coef = 0.0\n        return clust_coef"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of tuples where each tuple contains the node and the hop.", "response": "def get_hops(self, start, end=None, forward=True):\n        \"\"\"\n        Computes the hop distance to all nodes centered around a specified node.\n\n        First order neighbours are at hop 1, their neigbours are at hop 2 etc.\n        Uses :py:meth:`forw_bfs` or :py:meth:`back_bfs` depending on the value of the forward\n        parameter.  If the distance between all neighbouring nodes is 1 the hop\n        number corresponds to the shortest distance between the nodes.\n\n        :param start: the starting node\n        :param end: ending node (optional). When not specified will search the whole graph.\n        :param forward: directionality parameter (optional). If C{True} (default) it uses L{forw_bfs} otherwise L{back_bfs}.\n        :return: returns a list of tuples where each tuple contains the node and the hop.\n\n        Typical usage::\n\n            >>> print graph.get_hops(1, 8)\n            >>> [(1, 0), (2, 1), (3, 1), (4, 2), (5, 3), (7, 4), (8, 5)]\n            # node 1 is at 0 hops\n            # node 2 is at 1 hop\n            # ...\n            # node 8 is at 5 hops\n        \"\"\"\n        if forward:\n            return list(self._iterbfs(start=start, end=end, forward=True))\n        else:\n            return list(self._iterbfs(start=start, end=end, forward=False))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode the binary string into an in memory list. A binary string is a binary string.", "response": "def frombinary(self, s):\n        \"\"\"Decode the binary string into an in memory list.\n\n            S is a binary string.\"\"\"\n        entrylen = struct.calcsize(self.ENTRYSTRUCT)\n        p = 0\n        while p<len(s):\n            (slen, dpos, dlen, ulen, flag, typcd) = struct.unpack(self.ENTRYSTRUCT,\n                                                        s[p:p+entrylen])\n            nmlen = slen - entrylen\n            p = p + entrylen\n            (nm,) = struct.unpack(`nmlen`+'s', s[p:p+nmlen])\n            p = p + nmlen\n            # version 4\n            # self.data.append((dpos, dlen, ulen, flag, typcd, nm[:-1]))\n            # version 5\n            # nm may have up to 15 bytes of padding\n            pos = nm.find('\\0')\n            if pos < 0:\n                self.data.append((dpos, dlen, ulen, flag, typcd, nm))\n            else:\n                self.data.append((dpos, dlen, ulen, flag, typcd, nm[:pos]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tobinary(self):\n        entrylen = struct.calcsize(self.ENTRYSTRUCT)\n        rslt = []\n        for (dpos, dlen, ulen, flag, typcd, nm) in self.data:\n            nmlen = len(nm) + 1       # add 1 for a '\\0'\n            # version 4\n            # rslt.append(struct.pack(self.ENTRYSTRUCT+`nmlen`+'s',\n            #   nmlen+entrylen, dpos, dlen, ulen, flag, typcd, nm+'\\0'))\n            # version 5\n            #   align to 16 byte boundary so xplatform C can read\n            toclen = nmlen + entrylen\n            if toclen % 16 == 0:\n                pad = '\\0'\n            else:\n                padlen = 16 - (toclen % 16)\n                pad = '\\0'*padlen\n                nmlen = nmlen + padlen\n            rslt.append(struct.pack(self.ENTRYSTRUCT+`nmlen`+'s',\n                            nmlen+entrylen, dpos, dlen, ulen, flag, typcd, nm+pad))\n            # end version 5\n\n        return ''.join(rslt)", "response": "Return a binary string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an entry to the table of contents.", "response": "def add(self, dpos, dlen, ulen, flag, typcd, nm):\n        \"\"\"Add an entry to the table of contents.\n\n           DPOS is data position.\n           DLEN is data length.\n           ULEN is the uncompressed data len.\n           FLAG says if the data is compressed.\n           TYPCD is the \"type\" of the entry (used by the C code)\n           NM is the entry's name.\"\"\"\n        self.data.append((dpos, dlen, ulen, flag, typcd, nm))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find(self, name):\n        for i, nm in enumerate(self.data):\n            if nm[-1] == name:\n                return i\n        return -1", "response": "Return the index of the toc entry with name NAME. Return - 1 for failure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkmagic(self):\n        #magic is at EOF; if we're embedded, we need to figure where that is\n        if self.len:\n            self.lib.seek(self.start+self.len, 0)\n        else:\n            self.lib.seek(0, 2)\n        filelen = self.lib.tell()\n        if self.len:\n            self.lib.seek(self.start+self.len-self.TRLLEN, 0)\n        else:\n            self.lib.seek(-self.TRLLEN, 2)\n        (magic, totallen, tocpos, toclen, pyvers) = struct.unpack(self.TRLSTRUCT,\n                            self.lib.read(self.TRLLEN))\n        if magic != self.MAGIC:\n            raise RuntimeError(\"%s is not a valid %s archive file\"\n                               % (self.path, self.__class__.__name__))\n        self.pkgstart = filelen - totallen\n        if self.len:\n            if totallen != self.len or self.pkgstart != self.start:\n                raise RuntimeError, \"Problem with embedded archive in %s\" % self.path\n        self.tocpos, self.toclen = tocpos, toclen", "response": "Verify that self is a valid CArchive.\n            Magic signature is at end of the archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the table of contents into memory.", "response": "def loadtoc(self):\n        \"\"\"Load the table of contents into memory.\"\"\"\n        self.toc = self.TOCTMPLT()\n        self.lib.seek(self.pkgstart+self.tocpos)\n        tocstr = self.lib.read(self.toclen)\n        self.toc.frombinary(tocstr)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the contents of an entry.", "response": "def extract(self, name):\n        \"\"\"Get the contents of an entry.\n\n           NAME is an entry name.\n           Return the tuple (ispkg, contents).\n           For non-Python resoures, ispkg is meaningless (and 0).\n           Used by the import mechanism.\"\"\"\n        if type(name) == type(''):\n            ndx = self.toc.find(name)\n            if ndx == -1:\n                return None\n        else:\n            ndx = name\n        (dpos, dlen, ulen, flag, typcd, nm) = self.toc.get(ndx)\n        self.lib.seek(self.pkgstart+dpos)\n        rslt = self.lib.read(dlen)\n        if flag == 2:\n            global AES\n            import AES\n            key = rslt[:32]\n            # Note: keep this in sync with bootloader's code\n            rslt = AES.new(key, AES.MODE_CFB, \"\\0\"*AES.block_size).decrypt(rslt[32:])\n        if flag == 1 or flag == 2:\n            rslt = zlib.decompress(rslt)\n        if typcd == 'M':\n            return (1, rslt)\n        return (0, rslt)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contents(self):\n        rslt = []\n        for (dpos, dlen, ulen, flag, typcd, nm) in self.toc:\n            rslt.append(nm)\n        return rslt", "response": "Return the names of the entries in the table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an entry to the CArchive.", "response": "def add(self, entry):\n        \"\"\"Add an ENTRY to the CArchive.\n\n           ENTRY must have:\n             entry[0] is name (under which it will be saved).\n             entry[1] is fullpathname of the file.\n             entry[2] is a flag for it's storage format (0==uncompressed,\n             1==compressed)\n             entry[3] is the entry's type code.\n             Version 5:\n               If the type code is 'o':\n                 entry[0] is the runtime option\n                 eg: v  (meaning verbose imports)\n                     u  (menaing unbuffered)\n                     W arg (warning option arg)\n                     s  (meaning do site.py processing.\"\"\"\n        (nm, pathnm, flag, typcd) = entry[:4]\n        # version 5 - allow type 'o' = runtime option\n        try:\n            if typcd in ('o', 'd'):\n                s = ''\n                flag = 0\n            elif typcd == 's':\n                # If it's a source code file, add \\0 terminator as it will be\n                # executed as-is by the bootloader.\n                s = open(pathnm, 'rU').read()\n                s = s + '\\n\\0'\n            else:\n                s = open(pathnm, 'rb').read()\n        except IOError:\n            print \"Cannot find ('%s', '%s', %s, '%s')\" % (nm, pathnm, flag, typcd)\n            raise\n        ulen = len(s)\n        assert flag in range(3)\n        if flag == 1 or flag == 2:\n            s = zlib.compress(s, self.LEVEL)\n        if flag == 2:\n            global AES\n            import AES, Crypt\n            key = Crypt.gen_random_key(32)\n            # Note: keep this in sync with bootloader's code\n            s = key + AES.new(key, AES.MODE_CFB, \"\\0\"*AES.block_size).encrypt(s)\n        dlen = len(s)\n        where = self.lib.tell()\n        if typcd == 'm':\n            if pathnm.find('.__init__.py') > -1:\n                typcd = 'M'\n        self.toc.add(where, dlen, ulen, flag, typcd, nm)\n        self.lib.write(s)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the table of contents to disk.", "response": "def save_toc(self, tocpos):\n        \"\"\"Save the table of contents to disk.\"\"\"\n        self.tocpos = tocpos\n        tocstr = self.toc.tobinary()\n        self.toclen = len(tocstr)\n        self.lib.write(tocstr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the trailer to disk.", "response": "def save_trailer(self, tocpos):\n        \"\"\"Save the trailer to disk.\n\n           CArchives can be opened from the end - the trailer points\n           back to the start. \"\"\"\n        totallen = tocpos + self.toclen + self.TRLLEN\n        pyvers = sys.version_info[0]*10 + sys.version_info[1]\n        trl = struct.pack(self.TRLSTRUCT, self.MAGIC, totallen,\n                          tocpos, self.toclen, pyvers)\n        self.lib.write(trl)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef openEmbedded(self, name):\n        ndx = self.toc.find(name)\n        if ndx == -1:\n            raise KeyError, \"Member '%s' not found in %s\" % (name, self.path)\n        (dpos, dlen, ulen, flag, typcd, nm) = self.toc.get(ndx)\n        if flag:\n            raise ValueError, \"Cannot open compressed archive %s in place\"\n        return CArchive(self.path, self.pkgstart+dpos, dlen)", "response": "Open a CArchive of name NAME embedded within this CArchive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the whole path to the hadoop streaming jar.", "response": "def _find_hstreaming():\n    \"\"\"Finds the whole path to the hadoop streaming jar.\n\n    If the environmental var HADOOP_HOME is specified, then start the search\n    from there.\n\n    Returns:\n        Full path to the hadoop streaming jar if found, else return an empty\n        string.\n    \"\"\"\n    global WARNED_HADOOP_HOME, HADOOP_STREAMING_PATH_CACHE\n    if HADOOP_STREAMING_PATH_CACHE:\n        return HADOOP_STREAMING_PATH_CACHE\n    try:\n        search_root = os.environ['HADOOP_HOME']\n    except KeyError:\n        search_root = '/'\n    cmd = 'find %s -name hadoop*streaming*.jar' % (search_root)\n    p = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    HADOOP_STREAMING_PATH_CACHE = p.communicate()[0].split('\\n')[0]\n    if search_root == '/' and not WARNED_HADOOP_HOME:\n        WARNED_HADOOP_HOME = True\n        hadoop_home = HADOOP_STREAMING_PATH_CACHE[:HADOOP_STREAMING_PATH_CACHE.rfind('/contrib/')]\n        logging.warn('Set the HADOOP_HOME environmental variable to your hadoop path to improve performance. Put the following [export HADOOP_HOME=\"%s\"] in ~/.bashrc' % hadoop_home)\n    return HADOOP_STREAMING_PATH_CACHE"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _listeq_to_dict(jobconfs):\n    if not isinstance(jobconfs, dict):\n        jobconfs = dict(x.split('=', 1) for x in jobconfs)\n    return dict((str(k), str(v)) for k, v in jobconfs.items())", "response": "Convert iterators of key = val into a dictionary with later values taking priority."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef launch(in_name, out_name, script_path, partitioner=False, files=(), jobconfs=(),\n           cmdenvs=(), libjars=(), input_format=None, output_format=None, copy_script=True,\n           wait=True, hstreaming=None, name=None,\n           use_typedbytes=True, use_seqoutput=True, use_autoinput=True,\n           remove_output=False, add_python=True, config=None, pipe=True,\n           python_cmd=\"python\", num_mappers=None, num_reducers=None,\n           script_dir='', remove_ext=False, check_script=True, make_executable=True,\n           required_files=(), required_cmdenvs=(), **kw):\n    \"\"\"Run Hadoop given the parameters\n\n    :param in_name: Input path (string or list)\n    :param out_name: Output path\n    :param script_path: Path to the script (e.g., script.py)\n    :param partitioner: If True, the partitioner is the value.\n    :param files: Extra files (other than the script) (iterator).  NOTE: Hadoop copies the files into working directory\n    :param jobconfs: Extra jobconf parameters (iterator of strings or dict)\n    :param cmdenvs: Extra cmdenv parameters (iterator of strings or dict)\n    :param libjars: Extra jars to include with the job (iterator of strings).\n    :param input_format: Custom input format (if set overrides use_autoinput)\n    :param output_format: Custom output format (if set overrides use_seqoutput)\n    :param copy_script: If True, the script is added to the files list.\n    :param wait: If True, wait till the process is completed (default True) this is useful if you want to run multiple jobs concurrently by using the 'process' entry in the output.\n    :param hstreaming: The full hadoop streaming path to call.\n    :param name: Set the job name to this (default None, job name is the script name)\n    :param use_typedbytes: If True (default), use typedbytes IO.\n    :param use_seqoutput: True (default), output sequence file. If False, output is text.  If output_format is set, this is not used.\n    :param use_autoinput: If True (default), sets the input format to auto.  If input_format is set, this is not used.\n    :param remove_output: If True, output directory is removed if it exists. (defaults to False)\n    :param add_python: If true, use 'python script_name.py'\n    :param config: If a string, set the hadoop config path\n    :param pipe: If true (default) then call user code through a pipe to isolate it and stop bugs when printing to stdout.  See project docs.\n    :param python_cmd: The python command to use. The default is \"python\". Can be used to override the system default python, e.g. python_cmd = \"python2.6\"\n\n    :param num_mappers: The number of mappers to use (i.e., jobconf mapred.map.tasks=num_mappers).\n    :param num_reducers: The number of reducers to use (i.e., jobconf mapred.reduce.tasks=num_reducers).\n    :param script_dir: Where the script is relative to working dir, will be prefixed to script_path with a / (default '' is current dir)\n    :param remove_ext: If True, remove the script extension (default False)\n    :param check_script: If True, then copy script and .py(c) files to a temporary directory and verify that it can be executed.  This catches the majority of errors related to not included locally imported files. (default True)\n    :param make_executable: If True, ensure that script is executable and has a #! line at the top.\n    :param required_files: Iterator of files that must be specified (verified against the files argument)\n    :param required_cmdenvs: Iterator of cmdenvs that must be specified (verified against the cmdenvs argument)\n\n    :rtype: Dictionary with some of the following entries (depending on options)\n    :returns: freeze_cmds: Freeze command(s) ran\n    :returns: frozen_tar_path: HDFS path to frozen file\n    :returns: hadoop_cmds: Hadoopy command(s) ran\n    :returns: process: subprocess.Popen object\n    :returns: output: Iterator of (key, value) pairs\n    :raises: subprocess.CalledProcessError: Hadoop error.\n    :raises: OSError: Hadoop streaming not found.\n    :raises: TypeError: Input types are not correct.\n    :raises: ValueError: Script not found or check_script failed\n    \"\"\"\n    if isinstance(files, (str, unicode)) or isinstance(jobconfs, (str, unicode)) or isinstance(cmdenvs, (str, unicode)):\n        raise TypeError('files,  jobconfs, and cmdenvs must be iterators of strings and not strings!')\n    jobconfs = _listeq_to_dict(jobconfs)\n    cmdenvs = _listeq_to_dict(cmdenvs)\n    libjars = list(libjars)\n    script_info = _parse_info(script_path, python_cmd)\n    if make_executable and script_path.endswith('.py') and pipe:\n        script_path = hadoopy._runner._make_script_executable(script_path)\n    job_name = os.path.basename(script_path).rsplit('.', 1)[0]\n    script_name = os.path.basename(script_path)\n    # Add required cmdenvs/files, num_reducers from script\n    required_cmdenvs = list(script_info.get('required_cmdenvs', ())) + list(required_cmdenvs)\n    required_files = list(script_info.get('required_files', ())) + list(required_files)\n    _check_requirements(files, cmdenvs, required_files, required_cmdenvs)\n    try:\n        hadoop_cmd = 'hadoop jar ' + hstreaming\n    except TypeError:\n        hadoop_cmd = 'hadoop jar ' + _find_hstreaming()\n    if remove_ext:\n        script_name = script_name.rsplit('.', 1)[0]\n    if add_python:\n        script_name = '%s %s' % (python_cmd, script_name)\n    if script_dir:\n        script_name = ''.join([script_dir, '/', script_name])\n    if 'map' in script_info['tasks']:\n        c = 'pipe map' if pipe else 'map'\n        mapper = ' '.join((script_name, c))\n    if 'reduce' in script_info['tasks']:\n        c = 'pipe reduce' if pipe else 'reduce'\n        reducer = ' '.join((script_name, c))\n    else:\n        reducer = None\n    if 'combine' in script_info['tasks']:\n        c = 'pipe combine' if pipe else 'combine'\n        combiner = ' '.join((script_name, c))\n    else:\n        combiner = None\n    cmd = [hadoop_cmd]\n    # Add libjars\n    if libjars:\n        cmd += ['--libjars', ','.join(libjars)]\n    cmd += ['-output', out_name]\n    # Add inputs\n    if isinstance(in_name, (str, unicode)):\n        in_name = [in_name]\n    for f in in_name:\n        cmd += ['-input', f]\n    # Add mapper/reducer\n    cmd += ['-mapper',\n            '\"%s\"' % (mapper)]\n    if reducer:\n        cmd += ['-reducer',\n                '\"%s\"' % (reducer)]\n    else:\n        cmd += ['-reducer',\n                'NONE']\n    if combiner:\n        cmd += ['-combiner',\n                '\"%s\"' % (combiner)]\n    if partitioner:\n        cmd += ['-partitioner',\n                '\"%s\"' % (partitioner)]\n    if num_mappers is not None:\n        if 'mapred.map.tasks' not in jobconfs:\n            jobconfs['mapred.map.tasks'] = str(num_mappers)\n    if num_reducers is not None:\n        if 'mapred.reduce.tasks' not in jobconfs:\n            jobconfs['mapred.reduce.tasks'] = str(num_reducers)\n    # Add files\n    if copy_script:\n        files = list(files)\n        files.append(script_path)\n    # BUG: CDH3 doesn't copy directories properly this enumerates them\n    new_files = []\n    for f in files:\n        if os.path.isdir(f):\n            new_files += ['%s/%s' % (f, x) for x in os.listdir(f)]\n        else:\n            new_files.append(f)\n    files = new_files\n    del new_files\n    # END BUG\n    if check_script:\n        _check_script(script_path, files, python_cmd)\n    for f in files:\n        cmd += ['-file', f]\n    # Add jobconfs\n    if name is None:\n        jobconfs['mapred.job.name'] = job_name\n    else:\n        jobconfs['mapred.job.name'] = str(name)\n    # Handle additional jobconfs listed in the job itself\n    # these go at the beginning of the list as later jobconfs\n    # override them.  Launch specified confs override job specified ones\n    # as Hadoop takes the last one you provide.\n    jobconfs_all = dict(script_info['jobconfs'])\n    jobconfs_all.update(jobconfs)\n    jobconfs = jobconfs_all\n    for x in jobconfs_all.items():\n        cmd += ['-jobconf', '\"%s=%s\"' % x]\n    # Add cmdenv\n    for x in cmdenvs.items():\n        cmd += ['-cmdenv', '\"%s=%s\"' % x]\n    # Add IO\n    if use_typedbytes:\n        cmd += ['-io', 'typedbytes']\n    # Add Outputformat\n    if output_format is not None:\n        cmd += ['-outputformat', output_format]\n    else:\n        if use_seqoutput:\n            cmd += ['-outputformat',\n                    'org.apache.hadoop.mapred.SequenceFileOutputFormat']\n    # Add InputFormat\n    if input_format is not None:\n        cmd += ['-inputformat', input_format]\n    else:\n        if use_autoinput:\n            cmd += ['-inputformat', 'AutoInputFormat']\n    # Add config\n    if config:\n        cmd += ['--config', config]\n    # Remove output\n    if remove_output and hadoopy.exists(out_name):\n        logging.warn('Removing output directory [%s]' % out_name)\n        hadoopy.rmr(out_name)\n    # Run command and wait till it has completed\n    hadoop_start_time = time.time()\n    logging.info('/\\\\%s%s Output%s/\\\\' % ('-' * 10, 'Hadoop', '-' * 10))\n    logging.info('hadoopy: Running[%s]' % (' '.join(cmd)))\n\n    out = {}\n    out['process'] = process = subprocess.Popen(' '.join(cmd), shell=True,\n                                                stdout=subprocess.PIPE,\n                                                stderr=subprocess.PIPE)\n    # NOTE(brandyn): Read a line from stdout/stderr and log it.  This allows us\n    # to use the logging system instead printing to the console.\n    if wait:\n        def check_fps():\n            fps = select.select([process.stdout, process.stderr], [], [], 1.)[0]\n            for fp in fps:\n                line = fp.readline()\n                if line:  # NOTE(brandyn): Should have at least a newline\n                    line = line[:-1]\n                    if line.find(' ERROR ') != -1:\n                        logging.error(line)\n                    elif line.find(' WARN ') != -1:\n                        logging.warn(line)\n                    else:\n                        logging.info(line)\n        while process.poll() is None:\n            check_fps()\n        check_fps()\n        if process.wait():\n            raise subprocess.CalledProcessError(process.returncode, ' '.join(cmd))\n        logging.info('Hadoop took [%f] seconds' % (time.time() - hadoop_start_time))\n    logging.info('\\\\/%s%s Output%s\\\\/' % ('-' * 10, 'Hadoop', '-' * 10))\n    # NOTE(brandyn): Postpones calling readtb\n\n    def _read_out():\n        for x in hadoopy.readtb(out_name):\n            yield x\n        out['output'] = _read_out()\n    out['hadoop_cmds'] = [' '.join(cmd)]\n    return out", "response": "Launch a Hadoop job given the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfreezing a script and then launches it.", "response": "def launch_frozen(in_name, out_name, script_path, frozen_tar_path=None,\n                  temp_path='_hadoopy_temp', cache=True, check_script=False,\n                  **kw):\n    \"\"\"Freezes a script and then launches it.\n\n    This function will freeze your python program, and place it on HDFS\n    in 'temp_path'.  It will not remove it afterwards as they are typically\n    small, you can easily reuse/debug them, and to avoid any risks involved\n    with removing the file.\n\n    :param in_name: Input path (string or list)\n    :param out_name: Output path\n    :param script_path: Path to the script (e.g., script.py)\n    :param frozen_tar_path: If not None, use this path to a previously frozen archive.  You can get such a path from the return value of this function, it is particularly helpful in iterative programs.\n    :param cache: If True (default) then use previously frozen scripts.  Cache is stored in memory (not persistent).\n    :param temp_path: HDFS path that we can use to store temporary files (default to _hadoopy_temp)\n    :param partitioner: If True, the partitioner is the value.\n    :param wait: If True, wait till the process is completed (default True) this is useful if you want to run multiple jobs concurrently by using the 'process' entry in the output.\n    :param files: Extra files (other than the script) (iterator).  NOTE: Hadoop copies the files into working directory\n    :param jobconfs: Extra jobconf parameters (iterator)\n    :param cmdenvs: Extra cmdenv parameters (iterator)\n    :param hstreaming: The full hadoop streaming path to call.\n    :param name: Set the job name to this (default None, job name is the script name)\n    :param use_typedbytes: If True (default), use typedbytes IO.\n    :param use_seqoutput: True (default), output sequence file. If False, output is text.\n    :param use_autoinput: If True (default), sets the input format to auto.\n    :param config: If a string, set the hadoop config path\n    :param pipe: If true (default) then call user code through a pipe to isolate it and stop bugs when printing to stdout.  See project docs.\n    :param python_cmd: The python command to use. The default is \"python\". Can be used to override the system default python, e.g. python_cmd = \"python2.6\"\n\n    :param num_mappers: The number of mappers to use, i.e. the argument given to 'numMapTasks'. If None, then do not specify this argument to hadoop streaming.\n    :param num_reducers: The number of reducers to use, i.e. the argument given to 'numReduceTasks'. If None, then do not specify this argument to hadoop streaming.\n    :param check_script: If True, then copy script and .py(c) files to a temporary directory and verify that it can be executed.  This catches the majority of errors related to not included locally imported files. The default is False when using launch_frozen as the freeze process packages local files.\n\n    :rtype: Dictionary with some of the following entries (depending on options)\n    :returns: freeze_cmds: Freeze command(s) ran\n    :returns: frozen_tar_path: HDFS path to frozen file\n    :returns: hadoop_cmds: Hadoopy command(s) ran\n    :returns: process: subprocess.Popen object\n    :returns: output: Iterator of (key, value) pairs\n    :raises: subprocess.CalledProcessError: Hadoop error.\n    :raises: OSError: Hadoop streaming not found.\n    :raises: TypeError: Input types are not correct.\n    :raises: ValueError: Script not found\n    \"\"\"\n    if (('files' in kw and isinstance(kw['files'], (str, unicode))) or\n        ('jobconfs' in kw and isinstance(kw['jobconfs'], (str, unicode))) or\n        ('cmdenvs' in kw and isinstance(kw['cmdenvs'], (str, unicode)))):\n        raise TypeError('files,  jobconfs, and cmdenvs must be iterators of strings and not strings!')\n    if 'jobconfs' in kw:\n        kw['jobconfs'] = _listeq_to_dict(kw['jobconfs'])\n    if 'cmdenvs' in kw:\n        kw['cmdenvs'] = _listeq_to_dict(kw['cmdenvs'])\n    cmds = []\n    if not frozen_tar_path:\n        freeze_out = hadoopy.freeze_script(script_path, temp_path=temp_path, cache=cache)\n        frozen_tar_path = freeze_out['frozen_tar_path']\n        cmds += freeze_out['cmds']\n    jobconfs = kw.get('jobconfs', {})\n    jobconfs['mapred.cache.archives'] = '%s#_frozen' % frozen_tar_path\n    jobconfs['mapreduce.job.cache.archives'] = '%s#_frozen' % frozen_tar_path\n    kw['copy_script'] = False\n    kw['add_python'] = False\n    kw['jobconfs'] = jobconfs\n    out = launch(in_name, out_name, script_path,\n                 script_dir='_frozen', remove_ext=True, check_script=check_script,\n                 make_executable=False, **kw)\n    out['freeze_cmds'] = cmds\n    out['frozen_tar_path'] = frozen_tar_path\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef include_library(libname):\n    # For configuration phase we need to have exclude / include lists None\n    # so these checking is skipped and library gets included.\n    if exclude_list:\n        if exclude_list.search(libname) and not include_list.search(libname):\n            # Library is excluded and is not overriden by include list.\n            # It should be then excluded.\n            return False\n        else:\n            # Include library\n            return True\n    else:\n        # By default include library.\n        return True", "response": "Check if a dynamic library should be included with application or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_rabbitmq(self):\n\n        if not self.rabbitmq_key:\n            self.rabbitmq_key = '{}:start_urls'.format(self.name)\n\n        self.server = connection.from_settings(self.crawler.settings)\n        self.crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)\n        self.crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)", "response": "Setup RabbitMQ connection.\n\n            Call this method after spider has set its crawler object.\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides a request to be scheduled.", "response": "def next_request(self):\n        \"\"\" Provides a request to be scheduled.\n        :return: Request object or None\n        \"\"\"\n\n        method_frame, header_frame, url = self.server.basic_get(queue=self.rabbitmq_key)\n\n        # TODO(royce): Remove print\n        print url\n\n        if url:\n            return self.make_requests_from_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscheduling a request if exists.", "response": "def schedule_next_request(self):\n        \"\"\" Schedules a request, if exists.\n\n        :return:\n        \"\"\"\n        req = self.next_request()\n\n        if req:\n            self.crawler.engine.crawl(req, spider=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps image to a tuple.", "response": "def map(self, key, value):\n        \"\"\"\n        Args:\n            key: Image name\n            value: Image as jpeg byte data\n\n        Yields:\n            A tuple in the form of (key, value)\n            key: Image name\n            value: (image, faces) where image is the input value and faces is\n                a list of ((x, y, w, h), n)\n        \"\"\"\n        try:\n            image = self._load_cv_image(value)\n        except:\n            hadoopy.counter('DATA_ERRORS', 'ImageLoadError')\n            return\n        faces = self._detect_faces(image)\n        if faces:\n            yield key, (value, faces)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef retrieve_flags(flag_dict, flag_filter):\n    \n    return [(f[0], f[1]) for f in flag_dict.items() if\n            isinstance(f[0], str) and f[0].startswith(flag_filter)]", "response": "Read the flags from a dictionary and return them in a usable form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the attributes of the object according to the flags provided in flag_field.", "response": "def set_flags(obj, flag_field, flags):\n    \"\"\"Will process the flags and set attributes in the object accordingly.\n\n    The object \"obj\" will gain attributes named after the flags provided in\n    \"flags\" and valued True/False, matching the results of applying each\n    flag value from \"flags\" to flag_field.\n    \"\"\"\n\n    for flag in flags:\n        if flag[1] & flag_field:\n            #setattr(obj, flag[0], True)\n            obj.__dict__[flag[0]] = True\n        else:\n            #setattr(obj, flag[0], False)\n            obj.__dict__[flag[0]] = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd some text to the log.", "response": "def add(self, txt, indent=0):\n        \"\"\"Adds some text, no newline will be appended.\n        \n        The text can be indented with the optional argument 'indent'.\n        \"\"\"\n        \n        if isinstance(txt, unicode):\n            try:\n                txt = str(txt)\n            except UnicodeEncodeError:\n                s = []\n                for c in txt:\n                    try:\n                        s.append(str(c))\n                    except UnicodeEncodeError:\n                        s.append(repr(c))\n                \n                txt = ''.join(s)\n        \n        self.text.append( ' '*indent + txt )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string representation of the structure.", "response": "def dump(self, indentation=0):\n        \"\"\"Returns a string representation of the structure.\"\"\"\n        \n        dump = []\n        \n        dump.append('[%s]' % self.name)\n        \n        # Refer to the __set_format__ method for an explanation\n        # of the following construct.\n        for keys in self.__keys__:\n            for key in keys:\n                \n                val = getattr(self, key)\n                if isinstance(val, int) or isinstance(val, long):\n                    val_str = '0x%-8X' % (val)\n                    if key == 'TimeDateStamp' or key == 'dwTimeStamp':\n                        try:\n                            val_str += ' [%s UTC]' % time.asctime(time.gmtime(val))\n                        except exceptions.ValueError, e:\n                            val_str += ' [INVALID TIME]'\n                else:\n                    val_str = ''.join(filter(lambda c:c != '\\0', str(val)))\n                \n                dump.append('0x%-8X 0x%-3X %-30s %s' % (\n                    self.__field_offsets__[key] + self.__file_offset__, \n                    self.__field_offsets__[key], key+':', val_str))\n        \n        return dump"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains_offset(self, offset):\n        \n        if self.PointerToRawData is None:\n           # bss and other sections containing only uninitialized data must have 0\n           # and do not take space in the file\n           return False\n        return ( adjust_FileAlignment( self.PointerToRawData, \n                self.pe.OPTIONAL_HEADER.FileAlignment ) <= \n                    offset < \n                        adjust_FileAlignment( self.PointerToRawData,\n                            self.pe.OPTIONAL_HEADER.FileAlignment ) + \n                                self.SizeOfRawData )", "response": "Check whether the section contains the file offset provided."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether the section contains the provided RVA.", "response": "def contains_rva(self, rva):\n        \"\"\"Check whether the section contains the address provided.\"\"\"\n        \n        # Check if the SizeOfRawData is realistic. If it's bigger than the size of\n        # the whole PE file minus the start address of the section it could be\n        # either truncated or the SizeOfRawData contain a misleading value.\n        # In either of those cases we take the VirtualSize\n        #\n        if len(self.pe.__data__) - adjust_FileAlignment( self.PointerToRawData,\n            self.pe.OPTIONAL_HEADER.FileAlignment ) < self.SizeOfRawData:\n            # PECOFF documentation v8 says:\n            # VirtualSize: The total size of the section when loaded into memory.\n            # If this value is greater than SizeOfRawData, the section is zero-padded.\n            # This field is valid only for executable images and should be set to zero\n            # for object files.\n            #\n            size = self.Misc_VirtualSize\n        else:\n            size = max(self.SizeOfRawData, self.Misc_VirtualSize)\n        \n        VirtualAddress_adj = adjust_SectionAlignment( self.VirtualAddress, \n            self.pe.OPTIONAL_HEADER.SectionAlignment, self.pe.OPTIONAL_HEADER.FileAlignment )\n        \n        return VirtualAddress_adj <= rva < VirtualAddress_adj + size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the entropy of a chunk of data.", "response": "def entropy_H(self, data):\n        \"\"\"Calculate the entropy of a chunk of data.\"\"\"\n        \n        if len(data) == 0:\n            return 0.0\n        \n        occurences = array.array('L', [0]*256)\n        \n        for x in data:\n            occurences[ord(x)] += 1\n        \n        entropy = 0\n        for x in occurences:\n            if x:\n                p_x = float(x) / len(data)\n                entropy -= p_x*math.log(p_x, 2)\n        \n        return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the rich header from the ntcore.", "response": "def parse_rich_header(self):\n        \"\"\"Parses the rich header\n        see http://www.ntcore.com/files/richsign.htm for more information\n        \n        Structure:\n        00 DanS ^ checksum, checksum, checksum, checksum\n        10 Symbol RVA ^ checksum, Symbol size ^ checksum...\n        ...\n        XX Rich, checksum, 0, 0,...\n        \"\"\"\n        \n        # Rich Header constants\n        #\n        DANS = 0x536E6144 # 'DanS' as dword\n        RICH = 0x68636952 # 'Rich' as dword\n        \n        # Read a block of data\n        #\n        try:\n            data = list(struct.unpack(\"<32I\", self.get_data(0x80, 0x80)))\n        except:\n            # In the cases where there's not enough data to contain the Rich header\n            # we abort its parsing\n            return None\n\n        # the checksum should be present 3 times after the DanS signature\n        #\n        checksum = data[1]\n        if (data[0] ^ checksum != DANS\n            or data[2] != checksum\n            or data[3] != checksum):\n            return None\n\n        result = {\"checksum\": checksum}\n        headervalues = []\n        result [\"values\"] = headervalues\n\n        data = data[4:] \n        for i in xrange(len(data) / 2):\n            \n            # Stop until the Rich footer signature is found\n            #\n            if data[2 * i] == RICH:\n                \n                # it should be followed by the checksum\n                #\n                if data[2 * i + 1] != checksum:\n                    self.__warnings.append('Rich Header corrupted')\n                break\n            \n            # header values come by pairs\n            #\n            headervalues += [data[2 * i] ^ checksum, data[2 * i + 1] ^ checksum]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, filename=None):\n        \n        file_data = list(self.__data__)\n        for structure in self.__structures__:\n            \n            struct_data = list(structure.__pack__())\n            offset = structure.get_file_offset()\n            \n            file_data[offset:offset+len(struct_data)] = struct_data\n        \n        if hasattr(self, 'VS_VERSIONINFO'):\n            if hasattr(self, 'FileInfo'):\n                for entry in self.FileInfo:\n                    if hasattr(entry, 'StringTable'):\n                        for st_entry in entry.StringTable:\n                            for key, entry in st_entry.entries.items():\n                                \n                                offsets = st_entry.entries_offsets[key]\n                                lengths = st_entry.entries_lengths[key]\n                                \n                                if len( entry ) > lengths[1]:\n                                    \n                                    l = list()\n                                    for idx, c in enumerate(entry):\n                                        if ord(c) > 256:\n                                            l.extend( [ chr(ord(c) & 0xff), chr( (ord(c) & 0xff00) >>8) ]  )\n                                        else:\n                                            l.extend( [chr( ord(c) ), '\\0'] )\n                                    \n                                    file_data[\n                                        offsets[1] : offsets[1] + lengths[1]*2 ] = l\n                                \n                                else:\n                                    \n                                    l = list()\n                                    for idx, c in enumerate(entry):\n                                        if ord(c) > 256:\n                                            l.extend( [ chr(ord(c) & 0xff), chr( (ord(c) & 0xff00) >>8) ]  )\n                                        else:\n                                            l.extend( [chr( ord(c) ), '\\0'] )\n\n                                    file_data[\n                                        offsets[1] : offsets[1] + len(entry)*2 ] = l\n                                    \n                                    remainder = lengths[1] - len(entry)\n                                    file_data[\n                                        offsets[1] + len(entry)*2 :\n                                        offsets[1] + lengths[1]*2 ] = [\n                                            u'\\0' ] * remainder*2\n                \n        new_file_data = ''.join( [ chr(ord(c)) for c in file_data] )\n        \n        if filename:\n            f = file(filename, 'wb+')\n            f.write(new_file_data)\n            f.close()\n        else:\n            return new_file_data", "response": "This function writes the contents of the object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the PE file sections.", "response": "def parse_sections(self, offset):\n        \"\"\"Fetch the PE file sections.\n        \n        The sections will be readily available in the \"sections\" attribute.\n        Its attributes will contain all the section information plus \"data\"\n        a buffer containing the section's data.\n        \n        The \"Characteristics\" member will be processed and attributes\n        representing the section characteristics (with the 'IMAGE_SCN_'\n        string trimmed from the constant's names) will be added to the\n        section instance.\n        \n        Refer to the SectionStructure class for additional info.\n        \"\"\"\n        \n        self.sections = []\n        \n        for i in xrange(self.FILE_HEADER.NumberOfSections):\n            section = SectionStructure( self.__IMAGE_SECTION_HEADER_format__, pe=self )\n            if not section:\n                break\n            section_offset = offset + section.sizeof() * i\n            section.set_file_offset(section_offset)\n            section.__unpack__(self.__data__[section_offset : section_offset + section.sizeof()])\n            self.__structures__.append(section)\n            \n            if section.SizeOfRawData > len(self.__data__):\n                self.__warnings.append(\n                    ('Error parsing section %d. ' % i) +\n                    'SizeOfRawData is larger than file.')\n            \n            if adjust_FileAlignment( section.PointerToRawData,\n                self.OPTIONAL_HEADER.FileAlignment ) > len(self.__data__):\n                \n                self.__warnings.append(\n                    ('Error parsing section %d. ' % i) +\n                    'PointerToRawData points beyond the end of the file.')\n            \n            if section.Misc_VirtualSize > 0x10000000:\n                self.__warnings.append(\n                    ('Suspicious value found parsing section %d. ' % i) +\n                    'VirtualSize is extremely large > 256MiB.')\n            \n            if adjust_SectionAlignment( section.VirtualAddress, \n                self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment ) > 0x10000000:\n                self.__warnings.append(\n                    ('Suspicious value found parsing section %d. ' % i) +\n                    'VirtualAddress is beyond 0x10000000.')\n            \n            #\n            # Some packer used a non-aligned PointerToRawData in the sections,\n            # which causes several common tools not to load the section data\n            # properly as they blindly read from the indicated offset.\n            # It seems that Windows will round the offset down to the largest\n            # offset multiple of FileAlignment which is smaller than\n            # PointerToRawData. The following code will do the same.\n            #\n            \n            #alignment = self.OPTIONAL_HEADER.FileAlignment\n            #self.update_section_data(section)\n            \n            if ( self.OPTIONAL_HEADER.FileAlignment != 0 and\n                ( section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment) != 0):\n                self.__warnings.append(\n                    ('Error parsing section %d. ' % i) +\n                    'Suspicious value for FileAlignment in the Optional Header. ' +\n                    'Normally the PointerToRawData entry of the sections\\' structures ' +\n                    'is a multiple of FileAlignment, this might imply the file ' +\n                    'is trying to confuse tools which parse this incorrectly')\n            \n            \n            section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')\n            \n            # Set the section's flags according the the Characteristics member\n            set_flags(section, section.Characteristics, section_flags)\n            \n            if ( section.__dict__.get('IMAGE_SCN_MEM_WRITE', False)  and\n                section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False) ):\n                \n                self.__warnings.append(\n                    ('Suspicious flags set for section %d. ' % i) +\n                    'Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. ' +\n                    'This might indicate a packed executable.')\n            \n            self.sections.append(section)\n        \n        if self.FILE_HEADER.NumberOfSections > 0 and self.sections:\n            return offset + self.sections[0].sizeof()*self.FILE_HEADER.NumberOfSections\n        else:\n            return offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the resources directory.", "response": "def parse_resources_directory(self, rva, size=0, base_rva = None, level = 0, dirs=None):\n        \"\"\"Parse the resources directory.\n        \n        Given the RVA of the resources directory, it will process all\n        its entries.\n        \n        The root will have the corresponding member of its structure,\n        IMAGE_RESOURCE_DIRECTORY plus 'entries', a list of all the\n        entries in the directory.\n        \n        Those entries will have, correspondingly, all the structure's\n        members (IMAGE_RESOURCE_DIRECTORY_ENTRY) and an additional one,\n        \"directory\", pointing to the IMAGE_RESOURCE_DIRECTORY structure\n        representing upper layers of the tree. This one will also have\n        an 'entries' attribute, pointing to the 3rd, and last, level.\n        Another directory with more entries. Those last entries will\n        have a new attribute (both 'leaf' or 'data_entry' can be used to\n        access it). This structure finally points to the resource data.\n        All the members of this structure, IMAGE_RESOURCE_DATA_ENTRY,\n        are available as its attributes.\n        \"\"\"\n        \n        # OC Patch:\n        if dirs is None:\n            dirs = [rva]\n        \n        if base_rva is None:\n            base_rva = rva\n        \n        resources_section = self.get_section_by_rva(rva)\n        \n        try:\n            # If the RVA is invalid all would blow up. Some EXEs seem to be\n            # specially nasty and have an invalid RVA.\n            data = self.get_data(rva, Structure(self.__IMAGE_RESOURCE_DIRECTORY_format__).sizeof() )\n        except PEFormatError, e:\n            self.__warnings.append(\n                'Invalid resources directory. Can\\'t read ' +\n                'directory data at RVA: 0x%x' % rva)\n            return None\n        \n        # Get the resource directory structure, that is, the header\n        # of the table preceding the actual entries\n        #\n        resource_dir = self.__unpack_data__(\n            self.__IMAGE_RESOURCE_DIRECTORY_format__, data,\n            file_offset = self.get_offset_from_rva(rva) )\n        if resource_dir is None:\n            # If can't parse resources directory then silently return.\n            # This directory does not necessarily have to be valid to\n            # still have a valid PE file\n            self.__warnings.append(\n                'Invalid resources directory. Can\\'t parse ' +\n                'directory data at RVA: 0x%x' % rva)\n            return None\n        \n        dir_entries = []\n        \n        # Advance the RVA to the positon immediately following the directory\n        # table header and pointing to the first entry in the table\n        #\n        rva += resource_dir.sizeof()\n        \n        number_of_entries = (\n            resource_dir.NumberOfNamedEntries +\n            resource_dir.NumberOfIdEntries )\n        \n        # Set a hard limit on the maximum resonable number of entries\n        MAX_ALLOWED_ENTRIES = 4096\n        if number_of_entries > MAX_ALLOWED_ENTRIES:\n            self.__warnings.append(\n                'Error parsing the resources directory, '\n                'The directory contains %d entries (>%s)' %\n                (number_of_entries, MAX_ALLOWED_ENTRIES) )\n            return None\n        \n        strings_to_postprocess = list()\n        \n        for idx in xrange(number_of_entries):\n            \n            \n            res = self.parse_resource_entry(rva)\n            if res is None:\n                self.__warnings.append(\n                    'Error parsing the resources directory, '\n                    'Entry %d is invalid, RVA = 0x%x. ' %\n                    (idx, rva) )\n                break\n            \n            entry_name = None\n            entry_id = None\n            \n            # If all named entries have been processed, only Id ones\n            # remain\n            \n            if idx >= resource_dir.NumberOfNamedEntries:\n                entry_id = res.Name\n            else:\n                ustr_offset = base_rva+res.NameOffset\n                try:\n                    #entry_name = self.get_string_u_at_rva(ustr_offset, max_length=16)\n                    entry_name = UnicodeStringWrapperPostProcessor(self, ustr_offset)\n                    strings_to_postprocess.append(entry_name)\n                \n                except PEFormatError, excp:\n                    self.__warnings.append(\n                        'Error parsing the resources directory, '\n                        'attempting to read entry name. '\n                        'Can\\'t read unicode string at offset 0x%x' %\n                        (ustr_offset) )\n                \n            \n            if res.DataIsDirectory:\n                # OC Patch:\n                #\n                # One trick malware can do is to recursively reference\n                # the next directory. This causes hilarity to ensue when\n                # trying to parse everything correctly.\n                # If the original RVA given to this function is equal to\n                # the next one to parse, we assume that it's a trick.\n                # Instead of raising a PEFormatError this would skip some\n                # reasonable data so we just break.\n                #\n                # 9ee4d0a0caf095314fd7041a3e4404dc is the offending sample\n                if (base_rva + res.OffsetToDirectory) in dirs:\n                    \n                    break\n                \n                else:\n                    entry_directory = self.parse_resources_directory(\n                        base_rva+res.OffsetToDirectory,\n                        size-(rva-base_rva), # size\n                        base_rva=base_rva, level = level+1,\n                        dirs=dirs + [base_rva + res.OffsetToDirectory])\n                \n                if not entry_directory:\n                    break\n                    \n                # Ange Albertini's code to process resources' strings\n                #\n                strings = None\n                if entry_id == RESOURCE_TYPE['RT_STRING']:\n                    strings = dict()\n                    for resource_id in entry_directory.entries:\n                        if hasattr(resource_id, 'directory'):\n                            for resource_lang in resource_id.directory.entries:\n                                \n                                resource_strings = dict()\n                                \n                                string_entry_rva = resource_lang.data.struct.OffsetToData\n                                string_entry_size = resource_lang.data.struct.Size\n                                string_entry_id = resource_id.id\n                                \n                                if resource_lang.data.struct.Size is None or resource_id.id is None:\n                                    continue\n                                    \n                                string_entry_data = self.get_data(string_entry_rva, string_entry_size)\n                                parse_strings( string_entry_data, (int(string_entry_id) - 1) * 16, resource_strings )\n                                strings.update(resource_strings)\n                                \n                            resource_id.directory.strings = resource_strings\n                                \n                dir_entries.append(\n                    ResourceDirEntryData(\n                        struct = res,\n                        name = entry_name,\n                        id = entry_id,\n                        directory = entry_directory))\n            \n            else:\n                struct = self.parse_resource_data_entry(\n                    base_rva + res.OffsetToDirectory)\n                \n                if struct:\n                    entry_data = ResourceDataEntryData(\n                        struct = struct,\n                        lang = res.Name & 0x3ff,\n                        sublang = res.Name >> 10 )\n                    \n                    dir_entries.append(\n                        ResourceDirEntryData(\n                            struct = res,\n                            name = entry_name,\n                            id = entry_id,\n                            data = entry_data))\n                \n                else:\n                    break\n                \n            \n            \n            # Check if this entry contains version information\n            #\n            if level == 0 and res.Id == RESOURCE_TYPE['RT_VERSION']:\n                if len(dir_entries)>0:\n                    last_entry = dir_entries[-1]\n                \n                rt_version_struct = None\n                try:\n                    rt_version_struct = last_entry.directory.entries[0].directory.entries[0].data.struct\n                except:\n                    # Maybe a malformed directory structure...?\n                    # Lets ignore it\n                    pass\n                \n                if rt_version_struct is not None:\n                    self.parse_version_information(rt_version_struct)\n            \n            rva += res.sizeof()\n                    \n        \n        string_rvas = [s.get_rva() for s in strings_to_postprocess]\n        string_rvas.sort()\n        \n        for idx, s in enumerate(strings_to_postprocess):\n            s.render_pascal_16()\n            \n        \n        resource_directory_data = ResourceDirData(\n            struct = resource_dir,\n            entries = dir_entries)\n        \n        return resource_directory_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_resource_entry(self, rva):\n\n        try:\n            data = self.get_data( rva, Structure(self.__IMAGE_RESOURCE_DIRECTORY_ENTRY_format__).sizeof() )\n        except PEFormatError, excp:\n            # A warning will be added by the caller if this method returns None\n            return None\n\n        resource = self.__unpack_data__(\n            self.__IMAGE_RESOURCE_DIRECTORY_ENTRY_format__, data,\n            file_offset = self.get_offset_from_rva(rva) )\n        \n        if resource is None:\n            return None\n        \n        #resource.NameIsString = (resource.Name & 0x80000000L) >> 31\n        resource.NameOffset = resource.Name & 0x7FFFFFFFL\n        \n        resource.__pad = resource.Name & 0xFFFF0000L\n        resource.Id = resource.Name & 0x0000FFFFL\n        \n        resource.DataIsDirectory = (resource.OffsetToData & 0x80000000L) >> 31\n        resource.OffsetToDirectory = resource.OffsetToData & 0x7FFFFFFFL\n        \n        return resource", "response": "Parse a resource entry from the resources directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_version_information(self, version_struct):\n        \n        \n        # Retrieve the data for the version info resource\n        #\n        start_offset = self.get_offset_from_rva( version_struct.OffsetToData )\n        raw_data = self.__data__[ start_offset : start_offset+version_struct.Size ]\n        \n        \n        # Map the main structure and the subsequent string\n        #\n        versioninfo_struct = self.__unpack_data__(\n            self.__VS_VERSIONINFO_format__, raw_data,\n            file_offset = start_offset )\n        \n        if versioninfo_struct is None:\n            return\n        \n        ustr_offset = version_struct.OffsetToData + versioninfo_struct.sizeof()\n        try:\n            versioninfo_string = self.get_string_u_at_rva( ustr_offset )\n        except PEFormatError, excp:\n            self.__warnings.append(\n                'Error parsing the version information, ' +\n                'attempting to read VS_VERSION_INFO string. Can\\'t ' +\n                'read unicode string at offset 0x%x' % (\n                ustr_offset ) )\n            \n            versioninfo_string = None\n        \n        # If the structure does not contain the expected name, it's assumed to be invalid\n        #\n        if versioninfo_string != u'VS_VERSION_INFO':\n            \n            self.__warnings.append('Invalid VS_VERSION_INFO block')\n            return\n        \n        \n        # Set the PE object's VS_VERSIONINFO to this one\n        #\n        self.VS_VERSIONINFO = versioninfo_struct\n        \n        # The the Key attribute to point to the unicode string identifying the structure\n        #\n        self.VS_VERSIONINFO.Key = versioninfo_string\n        \n        \n        # Process the fixed version information, get the offset and structure\n        #\n        fixedfileinfo_offset = self.dword_align(\n            versioninfo_struct.sizeof() + 2 * (len(versioninfo_string) + 1),\n            version_struct.OffsetToData)\n        fixedfileinfo_struct = self.__unpack_data__(\n            self.__VS_FIXEDFILEINFO_format__,\n            raw_data[fixedfileinfo_offset:],\n            file_offset = start_offset+fixedfileinfo_offset )\n        \n        if not fixedfileinfo_struct:\n            return\n        \n        # Set the PE object's VS_FIXEDFILEINFO to this one\n        #\n        self.VS_FIXEDFILEINFO = fixedfileinfo_struct\n        \n        \n        # Start parsing all the StringFileInfo and VarFileInfo structures\n        #\n        \n        # Get the first one\n        #\n        stringfileinfo_offset = self.dword_align(\n            fixedfileinfo_offset + fixedfileinfo_struct.sizeof(),\n            version_struct.OffsetToData)\n        original_stringfileinfo_offset = stringfileinfo_offset\n        \n        \n        # Set the PE object's attribute that will contain them all.\n        #\n        self.FileInfo = list()\n        \n        \n        while True:\n            \n            # Process the StringFileInfo/VarFileInfo struct\n            #\n            stringfileinfo_struct = self.__unpack_data__(\n                self.__StringFileInfo_format__,\n                raw_data[stringfileinfo_offset:],\n                file_offset = start_offset+stringfileinfo_offset )\n            \n            if stringfileinfo_struct is None:\n                self.__warnings.append(\n                    'Error parsing StringFileInfo/VarFileInfo struct' )\n                return None\n            \n            # Get the subsequent string defining the structure.\n            #\n            ustr_offset = ( version_struct.OffsetToData +\n                stringfileinfo_offset + versioninfo_struct.sizeof() )\n            try:\n                stringfileinfo_string = self.get_string_u_at_rva( ustr_offset )\n            except PEFormatError, excp:\n                self.__warnings.append(\n                    'Error parsing the version information, ' +\n                    'attempting to read StringFileInfo string. Can\\'t ' +\n                    'read unicode string at offset 0x%x' %  ( ustr_offset ) )\n                break\n            \n            # Set such string as the Key attribute\n            #\n            stringfileinfo_struct.Key = stringfileinfo_string\n            \n            \n            # Append the structure to the PE object's list\n            #\n            self.FileInfo.append(stringfileinfo_struct)\n            \n            \n            # Parse a StringFileInfo entry\n            #\n            if stringfileinfo_string and stringfileinfo_string.startswith(u'StringFileInfo'):\n                \n                if stringfileinfo_struct.Type == 1 and stringfileinfo_struct.ValueLength == 0:\n                    \n                    stringtable_offset = self.dword_align(\n                        stringfileinfo_offset + stringfileinfo_struct.sizeof() +\n                            2*(len(stringfileinfo_string)+1),\n                        version_struct.OffsetToData)\n                    \n                    stringfileinfo_struct.StringTable = list()\n                    \n                    # Process the String Table entries\n                    #\n                    while True:\n                        \n                        stringtable_struct = self.__unpack_data__(\n                            self.__StringTable_format__,\n                            raw_data[stringtable_offset:],\n                            file_offset = start_offset+stringtable_offset )\n                        \n                        if not stringtable_struct:\n                            break\n                        \n                        ustr_offset = ( version_struct.OffsetToData + stringtable_offset +\n                            stringtable_struct.sizeof() )\n                        try:\n                            stringtable_string = self.get_string_u_at_rva( ustr_offset )\n                        except PEFormatError, excp:\n                            self.__warnings.append(\n                                'Error parsing the version information, ' +\n                                'attempting to read StringTable string. Can\\'t ' +\n                                'read unicode string at offset 0x%x' % ( ustr_offset ) )\n                            break\n                        \n                        stringtable_struct.LangID = stringtable_string\n                        stringtable_struct.entries = dict()\n                        stringtable_struct.entries_offsets = dict()\n                        stringtable_struct.entries_lengths = dict()\n                        stringfileinfo_struct.StringTable.append(stringtable_struct)\n                        \n                        entry_offset = self.dword_align(\n                            stringtable_offset + stringtable_struct.sizeof() +\n                                2*(len(stringtable_string)+1),\n                            version_struct.OffsetToData)\n                        \n                        # Process all entries in the string table\n                        #\n                        \n                        while entry_offset < stringtable_offset + stringtable_struct.Length:\n                            \n                            string_struct = self.__unpack_data__(\n                                self.__String_format__, raw_data[entry_offset:],\n                                file_offset = start_offset+entry_offset )\n                            \n                            if not string_struct:\n                                break\n                            \n                            ustr_offset = ( version_struct.OffsetToData + entry_offset +\n                                string_struct.sizeof() )\n                            try:\n                                key = self.get_string_u_at_rva( ustr_offset )\n                                key_offset = self.get_offset_from_rva( ustr_offset )\n                            except PEFormatError, excp:\n                                self.__warnings.append(\n                                    'Error parsing the version information, ' +\n                                    'attempting to read StringTable Key string. Can\\'t ' +\n                                    'read unicode string at offset 0x%x' % ( ustr_offset ) )\n                                break\n                            \n                            value_offset = self.dword_align(\n                                2*(len(key)+1) + entry_offset + string_struct.sizeof(),\n                                version_struct.OffsetToData)\n                            \n                            ustr_offset = version_struct.OffsetToData + value_offset\n                            try:\n                                value = self.get_string_u_at_rva( ustr_offset,\n                                    max_length = string_struct.ValueLength )\n                                value_offset = self.get_offset_from_rva( ustr_offset )\n                            except PEFormatError, excp:\n                                self.__warnings.append(\n                                    'Error parsing the version information, ' +\n                                    'attempting to read StringTable Value string. ' +\n                                    'Can\\'t read unicode string at offset 0x%x' % (\n                                    ustr_offset ) )\n                                break\n                            \n                            if string_struct.Length == 0:\n                                entry_offset = stringtable_offset + stringtable_struct.Length\n                            else:\n                                entry_offset = self.dword_align(\n                                    string_struct.Length+entry_offset, version_struct.OffsetToData)\n                            \n                            key_as_char = []\n                            for c in key:\n                                if ord(c)>128:\n                                    key_as_char.append('\\\\x%02x' %ord(c))\n                                else:\n                                    key_as_char.append(c)\n                            \n                            key_as_char = ''.join(key_as_char)\n                            \n                            setattr(stringtable_struct, key_as_char, value)\n                            stringtable_struct.entries[key] = value\n                            stringtable_struct.entries_offsets[key] = (key_offset, value_offset)\n                            stringtable_struct.entries_lengths[key] = (len(key), len(value))\n                        \n                        \n                        new_stringtable_offset = self.dword_align(\n                            stringtable_struct.Length + stringtable_offset,\n                            version_struct.OffsetToData)\n                            \n                        # check if the entry is crafted in a way that would lead to an infinite\n                        # loop and break if so\n                        #\n                        if new_stringtable_offset == stringtable_offset:\n                            break\n                        stringtable_offset = new_stringtable_offset\n                            \n                        if stringtable_offset >= stringfileinfo_struct.Length:\n                            break\n            \n            # Parse a VarFileInfo entry\n            #\n            elif stringfileinfo_string and stringfileinfo_string.startswith( u'VarFileInfo' ):\n                \n                varfileinfo_struct = stringfileinfo_struct\n                varfileinfo_struct.name = 'VarFileInfo'\n                \n                if varfileinfo_struct.Type == 1 and varfileinfo_struct.ValueLength == 0:\n                    \n                    var_offset = self.dword_align(\n                        stringfileinfo_offset + varfileinfo_struct.sizeof() +\n                            2*(len(stringfileinfo_string)+1),\n                        version_struct.OffsetToData)\n                    \n                    varfileinfo_struct.Var = list()\n                    \n                    # Process all entries\n                    #\n                    \n                    while True:\n                        var_struct = self.__unpack_data__(\n                            self.__Var_format__,\n                            raw_data[var_offset:],\n                            file_offset = start_offset+var_offset )\n                        \n                        if not var_struct:\n                            break\n                        \n                        ustr_offset = ( version_struct.OffsetToData + var_offset +\n                            var_struct.sizeof() )\n                        try:\n                            var_string = self.get_string_u_at_rva( ustr_offset )\n                        except PEFormatError, excp:\n                            self.__warnings.append(\n                                'Error parsing the version information, ' +\n                                'attempting to read VarFileInfo Var string. ' +\n                                'Can\\'t read unicode string at offset 0x%x' % (ustr_offset))\n                            break\n                        \n                        \n                        varfileinfo_struct.Var.append(var_struct)\n                        \n                        varword_offset = self.dword_align(\n                            2*(len(var_string)+1) + var_offset + var_struct.sizeof(),\n                            version_struct.OffsetToData)\n                        orig_varword_offset = varword_offset\n                        \n                        while varword_offset < orig_varword_offset + var_struct.ValueLength:\n                            word1 = self.get_word_from_data(\n                                raw_data[varword_offset:varword_offset+2], 0)\n                            word2 = self.get_word_from_data(\n                                raw_data[varword_offset+2:varword_offset+4], 0)\n                            varword_offset += 4\n                            \n                            if isinstance(word1, (int, long)) and isinstance(word1, (int, long)):\n                                var_struct.entry = {var_string: '0x%04x 0x%04x' % (word1, word2)}\n                        \n                        var_offset = self.dword_align(\n                            var_offset+var_struct.Length, version_struct.OffsetToData)\n                        \n                        if var_offset <= var_offset+var_struct.Length:\n                            break\n                \n            \n            # Increment and align the offset\n            #\n            stringfileinfo_offset = self.dword_align(\n                stringfileinfo_struct.Length+stringfileinfo_offset,\n                version_struct.OffsetToData)\n            \n            # Check if all the StringFileInfo and VarFileInfo items have been processed\n            #\n            if stringfileinfo_struct.Length == 0 or stringfileinfo_offset >= versioninfo_struct.Length:\n                break", "response": "Parse the version information structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the export directory.", "response": "def parse_export_directory(self, rva, size):\n        \"\"\"Parse the export directory.\n        \n        Given the RVA of the export directory, it will process all\n        its entries.\n        \n        The exports will be made available through a list \"exports\"\n        containing a tuple with the following elements:\n            \n            (ordinal, symbol_address, symbol_name)\n        \n        And also through a dictionary \"exports_by_ordinal\" whose keys\n        will be the ordinals and the values tuples of the from:\n            \n            (symbol_address, symbol_name)\n        \n        The symbol addresses are relative, not absolute.\n        \"\"\"\n        \n        try:\n            export_dir =  self.__unpack_data__(\n                self.__IMAGE_EXPORT_DIRECTORY_format__,\n                self.get_data( rva, Structure(self.__IMAGE_EXPORT_DIRECTORY_format__).sizeof() ),\n                file_offset = self.get_offset_from_rva(rva) )\n        except PEFormatError:\n            self.__warnings.append(\n                'Error parsing export directory at RVA: 0x%x' % ( rva ) )\n            return\n        \n        if not export_dir:\n            return\n        \n        # We keep track of the bytes left in the file and use it to set a upper\n        # bound in the number of items that can be read from the different\n        # arrays\n        #\n        def length_until_eof(rva):\n            return len(self.__data__) - self.get_offset_from_rva(rva)\n        \n        try:\n            address_of_names = self.get_data(\n                export_dir.AddressOfNames, min( length_until_eof(export_dir.AddressOfNames), export_dir.NumberOfNames*4))\n            address_of_name_ordinals = self.get_data(\n                export_dir.AddressOfNameOrdinals, min( length_until_eof(export_dir.AddressOfNameOrdinals), export_dir.NumberOfNames*4) )\n            address_of_functions = self.get_data(\n                export_dir.AddressOfFunctions, min( length_until_eof(export_dir.AddressOfFunctions), export_dir.NumberOfFunctions*4) )\n        except PEFormatError:\n            self.__warnings.append(\n                'Error parsing export directory at RVA: 0x%x' % ( rva ) )\n            return\n        \n        exports = []\n        \n        max_failed_entries_before_giving_up = 10\n        \n        for i in xrange( min( export_dir.NumberOfNames, length_until_eof(export_dir.AddressOfNames)/4) ):\n            \n            symbol_name_address = self.get_dword_from_data(address_of_names, i)\n\n            symbol_name = self.get_string_at_rva( symbol_name_address )\n            try:\n                symbol_name_offset = self.get_offset_from_rva( symbol_name_address )\n            except PEFormatError:\n                max_failed_entries_before_giving_up -= 1\n                if max_failed_entries_before_giving_up <= 0:\n                    break\n                continue\n            \n            symbol_ordinal = self.get_word_from_data(\n                address_of_name_ordinals, i)\n\n            \n            if symbol_ordinal*4 < len(address_of_functions):\n                symbol_address = self.get_dword_from_data(\n                    address_of_functions, symbol_ordinal)\n            else:\n                # Corrupt? a bad pointer... we assume it's all\n                # useless, no exports\n                return None\n\n            if symbol_address is None or symbol_address == 0:\n                continue\n            \n            # If the funcion's RVA points within the export directory\n            # it will point to a string with the forwarded symbol's string\n            # instead of pointing the the function start address.\n            \n            if symbol_address >= rva and symbol_address < rva+size:\n                forwarder_str = self.get_string_at_rva(symbol_address)\n                try:\n                    forwarder_offset = self.get_offset_from_rva( symbol_address )\n                except PEFormatError:\n                    continue\n            else:\n                forwarder_str = None\n                forwarder_offset = None\n            \n            exports.append(\n                ExportData(\n                    pe = self,\n                    ordinal = export_dir.Base+symbol_ordinal,\n                    ordinal_offset = self.get_offset_from_rva( export_dir.AddressOfNameOrdinals + 2*i ),\n                    address = symbol_address,\n                    address_offset = self.get_offset_from_rva( export_dir.AddressOfFunctions + 4*symbol_ordinal ),\n                    name = symbol_name,\n                    name_offset = symbol_name_offset,\n                    forwarder = forwarder_str,\n                    forwarder_offset = forwarder_offset ))\n        \n        ordinals = [exp.ordinal for exp in exports]\n        \n        max_failed_entries_before_giving_up = 10\n        \n        for idx in xrange( min(export_dir.NumberOfFunctions, length_until_eof(export_dir.AddressOfFunctions)/4) ):\n            \n            if not idx+export_dir.Base in ordinals:\n                try:\n                    symbol_address = self.get_dword_from_data(\n                        address_of_functions, idx)\n                except PEFormatError:\n                    symbol_address = None\n                \n                if symbol_address is None:\n                    max_failed_entries_before_giving_up -= 1\n                    if max_failed_entries_before_giving_up <= 0:\n                        break\n                \n                if symbol_address == 0:\n                    continue\n                #\n                # Checking for forwarder again.\n                #\n                if symbol_address >= rva and symbol_address < rva+size:\n                    forwarder_str = self.get_string_at_rva(symbol_address)\n                else:\n                    forwarder_str = None\n\n                exports.append(\n                    ExportData(\n                        ordinal = export_dir.Base+idx,\n                        address = symbol_address,\n                        name = None,\n                        forwarder = forwarder_str))\n        \n        return ExportDirData(\n                struct = export_dir,\n                symbols = exports)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwalking and parse the delay import directory.", "response": "def parse_delay_import_directory(self, rva, size):\n        \"\"\"Walk and parse the delay import directory.\"\"\"\n        \n        import_descs =  []\n        while True:\n            try:\n                # If the RVA is invalid all would blow up. Some PEs seem to be\n                # specially nasty and have an invalid RVA.\n                data = self.get_data( rva, Structure(self.__IMAGE_DELAY_IMPORT_DESCRIPTOR_format__).sizeof() )\n            except PEFormatError, e:\n                self.__warnings.append(\n                    'Error parsing the Delay import directory at RVA: 0x%x' % ( rva ) )\n                break\n            \n            import_desc =  self.__unpack_data__(\n                self.__IMAGE_DELAY_IMPORT_DESCRIPTOR_format__,\n                data, file_offset = self.get_offset_from_rva(rva) )\n            \n            \n            # If the structure is all zeroes, we reached the end of the list\n            if not import_desc or import_desc.all_zeroes():\n                break\n            \n            \n            rva += import_desc.sizeof()\n            \n            try:\n                import_data =  self.parse_imports(\n                    import_desc.pINT,\n                    import_desc.pIAT,\n                    None)\n            except PEFormatError, e:\n                self.__warnings.append(\n                    'Error parsing the Delay import directory. ' +\n                    'Invalid import data at RVA: 0x%x' % ( rva ) )\n                break\n            \n            if not import_data:\n                continue\n            \n            \n            dll = self.get_string_at_rva(import_desc.szName)\n            if not is_valid_dos_filename(dll):\n                dll = '*invalid*'\n\n            if dll:\n                import_descs.append(\n                    ImportDescData(\n                        struct = import_desc,\n                        imports = import_data,\n                        dll = dll))\n        \n        return import_descs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwalk and parse the import directory.", "response": "def parse_import_directory(self, rva, size):\n        \"\"\"Walk and parse the import directory.\"\"\"\n        \n        import_descs =  []\n        while True:\n            try:\n                # If the RVA is invalid all would blow up. Some EXEs seem to be\n                # specially nasty and have an invalid RVA.\n                data = self.get_data(rva, Structure(self.__IMAGE_IMPORT_DESCRIPTOR_format__).sizeof() )\n            except PEFormatError, e:\n                self.__warnings.append(\n                    'Error parsing the import directory at RVA: 0x%x' % ( rva ) )\n                break\n            \n            import_desc =  self.__unpack_data__(\n                self.__IMAGE_IMPORT_DESCRIPTOR_format__,\n                data, file_offset = self.get_offset_from_rva(rva) )\n            \n            # If the structure is all zeroes, we reached the end of the list\n            if not import_desc or import_desc.all_zeroes():\n                break\n            \n            rva += import_desc.sizeof()\n            \n            try:\n                import_data =  self.parse_imports(\n                    import_desc.OriginalFirstThunk,\n                    import_desc.FirstThunk,\n                    import_desc.ForwarderChain)\n            except PEFormatError, excp:\n                self.__warnings.append(\n                    'Error parsing the import directory. ' +\n                    'Invalid Import data at RVA: 0x%x (%s)' % ( rva, str(excp) ) )\n                break\n                #raise excp\n            \n            if not import_data:\n                continue\n            \n            dll = self.get_string_at_rva(import_desc.Name)\n            if not is_valid_dos_filename(dll):\n                dll = '*invalid*'\n                \n            if dll:\n                import_descs.append(\n                    ImportDescData(\n                        struct = import_desc,\n                        imports = import_data,\n                        dll = dll))\n        \n        suspicious_imports = set([ 'LoadLibrary', 'GetProcAddress' ])\n        suspicious_imports_count = 0\n        total_symbols = 0\n        for imp_dll in import_descs:\n            for symbol in imp_dll.imports:\n                for suspicious_symbol in suspicious_imports:\n                    if symbol and symbol.name and symbol.name.startswith( suspicious_symbol ):\n                        suspicious_imports_count += 1\n                        break\n                total_symbols += 1\n        if suspicious_imports_count == len(suspicious_imports) and total_symbols < 20:\n            self.__warnings.append(\n                'Imported symbols contain entries typical of packed executables.' )\n            \n            \n        \n        return import_descs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the imported symbols.", "response": "def parse_imports(self, original_first_thunk, first_thunk, forwarder_chain):\n        \"\"\"Parse the imported symbols.\n        \n        It will fill a list, which will be available as the dictionary\n        attribute \"imports\". Its keys will be the DLL names and the values\n        all the symbols imported from that object.\n        \"\"\"\n        \n        imported_symbols = []\n        \n        # The following has been commented as a PE does not\n        # need to have the import data necessarily witin\n        # a section, it can keep it in gaps between sections\n        # or overlapping other data.\n        #\n        #imports_section = self.get_section_by_rva(first_thunk)\n        #if not imports_section:\n        #    raise PEFormatError, 'Invalid/corrupt imports.'\n        \n        # Import Lookup Table. Contains ordinals or pointers to strings.\n        ilt = self.get_import_table(original_first_thunk)\n        # Import Address Table. May have identical content to ILT if\n        # PE file is not bounded, Will contain the address of the\n        # imported symbols once the binary is loaded or if it is already\n        # bound.\n        iat = self.get_import_table(first_thunk)\n\n        # OC Patch:\n        # Would crash if IAT or ILT had None type\n        if (not iat or len(iat)==0) and (not ilt or len(ilt)==0):\n            raise PEFormatError(\n                'Invalid Import Table information. ' +\n                'Both ILT and IAT appear to be broken.')\n        \n        table = None\n        if ilt:\n            table = ilt\n        elif iat:\n            table = iat\n        else:\n            return None\n        \n        if self.PE_TYPE == OPTIONAL_HEADER_MAGIC_PE:\n            ordinal_flag = IMAGE_ORDINAL_FLAG\n        elif self.PE_TYPE == OPTIONAL_HEADER_MAGIC_PE_PLUS:\n            ordinal_flag = IMAGE_ORDINAL_FLAG64\n        \n        for idx in xrange(len(table)):\n            \n            imp_ord = None\n            imp_hint = None\n            imp_name = None\n            name_offset = None\n            hint_name_table_rva = None\n            \n            if table[idx].AddressOfData:\n                \n                # If imported by ordinal, we will append the ordinal number\n                #\n                if table[idx].AddressOfData & ordinal_flag:\n                    import_by_ordinal = True\n                    imp_ord = table[idx].AddressOfData & 0xffff\n                    imp_name = None\n                    name_offset = None\n                else:\n                    import_by_ordinal = False\n                    try:\n                        hint_name_table_rva = table[idx].AddressOfData & 0x7fffffff\n                        data = self.get_data(hint_name_table_rva, 2)\n                        # Get the Hint\n                        imp_hint = self.get_word_from_data(data, 0)\n                        imp_name = self.get_string_at_rva(table[idx].AddressOfData+2)\n                        if not is_valid_function_name(imp_name):\n                            imp_name = '*invalid*'\n                        \n                        name_offset = self.get_offset_from_rva(table[idx].AddressOfData+2)\n                    except PEFormatError, e:\n                        pass\n\n                # by nriva: we want the ThunkRVA and ThunkOffset\n                thunk_offset = table[idx].get_file_offset()\n                thunk_rva = self.get_rva_from_offset(thunk_offset)\n                \n            imp_address = first_thunk + self.OPTIONAL_HEADER.ImageBase + idx * 4\n            \n            struct_iat = None\n            try:\n                \n                if iat and ilt and ilt[idx].AddressOfData != iat[idx].AddressOfData:\n                    imp_bound = iat[idx].AddressOfData\n                    struct_iat = iat[idx]\n                else:\n                    imp_bound = None\n            except IndexError:\n                imp_bound = None\n            \n            # The file with hashes:\n            #\n            # MD5: bfe97192e8107d52dd7b4010d12b2924\n            # SHA256: 3d22f8b001423cb460811ab4f4789f277b35838d45c62ec0454c877e7c82c7f5\n            #\n            # has an invalid table built in a way that it's parseable but contains invalid\n            # entries that lead pefile to take extremely long amounts of time to\n            # parse. It also leads to extreme memory consumption.\n            # To prevent similar cases, if invalid entries are found in the middle of a\n            # table the parsing will be aborted\n            #\n            if imp_ord == None and imp_name == None:\n                raise PEFormatError( 'Invalid entries in the Import Table. Aborting parsing.' )\n            \n            if imp_name != '' and (imp_ord or imp_name):\n                imported_symbols.append(\n                    ImportData(\n                    pe = self,\n                        struct_table = table[idx],\n                        struct_iat = struct_iat, # for bound imports if any\n                        import_by_ordinal = import_by_ordinal,\n                        ordinal = imp_ord,\n                        ordinal_offset = table[idx].get_file_offset(),\n                        hint = imp_hint,\n                        name = imp_name,\n                        name_offset = name_offset,\n                        bound = imp_bound,\n                        address = imp_address,\n                        hint_name_table_rva = hint_name_table_rva,\n                        thunk_offset = thunk_offset,\n                        thunk_rva = thunk_rva ))\n        \n        return imported_symbols"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all the strings found withing the resources.", "response": "def get_resources_strings(self):\n        \"\"\"Returns a list of all the strings found withing the resources (if any).\n        \n        This method will scan all entries in the resources directory of the PE, if\n        there is one, and will return a list() with the strings.\n        \n        An empty list will be returned otherwise.\n        \"\"\"\n        \n        resources_strings = list()\n        \n        if hasattr(self, 'DIRECTORY_ENTRY_RESOURCE'):\n            \n            for resource_type in self.DIRECTORY_ENTRY_RESOURCE.entries:\n                if hasattr(resource_type, 'directory'):\n                    for resource_id in resource_type.directory.entries:\n                        if hasattr(resource_id, 'directory'):\n                            if hasattr(resource_id.directory, 'strings') and resource_id.directory.strings:\n                                for res_string in resource_id.directory.strings.values():\n                                    resources_strings.append( res_string )\n                                    \n        return resources_strings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data(self, rva=0, length=None):\n        \n        s = self.get_section_by_rva(rva)\n        \n        if length:\n            end = rva + length\n        else:\n            end = None\n\n        if not s:\n            if rva < len(self.header):\n                return self.header[rva:end]\n            \n            # Before we give up we check whether the file might\n            # contain the data anyway. There are cases of PE files\n            # without sections that rely on windows loading the first\n            # 8291 bytes into memory and assume the data will be\n            # there\n            # A functional file with these characteristics is:\n            # MD5: 0008892cdfbc3bda5ce047c565e52295\n            # SHA-1: c7116b9ff950f86af256defb95b5d4859d4752a9\n            #\n            if rva < len(self.__data__):\n                return self.__data__[rva:end]\n            \n            raise PEFormatError, 'data at RVA can\\'t be fetched. Corrupt header?'\n        \n        return s.get_data(rva, length)", "response": "Get the data from the PE file at the specified RVA."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rva_from_offset(self, offset):\n        \n        s = self.get_section_by_offset(offset)\n        if not s:\n            if self.sections:\n                lowest_rva = min( [ adjust_SectionAlignment( s.VirtualAddress, \n                    self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment ) for s in self.sections] )\n                if offset < lowest_rva:\n                    # We will assume that the offset lies within the headers, or\n                    # at least points before where the earliest section starts\n                    # and we will simply return the offset as the RVA\n                    #\n                    # The case illustrating this behavior can be found at:\n                    # http://corkami.blogspot.com/2010/01/hey-hey-hey-whats-in-your-head.html\n                    # where the import table is not contained by any section\n                    # hence the RVA needs to be resolved to a raw offset\n                    return offset\n            else:\n                return offset\n            #raise PEFormatError(\"specified offset (0x%x) doesn't belong to any section.\" % offset)\n        return s.get_rva_from_offset(offset)", "response": "Get the RVA corresponding to this file offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the file offset corresponding to this RVA. Given a RVA , this method will find the section where the data lies and return the offset within the file.", "response": "def get_offset_from_rva(self, rva):\n        \"\"\"Get the file offset corresponding to this RVA.\n        \n        Given a RVA , this method will find the section where the\n        data lies and return the offset within the file.\n        \"\"\"\n        \n        s = self.get_section_by_rva(rva)\n        if not s:\n\n            # If not found within a section assume it might\n            # point to overlay data or otherwise data present\n            # but not contained in any section. In those\n            # cases the RVA should equal the offset\n            if rva<len(self.__data__):\n                return rva\n                \n            raise PEFormatError, 'data at RVA can\\'t be fetched. Corrupt header?'\n        \n        return s.get_offset_from_rva(rva)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_string_at_rva(self, rva):\n\n        s = self.get_section_by_rva(rva)\n        if not s:\n            return self.get_string_from_data(0, self.__data__[rva:rva+MAX_STRING_LENGTH])\n        \n        return self.get_string_from_data( 0, s.get_data(rva, length=MAX_STRING_LENGTH) )", "response": "Get an ASCII string located at the given address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_string_from_data(self, offset, data):\n        \n        # OC Patch\n        b = None\n        \n        try:\n            b = data[offset]\n        except IndexError:\n            return ''\n        \n        s = ''\n        while ord(b):\n            s += b\n            offset += 1\n            try:\n                b = data[offset]\n            except IndexError:\n                break\n        \n        return s", "response": "Get an ASCII string from within the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an Unicode string located at the given RVA.", "response": "def get_string_u_at_rva(self, rva, max_length = 2**16):\n        \"\"\"Get an Unicode string located at the given address.\"\"\"\n        \n        try:\n            # If the RVA is invalid all would blow up. Some EXEs seem to be\n            # specially nasty and have an invalid RVA.\n            data = self.get_data(rva, 2)\n        except PEFormatError, e:\n            return None\n        \n        #length = struct.unpack('<H', data)[0]\n        \n        s = u''\n        for idx in xrange(max_length):\n            try:\n                uchr = struct.unpack('<H', self.get_data(rva+2*idx, 2))[0]\n            except struct.error:\n                break\n            \n            if unichr(uchr) == u'\\0':\n                break\n            s += unichr(uchr)\n        \n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_section_by_rva(self, rva):\n        \n        sections = [s for s in self.sections if s.contains_rva(rva)]\n        \n        if sections:\n            return sections[0]\n        \n        return None", "response": "Get the section containing the given address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping all the PE header information into human readable string.", "response": "def dump_info(self, dump=None):\n        \"\"\"Dump all the PE header information into human readable string.\"\"\"\n        \n        \n        if dump is None:\n            dump = Dump()\n        \n        warnings = self.get_warnings()\n        if warnings:\n            dump.add_header('Parsing Warnings')\n            for warning in warnings:\n                dump.add_line(warning)\n                dump.add_newline()\n        \n        \n        dump.add_header('DOS_HEADER')\n        dump.add_lines(self.DOS_HEADER.dump())\n        dump.add_newline()\n        \n        dump.add_header('NT_HEADERS')\n        dump.add_lines(self.NT_HEADERS.dump())\n        dump.add_newline()\n        \n        dump.add_header('FILE_HEADER')\n        dump.add_lines(self.FILE_HEADER.dump())\n        \n        image_flags = retrieve_flags(IMAGE_CHARACTERISTICS, 'IMAGE_FILE_')\n        \n        dump.add('Flags: ')\n        flags = []\n        for flag in image_flags:\n            if getattr(self.FILE_HEADER, flag[0]):\n                flags.append(flag[0])\n        dump.add_line(', '.join(flags))\n        dump.add_newline()\n        \n        if hasattr(self, 'OPTIONAL_HEADER') and self.OPTIONAL_HEADER is not None:\n            dump.add_header('OPTIONAL_HEADER')\n            dump.add_lines(self.OPTIONAL_HEADER.dump())\n        \n        dll_characteristics_flags = retrieve_flags(DLL_CHARACTERISTICS, 'IMAGE_DLL_CHARACTERISTICS_')\n        \n        dump.add('DllCharacteristics: ')\n        flags = []\n        for flag in dll_characteristics_flags:\n            if getattr(self.OPTIONAL_HEADER, flag[0]):\n                flags.append(flag[0])\n        dump.add_line(', '.join(flags))\n        dump.add_newline()\n        \n        \n        dump.add_header('PE Sections')\n        \n        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')\n        \n        for section in self.sections:\n            dump.add_lines(section.dump())\n            dump.add('Flags: ')\n            flags = []\n            for flag in section_flags:\n                if getattr(section, flag[0]):\n                    flags.append(flag[0])\n            dump.add_line(', '.join(flags))\n            dump.add_line('Entropy: %f (Min=0.0, Max=8.0)' % section.get_entropy() )\n            if md5 is not None:\n                dump.add_line('MD5     hash: %s' % section.get_hash_md5() )\n            if sha1 is not None:\n                dump.add_line('SHA-1   hash: %s' % section.get_hash_sha1() )\n            if sha256 is not None:\n                dump.add_line('SHA-256 hash: %s' % section.get_hash_sha256() )\n            if sha512 is not None:\n                dump.add_line('SHA-512 hash: %s' % section.get_hash_sha512() )\n            dump.add_newline()\n            \n        \n        \n        if (hasattr(self, 'OPTIONAL_HEADER') and\n            hasattr(self.OPTIONAL_HEADER, 'DATA_DIRECTORY') ):\n            \n            dump.add_header('Directories')\n            for idx in xrange(len(self.OPTIONAL_HEADER.DATA_DIRECTORY)):\n                directory = self.OPTIONAL_HEADER.DATA_DIRECTORY[idx]\n                dump.add_lines(directory.dump())\n            dump.add_newline()\n            \n        \n        def convert_char(char):\n            if char in string.ascii_letters or char in string.digits or char in string.punctuation or char in string.whitespace:\n                return char\n            else:\n                return r'\\x%02x' % ord(char)\n                \n        def convert_to_printable(s):\n            return ''.join([convert_char(c) for c in s])\n        \n        if hasattr(self, 'VS_VERSIONINFO'):\n            dump.add_header('Version Information')\n            dump.add_lines(self.VS_VERSIONINFO.dump())\n            dump.add_newline()\n            \n            if hasattr(self, 'VS_FIXEDFILEINFO'):\n                dump.add_lines(self.VS_FIXEDFILEINFO.dump())\n                dump.add_newline()\n            \n            if hasattr(self, 'FileInfo'):\n                for entry in self.FileInfo:\n                    dump.add_lines(entry.dump())\n                    dump.add_newline()\n                    \n                    if hasattr(entry, 'StringTable'):\n                        for st_entry in entry.StringTable:\n                            [dump.add_line('  '+line) for line in st_entry.dump()]\n                            dump.add_line('  LangID: '+st_entry.LangID)\n                            dump.add_newline()\n                            for str_entry in st_entry.entries.items():\n                                dump.add_line( '    ' +\n                                    convert_to_printable(str_entry[0]) + ': ' +\n                                    convert_to_printable(str_entry[1]) )\n                        dump.add_newline()\n                    \n                    elif hasattr(entry, 'Var'):\n                        for var_entry in entry.Var:\n                            if hasattr(var_entry, 'entry'):\n                                [dump.add_line('  '+line) for line in var_entry.dump()]\n                                dump.add_line(\n                                    '    ' + \n                                    convert_to_printable(var_entry.entry.keys()[0]) +\n                                    ': ' + var_entry.entry.values()[0])\n                        \n                        dump.add_newline()\n\n            \n        \n        if hasattr(self, 'DIRECTORY_ENTRY_EXPORT'):\n            dump.add_header('Exported symbols')\n            dump.add_lines(self.DIRECTORY_ENTRY_EXPORT.struct.dump())\n            dump.add_newline()\n            dump.add_line('%-10s   %-10s  %s' % ('Ordinal', 'RVA', 'Name'))\n            for export in self.DIRECTORY_ENTRY_EXPORT.symbols:\n                if export.address is not None:\n                    dump.add('%-10d 0x%08Xh    %s' % (\n                        export.ordinal, export.address, export.name))\n                    if export.forwarder:\n                        dump.add_line(' forwarder: %s' % export.forwarder)\n                    else:\n                        dump.add_newline()\n            \n            dump.add_newline()\n        \n        if hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            dump.add_header('Imported symbols')\n            for module in self.DIRECTORY_ENTRY_IMPORT:\n                dump.add_lines(module.struct.dump())\n                dump.add_newline()\n                for symbol in module.imports:\n                    \n                    if symbol.import_by_ordinal is True:\n                        dump.add('%s Ordinal[%s] (Imported by Ordinal)' % (\n                            module.dll, str(symbol.ordinal)))\n                    else:\n                        dump.add('%s.%s Hint[%s]' % (\n                            module.dll, symbol.name, str(symbol.hint)))\n                    \n                    if symbol.bound:\n                        dump.add_line(' Bound: 0x%08X' % (symbol.bound))\n                    else:\n                        dump.add_newline()\n                dump.add_newline()\n        \n        \n        if hasattr(self, 'DIRECTORY_ENTRY_BOUND_IMPORT'):\n            dump.add_header('Bound imports')\n            for bound_imp_desc in self.DIRECTORY_ENTRY_BOUND_IMPORT:\n                \n                dump.add_lines(bound_imp_desc.struct.dump())\n                dump.add_line('DLL: %s' % bound_imp_desc.name)\n                dump.add_newline()\n                \n                for bound_imp_ref in bound_imp_desc.entries:\n                    dump.add_lines(bound_imp_ref.struct.dump(), 4)\n                    dump.add_line('DLL: %s' % bound_imp_ref.name, 4)\n                    dump.add_newline()\n\n        \n        if hasattr(self, 'DIRECTORY_ENTRY_DELAY_IMPORT'):\n            dump.add_header('Delay Imported symbols')\n            for module in self.DIRECTORY_ENTRY_DELAY_IMPORT:\n                \n                dump.add_lines(module.struct.dump())\n                dump.add_newline()\n                \n                for symbol in module.imports:\n                    if symbol.import_by_ordinal is True:\n                        dump.add('%s Ordinal[%s] (Imported by Ordinal)' % (\n                            module.dll, str(symbol.ordinal)))\n                    else:\n                        dump.add('%s.%s Hint[%s]' % (\n                            module.dll, symbol.name, str(symbol.hint)))\n                    \n                    if symbol.bound:\n                        dump.add_line(' Bound: 0x%08X' % (symbol.bound))\n                    else:\n                        dump.add_newline()\n                dump.add_newline()\n        \n        \n        if hasattr(self, 'DIRECTORY_ENTRY_RESOURCE'):\n            dump.add_header('Resource directory')\n            \n            dump.add_lines(self.DIRECTORY_ENTRY_RESOURCE.struct.dump())\n            \n            for resource_type in self.DIRECTORY_ENTRY_RESOURCE.entries:\n                \n                if resource_type.name is not None:\n                    dump.add_line('Name: [%s]' % resource_type.name, 2)\n                else:\n                    dump.add_line('Id: [0x%X] (%s)' % (\n                        resource_type.struct.Id, RESOURCE_TYPE.get(\n                            resource_type.struct.Id, '-')),\n                        2)\n                \n                dump.add_lines(resource_type.struct.dump(), 2)\n                \n                if hasattr(resource_type, 'directory'):\n                    \n                    dump.add_lines(resource_type.directory.struct.dump(), 4)\n                    \n                    for resource_id in resource_type.directory.entries:\n                        \n                        if resource_id.name is not None:\n                            dump.add_line('Name: [%s]' % resource_id.name, 6)\n                        else:\n                            dump.add_line('Id: [0x%X]' % resource_id.struct.Id, 6)\n                        \n                        dump.add_lines(resource_id.struct.dump(), 6)\n                        \n                        if hasattr(resource_id, 'directory'):\n                            dump.add_lines(resource_id.directory.struct.dump(), 8)\n                            \n                            for resource_lang in resource_id.directory.entries:\n                                if hasattr(resource_lang, 'data'):\n                                    dump.add_line('\\\\--- LANG [%d,%d][%s,%s]' % (\n                                        resource_lang.data.lang,\n                                        resource_lang.data.sublang,\n                                        LANG.get(resource_lang.data.lang, '*unknown*'), \n                                        get_sublang_name_for_lang( resource_lang.data.lang, resource_lang.data.sublang ) ), 8)\n                                    dump.add_lines(resource_lang.struct.dump(), 10)\n                                    dump.add_lines(resource_lang.data.struct.dump(), 12)\n                            if hasattr(resource_id.directory, 'strings') and resource_id.directory.strings:\n                                dump.add_line( '[STRINGS]' , 10 )\n                                for idx, res_string in resource_id.directory.strings.items():\n                                    dump.add_line( '%6d: %s' % (idx, convert_to_printable(res_string) ), 12 )\n                                        \n                dump.add_newline()\n            \n            dump.add_newline()\n        \n        \n        if ( hasattr(self, 'DIRECTORY_ENTRY_TLS') and\n             self.DIRECTORY_ENTRY_TLS and\n             self.DIRECTORY_ENTRY_TLS.struct ):\n            \n            dump.add_header('TLS')\n            dump.add_lines(self.DIRECTORY_ENTRY_TLS.struct.dump())\n            dump.add_newline()\n        \n        \n        if ( hasattr(self, 'DIRECTORY_ENTRY_LOAD_CONFIG') and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG.struct ):\n             \n            dump.add_header('LOAD_CONFIG')\n            dump.add_lines(self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.dump())\n            dump.add_newline()\n        \n        \n        if hasattr(self, 'DIRECTORY_ENTRY_DEBUG'):\n            dump.add_header('Debug information')\n            for dbg in self.DIRECTORY_ENTRY_DEBUG:\n                dump.add_lines(dbg.struct.dump())\n                try:\n                    dump.add_line('Type: '+DEBUG_TYPE[dbg.struct.Type])\n                except KeyError:\n                    dump.add_line('Type: 0x%x(Unknown)' % dbg.struct.Type)\n                dump.add_newline()\n        \n        \n        if hasattr(self, 'DIRECTORY_ENTRY_BASERELOC'):\n            dump.add_header('Base relocations')\n            for base_reloc in self.DIRECTORY_ENTRY_BASERELOC:\n                dump.add_lines(base_reloc.struct.dump())\n                for reloc in base_reloc.entries:\n                    try:\n                        dump.add_line('%08Xh %s' % (\n                            reloc.rva, RELOCATION_TYPE[reloc.type][16:]), 4)\n                    except KeyError:\n                        dump.add_line('0x%08X 0x%x(Unknown)' % (\n                            reloc.rva, reloc.type), 4)\n                dump.add_newline()\n\n        \n        return dump.get_text()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_bytes_at_rva(self, rva, data):\n        \n        offset = self.get_physical_by_rva(rva)\n        if not offset:\n            raise False\n        \n        return self.set_bytes_at_offset(offset, data)", "response": "Overwrite with the given string the bytes at the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the PE image content with any individual section data that has been modified.", "response": "def merge_modified_section_data(self):\n        \"\"\"Update the PE image content with any individual section data that has been modified.\"\"\"\n        \n        for section in self.sections:\n            section_data_start = adjust_FileAlignment( section.PointerToRawData,\n                self.OPTIONAL_HEADER.FileAlignment )\n            section_data_end = section_data_start+section.SizeOfRawData\n            if section_data_start < len(self.__data__) and section_data_end < len(self.__data__):\n                self.__data__ = self.__data__[:section_data_start] + section.get_data() + self.__data__[section_data_end:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the relocation information to the image using the provided new image base. This method will apply the relocation information to the image. Given the new base, all the relocations will be processed and both the raw data and the section's data will be fixed accordingly. The resulting image can be retrieved as well through the method: get_memory_mapped_image() In order to get something that would more closely match what could be found in memory once the Windows loader finished its work.", "response": "def relocate_image(self, new_ImageBase):\n        \"\"\"Apply the relocation information to the image using the provided new image base.\n        \n        This method will apply the relocation information to the image. Given the new base,\n        all the relocations will be processed and both the raw data and the section's data\n        will be fixed accordingly.\n        The resulting image can be retrieved as well through the method:\n            \n            get_memory_mapped_image()\n        \n        In order to get something that would more closely match what could be found in memory\n        once the Windows loader finished its work.\n        \"\"\"\n        \n        relocation_difference = new_ImageBase - self.OPTIONAL_HEADER.ImageBase\n        \n        \n        for reloc in self.DIRECTORY_ENTRY_BASERELOC:\n            \n            virtual_address = reloc.struct.VirtualAddress\n            size_of_block = reloc.struct.SizeOfBlock\n            \n            # We iterate with an index because if the relocation is of type\n            # IMAGE_REL_BASED_HIGHADJ we need to also process the next entry\n            # at once and skip it for the next iteration\n            #\n            entry_idx = 0\n            while entry_idx<len(reloc.entries):\n                \n                entry = reloc.entries[entry_idx]\n                entry_idx += 1\n                \n                if entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_ABSOLUTE']:\n                    # Nothing to do for this type of relocation\n                    pass\n                \n                elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGH']:\n                    # Fix the high 16bits of a relocation\n                    #\n                    # Add high 16bits of relocation_difference to the\n                    # 16bit value at RVA=entry.rva\n                    \n                    self.set_word_at_rva(\n                        entry.rva,\n                        ( self.get_word_at_rva(entry.rva) + relocation_difference>>16)&0xffff )\n                \n                elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_LOW']:\n                    # Fix the low 16bits of a relocation\n                    #\n                    # Add low 16 bits of relocation_difference to the 16bit value\n                    # at RVA=entry.rva\n                    \n                    self.set_word_at_rva(\n                        entry.rva,\n                        ( self.get_word_at_rva(entry.rva) + relocation_difference)&0xffff)\n                \n                elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHLOW']:\n                    # Handle all high and low parts of a 32bit relocation\n                    #\n                    # Add relocation_difference to the value at RVA=entry.rva\n                    \n                    self.set_dword_at_rva(\n                        entry.rva,\n                        self.get_dword_at_rva(entry.rva)+relocation_difference)\n                \n                elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHADJ']:\n                    # Fix the high 16bits of a relocation and adjust\n                    #\n                    # Add high 16bits of relocation_difference to the 32bit value\n                    # composed from the (16bit value at RVA=entry.rva)<<16 plus\n                    # the 16bit value at the next relocation entry.\n                    #\n                    \n                    # If the next entry is beyond the array's limits,\n                    # abort... the table is corrupt\n                    #\n                    if entry_idx == len(reloc.entries):\n                        break\n                    \n                    next_entry = reloc.entries[entry_idx]\n                    entry_idx += 1\n                    self.set_word_at_rva( entry.rva,\n                        ((self.get_word_at_rva(entry.rva)<<16) + next_entry.rva +\n                        relocation_difference & 0xffff0000) >> 16 )\n                \n                elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_DIR64']:\n                    # Apply the difference to the 64bit value at the offset\n                    # RVA=entry.rva\n                    \n                    self.set_qword_at_rva(\n                        entry.rva,\n                        self.get_qword_at_rva(entry.rva) + relocation_difference)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_driver(self):\n\n        # Checking that the ImageBase field of the OptionalHeader is above or\n        # equal to 0x80000000 (that is, whether it lies in the upper 2GB of\n        # the address space, normally belonging to the kernel) is not a\n        # reliable enough indicator.  For instance, PEs that play the invalid\n        # ImageBase trick to get relocated could be incorrectly assumed to be\n        # drivers.\n         \n        # This is not reliable either...\n        #   \n        # if any( (section.Characteristics & SECTION_CHARACTERISTICS['IMAGE_SCN_MEM_NOT_PAGED']) for section in self.sections ):\n        #    return True\n            \n        if hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            \n            # If it imports from \"ntoskrnl.exe\" or other kernel components it should be a driver\n            #\n            if set( ('ntoskrnl.exe', 'hal.dll', 'ndis.sys', 'bootvid.dll', 'kdcom.dll' ) ).intersection( [ imp.dll.lower() for imp in self.DIRECTORY_ENTRY_IMPORT ] ):\n                return True\n                \n        return False", "response": "Check whether the file is a Windows driver."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the offset of data appended to the file and not contained within the area described in the headers.", "response": "def get_overlay_data_start_offset(self):\n        \"\"\"Get the offset of data appended to the file and not contained within the area described in the headers.\"\"\"\n\n        highest_PointerToRawData = 0\n        highest_SizeOfRawData = 0\n        for section in self.sections:\n            \n            # If a section seems to fall outside the boundaries of the file we assume it's either\n            # because of intentionally misleading values or because the file is truncated\n            # In either case we skip it\n            if section.PointerToRawData + section.SizeOfRawData > len(self.__data__):\n                continue\n                \n            if section.PointerToRawData + section.SizeOfRawData > highest_PointerToRawData + highest_SizeOfRawData:\n                highest_PointerToRawData = section.PointerToRawData\n                highest_SizeOfRawData = section.SizeOfRawData\n                \n        if len(self.__data__) > highest_PointerToRawData + highest_SizeOfRawData:\n            return highest_PointerToRawData + highest_SizeOfRawData\n            \n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _md5_file(fn, block_size=1048576):\n    h = hashlib.md5()\n    with open(fn) as fp:\n        d = 1\n        while d:\n            d = fp.read(block_size)\n            h.update(d)\n    return h.hexdigest()", "response": "Builds the MD5 of a file block by block_size"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfreezing a script and returns a tuple of the command and path to the frozen tar file.", "response": "def freeze_script(script_path, cache=True, temp_path='_hadoopy_temp'):\n    \"\"\"Freezes a script, puts it on hdfs, and gives you the path\n\n    'frozen_tar_path' can be given to launch_frozen and it will use that\n    instead of making its own, this is useful for repeated calls.  If a\n    file with the same md5 already exists in the temp_path, it is used\n    instead of putting a new copy there to avoid the file transfer.  The\n    files are put into a temporary file based on the timestamp first, then\n    moved to a location that is only a function of their md5 to prevent partial\n    files.\n\n    Args:\n        script_path: Path to a hadoopy script\n        cache: If True (default) then use previously frozen scripts.  Cache is stored in memory (not persistent).\n        temp_path: HDFS temporary path (default is '_hadoopy_temp')\n\n    Returns:\n        {'cmds': commands_ran, 'frozen_tar_path': frozen_tar_path}\n\n    Raises:\n        ValueError: Script cannot be found\n    \"\"\"\n    script_abspath = os.path.abspath(script_path)\n    if not os.path.exists(script_abspath):\n        raise ValueError('Script [%s] does not exist.' % script_abspath)\n    try:\n        if not cache:\n            raise KeyError  # NOTE(brandyn): Don't use cache item\n        cmds, frozen_tar_path = FREEZE_CACHE[script_abspath]\n    except KeyError:\n        tmp_frozen_tar_path = temp_path + '/%f.tar' % time.time()\n        freeze_fp = tempfile.NamedTemporaryFile(suffix='.tar')\n        cmds = hadoopy._freeze.freeze_to_tar(os.path.abspath(script_path), freeze_fp.name)\n        md5 = _md5_file(freeze_fp.name)\n        frozen_tar_path = temp_path + '/%s.tar' % md5\n        if not hadoopy.exists(frozen_tar_path):\n            if not hadoopy.exists(temp_path):  # CDH4 Fix\n                hadoopy.mkdir(temp_path)\n            hadoopy.put(freeze_fp.name, tmp_frozen_tar_path)\n            try:\n                hadoopy.mv(tmp_frozen_tar_path, frozen_tar_path)\n            except IOError:\n                if not hadoopy.exists(frozen_tar_path):  # Check again\n                    raise\n    FREEZE_CACHE[script_abspath] = cmds, frozen_tar_path\n    return {'cmds': cmds, 'frozen_tar_path': frozen_tar_path}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping pyinstaller and provides an easy to use interface ArcGIS.", "response": "def freeze(script_path, target_dir='frozen', **kw):\n    \"\"\"Wraps pyinstaller and provides an easy to use interface\n\n    Args:\n        script_path: Absolute path to python script to be frozen.\n\n    Returns:\n        List of freeze commands ran\n\n    Raises:\n        subprocess.CalledProcessError: Freeze error.\n        OSError: Freeze not found.\n    \"\"\"\n    cmds = []\n    freeze_start_time = time.time()\n    logging.debug('/\\\\%s%s Output%s/\\\\' % ('-' * 10, 'Pyinstaller', '-' * 10))\n    orig_dir = os.path.abspath('.')\n    script_path = os.path.abspath(script_path)\n    try:\n        os.chdir(target_dir)\n        cmds += _freeze_config()\n        pyinst_path = '%s/thirdparty/pyinstaller' % __path__[0]\n        cur_cmd = 'python -O %s/pyinstaller.py %s --skip-configure' % (pyinst_path, script_path)\n        cmds.append(cur_cmd)\n        if _run(cur_cmd):  # If there is a problem, try removing the config and re-doing\n            _freeze_config(force=True)\n            cur_cmd = 'python -O %s/pyinstaller.py %s' % (pyinst_path, script_path)\n            _run(cur_cmd)\n    finally:\n        os.chdir(orig_dir)\n    logging.debug('\\\\/%s%s Output%s\\\\/' % ('-' * 10, 'Pyinstaller', '-' * 10))\n    logging.info('Pyinstaller took [%f] seconds' % (time.time() - freeze_start_time))\n    return cmds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef freeze_to_tar(script_path, freeze_fn, extra_files=None):\n    if not extra_files:\n        extra_files = []\n    freeze_dir = tempfile.mkdtemp()\n    try:\n        cmds = freeze(script_path, target_dir=freeze_dir)\n        if freeze_fn.endswith('.tar.gz'):\n            mode = 'w|gz'\n        elif freeze_fn.endswith('.tar'):\n            mode = 'w'\n        else:\n            raise NameError('[%s] must end in .tar or .tar.gz' % freeze_fn)\n        fp = tarfile.open(freeze_fn, mode)\n        proj_name = os.path.basename(script_path)\n        proj_name = proj_name[:proj_name.rfind('.')]  # Remove extension\n        for x in glob.glob('%s/dist/%s/*' % (freeze_dir, proj_name)) + extra_files:\n            fp.add(x, arcname=os.path.basename(x))\n        fp.close()\n    finally:\n        shutil.rmtree(freeze_dir)\n    return cmds", "response": "Freezes a python script to a. tar or. tar. gz file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mobile_template(template):\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            ctx = stack.top\n            if ctx is not None and hasattr(ctx, 'request'):\n                request = ctx.request\n                is_mobile = getattr(request, 'MOBILE', None)\n                kwargs['template'] = re.sub(r'{(.+?)}',\n                                            r'\\1' if is_mobile else '',\n                                            template)\n            return f(*args, **kwargs)\n        return wrapper\n    return decorator", "response": "Decorator for creating a mobile template."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace a view function with a normal and mobile view. For example:: def view(): ... @mobilized(view) def view(): ... The second function is the mobile version of view. The original function is overwritten, and the decorator will choose the correct function based on ``request.MOBILE``.", "response": "def mobilized(normal_fn):\n    \"\"\"\n    Replace a view function with a normal and mobile view.\n\n    For example::\n\n        def view():\n            ...\n\n        @mobilized(view)\n        def view():\n            ...\n\n\n    The second function is the mobile version of view. The original\n    function is overwritten, and the decorator will choose the correct\n    function based on ``request.MOBILE``.\n\n    \"\"\"\n    def decorator(mobile_fn):\n        @functools.wraps(mobile_fn)\n        def wrapper(*args, **kwargs):\n            ctx = stack.top\n            if ctx is not None and hasattr(ctx, 'request'):\n                request = ctx.request\n                if not request.MOBILE:\n                    return normal_fn(*args, **kwargs)\n            return mobile_fn(*args, **kwargs)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_output(instances, flag):\n    out = []\n    line_format = '{0}\\t{1}\\t{2}\\t{3}'\n    name_len = _get_max_name_len(instances) + 3\n    if flag:\n        line_format = '{0:<' + str(name_len+5) + '}{1:<16}{2:<65}{3:<16}'\n\n    for i in instances:\n        endpoint = \"{0}:{1}\".format(i['Endpoint']['Address'], i['Endpoint']['Port'])\n        out.append(\n            line_format.format(i['DBInstanceIdentifier'], i['DBInstanceStatus'], endpoint, i['Engine']))\n    return out", "response": "return formatted string per instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rm(ctx, cluster_id):\n    session = create_session(ctx.obj['AWS_PROFILE_NAME'])\n\n    client = session.client('emr')\n    try:\n        result = client.describe_cluster(ClusterId=cluster_id)\n        target_dns = result['Cluster']['MasterPublicDnsName']\n        flag = click.prompt(\n            \"Are you sure you want to terminate {0}: {1}? [y/Y]\".format(\n                cluster_id, target_dns), type=str, default='n')\n        if flag.lower() == 'y':\n            result = client.terminate_job_flows(JobFlowIds=[cluster_id])\n    except ClientError as e:\n        click.echo(e, err=True)", "response": "Terminate a EMR cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_output(instances, flag):\n    out = []\n    line_format = '{0}\\t{1}\\t{2}\\t{3}\\t{4}'\n    name_len = _get_max_name_len(instances) + 3\n    if flag:\n        line_format = '{0:<' + str(name_len) + '}{1:<16}{2:<21}{3:<16}{4:<16}'\n\n    for i in instances:\n        tag_name = get_tag_value(i.tags, 'Name')\n        out.append(line_format.format(\n            tag_name, i.state['Name'], i.id, i.private_ip_address, str(i.public_ip_address)))\n    return out", "response": "return formatted string for instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets max length of Tag : Name", "response": "def _get_max_name_len(instances):\n    \"\"\"get max length of Tag:Name\"\"\"\n    # FIXME: ec2.instanceCollection doesn't have __len__\n    for i in instances:\n        return max([len(get_tag_value(i.tags, 'Name')) for i in instances])\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a value from tag", "response": "def get_tag_value(x, key):\n    \"\"\"Get a value from tag\"\"\"\n    if x is None:\n        return ''\n    result = [y['Value'] for y in x if y['Key'] == key]\n    if result:\n        return result[0]\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating SSH Login command string", "response": "def create_ssh_command(session, instance_id, instance_name, username, key_file, port, ssh_options,\n                       use_private_ip, gateway_instance_id, gateway_username):\n    \"\"\"Create SSH Login command string\"\"\"\n    ec2 = session.resource('ec2')\n    if instance_id is not None:\n        try:\n            instance = ec2.Instance(instance_id)\n            hostname = _get_instance_ip_address(instance, use_private_ip)\n        except botocore.exceptions.ClientError as e:\n            click.echo(\"Invalid instance ID {0} ({1})\".format(instance_id, e), err=True)\n            sys.exit(2)\n    elif instance_name is not None:\n        try:\n            conditions = [\n                {'Name': 'tag:Name', 'Values': [instance_name]},\n                {'Name': 'instance-state-name', 'Values': ['running']},\n            ]\n            instances = ec2.instances.filter(Filters=conditions)\n            target_instances = []\n            for idx, i in enumerate(instances):\n                target_instances.append(i)\n            if len(target_instances) == 1:\n                instance = target_instances[0]\n                hostname = _get_instance_ip_address(instance, use_private_ip)\n            else:\n                for idx, i in enumerate(instances):\n                    tag_name = get_tag_value(i.tags, 'Name')\n                    click.echo('[{0}]: {1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(\n                        idx, i.id, i.public_ip_address, i.state['Name'], tag_name, i.key_name))\n                selected_idx = click.prompt(\"Please enter a valid number\", type=int, default=0)\n                if len(target_instances) - 1 < selected_idx or selected_idx < 0:\n                    click.echo(\"selected number [{0}] is invalid\".format(selected_idx), err=True)\n                    sys.exit(2)\n                click.echo(\"{0} is selected.\".format(selected_idx))\n                instance = target_instances[selected_idx]\n                hostname = _get_instance_ip_address(instance, use_private_ip)\n        except botocore.exceptions.ClientError as e:\n            click.echo(\"Invalid instance ID {0} ({1})\".format(instance_id, e), err=True)\n            sys.exit(2)\n    # TODO: need to refactor and make it testable\n    if key_file is None:\n        key_file_option = ''\n    else:\n        key_file_option = ' -i {0}'.format(key_file)\n\n    gateway_username_option = build_option_username(gateway_username)\n    username_option = build_option_username(username)\n\n    if ssh_options is None:\n        ssh_options = ''\n    else:\n        ssh_options = ' {0}'.format(ssh_options)\n    if gateway_instance_id is not None:\n        gateway_instance = ec2.Instance(gateway_instance_id)\n        gateway_public_ip = gateway_instance.public_ip_address\n        hostname = instance.private_ip_address\n        cmd = 'ssh -tt{0} {1}{2} -p {3}{4} ssh{5} {6}'.format(\n            gateway_username_option, gateway_public_ip, key_file_option,\n            port, ssh_options, username_option, hostname)\n    else:\n        cmd = 'ssh{0} {1}{2} -p {3}{4}'.format(username_option, hostname, key_file_option, port, ssh_options)\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ssh(ctx, instance_id, instance_name, username, key_file, port, ssh_options, private_ip,\n        gateway_instance_id, gateway_username, dry_run):\n    \"\"\"SSH to EC2 instance\"\"\"\n    session = create_session(ctx.obj['AWS_PROFILE_NAME'])\n\n    if instance_id is None and instance_name is None:\n        click.echo(\n            \"One of --instance-id/-i or --instance-name/-n\"\n            \" has to be specified.\", err=True)\n        sys.exit(1)\n    elif instance_id is not None and instance_name is not None:\n        click.echo(\n            \"Both --instance-id/-i and --instance-name/-n \"\n            \"can't to be specified at the same time.\", err=True)\n        sys.exit(1)\n    cmd = create_ssh_command(\n        session, instance_id, instance_name, username, key_file, port, ssh_options, private_ip,\n        gateway_instance_id, gateway_username)\n    if not dry_run:\n        subprocess.call(cmd, shell=True)\n    else:\n        click.echo(cmd)", "response": "SSH to an EC2 instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting command from module", "response": "def get_command(self, ctx, name):\n        \"\"\"get command\"\"\"\n        try:\n            mod = __import__('jungle.' + name, None, None, ['cli'])\n            return mod.cli\n        except ImportError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn formatted string for instance", "response": "def format_output(groups, flag):\n    \"\"\"return formatted string for instance\"\"\"\n    out = []\n    line_format = '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'\n    for g in groups['AutoScalingGroups']:\n        out.append(line_format.format(\n            g['AutoScalingGroupName'],\n            g['LaunchConfigurationName'],\n            'desired:'+str(g['DesiredCapacity']),\n            'max:'+str(g['MaxSize']),\n            'min:'+str(g['MinSize']),\n            g['CreatedTime'].strftime('%Y/%m/%d %H:%M:%S'),\n        ))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self, provider):\n        callback_url = url_for(\".callback\", provider=provider, _external=True)\n        provider = self.get_provider(provider)\n        session['next'] = request.args.get('next') or ''\n        return provider.authorize(callback_url)", "response": "Starts OAuth authorization flow"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef callback(self, provider):\n        provider = self.get_provider(provider)\n        try:\n            return provider.authorized_handler(self.login)(provider=provider)\n        except OAuthException as ex:\n            logging.error(\"Data: %s\", ex.data)\n            raise", "response": "Handles 3rd party callback and processes it s data\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(model, fields, restrict_to=None, manager=None, properties=None, contexts=None):\n    if not contexts:\n        contexts = {}\n\n    global _REGISTRY\n    _REGISTRY[model] = {\n        'fields': fields,\n        'contexts': contexts,\n        'restrict_to': restrict_to,\n        'manager': manager,\n        'properties': properties,\n    }\n\n    for field in fields:\n        setattr(model, field, VinaigretteDescriptor(field, contexts.get(field, None)))\n    model.untranslated = lambda self, fieldname: self.__dict__[fieldname]\n\n    pre_save.connect(_vinaigrette_pre_save, sender=model)\n    post_save.connect(_vinaigrette_post_save, sender=model)", "response": "Register a new model with the vinaigrette."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the current page number from GET or POST data.", "response": "def get_page_number_from_request(\n        request, querystring_key=PAGE_LABEL, default=1):\n    \"\"\"Retrieve the current page number from *GET* or *POST* data.\n    If the page does not exists in *request*, or is not a number,\n    then *default* number is returned.\n    \"\"\"\n    try:\n        return int(request.GET[querystring_key])\n    except (KeyError, TypeError, ValueError):\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefaulting callable for page listing. Produce a Digg - style pagination.", "response": "def get_page_numbers(current_page, num_pages):\n    \"\"\"Default callable for page listing.\n    Produce a Digg-style pagination.\n    \"\"\"\n\n    if current_page <= 2:\n        start_page = 1\n    else:\n        start_page = current_page - 2\n\n    if num_pages <= 4:\n        end_page = num_pages\n    else:\n        end_page = start_page + 4\n        if end_page > num_pages:\n            end_page = num_pages\n\n    pages = []\n    if current_page != 1:\n        pages.append('first')\n        pages.append('previous')\n    pages.extend([i for i in range(start_page, end_page + 1)])\n    if current_page != num_pages:\n        pages.append('next')\n        pages.append('last')\n    return pages"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning paths to all relevant po files in the current project.", "response": "def _get_po_paths(locales=[]):\n    \"\"\"Returns paths to all relevant po files in the current project.\"\"\"\n    basedirs = [os.path.join('conf', 'locale'), 'locale']\n    if os.environ.get('DJANGO_SETTINGS_MODULE'):\n        from django.conf import settings\n        basedirs.extend(settings.LOCALE_PATHS)\n\n    # Gather existing directories.\n    basedirs = set(map(os.path.abspath, filter(os.path.isdir, basedirs)))\n\n    if not basedirs:\n        raise CommandError(\"This script should be run from the Django SVN tree or your project or app tree, or with the settings module specified.\")\n\n    po_paths = []\n    for basedir in basedirs:\n        for locale in locales:\n            basedir_locale = os.path.join(basedir, locale, 'LC_MESSAGES')\n            for dirpath, dirnames, filenames in os.walk(basedir_locale):\n                for f in filenames:\n                    if f.endswith('.po'):\n                        po_paths.append(os.path.join(dirpath, f))\n    return po_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_pageitems(_, token):\n    # Validate args.\n    if len(token.contents.split()) != 1:\n        msg = '%r tag takes no arguments' % token.contents.split()[0]\n        raise template.TemplateSyntaxError(msg)\n    # Call the node.\n    return ShowPageItemsNode()", "response": "Show page items.\n\n    Usage:\n\n    .. code-block:: html+django\n\n        {% show_pageitems per_page %}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _endless_page(self, number, label=None):\n        return EndlessPage(\n            self._request,\n            number,\n            self._page.number,\n            len(self),\n            self._querystring_key,\n            label=label,\n            default_number=self._default_number,\n            override_path=self._override_path,\n        )", "response": "Returns a EndlessPage instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and return a general user error response that is jsonapi compliant.", "response": "def user_error(status_code, title, detail, pointer):\n    \"\"\"Create and return a general user error response that is jsonapi compliant.\n\n    Required args:\n        status_code: The HTTP status code associated with the problem.\n        title: A short summary of the problem.\n        detail: An explanation specific to the occurence of the problem.\n        pointer: The request path associated with the source of the problem.\n    \"\"\"\n    response = {\n        'errors': [{\n            'status': status_code,\n            'source': {'pointer': '{0}'.format(pointer)},\n            'title': title,\n            'detail': detail,\n        }],\n        'jsonapi': {\n            'version': '1.0'\n        },\n        'meta': {\n            'sqlalchemy_jsonapi_version': __version__\n        }\n    }\n    return json.dumps(response), status_code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride the original value with the last returned value.", "response": "def override(original, results):\n    \"\"\"\n    If a receiver to a signal returns a value, we override the original value\n    with the last returned value.\n\n    :param original: The original value\n    :param results: The results from the signal\n    \"\"\"\n    overrides = [v for fn, v in results if v is not None]\n    if len(overrides) == 0:\n        return original\n    return overrides[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default value for the object.", "response": "def default(self, value):\n        \"\"\"\n        Handle UUID, datetime, and callables.\n\n        :param value: Value to encode\n        \"\"\"\n        if isinstance(value, uuid.UUID):\n            return str(value)\n        elif isinstance(value, datetime.datetime):\n            return value.isoformat()\n        elif callable(value):\n            return str(value)\n        return json.JSONEncoder.default(self, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_app(self, app, sqla, namespace='api', route_prefix='/api'):\n        self.app = app\n        self.sqla = sqla\n\n        self._setup_adapter(namespace, route_prefix)", "response": "Initialize the adapter if it hasn t already been initialized."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a handler in a chain.", "response": "def wrap_handler(self, api_types, methods, endpoints):\n        \"\"\"\n        Allow for a handler to be wrapped in a chain.\n\n        :param api_types: Types to wrap handlers for\n        :param methods: Methods to wrap handlers for\n        :param endpoints: Endpoints to wrap handlers for\n        \"\"\"\n\n        def wrapper(fn):\n            @wraps(fn)\n            def wrapped(*args, **kwargs):\n                return fn(*args, **kwargs)\n\n            for api_type in api_types:\n                for method in methods:\n                    for endpoint in endpoints:\n                        key = (api_type, method, endpoint)\n                        self._handler_chains.setdefault(key, [])\n                        self._handler_chains[key].append(wrapped)\n            return wrapped\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an express - like chain for handling requests.", "response": "def _call_next(self, handler_chain):\n        \"\"\"\n        Generates an express-like chain for handling requests.\n\n        :param handler_chain: The current chain of handlers\n        \"\"\"\n\n        def wrapped(*args, **kwargs):\n            if len(handler_chain) == 1:\n                return handler_chain[0](*args, **kwargs)\n            else:\n                return handler_chain[0](self._call_next(handler_chain[1:]),\n                                        *args, **kwargs)\n\n        return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _setup_adapter(self, namespace, route_prefix):\n        self.serializer = JSONAPI(\n            self.sqla.Model, prefix='{}://{}{}'.format(\n                self.app.config['PREFERRED_URL_SCHEME'],\n                self.app.config['SERVER_NAME'], route_prefix))\n        for view in views:\n            method, endpoint = view\n            pattern = route_prefix + endpoint.value\n            name = '{}_{}_{}'.format(namespace, method.name, endpoint.name)\n            view = self._generate_view(method, endpoint)\n            self.app.add_url_rule(pattern + '/',\n                                  name + '_slashed',\n                                  view,\n                                  methods=[method.name],\n                                  strict_slashes=False)\n            self.app.add_url_rule(pattern, name, view, methods=[method.name])", "response": "Initialize the serializer and loop through the views to generate the views."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a view for the specified method and endpoint.", "response": "def _generate_view(self, method, endpoint):\n        \"\"\"\n        Generate a view for the specified method and endpoint.\n\n        :param method: HTTP Method\n        :param endpoint: Pattern\n        \"\"\"\n\n        def new_view(**kwargs):\n            if method == Method.GET:\n                data = request.args\n            else:\n                content_length = request.headers.get('content-length', 0)\n                if content_length and int(content_length) > 0:\n                    content_type = request.headers.get('content-type', None)\n                    if content_type != 'application/vnd.api+json':\n                        data = MissingContentTypeError().data\n                        data = json.dumps(data, cls=JSONAPIEncoder)\n                        response = make_response(data)\n                        response.status_code = 409\n                        response.content_type = 'application/vnd.api+json'\n                        return response\n                    data = request.get_json(force=True)\n                else:\n                    data = None\n\n            event_kwargs = {\n                'method': method,\n                'endpoint': endpoint,\n                'data': data,\n                'req_args': kwargs\n            }\n            results = self.on_request.send(self, **event_kwargs)\n            data = override(data, results)\n\n            args = [self.sqla.session, data, kwargs['api_type']]\n            if 'obj_id' in kwargs.keys():\n                args.append(kwargs['obj_id'])\n            if 'relationship' in kwargs.keys():\n                args.append(kwargs['relationship'])\n\n            try:\n                attr = '{}_{}'.format(method.name, endpoint.name).lower()\n                handler = getattr(self.serializer, attr)\n                handler_chain = list(self._handler_chains.get((\n                    kwargs['api_type'], method, endpoint), []))\n                handler_chain.append(handler)\n                chained_handler = self._call_next(handler_chain)\n                response = chained_handler(*args)\n                results = self.on_success.send(self,\n                                               response=response,\n                                               **event_kwargs)\n                response = override(response, results)\n            except BaseError as exc:\n                self.sqla.session.rollback()\n                results = self.on_error.send(self, error=exc, **event_kwargs)\n                response = override(exc, results)\n            rendered_response = make_response('')\n            if response.status_code != 204:\n                data = json.dumps(response.data, cls=self.json_encoder)\n                rendered_response = make_response(data)\n            rendered_response.status_code = response.status_code\n            rendered_response.content_type = 'application/vnd.api+json'\n            results = self.on_response.send(self,\n                                            response=rendered_response,\n                                            **event_kwargs)\n            return override(rendered_response, results)\n\n        return new_view"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes resource ( s ) according to json - api spec.", "response": "def serialize(self, resources):\n        \"\"\"Serialize resource(s) according to json-api spec.\"\"\"\n        serialized = {\n            'meta': {\n                'sqlalchemy_jsonapi_version': '4.0.9'\n            },\n            'jsonapi': {\n                'version': '1.0'\n            }\n        }\n        # Determine multiple resources by checking for SQLAlchemy query count.\n        if hasattr(resources, 'count'):\n            serialized['data'] = []\n            for resource in resources:\n                serialized['data'].append(\n                    self._render_resource(resource))\n        else:\n            serialized['data'] = self._render_resource(resources)\n\n        return serialized"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _render_resource(self, resource):\n        if not resource:\n            return None\n        # Must not render a resource that has same named\n        # attributes as different model.\n        if not isinstance(resource, self.model):\n            raise TypeError(\n                'Resource(s) type must be the same as the serializer model type.')\n\n        top_level_members = {}\n        try:\n            top_level_members['id'] = str(getattr(resource, self.primary_key))\n        except AttributeError:\n            raise\n        top_level_members['type'] = resource.__tablename__\n        top_level_members['attributes'] = self._render_attributes(resource)\n        top_level_members['relationships'] = self._render_relationships(\n                                                resource)\n        return top_level_members", "response": "Renders a resource s top level members based on json - api spec."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the resources s attributes.", "response": "def _render_attributes(self, resource):\n        \"\"\"Render the resources's attributes.\"\"\"\n        attributes = {}\n        attrs_to_ignore = set()\n\n        for key, relationship in resource.__mapper__.relationships.items():\n            attrs_to_ignore.update(set(\n                [column.name for column in relationship.local_columns]).union(\n                    {key}))\n\n        if self.dasherize:\n            mapped_fields = {x: dasherize(underscore(x)) for x in self.fields}\n        else:\n            mapped_fields = {x: x for x in self.fields}\n\n        for attribute in self.fields:\n            if attribute == self.primary_key:\n                continue\n            # Per json-api spec, we cannot render foreign keys\n            # or relationsips in attributes.\n            if attribute in attrs_to_ignore:\n                raise AttributeError\n            try:\n                value = getattr(resource, attribute)\n                if isinstance(value, datetime.datetime):\n                    attributes[mapped_fields[attribute]] = value.isoformat()\n                else:\n                    attributes[mapped_fields[attribute]] = value\n            except AttributeError:\n                raise\n\n        return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _render_relationships(self, resource):\n        relationships = {}\n        related_models = resource.__mapper__.relationships.keys()\n        primary_key_val = getattr(resource, self.primary_key)\n        if self.dasherize:\n            mapped_relationships = {\n                x: dasherize(underscore(x)) for x in related_models}\n        else:\n            mapped_relationships = {x: x for x in related_models}\n\n        for model in related_models:\n            relationships[mapped_relationships[model]] = {\n                'links': {\n                    'self': '/{}/{}/relationships/{}'.format(\n                        resource.__tablename__,\n                        primary_key_val,\n                        mapped_relationships[model]),\n                    'related': '/{}/{}/{}'.format(\n                        resource.__tablename__,\n                        primary_key_val,\n                        mapped_relationships[model])\n                }\n            }\n\n        return relationships", "response": "Render the resource s relationships."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap a function that allows for getting or setting of an attribute. This allows for specific handling of an attribute when it comes to serializing and deserializing. :param action: The AttributeActions that this descriptor performs :param names: A list of names of the attributes this references", "response": "def attr_descriptor(action, *names):\n    \"\"\"\n    Wrap a function that allows for getting or setting of an attribute.  This\n    allows for specific handling of an attribute when it comes to serializing\n    and deserializing.\n\n    :param action: The AttributeActions that this descriptor performs\n    :param names: A list of names of the attributes this references\n    \"\"\"\n    if isinstance(action, AttributeActions):\n        action = [action]\n\n    def wrapped(fn):\n        if not hasattr(fn, '__jsonapi_action__'):\n            fn.__jsonapi_action__ = set()\n            fn.__jsonapi_desc_for_attrs__ = set()\n        fn.__jsonapi_desc_for_attrs__ |= set(names)\n        fn.__jsonapi_action__ |= set(action)\n        return fn\n\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef relationship_descriptor(action, *names):\n    if isinstance(action, RelationshipActions):\n        action = [action]\n\n    def wrapped(fn):\n        if not hasattr(fn, '__jsonapi_action__'):\n            fn.__jsonapi_action__ = set()\n            fn.__jsonapi_desc_for_rels__ = set()\n        fn.__jsonapi_desc_for_rels__ |= set(names)\n        fn.__jsonapi_action__ |= set(action)\n        return fn\n\n    return wrapped", "response": "A function that wraps a function for modification of a relationship."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking a permission for a given instance or field. Raises an error if the permission is not allowed.", "response": "def check_permission(instance, field, permission):\n    \"\"\"\n    Check a permission for a given instance or field.  Raises an error if\n    denied.\n\n    :param instance: The instance to check\n    :param field: The field name to check or None for instance\n    :param permission: The permission to check\n    \"\"\"\n    if not get_permission_test(instance, field, permission)(instance):\n        raise PermissionDeniedError(permission, instance, instance, field)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the appropriate descriptor for the attribute.", "response": "def get_attr_desc(instance, attribute, action):\n    \"\"\"\n    Fetch the appropriate descriptor for the attribute.\n\n    :param instance: Model instance\n    :param attribute: Name of the attribute\n    :param action: AttributeAction\n    \"\"\"\n    descs = instance.__jsonapi_attribute_descriptors__.get(attribute, {})\n    if action == AttributeActions.GET:\n        check_permission(instance, attribute, Permissions.VIEW)\n        return descs.get(action, lambda x: getattr(x, attribute))\n    check_permission(instance, attribute, Permissions.EDIT)\n    return descs.get(action, lambda x, v: setattr(x, attribute, v))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rel_desc(instance, key, action):\n    descs = instance.__jsonapi_rel_desc__.get(key, {})\n    if action == RelationshipActions.GET:\n        check_permission(instance, key, Permissions.VIEW)\n        return descs.get(action, lambda x: getattr(x, key))\n    elif action == RelationshipActions.APPEND:\n        check_permission(instance, key, Permissions.CREATE)\n        return descs.get(action, lambda x, v: getattr(x, key).append(v))\n    elif action == RelationshipActions.SET:\n        check_permission(instance, key, Permissions.EDIT)\n        return descs.get(action, lambda x, v: setattr(x, key, v))\n    else:\n        check_permission(instance, key, Permissions.DELETE)\n        return descs.get(action, lambda x, v: getattr(x, key).remove(v))", "response": "Fetch the appropriate descriptor for the relationship."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_json_data(self, json_data):\n        if not isinstance(json_data, dict):\n            raise BadRequestError('Request body should be a JSON hash')\n        if 'data' not in json_data.keys():\n            raise BadRequestError('Request should contain data key')", "response": "Ensure that the request body is both a hash and has a data key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch a resource by type and id.", "response": "def _fetch_resource(self, session, api_type, obj_id, permission):\n        \"\"\"\n        Fetch a resource by type and id, also doing a permission check.\n\n        :param session: SQLAlchemy session\n        :param api_type: The type\n        :param obj_id: ID for the resource\n        :param permission: Permission to check\n        \"\"\"\n        if api_type not in self.models.keys():\n            raise ResourceTypeNotFoundError(api_type)\n        obj = session.query(self.models[api_type]).get(obj_id)\n        if obj is None:\n            raise ResourceNotFoundError(self.models[api_type], obj_id)\n        check_permission(obj, None, permission)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a short version of the resource.", "response": "def _render_short_instance(self, instance):\n        \"\"\"\n        For those very short versions of resources, we have this.\n\n        :param instance: The instance to render\n        \"\"\"\n        check_permission(instance, None, Permissions.VIEW)\n        return {'type': instance.__jsonapi_type__, 'id': instance.id}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render_full_resource(self, instance, include, fields):\n        api_type = instance.__jsonapi_type__\n        orm_desc_keys = instance.__mapper__.all_orm_descriptors.keys()\n        to_ret = {\n            'id': instance.id,\n            'type': api_type,\n            'attributes': {},\n            'relationships': {},\n            'included': {}\n        }\n        attrs_to_ignore = {'__mapper__', 'id'}\n        if api_type in fields.keys():\n            local_fields = list(map((\n                lambda x: instance.__jsonapi_map_to_py__[x]),\n                fields[api_type]))\n        else:\n            local_fields = orm_desc_keys\n\n        for key, relationship in instance.__mapper__.relationships.items():\n            attrs_to_ignore |= set([c.name for c in relationship.local_columns\n                                    ]) | {key}\n\n            api_key = instance.__jsonapi_map_to_api__[key]\n\n            try:\n                desc = get_rel_desc(instance, key, RelationshipActions.GET)\n            except PermissionDeniedError:\n                continue\n\n            if relationship.direction == MANYTOONE:\n                if key in local_fields:\n                    to_ret['relationships'][api_key] = {\n                        'links': self._lazy_relationship(api_type, instance.id,\n                                                         api_key)\n                    }\n\n                if api_key in include.keys():\n                    related = desc(instance)\n                    if related is not None:\n                        perm = get_permission_test(\n                            related, None, Permissions.VIEW)\n                    if (key in local_fields\n                            and (related is None or not perm(related))):\n                        to_ret['relationships'][api_key]['data'] = None\n                        continue\n                    if key in local_fields:\n                        to_ret['relationships'][api_key]['data'] = self._render_short_instance(related)  # NOQA\n                    new_include = self._parse_include(include[api_key])\n                    built = self._render_full_resource(\n                        related, new_include, fields)\n                    included = built.pop('included')\n                    to_ret['included'].update(included)\n                    to_ret['included'][(related.__jsonapi_type__, related.id)] = built  # NOQA\n\n            else:\n\n                if key in local_fields:\n                    to_ret['relationships'][api_key] = {\n                        'links': self._lazy_relationship(\n                            api_type, instance.id, api_key),\n                    }\n\n                if api_key not in include.keys():\n                    continue\n\n                if key in local_fields:\n                    to_ret['relationships'][api_key]['data'] = []\n\n                related = desc(instance)\n\n                for item in related:\n                    try:\n                        check_permission(item, None, Permissions.VIEW)\n                    except PermissionDeniedError:\n                        continue\n\n                    if key in local_fields:\n                        to_ret['relationships'][api_key]['data'].append(\n                            self._render_short_instance(item))\n\n                    new_include = self._parse_include(include[api_key])\n                    built = self._render_full_resource(item, new_include,\n                                                       fields)\n                    included = built.pop('included')\n                    to_ret['included'].update(included)\n                    to_ret['included'][(item.__jsonapi_type__, item.id)] = built  # NOQA\n\n        for key in set(orm_desc_keys) - attrs_to_ignore:\n            try:\n                desc = get_attr_desc(instance, key, AttributeActions.GET)\n                if key in local_fields:\n                    to_ret['attributes'][instance.__jsonapi_map_to_api__[key]] = desc(instance)  # NOQA\n            except PermissionDeniedError:\n                continue\n\n        return to_ret", "response": "Generate a full resource for the given instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_instance_relationships_for_delete(self, instance):\n        check_permission(instance, None, Permissions.DELETE)\n        for rel_key, rel in instance.__mapper__.relationships.items():\n            check_permission(instance, rel_key, Permissions.EDIT)\n\n            if rel.cascade.delete:\n                if rel.direction == MANYTOONE:\n                    related = getattr(instance, rel_key)\n                    self._check_instance_relationships_for_delete(related)\n                else:\n                    instances = getattr(instance, rel_key)\n                    for to_check in instances:\n                        self._check_instance_relationships_for_delete(to_check)", "response": "Checks if the instance has the delete relationship."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_fields(self, query):\n        field_args = {\n            k: v\n            for k, v in query.items() if k.startswith('fields[')\n        }\n\n        fields = {}\n\n        for k, v in field_args.items():\n            fields[k[7:-1]] = v.split(',')\n\n        return fields", "response": "Parse the querystring args for fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_include(self, include):\n        ret = {}\n        for item in include:\n            if '.' in item:\n                local, remote = item.split('.', 1)\n            else:\n                local = item\n                remote = None\n\n            ret.setdefault(local, [])\n            if remote:\n                ret[local].append(remote)\n\n        return ret", "response": "Parse the querystring args or parent includes for includes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the querystring args for pagination.", "response": "def _parse_page(self, query):\n        \"\"\"\n        Parse the querystring args for pagination.\n\n        :param query: Dict of query args\n        \"\"\"\n        args = {k[5:-1]: v for k, v in query.items() if k.startswith('page[')}\n\n        if {'number', 'size'} == set(args.keys()):\n            if not args['number'].isdecimal() or not args['size'].isdecimal():\n                raise BadRequestError('Page query parameters must be integers')\n\n            number = int(args['number'])\n            size = int(args['size'])\n            start = number * size\n\n            return start, start + size - 1\n\n        if {'limit', 'offset'} == set(args.keys()):\n            if not args['limit'].isdecimal() or not args['offset'].isdecimal():\n                raise BadRequestError('Page query parameters must be integers')\n\n            limit = int(args['limit'])\n            offset = int(args['offset'])\n\n            return offset, offset + limit - 1\n\n        return 0, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a resource or multiple resources from a to - many relationship.", "response": "def delete_relationship(self, session, data, api_type, obj_id, rel_key):\n        \"\"\"\n        Delete a resource or multiple resources from a to-many relationship.\n\n        :param session: SQLAlchemy session\n        :param data: JSON data provided with the request\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        :param rel_key: Key of the relationship to fetch\n        \"\"\"\n        model = self._fetch_model(api_type)\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.EDIT)\n        relationship = self._get_relationship(resource, rel_key,\n                                              Permissions.DELETE)\n        self._check_json_data(data)\n\n        if not isinstance(data['data'], list):\n            raise ValidationError('Provided data must be an array.')\n\n        if relationship.direction == MANYTOONE:\n            return ToManyExpectedError(model, resource, relationship)\n\n        response = JSONAPIResponse()\n        response.data = {'data': []}\n\n        session.add(resource)\n\n        remove = get_rel_desc(resource, relationship.key,\n                              RelationshipActions.DELETE)\n        reverse_side = relationship.back_populates\n\n        for item in data['data']:\n            item = self._fetch_resource(session, item['type'], item['id'],\n                                        Permissions.EDIT)\n\n            if reverse_side:\n                reverse_rel = item.__mapper__.relationships[reverse_side]\n\n                if reverse_rel.direction == MANYTOONE:\n                    permission = Permissions.EDIT\n                else:\n                    permission = Permissions.DELETE\n\n                check_permission(item, reverse_side, permission)\n\n            remove(resource, item)\n\n        session.commit()\n        session.refresh(resource)\n\n        get = get_rel_desc(resource, relationship.key, RelationshipActions.GET)\n\n        for item in get(resource):\n            response.data['data'].append(self._render_short_instance(item))\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a resource. :param session: SQLAlchemy session :param data: JSON data provided with the request :param api_type: Type of the resource :param obj_id: ID of the resource", "response": "def delete_resource(self, session, data, api_type, obj_id):\n        \"\"\"\n        Delete a resource.\n\n        :param session: SQLAlchemy session\n        :param data: JSON data provided with the request\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        \"\"\"\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.VIEW)\n        self._check_instance_relationships_for_delete(resource)\n\n        session.delete(resource)\n        session.commit()\n\n        response = JSONAPIResponse()\n        response.status_code = 204\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch a collection of resources of a specified type.", "response": "def get_collection(self, session, query, api_key):\n        \"\"\"\n        Fetch a collection of resources of a specified type.\n\n        :param session: SQLAlchemy session\n        :param query: Dict of query args\n        :param api_type: The type of the model\n        \"\"\"\n        model = self._fetch_model(api_key)\n        include = self._parse_include(query.get('include', '').split(','))\n        fields = self._parse_fields(query)\n        included = {}\n        sorts = query.get('sort', '').split(',')\n        order_by = []\n\n        collection = session.query(model)\n\n        for attr in sorts:\n            if attr == '':\n                break\n\n            attr_name, is_asc = [attr[1:], False]\\\n                if attr[0] == '-'\\\n                else [attr, True]\n\n            if attr_name not in model.__mapper__.all_orm_descriptors.keys()\\\n                    or not hasattr(model, attr_name)\\\n                    or attr_name in model.__mapper__.relationships.keys():\n                return NotSortableError(model, attr_name)\n\n            attr = getattr(model, attr_name)\n            if not hasattr(attr, 'asc'):\n                # pragma: no cover\n                return NotSortableError(model, attr_name)\n\n            check_permission(model, attr_name, Permissions.VIEW)\n\n            order_by.append(attr.asc() if is_asc else attr.desc())\n\n        if len(order_by) > 0:\n            collection = collection.order_by(*order_by)\n\n        pos = -1\n        start, end = self._parse_page(query)\n\n        response = JSONAPIResponse()\n        response.data['data'] = []\n\n        for instance in collection:\n            try:\n                check_permission(instance, None, Permissions.VIEW)\n            except PermissionDeniedError:\n                continue\n\n            pos += 1\n            if end is not None and (pos < start or pos > end):\n                continue\n\n            built = self._render_full_resource(instance, include, fields)\n            included.update(built.pop('included'))\n            response.data['data'].append(built)\n\n        response.data['included'] = list(included.values())\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch a resource. :param session: SQLAlchemy session :param query: Dict of query args :param api_type: Type of the resource :param obj_id: ID of the resource", "response": "def get_resource(self, session, query, api_type, obj_id):\n        \"\"\"\n        Fetch a resource.\n\n        :param session: SQLAlchemy session\n        :param query: Dict of query args\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        \"\"\"\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.VIEW)\n        include = self._parse_include(query.get('include', '').split(','))\n        fields = self._parse_fields(query)\n\n        response = JSONAPIResponse()\n\n        built = self._render_full_resource(resource, include, fields)\n\n        response.data['included'] = list(built.pop('included').values())\n        response.data['data'] = built\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches a collection of related resource types and ids.", "response": "def get_relationship(self, session, query, api_type, obj_id, rel_key):\n        \"\"\"\n        Fetch a collection of related resource types and ids.\n\n        :param session: SQLAlchemy session\n        :param query: Dict of query args\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        :param rel_key: Key of the relationship to fetch\n        \"\"\"\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.VIEW)\n        if rel_key not in resource.__jsonapi_map_to_py__.keys():\n            raise RelationshipNotFoundError(resource, resource, rel_key)\n        py_key = resource.__jsonapi_map_to_py__[rel_key]\n        relationship = self._get_relationship(resource, py_key,\n                                              Permissions.VIEW)\n        response = JSONAPIResponse()\n\n        related = get_rel_desc(resource, relationship.key,\n                               RelationshipActions.GET)(resource)\n\n        if relationship.direction == MANYTOONE:\n            if related is None:\n                response.data['data'] = None\n            else:\n                try:\n                    response.data['data'] = self._render_short_instance(\n                        related)\n                except PermissionDeniedError:\n                    response.data['data'] = None\n        else:\n            response.data['data'] = []\n            for item in related:\n                try:\n                    response.data['data'].append(\n                        self._render_short_instance(item))\n                except PermissionDeniedError:\n                    continue\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the resource related to the object with the given key.", "response": "def patch_relationship(self, session, json_data, api_type, obj_id,\n                           rel_key):\n        \"\"\"\n        Replacement of relationship values.\n\n        :param session: SQLAlchemy session\n        :param json_data: Request JSON Data\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        :param rel_key: Key of the relationship to fetch\n        \"\"\"\n        model = self._fetch_model(api_type)\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.EDIT)\n        if rel_key not in resource.__jsonapi_map_to_py__.keys():\n            raise RelationshipNotFoundError(resource, resource, rel_key)\n        py_key = resource.__jsonapi_map_to_py__[rel_key]\n        relationship = self._get_relationship(resource, py_key,\n                                              Permissions.EDIT)\n        self._check_json_data(json_data)\n\n        session.add(resource)\n        remote_side = relationship.back_populates\n        try:\n            if relationship.direction == MANYTOONE:\n                if not isinstance(json_data['data'], dict)\\\n                        and json_data['data'] is not None:\n                    raise ValidationError('Provided data must be a hash.')\n\n                related = getattr(resource, relationship.key)\n                check_permission(related, None, Permissions.EDIT)\n                check_permission(related, remote_side, Permissions.EDIT)\n\n                setter = get_rel_desc(resource, relationship.key,\n                                      RelationshipActions.SET)\n\n                if json_data['data'] is None:\n                    setter(resource, None)\n                else:\n                    to_relate = self._fetch_resource(\n                        session, json_data['data']['type'],\n                        json_data['data']['id'], Permissions.EDIT)\n                    check_permission(to_relate, remote_side, Permissions.EDIT)\n                    setter(resource, to_relate)\n            else:\n                if not isinstance(json_data['data'], list):\n                    raise ValidationError('Provided data must be an array.')\n\n                related = getattr(resource, relationship.key)\n\n                remover = get_rel_desc(resource, relationship.key,\n                                       RelationshipActions.DELETE)\n                appender = get_rel_desc(resource, relationship.key,\n                                        RelationshipActions.APPEND)\n                for item in related:\n                    check_permission(item, None, Permissions.EDIT)\n                    remote = item.__mapper__.relationships[remote_side]\n                    if remote.direction == MANYTOONE:\n                        check_permission(item, remote_side, Permissions.EDIT)\n                    else:\n                        check_permission(item, remote_side, Permissions.DELETE)\n                    remover(resource, item)\n\n                for item in json_data['data']:\n                    to_relate = self._fetch_resource(\n                        session, item['type'], item['id'], Permissions.EDIT)\n                    remote = to_relate.__mapper__.relationships[remote_side]\n\n                    if remote.direction == MANYTOONE:\n                        check_permission(to_relate, remote_side,\n                                         Permissions.EDIT)\n                    else:\n                        check_permission(to_relate, remote_side,\n                                         Permissions.CREATE)\n                    appender(resource, to_relate)\n            session.commit()\n        except KeyError:\n            raise ValidationError('Incompatible Type')\n\n        return self.get_relationship(session, {}, model.__jsonapi_type__,\n                                     resource.id, rel_key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch_resource(self, session, json_data, api_type, obj_id):\n        model = self._fetch_model(api_type)\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.EDIT)\n        self._check_json_data(json_data)\n        orm_desc_keys = resource.__mapper__.all_orm_descriptors.keys()\n\n        if not ({'type', 'id'} <= set(json_data['data'].keys())):\n            raise BadRequestError('Missing type or id')\n\n        if str(json_data['data']['id']) != str(resource.id):\n            raise BadRequestError('IDs do not match')\n\n        if json_data['data']['type'] != resource.__jsonapi_type__:\n            raise BadRequestError('Type does not match')\n\n        json_data['data'].setdefault('relationships', {})\n        json_data['data'].setdefault('attributes', {})\n\n        missing_keys = set(json_data['data'].get('relationships', {}).keys()) \\\n            - set(resource.__jsonapi_map_to_py__.keys())\n\n        if missing_keys:\n            raise BadRequestError(\n                '{} not relationships for {}.{}'.format(\n                    ', '.join(list(missing_keys)),\n                    model.__jsonapi_type__, resource.id))\n\n        attrs_to_ignore = {'__mapper__', 'id'}\n\n        session.add(resource)\n\n        try:\n            for key, relationship in resource.__mapper__.relationships.items():\n                api_key = resource.__jsonapi_map_to_api__[key]\n                attrs_to_ignore |= set(relationship.local_columns) | {key}\n\n                if api_key not in json_data['data']['relationships'].keys():\n                    continue\n\n                self.patch_relationship(\n                    session, json_data['data']['relationships'][api_key],\n                    model.__jsonapi_type__, resource.id, api_key)\n\n            data_keys = set(map((\n                lambda x: resource.__jsonapi_map_to_py__.get(x, None)),\n                json_data['data']['attributes'].keys()))\n            model_keys = set(orm_desc_keys) - attrs_to_ignore\n\n            if not data_keys <= model_keys:\n                raise BadRequestError(\n                    '{} not attributes for {}.{}'.format(\n                        ', '.join(list(data_keys - model_keys)),\n                        model.__jsonapi_type__, resource.id))\n\n            for key in data_keys & model_keys:\n                setter = get_attr_desc(resource, key, AttributeActions.SET)\n                setter(resource, json_data['data']['attributes'][resource.__jsonapi_map_to_api__[key]])  # NOQA\n            session.commit()\n        except IntegrityError as e:\n            session.rollback()\n            raise ValidationError(str(e.orig))\n        except AssertionError as e:\n            # pragma: no cover\n            session.rollback()\n            raise ValidationError(e.msg)\n        except TypeError as e:\n            session.rollback()\n            raise ValidationError('Incompatible data type')\n        return self.get_resource(\n            session, {}, model.__jsonapi_type__, resource.id)", "response": "Updates the resource with the given JSON data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_collection(self, session, data, api_type):\n        model = self._fetch_model(api_type)\n        self._check_json_data(data)\n\n        orm_desc_keys = model.__mapper__.all_orm_descriptors.keys()\n\n        if 'type' not in data['data'].keys():\n            raise MissingTypeError()\n\n        if data['data']['type'] != model.__jsonapi_type__:\n            raise InvalidTypeForEndpointError(\n                model.__jsonapi_type__, data['data']['type'])\n\n        resource = model()\n        check_permission(resource, None, Permissions.CREATE)\n\n        data['data'].setdefault('relationships', {})\n        data['data'].setdefault('attributes', {})\n\n        data_keys = set(map((\n            lambda x: resource.__jsonapi_map_to_py__.get(x, MissingKey(x))),\n            data['data'].get('relationships', {}).keys()))\n        model_keys = set(resource.__mapper__.relationships.keys())\n        if not data_keys <= model_keys:\n            data_keys = set([key.elem if isinstance(key, MissingKey) else key for key in data_keys])\n            # pragma: no cover\n            raise BadRequestError(\n                    '{} not relationships for {}'.format(\n                    ', '.join([repr(key) for key in list(data_keys - model_keys)]), model.__jsonapi_type__))\n\n        attrs_to_ignore = {'__mapper__', 'id'}\n\n        setters = []\n\n        try:\n            if 'id' in data['data'].keys():\n                resource.id = data['data']['id']\n\n            for key, relationship in resource.__mapper__.relationships.items():\n                attrs_to_ignore |= set(relationship.local_columns) | {key}\n                api_key = resource.__jsonapi_map_to_api__[key]\n\n                if 'relationships' not in data['data'].keys()\\\n                        or api_key not in data['data']['relationships'].keys():\n                    continue\n\n                data_rel = data['data']['relationships'][api_key]\n                if 'data' not in data_rel.keys():\n                    raise BadRequestError(\n                        'Missing data key in relationship {}'.format(key))\n                data_rel = data_rel['data']\n\n                remote_side = relationship.back_populates\n                if relationship.direction == MANYTOONE:\n                    setter = get_rel_desc(resource, key,\n                                          RelationshipActions.SET)\n                    if data_rel is None:\n                        setters.append([setter, None])\n                    else:\n                        if not isinstance(data_rel, dict):\n                            raise BadRequestError(\n                                '{} must be a hash'.format(key))\n                        if not {'type', 'id'} == set(data_rel.keys()):\n                            raise BadRequestError(\n                                '{} must have type and id keys'.format(key))\n                        to_relate = self._fetch_resource(\n                            session, data_rel['type'], data_rel['id'],\n                            Permissions.EDIT)\n                        rem = to_relate.__mapper__.relationships[remote_side]\n                        if rem.direction == MANYTOONE:\n                            check_permission(to_relate, remote_side,\n                                             Permissions.EDIT)\n                        else:\n                            check_permission(to_relate, remote_side,\n                                             Permissions.CREATE)\n                        setters.append([setter, to_relate])\n                else:\n                    setter = get_rel_desc(resource, key,\n                                          RelationshipActions.APPEND)\n                    if not isinstance(data_rel, list):\n                        raise BadRequestError(\n                            '{} must be an array'.format(key))\n                    for item in data_rel:\n                        if 'type' not in item.keys() or 'id' not in item.keys():\n                            raise BadRequestError(\n                                '{} must have type and id keys'.format(key))\n                        # pragma: no cover\n                        to_relate = self._fetch_resource(session, item['type'],\n                                                         item['id'],\n                                                         Permissions.EDIT)\n                        rem = to_relate.__mapper__.relationships[remote_side]\n                        if rem.direction == MANYTOONE:\n                            check_permission(to_relate, remote_side,\n                                             Permissions.EDIT)\n                        else:\n                            check_permission(to_relate, remote_side,\n                                             Permissions.CREATE)\n                        setters.append([setter, to_relate])\n\n            data_keys = set(map((\n                lambda x: resource.__jsonapi_map_to_py__.get(x, None)),\n                data['data'].get('attributes', {}).keys()))\n            model_keys = set(orm_desc_keys) - attrs_to_ignore\n\n            if not data_keys <= model_keys:\n                raise BadRequestError(\n                    '{} not attributes for {}'.format(\n                        ', '.join(list(data_keys -\n                                       model_keys)), model.__jsonapi_type__))\n\n            with session.no_autoflush:\n                for setter, value in setters:\n                    setter(resource, value)\n\n                for key in data_keys:\n                    api_key = resource.__jsonapi_map_to_api__[key]\n                    setter = get_attr_desc(resource, key, AttributeActions.SET)\n                    setter(resource, data['data']['attributes'][api_key])\n\n                session.add(resource)\n                session.commit()\n\n        except IntegrityError as e:\n            session.rollback()\n            raise ValidationError(str(e.orig))\n        except AssertionError as e:\n            # pragma: no cover\n            session.rollback()\n            raise ValidationError(e.msg)\n        except TypeError as e:\n            session.rollback()\n            raise ValidationError('Incompatible data type')\n        session.refresh(resource)\n        response = self.get_resource(\n            session, {}, model.__jsonapi_type__, resource.id)\n        response.status_code = 201\n        return response", "response": "Create a new resource in SQLAlchemy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nappend to a resource.", "response": "def post_relationship(self, session, json_data, api_type, obj_id, rel_key):\n        \"\"\"\n        Append to a relationship.\n\n        :param session: SQLAlchemy session\n        :param json_data: Request JSON Data\n        :param api_type: Type of the resource\n        :param obj_id: ID of the resource\n        :param rel_key: Key of the relationship to fetch\n        \"\"\"\n        model = self._fetch_model(api_type)\n        resource = self._fetch_resource(session, api_type, obj_id,\n                                        Permissions.EDIT)\n        if rel_key not in resource.__jsonapi_map_to_py__.keys():\n            raise RelationshipNotFoundError(resource, resource, rel_key)\n        py_key = resource.__jsonapi_map_to_py__[rel_key]\n        relationship = self._get_relationship(resource, py_key,\n                                              Permissions.CREATE)\n        if relationship.direction == MANYTOONE:\n            raise ValidationError('Cannot post to to-one relationship')\n\n        if not isinstance(json_data['data'], list):\n            raise ValidationError('/data must be an array')\n\n        remote_side = relationship.back_populates\n\n        try:\n            for item in json_data['data']:\n                setter = get_rel_desc(resource, relationship.key,\n                                      RelationshipActions.APPEND)\n\n                if not isinstance(json_data['data'], list):\n                    raise BadRequestError(\n                        '{} must be an array'.format(relationship.key))\n\n                for item in json_data['data']:\n                    if {'type', 'id'} != set(item.keys()):\n                        raise BadRequestError(\n                            '{} must have type and id keys'\n                            .format(relationship.key))\n\n                    to_relate = self._fetch_resource(\n                        session, item['type'], item['id'], Permissions.EDIT)\n\n                    rem = to_relate.__mapper__.relationships[remote_side]\n\n                    if rem.direction == MANYTOONE:\n                        check_permission(to_relate, remote_side,\n                                         Permissions.EDIT)\n\n                    else:\n                        check_permission(to_relate, remote_side,\n                                         Permissions.CREATE)\n\n                    setter(resource, to_relate)\n\n            session.add(resource)\n            session.commit()\n\n        except KeyError:\n            raise ValidationError('Incompatible type provided')\n\n        return self.get_relationship(\n            session, {}, model.__jsonapi_type__, resource.id, rel_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute command line helper.", "response": "def call():\n    \"\"\"Execute command line helper.\"\"\"\n    args = get_arguments()\n\n    # Set up logging\n    if args.debug:\n        log_level = logging.DEBUG\n    elif args.quiet:\n        log_level = logging.WARN\n    else:\n        log_level = logging.INFO\n\n    setup_logging(log_level)\n\n    abode = None\n\n    if not args.cache:\n        if not args.username or not args.password:\n            raise Exception(\"Please supply a cache or username and password.\")\n\n    try:\n        # Create abodepy instance.\n        if args.cache and args.username and args.password:\n            abode = abodepy.Abode(username=args.username,\n                                  password=args.password,\n                                  get_devices=True,\n                                  cache_path=args.cache)\n        elif args.cache and not (not args.username or not args.password):\n            abode = abodepy.Abode(get_devices=True,\n                                  cache_path=args.cache)\n        else:\n            abode = abodepy.Abode(username=args.username,\n                                  password=args.password,\n                                  get_devices=True)\n\n        # Output current mode.\n        if args.mode:\n            _LOGGER.info(\"Current alarm mode: %s\", abode.get_alarm().mode)\n\n        # Change system mode.\n        if args.arm:\n            if abode.get_alarm().set_mode(args.arm):\n                _LOGGER.info(\"Alarm mode changed to: %s\", args.arm)\n            else:\n                _LOGGER.warning(\"Failed to change alarm mode to: %s\", args.arm)\n\n        # Set setting\n        for setting in args.set or []:\n            keyval = setting.split(\"=\")\n            if abode.set_setting(keyval[0], keyval[1]):\n                _LOGGER.info(\"Setting %s changed to %s\", keyval[0], keyval[1])\n\n        # Switch on\n        for device_id in args.on or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                if device.switch_on():\n                    _LOGGER.info(\"Switched on device with id: %s\", device_id)\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Switch off\n        for device_id in args.off or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                if device.switch_off():\n                    _LOGGER.info(\"Switched off device with id: %s\", device_id)\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Lock\n        for device_id in args.lock or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                if device.lock():\n                    _LOGGER.info(\"Locked device with id: %s\", device_id)\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Unlock\n        for device_id in args.unlock or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                if device.unlock():\n                    _LOGGER.info(\"Unlocked device with id: %s\", device_id)\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Output Json\n        for device_id in args.json or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                # pylint: disable=protected-access\n                _LOGGER.info(device_id + \" JSON:\\n\" +\n                             json.dumps(device._json_state, sort_keys=True,\n                                        indent=4, separators=(',', ': ')))\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Print\n        def _device_print(dev, append=''):\n            _LOGGER.info(\"%s%s\",\n                         dev.desc, append)\n\n        # Print out all automations\n        if args.automations:\n            for automation in abode.get_automations():\n                _device_print(automation)\n\n        # Enable automation\n        for automation_id in args.activate or []:\n            automation = abode.get_automation(automation_id)\n\n            if automation:\n                if automation.set_active(True):\n                    _LOGGER.info(\n                        \"Activated automation with id: %s\", automation_id)\n            else:\n                _LOGGER.warning(\n                    \"Could not find automation with id: %s\", automation_id)\n\n        # Disable automation\n        for automation_id in args.deactivate or []:\n            automation = abode.get_automation(automation_id)\n\n            if automation:\n                if automation.set_active(False):\n                    _LOGGER.info(\n                        \"Deactivated automation with id: %s\", automation_id)\n            else:\n                _LOGGER.warning(\n                    \"Could not find automation with id: %s\", automation_id)\n\n        # Trigger automation\n        for automation_id in args.trigger or []:\n            automation = abode.get_automation(automation_id)\n\n            if automation:\n                if automation.trigger():\n                    _LOGGER.info(\n                        \"Triggered automation with id: %s\", automation_id)\n            else:\n                _LOGGER.warning(\n                    \"Could not find automation with id: %s\", automation_id)\n\n        # Trigger image capture\n        for device_id in args.capture or []:\n            device = abode.get_device(device_id)\n\n            if device:\n                if device.capture():\n                    _LOGGER.info(\n                        \"Image requested from device with id: %s\", device_id)\n                else:\n                    _LOGGER.warning(\n                        \"Failed to request image from device with id: %s\",\n                        device_id)\n            else:\n                _LOGGER.warning(\"Could not find device with id: %s\", device_id)\n\n        # Save camera image\n        for keyval in args.image or []:\n            devloc = keyval.split(\"=\")\n            device = abode.get_device(devloc[0])\n\n            if device:\n                try:\n                    if (device.refresh_image() and\n                            device.image_to_file(devloc[1])):\n                        _LOGGER.info(\n                            \"Saved image to %s for device id: %s\", devloc[1],\n                            devloc[0])\n                except AbodeException as exc:\n                    _LOGGER.warning(\"Unable to save image: %s\", exc)\n            else:\n                _LOGGER.warning(\n                    \"Could not find device with id: %s\", devloc[0])\n\n        # Print out all devices.\n        if args.devices:\n            for device in abode.get_devices():\n                _device_print(device)\n\n        def _device_callback(dev):\n            _device_print(dev, \", At: \" + time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n        def _timeline_callback(tl_json):\n            event_code = int(tl_json['event_code'])\n            if 5100 <= event_code <= 5199:\n                # Ignore device changes\n                return\n\n            _LOGGER.info(\"%s - %s at %s %s\",\n                         tl_json['event_name'], tl_json['event_type'],\n                         tl_json['date'], tl_json['time'])\n\n        # Print out specific devices by device id.\n        if args.device:\n            for device_id in args.device:\n                device = abode.get_device(device_id)\n\n                if device:\n                    _device_print(device)\n\n                    # Register the specific devices if we decide to listen.\n                    abode.events.add_device_callback(device_id,\n                                                     _device_callback)\n                else:\n                    _LOGGER.warning(\n                        \"Could not find device with id: %s\", device_id)\n\n        # Start device change listener.\n        if args.listen:\n            # If no devices were specified then we listen to all devices.\n            if args.device is None:\n                _LOGGER.info(\"Adding all devices to listener...\")\n\n                for device in abode.get_devices():\n                    abode.events.add_device_callback(device.device_id,\n                                                     _device_callback)\n\n            abode.events.add_timeline_callback(TIMELINE.ALL,\n                                               _timeline_callback)\n\n            _LOGGER.info(\"Listening for device and timeline updates...\")\n            abode.events.start()\n            try:\n                while True:\n                    time.sleep(1)\n            except KeyboardInterrupt:\n                abode.events.stop()\n                _LOGGER.info(\"Device update listening stopped.\")\n    except abodepy.AbodeException as exc:\n        _LOGGER.error(exc)\n    finally:\n        if abode:\n            abode.logout()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map_event_code(event_code):\n    event_code = int(event_code)\n\n    # Honestly, these are just guessing based on the below event list.\n    # It could be wrong, I have no idea.\n    if 1100 <= event_code <= 1199:\n        return ALARM_GROUP\n\n    elif 3100 <= event_code <= 3199:\n        return ALARM_END_GROUP\n\n    elif 1300 <= event_code <= 1399:\n        return PANEL_FAULT_GROUP\n\n    elif 3300 <= event_code <= 3399:\n        return PANEL_RESTORE_GROUP\n\n    elif 1400 <= event_code <= 1499:\n        return DISARM_GROUP\n\n    elif 3400 <= event_code <= 3799:\n        return ARM_GROUP\n\n    elif 1600 <= event_code <= 1699:\n        return TEST_GROUP\n\n    elif 5000 <= event_code <= 5099:\n        return CAPTURE_GROUP\n\n    elif 5100 <= event_code <= 5199:\n        return DEVICE_GROUP\n\n    elif 5200 <= event_code <= 5299:\n        return AUTOMATION_GROUP\n\n    return None", "response": "Map a specific event_code to an event group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nturn the switch on.", "response": "def switch_on(self):\n        \"\"\"Turn the switch on.\"\"\"\n        success = self.set_status(CONST.STATUS_ON_INT)\n\n        if success:\n            self._json_state['status'] = CONST.STATUS_ON\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn the switch off.", "response": "def switch_off(self):\n        \"\"\"Turn the switch off.\"\"\"\n        success = self.set_status(CONST.STATUS_OFF_INT)\n\n        if success:\n            self._json_state['status'] = CONST.STATUS_OFF\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nactivates and deactivate an automation.", "response": "def set_active(self, active):\n        \"\"\"Activate and deactivate an automation.\"\"\"\n        url = CONST.AUTOMATION_EDIT_URL\n        url = url.replace(\n            '$AUTOMATIONID$', self.automation_id)\n\n        self._automation['is_active'] = str(int(active))\n\n        response = self._abode.send_request(\n            method=\"put\", url=url, data=self._automation)\n\n        response_object = json.loads(response.text)\n\n        if isinstance(response_object, (tuple, list)):\n            response_object = response_object[0]\n\n        if (str(response_object['id']) != str(self._automation['id']) or\n                response_object['is_active'] != self._automation['is_active']):\n            raise AbodeException((ERROR.INVALID_AUTOMATION_EDIT_RESPONSE))\n\n        self.update(response_object)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trigger(self, only_manual=True):\n        if not self.is_quick_action and only_manual:\n            raise AbodeException((ERROR.TRIGGER_NON_QUICKACTION))\n\n        url = CONST.AUTOMATION_APPLY_URL\n        url = url.replace(\n            '$AUTOMATIONID$', self.automation_id)\n\n        self._abode.send_request(\n            method=\"put\", url=url, data=self._automation)\n\n        return True", "response": "Trigger a quick - action automation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, automation):\n        self._automation.update(\n            {k: automation[k] for k in automation if self._automation.get(k)})", "response": "Update the internal automation json."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a short description of the automation.", "response": "def desc(self):\n        \"\"\"Get a short description of the automation.\"\"\"\n        # Auto Away (1) - Location - Enabled\n        active = 'inactive'\n        if self.is_active:\n            active = 'active'\n\n        return '{0} (ID: {1}) - {2} - {3}'.format(\n            self.name, self.automation_id, self.type, active)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_numeric_status(self, key):\n        value = self._get_status(key)\n\n        if value and any(i.isdigit() for i in value):\n            return float(re.sub(\"[^0-9.]\", \"\", value))\n        return None", "response": "Extract the numeric value from the statuses object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef temp_unit(self):\n        if CONST.UNIT_FAHRENHEIT in self._get_status(CONST.TEMP_STATUS_KEY):\n            return CONST.UNIT_FAHRENHEIT\n        elif CONST.UNIT_CELSIUS in self._get_status(CONST.TEMP_STATUS_KEY):\n            return CONST.UNIT_CELSIUS\n        return None", "response": "Get unit of temp."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef humidity_unit(self):\n        if CONST.UNIT_PERCENT in self._get_status(CONST.HUMI_STATUS_KEY):\n            return CONST.UNIT_PERCENT\n        return None", "response": "Get unit of humidity."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets unit of lux.", "response": "def lux_unit(self):\n        \"\"\"Get unit of lux.\"\"\"\n        if CONST.UNIT_LUX in self._get_status(CONST.LUX_STATUS_KEY):\n            return CONST.LUX\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_on(self):\n        if self._type == 'Occupancy':\n            return self.status not in CONST.STATUS_ONLINE\n        return self.status not in (CONST.STATUS_OFF, CONST.STATUS_OFFLINE,\n                                   CONST.STATUS_CLOSED)", "response": "Returns True if the current status is on."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning the switch on.", "response": "def switch_on(self):\n        \"\"\"Turn the switch on.\"\"\"\n        success = self.set_status(CONST.STATUS_OPEN_INT)\n\n        if success:\n            self._json_state['status'] = CONST.STATUS_OPEN\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef switch_off(self):\n        success = self.set_status(CONST.STATUS_CLOSED_INT)\n\n        if success:\n            self._json_state['status'] = CONST.STATUS_CLOSED\n\n        return success", "response": "Turn the switch off."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the Origin header.", "response": "def set_origin(self, origin=None):\r\n        \"\"\"Set the Origin header.\"\"\"\r\n        if origin:\r\n            self._origin = origin.encode()\r\n        else:\r\n            self._origin = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the Cookie header.", "response": "def set_cookie(self, cookie=None):\r\n        \"\"\"Set the Cookie header.\"\"\"\r\n        if cookie:\r\n            self._cookie = cookie.encode()\r\n        else:\r\n            self._cookie = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a callback for a SocketIO event.", "response": "def on(self, event_name, callback):\r\n        \"\"\"Register callback for a SocketIO event.\"\"\"\r\n        if not event_name:\r\n            return False\r\n\r\n        _LOGGER.debug(\"Adding callback for event name: %s\", event_name)\r\n\r\n        self._callbacks[event_name].append((callback))\r\n\r\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a thread to handle SocketIO notifications.", "response": "def start(self):\r\n        \"\"\"Start a thread to handle SocketIO notifications.\"\"\"\r\n        if not self._thread:\r\n            _LOGGER.info(\"Starting SocketIO thread...\")\r\n\r\n            self._thread = threading.Thread(target=self._run_socketio_thread,\r\n                                            name='SocketIOThread')\r\n            self._thread.deamon = True\r\n            self._thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntells the SocketIO thread to terminate.", "response": "def stop(self):\r\n        \"\"\"Tell the SocketIO thread to terminate.\"\"\"\r\n        if self._thread:\r\n            _LOGGER.info(\"Stopping SocketIO thread...\")\r\n\r\n            # pylint: disable=W0212\r\n            self._running = False\r\n\r\n            if self._exit_event:\r\n                self._exit_event.set()\r\n\r\n            self._thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a device callback.", "response": "def add_device_callback(self, devices, callback):\n        \"\"\"Register a device callback.\"\"\"\n        if not devices:\n            return False\n\n        if not isinstance(devices, (tuple, list)):\n            devices = [devices]\n\n        for device in devices:\n            # Device may be a device_id\n            device_id = device\n\n            # If they gave us an actual device, get that devices ID\n            if isinstance(device, AbodeDevice):\n                device_id = device.device_id\n\n            # Validate the device is valid\n            if not self._abode.get_device(device_id):\n                raise AbodeException((ERROR.EVENT_DEVICE_INVALID))\n\n            _LOGGER.debug(\n                \"Subscribing to updated for device_id: %s\", device_id)\n\n            self._device_callbacks[device_id].append((callback))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_event_callback(self, event_groups, callback):\n        if not event_groups:\n            return False\n\n        if not isinstance(event_groups, (tuple, list)):\n            event_groups = [event_groups]\n\n        for event_group in event_groups:\n            if event_group not in TIMELINE.ALL_EVENT_GROUPS:\n                raise AbodeException(ERROR.EVENT_GROUP_INVALID,\n                                     TIMELINE.ALL_EVENT_GROUPS)\n\n            _LOGGER.debug(\"Subscribing to event group: %s\", event_group)\n\n            self._event_callbacks[event_group].append((callback))\n\n        return True", "response": "Register callback for a group of timeline events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a callback for a specific timeline event.", "response": "def add_timeline_callback(self, timeline_events, callback):\n        \"\"\"Register a callback for a specific timeline event.\"\"\"\n        if not timeline_events:\n            return False\n\n        if not isinstance(timeline_events, (tuple, list)):\n            timeline_events = [timeline_events]\n\n        for timeline_event in timeline_events:\n            if not isinstance(timeline_event, dict):\n                raise AbodeException((ERROR.EVENT_CODE_MISSING))\n\n            event_code = timeline_event.get('event_code')\n\n            if not event_code:\n                raise AbodeException((ERROR.EVENT_CODE_MISSING))\n\n            _LOGGER.debug(\"Subscribing to timeline event: %s\", timeline_event)\n\n            self._timeline_callbacks[event_code].append((callback))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_socket_started(self):\n        # pylint: disable=W0212\n        cookies = self._abode._get_session().cookies.get_dict()\n        cookie_string = \"; \".join(\n            [str(x) + \"=\" + str(y) for x, y in cookies.items()])\n\n        self._socketio.set_cookie(cookie_string)", "response": "Socket IO startup callback."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _on_mode_change(self, mode):\n        if isinstance(mode, (tuple, list)):\n            mode = mode[0]\n\n        if mode is None:\n            _LOGGER.warning(\"Mode change event with no mode.\")\n            return\n\n        if not mode or mode.lower() not in CONST.ALL_MODES:\n            _LOGGER.warning(\"Mode change event with unknown mode: %s\", mode)\n            return\n\n        _LOGGER.debug(\"Alarm mode change event to: %s\", mode)\n\n        # We're just going to convert it to an Alarm device\n        alarm_device = self._abode.get_alarm(refresh=True)\n\n        # At the time of development, refreshing after mode change notification\n        # didn't seem to get the latest update immediately. As such, we will\n        # force the mode status now to match the notification.\n        # pylint: disable=W0212\n        alarm_device._json_state['mode']['area_1'] = mode\n\n        for callback in self._device_callbacks.get(alarm_device.device_id, ()):\n            _execute_callback(callback, alarm_device)", "response": "Mode change broadcast from Abode SocketIO server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_timeline_update(self, event):\n        if isinstance(event, (tuple, list)):\n            event = event[0]\n\n        event_type = event.get('event_type')\n        event_code = event.get('event_code')\n\n        if not event_type or not event_code:\n            _LOGGER.warning(\"Invalid timeline update event: %s\", event)\n            return\n\n        _LOGGER.debug(\"Timeline event received: %s - %s (%s)\",\n                      event.get('event_name'), event_type, event_code)\n\n        # Compress our callbacks into those that match this event_code\n        # or ones registered to get callbacks for all events\n        codes = (event_code, TIMELINE.ALL['event_code'])\n        all_callbacks = [self._timeline_callbacks[code] for code in codes]\n\n        for callbacks in all_callbacks:\n            for callback in callbacks:\n                _execute_callback(callback, event)\n\n        # Attempt to map the event code to a group and callback\n        event_group = TIMELINE.map_event_code(event_code)\n\n        if event_group:\n            for callback in self._event_callbacks.get(event_group, ()):\n                _execute_callback(callback, event)", "response": "Handle a timeline update event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_device(device_json, abode):\n    type_tag = device_json.get('type_tag')\n\n    if not type_tag:\n        raise AbodeException((ERROR.UNABLE_TO_MAP_DEVICE))\n\n    generic_type = CONST.get_generic_type(type_tag.lower())\n    device_json['generic_type'] = generic_type\n\n    if generic_type == CONST.TYPE_CONNECTIVITY or \\\n       generic_type == CONST.TYPE_MOISTURE or \\\n       generic_type == CONST.TYPE_OPENING:\n        return AbodeBinarySensor(device_json, abode)\n    elif generic_type == CONST.TYPE_CAMERA:\n        return AbodeCamera(device_json, abode)\n    elif generic_type == CONST.TYPE_COVER:\n        return AbodeCover(device_json, abode)\n    elif generic_type == CONST.TYPE_LIGHT:\n        return AbodeLight(device_json, abode)\n    elif generic_type == CONST.TYPE_LOCK:\n        return AbodeLock(device_json, abode)\n    elif generic_type == CONST.TYPE_SWITCH:\n        return AbodeSwitch(device_json, abode)\n    elif generic_type == CONST.TYPE_VALVE:\n        return AbodeValve(device_json, abode)\n    elif generic_type == CONST.TYPE_UNKNOWN_SENSOR:\n        return _new_sensor(device_json, abode)\n\n    return None", "response": "Create a new device object for the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes a full refresh of all devices and automations.", "response": "def refresh(self):\n        \"\"\"Do a full refresh of all devices and automations.\"\"\"\n        self.get_devices(refresh=True)\n        self.get_automations(refresh=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all devices from Abode.", "response": "def get_devices(self, refresh=False, generic_type=None):\n        \"\"\"Get all devices from Abode.\"\"\"\n        if refresh or self._devices is None:\n            if self._devices is None:\n                self._devices = {}\n\n            _LOGGER.info(\"Updating all devices...\")\n            response = self.send_request(\"get\", CONST.DEVICES_URL)\n            response_object = json.loads(response.text)\n\n            if (response_object and\n                    not isinstance(response_object, (tuple, list))):\n                response_object = [response_object]\n\n            _LOGGER.debug(\"Get Devices Response: %s\", response.text)\n\n            for device_json in response_object:\n                # Attempt to reuse an existing device\n                device = self._devices.get(device_json['id'])\n\n                # No existing device, create a new one\n                if device:\n                    device.update(device_json)\n                else:\n                    device = new_device(device_json, self)\n\n                    if not device:\n                        _LOGGER.debug(\n                            \"Skipping unknown device: %s\",\n                            device_json)\n\n                        continue\n\n                    self._devices[device.device_id] = device\n\n            # We will be treating the Abode panel itself as an armable device.\n            panel_response = self.send_request(\"get\", CONST.PANEL_URL)\n            panel_json = json.loads(panel_response.text)\n\n            self._panel.update(panel_json)\n\n            _LOGGER.debug(\"Get Mode Panel Response: %s\", response.text)\n\n            alarm_device = self._devices.get(CONST.ALARM_DEVICE_ID + '1')\n\n            if alarm_device:\n                alarm_device.update(panel_json)\n            else:\n                alarm_device = ALARM.create_alarm(panel_json, self)\n                self._devices[alarm_device.device_id] = alarm_device\n\n        if generic_type:\n            devices = []\n            for device in self._devices.values():\n                if (device.generic_type is not None and\n                        device.generic_type in generic_type):\n                    devices.append(device)\n            return devices\n\n        return list(self._devices.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a single device.", "response": "def get_device(self, device_id, refresh=False):\n        \"\"\"Get a single device.\"\"\"\n        if self._devices is None:\n            self.get_devices()\n            refresh = False\n\n        device = self._devices.get(device_id)\n\n        if device and refresh:\n            device.refresh()\n\n        return device"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a single automation.", "response": "def get_automation(self, automation_id, refresh=False):\n        \"\"\"Get a single automation.\"\"\"\n        if self._automations is None:\n            self.get_automations()\n            refresh = False\n\n        automation = self._automations.get(str(automation_id))\n\n        if automation and refresh:\n            automation.refresh()\n\n        return automation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_alarm(self, area='1', refresh=False):\n        if self._devices is None:\n            self.get_devices()\n            refresh = False\n\n        return self.get_device(CONST.ALARM_DEVICE_ID + area, refresh)", "response": "Shortcut method to get the alarm device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the default mode when alarms are turned on.", "response": "def set_default_mode(self, default_mode):\n        \"\"\"Set the default mode when alarms are turned 'on'.\"\"\"\n        if default_mode.lower() not in (CONST.MODE_AWAY, CONST.MODE_HOME):\n            raise AbodeException(ERROR.INVALID_DEFAULT_ALARM_MODE)\n\n        self._default_alarm_mode = default_mode.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_setting(self, setting, value, area='1', validate_value=True):\n        setting = setting.lower()\n\n        if setting not in CONST.ALL_SETTINGS:\n            raise AbodeException(ERROR.INVALID_SETTING, CONST.ALL_SETTINGS)\n\n        if setting in CONST.PANEL_SETTINGS:\n            url = CONST.SETTINGS_URL\n            data = self._panel_settings(setting, value, validate_value)\n        elif setting in CONST.AREA_SETTINGS:\n            url = CONST.AREAS_URL\n            data = self._area_settings(area, setting, value, validate_value)\n        elif setting in CONST.SOUND_SETTINGS:\n            url = CONST.SOUNDS_URL\n            data = self._sound_settings(area, setting, value, validate_value)\n        elif setting in CONST.SIREN_SETTINGS:\n            url = CONST.SIREN_URL\n            data = self._siren_settings(setting, value, validate_value)\n\n        return self.send_request(method=\"put\", url=url, data=data)", "response": "Set an abode system setting to a given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _area_settings(area, setting, value, validate_value):\n        if validate_value:\n            # Exit delay has some specific limitations apparently\n            if (setting == CONST.SETTING_EXIT_DELAY_AWAY\n                    and value not in CONST.VALID_SETTING_EXIT_AWAY):\n                raise AbodeException(ERROR.INVALID_SETTING_VALUE,\n                                     CONST.VALID_SETTING_EXIT_AWAY)\n            elif value not in CONST.ALL_SETTING_ENTRY_EXIT_DELAY:\n                raise AbodeException(ERROR.INVALID_SETTING_VALUE,\n                                     CONST.ALL_SETTING_ENTRY_EXIT_DELAY)\n\n        return {'area': area, setting: value}", "response": "Will validate area settings and values returns data packet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_request(self, method, url, headers=None,\n                     data=None, is_retry=False):\n        \"\"\"Send requests to Abode.\"\"\"\n        if not self._token:\n            self.login()\n\n        if not headers:\n            headers = {}\n\n        headers['Authorization'] = 'Bearer ' + self._oauth_token\n        headers['ABODE-API-KEY'] = self._token\n\n        try:\n            response = getattr(self._session, method)(\n                url, headers=headers, json=data)\n\n            if response and response.status_code < 400:\n                return response\n        except RequestException:\n            _LOGGER.info(\"Abode connection reset...\")\n\n        if not is_retry:\n            # Delete our current token and try again -- will force a login\n            # attempt.\n            self._token = None\n\n            return self.send_request(method, url, headers, data, True)\n\n        raise AbodeException((ERROR.REQUEST))", "response": "Send a request to Abode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_cache(self):\n        if not self._disable_cache and os.path.exists(self._cache_path):\n            _LOGGER.debug(\"Cache found at: %s\", self._cache_path)\n            loaded_cache = UTILS.load_cache(self._cache_path)\n\n            UTILS.update(self._cache, loaded_cache)\n\n        self._save_cache()", "response": "Load existing cache and merge for updating if required."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrigger a cache save.", "response": "def _save_cache(self):\n        \"\"\"Trigger a cache save.\"\"\"\n        if not self._disable_cache:\n            UTILS.save_cache(self._cache, self._cache_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_generic_type(type_tag):\n    return {\n        # Alarm\n        DEVICE_ALARM: TYPE_ALARM,\n\n        # Binary Sensors - Connectivity\n        DEVICE_GLASS_BREAK: TYPE_CONNECTIVITY,\n        DEVICE_KEYPAD: TYPE_CONNECTIVITY,\n        DEVICE_REMOTE_CONTROLLER: TYPE_CONNECTIVITY,\n        DEVICE_SIREN: TYPE_CONNECTIVITY,\n        DEVICE_STATUS_DISPLAY: TYPE_CONNECTIVITY,\n\n        # Binary Sensors - Opening\n        DEVICE_DOOR_CONTACT: TYPE_OPENING,\n\n        # Cameras\n        DEVICE_MOTION_CAMERA: TYPE_CAMERA,\n        DEVICE_MOTION_VIDEO_CAMERA: TYPE_CAMERA,\n        DEVICE_IP_CAM: TYPE_CAMERA,\n        DEVICE_OUTDOOR_MOTION_CAMERA: TYPE_CAMERA,\n\n        # Covers\n        DEVICE_SECURE_BARRIER: TYPE_COVER,\n\n        # Lights (Dimmers)\n        DEVICE_DIMMER: TYPE_LIGHT,\n        DEVICE_DIMMER_METER: TYPE_LIGHT,\n        DEVICE_HUE: TYPE_LIGHT,\n\n        # Locks\n        DEVICE_DOOR_LOCK: TYPE_LOCK,\n\n        # Moisture\n        DEVICE_WATER_SENSOR: TYPE_CONNECTIVITY,\n\n        # Switches\n        DEVICE_SWITCH: TYPE_SWITCH,\n        DEVICE_NIGHT_SWITCH: TYPE_SWITCH,\n        DEVICE_POWER_SWITCH_SENSOR: TYPE_SWITCH,\n        DEVICE_POWER_SWITCH_METER: TYPE_SWITCH,\n\n        # Water Valve\n        DEVICE_VALVE: TYPE_VALVE,\n\n        # Unknown Sensors\n        # More data needed to determine type\n        DEVICE_ROOM_SENSOR: TYPE_UNKNOWN_SENSOR,\n        DEVICE_TEMPERATURE_SENSOR: TYPE_UNKNOWN_SENSOR,\n        DEVICE_MULTI_SENSOR: TYPE_UNKNOWN_SENSOR,\n        DEVICE_PIR: TYPE_UNKNOWN_SENSOR,\n        DEVICE_POVS: TYPE_UNKNOWN_SENSOR,\n    }.get(type_tag.lower(), None)", "response": "Map type tag to generic type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_status(self, status):\n        if self._json_state['control_url']:\n            url = CONST.BASE_URL + self._json_state['control_url']\n\n            status_data = {\n                'status': str(status)\n            }\n\n            response = self._abode.send_request(\n                method=\"put\", url=url, data=status_data)\n            response_object = json.loads(response.text)\n\n            _LOGGER.debug(\"Set Status Response: %s\", response.text)\n\n            if response_object['id'] != self.device_id:\n                raise AbodeException((ERROR.SET_STATUS_DEV_ID))\n\n            if response_object['status'] != str(status):\n                raise AbodeException((ERROR.SET_STATUS_STATE))\n\n            # Note: Status result is of int type, not of new status of device.\n            # Seriously, why would you do that?\n            # So, can't set status here must be done at device level.\n\n            _LOGGER.info(\"Set device %s status to: %s\", self.device_id, status)\n\n            return True\n\n        return False", "response": "Set status of the current device."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes the devices json object data.", "response": "def refresh(self, url=CONST.DEVICE_URL):\n        \"\"\"Refresh the devices json object data.\n\n        Only needed if you're not using the notification service.\n        \"\"\"\n        url = url.replace('$DEVID$', self.device_id)\n\n        response = self._abode.send_request(method=\"get\", url=url)\n        response_object = json.loads(response.text)\n\n        _LOGGER.debug(\"Device Refresh Response: %s\", response.text)\n\n        if response_object and not isinstance(response_object, (tuple, list)):\n            response_object = [response_object]\n\n        for device in response_object:\n            self.update(device)\n\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, json_state):\n        self._json_state.update(\n            {k: json_state[k] for k in json_state if self._json_state.get(k)})\n        self._update_name()", "response": "Update the json data from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the name from _json_state with a sensible default.", "response": "def _update_name(self):\n        \"\"\"Set the device name from _json_state, with a sensible default.\"\"\"\n        self._name = self._json_state.get('name')\n        if not self._name:\n            self._name = self.type + ' ' + self.device_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets color temp for a specific device.", "response": "def set_color_temp(self, color_temp):\n        \"\"\"Set device color.\"\"\"\n        if self._json_state['control_url']:\n            url = CONST.INTEGRATIONS_URL + self._device_uuid\n\n            color_data = {\n                'action': 'setcolortemperature',\n                'colorTemperature': int(color_temp)\n            }\n\n            response = self._abode.send_request(\"post\", url, data=color_data)\n            response_object = json.loads(response.text)\n\n            _LOGGER.debug(\"Set Color Temp Response: %s\", response.text)\n\n            if response_object['idForPanel'] != self.device_id:\n                raise AbodeException((ERROR.SET_STATUS_DEV_ID))\n\n            if response_object['colorTemperature'] != int(color_temp):\n                _LOGGER.warning(\n                    (\"Set color temp mismatch for device %s. \"\n                     \"Request val: %s, Response val: %s \"),\n                    self.device_id, color_temp,\n                    response_object['colorTemperature'])\n\n            self.update(response_object)\n\n            _LOGGER.info(\"Set device %s color_temp to: %s\",\n                         self.device_id, color_temp)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_color(self):\n        if (self.get_value(CONST.STATUSES_KEY).get('color_mode')\n                == str(CONST.COLOR_MODE_ON)):\n            return True\n        return False", "response": "Check if the device is using color mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting a new camera image.", "response": "def capture(self):\n        \"\"\"Request a new camera image.\"\"\"\n        url = str.replace(CONST.CAMS_ID_CAPTURE_URL,\n                          '$DEVID$', self.device_id)\n\n        try:\n            response = self._abode.send_request(\"put\", url)\n\n            _LOGGER.debug(\"Capture image response: %s\", response.text)\n\n            return True\n        except AbodeException as exc:\n            _LOGGER.warning(\"Failed to capture image: %s\", exc)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refresh_image(self):\n        url = str.replace(CONST.TIMELINE_IMAGES_ID_URL,\n                          '$DEVID$', self.device_id)\n        response = self._abode.send_request(\"get\", url)\n\n        _LOGGER.debug(\"Get image response: %s\", response.text)\n\n        return self.update_image_location(json.loads(response.text))", "response": "Get the most recent camera image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_image_location(self, timeline_json):\n        if not timeline_json:\n            return False\n\n        # If we get a list of objects back (likely)\n        # then we just want the first one as it should be the \"newest\"\n        if isinstance(timeline_json, (tuple, list)):\n            timeline_json = timeline_json[0]\n\n        # Verify that the event code is of the \"CAPTURE IMAGE\" event\n        event_code = timeline_json.get('event_code')\n        if event_code != TIMELINE.CAPTURE_IMAGE['event_code']:\n            raise AbodeException((ERROR.CAM_TIMELINE_EVENT_INVALID))\n\n        # The timeline response has an entry for \"file_path\" that acts as the\n        # location of the image within the Abode servers.\n        file_path = timeline_json.get('file_path')\n        if not file_path:\n            raise AbodeException((ERROR.CAM_IMAGE_REFRESH_NO_FILE))\n\n        # Perform a \"head\" request for the image and look for a\n        # 302 Found response\n        url = CONST.BASE_URL + file_path\n        response = self._abode.send_request(\"head\", url)\n\n        if response.status_code != 302:\n            _LOGGER.warning(\"Unexected response code %s with body: %s\",\n                            str(response.status_code), response.text)\n            raise AbodeException((ERROR.CAM_IMAGE_UNEXPECTED_RESPONSE))\n\n        # The response should have a location header that is the actual\n        # location of the image stored on AWS\n        location = response.headers.get('location')\n        if not location:\n            raise AbodeException((ERROR.CAM_IMAGE_NO_LOCATION_HEADER))\n\n        self._image_url = location\n\n        return True", "response": "Update the image location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef image_to_file(self, path, get_image=True):\n        if not self.image_url or get_image:\n            if not self.refresh_image():\n                return False\n\n        response = requests.get(self.image_url, stream=True)\n\n        if response.status_code != 200:\n            _LOGGER.warning(\n                \"Unexpected response code %s when requesting image: %s\",\n                str(response.status_code), response.text)\n            raise AbodeException((ERROR.CAM_IMAGE_REQUEST_INVALID))\n\n        with open(path, 'wb') as imgfile:\n            copyfileobj(response.raw, imgfile)\n\n        return True", "response": "Write the image to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_alarm(panel_json, abode, area='1'):\n    panel_json['name'] = CONST.ALARM_NAME\n    panel_json['id'] = CONST.ALARM_DEVICE_ID + area\n    panel_json['type'] = CONST.ALARM_TYPE\n    panel_json['type_tag'] = CONST.DEVICE_ALARM\n    panel_json['generic_type'] = CONST.TYPE_ALARM\n\n    return AbodeAlarm(panel_json, abode, area)", "response": "Create a new alarm device from a panel response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets Abode alarm mode.", "response": "def set_mode(self, mode):\n        \"\"\"Set Abode alarm mode.\"\"\"\n        if not mode:\n            raise AbodeException(ERROR.MISSING_ALARM_MODE)\n        elif mode.lower() not in CONST.ALL_MODES:\n            raise AbodeException(ERROR.INVALID_ALARM_MODE, CONST.ALL_MODES)\n\n        mode = mode.lower()\n\n        response = self._abode.send_request(\n            \"put\", CONST.get_panel_mode_url(self._area, mode))\n\n        _LOGGER.debug(\"Set Alarm Home Response: %s\", response.text)\n\n        response_object = json.loads(response.text)\n\n        if response_object['area'] != self._area:\n            raise AbodeException(ERROR.SET_MODE_AREA)\n\n        if response_object['mode'] != mode:\n            raise AbodeException(ERROR.SET_MODE_MODE)\n\n        self._json_state['mode'][(self.device_id)] = response_object['mode']\n\n        _LOGGER.info(\"Set alarm %s mode to: %s\",\n                     self._device_id, response_object['mode'])\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrefresh the alarm device.", "response": "def refresh(self, url=CONST.PANEL_URL):\n        \"\"\"Refresh the alarm device.\"\"\"\n        response_object = AbodeDevice.refresh(self, url)\n        # pylint: disable=W0212\n        self._abode._panel.update(response_object[0])\n\n        return response_object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving cookies to a file.", "response": "def save_cache(data, filename):\n    \"\"\"Save cookies to a file.\"\"\"\n    with open(filename, 'wb') as handle:\n        pickle.dump(data, handle)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndistribute a list of weights, a dictionary of weights or a list of tuples containing weights to a fixed number of bins while trying to keep the weight distribution constant. INPUT: --- d: list containing weights, OR dictionary where each (key,value)-pair carries the weight as value, OR list of tuples where one entry in the tuple is the weight. The position of this weight has to be given in optional variable weight_pos optional: ~~~ weight_pos: int -- if d is a list of tuples, this integer number gives the position of the weight in a tuple ~~~ lower_bound: weights under this bound are not considered ~~~ upper_bound: weights exceeding this bound are not considered", "response": "def to_constant_bin_number(d,N_bin,weight_pos=None,lower_bound=None,upper_bound=None):\n    '''\n    Distributes a list of weights, a dictionary of weights or a list of tuples containing weights\n    to a fixed number of bins while trying to keep the weight distribution constant.\n    INPUT:\n    --- d: list containing weights, \n           OR dictionary where each (key,value)-pair carries the weight as value,\n           OR list of tuples where one entry in the tuple is the weight. The position of \n              this weight has to be given in optional variable weight_pos\n         \n    optional:\n    ~~~ weight_pos: int -- if d is a list of tuples, this integer number gives the position of the weight in a tuple\n    ~~~ lower_bound: weights under this bound are not considered\n    ~~~ upper_bound: weights exceeding this bound are not considered\n    '''\n\n    #define functions for the applying the bounds\n    if lower_bound is not None and upper_bound is not None and lower_bound<upper_bound:\n        get_valid_weight_ndcs = lambda a: np.nonzero(np.logical_and(a>lower_bound,a<upper_bound))[0]\n    elif lower_bound is not None:\n        get_valid_weight_ndcs = lambda a: np.nonzero(a>lower_bound)[0]\n    elif upper_bound is not None:\n        get_valid_weight_ndcs = lambda a: np.nonzero(a<upper_bound)[0]\n    elif lower_bound is None and upper_bound is None:\n        get_valid_weight_ndcs = lambda a: np.arange(len(a),dtype=int)\n    elif lower_bound>=upper_bound:\n        raise Exception(\"lower_bound is greater or equal to upper_bound\")\n    \n    isdict = isinstance(d,dict)\n    is_tuple_list = not isdict and hasattr(d[0],'__len__')\n\n    if is_tuple_list:\n        if weight_pos is not None:\n\n            new_dict = { i: tup for i,tup in enumerate(d) }\n            d = { i: tup[weight_pos] for i,tup in enumerate(d) }\n            isdict = True\n        else:\n            raise Exception(\"no weight axis provided for tuple list\")\n\n\n    if isdict:\n\n        #get keys and values (weights)\n        keys_vals = d.items()\n        keys = np.array([ k for k,v in keys_vals ])\n        vals = np.array([ v for k,v in keys_vals ])\n\n        #sort weights decreasingly\n        ndcs = np.argsort(vals)[::-1]\n\n        weights = vals[ndcs]\n        keys = keys[ndcs]\n\n        bins = [ {} for i in range(N_bin) ]\n    else:\n        weights = np.sort(np.array(d))[::-1]\n        bins = [ [] for i in range(N_bin) ]\n\n    #find the valid indices\n    valid_ndcs = get_valid_weight_ndcs(weights)\n    weights = weights[valid_ndcs]\n\n    if isdict:\n        keys = keys[valid_ndcs]\n\n    #the total volume is the sum of all weights\n    V_total = weights.sum()\n\n    #the first estimate of the maximum bin volume is \n    #the total volume divided to all bins\n    V_bin_max = V_total / float(N_bin)\n    \n    #prepare array containing the current weight of the bins\n    weight_sum = np.zeros(N_bin)\n\n    #iterate through the weight list, starting with heaviest\n    for item,weight in enumerate(weights):\n        \n        if isdict:\n            key = keys[item]\n\n        #put next value in bin with lowest weight sum\n        b = np.argmin(weight_sum)\n\n        #calculate new weight of this bin\n        new_weight_sum = weight_sum[b] + weight\n\n        found_bin = False\n        while not found_bin:\n\n            #if this weight fits in the bin\n            if new_weight_sum < V_bin_max:\n\n                #...put it in \n                if isdict:\n                    bins[b][key] = weight\n                else:\n                    bins[b].append(weight)\n\n                #increase weight sum of the bin and continue with\n                #next item \n                weight_sum[b] = new_weight_sum\n                found_bin = True\n\n            else:\n                #if not, increase the max volume by the sum of\n                #the rest of the bins per bin\n                V_bin_max += np.sum(weights[item:]) / float(N_bin)\n\n    if not is_tuple_list:\n        return bins\n    else:\n        new_bins = []\n        for b in range(N_bin):\n            new_bins.append([])\n            for key in bins[b]:\n                new_bins[b].append(new_dict[key])\n        return new_bins"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_valid_device(self, path, run_as_root=True):\n        if self.im_root:\n            try:\n                with open(path, 'r') as f:\n                    f.read(4096)\n            except Exception:\n                return False\n            return True\n\n        try:\n            self._execute('dd', 'if=' + path, 'of=/dev/null', 'bs=4096',\n                          'count=1', root_helper=self._root_helper,\n                          run_as_root=True)\n        except putils.ProcessExecutionError:\n            return False\n        return True", "response": "Verify an existing RBD handle is connected and valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_constant_volume(d,V_max,weight_pos=None,lower_bound=None,upper_bound=None):\n    '''\n    Distributes a list of weights, a dictionary of weights or a list of tuples containing weights\n    to a minimal number of bins which have a fixed volume.\n    INPUT:\n    --- d: list containing weights, \n           OR dictionary where each (key,value)-pair carries the weight as value,\n           OR list of tuples where one entry in the tuple is the weight. The position of \n              this weight has to be given in optional variable weight_pos\n         \n    optional:\n    ~~~ weight_pos: int -- if d is a list of tuples, this integer number gives the position of the weight in a tuple\n    ~~~ lower_bound: weights under this bound are not considered\n    ~~~ upper_bound: weights exceeding this bound are not considered\n    '''\n\n    #define functions for the applying the bounds\n    if lower_bound is not None and upper_bound is not None and lower_bound<upper_bound:\n        get_valid_weight_ndcs = lambda a: np.nonzero(np.logical_and(a>lower_bound,a<upper_bound))[0]\n    elif lower_bound is not None:\n        get_valid_weight_ndcs = lambda a: np.nonzero(a>lower_bound)[0]\n    elif upper_bound is not None:\n        get_valid_weight_ndcs = lambda a: np.nonzero(a<upper_bound)[0]\n    elif lower_bound is None and upper_bound is None:\n        get_valid_weight_ndcs = lambda a: np.arange(len(a),dtype=int)\n    elif lower_bound>=upper_bound:\n        raise Exception(\"lower_bound is greater or equal to upper_bound\")\n\n    isdict = isinstance(d,dict)\n    is_tuple_list = (not isdict) and (hasattr(d[0],'__len__'))\n\n    if is_tuple_list:\n        if weight_pos is not None:\n\n            new_dict = { i: tup for i,tup in enumerate(d) }\n            d = { i: tup[weight_pos] for i,tup in enumerate(d) }\n            isdict = True\n        else:\n            raise Exception(\"no weight axis provided for tuple list\")\n\n    if isdict:\n\n        #get keys and values (weights)\n        keys_vals = d.items()\n        keys = np.array([ k for k,v in keys_vals ])\n        vals = np.array([ v for k,v in keys_vals ])\n\n        #sort weights decreasingly\n        ndcs = np.argsort(vals)[::-1]\n\n        weights = vals[ndcs]\n        keys = keys[ndcs]\n\n        bins = [ {} ]\n    else:\n        weights = np.sort(np.array(d))[::-1]\n        bins = [ [] ]\n\n    #find the valid indices\n    valid_ndcs = get_valid_weight_ndcs(weights)\n    weights = weights[valid_ndcs]\n\n    if isdict:\n        keys = keys[valid_ndcs]\n\n    #the total volume is the sum of all weights\n    V_total = weights.sum()\n\n    #prepare array containing the current weight of the bins\n    weight_sum = np.array([ 0. ])\n\n    #iterate through the weight list, starting with heaviest\n    for item,weight in enumerate(weights):\n        \n        if isdict:\n            key = keys[item]\n\n        #find candidate bins where the weight might fit\n        candidate_bins = np.where(weight_sum+weight <= V_max)[0]\n\n        # if there are candidates where it fits\n        if len(candidate_bins)>0:\n\n            # find the fullest bin where this item fits and assign it\n            candidate_index = np.argmax(weight_sum[candidate_bins])\n            b = candidate_bins[candidate_index]\n\n        #if this weight doesn't fit in any existent bin\n        else:\n            # open a new bin\n            b = len(weight_sum)\n            weight_sum = np.append(weight_sum, 0.)\n            if isdict:\n                bins.append({})\n            else:\n                bins.append([])\n\n        #put it in \n        if isdict:\n            bins[b][key] = weight\n        else:\n            bins[b].append(weight)\n\n        #increase weight sum of the bin and continue with\n        #next item \n        weight_sum[b] += weight\n\n    if not is_tuple_list:\n        return bins\n    else:\n        new_bins = []\n        for b in range(len(bins)):\n            new_bins.append([])\n            for key in bins[b]:\n                new_bins[b].append(new_dict[key])\n        return new_bins", "response": "Converts a list of weights a dictionary of weights or a list of tuples containing weights and V_max to a minimal number of bins which have a fixed volume."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup(config):\n    if config is None:\n        config = {}\n    else:\n        config = config.copy()\n\n    # Prevent driver dynamic loading clearing configuration options\n    volume_cmd.CONF._ConfigOpts__cache = MyDict()\n\n    # Default configuration is using memory storage\n    storage = config.pop('storage', None) or DEFAULT_STORAGE\n    if isinstance(storage, base.PersistenceDriverBase):\n        return storage\n\n    if inspect.isclass(storage) and issubclass(storage,\n                                               base.PersistenceDriverBase):\n        return storage(**config)\n\n    if not isinstance(storage, six.string_types):\n        raise exception.InvalidPersistence(storage)\n\n    persistence_driver = driver.DriverManager(\n        namespace='cinderlib.persistence.storage',\n        name=storage,\n        invoke_on_load=True,\n        invoke_kwds=config,\n    )\n    return persistence_driver.driver", "response": "Setup persistence to be used in cinderlib."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting legacy stats to new stats with pools key.", "response": "def _transform_legacy_stats(self, stats):\n        \"\"\"Convert legacy stats to new stats with pools key.\"\"\"\n        # Fill pools for legacy driver reports\n        if stats and 'pools' not in stats:\n            pool = stats.copy()\n            pool['pool_name'] = self.id\n            for key in ('driver_version', 'shared_targets',\n                        'sparse_copy_volume', 'storage_protocol',\n                        'vendor_name', 'volume_backend_name'):\n                pool.pop(key, None)\n            stats['pools'] = [pool]\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse in - memory file to update OSLO configuration used by Cinder.", "response": "def _update_cinder_config(cls):\n        \"\"\"Parse in-memory file to update OSLO configuration used by Cinder.\"\"\"\n        cls._config_string_io.seek(0)\n        cls._parser.write(cls._config_string_io)\n\n        # Check if we have any multiopt\n        cls._config_string_io.seek(0)\n        current_cfg = cls._config_string_io.read()\n        if '\\n\\t' in current_cfg:\n            cls._config_string_io.seek(0)\n            cls._config_string_io.write(current_cfg.replace('\\n\\t', '\\n'))\n\n        cls._config_string_io.seek(0)\n        cfg.CONF.reload_config_files()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_cinder_config(cls, host, locks_path, cinder_config_params):\n        cfg.CONF.set_default('state_path', os.getcwd())\n        cfg.CONF.set_default('lock_path', '$state_path', 'oslo_concurrency')\n\n        cls._parser = six.moves.configparser.SafeConfigParser()\n        cls._parser.set('DEFAULT', 'enabled_backends', '')\n\n        if locks_path:\n            cls._parser.add_section('oslo_concurrency')\n            cls._parser.set('oslo_concurrency', 'lock_path', locks_path)\n            cls._parser.add_section('coordination')\n            cls._parser.set('coordination',\n                            'backend_url',\n                            'file://' + locks_path)\n        if host:\n            cls._parser.set('DEFAULT', 'host', host)\n\n        # All other configuration options go into the DEFAULT section\n        cls.__set_parser_kv(cinder_config_params, 'DEFAULT')\n\n        # We replace the OSLO's default parser to read from a StringIO instead\n        # of reading from a file.\n        cls._config_string_io = six.moves.StringIO()\n        cfg.ConfigParser.parse = six.create_unbound_method(cls._config_parse,\n                                                           cfg.ConfigParser)\n\n        # Replace command line arg parser so we ignore caller's args\n        cfg._CachedArgumentParser.parse_args = lambda *a, **kw: None\n\n        # Update the configuration with the options we have configured\n        cfg.CONF(project='cinder', version=cinderlib.__version__,\n                 default_config_files=['in_memory_file'])\n        cls._update_cinder_config()", "response": "Setup the parser with all the known Cinder configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with driver classes names as keys.", "response": "def list_supported_drivers():\n        \"\"\"Returns dictionary with driver classes names as keys.\"\"\"\n\n        def convert_oslo_config(oslo_options):\n            options = []\n            for opt in oslo_options:\n                tmp_dict = {k: str(v) for k, v in vars(opt).items()\n                            if not k.startswith('_')}\n                options.append(tmp_dict)\n            return options\n\n        def list_drivers(queue):\n            cwd = os.getcwd()\n            # Go to the parent directory directory where Cinder is installed\n            os.chdir(utils.__file__.rsplit(os.sep, 2)[0])\n            try:\n                drivers = cinder_interface_util.get_volume_drivers()\n                mapping = {d.class_name: vars(d) for d in drivers}\n                # Drivers contain class instances which are not serializable\n                for driver in mapping.values():\n                    driver.pop('cls', None)\n                    if 'driver_options' in driver:\n                        driver['driver_options'] = convert_oslo_config(\n                            driver['driver_options'])\n            finally:\n                os.chdir(cwd)\n            queue.put(mapping)\n\n        # Use a different process to avoid having all driver classes loaded in\n        # memory during our execution.\n        queue = multiprocessing.Queue()\n        p = multiprocessing.Process(target=list_drivers, args=(queue,))\n        p.start()\n        result = queue.get()\n        p.join()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload any json serialized cinderlib object.", "response": "def load(json_src, save=False):\n    \"\"\"Load any json serialized cinderlib object.\"\"\"\n    if isinstance(json_src, six.string_types):\n        json_src = json_lib.loads(json_src)\n\n    if isinstance(json_src, list):\n        return [getattr(objects, obj['class']).load(obj, save)\n                for obj in json_src]\n\n    return getattr(objects, json_src['class']).load(json_src, save)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(server_context, domain_definition, container_path=None):\n    # type: (ServerContext, dict, str) -> Domain\n    \"\"\"\n    Create a domain\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param domain_definition: A domain definition.\n    :param container_path: labkey container path if not already set in context\n    :return: Domain\n    \"\"\"\n    url = server_context.build_url('property', 'createDomain.api', container_path=container_path)\n\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    domain = None\n\n    raw_domain = server_context.make_request(url, json_dumps(domain_definition), headers=headers)\n\n    if raw_domain is not None:\n        domain = Domain.from_data(raw_domain)\n\n    return domain", "response": "Creates a domain in the server context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop(server_context, schema_name, query_name, container_path=None):\n    # type: (ServerContext, str, str, str) -> dict\n    \"\"\"\n    Delete a domain\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of table\n    :param query_name: table name of domain to drop\n    :param container_path: labkey container path if not already set in context\n    :return:\n    \"\"\"\n    url = server_context.build_url('property', 'deleteDomain.api', container_path=container_path)\n\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    payload = {\n        'schemaName': schema_name,\n        'queryName': query_name\n    }\n\n    return server_context.make_request(url, json_dumps(payload), headers=headers)", "response": "Delete a domain from LabKey."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(server_context, schema_name, query_name, container_path=None):\n    # type: (ServerContext, str, str, str) -> Domain\n    \"\"\"\n    Gets a domain design\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of table\n    :param query_name: table name of domain to get\n    :param container_path: labkey container path if not already set in context\n    :return: Domain\n    \"\"\"\n    url = server_context.build_url('property', 'getDomain.api', container_path=container_path)\n\n    payload = {\n        'schemaName': schema_name,\n        'queryName': query_name\n    }\n\n    domain = None\n\n    raw_domain = server_context.make_request(url, payload, method='GET')\n    if raw_domain is not None:\n        domain = Domain.from_data(raw_domain)\n\n    return domain", "response": "Gets a domain from a LabKey table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninferring fields for a domain from a file.", "response": "def infer_fields(server_context, data_file, container_path=None):\n    # type: (ServerContext, any) -> List[PropertyDescriptor]\n    \"\"\"\n    Infer fields for a domain from a file\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param data_file: the data file from which to determine the fields shape\n    :param container_path: labkey container path if not already set in context\n    :return:\n    \"\"\"\n    url = server_context.build_url('property', 'inferDomain.api', container_path=container_path)\n\n    raw_infer = server_context.make_request(url, None, file_payload={\n        'inferfile': data_file\n    })\n\n    fields = None\n    if 'fields' in raw_infer:\n        fields = []\n        for f in raw_infer['fields']:\n            fields.append(PropertyDescriptor.from_data(f))\n\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(server_context, schema_name, query_name, domain, container_path=None):\n    # type: (ServerContext, str, str, Domain, str) -> None\n    \"\"\"\n    Saves the provided domain design\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of domain\n    :param query_name: query name of domain\n    :param domain: Domain to save\n    :param container_path: labkey container path if not already set in context\n    :return:\n    \"\"\"\n    url = server_context.build_url('property', 'saveDomain.api', container_path=container_path)\n\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    payload = {\n        'domainDesign': domain.to_json(),\n        'queryName': query_name,\n        'schemaName': schema_name\n    }\n\n    return server_context.make_request(url, json_dumps(payload), headers=headers)", "response": "Saves the provided domain design to the provided labkey container path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef memoize(func):\n\n    def model(cls, energy, *args, **kwargs):\n        try:\n            memoize = cls._memoize\n            cache = cls._cache\n            queue = cls._queue\n        except AttributeError:\n            memoize = False\n\n        if memoize:\n            # Allow for dicts or tables with energy column, Quantity array or\n            # Quantity scalar\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\n                        \"ignore\",\n                        getattr(np, \"VisibleDeprecationWarning\", None),\n                    )\n                    energy = u.Quantity(energy[\"energy\"])\n            except (TypeError, ValueError, IndexError):\n                pass\n\n            try:\n                # tostring is 10 times faster than str(array).encode()\n                bstr = energy.value.tostring()\n            except AttributeError:\n                # scalar Quantity\n                bstr = str(energy.value).encode()\n\n            data = [hashlib.sha256(bstr).hexdigest()]\n\n            data.append(energy.unit.to_string())\n            data.append(str(kwargs.get(\"distance\", 0)))\n\n            if args:\n                data.append(str(args))\n            if hasattr(cls, \"particle_distribution\"):\n                models = [cls, cls.particle_distribution]\n            else:\n                models = [cls]\n            for model in models:\n                if hasattr(model, \"param_names\"):\n                    for par in model.param_names:\n                        data.append(str(getattr(model, par)))\n\n            token = \"\".join(data)\n            digest = hashlib.sha256(token.encode()).hexdigest()\n\n            if digest in cache:\n                return cache[digest]\n\n        result = func(cls, energy, *args, **kwargs)\n\n        if memoize:\n            # remove first item in queue and remove from cache\n            if len(queue) > 16:\n                key = queue.pop(0)\n                cache.pop(key, None)\n            # save last result to cache\n            queue.append(digest)\n            cache[digest] = result\n\n        return result\n\n    model.__name__ = func.__name__\n    model.__doc__ = func.__doc__\n\n    return model", "response": "Decorator for functions inside model classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lnprior(pars):\n    # Limit norm and B to be positive\n    logprob = (\n        naima.uniform_prior(pars[0], 0.0, np.inf)\n        + naima.uniform_prior(pars[1], -1, 5)\n        + naima.uniform_prior(pars[3], 0, np.inf)\n    )\n\n    return logprob", "response": "Return probability of parameter values according to prior knowledge."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nactivates user accounts :param server_context: A LabKey server context. See utils.create_server_context. :param target_ids: :param container_path: :return:", "response": "def activate_users(server_context, target_ids, container_path=None):\n    \"\"\"\n    Activate user accounts\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param target_ids:\n    :param container_path:\n    :return:\n    \"\"\"\n    return __make_user_api_request(server_context, target_ids=target_ids, api='activateUsers.api',\n                                   container_path=container_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd user to group", "response": "def add_to_group(server_context, user_ids, group_id, container_path=None):\n    \"\"\"\n    Add user to group\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param user_ids: users to add\n    :param group_id: to add to\n    :param container_path:\n    :return:\n    \"\"\"\n    return __make_security_group_api_request(server_context, 'addGroupMember.api', user_ids, group_id, container_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_to_role(server_context, role, user_id=None, email=None, container_path=None):\n    return __make_security_role_api_request(server_context, 'addAssignment.api', role, user_id=user_id, email=email,\n                                            container_path=container_path)", "response": "Add user to security role"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new user in a LabKey account.", "response": "def create_user(server_context, email, container_path=None, send_email=False):\n    \"\"\"\n    Create new account\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param email:\n    :param container_path:\n    :param send_email: true to send email notification to user\n    :return:\n    \"\"\"\n    url = server_context.build_url(security_controller, 'createNewUser.api', container_path)\n    payload = {\n        'email': email,\n        'sendEmail': send_email\n    }\n\n    return server_context.make_request(url, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndeactivate users and delete them", "response": "def deactivate_users(server_context, target_ids, container_path=None):\n    \"\"\"\n    Deactivate but do not delete user accounts\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param target_ids:\n    :param container_path:\n    :return:\n    \"\"\"\n    # This action responds with HTML so we just check if it responds OK\n    response = __make_user_api_request(server_context, target_ids=target_ids, api='deactivateUsers.view',\n                                       container_path=container_path)\n    if response is not None and response['status_code'] == 200:\n        return dict(success=True)\n    else:\n        raise ValueError(\"Unable to deactivate users {0}\".format(target_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_roles(server_context, container_path=None):\n    url = server_context.build_url(security_controller, 'getRoles.api', container_path=container_path)\n    return server_context.make_request(url, None)", "response": "Gets the set of permissions and roles available from the server\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the user with the provided email. Throws a ValueError if not found.", "response": "def get_user_by_email(server_context, email):\n    \"\"\"\n    Get the user with the provided email. Throws a ValueError if not found.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param email:\n    :return:\n    \"\"\"\n    url = server_context.build_url(user_controller, 'getUsers.api')\n    payload = dict(includeDeactivatedAccounts=True)\n    result = server_context.make_request(url, payload)\n\n    if result is None or result['users'] is None:\n        raise ValueError(\"No Users in container\" + email)\n\n    for user in result['users']:\n        if user['email'] == email:\n            return user\n    else:\n        raise ValueError(\"User not found: \" + email)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_from_group(server_context, user_ids, group_id, container_path=None):\n    return __make_security_group_api_request(server_context, 'removeGroupMember.api', user_ids,\n                                             group_id, container_path)", "response": "Remove user from group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving user from security role", "response": "def remove_from_role(server_context, role, user_id=None, email=None, container_path=None):\n    \"\"\"\n    Remove user/group from security role\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param role: (from get_roles) to remove user from\n    :param user_id: to remove permissions from (must supply this or email or both)\n    :param email: to remove permissions from (must supply this or user_id or both)\n    :param container_path: additional project path context\n    :return:\n    \"\"\"\n    return __make_security_role_api_request(server_context, 'removeAssignment.api', role, user_id=user_id, email=email,\n                                            container_path=container_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_password(server_context, email, container_path=None):\n    url = server_context.build_url(security_controller, 'adminRotatePassword.api', container_path)\n\n    return server_context.make_request(url, {\n        'email': email\n    })", "response": "Change password for a user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a request against the LabKey Security Controller Group Membership apis", "response": "def __make_security_group_api_request(server_context, api, user_ids, group_id, container_path):\n    \"\"\"\n    Execute a request against the LabKey Security Controller Group Membership apis\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param api: Action to execute\n    :param user_ids: user ids to apply action to\n    :param group_id: group id to apply action to\n    :param container_path: Additional container context path\n    :return: Request json object\n    \"\"\"\n    url = server_context.build_url(security_controller, api, container_path)\n\n    # if user_ids is only a single scalar make it an array\n    if not hasattr(user_ids, \"__iter__\"):\n        user_ids = [user_ids]\n\n    return server_context.make_request(url, {\n        'groupId': group_id,\n        'principalIds': user_ids\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __make_security_role_api_request(server_context, api, role, email=None, user_id=None, container_path=None):\n    if email is None and user_id is None:\n        raise ValueError(\"Must supply either/both [email] or [user_id]\")\n\n    url = server_context.build_url(security_controller, api, container_path)\n\n    return server_context.make_request(url, {\n        'roleClassName': role['uniqueName'],\n        'principalId': user_id,\n        'email': email\n    })", "response": "Execute a request against the LabKey Security Controller Group Membership apis"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a request to the LabKey User Controller", "response": "def __make_user_api_request(server_context, target_ids, api, container_path=None):\n    \"\"\"\n    Make a request to the LabKey User Controller\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param target_ids: Array of User ids to affect\n    :param api: action to take\n    :param container_path: container context\n    :return: response json\n    \"\"\"\n    url = server_context.build_url(user_controller, api, container_path)\n\n    return server_context.make_request(url, {\n        'userId': target_ids\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs - uniform prior distribution.", "response": "def log_uniform_prior(value, umin=0, umax=None):\n    \"\"\"Log-uniform prior distribution.\n    \"\"\"\n    if value > 0 and value >= umin:\n        if umax is not None:\n            if value <= umax:\n                return 1 / value\n            else:\n                return -np.inf\n        else:\n            return 1 / value\n    else:\n        return -np.inf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sampler(\n    data_table=None,\n    p0=None,\n    model=None,\n    prior=None,\n    nwalkers=500,\n    nburn=100,\n    guess=True,\n    interactive=False,\n    prefit=False,\n    labels=None,\n    threads=4,\n    data_sed=None,\n):\n    \"\"\"Generate a new MCMC sampler.\n\n    Parameters\n    ----------\n    data_table : `~astropy.table.Table` or list of `~astropy.table.Table`\n        Table containing the observed spectrum. If multiple tables are passed\n        as a string, they will be concatenated in the order given. Each table\n        needs at least these columns, with the appropriate associated units\n        (with the physical type indicated in brackets below) as either a\n        `~astropy.units.Unit` instance or parseable string:\n\n        - ``energy``: Observed photon energy [``energy``]\n        - ``flux``: Observed fluxes [``flux`` or ``differential flux``]\n        - ``flux_error``: 68% CL gaussian uncertainty of the flux [``flux`` or\n          ``differential flux``]. It can also be provided as ``flux_error_lo``\n          and ``flux_error_hi`` (see below).\n\n        Optional columns:\n\n        - ``energy_width``: Width of the energy bin [``energy``], or\n        - ``energy_error``: Half-width of the energy bin [``energy``], or\n        - ``energy_error_lo`` and ``energy_error_hi``: Distance from bin center\n          to lower and upper bin edges [``energy``], or\n        - ``energy_lo`` and ``energy_hi``: Energy edges of the corresponding\n          energy bin [``energy``]\n        - ``flux_error_lo`` and ``flux_error_hi``: 68% CL gaussian lower and\n          upper uncertainties of the flux.\n        - ``ul``: Flag to indicate that a flux measurement is an upper limit.\n        - ``flux_ul``: Upper limit to the flux. If not present, the ``flux``\n          column will be taken as an upper limit for those measurements with\n          the ``ul`` flag set to True or 1.\n\n        The ``keywords`` metadata field of the table can be used to provide the\n        confidence level of the upper limits with the keyword ``cl``, which\n        defaults to 90%. The `astropy.io.ascii` reader can recover all\n        the needed information from ASCII tables in the\n        :class:`~astropy.io.ascii.Ipac` and :class:`~astropy.io.ascii.Daophot`\n        formats, and everything except the ``cl`` keyword from tables in the\n        :class:`~astropy.io.ascii.Sextractor`.  For the latter format, the cl\n        keyword can be added after reading the table with::\n\n            data.meta['keywords']['cl']=0.99\n\n    p0 : array\n        Initial position vector. The distribution for the ``nwalkers`` walkers\n        will be computed as a multidimensional gaussian of width 5% around the\n        initial position vector ``p0``.\n    model : function\n        A function that takes a vector in the parameter space and the data\n        dictionary, and returns the expected fluxes at the energies in the\n        spectrum. Additional return objects will be saved as blobs in the\n        sampler chain, see `the emcee documentation for the\n        format\n        <http://dan.iel.fm/emcee/current/user/advanced/#arbitrary-metadata-blobs>`_.\n    prior : function, optional\n        A function that takes a vector in the parameter space and returns the\n        log-likelihood of the Bayesian prior. Parameter limits can be specified\n        through a uniform prior, returning 0. if the vector is within the\n        parameter bounds and ``-np.inf`` otherwise.\n    nwalkers : int, optional\n        The number of Goodman & Weare \u201cwalkers\u201d. Default is 500.\n    nburn : int, optional\n        Number of burn-in steps. After ``nburn`` steps, the sampler is reset\n        and chain history discarded. It is necessary to settle the sampler into\n        the maximum of the parameter space density. Default is 100.\n    labels : iterable of strings, optional\n        Labels for the parameters included in the position vector ``p0``. If\n        not provided ``['par1','par2', ... ,'parN']`` will be used.\n    threads : int, optional\n        Number of threads to use for sampling. Default is 4.\n    guess : bool, optional\n        Whether to attempt to guess the normalization (first) parameter of the\n        model. Default is True.\n    interactive : bool, optional\n        Whether to launch the interactive fitting window to set the initial\n        values for the prefitting or the MCMC run. Requires matplotlib. Default\n        is False.\n    prefit : bool, optional\n        Whether to attempt to find the maximum likelihood parameters with a\n        Nelder-Mead algorithm and use them as starting point of the MCMC run.\n        The parameter values in `p0` will be used as starting points for the\n        minimization. Note that the initial optimization is done without taking\n        the prior function into account to avoid the possibility of infinite\n        values in the objective function. If the best-fit parameter vector\n        without prior is forbidden by the prior given, it will be discarded.\n    data_sed : bool, optional\n        When providing more than one data table, whether to convert them to SED\n        format. If unset or None, all tables will be converted to the format of\n        the first table.\n\n    Returns\n    -------\n    sampler : :class:`~emcee.EnsembleSampler` instance\n        Ensemble sampler with walker positions after ``nburn`` burn-in steps.\n    pos : :class:`~numpy.ndarray`\n        Final position vector array.\n\n    See also\n    --------\n    emcee.EnsembleSampler\n    \"\"\"\n    import emcee\n\n    if data_table is None:\n        raise TypeError(\"Data table is missing!\")\n    else:\n        data = validate_data_table(data_table, sed=data_sed)\n\n    if model is None:\n        raise TypeError(\"Model function is missing!\")\n\n    # Add parameter labels if not provided or too short\n    if labels is None:\n        # First is normalization\n        labels = [\"norm\"] + [\"par{0}\".format(i) for i in range(1, len(p0))]\n    elif len(labels) < len(p0):\n        labels += [\"par{0}\".format(i) for i in range(len(labels), len(p0))]\n\n    # Check that the model returns fluxes in same physical type as data\n    modelout = model(p0, data)\n    if (type(modelout) == tuple or type(modelout) == list) and (\n        type(modelout) != np.ndarray\n    ):\n        spec = modelout[0]\n    else:\n        spec = modelout\n\n    # check whether both can be converted to same physical type through\n    # sed_conversion\n    try:\n        # If both can be converted to differential flux, they can be compared\n        # Otherwise, sed_conversion will raise a u.UnitsError\n        sed_conversion(data[\"energy\"], spec.unit, False)\n        sed_conversion(data[\"energy\"], data[\"flux\"].unit, False)\n    except u.UnitsError:\n        raise u.UnitsError(\n            \"The physical type of the model and data units are not compatible,\"\n            \" please modify your model or data so they match:\\n\"\n            \" Model units: {0} [{1}]\\n Data units: {2} [{3}]\\n\".format(\n                spec.unit,\n                spec.unit.physical_type,\n                data[\"flux\"].unit,\n                data[\"flux\"].unit.physical_type,\n            )\n        )\n\n    if guess:\n        normNames = [\"norm\", \"Norm\", \"ampl\", \"Ampl\", \"We\", \"Wp\"]\n        normNameslog = [\"log({0}\".format(name) for name in normNames]\n        normNameslog10 = [\"log10({0}\".format(name) for name in normNames]\n        normNames += normNameslog + normNameslog10\n        idxs = []\n        for l in normNames:\n            for l2 in labels:\n                if l2.startswith(l):\n                    # check with startswith to include normalization,\n                    # amplitude, etc.\n                    idxs.append(labels.index(l2))\n\n        if len(idxs) == 1:\n\n            nunit, sedf = sed_conversion(data[\"energy\"], spec.unit, False)\n            currFlux = np.trapz(\n                data[\"energy\"] * (spec * sedf).to(nunit), data[\"energy\"]\n            )\n            nunit, sedf = sed_conversion(\n                data[\"energy\"], data[\"flux\"].unit, False\n            )\n            dataFlux = np.trapz(\n                data[\"energy\"] * (data[\"flux\"] * sedf).to(nunit),\n                data[\"energy\"],\n            )\n            ratio = dataFlux / currFlux\n            if labels[idxs[0]].startswith(\"log(\"):\n                p0[idxs[0]] += np.log(ratio)\n            elif labels[idxs[0]].startswith(\"log10(\"):\n                p0[idxs[0]] += np.log10(ratio)\n            else:\n                p0[idxs[0]] *= ratio\n\n        elif len(idxs) == 0:\n            log.warning(\n                \"No label starting with [{0}] found: not applying\"\n                \" normalization guess.\".format(\",\".join(normNames))\n            )\n        elif len(idxs) > 1:\n            log.warning(\n                \"More than one label starting with [{0}] found:\"\n                \" not applying normalization guess.\".format(\n                    \",\".join(normNames)\n                )\n            )\n\n    P0_IS_ML = False\n    if interactive:\n        try:\n            log.info(\"Launching interactive model fitter, close when finished\")\n            from .model_fitter import InteractiveModelFitter\n            import matplotlib.pyplot as plt\n\n            iprev = plt.rcParams[\"interactive\"]\n            plt.rcParams[\"interactive\"] = False\n            imf = InteractiveModelFitter(\n                model, p0, data, labels=labels, sed=True\n            )\n            p0 = imf.pars\n            P0_IS_ML = imf.P0_IS_ML\n            plt.rcParams[\"interactive\"] = iprev\n        except ImportError as e:\n            log.warning(\n                \"Interactive fitting is not available because\"\n                \" matplotlib is not installed: {0}\".format(e)\n            )\n\n    # If we already did the prefit call in ModelWidget (and didn't modify the\n    # parameters afterwards), avoid doing it here\n    if prefit and not P0_IS_ML:\n        p0, P0_IS_ML = _prefit(p0, data, model, prior)\n\n    sampler = emcee.EnsembleSampler(\n        nwalkers, len(p0), lnprob, args=[data, model, prior], threads=threads\n    )\n\n    # Add data and parameters properties to sampler\n    sampler.data_table = data_table\n    sampler.data = data\n    sampler.labels = labels\n    # Add model function to sampler\n    sampler.modelfn = model\n    # Add run_info dict\n    sampler.run_info = {\n        \"n_walkers\": nwalkers,\n        \"n_burn\": nburn,\n        # convert from np.float to regular float\n        \"p0\": [float(p) for p in p0],\n        \"guess\": guess,\n    }\n\n    # Initialize walkers in a ball of relative size 0.5% in all dimensions if\n    # the parameters have been fit to their ML values, or to 10% otherwise\n    spread = 0.005 if P0_IS_ML else 0.1\n    p0var = np.array([spread * pp for pp in p0])\n    p0 = emcee.utils.sample_ball(p0, p0var, nwalkers)\n\n    if nburn > 0:\n        print(\n            \"Burning in the {0} walkers with {1} steps...\".format(\n                nwalkers, nburn\n            )\n        )\n        sampler, pos = _run_mcmc(sampler, p0, nburn)\n    else:\n        pos = p0\n\n    sampler.run_info[\"p0_burn_median\"] = [\n        float(p) for p in np.median(pos, axis=0)\n    ]\n\n    return sampler, pos", "response": "Generate a new MCMC sampler."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns an MCMC sampler.", "response": "def run_sampler(nrun=100, sampler=None, pos=None, **kwargs):\n    \"\"\"Run an MCMC sampler.\n\n    If no sampler or initial position vector is provided, extra ``kwargs`` are\n    passed to `get_sampler` to generate a new sampler.\n\n    Parameters\n    ----------\n    nrun : int, optional\n        Number of steps to run\n    sampler : :class:`~emcee.EnsembleSampler` instance, optional\n        Sampler.\n    pos : :class:`~numpy.ndarray`, optional\n        A list of initial position vectors for the walkers. It should have\n        dimensions of ``(nwalkers,dim)``, where ``dim`` is the number of free\n        parameters. `emcee.utils.sample_ball` can be used to generate a\n        multidimensional gaussian distribution around a single initial\n        position.\n\n    Returns\n    -------\n    sampler : :class:`~emcee.EnsembleSampler` instance\n        Sampler containing the paths of the walkers during the ``nrun`` steps.\n    pos : array\n        List of final position vectors after the run.\n    \"\"\"\n\n    if sampler is None or pos is None:\n        sampler, pos = get_sampler(**kwargs)\n\n    sampler.run_info[\"n_run\"] = nrun\n\n    print(\"\\nWalker burn in finished, running {0} steps...\".format(nrun))\n    sampler.reset()\n    sampler, pos = _run_mcmc(sampler, pos, nrun)\n\n    return sampler, pos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fact_to_tuple(self, fact):\n        # Fields that may have ``None`` value will be represented by ''\n        if fact.category:\n            category = fact.category.name\n        else:\n            category = ''\n\n        description = fact.description or ''\n\n        return FactTuple(\n            start=fact.start.strftime(self.datetime_format),\n            end=fact.end.strftime(self.datetime_format),\n            activity=text_type(fact.activity.name),\n            duration=fact.get_string_delta(format='%M'),\n            category=text_type(category),\n            description=text_type(description),\n        )", "response": "Convert a Hamster Fact to its normalized tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a fact element to the fact_list.", "response": "def _write_fact(self, fact_tuple):\n        \"\"\"\n        Create new fact element and populate attributes.\n\n        Once the child is prepared append it to ``fact_list``.\n        \"\"\"\n        fact = self.document.createElement(\"fact\")\n        fact.setAttribute('start', fact_tuple.start)\n        fact.setAttribute('end', fact_tuple.end)\n        fact.setAttribute('activity', fact_tuple.activity)\n        fact.setAttribute('duration', fact_tuple.duration)\n        fact.setAttribute('category', fact_tuple.category)\n        fact.setAttribute('description', fact_tuple.description)\n        self.fact_list.appendChild(fact)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _close(self):\n\n        self.document.appendChild(self.fact_list)\n        self.file.write(self.document.toxml(encoding='utf-8'))\n        return super(XMLWriter, self)._close()", "response": "Append the xml fact list to the main document write file and cleanup."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a sql query against a LabKey server.", "response": "def execute_sql(server_context, schema_name, sql, container_path=None,\n                max_rows=None,\n                sort=None,\n                offset=None,\n                container_filter=None,\n                save_in_session=None,\n                parameters=None,\n                required_version=None,\n                timeout=_default_timeout):\n    \"\"\"\n    Execute sql query against a LabKey server.\n\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of table\n    :param sql: String of labkey sql to execute\n    :param container_path: labkey container path if not already set in context\n    :param max_rows: max number of rows to return\n    :param sort: comma separated list of column names to sort by\n    :param offset: number of rows to offset results by\n    :param container_filter: enumeration of the various container filters available. See:\n        https://www.labkey.org/download/clientapi_docs/javascript-api/symbols/LABKEY.Query.html#.containerFilter\n    :param save_in_session: save query result as a named view to the session\n    :param parameters: parameter values to pass through to a parameterized query\n    :param required_version: Api version of response\n    :param timeout: timeout of request in seconds (defaults to 30s)\n    :return:\n    \"\"\"\n    url = server_context.build_url('query', 'executeSql.api', container_path=container_path)\n\n    payload = {\n        'schemaName': schema_name,\n        'sql': sql\n    }\n\n    if container_filter is not None:\n        payload['containerFilter'] = container_filter\n\n    if max_rows is not None:\n        payload['maxRows'] = max_rows\n\n    if offset is not None:\n        payload['offset'] = offset\n\n    if sort is not None:\n        payload['query.sort'] = sort\n\n    if save_in_session is not None:\n        payload['saveInSession'] = save_in_session\n\n    if parameters is not None:\n        for key, value in parameters.items():\n            payload['query.param.' + key] = value\n\n    if required_version is not None:\n        payload['apiVersion'] = required_version\n\n    return server_context.make_request(url, payload, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery data from a LabKey table.", "response": "def select_rows(server_context, schema_name, query_name, view_name=None,\n                filter_array=None,\n                container_path=None,\n                columns=None,\n                max_rows=None,\n                sort=None,\n                offset=None,\n                container_filter=None,\n                parameters=None,\n                show_rows=None,\n                include_total_count=None,\n                include_details_column=None,\n                include_update_column=None,\n                selection_key=None,\n                required_version=None,\n                timeout=_default_timeout\n                ):\n    \"\"\"\n    Query data from a LabKey server\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of table\n    :param query_name: table name to select from\n    :param view_name: pre-existing named view\n    :param filter_array: set of filter objects to apply\n    :param container_path: folder path if not already part of server_context\n    :param columns: set of columns to retrieve\n    :param max_rows: max number of rows to retrieve\n    :param sort: comma separated list of column names to sort by, prefix a column with '-' to sort descending\n    :param offset: number of rows to offset results by\n    :param container_filter: enumeration of the various container filters available. See:\n        https://www.labkey.org/download/clientapi_docs/javascript-api/symbols/LABKEY.Query.html#.containerFilter\n    :param parameters: Set of parameters to pass along to a parameterized query\n    :param show_rows: An enumeration of various paging styles\n    :param include_total_count: Boolean value that indicates whether to include a total count value in response\n    :param include_details_column: Boolean value that indicates whether to include a Details link column in results\n    :param include_update_column: Boolean value that indicates whether to include an Update link column in results\n    :param selection_key:\n    :param required_version: decimal value that indicates the response version of the api\n    :param timeout: Request timeout in seconds (defaults to 30s)\n    :return:\n    \"\"\"\n    url = server_context.build_url('query', 'getQuery.api', container_path=container_path)\n\n    payload = {\n        'schemaName': schema_name,\n        'query.queryName': query_name\n    }\n\n    if view_name is not None:\n        payload['query.viewName'] = view_name\n\n    if filter_array is not None:\n        for query_filter in filter_array:\n            prefix = query_filter.get_url_parameter_name()\n            payload[prefix] = query_filter.get_url_parameter_value()\n\n    if columns is not None:\n        payload['query.columns'] = columns\n\n    if max_rows is not None:\n        payload['query.maxRows'] = max_rows\n\n    if sort is not None:\n        payload['query.sort'] = sort\n\n    if offset is not None:\n        payload['query.offset'] = offset\n\n    if container_filter is not None:\n        payload['containerFilter'] = container_filter\n\n    if parameters is not None:\n        for key, value in parameters.items():\n            payload['query.param.' + key] = value\n\n    if show_rows is not None:\n        payload['query.showRows'] = show_rows\n\n    if include_total_count is not None:\n        payload['includeTotalCount'] = include_total_count\n\n    if include_details_column is not None:\n        payload['includeDetailsColumn'] = include_details_column\n\n    if include_update_column is not None:\n        payload['includeUpdateColumn'] = include_update_column\n\n    if selection_key is not None:\n        payload['query.selectionKey'] = selection_key\n\n    if required_version is not None:\n        payload['apiVersion'] = required_version\n\n    return server_context.make_request(url, payload, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a set of rows in a table.", "response": "def update_rows(server_context, schema_name, query_name, rows, container_path=None, timeout=_default_timeout):\n    \"\"\"\n    Update a set of rows\n\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param schema_name: schema of table\n    :param query_name: table name to update\n    :param rows: Set of rows to update\n    :param container_path: labkey container path if not already set in context\n    :param timeout: timeout of request in seconds (defaults to 30s)\n    :return:\n    \"\"\"\n    url = server_context.build_url('query', 'updateRows.api', container_path=container_path)\n\n    payload = {\n        'schemaName': schema_name,\n        'queryName': query_name,\n        'rows': rows\n    }\n\n    return server_context.make_request(url, json_dumps(payload, sort_keys=True), headers=_query_headers,\n                                       timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_day_end(config):\n    day_start_datetime = datetime.datetime.combine(datetime.date.today(), config['day_start'])\n    day_end_datetime = day_start_datetime - datetime.timedelta(seconds=1)\n    return day_end_datetime.time()", "response": "Get the day end time given the day start."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a given end day to its proper datetime.", "response": "def end_day_to_datetime(end_day, config):\n    \"\"\"\n    Convert a given end day to its proper datetime.\n\n    This is non trivial because of variable ``day_start``. We want to make sure\n    that even if an 'end day' is specified the actual point in time may reach into the following\n    day.\n\n    Args:\n        end (datetime.date): Raw end date that is to be adjusted.\n        config: Controller config containing information on when a workday starts.\n\n    Returns:\n        datetime.datetime: The endday as a adjusted datetime object.\n\n    Example:\n        Given a ``day_start`` of ``5:30`` and end date of ``2015-04-01`` we actually want to\n        consider even points in time up to ``2015-04-02 5:29``. That is to represent that a\n        *work day*\n        does not match *calendar days*.\n\n    Note:\n        An alternative implementation for the similar problem in legacy hamster:\n            ``hamster.storage.db.Storage.__get_todays_facts``.\n    \"\"\"\n\n    day_start_time = config['day_start']\n    day_end_time = get_day_end(config)\n\n    if day_start_time == datetime.time(0, 0, 0):\n        end = datetime.datetime.combine(end_day, day_end_time)\n    else:\n        end = datetime.datetime.combine(end_day, day_end_time) + datetime.timedelta(days=1)\n    return end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting valid time - range information from a string.", "response": "def extract_time_info(text):\n    \"\"\"\n    Extract valid time(-range) information from a string according to our specs.\n\n    Args:\n        text (text_type): Raw string containing encoded time(-span) information.\n            Date/Time-combinations are expected in a ``YYYY-MM-DD hh:mm`` format.\n            Relative  times can be given with ``-minutes``.\n            Please note that either *relative* or *absolute* times will be considered.\n            It is possible to either just specify a start date (as time, date,\n            or datetime) or a timerange (start and end). If a timerange is given\n            start and end need to be delimited exactly by ' - '.\n\n    Returns:\n        tuple: ``(timeframe, rest)`` tuple. Where ``timeframe`` is a tuple that\n            provides convinient access to all seperate elements extracted from\n            the raw string and ``rest`` is any substring stat has not been\n            matched to valid time/date info.\n\n    Note:\n        * Relative times always return just ``(None, None, None, None, timedelta)``.\n    \"\"\"\n\n    # [TODO] Add a list of supported formats.\n\n    def get_time(time, seconds=None):\n        \"\"\"Convert a times string representation to datetime.time instance.\"\"\"\n        if time is None:\n            return time\n\n        if seconds:\n            time_format = '%H:%M:%S'\n        else:\n            time_format = '%H:%M'\n\n        return datetime.datetime.strptime(time.strip(), time_format).time()\n\n    def get_date(date):\n        \"\"\"Convert a dates string representation to datetime.date instance.\"\"\"\n        if date:\n            date = datetime.datetime.strptime(date.strip(), \"%Y-%m-%d\").date()\n        return date\n\n    def date_time_from_groupdict(groupdict):\n        \"\"\"Return a date/time tuple by introspecting a passed dict.\"\"\"\n        if groupdict['datetime']:\n            dt = parse_time(groupdict['datetime'])\n            time = dt.time()\n            date = dt.date()\n        else:\n            date = get_date(groupdict.get('date'))\n            time = get_time(groupdict.get('time'), groupdict.get('seconds'))\n        return (date, time)\n\n    # Baseline/default values.\n    result = {\n        'start_date': None,\n        'start_time': None,\n        'end_date': None,\n        'end_time': None,\n        'offset': None\n    }\n    rest = None\n\n    # Individual patterns for time/date  substrings.\n    relative_pattern = '(?P<relative>-\\d+)'\n    time_pattern = '(?P<time>\\d{2}:\\d{2}(?P<seconds>:\\d{2})?)'\n    date_pattern = '(?P<date>\\d{4}-\\d{2}-\\d{2})'\n    datetime_pattern = '(?P<datetime>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}(:\\d{2})?)'\n\n    start = re.match('^({}|{}|{}|{}) (?P<rest>.+)'.format(relative_pattern, datetime_pattern,\n        date_pattern, time_pattern), text)\n    if start:\n        start_groups = start.groupdict()\n        if start_groups['relative']:\n            result['offset'] = datetime.timedelta(minutes=abs(int(start_groups['relative'])))\n        else:\n            date, time = date_time_from_groupdict(start_groups)\n            result['start_date'] = date\n            result['start_time'] = time\n        rest = start_groups['rest']\n\n        if rest:\n            end = re.match('^- ({}|{}|{}) (?P<rest>.+)'.format(datetime_pattern, date_pattern,\n                time_pattern), rest)\n        else:\n            end = None\n\n        if end and not start_groups['relative']:\n            end_groups = end.groupdict()\n            date, time = date_time_from_groupdict(end_groups)\n            result['end_date'] = date\n            result['end_time'] = time\n            rest = end_groups['rest']\n\n    result = TimeFrame(result['start_date'], result['start_time'], result['end_date'],\n        result['end_time'], result['offset'])\n\n    # Consider the whole string as 'rest' if no time/date info was extracted\n    if not rest:\n        rest = text\n    return (result, rest.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncomplete all available info for a given timeframe.", "response": "def complete_timeframe(timeframe, config, partial=False):\n    \"\"\"\n    Apply fallback strategy to incomplete timeframes.\n\n    Our fallback strategy is as follows:\n        * Missing start-date: Fallback to ``today``.\n        * Missing start-time: Fallback to ``store.config['day_start']``.\n        * Missing end-date: Fallback to ``today`` for ``day_start='00:00`,\n          ``tomorrow`` otherwise.\n          See ``hamster_lib.helpers.end_day_to_datetime`` for details and\n          explanations.\n        * Missing end-time: 1 second before ``store.config['day_start']``.\n\n    Args:\n        timeframe (TimeFrame): ``TimeFrame`` instance incorporating all\n            available information available about the timespan. Any missing info\n            will be completed per fallback strategy.\n        config (dict): A config-dict providing settings relevant to determine\n            fallback values.\n        partial (bool, optional): If true, we will only complete start/end times if there\n            is at least either date or time information present. Defaults to\n            ``False``.\n\n    Returns:\n        tuple: ``(start, end)`` tuple. Where ``start`` and ``end`` are full\n        ``datetime.datetime`` instances.\n\n    Raises:\n        TypeError: If any of the ``timeframe`` values is of inappropriate\n            datetime type.\n    \"\"\"\n\n    def complete_start_date(date):\n        \"\"\"\n        Assign ``today`` if ``date=None``, else ensure its a ``datetime.date`` instance.\n\n        Args:\n            date (datetime.date): Startdate information.\n\n        Returns:\n            datetime.date: Either the original date or the default solution.\n\n        Raises:\n            TypeError: If ``date``` is neither ``None`` nor  ``datetime.date`` instance.\n\n        Note:\n            Reference behavior taken from [hamster-cli](https://github.com/projecthamster/\n            hamster/blob/master/src/hamster-cli#L368).\n        \"\"\"\n\n        if not date:\n            date = datetime.date.today()\n        else:\n            if not isinstance(date, datetime.date):\n                raise TypeError(_(\n                    \"Expected datetime.date instance, got {type} instead.\".format(\n                        type=type(date))\n                ))\n        return date\n\n    def complete_start_time(time, day_start):\n        \"\"\"Assign ``day_start`` if no start-time is given.\"\"\"\n        if not time:\n            time = day_start\n        else:\n            if not isinstance(time, datetime.time):\n                raise TypeError(_(\n                    \"Expected datetime.time instance, got {type} instead.\".format(\n                        type=type(time))\n                ))\n        return time\n\n    def complete_start(date, time, config):\n        return datetime.datetime.combine(\n            complete_start_date(timeframe.start_date),\n            complete_start_time(timeframe.start_time, config['day_start']),\n        )\n\n    def complete_end_date(date):\n        if not date:\n            date = datetime.date.today()\n        else:\n            if not isinstance(date, datetime.date):\n                raise TypeError(_(\n                    \"Expected datetime.date instance, got {type} instead.\".format(\n                        type=type(date))\n                ))\n        return date\n\n    def complete_end(date, time, config):\n        date = complete_end_date(date)\n        if time:\n            result = datetime.datetime.combine(date, time)\n        else:\n            result = end_day_to_datetime(date, config)\n        return result\n\n    start, end = None, None\n\n    if any((timeframe.offset, timeframe.start_time, timeframe.start_date)) or not partial:\n        if not timeframe.offset:\n            start = complete_start(timeframe.start_date, timeframe.start_time, config)\n        else:\n            start = datetime.datetime.now() - timeframe.offset\n\n    if any((timeframe.end_date, timeframe.end_time)) or not partial:\n        end = complete_end(timeframe.end_date, timeframe.end_time, config)\n\n    return (start, end)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a date or time string and return a corresponding datetime object.", "response": "def parse_time(time):\n    \"\"\"\n    Parse a date/time string and return a corresponding datetime object.\n\n    Args:\n        time (str): A ``string` of one of the following formats: ``%H:%M``, ``%Y-%m-%d`` or\n            ``%Y-%m-%d %H:%M``.\n\n    Returns:\n        datetime.datetime: Depending on input string either returns ``datetime.date``,\n            ``datetime.time`` or ``datetime.datetime``.\n\n    Raises:\n        ValueError: If ``time`` can not be matched against any of the accepted formats.\n\n    Note:\n        This parse just a singlular date, time or datetime representation.\n    \"\"\"\n\n    length = len(time.strip().split())\n    if length == 1:\n        try:\n            result = datetime.datetime.strptime(time, '%H:%M:%S').time()\n        except ValueError:\n            try:\n                result = datetime.datetime.strptime(time, '%H:%M').time()\n            except ValueError:\n                result = datetime.datetime.strptime(time, '%Y-%m-%d').date()\n    elif length == 2:\n        try:\n            result = datetime.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n        except ValueError:\n            result = datetime.datetime.strptime(time, '%Y-%m-%d %H:%M')\n    else:\n        raise ValueError(_(\n            \"String does not seem to be in one of our supported time formats.\"\n        ))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming basic sanity checks on a timeframe.", "response": "def validate_start_end_range(range_tuple):\n    \"\"\"\n    Perform basic sanity checks on a timeframe.\n\n    Args:\n        range_tuple (tuple): ``(start, end)`` tuple as returned by\n            ``complete_timeframe``.\n\n    Raises:\n        ValueError: If start > end.\n\n    Returns:\n        tuple: ``(start, end)`` tuple that passed validation.\n\n    Note:\n        ``timeframes`` may be incomplete, especially if ``complete_timeframe(partial=True)`` has\n        been used to construct them.\n    \"\"\"\n\n    start, end = range_tuple\n\n    if (start and end) and (start > end):\n        raise ValueError(_(\"Start after end!\"))\n\n    return range_tuple"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_batch(server_context, assay_id, batch_id):\n    # type: (ServerContext, int, int) -> Union[Batch, None]\n    \"\"\"\n    Loads a batch from the server.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param assay_id: The protocol id of the assay from which to load a batch.\n    :param batch_id:\n    :return:\n    \"\"\"\n    load_batch_url = server_context.build_url('assay', 'getAssayBatch.api')\n    loaded_batch = None\n\n    payload = {\n        'assayId': assay_id,\n        'batchId': batch_id\n    }\n\n    headers = {\n        'Content-type': 'application/json',\n        'Accept': 'text/plain'\n    }\n\n    json_body = server_context.make_request(load_batch_url, json_dumps(payload, sort_keys=True), headers=headers)\n    if json_body is not None:\n        loaded_batch = Batch.from_data(json_body['batch'])\n\n    return loaded_batch", "response": "Loads a batch from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_batch(server_context, assay_id, batch):\n    # type: (ServerContext, int, Batch) -> Union[Batch, None]\n    \"\"\"\n    Saves a modified batch.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param assay_id: The assay protocol id.\n    :param batch: The Batch to save.\n    :return:\n    \"\"\"\n    result = save_batches(server_context, assay_id, [batch])\n\n    if result is not None:\n        return result[0]\n    return None", "response": "Saves a batch to the LabKey database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving a list of batches to a LabKey resource.", "response": "def save_batches(server_context, assay_id, batches):\n    # type: (ServerContext, int, List[Batch]) -> Union[List[Batch], None]\n    \"\"\"\n    Saves a modified batches.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param assay_id: The assay protocol id.\n    :param batches: The Batch(es) to save.\n    :return:\n    \"\"\"\n    save_batch_url = server_context.build_url('assay', 'saveAssayBatch.api')\n    json_batches = []\n\n    if batches is None:\n        return None  # Nothing to save\n\n    for batch in batches:\n        if isinstance(batch, Batch):\n            json_batches.append(batch.to_json())\n        else:\n            raise Exception('save_batch() \"batches\" expected to be a set Batch instances')\n\n    payload = {\n        'assayId': assay_id,\n        'batches': json_batches\n    }\n    headers = {\n        'Content-type': 'application/json',\n        'Accept': 'text/plain'\n    }\n\n    json_body = server_context.make_request(save_batch_url, json_dumps(payload, sort_keys=True), headers=headers)\n    if json_body is not None:\n        resp_batches = json_body['batches']\n        return [Batch.from_data(resp_batch) for resp_batch in resp_batches]\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_data_table(data_table, sed=None):\n    if isinstance(data_table, Table) or isinstance(data_table, QTable):\n        data_table = [data_table]\n\n    try:\n        for dt in data_table:\n            if not isinstance(dt, Table) and not isinstance(dt, QTable):\n                raise TypeError(\n                    \"An object passed as data_table is not an astropy Table!\"\n                )\n    except TypeError:\n        raise TypeError(\n            \"Argument passed to validate_data_table is not a table and \"\n            \"not a list\"\n        )\n\n    def dt_sed_conversion(dt, sed):\n        f_unit, sedf = sed_conversion(dt[\"energy\"], dt[\"flux\"].unit, sed)\n        # roundtrip to Table to change the units\n        t = Table(dt)\n        for col in [\"flux\", \"flux_error_lo\", \"flux_error_hi\"]:\n            t[col].unit = f_unit\n        ndt = QTable(t)\n\n        ndt[\"flux\"] = (dt[\"flux\"] * sedf).to(f_unit)\n        ndt[\"flux_error_lo\"] = (dt[\"flux_error_lo\"] * sedf).to(f_unit)\n        ndt[\"flux_error_hi\"] = (dt[\"flux_error_hi\"] * sedf).to(f_unit)\n\n        return ndt\n\n    data_list = []\n    for group, dt in enumerate(data_table):\n        dt_val = _validate_single_data_table(dt, group=group)\n        data_list.append(dt_val)\n\n    # concatenate input data tables\n    data_new = data_list[0].copy()\n    f_pt = data_new[\"flux\"].unit.physical_type\n    if sed is None:\n        sed = f_pt in [\"flux\", \"power\"]\n\n    data_new = dt_sed_conversion(data_new, sed)\n\n    for dt in data_list[1:]:\n        nf_pt = dt[\"flux\"].unit.physical_type\n        if (\"flux\" in nf_pt and \"power\" in f_pt) or (\n            \"power\" in nf_pt and \"flux\" in f_pt\n        ):\n            raise TypeError(\n                \"The physical types of the data tables could not be \"\n                \"matched: Some are in flux and others in luminosity units\"\n            )\n\n        dt = dt_sed_conversion(dt, sed)\n\n        for row in dt:\n            data_new.add_row(row)\n\n    return data_new", "response": "Validate all columns of a data table and concatenate them into a single tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmanage conversion between differential spectrum and SED", "response": "def sed_conversion(energy, model_unit, sed):\n    \"\"\"\n    Manage conversion between differential spectrum and SED\n    \"\"\"\n\n    model_pt = model_unit.physical_type\n\n    ones = np.ones(energy.shape)\n\n    if sed:\n        # SED\n        f_unit = u.Unit(\"erg/s\")\n        if model_pt == \"power\" or model_pt == \"flux\" or model_pt == \"energy\":\n            sedf = ones\n        elif \"differential\" in model_pt:\n            sedf = energy ** 2\n        else:\n            raise u.UnitsError(\n                \"Model physical type ({0}) is not supported\".format(model_pt),\n                \"Supported physical types are: power, flux, differential\"\n                \" power, differential flux\",\n            )\n\n        if \"flux\" in model_pt:\n            f_unit /= u.cm ** 2\n        elif \"energy\" in model_pt:\n            # particle energy distributions\n            f_unit = u.erg\n    else:\n        # Differential spectrum\n        f_unit = u.Unit(\"1/(s TeV)\")\n        if \"differential\" in model_pt:\n            sedf = ones\n        elif model_pt == \"power\" or model_pt == \"flux\" or model_pt == \"energy\":\n            # From SED to differential\n            sedf = 1 / (energy ** 2)\n        else:\n            raise u.UnitsError(\n                \"Model physical type ({0}) is not supported\".format(model_pt),\n                \"Supported physical types are: power, flux, differential\"\n                \" power, differential flux\",\n            )\n\n        if \"flux\" in model_pt:\n            f_unit /= u.cm ** 2\n        elif \"energy\" in model_pt:\n            # particle energy distributions\n            f_unit = u.Unit(\"1/TeV\")\n\n    log.debug(\n        \"Converted from {0} ({1}) into {2} ({3}) for sed={4}\".format(\n            model_unit, model_pt, f_unit, f_unit.physical_type, sed\n        )\n    )\n\n    return f_unit, sedf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trapz_loglog(y, x, axis=-1, intervals=False):\n    try:\n        y_unit = y.unit\n        y = y.value\n    except AttributeError:\n        y_unit = 1.0\n    try:\n        x_unit = x.unit\n        x = x.value\n    except AttributeError:\n        x_unit = 1.0\n\n    y = np.asanyarray(y)\n    x = np.asanyarray(x)\n\n    slice1 = [slice(None)] * y.ndim\n    slice2 = [slice(None)] * y.ndim\n    slice1[axis] = slice(None, -1)\n    slice2[axis] = slice(1, None)\n\n    slice1 = tuple(slice1)\n    slice2 = tuple(slice2)\n\n    if x.ndim == 1:\n        shape = [1] * y.ndim\n        shape[axis] = x.shape[0]\n        x = x.reshape(shape)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Compute the power law indices in each integration bin\n        b = np.log10(y[slice2] / y[slice1]) / np.log10(x[slice2] / x[slice1])\n\n        # if local powerlaw index is -1, use \\int 1/x = log(x); otherwise use\n        # normal powerlaw integration\n        trapzs = np.where(\n            np.abs(b + 1.0) > 1e-10,\n            (\n                y[slice1]\n                * (x[slice2] * (x[slice2] / x[slice1]) ** b - x[slice1])\n            )\n            / (b + 1),\n            x[slice1] * y[slice1] * np.log(x[slice2] / x[slice1]),\n        )\n\n    tozero = (y[slice1] == 0.0) + (y[slice2] == 0.0) + (x[slice1] == x[slice2])\n    trapzs[tozero] = 0.0\n\n    if intervals:\n        return trapzs * x_unit * y_unit\n\n    ret = np.add.reduce(trapzs, axis) * x_unit * y_unit\n\n    return ret", "response": "Integrate y along the given axis using the composite trapezoidal rule in loglog space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_energy_edges_single(ene):\n    midene = np.sqrt((ene[1:] * ene[:-1]))\n    elo, ehi = np.zeros(len(ene)) * ene.unit, np.zeros(len(ene)) * ene.unit\n    elo[1:] = ene[1:] - midene\n    ehi[:-1] = midene - ene[:-1]\n    elo[0] = ene[0] * (1 - ene[0] / (ene[0] + ehi[0]))\n    ehi[-1] = elo[-1]\n    return u.Quantity([elo, ehi])", "response": "Generate energy edges for a single group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_energy_edges(ene, groups=None):\n    if groups is None or len(ene) != len(groups):\n        return _generate_energy_edges_single(ene)\n    else:\n        eloehi = np.zeros((2, len(ene))) * ene.unit\n        for g in np.unique(groups):\n            group_edges = _generate_energy_edges_single(ene[groups == g])\n            eloehi[:, groups == g] = group_edges\n        # hstack throws away units\n        return eloehi", "response": "Generates an array of energy edges corresponding to a given energy array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding an astropy. table. QTable from data stored in an astropy. table. QTable.", "response": "def build_data_table(\n    energy,\n    flux,\n    flux_error=None,\n    flux_error_lo=None,\n    flux_error_hi=None,\n    energy_width=None,\n    energy_lo=None,\n    energy_hi=None,\n    ul=None,\n    cl=None,\n):\n    \"\"\"\n    Read data into data dict.\n\n    Parameters\n    ----------\n\n    energy : :class:`~astropy.units.Quantity` array instance\n        Observed photon energy array [physical type ``energy``]\n\n    flux : :class:`~astropy.units.Quantity` array instance\n        Observed flux array [physical type ``flux`` or ``differential flux``]\n\n    flux_error, flux_error_hi, flux_error_lo : :class:`~astropy.units.Quantity` array instance\n        68% CL gaussian uncertainty of the flux [physical type ``flux`` or\n        ``differential flux``]. Either ``flux_error`` (symmetrical uncertainty)\n        or ``flux_error_hi`` and ``flux_error_lo`` (asymmetrical uncertainties)\n        must be provided.\n\n    energy_width, energy_lo, energy_hi : :class:`~astropy.units.Quantity` array instance, optional\n        Width of the energy bins [physical type ``energy``]. Either\n        ``energy_width`` (bin width) or ``energy_lo`` and ``energy_hi``\n        (Energies of the lower and upper bin edges) can be provided. If none\n        are provided, ``generate_energy_edges`` will be used.\n\n    ul : boolean or int array, optional\n        Boolean array indicating which of the flux values given in ``flux``\n        correspond to upper limits.\n\n    cl : float, optional\n        Confidence level of the flux upper limits given by ``ul``.\n\n    Returns\n    -------\n    data : :class:`astropy.table.QTable`\n        Data stored in an astropy Table.\n    \"\"\"\n    table = QTable()\n\n    if cl is not None:\n        cl = validate_scalar(\"cl\", cl)\n        table.meta[\"keywords\"] = {\"cl\": {\"value\": cl}}\n\n    table[\"energy\"] = energy\n\n    if energy_width is not None:\n        table[\"energy_width\"] = energy_width\n    elif energy_lo is not None and energy_hi is not None:\n        table[\"energy_lo\"] = energy_lo\n        table[\"energy_hi\"] = energy_hi\n\n    table[\"flux\"] = flux\n\n    if flux_error is not None:\n        table[\"flux_error\"] = flux_error\n    elif flux_error_lo is not None and flux_error_hi is not None:\n        table[\"flux_error_lo\"] = flux_error_lo\n        table[\"flux_error_hi\"] = flux_error_hi\n    else:\n        raise TypeError(\"Flux error not provided!\")\n\n    if ul is not None:\n        ul = np.array(ul, dtype=np.int)\n        table[\"ul\"] = ul\n\n    table.meta[\"comments\"] = [\"Table generated with naima.build_data_table\"]\n\n    # test table units, format, etc\n    validate_data_table(table)\n\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef estimate_B(\n    xray_table, vhe_table, photon_energy_density=0.261 * u.eV / u.cm ** 3\n):\n    \"\"\" Estimate magnetic field from synchrotron to Inverse Compton luminosity\n    ratio\n\n    Estimate the magnetic field from the ratio of X-ray to gamma-ray emission\n    according to:\n\n    .. math::\n\n        \\\\frac{L_\\mathrm{xray}}{L_\\gamma} =\n        \\\\frac{u_\\mathrm{B}}{u_\\mathrm{ph}} =\n        \\\\frac{B^2}{ 8 \\pi u_\\mathrm{ph}}\n\n    where :math:`L_\\mathrm{xray}` is the X-ray luminosity, :math:`L_\\gamma` is\n    the gamma-ray luminosity, and :math:`u_\\mathrm{ph}` is the seed photon\n    field energy density.\n\n    Note that this assumes that the ratio of observed fluxes is equal to the\n    ratio of bolometric synchrotron and IC luminosities, and that IC proceeds\n    in the Thomson regims. This assumption is safe as long as the X-ray and\n    gamma-ray emission contain the bulk of the bolometric emission (i.e., the\n    peak in the SED is in the X-ray and gamma-ray observed bands). Even if the\n    assumption does not hold, this is a good starting point for the magnetic\n    field when doing simultaneous X-ray and gamma-ray spectral fits.\n\n    Parameters\n    ----------\n\n    xray_table : :class:`~astropy.table.Table`\n        Data table (see :ref:`dataformat` for details on the format) containing\n        the X-ray spectrum.\n    vhe_table : :class:`~astropy.table.Table`\n        Data table (see :ref:`dataformat` for details on the format) containing\n        the HE/VHE gamma-ray spectrum.\n\n    photon_energy_density : :class:`~astropy.units.Quantity` float, optional\n        Energy density of the seed photon field for IC emission. Defaults to\n        0.261 eV/cm3, the energy density of the CMB.\n\n    Returns\n    -------\n\n    B : :class:`~astropy.units.Quantity` float\n        Estimate of the magnetic flux density at the emitter.\n    \"\"\"\n\n    xray = validate_data_table(xray_table, sed=False)\n    vhe = validate_data_table(vhe_table, sed=False)\n\n    xray_lum = trapz_loglog(xray[\"flux\"] * xray[\"energy\"], xray[\"energy\"])\n    vhe_lum = trapz_loglog(vhe[\"flux\"] * vhe[\"energy\"], vhe[\"energy\"])\n\n    uph = (photon_energy_density.to(\"erg/cm3\")).value\n\n    B0 = (\n        np.sqrt((xray_lum / vhe_lum).decompose().value * 8 * np.pi * uph) * u.G\n    ).to(\"uG\")\n\n    return B0", "response": "Estimate the magnetic field from synchrotron to Inverse Compton luminosity ratio of X - ray and gamma - ray emissions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_server_context(domain, container_path, context_path=None, use_ssl=True, verify_ssl=True, api_key=None):\n    # type: (str, str, str, bool, bool, str) -> ServerContext\n    \"\"\"\n    Create a LabKey server context. This context is used to encapsulate properties\n    about the LabKey server that is being requested against. This includes, but is not limited to,\n    the domain, container_path, if the server is using SSL, and CSRF token request.\n    :param domain:\n    :param container_path:\n    :param context_path:\n    :param use_ssl:\n    :param verify_ssl:\n    :param api_key:\n    :return:\n    \"\"\"\n    config = dict(\n        domain=domain,\n        container_path=container_path,\n        context_path=context_path,\n        use_ssl=use_ssl,\n        verify_ssl=verify_ssl,\n        api_key=api_key\n    )\n\n    return ServerContext(**config)", "response": "Create a LabKey server context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a URL from a controller and action.", "response": "def build_url(server_context, controller, action, container_path=None):\n    # type: (ServerContext, str, str, str) -> str\n    \"\"\"\n    Builds a URL from a controller and an action. Users the server context to determine domain,\n    context path, container, etc.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param controller: The controller to use in building the URL\n    :param action: The action to use in building the URL\n    :param container_path:\n    :return:\n    \"\"\"\n    return server_context.build_url(controller, action, container_path=container_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_wiki(server_context, wiki_name, wiki_body, container_path=None):\n    # Build the URL for reading the wiki page\n    read_wiki_url = server_context.build_url('wiki', 'editWiki.api', container_path=container_path)\n    payload = {\n        'name': wiki_name\n    }\n    headers = {\n        'Content-type': 'application/json'\n    }\n\n    try:\n        read_response = server_context.make_request(read_wiki_url, payload, headers=headers, method='GET',\n                                                    non_json_response=True)\n    except SSLError as e:\n        print(\"There was a problem while attempting to submit the read for the wiki page \" + str(wiki_name) + \" via the URL \" + str(e.geturl()) + \". The HTTP response code was \" + str(e.getcode()))\n        print(\"The HTTP client error was: \" + format(e))\n        return 1  # TODO: this is incorrect, should return 'success'/'error' properly like the docs say\n\n    data = read_response.text\n\n    # Search HTML response for required information on wiki. This is stored in the javascript \n    # variable named \n    #  - _wikiProps: for 14.3 and earlier\n    #  - LABKEY._wiki.setProps for 15.1 and later\n    data_list = data.split('\\n')\n\n    # If LabKey Server is v14.3 or earlier find line containing '_wikiProp'\n    v = next((i for i in range(len(data_list)) if '_wikiProp' in data_list[i]), None)\n\n    # If v = None, then server is running 15.1 or later and find the line  \n    # containing 'LABKEY._wiki.setProps'\n    if v is None: \n        v = next((i for i in range(len(data_list)) if 'LABKEY._wiki.setProps' in data_list[i]), None)\n\n    # Verify that we found the variable in the HTML response. If not \n    # do not proceed\n    if v is None: \n        print(\"There was a problem while attempting to read the data for the wiki page '\" + str(wiki_name) + \"'.\")\n        print(\"The script is unable to find the wiki properties in the HTML response\")\n        return 1  # TODO: this is incorrect, should return 'success'/'error' properly like the docs say\n\n    wiki_vars = {} \n    for j in range(100):\n        # Read each line, until find a javascript closing bracket. \n        if '};' in data_list[v+j+1]:\n            break\n        if '});' in data_list[v+j+1]:\n            break\n        wvar = data_list[v+j+1].rstrip().lstrip().replace('\\'', '').replace(',', '')\n        wiki_vars[wvar.split(':')[0]] = wvar.split(':')[1]\n    \n    # Build the URL for updating the wiki page\n    update_wiki_url = server_context.build_url('wiki', 'saveWiki.api', container_path=container_path)\n    headers = {\n        'Content-type': 'application/json'\n    }\n\n    # Update wiki_vars to use the new wiki content.\n    wiki_vars['name'] = wiki_name\n    wiki_vars['body'] = wiki_body\n\n    try:\n        data = server_context.make_request(update_wiki_url, payload=json_dumps(wiki_vars, sort_keys=True),\n                                           headers=headers, non_json_response=True)\n    except SSLError as e:\n        print(\"There was a problem while attempting to submit the read for the wiki page '\" + str(wiki_name) + \"' via the URL \" + str(e.geturl()) + \". The HTTP response code was \" + str(e.getcode()))\n        print(\"The HTTP client error was: \" + format(e))\n        return 1  # TODO: this is incorrect, should return 'success'/'error' properly like the docs say\n\n    return data", "response": "Update a wiki page with the given body."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initializer_wrapper(actual_initializer, *rest):\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    if actual_initializer is not None:\n        actual_initializer(*rest)", "response": "A wrapper for the actual_initializer function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eval(e, amplitude, e_0, alpha):\n\n        xx = e / e_0\n        return amplitude * xx ** (-alpha)", "response": "One dimensional power law model function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eval(e, amplitude, e_0, alpha, e_cutoff, beta):\n\n        xx = e / e_0\n        return amplitude * xx ** (-alpha) * np.exp(-(e / e_cutoff) ** beta)", "response": "One dimensional power law with an exponential cutoff model function\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eval(e, amplitude, e_0, e_break, alpha_1, alpha_2):\n        K = np.where(e < e_break, 1, (e_break / e_0) ** (alpha_2 - alpha_1))\n        alpha = np.where(e < e_break, alpha_1, alpha_2)\n        return amplitude * K * (e / e_0) ** -alpha", "response": "One dimensional broken power law model function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef G12(x, a):\n    alpha, a, beta, b = a\n    pi26 = np.pi ** 2 / 6.0\n    G = (pi26 + x) * np.exp(-x)\n    tmp = 1 + b * x ** beta\n    g = 1.0 / (a * x ** alpha / tmp + 1.0)\n    return G * g", "response": "Compute G1 from x and a"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the differential flux at a given distance from the source.", "response": "def flux(self, photon_energy, distance=1 * u.kpc):\n        \"\"\"Differential flux at a given distance from the source.\n\n        Parameters\n        ----------\n        photon_energy : :class:`~astropy.units.Quantity` float or array\n            Photon energy array.\n\n        distance : :class:`~astropy.units.Quantity` float, optional\n            Distance to the source. If set to 0, the intrinsic differential\n            luminosity will be returned. Default is 1 kpc.\n        \"\"\"\n\n        spec = self._spectrum(photon_energy)\n\n        if distance != 0:\n            distance = validate_scalar(\n                \"distance\", distance, physical_type=\"length\"\n            )\n            spec /= 4 * np.pi * distance.to(\"cm\") ** 2\n            out_unit = \"1/(s cm2 eV)\"\n        else:\n            out_unit = \"1/(s eV)\"\n\n        return spec.to(out_unit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sed(self, photon_energy, distance=1 * u.kpc):\n        if distance != 0:\n            out_unit = \"erg/(cm2 s)\"\n        else:\n            out_unit = \"erg/s\"\n\n        photon_energy = _validate_ene(photon_energy)\n\n        sed = (self.flux(photon_energy, distance) * photon_energy ** 2.0).to(\n            out_unit\n        )\n\n        return sed", "response": "Calculates the spectral energy distribution at a given distance from the source."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the lorentz factor of the logarithmic system", "response": "def _nelec(self):\n        \"\"\" Particles per unit lorentz factor\n        \"\"\"\n        pd = self.particle_distribution(self._gam * mec2)\n        return pd.to(1 / mec2_unit).value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef We(self):\n        We = trapz_loglog(self._gam * self._nelec, self._gam * mec2)\n        return We", "response": "Total energy in electrons used for the radiative calculation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the total energy in electrons between energies Eemin and Eemax.", "response": "def compute_We(self, Eemin=None, Eemax=None):\n        \"\"\" Total energy in electrons between energies Eemin and Eemax\n\n        Parameters\n        ----------\n        Eemin : :class:`~astropy.units.Quantity` float, optional\n            Minimum electron energy for energy content calculation.\n\n        Eemax : :class:`~astropy.units.Quantity` float, optional\n            Maximum electron energy for energy content calculation.\n        \"\"\"\n        if Eemin is None and Eemax is None:\n            We = self.We\n        else:\n            if Eemax is None:\n                Eemax = self.Eemax\n            if Eemin is None:\n                Eemin = self.Eemin\n\n            log10gmin = np.log10(Eemin / mec2).value\n            log10gmax = np.log10(Eemax / mec2).value\n            gam = np.logspace(\n                log10gmin, log10gmax, int(self.nEed * (log10gmax - log10gmin))\n            )\n            nelec = (\n                self.particle_distribution(gam * mec2).to(1 / mec2_unit).value\n            )\n            We = trapz_loglog(gam * nelec, gam * mec2)\n\n        return We"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_We(self, We, Eemin=None, Eemax=None, amplitude_name=None):\n\n        We = validate_scalar(\"We\", We, physical_type=\"energy\")\n        oldWe = self.compute_We(Eemin=Eemin, Eemax=Eemax)\n\n        if amplitude_name is None:\n            try:\n                self.particle_distribution.amplitude *= (\n                    We / oldWe\n                ).decompose()\n            except AttributeError:\n                log.error(\n                    \"The particle distribution does not have an attribute\"\n                    \" called amplitude to modify its normalization: you can\"\n                    \" set the name with the amplitude_name parameter of set_We\"\n                )\n        else:\n            oldampl = getattr(self.particle_distribution, amplitude_name)\n            setattr(\n                self.particle_distribution,\n                amplitude_name,\n                oldampl * (We / oldWe).decompose(),\n            )", "response": "Set the particle distribution to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes intrinsic synchrotron differential spectrum for the current species.", "response": "def _spectrum(self, photon_energy):\n        \"\"\"Compute intrinsic synchrotron differential spectrum for energies in\n        ``photon_energy``\n\n        Compute synchrotron for random magnetic field according to\n        approximation of Aharonian, Kelner, and Prosekin 2010, PhysRev D 82,\n        3002 (`arXiv:1006.1045 <http://arxiv.org/abs/1006.1045>`_).\n\n        Parameters\n        ----------\n        photon_energy : :class:`~astropy.units.Quantity` instance\n            Photon energy array.\n        \"\"\"\n\n        outspecene = _validate_ene(photon_energy)\n\n        from scipy.special import cbrt\n\n        def Gtilde(x):\n            \"\"\"\n            AKP10 Eq. D7\n\n            Factor ~2 performance gain in using cbrt(x)**n vs x**(n/3.)\n            Invoking crbt only once reduced time by ~40%\n            \"\"\"\n            cb = cbrt(x)\n            gt1 = 1.808 * cb / np.sqrt(1 + 3.4 * cb ** 2.0)\n            gt2 = 1 + 2.210 * cb ** 2.0 + 0.347 * cb ** 4.0\n            gt3 = 1 + 1.353 * cb ** 2.0 + 0.217 * cb ** 4.0\n            return gt1 * (gt2 / gt3) * np.exp(-x)\n\n        log.debug(\"calc_sy: Starting synchrotron computation with AKB2010...\")\n\n        # strip units, ensuring correct conversion\n        # astropy units do not convert correctly for gyroradius calculation\n        # when using cgs (SI is fine, see\n        # https://github.com/astropy/astropy/issues/1687)\n        CS1_0 = np.sqrt(3) * e.value ** 3 * self.B.to(\"G\").value\n        CS1_1 = (\n            2\n            * np.pi\n            * m_e.cgs.value\n            * c.cgs.value ** 2\n            * hbar.cgs.value\n            * outspecene.to(\"erg\").value\n        )\n        CS1 = CS1_0 / CS1_1\n\n        # Critical energy, erg\n        Ec = (\n            3\n            * e.value\n            * hbar.cgs.value\n            * self.B.to(\"G\").value\n            * self._gam ** 2\n        )\n        Ec /= 2 * (m_e * c).cgs.value\n\n        EgEc = outspecene.to(\"erg\").value / np.vstack(Ec)\n        dNdE = CS1 * Gtilde(EgEc)\n        # return units\n        spec = (\n            trapz_loglog(np.vstack(self._nelec) * dNdE, self._gam, axis=0)\n            / u.s\n            / u.erg\n        )\n        spec = spec.to(\"1/(s eV)\")\n\n        return spec"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake input list of seed_photon_fields and fix them into usable format", "response": "def _process_input_seed(seed_photon_fields):\n        \"\"\"\n        take input list of seed_photon_fields and fix them into usable format\n        \"\"\"\n\n        Tcmb = 2.72548 * u.K  # 0.00057 K\n        Tfir = 30 * u.K\n        ufir = 0.5 * u.eV / u.cm ** 3\n        Tnir = 3000 * u.K\n        unir = 1.0 * u.eV / u.cm ** 3\n\n        # Allow for seed_photon_fields definitions of the type 'CMB-NIR-FIR' or\n        # 'CMB'\n        if type(seed_photon_fields) != list:\n            seed_photon_fields = seed_photon_fields.split(\"-\")\n\n        result = OrderedDict()\n\n        for idx, inseed in enumerate(seed_photon_fields):\n            seed = {}\n            if isinstance(inseed, six.string_types):\n                name = inseed\n                seed[\"type\"] = \"thermal\"\n                if inseed == \"CMB\":\n                    seed[\"T\"] = Tcmb\n                    seed[\"u\"] = ar * Tcmb ** 4\n                    seed[\"isotropic\"] = True\n                elif inseed == \"FIR\":\n                    seed[\"T\"] = Tfir\n                    seed[\"u\"] = ufir\n                    seed[\"isotropic\"] = True\n                elif inseed == \"NIR\":\n                    seed[\"T\"] = Tnir\n                    seed[\"u\"] = unir\n                    seed[\"isotropic\"] = True\n                else:\n                    log.warning(\n                        \"Will not use seed {0} because it is not \"\n                        \"CMB, FIR or NIR\".format(inseed)\n                    )\n                    raise TypeError\n            elif type(inseed) == list and (\n                len(inseed) == 3 or len(inseed) == 4\n            ):\n                isotropic = len(inseed) == 3\n\n                if isotropic:\n                    name, T, uu = inseed\n                    seed[\"isotropic\"] = True\n                else:\n                    name, T, uu, theta = inseed\n                    seed[\"isotropic\"] = False\n                    seed[\"theta\"] = validate_scalar(\n                        \"{0}-theta\".format(name), theta, physical_type=\"angle\"\n                    )\n\n                thermal = T.unit.physical_type == \"temperature\"\n\n                if thermal:\n                    seed[\"type\"] = \"thermal\"\n                    validate_scalar(\n                        \"{0}-T\".format(name),\n                        T,\n                        domain=\"positive\",\n                        physical_type=\"temperature\",\n                    )\n                    seed[\"T\"] = T\n                    if uu == 0:\n                        seed[\"u\"] = ar * T ** 4\n                    else:\n                        # pressure has same physical type as energy density\n                        validate_scalar(\n                            \"{0}-u\".format(name),\n                            uu,\n                            domain=\"positive\",\n                            physical_type=\"pressure\",\n                        )\n                        seed[\"u\"] = uu\n                else:\n                    seed[\"type\"] = \"array\"\n                    # Ensure everything is in arrays\n                    T = u.Quantity((T,)).flatten()\n                    uu = u.Quantity((uu,)).flatten()\n\n                    seed[\"energy\"] = validate_array(\n                        \"{0}-energy\".format(name),\n                        T,\n                        domain=\"positive\",\n                        physical_type=\"energy\",\n                    )\n\n                    if np.isscalar(seed[\"energy\"]) or seed[\"energy\"].size == 1:\n                        seed[\"photon_density\"] = validate_scalar(\n                            \"{0}-density\".format(name),\n                            uu,\n                            domain=\"positive\",\n                            physical_type=\"pressure\",\n                        )\n                    else:\n                        if uu.unit.physical_type == \"pressure\":\n                            uu /= seed[\"energy\"] ** 2\n                        seed[\"photon_density\"] = validate_array(\n                            \"{0}-density\".format(name),\n                            uu,\n                            domain=\"positive\",\n                            physical_type=\"differential number density\",\n                        )\n            else:\n                raise TypeError(\n                    \"Unable to process seed photon\"\n                    \" field: {0}\".format(inseed)\n                )\n\n            result[name] = seed\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ani_ic_on_planck(\n        electron_energy, soft_photon_temperature, gamma_energy, theta\n    ):\n        \"\"\"\n        IC cross-section for anisotropic interaction with a blackbody photon\n        spectrum following Eq. 11 of Khangulyan, Aharonian, and Kelner 2014,\n        ApJ 783, 100 (`arXiv:1310.7971 <http://www.arxiv.org/abs/1310.7971>`_).\n\n        `electron_energy` and `gamma_energy` are in units of m_ec^2\n        `soft_photon_temperature` is in units of K\n        `theta` is in radians\n        \"\"\"\n        Ktomec2 = 1.6863699549e-10\n        soft_photon_temperature *= Ktomec2\n\n        gamma_energy = gamma_energy[:, None]\n        # Parameters from Eqs 21, 22\n        a1 = [0.857, 0.153, 1.840, 0.254]\n        a2 = [0.691, 1.330, 1.668, 0.534]\n        z = gamma_energy / electron_energy\n        ttheta = (\n            2.0\n            * electron_energy\n            * soft_photon_temperature\n            * (1.0 - np.cos(theta))\n        )\n        x = z / (1 - z) / ttheta\n        # Eq. 11\n        cross_section = z ** 2 / (2 * (1 - z)) * G12(x, a1) + G12(x, a2)\n        tmp = (soft_photon_temperature / electron_energy) ** 2\n        # r0 = (e**2 / m_e / c**2).to('cm')\n        # (2 * r0 ** 2 * m_e ** 3 * c ** 4 / (pi * hbar ** 3)).cgs\n        tmp *= 2.6318735743809104e16\n        cross_section = tmp * cross_section\n        cc = (gamma_energy < electron_energy) * (electron_energy > 1)\n        return np.where(cc, cross_section, np.zeros_like(cross_section))", "response": "Anisotropic interaction with a planck photon."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iso_ic_on_monochromatic(\n        electron_energy, seed_energy, seed_edensity, gamma_energy\n    ):\n        \"\"\"\n        IC cross-section for an isotropic interaction with a monochromatic\n        photon spectrum following Eq. 22 of Aharonian & Atoyan 1981, Ap&SS 79,\n        321 (`http://adsabs.harvard.edu/abs/1981Ap%26SS..79..321A`_)\n        \"\"\"\n        photE0 = (seed_energy / mec2).decompose().value\n        phn = seed_edensity\n\n        # electron_energy = electron_energy[:, None]\n        gamma_energy = gamma_energy[:, None]\n        photE0 = photE0[:, None, None]\n        phn = phn[:, None, None]\n\n        b = 4 * photE0 * electron_energy\n        w = gamma_energy / electron_energy\n        q = w / (b * (1 - w))\n        fic = (\n            2 * q * np.log(q)\n            + (1 + 2 * q) * (1 - q)\n            + (1.0 / 2.0) * (b * q) ** 2 * (1 - q) / (1 + b * q)\n        )\n\n        gamint = (\n            fic\n            * heaviside(1 - q)\n            * heaviside(q - 1.0 / (4 * electron_energy ** 2))\n        )\n        gamint[np.isnan(gamint)] = 0.0\n\n        if phn.size > 1:\n            phn = phn.to(1 / (mec2_unit * u.cm ** 3)).value\n            gamint = trapz_loglog(gamint * phn / photE0, photE0, axis=0)  # 1/s\n        else:\n            phn = phn.to(mec2_unit / u.cm ** 3).value\n            gamint *= phn / photE0 ** 2\n            gamint = gamint.squeeze()\n\n        # gamint /= mec2.to('erg').value\n\n        # r0 = (e**2 / m_e / c**2).to('cm')\n        # sigt = ((8 * np.pi) / 3 * r0**2).cgs\n        sigt = 6.652458734983284e-25\n        c = 29979245800.0\n\n        gamint *= (3.0 / 4.0) * sigt * c / electron_energy ** 2\n\n        return gamint", "response": "This function calculates the IC cross - section for an isotropic interaction with a monochromatic system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _spectrum(self, photon_energy):\n        outspecene = _validate_ene(photon_energy)\n\n        self.specic = []\n\n        for seed in self.seed_photon_fields:\n            # Call actual computation, detached to allow changes in subclasses\n            self.specic.append(\n                self._calc_specic(seed, outspecene).to(\"1/(s eV)\")\n            )\n\n        return np.sum(u.Quantity(self.specic), axis=0)", "response": "Compute differential IC spectrum for energies in photon_energy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flux(self, photon_energy, distance=1 * u.kpc, seed=None):\n        model = super(InverseCompton, self).flux(\n            photon_energy, distance=distance\n        )\n\n        if seed is not None:\n            # Test seed argument\n            if not isinstance(seed, int):\n                if seed not in self.seed_photon_fields:\n                    raise ValueError(\n                        \"Provided seed photon field name is not in\"\n                        \" the definition of the InverseCompton instance\"\n                    )\n                else:\n                    seed = list(self.seed_photon_fields.keys()).index(seed)\n            elif seed > len(self.seed_photon_fields):\n                raise ValueError(\n                    \"Provided seed photon field number is larger\"\n                    \" than the number of seed photon fields defined in the\"\n                    \" InverseCompton instance\"\n                )\n\n            if distance != 0:\n                distance = validate_scalar(\n                    \"distance\", distance, physical_type=\"length\"\n                )\n                dfac = 4 * np.pi * distance.to(\"cm\") ** 2\n                out_unit = \"1/(s cm2 eV)\"\n            else:\n                dfac = 1\n                out_unit = \"1/(s eV)\"\n\n            model = (self.specic[seed] / dfac).to(out_unit)\n\n        return model", "response": "Returns the flux of a single photon field at a given distance from the source from a single photon field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the sigma of the log2 distribution", "response": "def _sigma_2(gam, eps):\n        \"\"\"\n        gam and eps in units of m_e c^2\n        Eq. A3 of Baring et al. (1999)\n        Return in units of cm2 / mec2\n        \"\"\"\n        s0 = r0 ** 2 * alpha / (3 * eps) / mec2_unit\n\n        s1_1 = 16 * (1 - eps + eps ** 2) * np.log(gam / eps)\n        s1_2 = -1 / eps ** 2 + 3 / eps - 4 - 4 * eps - 8 * eps ** 2\n        s1_3 = -2 * (1 - 2 * eps) * np.log(1 - 2 * eps)\n        s1_4 = 1 / (4 * eps ** 3) - 1 / (2 * eps ** 2) + 3 / eps - 2 + 4 * eps\n        s1 = s1_1 + s1_2 + s1_3 * s1_4\n\n        s2_1 = 2 / eps\n        s2_2 = (4 - 1 / eps + 1 / (4 * eps ** 2)) * np.log(2 * gam)\n        s2_3 = -2 + 2 / eps - 5 / (8 * eps ** 2)\n        s2 = s2_1 * (s2_2 + s2_3)\n\n        return s0 * np.where(eps <= 0.5, s1, s2) * heaviside(gam - eps)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sigma_ee_rel(self, gam, eps):\n        A = 1 - 8 / 3 * (gam - 1) ** 0.2 / (gam + 1) * (eps / gam) ** (\n            1.0 / 3.0\n        )\n\n        return (self._sigma_1(gam, eps) + self._sigma_2(gam, eps)) * A", "response": "Compute the relative sigma of the EE."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the Fisher - Morris term for a given time period.", "response": "def _F(x, gam):\n        \"\"\"\n        Eqs. A6, A7 of Baring et al. (1999)\n        \"\"\"\n        beta = np.sqrt(1 - gam ** -2)\n        B = 1 + 0.5 * (gam ** 2 - 1)\n        C = 10 * x * gam * beta * (2 + gam * beta)\n        C /= 1 + x ** 2 * (gam ** 2 - 1)\n\n        F_1 = (17 - 3 * x ** 2 / (2 - x) ** 2 - C) * np.sqrt(1 - x)\n        F_2 = 12 * (2 - x) - 7 * x ** 2 / (2 - x) - 3 * x ** 4 / (2 - x) ** 3\n        F_3 = np.log((1 + np.sqrt(1 - x)) / np.sqrt(x))\n\n        return B * F_1 + F_2 * F_3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the sigma_ee_nonrel parameter for the entry point.", "response": "def _sigma_ee_nonrel(self, gam, eps):\n        \"\"\"\n        Eq. A5 of Baring et al. (1999)\n        Use for Ee < 2 MeV\n        \"\"\"\n        s0 = 4 * r0 ** 2 * alpha / (15 * eps)\n        x = 4 * eps / (gam ** 2 - 1)\n        sigma_nonrel = s0 * self._F(x, gam)\n        sigma_nonrel[np.where(eps >= 0.25 * (gam ** 2 - 1.0))] = 0.0\n        sigma_nonrel[np.where(gam * np.ones_like(eps) < 1.0)] = 0.0\n        return sigma_nonrel / mec2_unit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing sigma_1 only applies to the ultrarelativistic regime.", "response": "def _sigma_ep(self, gam, eps):\n        \"\"\"\n        Using sigma_1 only applies to the ultrarelativistic regime.\n        Eph > 10 MeV\n        ToDo: add complete e-p cross-section\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            return self._sigma_1(gam, eps)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _emiss_ee(self, Eph):\n        if self.weight_ee == 0.0:\n            return np.zeros_like(Eph)\n\n        gam = np.vstack(self._gam)\n        # compute integral with electron distribution\n        emiss = c.cgs * trapz_loglog(\n            np.vstack(self._nelec) * self._sigma_ee(gam, Eph),\n            self._gam,\n            axis=0,\n        )\n        return emiss", "response": "Compute the emissivity per unit photon energy for a given set of energy energy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _emiss_ep(self, Eph):\n        if self.weight_ep == 0.0:\n            return np.zeros_like(Eph)\n\n        gam = np.vstack(self._gam)\n        eps = (Eph / mec2).decompose().value\n        # compute integral with electron distribution\n        emiss = c.cgs * trapz_loglog(\n            np.vstack(self._nelec) * self._sigma_ep(gam, eps),\n            self._gam,\n            axis=0,\n        ).to(u.cm ** 2 / Eph.unit)\n        return emiss", "response": "Compute the emissivity of a specific entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing differential bremsstrahlung spectrum for energies in photon_energy.", "response": "def _spectrum(self, photon_energy):\n        \"\"\"Compute differential bremsstrahlung spectrum for energies in\n        ``photon_energy``.\n\n        Parameters\n        ----------\n        photon_energy : :class:`~astropy.units.Quantity` instance\n            Photon energy array.\n        \"\"\"\n\n        Eph = _validate_ene(photon_energy)\n\n        spec = self.n0 * (\n            self.weight_ee * self._emiss_ee(Eph)\n            + self.weight_ep * self._emiss_ep(Eph)\n        )\n\n        return spec"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _J(self):\n        pd = self.particle_distribution(self._Ep * u.GeV)\n        return pd.to(\"1/GeV\").value", "response": "Returns the total number of particles per unit proton energy in particles per GeV"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Wp(self):\n        Wp = trapz_loglog(self._Ep * self._J, self._Ep) * u.GeV\n        return Wp.to(\"erg\")", "response": "Total energy in protons\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_Wp(self, Epmin=None, Epmax=None):\n        if Epmin is None and Epmax is None:\n            Wp = self.Wp\n        else:\n            if Epmax is None:\n                Epmax = self.Epmax\n            if Epmin is None:\n                Epmin = self.Epmin\n\n            log10Epmin = np.log10(Epmin.to(\"GeV\").value)\n            log10Epmax = np.log10(Epmax.to(\"GeV\").value)\n            Ep = (\n                np.logspace(\n                    log10Epmin,\n                    log10Epmax,\n                    int(self.nEpd * (log10Epmax - log10Epmin)),\n                )\n                * u.GeV\n            )\n            pdist = self.particle_distribution(Ep)\n            Wp = trapz_loglog(Ep * pdist, Ep).to(\"erg\")\n\n        return Wp", "response": "Compute total energy in protons between energies Epmin and Epmax."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_Wp(self, Wp, Epmin=None, Epmax=None, amplitude_name=None):\n\n        Wp = validate_scalar(\"Wp\", Wp, physical_type=\"energy\")\n        oldWp = self.compute_Wp(Epmin=Epmin, Epmax=Epmax)\n\n        if amplitude_name is None:\n            try:\n                self.particle_distribution.amplitude *= (\n                    Wp / oldWp\n                ).decompose()\n            except AttributeError:\n                log.error(\n                    \"The particle distribution does not have an attribute\"\n                    \" called amplitude to modify its normalization: you can\"\n                    \" set the name with the amplitude_name parameter of set_Wp\"\n                )\n        else:\n            oldampl = getattr(self.particle_distribution, amplitude_name)\n            setattr(\n                self.particle_distribution,\n                amplitude_name,\n                oldampl * (Wp / oldWp).decompose(),\n            )", "response": "Set the particle distribution to the desired energy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the sigma of the inelastic logarithm of the logarithm of the current state of the entry.", "response": "def _sigma_inel(self, Tp):\n        \"\"\"\n        Inelastic cross-section for p-p interaction. KATV14 Eq. 1\n\n        Parameters\n        ----------\n        Tp : float\n            Kinetic energy of proton (i.e. Ep - m_p*c**2) [GeV]\n\n        Returns\n        -------\n        sigma_inel : float\n            Inelastic cross-section for p-p interaction [1/cm2].\n\n        \"\"\"\n        L = np.log(Tp / self._Tth)\n        sigma = 30.7 - 0.96 * L + 0.18 * L ** 2\n        sigma *= (1 - (self._Tth / Tp) ** 1.9) ** 3\n        return sigma * 1e-27"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sigma_pi_loE(self, Tp):\n        m_p = self._m_p\n        m_pi = self._m_pi\n        Mres = 1.1883  # GeV\n        Gres = 0.2264  # GeV\n        s = 2 * m_p * (Tp + 2 * m_p)  # center of mass energy\n        gamma = np.sqrt(Mres ** 2 * (Mres ** 2 + Gres ** 2))\n        K = np.sqrt(8) * Mres * Gres * gamma\n        K /= np.pi * np.sqrt(Mres ** 2 + gamma)\n\n        fBW = m_p * K\n        fBW /= (\n            (np.sqrt(s) - m_p) ** 2 - Mres ** 2\n        ) ** 2 + Mres ** 2 * Gres ** 2\n\n        mu = np.sqrt(\n            (s - m_pi ** 2 - 4 * m_p ** 2) ** 2 - 16 * m_pi ** 2 * m_p ** 2\n        )\n        mu /= 2 * m_pi * np.sqrt(s)\n\n        sigma0 = 7.66e-3  # mb\n\n        sigma1pi = sigma0 * mu ** 1.95 * (1 + mu + mu ** 5) * fBW ** 1.86\n\n        # two pion production\n        sigma2pi = 5.7  # mb\n        sigma2pi /= 1 + np.exp(-9.3 * (Tp - 1.4))\n\n        E2pith = 0.56  # GeV\n        sigma2pi[np.where(Tp < E2pith)] = 0.0\n\n        return (sigma1pi + sigma2pi) * 1e-27", "response": "Compute the sigma of the high - level entry point for the given Tth and Tp."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sigma_pi_midE(self, Tp):\n        m_p = self._m_p\n        Qp = (Tp - self._Tth) / m_p\n        multip = -6e-3 + 0.237 * Qp - 0.023 * Qp ** 2\n        return self._sigma_inel(Tp) * multip", "response": "Compute the sigma of the current pi - mid - E given a given time Tp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sigma_pi_hiE(self, Tp, a):\n        m_p = self._m_p\n        csip = (Tp - 3.0) / m_p\n        m1 = a[0] * csip ** a[3] * (1 + np.exp(-a[1] * csip ** a[4]))\n        m2 = 1 - np.exp(-a[2] * csip ** 0.25)\n        multip = m1 * m2\n        return self._sigma_inel(Tp) * multip", "response": "General expression for Tp > 5 GeV"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing nuclear enhancement factor for a given Tp.", "response": "def _nuclear_factor(self, Tp):\n        \"\"\"\n        Compute nuclear enhancement factor\n        \"\"\"\n        sigmaRpp = 10 * np.pi * 1e-27\n        sigmainel = self._sigma_inel(Tp)\n        sigmainel0 = self._sigma_inel(1e3)  # at 1e3 GeV\n        f = sigmainel / sigmainel0\n        f2 = np.where(f > 1, f, 1.0)\n        G = 1.0 + np.log(f2)\n        # epsilon factors computed from Eqs 21 to 23 with local ISM abundances\n        epsC = 1.37\n        eps1 = 0.29\n        eps2 = 0.1\n\n        epstotal = np.where(\n            Tp > self._Tth,\n            epsC + (eps1 + eps2) * sigmaRpp * G / sigmainel,\n            0.0,\n        )\n\n        if np.any(Tp < 1.0):\n            # nuclear enhancement factor diverges towards Tp = Tth, fix Tp<1 to\n            # eps(1.0) = 1.91\n            loE = np.where((Tp > self._Tth) * (Tp < 1.0))\n            epstotal[loE] = 1.9141\n\n        return epstotal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _spectrum(self, photon_energy):\n\n        # Load LUT if available, otherwise use self._diffsigma\n        if self.useLUT:\n            LUT_base = \"PionDecayKafexhiu14_LUT_\"\n            if self.nuclear_enhancement:\n                LUT_base += \"NucEnh_\"\n            LUT_fname = LUT_base + \"{0}.npz\".format(self.hiEmodel)\n            # only reload LUT if it has changed or hasn't been loaded yet\n            try:\n                if os.path.basename(self.diffsigma.fname) != LUT_fname:\n                    self._loadLUT(LUT_fname)\n            except AttributeError:\n                self._loadLUT(LUT_fname)\n        else:\n            self.diffsigma = self._diffsigma\n\n        Egamma = _validate_ene(photon_energy).to(\"GeV\")\n        Ep = self._Ep * u.GeV\n        J = self._J * u.Unit(\"1/GeV\")\n\n        specpp = []\n        for Eg in Egamma:\n            diffsigma = self.diffsigma(Ep.value, Eg.value) * u.Unit(\"cm2/GeV\")\n            specpp.append(trapz_loglog(diffsigma * J, Ep))\n\n        self.specpp = u.Quantity(specpp)\n\n        self.specpp *= self.nh * c.cgs\n\n        return self.specpp.to(\"1/(s eV)\")", "response": "Compute differential spectrum from the given photon energy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions to compute the Fgamma of the current state of the entry in the logarithm of the logarithm of the entry.", "response": "def _Fgamma(self, x, Ep):\n        \"\"\"\n        KAB06 Eq.58\n\n        Note: Quantities are not used in this function\n\n        Parameters\n        ----------\n        x : float\n            Egamma/Eprot\n        Ep : float\n            Eprot [TeV]\n        \"\"\"\n        L = np.log(Ep)\n        B = 1.30 + 0.14 * L + 0.011 * L ** 2  # Eq59\n        beta = (1.79 + 0.11 * L + 0.008 * L ** 2) ** -1  # Eq60\n        k = (0.801 + 0.049 * L + 0.014 * L ** 2) ** -1  # Eq61\n        xb = x ** beta\n\n        F1 = B * (np.log(x) / x) * ((1 - xb) / (1 + k * xb * (1 - xb))) ** 4\n        F2 = (\n            1.0 / np.log(x)\n            - (4 * beta * xb) / (1 - xb)\n            - (4 * k * beta * xb * (1 - 2 * xb)) / (1 + k * xb * (1 - xb))\n        )\n\n        return F1 * F2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _photon_integrand(self, x, Egamma):\n        try:\n            return (\n                self._sigma_inel(Egamma / x)\n                * self._particle_distribution((Egamma / x))\n                * self._Fgamma(x, Egamma / x)\n                / x\n            )\n        except ZeroDivisionError:\n            return np.nan", "response": "Integrand of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72 of Eq. 72"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the specpp of the current object for the given Egamma.", "response": "def _calc_specpp_hiE(self, Egamma):\n        \"\"\"\n        Spectrum computed as in Eq. 42 for Egamma >= 0.1 TeV\n        \"\"\"\n        # Fixed quad with n=40 is about 15 times faster and is always within\n        # 0.5% of the result of adaptive quad for Egamma>0.1\n        # WARNING: It also produces artifacts for steep distributions (e.g.\n        # Maxwellian) at ~500 GeV. Reverting to adaptative quadrature\n        # from scipy.integrate import fixed_quad\n        # result=c*fixed_quad(self._photon_integrand, 0., 1., args = [Egamma,\n        # ], n = 40)[0]\n        from scipy.integrate import quad\n\n        Egamma = Egamma.to(\"TeV\").value\n        specpp = (\n            c.cgs.value\n            * quad(\n                self._photon_integrand,\n                0.0,\n                1.0,\n                args=Egamma,\n                epsrel=1e-3,\n                epsabs=0,\n            )[0]\n        )\n\n        return specpp * u.Unit(\"1/(s TeV)\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calc_specpp_loE(self, Egamma):\n        from scipy.integrate import quad\n\n        Egamma = Egamma.to(\"TeV\").value\n        Epimin = Egamma + self._m_pi ** 2 / (4 * Egamma)\n\n        result = (\n            2\n            * quad(\n                self._delta_integrand, Epimin, np.inf, epsrel=1e-3, epsabs=0\n            )[0]\n        )\n\n        return result * u.Unit(\"1/(s TeV)\")", "response": "Calculates the specpp for low energies Egamma < 0. 1 TeV"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntotals energy in protons above 1. 22 GeV threshold (erg.", "response": "def Wp(self):\n        \"\"\"Total energy in protons above 1.22 GeV threshold (erg).\n        \"\"\"\n        from scipy.integrate import quad\n\n        Eth = 1.22e-3\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            Wp = quad(\n                lambda x: x * self._particle_distribution(x), Eth, np.Inf\n            )[0]\n\n        return (Wp * u.TeV).to(\"erg\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _spectrum(self, photon_energy):\n\n        outspecene = _validate_ene(photon_energy)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            self.nhat = 1.0  # initial value, works for index~2.1\n            if np.any(outspecene < self.Etrans) and np.any(\n                outspecene >= self.Etrans\n            ):\n                # compute value of nhat so that delta functional matches\n                # accurate calculation at 0.1TeV\n                full = self._calc_specpp_hiE(self.Etrans)\n                delta = self._calc_specpp_loE(self.Etrans)\n                self.nhat *= (full / delta).decompose().value\n\n            self.specpp = np.zeros(len(outspecene)) * u.Unit(\"1/(s TeV)\")\n\n            for i, Egamma in enumerate(outspecene):\n                if Egamma >= self.Etrans:\n                    self.specpp[i] = self._calc_specpp_hiE(Egamma)\n                else:\n                    self.specpp[i] = self._calc_specpp_loE(Egamma)\n\n        density_factor = (self.nh / (1 * u.Unit(\"1/cm3\"))).decompose().value\n\n        return density_factor * self.specpp.to(\"1/(s eV)\")", "response": "Compute differential spectrum from the photon energy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config_path(appdirs=DEFAULT_APPDIRS, file_name=DEFAULT_CONFIG_FILENAME):\n    return os.path.join(appdirs.user_config_dir, file_name)", "response": "Returns the full path where the config file is stored."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_config_file(config_instance, appdirs=DEFAULT_APPDIRS,\n        file_name=DEFAULT_CONFIG_FILENAME):\n    \"\"\"\n    Write a ConfigParser instance to file at the correct location.\n\n    Args:\n        config_instance: Config instance to safe to file.\n        appdirs (HamsterAppDirs, optional): ``HamsterAppDirs`` instance storing app/user specific\n            path information.\n        file_name (text_type, optional): Name of the config file. Defaults to\n        ``DEFAULT_CONFIG_FILENAME``.\n\n    Returns:\n        SafeConfigParser: Instance written to file.\n    \"\"\"\n\n    path = get_config_path(appdirs, file_name)\n    with open(path, 'w') as fobj:\n        config_instance.write(fobj)\n    return config_instance", "response": "Writes a ConfigParser instance to file at the correct location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_config_file(appdirs=DEFAULT_APPDIRS, file_name=DEFAULT_CONFIG_FILENAME,\n        fallback_config_instance=None):\n    \"\"\"\n    Retrieve config information from file at default location.\n\n    If no config file is found a new one will be created either with ``fallback_config_instance``\n    as content or if none is provided with the result of ``get_default_backend_config``.\n\n    Args:\n        appdirs (HamsterAppDirs, optional): ``HamsterAppDirs`` instance storing app/user specific\n            path information.\n        file_name (text_type, optional): Name of the config file. Defaults to\n        ``DEFAULT_CONFIG_FILENAME``.\n        fallback_config_instance (ConfigParser): Backend config that is to be used to populate the\n            config file that is created if no pre-existing one can be found.\n\n    Returns:\n        SafeConfigParser: Config loaded from file, either from the the  pre-existing config\n            file or the one created with fallback values.\n    \"\"\"\n    if not fallback_config_instance:\n        fallback_config_instance = backend_config_to_configparser(\n            get_default_backend_config(appdirs)\n        )\n\n    config = SafeConfigParser()\n    path = get_config_path(appdirs, file_name)\n    if not config.read(path):\n        config = write_config_file(\n            fallback_config_instance, appdirs=appdirs, file_name=file_name\n        )\n    return config", "response": "Loads the config file at the default location."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary with a default configuration for the current backend.", "response": "def get_default_backend_config(appdirs):\n    \"\"\"\n    Return a default config dictionary.\n\n    Args:\n        appdirs (HamsterAppDirs): ``HamsterAppDirs`` instance encapsulating the apps details.\n\n    Returns:\n        dict: Dictionary with a default configuration.\n\n    Note:\n        Those defaults are independent of the particular config-store.\n    \"\"\"\n    return {\n        'store': 'sqlalchemy',\n        'day_start': datetime.time(5, 30, 0),\n        'fact_min_delta': 1,\n        'tmpfile_path': os.path.join(appdirs.user_data_dir, '{}.tmp'.format(appdirs.appname)),\n        'db_engine': 'sqlite',\n        'db_path': os.path.join(appdirs.user_data_dir, '{}.sqlite'.format(appdirs.appname)),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef backend_config_to_configparser(config):\n    def get_store():\n        return config.get('store')\n\n    def get_day_start():\n        day_start = config.get('day_start')\n        if day_start:\n            day_start = day_start.strftime('%H:%M:%S')\n        return day_start\n\n    def get_fact_min_delta():\n        return text_type(config.get('fact_min_delta'))\n\n    def get_tmpfile_path():\n        return text_type(config.get('tmpfile_path'))\n\n    def get_db_engine():\n        return text_type(config.get('db_engine'))\n\n    def get_db_path():\n        return text_type(config.get('db_path'))\n\n    def get_db_host():\n        return text_type(config.get('db_host'))\n\n    def get_db_port():\n        return text_type(config.get('db_port'))\n\n    def get_db_name():\n        return text_type(config.get('db_name'))\n\n    def get_db_user():\n        return text_type(config.get('db_user'))\n\n    def get_db_password():\n        return text_type(config.get('db_password'))\n\n    cp_instance = SafeConfigParser()\n    cp_instance.add_section('Backend')\n    cp_instance.set('Backend', 'store', get_store())\n    cp_instance.set('Backend', 'day_start', get_day_start())\n    cp_instance.set('Backend', 'fact_min_delta', get_fact_min_delta())\n    cp_instance.set('Backend', 'tmpfile_path', get_tmpfile_path())\n    cp_instance.set('Backend', 'db_engine', get_db_engine())\n    cp_instance.set('Backend', 'db_path', get_db_path())\n    cp_instance.set('Backend', 'db_host', get_db_host())\n    cp_instance.set('Backend', 'db_port', get_db_port())\n    cp_instance.set('Backend', 'db_name', get_db_name())\n    cp_instance.set('Backend', 'db_user', get_db_user())\n    cp_instance.set('Backend', 'db_password', get_db_password())\n\n    return cp_instance", "response": "Converts a backend config dictionary to a ConfigParser instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configparser_to_backend_config(cp_instance):\n    def get_store():\n        # [TODO]\n        # This should be deligated to a dedicated validation function!\n        store = cp_instance.get('Backend', 'store')\n        if store not in hamster_lib.REGISTERED_BACKENDS.keys():\n            raise ValueError(_(\"Unrecognized store option.\"))\n        return store\n\n    def get_day_start():\n        try:\n            day_start = datetime.datetime.strptime(cp_instance.get('Backend',\n                'day_start'), '%H:%M:%S').time()\n        except ValueError:\n            raise ValueError(_(\n                \"We encountered an error when parsing configs 'day_start'\"\n                \" value! Aborting ...\"\n            ))\n        return day_start\n\n    def get_fact_min_delta():\n        return cp_instance.getint('Backend', 'fact_min_delta')\n\n    def get_tmpfile_path():\n        return cp_instance.get('Backend', 'tmpfile_path')\n\n    def get_db_engine():\n        return text_type(cp_instance.get('Backend', 'db_engine'))\n\n    def get_db_path():\n        return text_type(cp_instance.get('Backend', 'db_path'))\n\n    def get_db_host():\n        return text_type(cp_instance.get('Backend', 'db_host'))\n\n    def get_db_port():\n        return cp_instance.getint('Backend', 'db_port')\n\n    def get_db_name():\n        return text_type(cp_instance.get('Backend', 'db_name'))\n\n    def get_db_user():\n        return text_type(cp_instance.get('Backend', 'db_user'))\n\n    def get_db_password():\n        return text_type(cp_instance.get('Backend', 'db_password'))\n\n    result = {\n        'store': get_store(),\n        'day_start': get_day_start(),\n        'fact_min_delta': get_fact_min_delta(),\n        'tmpfile_path': get_tmpfile_path(),\n        'db_engine': get_db_engine(),\n        'db_path': get_db_path(),\n        'db_host': get_db_host(),\n        'db_port': get_db_port(),\n        'db_name': get_db_name(),\n        'db_user': get_db_user(),\n        'db_password': get_db_password(),\n    }\n    return result", "response": "Return a config dict generated from a configparser instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_data_dir(self):\n        directory = appdirs.user_data_dir(self.appname, self.appauthor,\n                             version=self.version, roaming=self.roaming)\n        if self.create:\n            self._ensure_directory_exists(directory)\n        return directory", "response": "Return the user data directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the site config directory.", "response": "def site_config_dir(self):\n        \"\"\"Return ``site_config_dir``.\"\"\"\n        directory = appdirs.site_config_dir(self.appname, self.appauthor,\n                             version=self.version, multipath=self.multipath)\n        if self.create:\n            self._ensure_directory_exists(directory)\n        return directory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_cache_dir(self):\n        directory = appdirs.user_cache_dir(self.appname, self.appauthor,\n                              version=self.version)\n        if self.create:\n            self._ensure_directory_exists(directory)\n        return directory", "response": "Return the user cache directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that the passed path exists.", "response": "def _ensure_directory_exists(self, directory):\n        \"\"\"Ensure that the passed path exists.\"\"\"\n        if not os.path.lexists(directory):\n            os.makedirs(directory)\n        return directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_tmp_fact(filepath):\n    from hamster_lib import Fact\n\n    try:\n        with open(filepath, 'rb') as fobj:\n            fact = pickle.load(fobj)\n    except IOError:\n        fact = False\n    else:\n        if not isinstance(fact, Fact):\n            raise TypeError(_(\n                \"Something went wrong. It seems our pickled file does not contain\"\n                \" valid Fact instance. [Content: '{content}'; Type: {type}\".format(\n                    content=fact, type=type(fact))\n            ))\n    return fact", "response": "Loads an ongoing fact from a given location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_raw_fact(raw_fact):\n    def at_split(string):\n        \"\"\"\n        Return everything in front of the (leftmost) '@'-symbol, if it was used.\n\n        Args:\n            string (str):  The string to be parsed.\n\n        Returns:\n            tuple: (front, back) representing the substrings before and after the\n                most left ``@`` symbol. If no such symbol was present at all,\n                ``back=None``. Both substrings have been trimmed of any leading\n                and trailing whitespace.\n\n        Note:\n            If our string contains multiple ``@`` symbols, all but the most left\n            one will be treated as part of the regular ``back`` string.\n            This allows for usage of the symbol in descriptions, categories and tags.\n\n            Also note that *no tags are extracted* any tags included will be considered\n            part of the ``category`` string. We are likely to remove this parsing function\n            in ``0.14.0`` in favour of a regex based solution so we will not spend\n            time on tags for now\n        \"\"\"\n        result = string.split('@', 1)\n        length = len(result)\n        if length == 1:\n            front, back = result[0].strip(), None\n        else:\n            front, back = result\n            front, back = front.strip(), back.strip()\n        return (front, back)\n\n    def comma_split(string):\n        \"\"\"\n        Split string at the most left comma.\n\n        Args:\n            string (str): String to be processed. At this stage this should\n                look something like ``<Category> and <tags>, <Description>\n\n\n        Returns\n            tuple: (category_and_tags, description). Both substrings have their\n                leading/trailing whitespace removed.\n                ``category_and_tags`` may include >=0 tags indicated by a leading ``#``.\n                As we have used the most left ``,`` to separate both substrings that\n                means that categories and tags can not contain any ``,`` but the\n                description text may contain as many as wished.\n        \"\"\"\n\n        result = string.split(',', 1)\n        length = len(result)\n        if length == 1:\n            category, description = result[0].strip(), None\n        else:\n            category, description = tuple(result)\n            category, description = category.strip(), description.strip()\n        return (category.strip(), description)\n\n    time_info, rest = time_helpers.extract_time_info(raw_fact)\n    activity_name, back = at_split(rest)\n\n    if back:\n        category_name, description = comma_split(back)\n    else:\n        category_name, description = None, None\n\n    return {\n        'timeinfo': time_info,\n        'category': category_name,\n        'activity': activity_name,\n        'description': description,\n    }", "response": "Parses a raw fact text into a dictionary of semantically meaningful sub - components."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_diagnostic_plots(\n    outname,\n    sampler,\n    modelidxs=None,\n    pdf=False,\n    sed=True,\n    blob_labels=None,\n    last_step=False,\n    dpi=100,\n):\n    \"\"\"\n    Generate diagnostic plots.\n\n    - A plot for each of the chain parameters showing walker progression, final\n      sample distribution and several statistical measures of this\n      distribution: ``outname_chain_parN.png`` (see `naima.plot_chain`).\n    - A corner plot of sample density in the two dimensional parameter space of\n      all parameter pairs of the run, with the Maximum Likelihood parameter\n      vector indicated in blue: ``outname_corner.png`` (see `corner.corner`).\n    - A plot for each of the models returned as blobs by the model function.\n      The maximum likelihood model is shown, as well as the 1 and 3 sigma\n      confidence level contours. The first model will be compared with\n      observational data and residuals shown. ``outname_fit_modelN.png`` (see\n      `naima.plot_fit` and `naima.plot_blob`).\n\n    Parameters\n    ----------\n    outname : str\n        Name to be used to save diagnostic plot files.\n\n    sampler : `emcee.EnsembleSampler` instance\n        Sampler instance from which chains, blobs and data are read.\n\n    modelidxs : iterable of integers, optional\n        Model numbers to be plotted. Default: All returned in sampler.blobs\n\n    blob_labels : list of strings, optional\n        Label for each of the outputs of the model. They will be used as title\n        for the corresponding plot.\n\n    pdf : bool, optional\n        Whether to save plots to multipage pdf.\n    \"\"\"\n\n    from .plot import plot_chain, plot_blob, plot_corner\n    from matplotlib import pyplot as plt\n\n    # This function should never be interactive\n    old_interactive = plt.rcParams[\"interactive\"]\n    plt.rcParams[\"interactive\"] = False\n\n    if pdf:\n        plt.rc(\"pdf\", fonttype=42)\n        log.info(\n            \"Saving diagnostic plots in file \" \"{0}_plots.pdf\".format(outname)\n        )\n        from matplotlib.backends.backend_pdf import PdfPages\n\n        outpdf = PdfPages(\"{0}_plots.pdf\".format(outname))\n\n    # Chains\n\n    for par, label in six.moves.zip(\n        six.moves.range(sampler.chain.shape[-1]), sampler.labels\n    ):\n        try:\n            log.info(\"Plotting chain of parameter {0}...\".format(label))\n            f = plot_chain(sampler, par, last_step=last_step)\n            if pdf:\n                f.savefig(outpdf, format=\"pdf\", dpi=dpi)\n            else:\n                if \"log(\" in label or \"log10(\" in label:\n                    label = label.split(\"(\")[-1].split(\")\")[0]\n                f.savefig(\"{0}_chain_{1}.png\".format(outname, label), dpi=dpi)\n            f.clf()\n            plt.close(f)\n        except Exception as e:\n            log.warning(\n                \"plot_chain failed for paramter\"\n                \" {0} ({1}): {2}\".format(label, par, e)\n            )\n\n    # Corner plot\n\n    log.info(\"Plotting corner plot...\")\n\n    f = plot_corner(sampler)\n    if f is not None:\n        if pdf:\n            f.savefig(outpdf, format=\"pdf\", dpi=dpi)\n        else:\n            f.savefig(\"{0}_corner.png\".format(outname), dpi=dpi)\n        f.clf()\n        plt.close(f)\n\n    # Fit\n\n    if modelidxs is None:\n        nmodels = len(sampler.blobs[-1][0])\n        modelidxs = list(six.moves.range(nmodels))\n\n    if isinstance(sed, bool):\n        sed = [sed for idx in modelidxs]\n\n    if blob_labels is None:\n        blob_labels = [\"Model output {0}\".format(idx) for idx in modelidxs]\n    elif len(modelidxs) == 1 and isinstance(blob_labels, str):\n        blob_labels = [blob_labels]\n    elif len(blob_labels) < len(modelidxs):\n        # Add labels\n        n = len(blob_labels)\n        blob_labels += [\n            \"Model output {0}\".format(idx) for idx in modelidxs[n:]\n        ]\n\n    for modelidx, plot_sed, label in six.moves.zip(\n        modelidxs, sed, blob_labels\n    ):\n\n        try:\n            log.info(\"Plotting {0}...\".format(label))\n            f = plot_blob(\n                sampler,\n                blobidx=modelidx,\n                label=label,\n                sed=plot_sed,\n                n_samples=100,\n                last_step=last_step,\n            )\n            if pdf:\n                f.savefig(outpdf, format=\"pdf\", dpi=dpi)\n            else:\n                f.savefig(\n                    \"{0}_model{1}.png\".format(outname, modelidx), dpi=dpi\n                )\n            f.clf()\n            plt.close(f)\n        except Exception as e:\n            log.warning(\"plot_blob failed for {0}: {1}\".format(label, e))\n\n    if pdf:\n        outpdf.close()\n\n    # set interactive back to original\n    plt.rcParams[\"interactive\"] = old_interactive", "response": "Generates and saves the diagnostic plots for each of the chain parameters and the model functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_results_table(\n    outname,\n    sampler,\n    format=\"ascii.ecsv\",\n    convert_log=True,\n    last_step=False,\n    include_blobs=True,\n):\n    \"\"\"\n    Save an ASCII table with the results stored in the\n    `~emcee.EnsembleSampler`.\n\n    The table contains the median, 16th and 84th percentile confidence region\n    (~1sigma) for each parameter.\n\n    Parameters\n    ----------\n    outname : str\n        Root name to be used to save the table. ``_results.dat`` will be\n        appended for the output filename.\n\n    sampler : `emcee.EnsembleSampler` instance\n        Sampler instance from which chains, blobs and data are read.\n\n    format : str, optional\n        Format of the saved table. Must be a format string accepted by\n        `astropy.table.Table.write`, see the `astropy unified file read/write\n        interface documentation\n        <https://astropy.readthedocs.org/en/latest/io/unified.html>`_. Only the\n        ``ascii.ecsv`` and ``ascii.ipac`` formats are able to preserve all the\n        information stored in the ``run_info`` dictionary of the sampler.\n        Defaults to ``ascii.ecsv`` if available (only in astropy > v1.0), else\n        ``ascii.ipac``.\n\n    convert_log : bool, optional\n        Whether to convert natural or base-10 logarithms into original values\n        in addition to saving the logarithm value.\n\n    last_step : bool, optional\n        Whether to only use the positions in the final step of the run (True,\n        default) or the whole chain (False).\n\n    include_blobs : bool, optional\n        Whether to save the distribution properties of the scalar blobs in the\n        sampler. Default is True.\n\n    Returns\n    -------\n\n    table : `~astropy.table.Table`\n        Table with the results.\n    \"\"\"\n\n    if not HAS_PYYAML and format == \"ascii.ecsv\":\n        format = \"ascii.ipac\"\n        log.warning(\n            \"PyYAML package is required for ECSV format,\"\n            \" falling back to {0}...\".format(format)\n        )\n    elif format not in [\"ascii.ecsv\", \"ascii.ipac\"]:\n        log.warning(\n            \"The chosen table format does not have an astropy\"\n            \" writer that suppports metadata writing, no run info\"\n            \" will be saved to the file!\"\n        )\n\n    file_extension = \"dat\"\n    if format == \"ascii.ecsv\":\n        file_extension = \"ecsv\"\n\n    log.info(\n        \"Saving results table in {0}_results.{1}\".format(\n            outname, file_extension\n        )\n    )\n\n    labels = sampler.labels\n\n    if last_step:\n        dists = sampler.chain[:, -1, :]\n    else:\n        dists = sampler.flatchain\n\n    quant = [16, 50, 84]\n    # Do we need more info on the distributions?\n    t = Table(\n        names=[\"label\", \"median\", \"unc_lo\", \"unc_hi\"],\n        dtype=[\"S72\", \"f8\", \"f8\", \"f8\"],\n    )\n    t[\"label\"].description = \"Name of the parameter\"\n    t[\"median\"].description = \"Median of the posterior distribution function\"\n    t[\"unc_lo\"].description = (\n        \"Difference between the median and the\"\n        \" {0}th percentile of the pdf, ~1sigma lower uncertainty\".format(\n            quant[0]\n        )\n    )\n    t[\"unc_hi\"].description = (\n        \"Difference between the {0}th percentile\"\n        \" and the median of the pdf, ~1sigma upper uncertainty\".format(\n            quant[2]\n        )\n    )\n\n    metadata = {}\n    # Start with info from the distributions used for storing the results\n    metadata[\"n_samples\"] = dists.shape[0]\n    # save ML parameter vector and best/median loglikelihood\n    ML, MLp, MLerr, _ = find_ML(sampler, None)\n    metadata[\"ML_pars\"] = [float(p) for p in MLp]\n    metadata[\"MaxLogLikelihood\"] = float(ML)\n\n    # compute and save BIC\n    BIC = len(MLp) * np.log(len(sampler.data)) - 2 * ML\n    metadata[\"BIC\"] = BIC\n\n    # And add all info stored in the sampler.run_info dict\n    if hasattr(sampler, \"run_info\"):\n        metadata.update(sampler.run_info)\n\n    for p, label in enumerate(labels):\n        dist = dists[:, p]\n        xquant = np.percentile(dist, quant)\n        quantiles = dict(six.moves.zip(quant, xquant))\n        med = quantiles[50]\n        lo, hi = med - quantiles[16], quantiles[84] - med\n\n        t.add_row((label, med, lo, hi))\n\n        if convert_log and (\"log10(\" in label or \"log(\" in label):\n            nlabel = label.split(\"(\")[-1].split(\")\")[0]\n            ltype = label.split(\"(\")[0]\n            if ltype == \"log10\":\n                new_dist = 10 ** dist\n            elif ltype == \"log\":\n                new_dist = np.exp(dist)\n\n            quantiles = dict(\n                six.moves.zip(quant, np.percentile(new_dist, quant))\n            )\n            med = quantiles[50]\n            lo, hi = med - quantiles[16], quantiles[84] - med\n\n            t.add_row((nlabel, med, lo, hi))\n\n    if include_blobs:\n        nblobs = len(sampler.blobs[-1][0])\n        for idx in range(nblobs):\n            blob0 = sampler.blobs[-1][0][idx]\n\n            IS_SCALAR = False\n            if isinstance(blob0, u.Quantity):\n                if blob0.size == 1:\n                    IS_SCALAR = True\n                    unit = blob0.unit\n            elif np.isscalar(blob0):\n                IS_SCALAR = True\n                unit = None\n\n            if IS_SCALAR:\n                if last_step:\n                    blobl = [m[idx] for m in sampler.blobs[-1]]\n                else:\n                    blobl = []\n                    for step in sampler.blobs:\n                        for walkerblob in step:\n                            blobl.append(walkerblob[idx])\n                if unit:\n                    dist = np.array([b.value for b in blobl])\n                    metadata[\"blob{0}_unit\".format(idx)] = unit.to_string()\n                else:\n                    dist = np.array(blobl)\n\n                quantiles = dict(\n                    six.moves.zip(quant, np.percentile(dist, quant))\n                )\n                med = quantiles[50]\n                lo, hi = med - quantiles[16], quantiles[84] - med\n\n                t.add_row((\"blob{0}\".format(idx), med, lo, hi))\n\n    if format == \"ascii.ipac\":\n        # Only keywords are written to IPAC tables\n        t.meta[\"keywords\"] = {}\n        for di in metadata.items():\n            t.meta[\"keywords\"][di[0]] = {\"value\": di[1]}\n    else:\n        if format == \"ascii.ecsv\":\n            # there can be no numpy arrays in the metadata (YAML doesn't like\n            # them)\n            for di in list(metadata.items()):\n                if type(di[1]).__module__ == np.__name__:\n                    try:\n                        # convert arrays\n                        metadata[di[0]] = [np.asscalar(a) for a in di[1]]\n                    except TypeError:\n                        # convert scalars\n                        metadata[di[0]] = np.asscalar(di[1])\n        # Save it directly in meta for readability in ECSV\n        t.meta.update(metadata)\n\n    t.write(\"{0}_results.{1}\".format(outname, file_extension), format=format)\n\n    return t", "response": "Save the results table for the given sampler."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the sampler chain data table parameter labels metadata blobs and run information to an hdf5 file.", "response": "def save_run(filename, sampler, compression=True, clobber=False):\n    \"\"\"\n    Save the sampler chain, data table, parameter labels, metadata blobs, and\n    run information to a hdf5 file.\n\n    The data table and parameter labels stored in the sampler will also be\n    saved to the hdf5 file.\n\n    Parameters\n    ----------\n    filename : str\n        Filename for hdf5 file. If the filename extension is not 'h5' or\n        'hdf5', the suffix '_chain.h5' will be appended to the filename.\n\n    sampler : `emcee.EnsembleSampler` instance\n        Sampler instance for which chain and run information is saved.\n\n    compression : bool, optional\n        Whether gzip compression is applied to the dataset on write. Default is\n        True.\n\n    clobber : bool, optional\n        Whether to overwrite the output filename if it exists.\n    \"\"\"\n\n    if filename.split(\".\")[-1] not in [\"h5\", \"hdf5\"]:\n        filename += \"_chain.h5\"\n\n    if os.path.exists(filename) and not clobber:\n        log.warning(\n            \"Not writing file because file exists and clobber is False\"\n        )\n        return\n\n    f = h5py.File(filename, \"w\")\n    group = f.create_group(\"sampler\")\n    group.create_dataset(\"chain\", data=sampler.chain, compression=compression)\n    group.create_dataset(\n        \"lnprobability\", data=sampler.lnprobability, compression=compression\n    )\n\n    # blobs\n    blob = sampler.blobs[-1][0]\n    for idx, item in enumerate(blob):\n        if isinstance(item, u.Quantity):\n            # scalar or array quantity\n            units = [item.unit.to_string()]\n        elif isinstance(item, float):\n            units = [\"\"]\n        elif isinstance(item, tuple) or isinstance(item, list):\n            arearrs = np.all([isinstance(x, np.ndarray) for x in item])\n            if arearrs:\n                units = []\n                for x in item:\n                    if isinstance(x, u.Quantity):\n                        units.append(x.unit.to_string())\n                    else:\n                        units.append(\"\")\n        else:\n            log.warning(\n                \"blob number {0} has unknown format and cannot be saved \"\n                \"in HDF5 file\"\n            )\n            continue\n\n        # traverse blobs list. This will probably be slow and there should be a\n        # better way\n        blob = []\n        for step in sampler.blobs:\n            for walkerblob in step:\n                blob.append(walkerblob[idx])\n        blob = u.Quantity(blob).value\n\n        blobdataset = group.create_dataset(\n            \"blob{0}\".format(idx), data=blob, compression=compression\n        )\n        if len(units) > 1:\n            for j, unit in enumerate(units):\n                blobdataset.attrs[\"unit{0}\".format(j)] = unit\n        else:\n            blobdataset.attrs[\"unit\"] = units[0]\n\n    if hasattr(sampler, \"data\"):\n        data = group.create_dataset(\n            \"data\",\n            data=Table(sampler.data).as_array(),\n            compression=compression,\n        )\n\n        for col in sampler.data.colnames:\n            f[\"sampler/data\"].attrs[col + \"unit\"] = str(sampler.data[col].unit)\n\n        for key in sampler.data.meta:\n            val = sampler.data.meta[key]\n            try:\n                data.attrs[key] = val\n            except TypeError:\n                try:\n                    data.attrs[key] = str(val)\n                except:\n                    warnings.warn(\n                        \"Attribute `{0}` of type {1} of the data table\"\n                        \" of the sampler cannot be written to HDF5 files\"\n                        \"- skipping\".format(key, type(val)),\n                        AstropyUserWarning,\n                    )\n\n    # add all run info to group attributes\n    if hasattr(sampler, \"run_info\"):\n        for key in sampler.run_info.keys():\n            val = sampler.run_info[key]\n            try:\n                group.attrs[key] = val\n            except TypeError:\n                group.attrs[key] = str(val)\n\n    # add other sampler info to the attrs\n    group.attrs[\"acceptance_fraction\"] = np.mean(sampler.acceptance_fraction)\n\n    # add labels as individual attrs (there might be a better way)\n    for i, label in enumerate(sampler.labels):\n        group.attrs[\"label{0}\".format(i)] = label\n\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_run(filename, modelfn=None):\n\n    # initialize empty sampler class to return\n    result = _result()\n    result.modelfn = modelfn\n    result.run_info = {}\n\n    f = h5py.File(filename, \"r\")\n    # chain and lnprobability\n    result.chain = np.array(f[\"sampler/chain\"])\n    result.lnprobability = np.array(f[\"sampler/lnprobability\"])\n\n    # blobs\n    result.blobs = []\n    shape = result.chain.shape[:2]\n    blobs = []\n    blobrank = []\n    for i in range(100):\n        # first read each of the blobs and convert to Quantities\n        try:\n            ds = f[\"sampler/blob{0}\".format(i)]\n            rank = np.ndim(ds[0])\n            blobrank.append(rank)\n            if rank <= 1:\n                blobs.append(u.Quantity(ds.value, unit=ds.attrs[\"unit\"]))\n            else:\n                blob = []\n                for j in range(np.ndim(ds[0])):\n                    blob.append(\n                        u.Quantity(\n                            ds.value[:, j, :],\n                            unit=ds.attrs[\"unit{0}\".format(j)],\n                        )\n                    )\n                blobs.append(blob)\n        except KeyError:\n            break\n\n    # Now organize in an awful list of lists of arrays\n    for step in range(shape[1]):\n        steplist = []\n        for walker in range(shape[0]):\n            n = step * shape[0] + walker\n            walkerblob = []\n            for j in range(len(blobs)):\n                if blobrank[j] <= 1:\n                    walkerblob.append(blobs[j][n])\n                else:\n                    blob = []\n                    for k in range(blobrank[j]):\n                        blob.append(blobs[j][k][n])\n                    walkerblob.append(blob)\n            steplist.append(walkerblob)\n        result.blobs.append(steplist)\n\n    # run info\n    result.run_info = dict(f[\"sampler\"].attrs)\n    result.acceptance_fraction = f[\"sampler\"].attrs[\"acceptance_fraction\"]\n    # labels\n    result.labels = []\n    for i in range(result.chain.shape[2]):\n        result.labels.append(f[\"sampler\"].attrs[\"label{0}\".format(i)])\n\n    # data\n    data = Table(np.array(f[\"sampler/data\"]))\n    data.meta.update(f[\"sampler/data\"].attrs)\n    for col in data.colnames:\n        if f[\"sampler/data\"].attrs[col + \"unit\"] != \"None\":\n            data[col].unit = f[\"sampler/data\"].attrs[col + \"unit\"]\n    result.data = QTable(data)\n\n    return result", "response": "Reads a single run from a hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ElectronIC(pars, data):\n\n    ECPL = ExponentialCutoffPowerLaw(\n        pars[0] / u.eV, 10.0 * u.TeV, pars[1], 10 ** pars[2] * u.TeV\n    )\n    IC = InverseCompton(ECPL, seed_photon_fields=[\"CMB\"])\n\n    return IC.flux(data, distance=1.0 * u.kpc)", "response": "Define particle distribution model radiative model and flux at data energy values and return model flux at data energy values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npost a message to a LabKey instance.", "response": "def post_message(server_context, message_title, message_body, render_as, container_path=None):\n    \"\"\"\n    Post a message to a message board on a LabKey instance.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param message_title: The title of the message.\n    :param message_body: The content of the message.\n    :param render_as: The content of the message.\n    :param container_path: Optional container path that can be used to override the server_context container path\n    :return: Returns 1 if successful, 0 is post failed.\n    \"\"\"\n    message_url = server_context.build_url('announcements', 'insert.api', container_path=container_path)\n\n    message_data = {\n        'title': message_title,\n        'body': message_body,\n        'rendererType': render_as\n    }\n\n    try:\n        server_context.make_request(message_url, message_data)\n    except SSLError as e:\n        print(\"There was problem while attempting to submit the message to \" + str(e.geturl()) + \". The HTTP response code was \" + str(e.getcode()))\n        print(\"The HTTP client error was: \" + format(e))\n        return 0\n        \n    return 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning PL or ECPL instance based on parameters p", "response": "def _pdist(p):\n        \"\"\" Return PL or ECPL instance based on parameters p \"\"\"\n        index, ref, ampl, cutoff, beta = p[:5]\n        if cutoff == 0.0:\n            pdist = models.PowerLaw(\n                ampl * 1e30 * u.Unit(\"1/eV\"), ref * u.TeV, index\n            )\n        else:\n            pdist = models.ExponentialCutoffPowerLaw(\n                ampl * 1e30 * u.Unit(\"1/eV\"),\n                ref * u.TeV,\n                index,\n                cutoff * u.TeV,\n                beta=beta,\n            )\n        return pdist"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a diagnostic plot of the sampler chains.", "response": "def plot_chain(sampler, p=None, **kwargs):\n    \"\"\"Generate a diagnostic plot of the sampler chains.\n\n    Parameters\n    ----------\n    sampler : `emcee.EnsembleSampler`\n        Sampler containing the chains to be plotted.\n    p : int (optional)\n        Index of the parameter to plot. If omitted, all chains are plotted.\n    last_step : bool (optional)\n        Whether to plot the last step of the chain or the complete chain\n        (default).\n\n    Returns\n    -------\n    figure : `matplotlib.figure.Figure`\n        Figure\n    \"\"\"\n    if p is None:\n        npars = sampler.chain.shape[-1]\n        for pp in six.moves.range(npars):\n            _plot_chain_func(sampler, pp, **kwargs)\n        fig = None\n    else:\n        fig = _plot_chain_func(sampler, p, **kwargs)\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _latex_float(f, format=\".3g\"):\n    float_str = \"{{0:{0}}}\".format(format).format(f)\n    if \"e\" in float_str:\n        base, exponent = float_str.split(\"e\")\n        return r\"{0}\\times 10^{{{1}}}\".format(base, int(exponent))\n    else:\n        return float_str", "response": "Convert a float to LaTeX"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing binary blob in sampler.", "response": "def _process_blob(sampler, modelidx, last_step=False, energy=None):\n    \"\"\"\n    Process binary blob in sampler. If blob in position modelidx is:\n\n    - a Quantity array of len(blob[i])=len(data['energy']: use blob as model,\n      data['energy'] as modelx\n    - a tuple: use first item as modelx, second as model\n    - a Quantity scalar: return array of scalars\n    \"\"\"\n\n    # Allow process blob to be used by _calc_samples and _calc_ML by sending\n    # only blobs, not full sampler\n    if hasattr(sampler, \"blobs\"):\n        blob0 = sampler.blobs[-1][0][modelidx]\n        blobs = sampler.blobs\n        energy = sampler.data[\"energy\"]\n    else:\n        blobs = [sampler]\n        blob0 = sampler[0][modelidx]\n        last_step = True\n\n    if isinstance(blob0, u.Quantity):\n        if blob0.size == energy.size:\n            # Energy array for blob is not provided, use data['energy']\n            modelx = energy\n        elif blob0.size == 1:\n            modelx = None\n\n        if last_step:\n            model = u.Quantity([m[modelidx] for m in blobs[-1]])\n        else:\n            model = []\n            for step in blobs:\n                for walkerblob in step:\n                    model.append(walkerblob[modelidx])\n            model = u.Quantity(model)\n    elif np.isscalar(blob0):\n        modelx = None\n\n        if last_step:\n            model = u.Quantity([m[modelidx] for m in blobs[-1]])\n        else:\n            model = []\n            for step in blobs:\n                for walkerblob in step:\n                    model.append(walkerblob[modelidx])\n            model = u.Quantity(model)\n    elif isinstance(blob0, list) or isinstance(blob0, tuple):\n        if (\n            len(blob0) == 2\n            and isinstance(blob0[0], u.Quantity)\n            and isinstance(blob0[1], u.Quantity)\n        ):\n            # Energy array for model is item 0 in blob, model flux is item 1\n            modelx = blob0[0]\n\n            if last_step:\n                model = u.Quantity([m[modelidx][1] for m in blobs[-1]])\n            else:\n                model = []\n                for step in blobs:\n                    for walkerblob in step:\n                        model.append(walkerblob[modelidx][1])\n                model = u.Quantity(model)\n        else:\n            raise TypeError(\"Model {0} has wrong blob format\".format(modelidx))\n\n    else:\n        raise TypeError(\"Model {0} has wrong blob format\".format(modelidx))\n\n    return modelx, model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_or_calc_samples(\n    sampler,\n    modelidx=0,\n    n_samples=100,\n    last_step=False,\n    e_range=None,\n    e_npoints=100,\n    threads=None,\n):\n    \"\"\"Get samples from blob or compute them from chain and sampler.modelfn\n    \"\"\"\n\n    if not e_range:\n        # return the results saved in blobs\n        modelx, model = _process_blob(sampler, modelidx, last_step=last_step)\n    else:\n        # prepare bogus data for calculation\n        e_range = validate_array(\n            \"e_range\", u.Quantity(e_range), physical_type=\"energy\"\n        )\n        e_unit = e_range.unit\n        energy = (\n            np.logspace(\n                np.log10(e_range[0].value),\n                np.log10(e_range[1].value),\n                e_npoints,\n            )\n            * e_unit\n        )\n        data = {\n            \"energy\": energy,\n            \"flux\": np.zeros(energy.shape) * sampler.data[\"flux\"].unit,\n        }\n        # init pool and select parameters\n        chain = sampler.chain[-1] if last_step else sampler.flatchain\n        pars = chain[np.random.randint(len(chain), size=n_samples)]\n        blobs = []\n\n        p = Pool(threads)\n        modelouts = p.map(partial(sampler.modelfn, data=data), pars)\n        p.close()\n        p.terminate()\n\n        for modelout in modelouts:\n            if isinstance(modelout, np.ndarray):\n                blobs.append([modelout])\n            else:\n                blobs.append(modelout)\n\n        modelx, model = _process_blob(\n            blobs, modelidx=modelidx, energy=data[\"energy\"]\n        )\n\n    return modelx, model", "response": "Get samples from blob or compute them from chain and sampler. modelfn\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _calc_ML(sampler, modelidx=0, e_range=None, e_npoints=100):\n\n    ML, MLp, MLerr, ML_model = find_ML(sampler, modelidx)\n\n    if e_range is not None:\n        # prepare bogus data for calculation\n        e_range = validate_array(\n            \"e_range\", u.Quantity(e_range), physical_type=\"energy\"\n        )\n        e_unit = e_range.unit\n        energy = (\n            np.logspace(\n                np.log10(e_range[0].value),\n                np.log10(e_range[1].value),\n                e_npoints,\n            )\n            * e_unit\n        )\n        data = {\n            \"energy\": energy,\n            \"flux\": np.zeros(energy.shape) * sampler.data[\"flux\"].unit,\n        }\n        modelout = sampler.modelfn(MLp, data)\n\n        if isinstance(modelout, np.ndarray):\n            blob = modelout\n        else:\n            blob = modelout[modelidx]\n\n        if isinstance(blob, u.Quantity):\n            modelx = data[\"energy\"].copy()\n            model_ML = blob.copy()\n        elif len(blob) == 2:\n            modelx = blob[0].copy()\n            model_ML = blob[1].copy()\n        else:\n            raise TypeError(\"Model {0} has wrong blob format\".format(modelidx))\n\n        ML_model = (modelx, model_ML)\n\n    return ML, MLp, MLerr, ML_model", "response": "Get ML model from blob or compute them from chain and sampler. modelfn\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the confidence interval of a single object.", "response": "def _calc_CI(\n    sampler,\n    modelidx=0,\n    confs=[3, 1],\n    last_step=False,\n    e_range=None,\n    e_npoints=100,\n    threads=None,\n):\n    \"\"\"Calculate confidence interval.\n    \"\"\"\n    from scipy import stats\n\n    # If we are computing the samples for the confidence intervals, we need at\n    # least one sample to constrain the highest confidence band\n    # 1 sigma -> 6 samples\n    # 2 sigma -> 43 samples\n    # 3 sigma -> 740 samples\n    # 4 sigma -> 31574 samples\n    # 5 sigma -> 3488555 samples\n    # We limit it to 1000 samples and warn that it might not be enough\n    if e_range:\n        maxconf = np.max(confs)\n        minsamples = min(100, int(1 / stats.norm.cdf(-maxconf) + 1))\n        if minsamples > 1000:\n            log.warning(\n                \"In order to sample the confidence band for {0} sigma,\"\n                \" {1} new samples need to be computed, but we are limiting\"\n                \" it to 1000 samples, so the confidence band might not be\"\n                \" well constrained.\"\n                \" Consider reducing the maximum\"\n                \" confidence significance or using the samples stored in\"\n                \" the sampler by setting e_range\"\n                \" to None\".format(maxconf, minsamples)\n            )\n            minsamples = 1000\n    else:\n        minsamples = None\n\n    modelx, model = _read_or_calc_samples(\n        sampler,\n        modelidx,\n        last_step=last_step,\n        e_range=e_range,\n        e_npoints=e_npoints,\n        n_samples=minsamples,\n        threads=threads,\n    )\n\n    nwalkers = len(model) - 1\n    CI = []\n    for conf in confs:\n        fmin = stats.norm.cdf(-conf)\n        fmax = stats.norm.cdf(conf)\n        ymin, ymax = [], []\n        for fr, y in ((fmin, ymin), (fmax, ymax)):\n            nf = int((fr * nwalkers))\n            for i in six.moves.range(len(modelx)):\n                ysort = np.sort(model[:, i])\n                y.append(ysort[nf])\n\n        # create an array from lists ymin and ymax preserving units\n        CI.append((u.Quantity(ymin), u.Quantity(ymax)))\n\n    return modelx, CI"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and plot ML model", "response": "def _plot_MLmodel(ax, sampler, modelidx, e_range, e_npoints, e_unit, sed):\n    \"\"\"compute and plot ML model\"\"\"\n\n    ML, MLp, MLerr, ML_model = _calc_ML(\n        sampler, modelidx, e_range=e_range, e_npoints=e_npoints\n    )\n    f_unit, sedf = sed_conversion(ML_model[0], ML_model[1].unit, sed)\n\n    ax.loglog(\n        ML_model[0].to(e_unit).value,\n        (ML_model[1] * sedf).to(f_unit).value,\n        color=\"k\",\n        lw=2,\n        alpha=0.8,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_CI(\n    ax,\n    sampler,\n    modelidx=0,\n    sed=True,\n    confs=[3, 1, 0.5],\n    e_unit=u.eV,\n    label=None,\n    e_range=None,\n    e_npoints=100,\n    threads=None,\n    last_step=False,\n):\n    \"\"\"Plot confidence interval.\n\n    Parameters\n    ----------\n    ax : `matplotlib.Axes`\n        Axes to plot on.\n    sampler : `emcee.EnsembleSampler`\n        Sampler\n    modelidx : int, optional\n        Model index. Default is 0\n    sed : bool, optional\n        Whether to plot SED or differential spectrum. If `None`, the units of\n        the observed spectrum will be used.\n    confs : list, optional\n        List of confidence levels (in sigma) to use for generating the\n        confidence intervals. Default is `[3,1,0.5]`\n    e_unit : :class:`~astropy.units.Unit` or str parseable to unit\n        Unit in which to plot energy axis.\n    e_npoints : int, optional\n        How many points to compute for the model samples and ML model if\n        `e_range` is set.\n    threads : int, optional\n        How many parallel processing threads to use when computing the samples.\n        Defaults to the number of available cores.\n    last_step : bool, optional\n        Whether to only use the positions in the final step of the run (True,\n        default) or the whole chain (False).\n    \"\"\"\n    confs.sort(reverse=True)\n\n    modelx, CI = _calc_CI(\n        sampler,\n        modelidx=modelidx,\n        confs=confs,\n        e_range=e_range,\n        e_npoints=e_npoints,\n        last_step=last_step,\n        threads=threads,\n    )\n    # pick first confidence interval curve for units\n    f_unit, sedf = sed_conversion(modelx, CI[0][0].unit, sed)\n\n    for (ymin, ymax), conf in zip(CI, confs):\n        color = np.log(conf) / np.log(20) + 0.4\n        ax.fill_between(\n            modelx.to(e_unit).value,\n            (ymax * sedf).to(f_unit).value,\n            (ymin * sedf).to(f_unit).value,\n            lw=0.001,\n            color=(color,) * 3,\n            alpha=0.6,\n            zorder=-10,\n        )\n\n    _plot_MLmodel(ax, sampler, modelidx, e_range, e_npoints, e_unit, sed)\n\n    if label is not None:\n        ax.set_ylabel(\n            \"{0} [{1}]\".format(label, f_unit.to_string(\"latex_inline\"))\n        )", "response": "Plots the confidence interval curve for the given sampler and model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a number of samples from the sampler chain.", "response": "def plot_samples(\n    ax,\n    sampler,\n    modelidx=0,\n    sed=True,\n    n_samples=100,\n    e_unit=u.eV,\n    e_range=None,\n    e_npoints=100,\n    threads=None,\n    label=None,\n    last_step=False,\n):\n    \"\"\"Plot a number of samples from the sampler chain.\n\n    Parameters\n    ----------\n    ax : `matplotlib.Axes`\n        Axes to plot on.\n    sampler : `emcee.EnsembleSampler`\n        Sampler\n    modelidx : int, optional\n        Model index. Default is 0\n    sed : bool, optional\n        Whether to plot SED or differential spectrum. If `None`, the units of\n        the observed spectrum will be used.\n    n_samples : int, optional\n        Number of samples to plot. Default is 100.\n    e_unit : :class:`~astropy.units.Unit` or str parseable to unit\n        Unit in which to plot energy axis.\n    e_range : list of `~astropy.units.Quantity`, length 2, optional\n        Limits in energy for the computation of the model samples and ML model.\n        Note that setting this parameter will mean that the samples for the\n        model are recomputed and depending on the model speed might be quite\n        slow.\n    e_npoints : int, optional\n        How many points to compute for the model samples and ML model if\n        `e_range` is set.\n    threads : int, optional\n        How many parallel processing threads to use when computing the samples.\n        Defaults to the number of available cores.\n    last_step : bool, optional\n        Whether to only use the positions in the final step of the run (True,\n        default) or the whole chain (False).\n    \"\"\"\n\n    modelx, model = _read_or_calc_samples(\n        sampler,\n        modelidx,\n        last_step=last_step,\n        e_range=e_range,\n        e_npoints=e_npoints,\n        threads=threads,\n    )\n    # pick first model sample for units\n    f_unit, sedf = sed_conversion(modelx, model[0].unit, sed)\n\n    sample_alpha = min(5.0 / n_samples, 0.5)\n    for my in model[np.random.randint(len(model), size=n_samples)]:\n        ax.loglog(\n            modelx.to(e_unit).value,\n            (my * sedf).to(f_unit).value,\n            color=(0.1,) * 3,\n            alpha=sample_alpha,\n            lw=1.0,\n        )\n\n    _plot_MLmodel(ax, sampler, modelidx, e_range, e_npoints, e_unit, sed)\n\n    if label is not None:\n        ax.set_ylabel(\n            \"{0} [{1}]\".format(label, f_unit.to_string(\"latex_inline\"))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_ML(sampler, modelidx):\n    index = np.unravel_index(\n        np.argmax(sampler.lnprobability), sampler.lnprobability.shape\n    )\n    MLp = sampler.chain[index]\n    if modelidx is not None and hasattr(sampler, \"blobs\"):\n        blob = sampler.blobs[index[1]][index[0]][modelidx]\n        if isinstance(blob, u.Quantity):\n            modelx = sampler.data[\"energy\"].copy()\n            model_ML = blob.copy()\n        elif len(blob) == 2:\n            modelx = blob[0].copy()\n            model_ML = blob[1].copy()\n        else:\n            raise TypeError(\"Model {0} has wrong blob format\".format(modelidx))\n    elif modelidx is not None and hasattr(sampler, \"modelfn\"):\n        blob = _process_blob(\n            [sampler.modelfn(MLp, sampler.data)],\n            modelidx,\n            energy=sampler.data[\"energy\"],\n        )\n        modelx, model_ML = blob[0], blob[1][0]\n    else:\n        modelx, model_ML = None, None\n\n    MLerr = []\n    for dist in sampler.flatchain.T:\n        hilo = np.percentile(dist, [16.0, 84.0])\n        MLerr.append((hilo[1] - hilo[0]) / 2.0)\n    ML = sampler.lnprobability[index]\n\n    return ML, MLp, MLerr, (modelx, model_ML)", "response": "Find Maximum Likelihood parameters as those in the chain with a highest log\n    probability."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a metadata blob as a fit to spectral data or value distribution.", "response": "def plot_blob(\n    sampler, blobidx=0, label=None, last_step=False, figure=None, **kwargs\n):\n    \"\"\"\n    Plot a metadata blob as a fit to spectral data or value distribution\n\n    Additional ``kwargs`` are passed to `plot_fit`.\n\n    Parameters\n    ----------\n    sampler : `emcee.EnsembleSampler`\n        Sampler with a stored chain.\n    blobidx : int, optional\n        Metadata blob index to plot.\n    label : str, optional\n        Label for the value distribution. Labels for the fit plot can be passed\n        as ``xlabel`` and ``ylabel`` and will be passed to `plot_fit`.\n\n    Returns\n    -------\n    figure : `matplotlib.pyplot.Figure`\n        `matplotlib` figure instance containing the plot.\n    \"\"\"\n\n    modelx, model = _process_blob(sampler, blobidx, last_step)\n    if label is None:\n        label = \"Model output {0}\".format(blobidx)\n\n    if modelx is None:\n        # Blob is scalar, plot distribution\n        f = plot_distribution(model, label, figure=figure)\n    else:\n        f = plot_fit(\n            sampler,\n            modelidx=blobidx,\n            last_step=last_step,\n            label=label,\n            figure=figure,\n            **kwargs\n        )\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots data with fit confidence regions.", "response": "def plot_fit(\n    sampler,\n    modelidx=0,\n    label=None,\n    sed=True,\n    last_step=False,\n    n_samples=100,\n    confs=None,\n    ML_info=False,\n    figure=None,\n    plotdata=None,\n    plotresiduals=None,\n    e_unit=None,\n    e_range=None,\n    e_npoints=100,\n    threads=None,\n    xlabel=None,\n    ylabel=None,\n    ulim_opts={},\n    errorbar_opts={},\n):\n    \"\"\"\n    Plot data with fit confidence regions.\n\n    Parameters\n    ----------\n    sampler : `emcee.EnsembleSampler`\n        Sampler with a stored chain.\n    modelidx : int, optional\n        Model index to plot.\n    label : str, optional\n        Label for the title of the plot.\n    sed : bool, optional\n        Whether to plot SED or differential spectrum.\n    last_step : bool, optional\n        Whether to use only the samples of the last step in the run when\n        showing either the model samples or the confidence intervals.\n    n_samples : int, optional\n        If not ``None``, number of sample models to plot. If ``None``,\n        confidence bands will be plotted instead of samples. Default is 100.\n    confs : list, optional\n        List of confidence levels (in sigma) to use for generating the\n        confidence intervals. Default is to plot sample models instead of\n        confidence bands.\n    ML_info : bool, optional\n        Whether to plot information about the maximum likelihood parameters and\n        the standard deviation of their distributions. Default is True.\n    figure : `matplotlib.figure.Figure`, optional\n        `matplotlib` figure to plot on. If omitted a new one will be generated.\n    plotdata : bool, optional\n        Wheter to plot data on top of model confidence intervals. Default is\n        True if the physical types of the data and the model match.\n    plotresiduals : bool, optional\n        Wheter to plot the residuals with respect to the maximum likelihood\n        model. Default is True if ``plotdata`` is True and either ``confs`` or\n        ``n_samples`` are set.\n    e_unit : `~astropy.units.Unit`, optional\n        Units for the energy axis of the plot. The default is to use the units\n        of the energy array of the observed data.\n    e_range : list of `~astropy.units.Quantity`, length 2, optional\n        Limits in energy for the computation of the model samples and ML model.\n        Note that setting this parameter will mean that the samples for the\n        model are recomputed and depending on the model speed might be quite\n        slow.\n    e_npoints : int, optional\n        How many points to compute for the model samples and ML model if\n        `e_range` is set.\n    threads : int, optional\n        How many parallel processing threads to use when computing the samples.\n        Defaults to the number of available cores.\n    xlabel : str, optional\n        Label for the ``x`` axis of the plot.\n    ylabel : str, optional\n        Label for the ``y`` axis of the plot.\n    ulim_opts : dict\n        Option for upper-limit plotting. Available options are capsize (arrow\n        width) and height_fraction (arrow length in fraction of flux value).\n    errorbar_opts : dict\n        Addtional options to pass to `matplotlib.plt.errorbar` for plotting the\n        spectral flux points.\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    ML, MLp, MLerr, model_ML = find_ML(sampler, modelidx)\n    infostr = \"Maximum log probability: {0:.3g}\\n\".format(ML)\n    infostr += \"Maximum Likelihood values:\\n\"\n    maxlen = np.max([len(ilabel) for ilabel in sampler.labels])\n    vartemplate = \"{{2:>{0}}}: {{0:>8.3g}} +/- {{1:<8.3g}}\\n\".format(maxlen)\n    for p, v, ilabel in zip(MLp, MLerr, sampler.labels):\n        infostr += vartemplate.format(p, v, ilabel)\n\n    # log.info(infostr)\n\n    data = sampler.data\n\n    if e_range is None and not hasattr(sampler, \"blobs\"):\n        e_range = data[\"energy\"][[0, -1]] * np.array((1.0 / 3.0, 3.0))\n\n    if plotdata is None and len(model_ML[0]) == len(data[\"energy\"]):\n        model_unit, _ = sed_conversion(model_ML[0], model_ML[1].unit, sed)\n        data_unit, _ = sed_conversion(data[\"energy\"], data[\"flux\"].unit, sed)\n        plotdata = model_unit.is_equivalent(data_unit)\n    elif plotdata is None:\n        plotdata = False\n\n    if plotresiduals is None and plotdata and (confs is not None or n_samples):\n        plotresiduals = True\n\n    if confs is None and not n_samples and plotdata and not plotresiduals:\n        # We actually only want to plot the data, so let's go there\n        return plot_data(\n            sampler.data,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            sed=sed,\n            figure=figure,\n            e_unit=e_unit,\n            ulim_opts=ulim_opts,\n            errorbar_opts=errorbar_opts,\n        )\n\n    if figure is None:\n        f = plt.figure()\n    else:\n        f = figure\n\n    if plotdata and plotresiduals:\n        ax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\n        ax2 = plt.subplot2grid((4, 1), (3, 0), sharex=ax1)\n        for subp in [ax1, ax2]:\n            f.add_subplot(subp)\n    else:\n        ax1 = f.add_subplot(111)\n\n    if e_unit is None:\n        e_unit = data[\"energy\"].unit\n\n    if confs is not None:\n        plot_CI(\n            ax1,\n            sampler,\n            modelidx,\n            sed=sed,\n            confs=confs,\n            e_unit=e_unit,\n            label=label,\n            e_range=e_range,\n            e_npoints=e_npoints,\n            last_step=last_step,\n            threads=threads,\n        )\n    elif n_samples:\n        plot_samples(\n            ax1,\n            sampler,\n            modelidx,\n            sed=sed,\n            n_samples=n_samples,\n            e_unit=e_unit,\n            label=label,\n            e_range=e_range,\n            e_npoints=e_npoints,\n            last_step=last_step,\n            threads=threads,\n        )\n    else:\n        # plot only ML model\n        _plot_MLmodel(ax1, sampler, modelidx, e_range, e_npoints, e_unit, sed)\n\n    xlaxis = ax1\n    if plotdata:\n        _plot_data_to_ax(\n            data,\n            ax1,\n            e_unit=e_unit,\n            sed=sed,\n            ylabel=ylabel,\n            ulim_opts=ulim_opts,\n            errorbar_opts=errorbar_opts,\n        )\n        if plotresiduals:\n            _plot_residuals_to_ax(\n                data,\n                model_ML,\n                ax2,\n                e_unit=e_unit,\n                sed=sed,\n                errorbar_opts=errorbar_opts,\n            )\n            xlaxis = ax2\n            for tl in ax1.get_xticklabels():\n                tl.set_visible(False)\n        xmin = 10 ** np.floor(\n            np.log10(\n                np.min(data[\"energy\"] - data[\"energy_error_lo\"])\n                .to(e_unit)\n                .value\n            )\n        )\n        xmax = 10 ** np.ceil(\n            np.log10(\n                np.max(data[\"energy\"] + data[\"energy_error_hi\"])\n                .to(e_unit)\n                .value\n            )\n        )\n        ax1.set_xlim(xmin, xmax)\n    else:\n        ax1.set_xscale(\"log\")\n        ax1.set_yscale(\"log\")\n        if sed:\n            ndecades = 10\n        else:\n            ndecades = 20\n        # restrict y axis to ndecades to avoid autoscaling deep exponentials\n        xmin, xmax, ymin, ymax = ax1.axis()\n        ymin = max(ymin, ymax / 10 ** ndecades)\n        ax1.set_ylim(bottom=ymin)\n        # scale x axis to largest model_ML x point within ndecades decades of\n        # maximum\n        f_unit, sedf = sed_conversion(model_ML[0], model_ML[1].unit, sed)\n        hi = np.where((model_ML[1] * sedf).to(f_unit).value > ymin)\n        xmax = np.max(model_ML[0][hi])\n        ax1.set_xlim(right=10 ** np.ceil(np.log10(xmax.to(e_unit).value)))\n\n    if e_range:\n        # ensure that xmin/xmax contains e_range\n        xmin, xmax, ymin, ymax = ax1.axis()\n        xmin = min(xmin, e_range[0].to(e_unit).value)\n        xmax = max(xmax, e_range[1].to(e_unit).value)\n        ax1.set_xlim(xmin, xmax)\n\n    if ML_info and (confs is not None or n_samples):\n        ax1.text(\n            0.05,\n            0.05,\n            infostr,\n            ha=\"left\",\n            va=\"bottom\",\n            transform=ax1.transAxes,\n            family=\"monospace\",\n        )\n\n    if label is not None:\n        ax1.set_title(label)\n\n    if xlabel is None:\n        xlaxis.set_xlabel(\n            \"Energy [{0}]\".format(e_unit.to_string(\"latex_inline\"))\n        )\n    else:\n        xlaxis.set_xlabel(xlabel)\n\n    f.subplots_adjust(hspace=0)\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the upper limits as arrows with cap at value of upper limit.", "response": "def _plot_ulims(\n    ax, x, y, xerr, color, capsize=5, height_fraction=0.25, elinewidth=2\n):\n    \"\"\"\n    Plot upper limits as arrows with cap at value of upper limit.\n\n    uplim behaviour has been fixed in matplotlib 1.4\n    \"\"\"\n    ax.errorbar(\n        x, y, xerr=xerr, ls=\"\", color=color, elinewidth=elinewidth, capsize=0\n    )\n\n    from distutils.version import LooseVersion\n    import matplotlib\n\n    mpl_version = LooseVersion(matplotlib.__version__)\n    if mpl_version >= LooseVersion(\"1.4.0\"):\n        ax.errorbar(\n            x,\n            y,\n            yerr=height_fraction * y,\n            ls=\"\",\n            uplims=True,\n            color=color,\n            elinewidth=elinewidth,\n            capsize=capsize,\n            zorder=10,\n        )\n    else:\n        ax.errorbar(\n            x,\n            (1 - height_fraction) * y,\n            yerr=height_fraction * y,\n            ls=\"\",\n            lolims=True,\n            color=color,\n            elinewidth=elinewidth,\n            capsize=capsize,\n            zorder=10,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot data and errorbars on the given axis.", "response": "def _plot_data_to_ax(\n    data_all,\n    ax1,\n    e_unit=None,\n    sed=True,\n    ylabel=None,\n    ulim_opts={},\n    errorbar_opts={},\n):\n    \"\"\" Plots data errorbars and upper limits onto ax.\n    X label is left to plot_data and plot_fit because they depend on whether\n    residuals are plotted.\n    \"\"\"\n\n    if e_unit is None:\n        e_unit = data_all[\"energy\"].unit\n\n    f_unit, sedf = sed_conversion(\n        data_all[\"energy\"], data_all[\"flux\"].unit, sed\n    )\n\n    if \"group\" not in data_all.keys():\n        data_all[\"group\"] = np.zeros(len(data_all))\n\n    groups = np.unique(data_all[\"group\"])\n\n    for g in groups:\n        data = data_all[np.where(data_all[\"group\"] == g)]\n        _, sedfg = sed_conversion(data[\"energy\"], data[\"flux\"].unit, sed)\n\n        # wrap around color and marker cycles\n        color = color_cycle[int(g) % len(color_cycle)]\n        marker = marker_cycle[int(g) % len(marker_cycle)]\n\n        ul = data[\"ul\"]\n        notul = ~ul\n\n        # Hack to show y errors compatible with 0 in loglog plot\n        yerr_lo = data[\"flux_error_lo\"][notul]\n        y = data[\"flux\"][notul].to(yerr_lo.unit)\n        bad_err = np.where((y - yerr_lo) <= 0.0)\n        yerr_lo[bad_err] = y[bad_err] * (1.0 - 1e-7)\n        yerr = u.Quantity((yerr_lo, data[\"flux_error_hi\"][notul]))\n        xerr = u.Quantity((data[\"energy_error_lo\"], data[\"energy_error_hi\"]))\n\n        opts = dict(\n            zorder=100,\n            marker=marker,\n            ls=\"\",\n            elinewidth=2,\n            capsize=0,\n            mec=color,\n            mew=0.1,\n            ms=5,\n            color=color,\n        )\n        opts.update(**errorbar_opts)\n\n        ax1.errorbar(\n            data[\"energy\"][notul].to(e_unit).value,\n            (data[\"flux\"][notul] * sedfg[notul]).to(f_unit).value,\n            yerr=(yerr * sedfg[notul]).to(f_unit).value,\n            xerr=xerr[:, notul].to(e_unit).value,\n            **opts\n        )\n\n        if np.any(ul):\n            if \"elinewidth\" in errorbar_opts:\n                ulim_opts[\"elinewidth\"] = errorbar_opts[\"elinewidth\"]\n\n            _plot_ulims(\n                ax1,\n                data[\"energy\"][ul].to(e_unit).value,\n                (data[\"flux\"][ul] * sedfg[ul]).to(f_unit).value,\n                (xerr[:, ul]).to(e_unit).value,\n                color,\n                **ulim_opts\n            )\n\n    ax1.set_xscale(\"log\")\n    ax1.set_yscale(\"log\")\n    xmin = 10 ** np.floor(\n        np.log10(\n            np.min(data[\"energy\"] - data[\"energy_error_lo\"]).to(e_unit).value\n        )\n    )\n    xmax = 10 ** np.ceil(\n        np.log10(\n            np.max(data[\"energy\"] + data[\"energy_error_hi\"]).to(e_unit).value\n        )\n    )\n    ax1.set_xlim(xmin, xmax)\n    # avoid autoscaling to errorbars to 0\n    notul = ~data_all[\"ul\"]\n    if np.any(data_all[\"flux_error_lo\"][notul] >= data_all[\"flux\"][notul]):\n        elo = (data_all[\"flux\"][notul] * sedf[notul]).to(f_unit).value - (\n            data_all[\"flux_error_lo\"][notul] * sedf[notul]\n        ).to(f_unit).value\n        gooderr = np.where(\n            data_all[\"flux_error_lo\"][notul] < data_all[\"flux\"][notul]\n        )\n        ymin = 10 ** np.floor(np.log10(np.min(elo[gooderr])))\n        ax1.set_ylim(bottom=ymin)\n\n    if ylabel is None:\n        if sed:\n            ax1.set_ylabel(\n                r\"$E^2\\mathrm{{d}}N/\\mathrm{{d}}E$\"\n                \" [{0}]\".format(u.Unit(f_unit).to_string(\"latex_inline\"))\n            )\n        else:\n            ax1.set_ylabel(\n                r\"$\\mathrm{{d}}N/\\mathrm{{d}}E$\"\n                \" [{0}]\".format(u.Unit(f_unit).to_string(\"latex_inline\"))\n            )\n    else:\n        ax1.set_ylabel(ylabel)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to compute and plot residuals in units of the uncertainty", "response": "def _plot_residuals_to_ax(\n    data_all, model_ML, ax, e_unit=u.eV, sed=True, errorbar_opts={}\n):\n    \"\"\"Function to compute and plot residuals in units of the uncertainty\"\"\"\n    if \"group\" not in data_all.keys():\n        data_all[\"group\"] = np.zeros(len(data_all))\n\n    groups = np.unique(data_all[\"group\"])\n\n    MLf_unit, MLsedf = sed_conversion(model_ML[0], model_ML[1].unit, sed)\n    MLene = model_ML[0].to(e_unit)\n    MLflux = (model_ML[1] * MLsedf).to(MLf_unit)\n\n    ax.axhline(0, color=\"k\", lw=1, ls=\"--\")\n\n    interp = False\n    if data_all[\"energy\"].size != MLene.size or not np.allclose(\n        data_all[\"energy\"].value, MLene.value\n    ):\n        interp = True\n        from scipy.interpolate import interp1d\n\n        modelfunc = interp1d(MLene.value, MLflux.value, bounds_error=False)\n\n    for g in groups:\n        groupidx = np.where(data_all[\"group\"] == g)\n\n        data = data_all[groupidx]\n\n        notul = ~data[\"ul\"]\n        df_unit, dsedf = sed_conversion(data[\"energy\"], data[\"flux\"].unit, sed)\n        ene = data[\"energy\"].to(e_unit)\n        xerr = u.Quantity((data[\"energy_error_lo\"], data[\"energy_error_hi\"]))\n        flux = (data[\"flux\"] * dsedf).to(df_unit)\n        dflux = (data[\"flux_error_lo\"] + data[\"flux_error_hi\"]) / 2.0\n        dflux = (dflux * dsedf).to(df_unit)[notul]\n\n        if interp:\n            difference = flux[notul] - modelfunc(ene[notul]) * flux.unit\n        else:\n            difference = flux[notul] - MLflux[groupidx][notul]\n\n        # wrap around color and marker cycles\n        color = color_cycle[int(g) % len(color_cycle)]\n        marker = marker_cycle[int(g) % len(marker_cycle)]\n\n        opts = dict(\n            zorder=100,\n            marker=marker,\n            ls=\"\",\n            elinewidth=2,\n            capsize=0,\n            mec=color,\n            mew=0.1,\n            ms=6,\n            color=color,\n        )\n        opts.update(errorbar_opts)\n\n        ax.errorbar(\n            ene[notul].value,\n            (difference / dflux).decompose().value,\n            yerr=(dflux / dflux).decompose().value,\n            xerr=xerr[:, notul].to(e_unit).value,\n            **opts\n        )\n\n    from matplotlib.ticker import MaxNLocator\n\n    ax.yaxis.set_major_locator(\n        MaxNLocator(5, integer=\"True\", prune=\"upper\", symmetric=True)\n    )\n\n    ax.set_ylabel(r\"$\\Delta\\sigma$\")\n    ax.set_xscale(\"log\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the spectral data for the current node.", "response": "def plot_data(\n    input_data,\n    xlabel=None,\n    ylabel=None,\n    sed=True,\n    figure=None,\n    e_unit=None,\n    ulim_opts={},\n    errorbar_opts={},\n):\n    \"\"\"\n    Plot spectral data.\n\n    Parameters\n    ----------\n    input_data : `emcee.EnsembleSampler`, `astropy.table.Table`, or `dict`\n        Spectral data to plot. Can be given as a data table, a dict generated\n        with `validate_data_table` or a `emcee.EnsembleSampler` with a data\n        property.\n    xlabel : str, optional\n        Label for the ``x`` axis of the plot.\n    ylabel : str, optional\n        Label for the ``y`` axis of the plot.\n    sed : bool, optional\n        Whether to plot SED or differential spectrum.\n    figure : `matplotlib.figure.Figure`, optional\n        `matplotlib` figure to plot on. If omitted a new one will be generated.\n    e_unit : `astropy.unit.Unit`, optional\n        Units for energy axis. Defaults to those of the data.\n    ulim_opts : dict\n        Options for upper-limit plotting. Available options are capsize (arrow\n        width) and height_fraction (arrow length in fraction of flux value).\n    errorbar_opts : dict\n        Addtional options to pass to `matplotlib.plt.errorbar` for plotting the\n        spectral flux points.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    try:\n        data = validate_data_table(input_data)\n    except TypeError as exc:\n        if hasattr(input_data, \"data\"):\n            data = input_data.data\n        elif isinstance(input_data, dict) and \"energy\" in input_data.keys():\n            data = input_data\n        else:\n            log.warning(\n                \"input_data format unknown, no plotting data! \"\n                \"Data loading exception: {}\".format(exc)\n            )\n            raise\n\n    if figure is None:\n        f = plt.figure()\n    else:\n        f = figure\n\n    if len(f.axes) > 0:\n        ax1 = f.axes[0]\n    else:\n        ax1 = f.add_subplot(111)\n\n    # try to get units from previous plot in figure\n    try:\n        old_e_unit = u.Unit(ax1.get_xlabel().split(\"[\")[-1].split(\"]\")[0])\n    except ValueError:\n        old_e_unit = u.Unit(\"\")\n\n    if e_unit is None and old_e_unit.physical_type == \"energy\":\n        e_unit = old_e_unit\n    elif e_unit is None:\n        e_unit = data[\"energy\"].unit\n\n    _plot_data_to_ax(\n        data,\n        ax1,\n        e_unit=e_unit,\n        sed=sed,\n        ylabel=ylabel,\n        ulim_opts=ulim_opts,\n        errorbar_opts=errorbar_opts,\n    )\n\n    if xlabel is not None:\n        ax1.set_xlabel(xlabel)\n    elif xlabel is None and ax1.get_xlabel() == \"\":\n        ax1.set_xlabel(\n            r\"$\\mathrm{Energy}$\"\n            + \" [{0}]\".format(e_unit.to_string(\"latex_inline\"))\n        )\n\n    ax1.autoscale()\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_distribution(samples, label, figure=None):\n\n    from scipy import stats\n    import matplotlib.pyplot as plt\n\n    quant = [16, 50, 84]\n    quantiles = dict(six.moves.zip(quant, np.percentile(samples, quant)))\n    std = np.std(samples)\n\n    if isinstance(samples[0], u.Quantity):\n        unit = samples[0].unit\n    else:\n        unit = \"\"\n\n    if isinstance(std, u.Quantity):\n        std = std.value\n\n    dist_props = \"{label} distribution properties:\\n \\\n    $-$ median: ${median}$ {unit}, std: ${std}$ {unit}\\n \\\n    $-$ Median with uncertainties based on \\n \\\n      the 16th and 84th percentiles ($\\sim$1$\\sigma$):\\n\\\n          {label} = {value_error} {unit}\".format(\n        label=label,\n        median=_latex_float(quantiles[50]),\n        std=_latex_float(std),\n        value_error=_latex_value_error(\n            quantiles[50],\n            quantiles[50] - quantiles[16],\n            quantiles[84] - quantiles[50],\n        ),\n        unit=unit,\n    )\n\n    if figure is None:\n        f = plt.figure()\n    else:\n        f = figure\n\n    ax = f.add_subplot(111)\n    f.subplots_adjust(bottom=0.40, top=0.93, left=0.06, right=0.95)\n\n    f.text(0.2, 0.27, dist_props, ha=\"left\", va=\"top\")\n\n    histnbins = min(max(25, int(len(samples) / 100.0)), 100)\n    xlabel = \"\" if label is None else label\n\n    if isinstance(samples, u.Quantity):\n        samples_nounit = samples.value\n    else:\n        samples_nounit = samples\n\n    n, x, _ = ax.hist(\n        samples_nounit,\n        histnbins,\n        histtype=\"stepfilled\",\n        color=color_cycle[0],\n        lw=0,\n        density=True,\n    )\n\n    kde = stats.kde.gaussian_kde(samples_nounit)\n    ax.plot(x, kde(x), color=\"k\", label=\"KDE\")\n\n    ax.axvline(\n        quantiles[50],\n        ls=\"--\",\n        color=\"k\",\n        alpha=0.5,\n        lw=2,\n        label=\"50% quantile\",\n    )\n    ax.axvspan(\n        quantiles[16],\n        quantiles[84],\n        color=(0.5,) * 3,\n        alpha=0.25,\n        label=\"68% CI\",\n        lw=0,\n    )\n    # ax.legend()\n    for l in ax.get_xticklabels():\n        l.set_rotation(45)\n    # [l.set_rotation(45) for l in ax.get_yticklabels()]\n    if unit != \"\":\n        xlabel += \" [{0}]\".format(unit)\n    ax.set_xlabel(xlabel)\n    ax.set_title(\"posterior distribution of {0}\".format(label))\n    ax.set_ylim(top=n.max() * 1.05)\n\n    return f", "response": "Plot a distribution and print statistics about it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_y(self, xq, **kwargs):\n        _, indexes = self.dataset.nn_x(xq, k = 1)\n        return self.dataset.get_y(indexes[0])", "response": "Provide an prediction of xq in the output space\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict_given_context(self, x, c, c_dims):\n        assert len(c) == len(c_dims)\n        _, index = self.dataset.nn_dims(x, c, range(self.dim_x), list(np.array(c_dims) + self.dim_x), k=1)\n        return self.dataset.get_y(index[0])", "response": "Provide a prediction of x with context c on dimensions c_dims in the output space being S - c_dims\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides an prediction of xq in the output space", "response": "def predict_y(self, xq, sigma=None, k = None):\n        \"\"\"Provide an prediction of xq in the output space\n\n        @param xq  an array of float of length dim_x\n        @param estimated_sigma  if False (default), sigma_sq=self.sigma_sq, else it is estimated from the neighbor distances in self._weights(.)\n        \"\"\"\n        #print(len(xq) , self.dataset.dim_x)\n        assert len(xq) == self.dataset.dim_x\n        sigma_sq = self.sigma_sq if sigma is None else sigma*sigma\n        k = k or self.k\n\n        dists, index = self.dataset.nn_x(xq, k = k)\n        #print(list(index))\n\n        w = self._weights(dists, index, sigma_sq)\n        idx = index[np.argmax(w)]\n        return self.dataset.get_y(idx)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the expected sensory effect of the motor command m.", "response": "def forward_prediction(self, m):\n        \"\"\" Compute the expected sensory effect of the motor command m. It is a shortcut for self.infer(self.conf.m_dims, self.conf.s_dims, m)\n        \"\"\"\n        return self.infer(self.conf.m_dims, self.conf.s_dims, m)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing a motor command to reach the sensory goal s_g.", "response": "def inverse_prediction(self, s_g):\n        \"\"\" Compute a motor command to reach the sensory goal s_g. It is a shortcut for self.infer(self.conf.s_dims, self.conf.m_dims, s_g)\n        \"\"\"\n        return self.infer(self.conf.s_dims, self.conf.m_dims, s_g)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_x(self, y, sigma=None, k=None, **kwargs):\n        assert len(y) == self.fmodel.dim_y, \"Wrong dimension for y. Expected %i, got %i\" % (self.fmodel.dim_y, len(y))\n        k = k or self.k\n        sigma = sigma or self.sigma\n        x_guess = self._guess_x(y, k = k)[0]\n        dists, index = self.fmodel.dataset.nn_x(x_guess, k = k)\n        w = self._weights(index, dists, sigma*sigma,  y)\n        return [np.sum([wi*self.fmodel.dataset.get_x(idx)\n                for wi, idx in zip(w, index)], axis = 0)]", "response": "Infer probable x from input y."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the centre of the Gaussian basis functions be spaced evenly throughout run time", "response": "def gen_centers(self):\n        \"\"\"Set the centre of the Gaussian basis \n        functions be spaced evenly throughout run time\"\"\"\n\n        c = np.linspace(0, 2*np.pi, self.bfs+1)\n        c = c[0:-1]\n        self.c = c"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_front_term(self, x, dmp_num):\n        \n        if isinstance(x, np.ndarray):\n            return np.ones(x.shape)\n        return 1", "response": "Generates the front term on the forcing term."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the goal for path imitation.", "response": "def gen_goal(self, y_des): \n        \"\"\"Generate the goal for path imitation. \n        For rhythmic DMPs the goal is the average of the \n        desired trajectory.\n    \n        y_des np.array: the desired trajectory to follow\n        \"\"\"\n\n        goal = np.zeros(self.dmps)\n        for n in range(self.dmps):\n            num_idx = ~np.isnan(y_des[n]) # ignore nan's when calculating goal\n            goal[n] = .5 * (y_des[n,num_idx].min() + \\\n                           y_des[n,num_idx].max()) \n\n        return goal"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_weights(self, f_target): \n\n        # calculate x and psi   \n        x_track = self.cs.rollout()\n        psi_track = self.gen_psi(x_track)\n\n        #efficiently calculate weights for BFs using weighted linear regression\n        for d in range(self.dmps):\n            for b in range(self.bfs): \n                self.w[d,b] = np.dot(psi_track[:,b], f_target[:,d]) / \\\n                              (np.sum(psi_track[:,b]) + 1e-10)", "response": "Generate a set of weights over the basis functions such \n        that the target forcing term trajectory is matched."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a link object that is the forward of the joint segment.", "response": "def forward(angles, lengths):\n    \"\"\" Link object as defined by the standard DH representation.\n\n    :param list angles: angles of each joint\n\n    :param list lengths: length of each segment\n\n    :returns: a tuple (x, y) of the end-effector position\n\n    .. warning:: angles and lengths should be the same size.\n    \"\"\"\n    x, y = joint_positions(angles, lengths)\n    return x[-1], y[-1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the x and y positions of each joint in the segment.", "response": "def joint_positions(angles, lengths, unit='rad'):\n    \"\"\" Link object as defined by the standard DH representation.\n\n    :param list angles: angles of each joint\n\n    :param list lengths: length of each segment\n\n    :returns: x positions of each joint, y positions of each joints, except the first one wich is fixed at (0, 0)\n\n    .. warning:: angles and lengths should be the same size.\n    \"\"\"\n    if len(angles) != len(lengths):\n        raise ValueError('angles and lengths must be the same size!')\n\n    if unit == 'rad':\n        a = np.array(angles)\n    elif unit == 'std':\n        a = np.pi * np.array(angles)\n    else:\n        raise NotImplementedError\n     \n    a = np.cumsum(a)\n    return np.cumsum(np.cos(a)*lengths), np.cumsum(np.sin(a)*lengths)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict_y(self, xq, sigma=None, k=None):\n        assert len(xq) == self.dataset.dim_x\n        sigma_sq = self.sigma_sq if sigma is None else sigma*sigma\n        k = k or self.k\n\n        dists, index = self.dataset.nn_x(xq, k=k)\n\n        w = self._weights(dists, index, sigma_sq)\n        Xq  = np.array(np.append([1.0], xq), ndmin = 2)\n        X   = np.array([self.dataset.get_x_padded(i) for i in index])\n        Y = np.array([self.dataset.get_y(i) for i in index])\n        \n        W   = np.diag(w)\n        WX  = np.dot(W, X)\n        WXT = WX.T\n\n        B   = np.dot(np.linalg.pinv(np.dot(WXT, WX)),WXT)\n\n        self.mat = np.dot(B, np.dot(W, Y))\n        Yq  = np.dot(Xq, self.mat)\n\n        return Yq.ravel()", "response": "Provide a prediction of xq in the output space\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict_dims(self, q, dims_x, dims_y, dims_out, sigma=None, k=None):\n        assert len(q) == len(dims_x) + len(dims_y)\n        sigma_sq = self.sigma_sq if sigma is None else sigma*sigma\n        k = k or self.k       \n        \n        dists, index = self.dataset.nn_dims(q[:len(dims_x)], q[len(dims_x):], dims_x, dims_y, k=k)\n\n        w = self._weights(dists, index, sigma_sq)\n        Xq  = np.array(np.append([1.0], q), ndmin = 2)\n        X   = np.array([np.append([1.0], self.dataset.get_dims(i, dims_x=dims_x, dims_y=dims_y)) for i in index])\n        Y = np.array([self.dataset.get_dims(i, dims=dims_out) for i in index])\n        \n        W   = np.diag(w)\n        WX  = np.dot(W, X)\n        WXT = WX.T\n\n        B   = np.dot(np.linalg.pinv(np.dot(WXT, WX)),WXT)\n\n        self.mat = np.dot(B, np.dot(W, Y))\n        \n        Yq  = np.dot(Xq, self.mat)\n\n        return Yq.ravel()", "response": "Provide a prediction of q in the output space."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pack(self, topic_dims, t):\n\n        data = []\n        for topic, dims in topic_dims:\n            for d in dims:\n                data.append(self.logs[topic][t, d])\n        return array(data).T", "response": "Packs selected logs into a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_given_context(self, c, c_dims):\n        '''\n        Sample randomly on dimensions not in context\n            c: context value on c_dims dimensions, not used\n            c_dims: w.r.t sensori space dimensions\n        '''\n        return self.sample()[list(set(range(self.ndims)) - set(c_dims))]", "response": "Sample randomly on dimensions not in context\n            c_dims"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_nodes(self):\n        return self.fold_up(lambda n, fl, fg: [n] + fl + fg, lambda leaf: [leaf])", "response": "Get the list of all nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of all leaves in the tree.", "response": "def get_leaves(self):\n        \"\"\"\n        Get the list of all leaves.\n        \n        \"\"\"\n        return self.fold_up(lambda n, fl, fg: fl + fg, lambda leaf: [leaf])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the depth of the tree.", "response": "def depth(self):\n        \"\"\"\n        Compute the depth of the tree (depth of a leaf=0).\n        \n        \"\"\"\n        return self.fold_up(lambda n, fl, fg: max(fl + 1, fg + 1), lambda leaf: 0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pt2leaf(self, x):\n        if self.leafnode:\n            return self\n        else:\n            if x[self.split_dim] < self.split_value:\n                return self.lower.pt2leaf(x)\n            else:\n                return self.greater.pt2leaf(x)", "response": "Return the leaf which domain contains x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsample a point in a random leaf.", "response": "def sample_random(self):\n        \"\"\"\n        Sample a point in a random leaf.\n        \n        \"\"\"\n        if self.sampling_mode['volume']:  \n            # Choose a leaf weighted by volume, randomly\n            if self.leafnode:\n                return self.sample_bounds()\n            else:\n                split_ratio = ((self.split_value - self.bounds_x[0,self.split_dim]) / \n                               (self.bounds_x[1,self.split_dim] - self.bounds_x[0,self.split_dim]))\n                if split_ratio > np.random.random():\n                    return self.lower.sample(sampling_mode=['random'])\n                else:\n                    return self.greater.sample(sampling_mode=['random'])\n        else: \n            # Choose a leaf randomly\n            return np.random.choice(self.get_leaves()).sample_bounds()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling a point in the leaf with the max progress.", "response": "def sample_greedy(self):\n        \"\"\"        \n        Sample a point in the leaf with the max progress.\n        \n        \"\"\"    \n        if self.leafnode:\n            return self.sample_bounds()\n        else:\n            lp = self.lower.max_leaf_progress\n            gp = self.greater.max_leaf_progress\n            maxp = max(lp, gp)\n        \n            if self.sampling_mode['multiscale']:                \n                tp = self.progress        \n                if tp > maxp:\n                    return self.sample_bounds()\n            if gp == maxp:\n                sampling_mode = self.sampling_mode\n                sampling_mode['mode'] = 'greedy'\n                return self.greater.sample(sampling_mode=sampling_mode)\n            else:\n                sampling_mode = self.sampling_mode\n                sampling_mode['mode'] = 'greedy'\n                return self.lower.sample(sampling_mode=sampling_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample a point in the leaf with the max progress with probability epsilon.", "response": "def sample_epsilon_greedy(self, epsilon=0.1):\n        \"\"\"\n        Sample a point in the leaf with the max progress with probability (1-eps) and a random leaf with probability (eps).\n        \n        Parameters\n        ----------\n        epsilon : float \n            \n        \"\"\"\n        if epsilon > np.random.random():\n            sampling_mode = self.sampling_mode\n            sampling_mode['mode'] = 'random'\n            return self.sample(sampling_mode=sampling_mode)\n        else:\n            sampling_mode = self.sampling_mode\n            sampling_mode['mode'] = 'greedy'\n            return self.sample(sampling_mode=sampling_mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsamples the softmax exploration of the current node.", "response": "def sample_softmax(self, temperature=1.):\n        \"\"\"\n        Sample leaves with probabilities progress*volume and a softmax exploration (with a temperature parameter).\n        \n        Parameters\n        ----------\n        temperature : float \n        \n        \"\"\"\n        if self.leafnode:\n            return self.sample_bounds()\n        else:\n            if self.sampling_mode['multiscale']:\n                nodes = self.get_nodes()\n            else:\n                nodes = self.get_leaves()\n                \n            if  self.sampling_mode['volume']:\n                progresses = np.array(map(lambda node:node.progress*node.volume, nodes)) #by volume\n            else:\n                progresses = np.array(map(lambda node:node.progress, nodes)) #by volume\n                \n            progress_max = max(progresses)\n            probas = np.exp(progresses / (progress_max*temperature))\n            probas = probas / np.sum(probas)\n            \n            if np.isnan(np.sum(probas)): # if progress_max = 0 or nan value in dataset, eps-greedy sample\n                return self.sample_epsilon_greedy()\n            else:\n                node = nodes[np.where(np.random.multinomial(1, probas) == 1)[0][0]]\n                return node.sample_bounds()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsampling a point in the leaf region with max competence progress.", "response": "def sample(self, sampling_mode=None):\n        \"\"\"\n        Sample a point in the leaf region with max competence progress (recursive).\n        \n        Parameters\n        ----------\n        sampling_mode : dict\n            How to sample a point in the tree: {'multiscale':bool, 'mode':string, 'param':float}\n            \n        \"\"\"\n        if sampling_mode is None:\n            sampling_mode = self.sampling_mode\n\n        if sampling_mode['mode'] == 'random':\n            return self.sample_random()\n                \n        elif sampling_mode['mode'] == 'greedy':\n            return self.sample_greedy()\n            \n        elif sampling_mode['mode'] == 'epsilon_greedy':\n            return self.sample_epsilon_greedy(sampling_mode['param'])\n            \n        elif sampling_mode['mode'] == 'softmax':\n            return self.sample_softmax(sampling_mode['param'])\n            \n        else:\n            raise NotImplementedError(sampling_mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all the progress items in the overall tree.", "response": "def progress_all(self):\n        \"\"\"\n        Competence progress of the overall tree.\n        \n        \"\"\"\n        return self.progress_idxs(range(np.shape(self.get_data_x())[0] - self.progress_win_size, \n                                        np.shape(self.get_data_x())[0]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef progress_idxs(self, idxs):\n        if self.progress_measure == 'abs_deriv_cov':\n            if len(idxs) <= 1:\n                return 0\n            else:\n                idxs = sorted(idxs)[- self.progress_win_size:]\n                return abs(np.cov(zip(range(len(idxs)), self.get_data_c()[idxs]), rowvar=0)[0, 1])\n            \n        elif self.progress_measure == 'abs_deriv':\n            if len(idxs) <= 1:\n                return 0\n            else:\n                idxs = sorted(idxs)[- self.progress_win_size:]\n                return np.abs(np.mean(np.diff(self.get_data_c()[idxs], axis=0)))\n\n        elif self.progress_measure == 'abs_deriv_smooth':\n            if len(idxs) <= 1:\n                return 0\n            else:\n                idxs = sorted(idxs)[- self.progress_win_size:]\n                v = self.get_data_c()[idxs]\n                n = len(v)\n                comp_beg = np.mean(v[:int(float(n)/2.)])\n                comp_end = np.mean(v[int(float(n)/2.):])\n                return np.abs(comp_end - comp_beg)\n        else:\n            raise NotImplementedError(self.progress_measure)", "response": "Returns the progress on points of given indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_max_progress(self):\n        self.update_progress()\n        if self.leafnode:\n            self.max_leaf_progress = self.progress\n        else:\n            self.max_leaf_progress = max(self.lower.max_leaf_progress, self.greater.max_leaf_progress)", "response": "Compute progress of tree and max progress of sub - trees."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an index to the tree.", "response": "def add(self, idx):\n        \"\"\"\n        Add an index to the tree (recursive).\n        \n        \"\"\"\n        if self.leafnode and self.children >= self.max_points_per_region and self.max_depth > 0:\n            self.split() \n        self.idxs.append(idx)\n        if self.leafnode:\n            leaf_add = self\n        else:\n            if self.get_data_x()[idx, self.split_dim] >= self.split_value:\n                leaf_add = self.greater.add(idx)            \n            else:\n                leaf_add = self.lower.add(idx)\n        self.update_max_progress()\n        self.children = self.children + 1\n        return leaf_add"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit the leaf node.", "response": "def split(self):\n        \"\"\"\n        Split the leaf node.\n        \n        \"\"\"\n        if self.split_mode == 'random':\n            # Split randomly between min and max of node's points on split dimension\n            split_dim_data = self.get_data_x()[self.idxs, self.split_dim] # data on split dim\n            split_min = min(split_dim_data)\n            split_max = max(split_dim_data)\n            split_value = split_min + np.random.rand() * (split_max - split_min)\n            \n        elif self.split_mode == 'median':\n            # Split on median (which fall on the middle of two points for even max_points_per_region) \n            # of node's points on split dimension\n            split_dim_data = self.get_data_x()[self.idxs, self.split_dim] # data on split dim\n            split_value = np.median(split_dim_data)\n            \n        elif self.split_mode == 'middle':\n            # Split on the middle of the region: might cause empty leaf\n            split_dim_data = self.get_data_x()[self.idxs, self.split_dim] # data on split dim\n            split_value = (self.bounds_x[0, self.split_dim] + self.bounds_x[1, self.split_dim]) / 2\n            \n        elif self.split_mode == 'best_interest_diff': \n            # See Baranes2012: Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots\n            # if strictly more than self.max_points_per_region points: chooses between self.max_points_per_region points random split values\n            # the one that maximizes card(lower)*card(greater)* progress difference between the two\n            # if equal or lower than self.max_points_per_region points: chooses between splits at the middle of each pair of consecutive points, \n            # the one that maximizes card(lower)*card(greater)* progress difference between the two\n            split_dim_data = self.get_data_x()[self.idxs, self.split_dim] # data on split dim\n            split_min = min(split_dim_data)\n            split_max = max(split_dim_data)\n                        \n            if len(self.idxs) > self.max_points_per_region:\n                m = self.max_points_per_region # Constant that might be tuned: number of random split values to choose between\n                rand_splits = split_min + np.random.rand(m) * (split_max - split_min)\n                splits_fitness = np.zeros(m)\n                for i in range(m):\n                    lower_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data <= rand_splits[i])[0]])\n                    greater_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data > rand_splits[i])[0]])\n                    splits_fitness[i] = len(lower_idx) * len(greater_idx) * abs(self.progress_idxs(lower_idx) - \n                                                                               self.progress_idxs(greater_idx))\n                split_value = rand_splits[np.argmax(splits_fitness)]\n                \n            else:\n                m = self.max_points_per_region - 1\n                splits = (np.sort(split_dim_data)[0:-1] + np.sort(split_dim_data)[1:]) / 2\n                splits_fitness = np.zeros(m)\n                for i in range(m):\n                    lower_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data <= splits[i])[0]])\n                    greater_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data > splits[i])[0]])\n                    splits_fitness[i] = len(lower_idx) * len(greater_idx) * abs(self.progress_idxs(lower_idx) - \n                                                                               self.progress_idxs(greater_idx))\n                split_value = splits[np.argmax(splits_fitness)]\n            \n        else:\n            raise NotImplementedError\n    \n        lower_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data <= split_value)[0]])\n        greater_idx = list(np.array(self.idxs)[np.nonzero(split_dim_data > split_value)[0]])\n\n        self.leafnode = False\n        self.split_value = split_value\n        \n        split_dim = np.mod(self.split_dim + 1, np.shape(self.get_data_x())[1])\n        \n        l_bounds_x = np.array(self.bounds_x)\n        l_bounds_x[1, self.split_dim] = split_value\n        \n        g_bounds_x = np.array(self.bounds_x)\n        g_bounds_x[0, self.split_dim] = split_value\n        \n        self.lower = Tree(self.get_data_x, \n                         l_bounds_x, \n                         self.get_data_c, \n                         self.max_points_per_region, \n                         self.max_depth - 1,\n                         self.split_mode, \n                         self.progress_win_size, \n                         self.progress_measure, \n                         self.sampling_mode, \n                         idxs = lower_idx, \n                         split_dim = split_dim)\n        \n        self.greater = Tree(self.get_data_x, \n                            g_bounds_x, \n                            self.get_data_c, \n                            self.max_points_per_region, \n                            self.max_depth - 1,\n                            self.split_mode, \n                            self.progress_win_size, \n                            self.progress_measure, \n                            self.sampling_mode, \n                            idxs = greater_idx, \n                            split_dim = split_dim)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the tree for nearest neighbors Parameters ---------- x : array_like, last dimension self.m An array of points to query. k : integer The number of nearest neighbors to return. eps : nonnegative float Return approximate nearest neighbors; the kth returned value is guaranteed to be no further than (1+eps) times the distance to the real kth nearest neighbor. p : float, 1<=p<=infinity Which Minkowski p-norm to use. 1 is the sum-of-absolute-values \"Manhattan\" distance 2 is the usual Euclidean distance infinity is the maximum-coordinate-difference distance distance_upper_bound : nonnegative float Return only neighbors within this distance. This is used to prune tree searches, so if you are doing a series of nearest-neighbor queries, it may help to supply the distance to the nearest neighbor of the most recent point. Returns ------- d : float or array of floats The distances to the nearest neighbors. If x has shape tuple+(self.m,), then d has shape tuple if k is one, or tuple+(k,) if k is larger than one. Missing neighbors (e.g. when k > n or distance_upper_bound is given) are indicated with infinite distances. If k is None, then d is an object array of shape tuple, containing lists of distances. In either case the hits are sorted by distance (nearest first). i : integer or array of integers The locations of the neighbors in self.data. i is the same shape as d.", "response": "def nn(self, x, k=1, eps=0, p=2, distance_upper_bound=np.inf):\n        \"\"\"\n        Query the tree for nearest neighbors\n\n        Parameters\n        ----------\n        x : array_like, last dimension self.m\n            An array of points to query.\n        k : integer\n            The number of nearest neighbors to return.\n        eps : nonnegative float\n            Return approximate nearest neighbors; the kth returned value\n            is guaranteed to be no further than (1+eps) times the\n            distance to the real kth nearest neighbor.\n        p : float, 1<=p<=infinity\n            Which Minkowski p-norm to use.\n            1 is the sum-of-absolute-values \"Manhattan\" distance\n            2 is the usual Euclidean distance\n            infinity is the maximum-coordinate-difference distance\n        distance_upper_bound : nonnegative float\n            Return only neighbors within this distance. This is used to prune\n            tree searches, so if you are doing a series of nearest-neighbor\n            queries, it may help to supply the distance to the nearest neighbor\n            of the most recent point.\n\n        Returns\n        -------\n        d : float or array of floats\n            The distances to the nearest neighbors.\n            If x has shape tuple+(self.m,), then d has shape tuple if\n            k is one, or tuple+(k,) if k is larger than one. Missing\n            neighbors (e.g. when k > n or distance_upper_bound is\n            given) are indicated with infinite distances.  If k is None,\n            then d is an object array of shape tuple, containing lists\n            of distances. In either case the hits are sorted by distance\n            (nearest first).\n        i : integer or array of integers\n            The locations of the neighbors in self.data. i is the same\n            shape as d.\n\n        \"\"\"\n        self.n, self.m = np.shape(self.get_data_x())\n        x = np.asarray(x)\n        if np.shape(x)[-1] != self.m:\n            raise ValueError(\"x must consist of vectors of length %d but has shape %s\" % (self.m, np.shape(x)))\n        if p < 1:\n            raise ValueError(\"Only p-norms with 1<=p<=infinity permitted\")\n        retshape = np.shape(x)[:-1]\n        if retshape != ():\n            if k is None:\n                dd = np.empty(retshape,dtype=np.object)\n                ii = np.empty(retshape,dtype=np.object)\n            elif k > 1:\n                dd = np.empty(retshape+(k,),dtype=np.float)\n                dd.fill(np.inf)\n                ii = np.empty(retshape+(k,),dtype=np.int)\n                ii.fill(self.n)\n            elif k == 1:\n                dd = np.empty(retshape,dtype=np.float)\n                dd.fill(np.inf)\n                ii = np.empty(retshape,dtype=np.int)\n                ii.fill(self.n)\n            else:\n                raise ValueError(\"Requested %s nearest neighbors; acceptable numbers are integers greater than or equal to one, or None\")\n            for c in np.ndindex(retshape):\n                hits = self.__query(x[c], k=k, eps=eps, p=p, distance_upper_bound=distance_upper_bound)\n                if k is None:\n                    dd[c] = [d for (d,i) in hits]\n                    ii[c] = [i for (d,i) in hits]\n                elif k > 1:\n                    for j in range(len(hits)):\n                        dd[c+(j,)], ii[c+(j,)] = hits[j]\n                elif k == 1:\n                    if len(hits) > 0:\n                        dd[c], ii[c] = hits[0]\n                    else:\n                        dd[c] = np.inf\n                        ii[c] = self.n\n            return dd, ii\n        else:\n            hits = self.__query(x, k=k, eps=eps, p=p, distance_upper_bound=distance_upper_bound)\n            if k is None:\n                return [d for (d,i) in hits], [i for (d,i) in hits]\n            elif k == 1:\n                if len(hits) > 0:\n                    return hits[0]\n                else:\n                    return np.inf, self.n\n            elif k > 1:\n                dd = np.empty(k,dtype=np.float)\n                dd.fill(np.inf)\n                ii = np.empty(k,dtype=np.int)\n                ii.fill(self.n)\n                for j in range(len(hits)):\n                    dd[j], ii[j] = hits[j]\n                return dd, ii\n            else:\n                raise ValueError(\"Requested %s nearest neighbors; acceptable numbers are integers greater than or equal to one, or None\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying recursively the function f_inter from leaves to root", "response": "def fold_up(self, f_inter, f_leaf):\n        \"\"\"\n        Apply recursively the function f_inter from leaves to root, begining with function f_leaf on leaves.\n        \n        \"\"\"\n        return f_leaf(self) if self.leafnode else f_inter(self,\n                                                          self.lower.fold_up(f_inter, f_leaf), \n                                                          self.greater.fold_up(f_inter, f_leaf))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, ax, scatter=True, grid=True, progress_colors=True, progress_max=1., depth=10, plot_dims=[0,1]):\n        \n        if scatter and self.get_data_x() is not None:\n            self.plot_scatter(ax, plot_dims)\n            \n        if grid:\n            self.plot_grid(ax, progress_colors, progress_max, depth, plot_dims)", "response": "Plots a projection on 2D of the Tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsample the region with max progress among regions that have the same context c_dims.", "response": "def sample_given_context(self, c, c_dims):\n        '''\n        Sample the region with max progress among regions that have the same context\n            c: context value on c_dims dimensions\n            c_dims: w.r.t sensory space dimensions\n        '''\n        index = self.discrete_progress.sample_given_context(c, c_dims, self.space)\n        return self.space.rand_value(index).flatten()[list(set(range(len(self.space.cardinalities))) - set(c_dims))]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f(self, m):\n        ''' Function to minimize '''\n        M = reshape(m,self.input_dim)\n        n = M.shape[0]\n        if M.shape[1] != self.input_dim:\n            return 'Wrong input dimension'\n        else:\n            S = self.environment.update(M)\n            m = M[0]\n            s = S[0]\n            # Store the points (m,s) explored during the optimisation\n            self.pointsList.append((m,s))\n            return self.dist_array(S)", "response": "Function to minimize '''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef infer_x(self, y):\n        assert len(y) == self.fmodel.dim_y, \"Wrong dimension for y. Expected %i, got %i\" % (self.fmodel.dim_y, len(y))\n        self.goal = np.array(y)", "response": "Infer probable x from input y\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a random vector.", "response": "def _random_x(self):\n        \"\"\"If the database is empty, generate a random vector.\"\"\"\n        return (tuple(random.random() for _ in range(self.fmodel.dim_x)),)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchoosing the relevant neighborhood to feed the inverse model based on the relevant neighborhood of x y_desired.", "response": "def _guess_x(self, y_desired, **kwargs):\n        \"\"\"Choose the relevant neighborhood to feed the inverse model, based\n        on the minimum spread of the corresponding neighborhood in S.\n        for each (x, y) with y neighbor of y_desired,\n            1. find the neighborhood of x, (xi, yi)_k.\n            2. compute the standart deviation of the error between yi and y_desired.\n            3. select the neighborhood of minimum standart deviation\n\n        TODO : Implement another method taking the spread in M too.\n        \"\"\"\n        k = kwargs.get('k', self.k)\n        _, indexes = self.fmodel.dataset.nn_y(y_desired, k = k)\n        min_std, min_xi = float('inf'), None\n        for i in indexes:\n            xi = self.fmodel.dataset.get_x(i)\n            _, indexes_xi = self.fmodel.dataset.nn_x(xi, k = k)\n            std_xi = np.std([dist(self.fmodel.dataset.get_y(j), y_desired) for j in indexes_xi])\n            if std_xi < min_std:\n                min_std, min_xi = std_xi, xi\n            #print(std_xi, tuple(yi))\n        return [min_xi]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _guess_x_simple(self, y_desired, y_dims=None, **kwargs):\n        _, indexes = self.fmodel.dataset.nn_y(y_desired, dims=y_dims, k = 10)\n        return [self.fmodel.get_x(i) for i in indexes]", "response": "Provide an initial guesses for a probable x from y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproviding an initial guesses for a probable x from y", "response": "def _guess_x_kmeans(self, y_desired, **kwargs):\n        \"\"\"Provide an initial guesses for a probable x from y\"\"\"\n        k = kwargs.get('k', self.k)\n        _, indexes = self.fmodel.dataset.nn_y(y_desired, k=k)\n        X = np.array([self.fmodel.get_x(i) for i in indexes])\n        if np.sum(X) == 0.:\n            centroids = [self.fmodel.get_x(indexes[0])]\n        else:\n            try:\n                centroids, _ = kmeans2(X, 2)\n            except np.linalg.linalg.LinAlgError:\n                centroids = [self.fmodel.get_x(indexes[0])]\n        return centroids"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config(self):\n        return \", \".join('%s:%s' % (key, value) for key, value in self.conf.items())", "response": "Return a string with the configuration"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_x(self, y_desired):\n        InverseModel.infer_x(y_desired)\n        if self.fmodel.size() == 0:\n            return self._random_x()\n        else:\n            idx = random.randint(0, self.fmodel.size()-1)\n            return self.dataset.get_x(idx)", "response": "Infer probable x from input y_desired"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the schur complement of the matrix block mat [ row col )", "response": "def schur_complement(mat, row, col):\n    \"\"\" compute the schur complement of the matrix block mat[row:,col:] of the matrix mat \"\"\"\n    a = mat[:row, :col]\n    b = mat[:row, col:]\n    c = mat[row:, :col]\n    d = mat[row:, col:]\n    return a - b.dot(d.I).dot(c)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef conditional(mean, covar, dims_in, dims_out, covariance_type='full'):\n    in_in = covar[ix_(dims_in, dims_in)]\n    in_out = covar[ix_(dims_in, dims_out)]\n    out_in = covar[ix_(dims_out, dims_in)]\n    out_out = covar[ix_(dims_out, dims_out)]\n    in_in_inv = inv(in_in)\n    out_in_dot_in_in_inv = out_in.dot(in_in_inv)\n\n    cond_covar = out_out - out_in_dot_in_in_inv.dot(in_out)\n    cond_mean = lambda x: mean[dims_out] + out_in_dot_in_in_inv.dot(x - mean[dims_in])\n    return lambda x: [cond_mean(x), cond_covar]", "response": "Return a function f that returns the mean and covariance of the conditional distribution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inference(self, in_dims, out_dims, value=None):\n\n        if self.covariance_type != 'diag' and self.covariance_type != 'full':\n            raise ValueError(\"covariance type other than 'full' and 'diag' not allowed\")\n        in_dims = array(in_dims)\n        out_dims = array(out_dims)\n        value = array(value)\n\n        means = zeros((self.n_components, len(out_dims)))\n        covars = zeros((self.n_components, len(out_dims), len(out_dims)))\n        weights = zeros((self.n_components,))\n        if in_dims.size:\n            for k, (weight_k, mean_k, covar_k) in enumerate(self):\n\n                sig_in = covar_k[ix_(in_dims, in_dims)]\n                inin_inv = matrix(sig_in).I\n                out_in = covar_k[ix_(out_dims, in_dims)]\n                mu_in = mean_k[in_dims].reshape(-1, 1)\n                means[k, :] = (mean_k[out_dims] +\n                               (out_in *\n                                inin_inv *\n                                (value.reshape(-1, 1) - mu_in)).T)\n                if self.covariance_type == 'full':\n                    covars[k, :, :] = (covar_k[ix_(out_dims, out_dims)] -\n                                       out_in *\n                                       inin_inv *\n                                       covar_k[ix_(in_dims, out_dims)])\n                elif self.covariance_type == 'diag':\n                    covars[k, :] = covar_k[out_dims]\n\n                weights[k] = weight_k * Gaussian(mu_in.reshape(-1,),\n                                                 sig_in).normal(value.reshape(-1,))\n            weights /= sum(weights)\n        else:\n            means = self.means_[:, out_dims]\n            if self.covariance_type == 'full':\n                covars = self.covariances_[ix_(range(self.n_components), out_dims, out_dims)]\n            if self.covariance_type == 'diag':\n                covars = self.covariances_[ix_(range(self.n_components), out_dims)]\n            weights = self.weights_\n\n        res = GMM(n_components=self.n_components,\n                  covariance_type=self.covariance_type)\n        res.weights_ = weights\n        res.means_ = means\n        res.covariances_ = covars\n        return res", "response": "Perform Bayesian inference on the current GMM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the experiment. :param int n_iter: Number of run iterations, by default will run until the last evaluation step. :param bool bg: whether to run in background (using a Thread)", "response": "def run(self, n_iter=-1, bg=False):\n        \"\"\" Run the experiment.\n\n            :param int n_iter: Number of run iterations, by default will run until the last evaluation step.\n            :param bool bg: whether to run in background (using a Thread)\n\n        \"\"\"\n        if n_iter == -1:\n            if not self.eval_at:\n                raise ValueError('Set n_iter or define evaluate_at.')\n\n            n_iter = self.eval_at[-1] + 1\n\n        self._running.set()\n\n        if bg:\n            self._t = threading.Thread(target=lambda: self._run(n_iter))\n            self._t.start()\n        else:\n            self._run(n_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_at(self, eval_at, testcases, mode=None):\n        self.eval_at = eval_at\n        self.log.eval_at = eval_at\n\n        if mode is None:\n            if self.context_mode is None or (self.context_mode.has_key('choose_m') and self.context_mode['choose_m']):\n                mode = 'inverse'\n            else:\n                mode = self.context_mode[\"mode\"]\n                \n\n        self.evaluation = Evaluation(self.ag, self.env, testcases, mode=mode)\n        for test in testcases:\n            self.log.add('testcases', test)", "response": "Sets the evaluation interation indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an experiment object from the given ExperimentSettings.", "response": "def from_settings(cls, settings):\n        \"\"\" Creates a :class:`~explauto.experiment.experiment.Experiment` object thanks to the given settings. \"\"\"\n        env_cls, env_configs, _ = environments[settings.environment]\n        config = env_configs[settings.environment_config]\n\n        if settings.context_mode is None:\n            env = env_cls(**config)\n        else:\n            env = ContextEnvironment(env_cls, config, settings.context_mode)\n\n        im_cls, im_configs = interest_models[settings.interest_model]\n        sm_cls, sm_configs = sensorimotor_models[settings.sensorimotor_model]\n\n        babbling = settings.babbling_mode\n        if babbling not in ['goal', 'motor']:\n            raise ValueError(\"babbling argument must be in ['goal', 'motor']\")\n\n        expl_dims = env.conf.m_dims if (babbling == 'motor') else env.conf.s_dims\n        inf_dims = env.conf.s_dims if (babbling == 'motor') else env.conf.m_dims\n\n        if settings.context_mode is None:\n            agent = Agent.from_classes(im_cls, im_configs[settings.interest_model_config], expl_dims,\n                          sm_cls, sm_configs[settings.sensorimotor_model_config], inf_dims,\n                          env.conf.m_mins, env.conf.m_maxs, env.conf.s_mins, env.conf.s_maxs)\n        else:\n            agent = Agent.from_classes(im_cls, im_configs[settings.interest_model_config], expl_dims,\n                          sm_cls, sm_configs[settings.sensorimotor_model_config], inf_dims,\n                          env.conf.m_mins, env.conf.m_maxs, env.conf.s_mins, env.conf.s_maxs, context_mode=settings.context_mode)\n\n        return cls(env, agent, settings.context_mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate x for open loop movements.", "response": "def rollout(self, **kwargs):\n        \"\"\"Generate x for open loop movements.\n        \"\"\"\n        if kwargs.has_key('tau'):\n            timesteps = int(self.timesteps / kwargs['tau'])\n        else: \n            timesteps = self.timesteps\n        self.x_track = np.zeros(timesteps)\n        \n        self.reset_state()\n        for t in range(timesteps):\n            self.x_track[t] = self.x \n            self.step(**kwargs)\n\n        return self.x_track"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef step_discrete(self, tau=1.0, error_coupling=1.0):\n        self.x += (-self.ax * self.x * error_coupling) * tau * self.dt\n        return self.x", "response": "Generate a single step of discrete iteration of the system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef step_rhythmic(self, tau=1.0, error_coupling=1.0):\n        self.x += (1 * error_coupling * tau) * self.dt\n        return self.x", "response": "Generate a single step of x for rhythmic \n        closed loop movements."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncodes from Steve Nguyen", "response": "def discrete_random_draw(data, nb=1):\n    ''' Code from Steve Nguyen'''\n    data = np.array(data)\n    if not data.any():\n        data = np.ones_like(data)\n    data = data/data.sum()\n    xk = np.arange(len(data))\n    custm = stats.rv_discrete(name='custm', values=(xk, data))\n    return custm.rvs(size=nb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rk4(x, h, y, f):\n    k1 = h * f(x, y)\n    k2 = h * f(x + 0.5*h, y + 0.5*k1)\n    k3 = h * f(x + 0.5*h, y + 0.5*k2)\n    k4 = h * f(x + h, y + k3)\n    return x + h, y + (k1 + 2*(k2 + k3) + k4)/6.0", "response": "Code from http://doswa.com/2009/04/21/improved-rk4-implementation.html\n        :param int x: curent time\n        :param float h: time step (dt)\n        :param list y: [position, velocity]\n        :params function f: velocity, accelation[x+h] = f(x, state)\n        :return int t, list state: x+h, next state"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_dataset(cls, dataset, constraints = (), **kwargs):\n        fm = LWLRForwardModel(dataset.dim_x, dataset.dim_y, **kwargs)\n        fm.dataset = dataset\n        im = cls.from_forward(fm, constraints = constraints, **kwargs)\n        return im", "response": "Construct an optimized inverse model from an existing dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setuplimits(self, constraints):\n        self.constraints = tuple(constraints)\n        assert len(self.constraints) == self.fmodel.dim_x\n        self.goal = None", "response": "Setup the limits for every initialiser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the error function.", "response": "def _error(self, x):\n        \"\"\"Error function.\n        Once self.y_desired has been defined, compute the error\n        of input x using the forward model.\n        \"\"\"\n        y_pred = self.fmodel.predict_y(x)\n        err_v  = y_pred - self.goal\n        error = sum(e*e for e in err_v)\n        return error"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _error_dm(self, m, dm, s):\n        pred = self.fmodel.predict_given_context(np.hstack((m, dm)), s, range(len(s)))\n        err_v  = pred - self.goal\n        error = sum(e*e for e in err_v)\n        return error", "response": "Compute the error function."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates vectors from the Gaussian.", "response": "def generate(self, number=None):\n        \"\"\"Generates vectors from the Gaussian.\n\n        @param number: optional, if given, generates more than one vector.\n\n        @returns: generated vector(s), either as a one dimensional array\n            (shape (d,)) if number is not set, or as a two dimensional array\n            (shape (n, d)) if n is given as number parameter.\n        \"\"\"\n        if number is None:\n            return numpy.random.multivariate_normal(self.mu, self.sigma)\n        else:\n            return numpy.random.multivariate_normal(self.mu, self.sigma, number)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging - normal distribution of the log density of probability of x or the one dimensional array of all log probabilities if many vectors are given.", "response": "def log_normal(self, x):\n        \"\"\"\n        Returns the log density of probability of x or the one dimensional\n        array of all log probabilities if many vectors are given.\n\n        @param x : may be of (n,) shape\n        \"\"\"\n        d = self.mu.shape[0]\n        xc = x - self.mu\n        if len(x.shape) == 1:\n            exp_term = numpy.sum(numpy.multiply(xc, numpy.dot(self.inv, xc)))\n        else:\n            exp_term = numpy.sum(numpy.multiply(xc, numpy.dot(xc, self.inv)), axis=1)\n\n        return -.5 * (d * numpy.log(2 * numpy.pi) + numpy.log(self.det) + exp_term)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns mean and variance of the conditional probability probability at a given vector.", "response": "def cond_gaussian(self, dims, v):\n        \"\"\"\n        Returns mean  and variance of the conditional probability\n        defined by a set of dimension and at a given vector.\n\n        @param dims : set of dimension to which respect conditional\n            probability is taken\n        @param v : vector defining the position where the conditional\n            probability is taken. v shape is defined by the size\n            of the set of dims.\n        \"\"\"\n        (d, c, b, a) = numpy.split_matrix(self.sigma, dims)\n        (mu2, mu1) = numpy.split_vector(self.mu, dims)\n        d_inv = numpy.linalg.inv(d)\n        mu = mu1 + numpy.dot(numpy.dot(b, d_inv), v - mu2)\n        sigma = a - numpy.dot(b, numpy.dot(d_inv, c))\n        return Gaussian(mu, sigma)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_entropy(self):\n        dim = self.mu.shape[0]\n        entropy = 0.5 * (dim * (numpy.log(2. * numpy.pi) + 1.) + numpy.log(self.det))\n        return entropy", "response": "Computes the analyticaly entropy of the Gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, m_ag, reset=True, log=True):\n\n        if reset:\n            self.reset()\n        if len(array(m_ag).shape) == 1:\n            s = self.one_update(m_ag, log)\n        else:\n            s = []\n            for m in m_ag:\n                s.append(self.one_update(m, log))\n            s = array(s)\n        return s", "response": "Compute sensorimotor values from motor orders."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self.data     = []\n        self.size     = 0\n        self.kdtree   = None  # KDTree\n        self.nn_ready = False", "response": "Reset the dataset to zero elements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind k nearest neighbors of x in the observed input data.", "response": "def nn(self, x, k = 1, radius = np.inf, eps = 0.0, p = 2):\n        \"\"\"Find the k nearest neighbors of x in the observed input data\n        :arg x:      center\n        :arg k:      the number of nearest neighbors to return (default: 1)\n        :arg eps:    approximate nearest neighbors.\n                     the k-th returned value is guaranteed to be no further than\n                     (1 + eps) times the distance to the real k-th nearest neighbor.\n        :arg p:      Which Minkowski p-norm to use. (default: 2, euclidean)\n        :arg radius: the maximum radius (default: +inf)\n        :return:     distance and indexes of found nearest neighbors.\n        \"\"\"\n        assert len(x) == self.dim, 'dimension of input {} does not match expected dimension {}.'.format(len(x), self.dim)\n        k_x = min(k, self.size)\n        # Because linear models requires x vector to be extended to [1.0]+x\n        # to accomodate a constant, we store them that way.\n        return self._nn(np.array(x), k_x, radius = radius, eps = eps, p = p)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_tree(self):\n        if not self.nn_ready:\n            self.kdtree   = scipy.spatial.cKDTree(self.data)\n            self.nn_ready = True", "response": "Build the KDTree for the observed data\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_data(cls, data):\n        if len(data) == 0:\n            raise ValueError(\"data array is empty.\")\n        dim_x, dim_y = len(data[0][0]), len(data[0][1])\n        dataset = cls(dim_x, dim_y)\n        for x, y in data:\n            assert len(x) == dim_x and len(y) == dim_y\n            dataset.add_xy(x, y)\n        return dataset", "response": "Create a dataset from an array of data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_xy(cls, x_array, y_array):\n        if len(x_array) == 0:\n            raise ValueError(\"data array is empty.\")\n        dim_x, dim_y = len(x_array[0]), len(y_array[0])\n        dataset = cls(dim_x, dim_y)\n        for x, y in zip(x_array, y_array):\n            assert len(x) == dim_x and len(y) == dim_y\n            dataset.add_xy(x, y)\n        return dataset", "response": "Create a dataset from two arrays of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nn_x(self, x, k=1, radius=np.inf, eps=0.0, p=2):\n        assert len(x) == self.dim_x\n        k_x = min(k, self.size)\n        # Because linear models requires x vector to be extended to [1.0]+x\n        # to accomodate a constant, we store them that way.\n        return self._nn(DATA_X, x, k=k_x, radius=radius, eps=eps, p=p)", "response": "Find the k nearest neighbors of x in the observed input data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nn_y(self, y, k=1, radius=np.inf, eps=0.0, p=2):\n        assert len(y) == self.dim_y\n        k_y = min(k, self.size)\n        return self._nn(DATA_Y, y, k=k_y, radius=radius, eps=eps, p=p)", "response": "Find the k nearest neighbors of y in the observed output data\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nn_dims(self, x, y, dims_x, dims_y, k=1, radius=np.inf, eps=0.0, p=2):\n        assert len(x) == len(dims_x)\n        assert len(y) == len(dims_y)\n        if len(dims_x) == 0:\n            kdtree = scipy.spatial.cKDTree([np.array(data_y)[np.array(dims_y) - self.dim_x] for data_y in self.data[DATA_Y]])\n        elif len(dims_y) == 0:\n            kdtree = scipy.spatial.cKDTree([np.array(data_x)[dims_x] for data_x in self.data[DATA_X]])\n        else:\n            kdtree = scipy.spatial.cKDTree([np.hstack((np.array(data_x)[dims_x], np.array(data_y)[np.array(dims_y) - self.dim_x])) for data_x,data_y in zip(self.data[DATA_X], self.data[DATA_Y])])\n        dists, idxes =  kdtree.query(np.hstack((x, y)), \n                               k = k, \n                               distance_upper_bound = radius,\n                               eps = eps, \n                               p = p)\n        if k == 1:\n            dists, idxes = np.array([dists]), [idxes]\n        return dists, idxes", "response": "Find k nearest neighbors of a subset of dims of x and y in the observed output data\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _nn(self, side, v, k = 1, radius = np.inf, eps = 0.0, p = 2):\n        self._build_tree(side)\n        dists, idxes = self.kdtree[side].query(v, k = k, distance_upper_bound = radius,\n                                               eps = eps, p = p)\n        if k == 1:\n            dists, idxes = np.array([dists]), [idxes]\n        return dists, idxes", "response": "Compute the k nearest neighbors of v in the observed data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the KDTree for the observed data.", "response": "def _build_tree(self, side):\n        \"\"\"Build the KDTree for the observed data\n        :arg side  if equal to DATA_X, build input data tree.\n                     if equal to DATA_Y, build output data tree.\n        \"\"\"\n        if not self.nn_ready[side]:\n            self.kdtree[side]   = scipy.spatial.cKDTree(self.data[side], compact_nodes=False, balanced_tree=False) # Those options are required with scipy >= 0.16\n            self.nn_ready[side] = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nn_y(self, y, dims=None, k = 1, radius=np.inf, eps=0.0, p=2):\n        if dims is None:\n            assert len(y) == self.dim_y\n            k_y = min(k, self.__len__())\n            return self._nn(DATA_Y, y, k=k_y, radius=radius, eps=eps, p=p)\n        else:\n            return self.nn_y_sub(y, dims, k, radius, eps, p)", "response": "Find the k nearest neighbors of y in the observed output data\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding k nearest neighbors of a subset of dims of x and y in the observed output data.", "response": "def nn_dims(self, x, y, dims_x, dims_y, k=1, radius=np.inf, eps=0.0, p=2):\n        \"\"\"Find the k nearest neighbors of a subset of dims of x and y in the observed output data\n        @see Databag.nn() for argument description\n        @return  distance and indexes of found nearest neighbors.\n        \"\"\"\n        if self.size > 0:\n            dists, idxes = Dataset.nn_dims(self, x, y, dims_x, dims_y, k, radius, eps, p)\n        else:\n            return self.buffer.nn_dims(x, y, dims_x, dims_y, k, radius, eps, p)\n        if self.buffer.size > 0:\n            buffer_dists, buffer_idxes = self.buffer.nn_dims(x, y, dims_x, dims_y, k, radius, eps, p)\n            buffer_idxes = [i + self.size for i in buffer_idxes]\n            ziped = zip(dists, idxes)\n            buffer_ziped = zip(buffer_dists, buffer_idxes)\n            sorted_dists_idxes = sorted(ziped + buffer_ziped, key=lambda di:di[0])\n            knns = sorted_dists_idxes[:k]\n            return [knn[0] for knn in knns], [knn[1] for knn in knns] \n        else:\n            return dists, idxes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the k nearest neighbors of v in the observed data.", "response": "def _nn(self, side, v, k=1, radius=np.inf, eps=0.0, p=2):\n        \"\"\"Compute the k nearest neighbors of v in the observed data,\n        :arg side  if equal to DATA_X, search among input data.\n                     if equal to DATA_Y, search among output data.\n        @return  distance and indexes of found nearest neighbors.\n        \"\"\"\n        if self.size > 0:\n            dists, idxes = Dataset._nn(self, side, v, k, radius, eps, p)\n        else:\n            return self.buffer._nn(side, v, k, radius, eps, p)\n        if self.buffer.size > 0:\n            buffer_dists, buffer_idxes = self.buffer._nn(side, v, k, radius, eps, p)\n            buffer_idxes = [i + self.size for i in buffer_idxes]\n            ziped = zip(dists, idxes)\n            buffer_ziped = zip(buffer_dists, buffer_idxes)\n            sorted_dists_idxes = sorted(ziped + buffer_ziped, key=lambda di:di[0])\n            knns = sorted_dists_idxes[:k]\n            return [knn[0] for knn in knns], [knn[1] for knn in knns] \n        else:\n            return dists, idxes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize an Agent object from the given classes.", "response": "def from_classes(cls,\n                 im_model_cls, im_model_config, expl_dims,\n                 sm_model_cls, sm_model_config, inf_dims,\n                 m_mins, m_maxs, s_mins, s_maxs, n_bootstrap=0, context_mode=None):\n        \"\"\"Initialize agent class\n\n        :param class im_model_cls: a subclass of InterestedModel, as those registered in the interest_model package\n\n        :param dict im_model_config: a configuration dict as those registered in the interest_model package\n\n        :param list expl_dims: the sensorimotor dimensions where exploration is driven in the interest model\n\n        :param list inf_dims: the output sensorimotor dimensions of the sensorimotor model (input being expl_dims)\n\n        :param class sm_model_cls: a subclass of SensorimotorModel, as those registered in the sensorimotor_model package\n\n        :param dict sensorimotor_model_config: a configuration dict as those registered in the sensorimotor_model package\n\n        :param list m_mins, m_maxs, s_mins, s_max: lower and upper bounds of motor and sensory values on each dimension\n\n        \"\"\"\n\n        conf = make_configuration(m_mins, m_maxs, s_mins, s_maxs)\n\n        sm_model = sm_model_cls(conf, **sm_model_config)\n        im_model = im_model_cls(conf, expl_dims,\n                                           **im_model_config)\n\n        return cls(conf, sm_model, im_model, n_bootstrap, context_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choose(self, context_ms=None):\n        try:\n            if self.context_mode is None:\n                x = self.interest_model.sample()\n            else:\n                if self.context_mode[\"mode\"] == 'mdmsds':\n                    if self.expl_dims == self.conf.s_dims:\n                        x = np.hstack((context_ms[self.conf.m_ndims//2:], self.interest_model.sample_given_context(context_ms[self.conf.m_ndims//2:], range(self.conf.s_ndims//2))))\n                    else:\n                        if self.context_mode['choose_m']:\n                            x = self.interest_model.sample()\n                        else:\n                            x = np.hstack((context_ms[:self.conf.m_ndims//2], self.interest_model.sample_given_context(context_ms[:self.conf.m_ndims//2], range(self.conf.m_ndims//2))))                \n                elif self.context_mode[\"mode\"] == 'mcs':\n                    x = np.hstack((context_ms, self.interest_model.sample_given_context(context_ms, range(self.context_mode[\"context_n_dims\"]))))\n        except ExplautoBootstrapError:\n            logger.warning('Interest model not bootstrapped yet')\n            x = rand_bounds(self.conf.bounds[:, self.expl_dims]).flatten()\n            if self.context_mode is not None:\n                x = x[list(set(self.expl_dims) - set(self.context_mode['context_dims']))]\n        return x", "response": "Returns a point chosen by the interest model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer(self, expl_dims, inf_dims, x):\n        try:\n            if self.n_bootstrap > 0:\n                self.n_bootstrap -= 1\n                raise ExplautoBootstrapError\n            y = self.sensorimotor_model.infer(expl_dims,\n                                              inf_dims,\n                                              x.flatten())\n        except ExplautoBootstrapError:\n            logger.warning('Sensorimotor model not bootstrapped yet')\n            y = rand_bounds(self.conf.bounds[:, inf_dims]).flatten()\n        return y", "response": "Infer the value on the specified expl_dims from the input value x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_ms(self, x, y):\n        ms = np.zeros(self.conf.ndims)\n        ms[self.expl_dims] = x\n        ms[self.inf_dims] = y\n        return ms[self.conf.m_dims], ms[self.conf.s_dims]", "response": "Extract the motor and sensory parts from a point in the exploration\n        space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef motor_primitive(self, m):\n        return bounds_min_max(m, self.conf.m_mins, self.conf.m_maxs)", "response": "Prepare the movement from a command m."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract features from a sensory effect s.", "response": "def sensory_primitive(self, s):\n        \"\"\" Extract features from a sensory effect s. To be overridded in order to process more complex feature extraction (tutorial to come). This version simply bounds the sensory effect.\n        \"\"\"\n        return bounds_min_max(s, self.conf.s_mins, self.conf.s_maxs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a new version of the class.", "response": "def produce(self, context_ms=None):\n        \"\"\" Exploration (see the `Explauto introduction <about.html>`__ for more detail):\n\n        * Choose a value x on expl_dims according to the interest model\n        * Infer a value y on inf_dims from x using the :meth:`~explauto.agent.agent.Agent.infer` method\n        * Extract the motor m and sensory s parts of x, y\n        * generate a movement from m\n\n        :returns: the generated movement\n\n        .. note:: This correspond to motor babbling if expl_dims=self.conf.m_dims and inf_dims=self.conf.s_dims and to  goal babbling if expl_dims=self.conf.s_dims and inf_dims=self.conf.m_dims.\n        \"\"\"\n        if context_ms is None:\n            self.x = self.choose()\n            self.y = self.infer(self.expl_dims, self.inf_dims, self.x)\n        else:\n            if self.context_mode[\"mode\"] == 'mdmsds':\n                self.x = self.choose(context_ms) \n                if self.expl_dims == self.conf.s_dims and not self.context_mode['choose_m']:\n                    m = context_ms[:self.conf.m_ndims//2]\n                    in_dims = range(self.conf.m_ndims//2) + range(self.conf.m_ndims, self.conf.m_ndims + self.conf.s_ndims)\n                    out_dims = range(self.conf.m_ndims//2, self.conf.m_ndims)\n                    dm = self.infer(in_dims, \n                                    out_dims, \n                                    np.array(m + list(self.x)))\n                    self.y = np.hstack((m, dm))\n                else:\n                    self.y = self.infer(self.expl_dims, self.inf_dims, self.x)\n            elif self.context_mode[\"mode\"] == 'mcs':\n                self.x = self.choose(context_ms) \n                self.y = self.infer(self.expl_dims, self.inf_dims, self.x)\n                \n\n        self.m, self.s = self.extract_ms(self.x, self.y)\n\n        movement = self.motor_primitive(self.m)\n\n        self.emit('choice', self.x)\n        self.emit('inference', self.y)\n        self.emit('movement', movement)\n\n        return movement"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef perceive(self, s_, context=None):\n        s = self.sensory_primitive(s_)\n        self.emit('perception', s)\n        if context is None:                \n            self.sensorimotor_model.update(self.m, s)\n            self.interest_model.update(np.hstack((self.m, self.s)), np.hstack((self.m, s)))\n        else:       \n            if self.context_mode[\"mode\"] == 'mdmsds':  \n                m = self.m[:len(self.m)//2]\n                dm = self.m[len(self.m)//2:]\n                ds = s[len(s)//2:]\n                s = s[:len(s)//2]\n                ds_g = self.s[:len(self.s)//2]\n                self.sensorimotor_model.update(np.hstack((m, dm)), np.hstack((s, ds)))\n                self.interest_model.update(np.hstack((m, dm, context, ds_g)), np.hstack((m, dm, s, ds)))\n            elif self.context_mode[\"mode\"] == 'mcs':\n                s = s[len(context):]\n                s_g = self.s[len(context):]\n                self.sensorimotor_model.update(self.m, np.hstack((context, s)))\n                self.interest_model.update(np.hstack((self.m, context, s_g)), np.hstack((self.m, context, s)))\n                \n        self.t += 1", "response": "Update the sensorimotor model with the given s and context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict_y(self, xq, sigma=None, k=None, **kwargs):\n        sigma = sigma or self.sigma\n        k = k or self.k\n        dists, index = self.dataset.nn_x(xq, k = k)\n        w = self._weights(dists, sigma*sigma)\n        return np.sum([wi*self.dataset.get_y(idx) for wi, idx in zip(w, index)], axis = 0)", "response": "Provide an prediction of xq in the output space\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_motor_command(self, m_ag):\n        m_env = bounds_min_max(m_ag, self.conf.m_mins, self.conf.m_maxs)\n        return m_env", "response": "Compute the motor command by restricting it to the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove the motors and retrieve the tracked object position.", "response": "def compute_sensori_effect(self, m_env):\n        \"\"\" Move the robot motors and retrieve the tracked object position. \"\"\"\n        pos = {m.name: pos for m, pos in zip(self.motors, m_env)}\n        self.robot.goto_position(pos, self.move_duration, wait=True)\n\n        # This allows to actually apply a motor command\n        # Without having a tracker\n        if self.tracker is not None:\n            return self.tracker.get_object_position(self.tracked_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_xy(self, x, y):\n        self.imodel.add_xy(self._pre_x(x), self._pre_y(y))", "response": "Add an x y vector to the model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninfer an order in order to obtain an effect close to goal given the observations of the model.", "response": "def infer_order(self, goal, **kwargs):\n        \"\"\"Infer an order in order to obtain an effect close to goal, given the\n        observation data of the model.\n\n        :arg goal:  a goal vector compatible with self.Sfeats\n        :rtype:  same as goal (if list, tuple, or pandas.Series, else tuple)\n        \"\"\"\n        x = self.imodel.infer_x(np.array(self._pre_y(goal)), **kwargs)[0]\n        return self._post_x(x, goal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_effect(self, order, **kwargs):\n        y = self.imodel.fmodel.predict_y(np.array(self._pre_x(order)), **kwargs)\n        return self._post_y(y, order)", "response": "Predict the effect of a goal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a Nearest Neighbor forward model from an existing dataset.", "response": "def from_dataset(cls, dataset, **kwargs):\n        \"\"\"Construct a Nearest Neighbor forward model from an existing dataset.\"\"\"\n        m = cls(dataset.dim_x, dataset.dim_y, **kwargs)\n        m.dataset = dataset\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_robot(cls, robot, **kwargs):\n        m = cls(len(robot.m_feats), len(robot.s_feats), **kwargs)\n        return m", "response": "Construct a Nearest Neighbor forward model from an existing dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_x(self, y):\n        OptimizedInverseModel.infer_x(self, y)\n        if self.fmodel.size() == 0:\n            return self._random_x()\n\n        x_guesses = [self._guess_x_simple(y)[0]]\n        result = []\n        for i, xg in enumerate(x_guesses):\n            res = scipy.optimize.minimize(self._error, xg,\n                                          args        = (),\n                                          method      = self.algo,\n                                          bounds      = self.constraints,\n                                          options     = self.conf\n                                         )\n\n\n\n            d = self._error(res.x)\n            result.append((d, i, res.x))\n        return [self._enforce_bounds(xi) for fi, i, xi in sorted(result)]", "response": "Infer probable x from input y."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninferring probable output from input x y", "response": "def infer_dm(self, m, s, ds):\n        \"\"\"Infer probable output from input x, y\n        \"\"\"\n        OptimizedInverseModel.infer_dm(self, ds)\n        if len(self.fmodel.dataset) == 0:\n            return [[0.0]*self.dim_out]\n        else:\n            _, index = self.fmodel.dataset.nn_dims(m, np.hstack((s, ds)), range(len(m)), range(self.dim_x, self.dim_x + self.dim_y), k=1)\n            guesses = [self.fmodel.dataset.get_dims(index[0], dims=range(len(m), self.dim_x))]\n            result = []\n                    \n            for g in guesses:\n                res = scipy.optimize.minimize(lambda dm:self._error_dm(m, dm, s), g,\n                                              args        = (),\n                                              method      = self.algo,\n                                              options     = self.conf\n                                             )\n    \n    \n    \n                d = self._error_dm(m, res.x, s)\n                result.append((d, res.x))\n            return [xi for fi, xi in sorted(result)][0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nenforce the bounds on x if only infinitesimal violations occurs occurs", "response": "def _enforce_bounds(self, x):\n        \"\"\"\"Enforce the bounds on x if only infinitesimal violations occurs\"\"\"\n        assert len(x) == len(self.bounds)\n        x_enforced = []\n        for x_i, (lb, ub) in zip(x, self.bounds):\n            if x_i < lb:\n                if x_i > lb - (ub-lb)/1e10:\n                    x_enforced.append(lb)\n                else:\n                    x_enforced.append(x_i)\n            elif x_i > ub:\n                if x_i < ub + (ub-lb)/1e10:\n                    x_enforced.append(ub)\n                else:\n                    x_enforced.append(x_i)\n            else:\n                x_enforced.append(x_i)\n        return np.array(x_enforced)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit(self, counts_df, val_set=None):\n\n\t\t## a basic check\n\t\tif self.stop_crit == 'val-llk':\n\t\t\tif val_set is None:\n\t\t\t\traise ValueError(\"If 'stop_crit' is set to 'val-llk', must provide a validation set.\")\n\n\t\t## running each sub-process\n\t\tif self.verbose:\n\t\t\tself._print_st_msg()\n\t\tself._process_data(counts_df)\n\t\tif self.verbose:\n\t\t\tself._print_data_info()\n\t\tif (val_set is not None) and (self.stop_crit!='diff-norm') and (self.stop_crit!='train-llk'):\n\t\t\tself._process_valset(val_set)\n\t\telse:\n\t\t\tself.val_set = None\n\t\t\t\n\t\tself._cast_before_fit()\n\t\tself._fit()\n\t\t\n\t\t## after terminating optimization\n\t\tif self.keep_data:\n\t\t\tif self.users_per_batch == 0:\n\t\t\t\tself._store_metadata()\n\t\t\telse:\n\t\t\t\tself._st_ix_user = self._st_ix_user[:-1]\n\t\tif self.produce_dicts and self.reindex:\n\t\t\tself.user_dict_ = {self.user_mapping_[i]:i for i in range(self.user_mapping_.shape[0])}\n\t\t\tself.item_dict_ = {self.item_mapping_[i]:i for i in range(self.item_mapping_.shape[0])}\n\t\tself.is_fitted = True\n\t\tdel self.input_df\n\t\tdel self.val_set\n\t\t\n\t\treturn self", "response": "Fit the object to a sparse count data frame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the model with batches of data from a subset of users or items Note ---- You must pass either the **full set of user-item interactions** that are non-zero for some subset of users, or the **full set of item-user intersactions** that are non-zero for some subset of items. Otherwise, if passing a random sample of triplets, the model will not converge to reasonable results. Note ---- All user and items IDs must be integers starting at one, without gaps in the numeration. Note ---- For better results, fit the model with full-batch iterations (using the 'fit' method). Adding new users and/or items without refitting the model might result in worsened results for existing users/items. For adding users without altering the parameters for items or for other users, see the method 'add_user'. Note ---- Fitting in mini-batches is more prone to numerical instability and compared to full-batch variational inference, it is more likely that all your parameters will turn to NaNs (which means the optimization procedure failed). Parameters ---------- counts_df : data frame (n_samples, 3) Data frame with the user-item interactions for some subset of users. Must have columns 'UserId', 'ItemId', 'Count'. batch_type : str, one of 'users' or 'items' Whether 'counts_df' contains a sample of users with all their item counts ('users'), or a sample of items with all their user counts ('items'). step_size : None or float in (0, 1) Step size with which to update the global variables in the model. Must be a number between zero and one. If passing None, will determine it according to the step size function with which the model was initialized and the number of iterations or calls to partial fit that have been performed. If no valid function was passed at the initialization, it will use 1/sqrt(i+1). nusers : int Total number of users (not just in this batch!). Only required if calling partial_fit for the first time on a model object that hasn't been fit. nitems : int Total number of items (not just in this batch!). Only required if calling partial_fit for the first time on a model object that hasn't been fit. users_in_batch : None or array (n_users_sample,) Users that are present int counts_df. If passing None, will determine the unique elements in counts_df.UserId, but passing them if you already have them will skip this step. items_in_batch : None or array (n_items_sample,) Items that are present int counts_df. If passing None, will determine the unique elements in counts_df.ItemId, but passing them if you already have them will skip this step. new_users : bool Whether the data contains new users with numeration greater than the number of users with which the model was initially fit. **For better results refit the model including all users/items instead of adding them afterwards**. new_items : bool Whether the data contains new items with numeration greater than the number of items with which the model was initially fit. **For better results refit the model including all users/items instead of adding them afterwards**. random_seed : int Random seed to be used for the initialization of new user/item parameters. Ignored when new_users=False and new_items=False. Returns ------- self : obj Copy of this object.", "response": "def partial_fit(self, counts_df, batch_type='users', step_size=None,\n\t\t\t\t\tnusers=None, nitems=None, users_in_batch=None, items_in_batch=None,\n\t\t\t\t\tnew_users=False, new_items=False, random_seed=None):\n\t\t\"\"\"\n\t\tUpdates the model with batches of data from a subset of users or items\n\n\t\tNote\n\t\t----\n\t\tYou must pass either the **full set of user-item interactions** that are non-zero for some\n\t\tsubset of users, or the **full set of item-user intersactions** that are non-zero for some\n\t\tsubset of items.\n\t\tOtherwise, if passing a random sample of triplets, the model will not converge to reasonable results.\n\n\t\tNote\n\t\t----\n\t\tAll user and items IDs must be integers starting at one, without gaps in the numeration.\n\n\t\tNote\n\t\t----\n\t\tFor better results, fit the model with full-batch iterations (using the 'fit' method).\n\t\tAdding new users and/or items without refitting the model might result in worsened results\n\t\tfor existing users/items. For adding users without altering the parameters for items or for\n\t\tother users, see the method 'add_user'.\n\n\t\tNote\n\t\t----\n\t\tFitting in mini-batches is more prone to numerical instability and compared to full-batch\n\t\tvariational inference, it is more likely that all your parameters will turn to NaNs (which\n\t\tmeans the optimization procedure failed).\n\n\t\tParameters\n\t\t----------\n\t\tcounts_df : data frame (n_samples, 3)\n\t\t\tData frame with the user-item interactions for some subset of users. Must have columns\n\t\t\t'UserId', 'ItemId', 'Count'.\n\t\tbatch_type : str, one of 'users' or 'items'\n\t\t\tWhether 'counts_df' contains a sample of users with all their item counts ('users'), or a\n\t\t\tsample of items with all their user counts ('items').\n\t\tstep_size : None or float in (0, 1)\n\t\t\tStep size with which to update the global variables in the model. Must be a number between\n\t\t\tzero and one. If passing None, will determine it according to the step size function with which\n\t\t\tthe model was initialized and the number of iterations or calls to partial fit that have been\n\t\t\tperformed. If no valid function was passed at the initialization, it will use 1/sqrt(i+1).\n\t\tnusers : int\n\t\t\tTotal number of users (not just in this batch!). Only required if calling partial_fit for the\n\t\t\tfirst time on a model object that hasn't been fit.\n\t\tnitems : int\n\t\t\tTotal number of items (not just in this batch!). Only required if calling partial_fit for the\n\t\t\tfirst time on a model object that hasn't been fit.\n\t\tusers_in_batch : None or array (n_users_sample,)\n\t\t\tUsers that are present int counts_df. If passing None, will determine the unique elements in\n\t\t\tcounts_df.UserId, but passing them if you already have them will skip this step.\n\t\titems_in_batch : None or array (n_items_sample,)\n\t\t\tItems that are present int counts_df. If passing None, will determine the unique elements in\n\t\t\tcounts_df.ItemId, but passing them if you already have them will skip this step.\n\t\tnew_users : bool\n\t\t\tWhether the data contains new users with numeration greater than the number of users with which\n\t\t\tthe model was initially fit. **For better results refit the model including all users/items instead\n\t\t\tof adding them afterwards**.\n\t\tnew_items : bool\n\t\t\tWhether the data contains new items with numeration greater than the number of items with which\n\t\t\tthe model was initially fit. **For better results refit the model including all users/items instead\n\t\t\tof adding them afterwards**.\n\t\trandom_seed : int\n\t\t\tRandom seed to be used for the initialization of new user/item parameters. Ignored when\n\t\t\tnew_users=False and new_items=False.\n\n\t\tReturns\n\t\t-------\n\t\tself : obj\n\t\t\tCopy of this object.\n\t\t\"\"\"\n\n\t\tif self.reindex:\n\t\t\traise ValueError(\"'partial_fit' can only be called when using reindex=False.\")\n\t\tif not self.keep_all_objs:\n\t\t\traise ValueError(\"'partial_fit' can only be called when using keep_all_objs=True.\")\n\t\tif self.keep_data:\n\t\t\ttry:\n\t\t\t\tself.seen\n\t\t\t\tmsg = \"When using 'partial_fit', the list of items seen by each user is not updated \"\n\t\t\t\tmsg += \"with the data passed here.\"\n\t\t\t\twarnings.warn(msg)\n\t\t\texcept:\n\t\t\t\tmsg = \"When fitting the model through 'partial_fit' without calling 'fit' beforehand, \"\n\t\t\t\tmsg += \"'keep_data' will be forced to False.\"\n\t\t\t\twarnings.warn(msg)\n\t\t\t\tself.keep_data = False\n\n\t\tassert batch_type in ['users', 'items']\n\t\tif batch_type == 'users':\n\t\t\tuser_batch = True\n\t\telse:\n\t\t\tuser_batch = False\n\n\t\tif nusers is None:\n\t\t\ttry:\n\t\t\t\tnusers = self.nusers\n\t\t\texcept:\n\t\t\t\traise ValueError(\"Must specify total number of users when calling 'partial_fit' for the first time.\")\n\t\tif nitems is None:\n\t\t\ttry:\n\t\t\t\tnitems = self.nitems\n\t\t\texcept:\n\t\t\t\traise ValueError(\"Must specify total number of items when calling 'partial_fit' for the first time.\")\n\n\t\ttry:\n\t\t\tif self.nusers is None:\n\t\t\t\tself.nusers = nusers\n\t\texcept:\n\t\t\tself.nusers = nusers\n\t\ttry:\n\t\t\tif self.nitems is None:\n\t\t\t\tself.nitems = nitems\n\t\texcept:\n\t\t\tself.nitems = nitems\n\n\t\tif step_size is None:\n\t\t\ttry:\n\t\t\t\tself.step_size(0)\n\t\t\t\ttry:\n\t\t\t\t\tstep_size = self.step_size(self.niter)\n\t\t\t\texcept:\n\t\t\t\t\tself.niter = 0\n\t\t\t\t\tstep_size = 1.0\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\tstep_size = 1 / np.sqrt(self.niter + 2)\n\t\t\t\texcept:\n\t\t\t\t\tself.niter = 0\n\t\t\t\t\tstep_size = 1.0\n\t\tassert step_size >= 0\n\t\tassert step_size <= 1\n\n\t\tif random_seed is not None:\n\t\t\tif isinstance(random_seed, float):\n\t\t\t\trandom_seed = int(random_seed)\n\t\t\tassert isinstance(random_seed, int)\n\n\t\tif counts_df.__class__.__name__ == \"ndarray\":\n\t\t\tcounts_df = pd.DataFrame(counts_df)\n\t\t\tcounts_df.columns[:3] = ['UserId', 'ItemId', 'Count']\n\n\t\tassert counts_df.__class__.__name__ == \"DataFrame\"\n\t\tassert 'UserId' in counts_df.columns.values\n\t\tassert 'ItemId' in counts_df.columns.values\n\t\tassert 'Count' in counts_df.columns.values\n\t\tassert counts_df.shape[0] > 0\n\n\t\tY_batch = counts_df.Count.values.astype('float32')\n\t\tix_u_batch = counts_df.UserId.values.astype(cython_loops.obj_ind_type)\n\t\tix_i_batch = counts_df.ItemId.values.astype(cython_loops.obj_ind_type)\n\n\t\tif users_in_batch is None:\n\t\t\tusers_in_batch = np.unique(ix_u_batch)\n\t\telse:\n\t\t\tusers_in_batch = np.array(users_in_batch).astype(cython_loops.obj_ind_type)\n\t\tif items_in_batch is None:\n\t\t\titems_in_batch = np.unique(ix_i_batch)\n\t\telse:\n\t\t\titems_in_batch = np.array(items_in_batch).astype(cython_loops.obj_ind_type)\n\n\t\tif (self.Theta is None) or (self.Beta is None):\n\t\t\tself._cast_before_fit()\n\t\t\tself.Gamma_shp, self.Gamma_rte, self.Lambda_shp, self.Lambda_rte, \\\n\t\t\tself.k_rte, self.t_rte = cython_loops.initialize_parameters(\n\t\t\t\tself.Theta, self.Beta, self.random_seed, self.a, self.a_prime,\n\t\t\t\tself.b_prime, self.c, self.c_prime, self.d_prime)\n\t\t\tself.Theta = self.Gamma_shp / self.Gamma_rte\n\t\t\tself.Beta = self.Lambda_shp / self.Lambda_rte\n\n\t\tif new_users:\n\t\t\tif not self.keep_all_objs:\n\t\t\t\traise ValueError(\"Can only add users without refitting when using keep_all_objs=True\")\n\t\t\tnusers_now = ix_u_batch.max() + 1\n\t\t\tnusers_add = self.nusers - nusers_now\n\t\t\tif nusers_add < 1:\n\t\t\t\traise ValueError(\"There are no new users in the data passed to 'partial_fit'.\")\n\t\t\tself._initialize_extra_users(nusers_add, random_seed)\n\t\t\tself.nusers += nusers_add\n\n\t\tif new_items:\n\t\t\tif not self.keep_all_objs:\n\t\t\t\traise ValueError(\"Can only add items without refitting when using keep_all_objs=True\")\n\t\t\tnitems_now = ix_i_batch.max() + 1\n\t\t\tnitems_add = self.nitems - nitems_now\n\t\t\tif nitems_add < 1:\n\t\t\t\traise ValueError(\"There are no new items in the data passed to 'partial_fit'.\")\n\t\t\tself._initialize_extra_items(nitems_add, random_seed)\n\t\t\tself.nitems += nitems_add\n\n\t\tk_shp = cython_loops.cast_float(self.a_prime + self.k * self.a)\n\t\tt_shp = cython_loops.cast_float(self.c_prime + self.k * self.c)\n\t\tadd_k_rte = cython_loops.cast_float(self.a_prime / self.b_prime)\n\t\tadd_t_rte = cython_loops.cast_float(self.c_prime / self.d_prime)\n\t\tmultiplier_batch = float(nusers) / users_in_batch.shape[0]\n\n\t\tcython_loops.partial_fit(\n\t\t\t\t\tY_batch,\n\t\t\t\t\tix_u_batch, ix_i_batch,\n\t\t\t\t\tself.Theta, self.Beta,\n\t\t\t\t\tself.Gamma_shp, self.Gamma_rte,\n\t\t\t\t\tself.Lambda_shp, self.Lambda_rte,\n\t\t\t\t\tself.k_rte, self.t_rte,\n\t\t\t\t\tadd_k_rte, add_t_rte, self.a, self.c,\n\t\t\t\t\tk_shp, t_shp, cython_loops.cast_ind_type(self.k),\n\t\t\t\t\tusers_in_batch, items_in_batch,\n\t\t\t\t\tcython_loops.cast_int(self.allow_inconsistent_math),\n\t\t\t\t\tcython_loops.cast_float(step_size), cython_loops.cast_float(multiplier_batch),\n\t\t\t\t\tself.ncores, user_batch\n\t\t\t\t)\n\n\t\tself.niter += 1\n\t\tself.is_fitted = True\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting latent factors for a user given her item counts This is similar to obtaining topics for a document in LDA. Note ---- This function will NOT modify any of the item parameters. Note ---- This function only works with one user at a time. Note ---- This function is prone to producing all NaNs values. Parameters ---------- counts_df : DataFrame or array (nsamples, 2) Data Frame with columns 'ItemId' and 'Count', indicating the non-zero item counts for a user for whom it's desired to obtain latent factors. maxiter : int Maximum number of iterations to run. ncores : int Number of threads/cores to use. With data for only one user, it's unlikely that using multiple threads would give a significant speed-up, and it might even end up making the function slower due to the overhead. If passing -1, it will determine the maximum number of cores in the system and use that. random_seed : int Random seed used to initialize parameters. stop_thr : float If the l2-norm of the difference between values of Theta_{u} between interations is less than this, it will stop. Smaller values of 'k' should require smaller thresholds. return_all : bool Whether to return also the intermediate calculations (Gamma_shp, Gamma_rte). When passing True here, the output will be a tuple containing (Theta, Gamma_shp, Gamma_rte, Phi) Returns ------- latent_factors : array (k,) Calculated latent factors for the user, given the input data", "response": "def predict_factors(self, counts_df, maxiter=10, ncores=1, random_seed=1, stop_thr=1e-3, return_all=False):\n\t\t\"\"\"\n\t\tGets latent factors for a user given her item counts\n\n\t\tThis is similar to obtaining topics for a document in LDA.\n\n\t\tNote\n\t\t----\n\t\tThis function will NOT modify any of the item parameters.\n\n\t\tNote\n\t\t----\n\t\tThis function only works with one user at a time.\n\n\t\tNote\n\t\t----\n\t\tThis function is prone to producing all NaNs values.\n\n\t\tParameters\n\t\t----------\n\t\tcounts_df : DataFrame or array (nsamples, 2)\n\t\t\tData Frame with columns 'ItemId' and 'Count', indicating the non-zero item counts for a\n\t\t\tuser for whom it's desired to obtain latent factors.\n\t\tmaxiter : int\n\t\t\tMaximum number of iterations to run.\n\t\tncores : int\n\t\t\tNumber of threads/cores to use. With data for only one user, it's unlikely that using\n\t\t\tmultiple threads would give a significant speed-up, and it might even end up making\n\t\t\tthe function slower due to the overhead.\n\t\t\tIf passing -1, it will determine the maximum number of cores in the system and use that.\n\t\trandom_seed : int\n\t\t\tRandom seed used to initialize parameters.\n\t\tstop_thr : float\n\t\t\tIf the l2-norm of the difference between values of Theta_{u} between interations is less\n\t\t\tthan this, it will stop. Smaller values of 'k' should require smaller thresholds.\n\t\treturn_all : bool\n\t\t\tWhether to return also the intermediate calculations (Gamma_shp, Gamma_rte). When\n\t\t\tpassing True here, the output will be a tuple containing (Theta, Gamma_shp, Gamma_rte, Phi)\n\n\t\tReturns\n\t\t-------\n\t\tlatent_factors : array (k,)\n\t\t\tCalculated latent factors for the user, given the input data\n\t\t\"\"\"\n\n\t\tncores, random_seed, stop_thr, maxiter = self._check_input_predict_factors(ncores, random_seed, stop_thr, maxiter)\n\n\t\t## processing the data\n\t\tcounts_df = self._process_data_single(counts_df)\n\n\t\t## calculating the latent factors\n\t\tTheta = np.empty(self.k, dtype='float32')\n\t\ttemp = cython_loops.calc_user_factors(\n\t\t\t\t\t\t\t\t self.a, self.a_prime, self.b_prime,\n\t\t\t\t\t\t\t\t self.c, self.c_prime, self.d_prime,\n\t\t\t\t\t\t\t\t counts_df.Count.values,\n\t\t\t\t\t\t\t\t counts_df.ItemId.values,\n\t\t\t\t\t\t\t\t Theta, self.Beta,\n\t\t\t\t\t\t\t\t self.Lambda_shp,\n\t\t\t\t\t\t\t\t self.Lambda_rte,\n\t\t\t\t\t\t\t\t cython_loops.cast_ind_type(counts_df.shape[0]), cython_loops.cast_ind_type(self.k),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(int(maxiter)), cython_loops.cast_int(ncores),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(int(random_seed)), cython_loops.cast_float(stop_thr),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(bool(return_all))\n\t\t\t\t\t\t\t\t )\n\n\t\tif np.isnan(Theta).sum() > 0:\n\t\t\traise ValueError(\"NaNs encountered in the result. Failed to produce latent factors.\")\n\n\t\tif return_all:\n\t\t\treturn (Theta, temp[0], temp[1], temp[2])\n\t\telse:\n\t\t\treturn Theta"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a new user to the model or update parameters for a user according to new data Note ---- This function will NOT modify any of the item parameters. Note ---- This function only works with one user at a time. For updating many users at the same time, use 'partial_fit' instead. Note ---- For betters results, refit the model again from scratch. Note ---- This function is prone to producing all NaNs values. Parameters ---------- user_id : obj Id to give to be user (when adding a new one) or Id of the existing user whose parameters are to be updated according to the data in 'counts_df'. **Make sure that the data type is the same that was passed in the training data, so if you have integer IDs, don't pass a string as ID**. counts_df : data frame or array (nsamples, 2) Data Frame with columns 'ItemId' and 'Count'. If passing a numpy array, will take the first two columns in that order. Data containing user/item interactions **from one user only** for which to add or update parameters. Note that you need to pass *all* the user-item interactions for this user when making an update, not just the new ones. update_existing : bool Whether this should be an update of the parameters for an existing user (when passing True), or an addition of a new user that was not in the model before (when passing False). maxiter : int Maximum number of iterations to run. ncores : int Number of threads/cores to use. With data for only one user, it's unlikely that using multiple threads would give a significant speed-up, and it might even end up making the function slower due to the overhead. random_seed : int Random seed used to initialize parameters. stop_thr : float If the l2-norm of the difference between values of Theta_{u} between interations is less than this, it will stop. Smaller values of 'k' should require smaller thresholds. update_all_params : bool Whether to update also the item parameters in each iteration. If passing True, will update them with a step size given determined by the number of iterations already taken and the step_size function given as input in the model constructor call. Returns ------- True : bool Will return True if the process finishes successfully.", "response": "def add_user(self, user_id, counts_df, update_existing=False, maxiter=10, ncores=1,\n\t\t\t\t random_seed=1, stop_thr=1e-3, update_all_params=None):\n\t\t\"\"\"\n\t\tAdd a new user to the model or update parameters for a user according to new data\n\t\t\n\t\tNote\n\t\t----\n\t\tThis function will NOT modify any of the item parameters.\n\n\t\tNote\n\t\t----\n\t\tThis function only works with one user at a time. For updating many users at the same time,\n\t\tuse 'partial_fit' instead.\n\n\t\tNote\n\t\t----\n\t\tFor betters results, refit the model again from scratch.\n\n\t\tNote\n\t\t----\n\t\tThis function is prone to producing all NaNs values.\n\n\t\tParameters\n\t\t----------\n\t\tuser_id : obj\n\t\t\tId to give to be user (when adding a new one) or Id of the existing user whose parameters are to be\n\t\t\tupdated according to the data in 'counts_df'. **Make sure that the data type is the same that was passed\n\t\t\tin the training data, so if you have integer IDs, don't pass a string as ID**.\n\t\tcounts_df : data frame or array (nsamples, 2)\n\t\t\tData Frame with columns 'ItemId' and 'Count'. If passing a numpy array, will take the first two columns\n\t\t\tin that order. Data containing user/item interactions **from one user only** for which to add or update\n\t\t\tparameters. Note that you need to pass *all* the user-item interactions for this user when making an update,\n\t\t\tnot just the new ones.\n\t\tupdate_existing : bool\n\t\t\tWhether this should be an update of the parameters for an existing user (when passing True), or\n\t\t\tan addition of a new user that was not in the model before (when passing False).\n\t\tmaxiter : int\n\t\t\tMaximum number of iterations to run.\n\t\tncores : int\n\t\t\tNumber of threads/cores to use. With data for only one user, it's unlikely that using\n\t\t\tmultiple threads would give a significant speed-up, and it might even end up making\n\t\t\tthe function slower due to the overhead.\n\t\trandom_seed : int\n\t\t\tRandom seed used to initialize parameters.\n\t\tstop_thr : float\n\t\t\tIf the l2-norm of the difference between values of Theta_{u} between interations is less\n\t\t\tthan this, it will stop. Smaller values of 'k' should require smaller thresholds.\n\t\tupdate_all_params : bool\n\t\t\tWhether to update also the item parameters in each iteration. If passing True, will update them\n\t\t\twith a step size given determined by the number of iterations already taken and the step_size function\n\t\t\tgiven as input in the model constructor call.\n\n\t\tReturns\n\t\t-------\n\t\tTrue : bool\n\t\t\tWill return True if the process finishes successfully.\n\t\t\"\"\"\n\n\t\tncores, random_seed, stop_thr, maxiter = self._check_input_predict_factors(ncores, random_seed, stop_thr, maxiter)\n\n\t\tif update_existing:\n\t\t\t## checking that the user already exists\n\t\t\tif self.produce_dicts and self.reindex:\n\t\t\t\tuser_id = self.user_dict_[user_id]\n\t\t\telse:\n\t\t\t\tif self.reindex:\n\t\t\t\t\tuser_id = pd.Categorical(np.array([user_id]), self.user_mapping_).codes[0]\n\t\t\t\t\tif user_id == -1:\n\t\t\t\t\t\traise ValueError(\"User was not present in the training data.\")\n\n\t\t## processing the data\n\t\tcounts_df = self._process_data_single(counts_df)\n\t\t\n\t\tif update_all_params:\n\t\t\tcounts_df['UserId'] = user_id\n\t\t\tcounts_df['UserId'] = counts_df.UserId.astype(cython_loops.obj_ind_type)\n\t\t\tself.partial_fit(counts_df, new_users=(not update_existing))\n\t\t\tTheta_prev = self.Theta[-1].copy()\n\t\t\tfor i in range(maxiter - 1):\n\t\t\t\tself.partial_fit(counts_df)\n\t\t\t\tnew_Theta = self.Theta[-1]\n\t\t\t\tif np.linalg.norm(new_Theta - Theta_prev) <= stop_thr:\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tTheta_prev = self.Theta[-1].copy()\n\t\telse:\n\t\t\t## calculating the latent factors\n\t\t\tTheta = np.empty(self.k, dtype='float32')\n\t\t\ttemp = cython_loops.calc_user_factors(\n\t\t\t\t\t\t\t\t self.a, self.a_prime, self.b_prime,\n\t\t\t\t\t\t\t\t self.c, self.c_prime, self.d_prime,\n\t\t\t\t\t\t\t\t counts_df.Count.values,\n\t\t\t\t\t\t\t\t counts_df.ItemId.values,\n\t\t\t\t\t\t\t\t Theta, self.Beta,\n\t\t\t\t\t\t\t\t self.Lambda_shp,\n\t\t\t\t\t\t\t\t self.Lambda_rte,\n\t\t\t\t\t\t\t\t cython_loops.cast_ind_type(counts_df.shape[0]), cython_loops.cast_ind_type(self.k),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(maxiter), cython_loops.cast_int(ncores),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(random_seed), cython_loops.cast_int(stop_thr),\n\t\t\t\t\t\t\t\t cython_loops.cast_int(self.keep_all_objs)\n\t\t\t\t\t\t\t\t )\n\n\t\t\tif np.isnan(Theta).sum() > 0:\n\t\t\t\traise ValueError(\"NaNs encountered in the result. Failed to produce latent factors.\")\n\n\t\t\t## adding the data to the model\n\t\t\tif update_existing:\n\t\t\t\tself.Theta[user_id] = Theta\n\t\t\t\tif self.keep_all_objs:\n\t\t\t\t\tself.Gamma_shp[user_id] = temp[0]\n\t\t\t\t\tself.Gamma_rte[user_id] = temp[1]\n\t\t\t\t\tself.k_rte[user_id] = self.a_prime/self.b_prime + \\\n\t\t\t\t\t\t\t\t\t\t(temp[0].reshape((1,-1))/temp[1].reshape((1,-1))).sum(axis=1, keepdims=True)\n\t\t\telse:\n\t\t\t\tif self.reindex:\n\t\t\t\t\tnew_id = self.user_mapping_.shape[0]\n\t\t\t\t\tself.user_mapping_ = np.r_[self.user_mapping_, user_id]\n\t\t\t\t\tif self.produce_dicts:\n\t\t\t\t\t\tself.user_dict_[user_id] = new_id\n\t\t\t\tself.Theta = np.r_[self.Theta, Theta.reshape((1, self.k))]\n\t\t\t\tif self.keep_all_objs:\n\t\t\t\t\tself.Gamma_shp = np.r_[self.Gamma_shp, temp[0].reshape((1, self.k))]\n\t\t\t\t\tself.Gamma_rte = np.r_[self.Gamma_rte, temp[1].reshape((1, self.k))]\n\t\t\t\t\tself.k_rte = np.r_[self.k_rte, self.a_prime/self.b_prime + \\\n\t\t\t\t\t\t\t\t\t(temp[0].reshape((1,-1))/temp[1].reshape((1,-1))).sum(axis=1, keepdims=True)]\n\t\t\t\tself.nusers += 1\n\n\t\t## updating the list of seen items for this user\n\t\tif self.keep_data:\n\t\t\tif update_existing:\n\t\t\t\tn_seen_by_user_before = self._n_seen_by_user[user_id]\n\t\t\t\tself._n_seen_by_user[user_id] = counts_df.shape[0]\n\t\t\t\tself.seen = np.r_[self.seen[:user_id], counts_df.ItemId.values, self.seen[(user_id + 1):]]\n\t\t\t\tself._st_ix_user[(user_id + 1):] += self._n_seen_by_user[user_id] - n_seen_by_user_before\n\t\t\telse:\n\t\t\t\tself._n_seen_by_user = np.r_[self._n_seen_by_user, counts_df.shape[0]]\n\t\t\t\tself._st_ix_user = np.r_[self._st_ix_user, self.seen.shape[0]]\n\t\t\t\tself.seen = np.r_[self.seen, counts_df.ItemId.values]\n\n\t\treturn True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, user, item):\n\t\tassert self.is_fitted\n\t\tif isinstance(user, list) or isinstance(user, tuple):\n\t\t\tuser = np.array(user)\n\t\tif isinstance(item, list) or isinstance(item, tuple):\n\t\t\titem = np.array(item)\n\t\tif user.__class__.__name__=='Series':\n\t\t\tuser = user.values\n\t\tif item.__class__.__name__=='Series':\n\t\t\titem = item.values\n\t\t\t\n\t\tif isinstance(user, np.ndarray):\n\t\t\tif len(user.shape) > 1:\n\t\t\t\tuser = user.reshape(-1)\n\t\t\tassert user.shape[0] > 0\n\t\t\tif self.reindex:\n\t\t\t\tif user.shape[0] > 1:\n\t\t\t\t\tuser = pd.Categorical(user, self.user_mapping_).codes\n\t\t\t\telse:\n\t\t\t\t\tif self.user_dict_ is not None:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tuser = self.user_dict_[user]\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\tuser = -1\n\t\t\t\t\telse:\n\t\t\t\t\t\tuser = pd.Categorical(user, self.user_mapping_).codes[0]\n\t\telse:\n\t\t\tif self.reindex:\n\t\t\t\tif self.user_dict_ is not None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tuser = self.user_dict_[user]\n\t\t\t\t\texcept:\n\t\t\t\t\t\tuser = -1\n\t\t\t\telse:\n\t\t\t\t\tuser = pd.Categorical(np.array([user]), self.user_mapping_).codes[0]\n\t\t\tuser = np.array([user])\n\t\t\t\n\t\tif isinstance(item, np.ndarray):\n\t\t\tif len(item.shape) > 1:\n\t\t\t\titem = item.reshape(-1)\n\t\t\tassert item.shape[0] > 0\n\t\t\tif self.reindex:\n\t\t\t\tif item.shape[0] > 1:\n\t\t\t\t\titem = pd.Categorical(item, self.item_mapping_).codes\n\t\t\t\telse:\n\t\t\t\t\tif self.item_dict_ is not None:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\titem = self.item_dict_[item]\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\titem = -1\n\t\t\t\t\telse:\n\t\t\t\t\t\titem = pd.Categorical(item, self.item_mapping_).codes[0]\n\t\telse:\n\t\t\tif self.reindex:\n\t\t\t\tif self.item_dict_ is not None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\titem = self.item_dict_[item]\n\t\t\t\t\texcept:\n\t\t\t\t\t\titem = -1\n\t\t\t\telse:\n\t\t\t\t\titem = pd.Categorical(np.array([item]), self.item_mapping_).codes[0]\n\t\t\titem = np.array([item])\n\n\t\tassert user.shape[0] == item.shape[0]\n\t\t\n\t\tif user.shape[0] == 1:\n\t\t\tif (user[0] == -1) or (item[0] == -1):\n\t\t\t\treturn np.nan\n\t\t\telse:\n\t\t\t\treturn self.Theta[user].dot(self.Beta[item].T).reshape(-1)[0]\n\t\telse:\n\t\t\tnan_entries = (user == -1) | (item == -1)\n\t\t\tif nan_entries.sum() == 0:\n\t\t\t\tif user.dtype != cython_loops.obj_ind_type:\n\t\t\t\t\tuser = user.astype(cython_loops.obj_ind_type)\n\t\t\t\tif item.dtype != cython_loops.obj_ind_type:\n\t\t\t\t\titem = item.astype(cython_loops.obj_ind_type)\n\t\t\t\treturn cython_loops.predict_arr(self.Theta, self.Beta, user, item, self.ncores)\n\t\t\telse:\n\t\t\t\tnon_na_user = user[~nan_entries]\n\t\t\t\tnon_na_item = item[~nan_entries]\n\t\t\t\tout = np.empty(user.shape[0], dtype=self.Theta.dtype)\n\t\t\t\tout[~nan_entries] = cython_loops.predict_arr(self.Theta, self.Beta, non_na_user.astype(cython_loops.obj_ind_type), non_na_item.astype(cython_loops.obj_ind_type), self.ncores)\n\t\t\t\tout[nan_entries] = np.nan\n\t\t\t\treturn out", "response": "predict for a user and item"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef topN(self, user, n=10, exclude_seen=True, items_pool=None):\n\t\tif isinstance(n, float):\n\t\t\tn = int(n)\n\t\tassert isinstance(n ,int)\n\t\tif self.reindex:\n\t\t\tif self.produce_dicts:\n\t\t\t\ttry:\n\t\t\t\t\tuser = self.user_dict_[user]\n\t\t\t\texcept:\n\t\t\t\t\traise ValueError(\"Can only predict for users who were in the training set.\")\n\t\t\telse:\n\t\t\t\tuser = pd.Categorical(np.array([user]), self.user_mapping_).codes[0]\n\t\t\t\tif user == -1:\n\t\t\t\t\traise ValueError(\"Can only predict for users who were in the training set.\")\n\t\tif exclude_seen and not self.keep_data:\n\t\t\traise Exception(\"Can only exclude seen items when passing 'keep_data=True' to .fit\")\n\t\t\t\n\t\tif items_pool is None:\n\t\t\tallpreds = - (self.Theta[user].dot(self.Beta.T))\n\t\t\tif exclude_seen:\n\t\t\t\tn_ext = np.min([n + self._n_seen_by_user[user], self.Beta.shape[0]])\n\t\t\t\trec = np.argpartition(allpreds, n_ext-1)[:n_ext]\n\t\t\t\tseen = self.seen[self._st_ix_user[user] : self._st_ix_user[user] + self._n_seen_by_user[user]]\n\t\t\t\trec = np.setdiff1d(rec, seen)\n\t\t\t\trec = rec[np.argsort(allpreds[rec])[:n]]\n\t\t\t\tif self.reindex:\n\t\t\t\t\treturn self.item_mapping_[rec]\n\t\t\t\telse:\n\t\t\t\t\treturn rec\n\n\t\t\telse:\n\t\t\t\tn = np.min([n, self.Beta.shape[0]])\n\t\t\t\trec = np.argpartition(allpreds, n-1)[:n]\n\t\t\t\trec = rec[np.argsort(allpreds[rec])]\n\t\t\t\tif self.reindex:\n\t\t\t\t\treturn self.item_mapping_[rec]\n\t\t\t\telse:\n\t\t\t\t\treturn rec\n\n\t\telse:\n\t\t\tif isinstance(items_pool, list) or isinstance(items_pool, tuple):\n\t\t\t\titems_pool = np.array(items_pool)\n\t\t\tif items_pool.__class__.__name__=='Series':\n\t\t\t\titems_pool = items_pool.values\n\t\t\tif isinstance(items_pool, np.ndarray):  \n\t\t\t\tif len(items_pool.shape) > 1:\n\t\t\t\t\titems_pool = items_pool.reshape(-1)\n\t\t\t\tif self.reindex:\n\t\t\t\t\titems_pool_reind = pd.Categorical(items_pool, self.item_mapping_).codes\n\t\t\t\t\tnan_ix = (items_pool_reind == -1)\n\t\t\t\t\tif nan_ix.sum() > 0:\n\t\t\t\t\t\titems_pool_reind = items_pool_reind[~nan_ix]\n\t\t\t\t\t\tmsg = \"There were \" + (\"%d\" % int(nan_ix.sum())) + \" entries from 'item_pool'\"\n\t\t\t\t\t\tmsg += \"that were not in the training data and will be exluded.\"\n\t\t\t\t\t\twarnings.warn(msg)\n\t\t\t\t\tdel nan_ix\n\t\t\t\t\tif items_pool_reind.shape[0] == 0:\n\t\t\t\t\t\traise ValueError(\"No items to recommend.\")\n\t\t\t\t\telif items_pool_reind.shape[0] == 1:\n\t\t\t\t\t\traise ValueError(\"Only 1 item to recommend.\")\n\t\t\t\t\telse:\n\t\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise ValueError(\"'items_pool' must be an array.\")\n\n\t\t\tif self.reindex:\n\t\t\t\tallpreds = - self.Theta[user].dot(self.Beta[items_pool_reind].T)\n\t\t\telse:\n\t\t\t\tallpreds = - self.Theta[user].dot(self.Beta[items_pool].T)\n\t\t\tn = np.min([n, items_pool.shape[0]])\n\t\t\tif exclude_seen:\n\t\t\t\tn_ext = np.min([n + self._n_seen_by_user[user], items_pool.shape[0]])\n\t\t\t\trec = np.argpartition(allpreds, n_ext-1)[:n_ext]\n\t\t\t\tseen = self.seen[self._st_ix_user[user] : self._st_ix_user[user] + self._n_seen_by_user[user]]\n\t\t\t\tif self.reindex:\n\t\t\t\t\trec = np.setdiff1d(items_pool_reind[rec], seen)\n\t\t\t\t\tallpreds = - self.Theta[user].dot(self.Beta[rec].T)\n\t\t\t\t\treturn self.item_mapping_[rec[np.argsort(allpreds)[:n]]]\n\t\t\t\telse:\n\t\t\t\t\trec = np.setdiff1d(items_pool[rec], seen)\n\t\t\t\t\tallpreds = - self.Theta[user].dot(self.Beta[rec].T)\n\t\t\t\t\treturn rec[np.argsort(allpreds)[:n]]\n\t\t\telse:\n\t\t\t\trec = np.argpartition(allpreds, n-1)[:n]\n\t\t\t\treturn items_pool[rec[np.argsort(allpreds[rec])]]", "response": "This function returns the top - N items for a user in the training set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates Poisson log - likelihood for a given dataset.", "response": "def eval_llk(self, input_df, full_llk=False):\n\t\t\"\"\"\n\t\tEvaluate Poisson log-likelihood (plus constant) for a given dataset\n\t\t\n\t\tNote\n\t\t----\n\t\tThis Poisson log-likelihood is calculated only for the combinations of users and items\n\t\tprovided here, so it's not a complete likelihood, and it might sometimes turn out to\n\t\tbe a positive number because of this.\n\t\tWill filter out the input data by taking only combinations of users\n\t\tand items that were present in the training set.\n\n\t\tParameters\n\t\t----------\n\t\tinput_df : pandas data frame (nobs, 3)\n\t\t\tInput data on which to calculate log-likelihood, consisting of IDs and counts.\n\t\t\tMust contain one row per non-zero observaion, with columns 'UserId', 'ItemId', 'Count'.\n\t\t\tIf a numpy array is provided, will assume the first 3 columns\n\t\t\tcontain that info.\n\t\tfull_llk : bool\n\t\t\tWhether to calculate terms of the likelihood that depend on the data but not on the\n\t\t\tparameters. Ommitting them is faster, but it's more likely to result in positive values.\n\n\t\tReturns\n\t\t-------\n\t\tllk : dict\n\t\t\tDictionary containing the calculated log-likelihood and the number of\n\t\t\tobservations that were used to calculate it.\n\t\t\"\"\"\n\t\tassert self.is_fitted\n\t\tself._process_valset(input_df, valset=False)\n\t\tself.ncores = cython_loops.cast_int(self.ncores)\n\t\tout = {'llk': cython_loops.calc_llk(self.val_set.Count.values,\n\t\t\t\t\t\t\t\t\t\t\tself.val_set.UserId.values,\n\t\t\t\t\t\t\t\t\t\t\tself.val_set.ItemId.values,\n\t\t\t\t\t\t\t\t\t\t\tself.Theta,\n\t\t\t\t\t\t\t\t\t\t\tself.Beta,\n\t\t\t\t\t\t\t\t\t\t\tself.k,\n\t\t\t\t\t\t\t\t\t\t\tself.ncores,\n\t\t\t\t\t\t\t\t\t\t\tcython_loops.cast_int(bool(full_llk))),\n\t\t\t   'nobs':self.val_set.shape[0]}\n\t\tdel self.val_set\n\t\treturn out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the centre of the Gaussian basisset functions be spaced evenly throughout run time", "response": "def gen_centers(self):\n        \"\"\"Set the centre of the Gaussian basis \n        functions be spaced evenly throughout run time\"\"\"\n\n        '''x_track = self.cs.discrete_rollout()\n        t = np.arange(len(x_track))*self.dt\n        # choose the points in time we'd like centers to be at\n        c_des = np.linspace(0, self.cs.run_time, self.bfs)\n        self.c = np.zeros(len(c_des))\n        for ii, point in enumerate(c_des): \n            diff = abs(t - point)\n            self.c[ii] = x_track[np.where(diff == min(diff))[0][0]]'''\n\n        # desired spacings along x\n        # need to be spaced evenly between 1 and exp(-ax)\n        # lowest number should be only as far as x gets \n        if self.bfs == 2:\n            first = 0.5 # MODIFIED to handle a small number of basis (e.g. 3)\n            last = 0.8\n        elif self.bfs == 3:\n            first = 0.4\n            last = 0.8\n        else:\n            first = 0.2\n            last = 0.8\n        #first = np.exp(-self.cs.ax*self.cs.run_time) \n        #last = 1.05 - first\n        des_c = np.linspace(first,last,self.bfs) \n\n        self.c = np.ones(len(des_c)) \n        for n in range(len(des_c)): \n            # x = exp(-c), solving for c\n            self.c[n] = -np.log(des_c[n])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a set of weights over the basis functions such that the target forcing term trajectory is matched.", "response": "def gen_weights(self, f_target):\n        \"\"\"Generate a set of weights over the basis functions such \n        that the target forcing term trajectory is matched.\n        \n        f_target np.array: the desired forcing term trajectory\n        \"\"\"\n\n        # calculate x and psi   \n        x_track = self.cs.rollout()\n        psi_track = self.gen_psi(x_track)\n\n        #efficiently calculate weights for BFs using weighted linear regression\n        self.w = np.zeros((self.dmps, self.bfs))\n        for d in range(self.dmps):\n            # spatial scaling term\n            k = 1.#(self.goal[d] - self.y0[d])\n            for b in range(self.bfs):\n                numer = np.sum(x_track * psi_track[:,b] * f_target[:,d])\n                denom = np.sum(x_track**2 * psi_track[:,b])\n                self.w[d,b] = numer / (k * denom)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_settings_product(cls, environments, babblings,\n                              interest_models, sensorimotor_models,\n                              evaluate_at,\n                              testcases=None, same_testcases=False):\n        \"\"\" Creates a ExperimentPool with the product of all the given settings.\n\n            :param environments: e.g. [('simple_arm', 'default'), ('simple_arm', 'high_dimensional')]\n            :type environments: list of (environment name, config name)\n            :param babblings: e.g. ['motor', 'goal']\n            :type bablings: list of babbling modes\n            :param interest_models: e.g. [('random', 'default')]\n            :type interest_models: list of (interest model name, config name)\n            :param sensorimotor_models: e.g. [('non_parametric', 'default')]\n            :type sensorimotor_models: list of (sensorimotor model name, config name)\n            :param evaluate_at: indices defining when to evaluate\n            :type evaluate_at: list of int\n            :param bool same_testcases: whether to use the same testcases for all experiments\n\n        \"\"\"\n        l = itertools.product(environments, babblings,\n                              interest_models, sensorimotor_models)\n\n        settings = [make_settings(env, bab, im, sm, env_conf, im_conf, sm_conf)\n                    for ((env, env_conf), bab, (im, im_conf), (sm, sm_conf)) in l]\n\n        return cls(settings, evaluate_at, testcases, same_testcases)", "response": "Creates an ExperimentPool with the product of all the given settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns all experiments using a multiprocessing. Pool.", "response": "def run(self, repeat=1, processes=None, use_thread=False):\n        \"\"\" Runs all experiments using a :py:class:`multiprocessing.Pool`.\n\n            :param int repeat: Number of time each experiment will be repeated.\n            :param int processes: Number of processes launched in parallel (Default: uses all the availabled CPUs)\n            :param bool use_thread: Use a :py:class:`~multiprocessing.pool.ThreadPool` instead of a :py:class:`~multiprocessing.pool.Pool`. By default, tries to guess depending on the environment which one to use.\n\n         \"\"\"\n        mega_config = [c for c in self._config for _ in range(repeat)]\n\n        env = [environments[s.environment][0] for s in self.settings]\n        use_process = array([e.use_process for e in env]).all() and (not use_thread)\n\n        pool = Pool(processes) if use_process else ThreadPool(processes)\n\n        logs = pool.map(_f, mega_config)\n        logs = array(logs).reshape(-1, repeat)\n\n        self._add_logs(logs)\n\n        return logs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logs(self):\n        if not hasattr(self, '_logs'):\n            raise ValueError('You have to run the pool of experiments first!')\n\n        logs = self._logs.reshape(-1) if self._logs.shape[1] == 1 else self._logs\n        return deepcopy(logs)", "response": "Returns the list of logs for this experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef animate_pendulum(self, m,  path=\"anim_npendulum\"):\n\n        if os.path.exists(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n\n        for t in range(0, 1000, 32):\n            ax = axes()\n            self.plot_npendulum(ax, m, t)\n            savefig(os.path.join(path, \"{}.png\".format(t)))\n            clf()", "response": "This function generates few images at different instants in order to animate the pendulum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fmin(objective_function, x0, sigma0,\n         options=None,\n         args=(),\n         gradf=None,\n         restarts=0,\n         restart_from_best='False',\n         incpopsize=2,\n         eval_initial_x=False,\n         noise_handler=None,\n         noise_change_sigma_exponent=1,\n         noise_kappa_exponent=0,  # TODO: add max kappa value as parameter\n         bipop=False):\n    \"\"\"functional interface to the stochastic optimizer CMA-ES\n    for non-convex function minimization.\n\n    Calling Sequences\n    =================\n        ``fmin(objective_function, x0, sigma0)``\n            minimizes `objective_function` starting at `x0` and with standard deviation\n            `sigma0` (step-size)\n        ``fmin(objective_function, x0, sigma0, options={'ftarget': 1e-5})``\n            minimizes `objective_function` up to target function value 1e-5, which\n            is typically useful for benchmarking.\n        ``fmin(objective_function, x0, sigma0, args=('f',))``\n            minimizes `objective_function` called with an additional argument ``'f'``.\n        ``fmin(objective_function, x0, sigma0, options={'ftarget':1e-5, 'popsize':40})``\n            uses additional options ``ftarget`` and ``popsize``\n        ``fmin(objective_function, esobj, None, options={'maxfevals': 1e5})``\n            uses the `CMAEvolutionStrategy` object instance `esobj` to optimize\n            `objective_function`, similar to `esobj.optimize()`.\n\n    Arguments\n    =========\n        `objective_function`\n            function to be minimized. Called as ``objective_function(x,\n            *args)``. `x` is a one-dimensional `numpy.ndarray`.\n            `objective_function` can return `numpy.NaN`,\n            which is interpreted as outright rejection of solution `x`\n            and invokes an immediate resampling and (re-)evaluation\n            of a new solution not counting as function evaluation.\n        `x0`\n            list or `numpy.ndarray`, initial guess of minimum solution\n            before the application of the geno-phenotype transformation\n            according to the ``transformation`` option.  It can also be\n            a string holding a Python expression that is evaluated\n            to yield the initial guess - this is important in case\n            restarts are performed so that they start from different\n            places.  Otherwise `x0` can also be a `cma.CMAEvolutionStrategy`\n            object instance, in that case `sigma0` can be ``None``.\n        `sigma0`\n            scalar, initial standard deviation in each coordinate.\n            `sigma0` should be about 1/4th of the search domain width\n            (where the optimum is to be expected). The variables in\n            `objective_function` should be scaled such that they\n            presumably have similar sensitivity.\n            See also option `scaling_of_variables`.\n        `options`\n            a dictionary with additional options passed to the constructor\n            of class ``CMAEvolutionStrategy``, see ``cma.CMAOptions()``\n            for a list of available options.\n        ``args=()``\n            arguments to be used to call the `objective_function`\n        ``gradf``\n            gradient of f, where ``len(gradf(x, *args)) == len(x)``.\n            `gradf` is called once in each iteration if\n            ``gradf is not None``.\n        ``restarts=0``\n            number of restarts with increasing population size, see also\n            parameter `incpopsize`, implementing the IPOP-CMA-ES restart\n            strategy, see also parameter `bipop`; to restart from\n            different points (recommended), pass `x0` as a string.\n        ``restart_from_best=False``\n            which point to restart from\n        ``incpopsize=2``\n            multiplier for increasing the population size `popsize` before\n            each restart\n        ``eval_initial_x=None``\n            evaluate initial solution, for `None` only with elitist option\n        ``noise_handler=None``\n            a ``NoiseHandler`` instance or ``None``, a simple usecase is\n            ``cma.fmin(f, 6 * [1], 1, noise_handler=cma.NoiseHandler(6))``\n            see ``help(cma.NoiseHandler)``.\n        ``noise_change_sigma_exponent=1``\n            exponent for sigma increment for additional noise treatment\n        ``noise_evaluations_as_kappa``\n            instead of applying reevaluations, the \"number of evaluations\"\n            is (ab)used as scaling factor kappa (experimental).\n        ``bipop``\n            if True, run as BIPOP-CMA-ES; BIPOP is a special restart\n            strategy switching between two population sizings - small\n            (like the default CMA, but with more focused search) and\n            large (progressively increased as in IPOP). This makes the\n            algorithm perform well both on functions with many regularly\n            or irregularly arranged local optima (the latter by frequently\n            restarting with small populations).  For the `bipop` parameter\n            to actually take effect, also select non-zero number of\n            (IPOP) restarts; the recommended setting is ``restarts<=9``\n            and `x0` passed as a string.  Note that small-population\n            restarts do not count into the total restart count.\n\n    Optional Arguments\n    ==================\n    All values in the `options` dictionary are evaluated if they are of\n    type `str`, besides `verb_filenameprefix`, see class `CMAOptions` for\n    details. The full list is available via ``cma.CMAOptions()``.\n\n    >>> import cma\n    >>> cma.CMAOptions()\n\n    Subsets of options can be displayed, for example like\n    ``cma.CMAOptions('tol')``, or ``cma.CMAOptions('bound')``,\n    see also class `CMAOptions`.\n\n    Return\n    ======\n    Return the list provided by `CMAEvolutionStrategy.result()` appended\n    with termination conditions, an `OOOptimizer` and a `BaseDataLogger`::\n\n        res = es.result() + (es.stop(), es, logger)\n\n    where\n        - ``res[0]`` (``xopt``) -- best evaluated solution\n        - ``res[1]`` (``fopt``) -- respective function value\n        - ``res[2]`` (``evalsopt``) -- respective number of function evaluations\n        - ``res[3]`` (``evals``) -- number of overall conducted objective function evaluations\n        - ``res[4]`` (``iterations``) -- number of overall conducted iterations\n        - ``res[5]`` (``xmean``) -- mean of the final sample distribution\n        - ``res[6]`` (``stds``) -- effective stds of the final sample distribution\n        - ``res[-3]`` (``stop``) -- termination condition(s) in a dictionary\n        - ``res[-2]`` (``cmaes``) -- class `CMAEvolutionStrategy` instance\n        - ``res[-1]`` (``logger``) -- class `CMADataLogger` instance\n\n    Details\n    =======\n    This function is an interface to the class `CMAEvolutionStrategy`. The\n    latter class should be used when full control over the iteration loop\n    of the optimizer is desired.\n\n    Examples\n    ========\n    The following example calls `fmin` optimizing the Rosenbrock function\n    in 10-D with initial solution 0.1 and initial step-size 0.5. The\n    options are specified for the usage with the `doctest` module.\n\n    >>> import cma\n    >>> # cma.CMAOptions()  # returns all possible options\n    >>> options = {'CMA_diagonal':100, 'seed':1234, 'verb_time':0}\n    >>>\n    >>> res = cma.fmin(cma.fcts.rosen, [0.1] * 10, 0.5, options)\n    (5_w,10)-CMA-ES (mu_w=3.2,w_1=45%) in dimension 10 (seed=1234)\n       Covariance matrix is diagonal for 10 iterations (1/ccov=29.0)\n    Iterat #Fevals   function value     axis ratio  sigma   minstd maxstd min:sec\n        1      10 1.264232686260072e+02 1.1e+00 4.40e-01  4e-01  4e-01\n        2      20 1.023929748193649e+02 1.1e+00 4.00e-01  4e-01  4e-01\n        3      30 1.214724267489674e+02 1.2e+00 3.70e-01  3e-01  4e-01\n      100    1000 6.366683525319511e+00 6.2e+00 2.49e-02  9e-03  3e-02\n      200    2000 3.347312410388666e+00 1.2e+01 4.52e-02  8e-03  4e-02\n      300    3000 1.027509686232270e+00 1.3e+01 2.85e-02  5e-03  2e-02\n      400    4000 1.279649321170636e-01 2.3e+01 3.53e-02  3e-03  3e-02\n      500    5000 4.302636076186532e-04 4.6e+01 4.78e-03  3e-04  5e-03\n      600    6000 6.943669235595049e-11 5.1e+01 5.41e-06  1e-07  4e-06\n      650    6500 5.557961334063003e-14 5.4e+01 1.88e-07  4e-09  1e-07\n    termination on tolfun : 1e-11\n    final/bestever f-value = 5.55796133406e-14 2.62435631419e-14\n    mean solution:  [ 1.          1.00000001  1.          1.\n        1.          1.00000001  1.00000002  1.00000003 ...]\n    std deviation: [ 3.9193387e-09  3.7792732e-09  4.0062285e-09  4.6605925e-09\n        5.4966188e-09   7.4377745e-09   1.3797207e-08   2.6020765e-08 ...]\n    >>>\n    >>> print('best solutions fitness = %f' % (res[1]))\n    best solutions fitness = 2.62435631419e-14\n    >>> assert res[1] < 1e-12\n\n    The above call is pretty much equivalent with the slightly more\n    verbose call::\n\n        es = cma.CMAEvolutionStrategy([0.1] * 10, 0.5,\n                    options=options).optimize(cma.fcts.rosen)\n\n    The following example calls `fmin` optimizing the Rastrigin function\n    in 3-D with random initial solution in [-2,2], initial step-size 0.5\n    and the BIPOP restart strategy (that progressively increases population).\n    The options are specified for the usage with the `doctest` module.\n\n    >>> import cma\n    >>> # cma.CMAOptions()  # returns all possible options\n    >>> options = {'seed':12345, 'verb_time':0, 'ftarget': 1e-8}\n    >>>\n    >>> res = cma.fmin(cma.fcts.rastrigin, '2. * np.random.rand(3) - 1', 0.5,\n    ...                options, restarts=9, bipop=True)\n    (3_w,7)-aCMA-ES (mu_w=2.3,w_1=58%) in dimension 3 (seed=12345)\n    Iterat #Fevals   function value    axis ratio  sigma  minstd maxstd min:sec\n        1       7 1.633489455763566e+01 1.0e+00 4.35e-01  4e-01  4e-01\n        2      14 9.762462950258016e+00 1.2e+00 4.12e-01  4e-01  4e-01\n        3      21 2.461107851413725e+01 1.4e+00 3.78e-01  3e-01  4e-01\n      100     700 9.949590571272680e-01 1.7e+00 5.07e-05  3e-07  5e-07\n      123     861 9.949590570932969e-01 1.3e+00 3.93e-06  9e-09  1e-08\n    termination on tolfun=1e-11\n    final/bestever f-value = 9.949591e-01 9.949591e-01\n    mean solution: [  9.94958638e-01  -7.19265205e-10   2.09294450e-10]\n    std deviation: [  8.71497860e-09   8.58994807e-09   9.85585654e-09]\n    [...]\n    (4_w,9)-aCMA-ES (mu_w=2.8,w_1=49%) in dimension 3 (seed=12349)\n    Iterat #Fevals   function value    axis ratio  sigma  minstd maxstd min:sec\n        1  5342.0 2.114883315350800e+01 1.0e+00 3.42e-02  3e-02  4e-02\n        2  5351.0 1.810102940125502e+01 1.4e+00 3.79e-02  3e-02  4e-02\n        3  5360.0 1.340222457448063e+01 1.4e+00 4.58e-02  4e-02  6e-02\n       50  5783.0 8.631491965616078e-09 1.6e+00 2.01e-04  8e-06  1e-05\n    termination on ftarget=1e-08 after 4 restarts\n    final/bestever f-value = 8.316963e-09 8.316963e-09\n    mean solution: [ -3.10652459e-06   2.77935436e-06  -4.95444519e-06]\n    std deviation: [  1.02825265e-05   8.08348144e-06   8.47256408e-06]\n\n    In either case, the method::\n\n        cma.plot();\n\n    (based on `matplotlib.pyplot`) produces a plot of the run and, if\n    necessary::\n\n        cma.show()\n\n    shows the plot in a window. Finally::\n\n        cma.savefig('myfirstrun')  # savefig from matplotlib.pyplot\n\n    will save the figure in a png.\n\n    We can use the gradient like\n\n    >>> import cma\n    >>> res = cma.fmin(cma.fcts.rosen, np.zeros(10), 0.1,\n    ...             options = {'ftarget':1e-8,},\n    ...             gradf=cma.fcts.grad_rosen,\n    ...         )\n    >>> assert cma.fcts.rosen(res[0]) < 1e-8\n    >>> assert res[2] < 3600  # 1% are > 3300\n    >>> assert res[3] < 3600  # 1% are > 3300\n\n    :See: `CMAEvolutionStrategy`, `OOOptimizer.optimize(), `plot()`,\n        `CMAOptions`, `scipy.optimize.fmin()`\n\n    \"\"\"  # style guides say there should be the above empty line\n    if 1 < 3:  # try: # pass on KeyboardInterrupt\n        if not objective_function:  # cma.fmin(0, 0, 0)\n            return CMAOptions()  # these opts are by definition valid\n\n        fmin_options = locals().copy()  # archive original options\n        del fmin_options['objective_function']\n        del fmin_options['x0']\n        del fmin_options['sigma0']\n        del fmin_options['options']\n        del fmin_options['args']\n\n        if options is None:\n            options = cma_default_options\n        CMAOptions().check_attributes(options)  # might modify options\n        # checked that no options.ftarget =\n        opts = CMAOptions(options.copy()).complement()\n\n        # BIPOP-related variables:\n        runs_with_small = 0\n        small_i = []\n        large_i = []\n        popsize0 = None  # to be evaluated after the first iteration\n        maxiter0 = None  # to be evaluated after the first iteration\n        base_evals = 0\n\n        irun = 0\n        best = BestSolution()\n        while True:  # restart loop\n            sigma_factor = 1\n\n            # Adjust the population according to BIPOP after a restart.\n            if not bipop:\n                # BIPOP not in use, simply double the previous population\n                # on restart.\n                if irun > 0:\n                    popsize_multiplier = fmin_options['incpopsize'] ** (irun - runs_with_small)\n                    opts['popsize'] = popsize0 * popsize_multiplier\n\n            elif irun == 0:\n                # Initial run is with \"normal\" population size; it is\n                # the large population before first doubling, but its\n                # budget accounting is the same as in case of small\n                # population.\n                poptype = 'small'\n\n            elif sum(small_i) < sum(large_i):\n                # An interweaved run with small population size\n                poptype = 'small'\n                runs_with_small += 1  # _Before_ it's used in popsize_lastlarge\n\n                sigma_factor = 0.01 ** np.random.uniform()  # Local search\n                popsize_multiplier = fmin_options['incpopsize'] ** (irun - runs_with_small)\n                opts['popsize'] = np.floor(popsize0 * popsize_multiplier ** (np.random.uniform() ** 2))\n                opts['maxiter'] = min(maxiter0, 0.5 * sum(large_i) / opts['popsize'])\n                # print('small basemul %s --> %s; maxiter %s' % (popsize_multiplier, opts['popsize'], opts['maxiter']))\n\n            else:\n                # A run with large population size; the population\n                # doubling is implicit with incpopsize.\n                poptype = 'large'\n\n                popsize_multiplier = fmin_options['incpopsize'] ** (irun - runs_with_small)\n                opts['popsize'] = popsize0 * popsize_multiplier\n                opts['maxiter'] = maxiter0\n                # print('large basemul %s --> %s; maxiter %s' % (popsize_multiplier, opts['popsize'], opts['maxiter']))\n\n            # recover from a CMA object\n            if irun == 0 and isinstance(x0, CMAEvolutionStrategy):\n                es = x0\n                x0 = es.inputargs['x0']  # for the next restarts\n                if isscalar(sigma0) and isfinite(sigma0) and sigma0 > 0:\n                    es.sigma = sigma0\n                # debatable whether this makes sense:\n                sigma0 = es.inputargs['sigma0']  # for the next restarts\n                if options is not None:\n                    es.opts.set(options)\n                # ignore further input args and keep original options\n            else:  # default case\n                if irun and eval(str(fmin_options['restart_from_best'])):\n                    print_warning('CAVE: restart_from_best is often not useful',\n                                  verbose=opts['verbose'])\n                    es = CMAEvolutionStrategy(best.x, sigma_factor * sigma0, opts)\n                else:\n                    es = CMAEvolutionStrategy(x0, sigma_factor * sigma0, opts)\n                if eval_initial_x or es.opts['CMA_elitist'] == 'initial' \\\n                   or (es.opts['CMA_elitist'] and eval_initial_x is None):\n                    x = es.gp.pheno(es.mean,\n                                    into_bounds=es.boundary_handler.repair,\n                                    archive=es.sent_solutions)\n                    es.best.update([x], es.sent_solutions,\n                                   [objective_function(x, *args)], 1)\n                    es.countevals += 1\n\n            opts = es.opts  # processed options, unambiguous\n            # a hack:\n            fmin_opts = CMAOptions(fmin_options.copy(), unchecked=True)\n            for k in fmin_opts:\n                # locals() cannot be modified directly, exec won't work\n                # in 3.x, therefore\n                fmin_opts.eval(k, loc={'N': es.N,\n                                       'popsize': opts['popsize']},\n                               correct_key=False)\n\n            append = opts['verb_append'] or es.countiter > 0 or irun > 0\n            # es.logger is \"the same\" logger, because the \"identity\"\n            # is only determined by the `filenameprefix`\n            logger = CMADataLogger(opts['verb_filenameprefix'],\n                                   opts['verb_log'])\n            logger.register(es, append).add()  # no fitness values here\n            es.logger = logger\n\n            if noise_handler:\n                noisehandler = noise_handler\n                noise_handling = True\n                if fmin_opts['noise_change_sigma_exponent'] > 0:\n                    es.opts['tolfacupx'] = inf\n            else:\n                noisehandler = NoiseHandler(es.N, 0)\n                noise_handling = False\n            es.noise_handler = noisehandler\n\n            # the problem: this assumes that good solutions cannot take longer than bad ones:\n            # with EvalInParallel(objective_function, 2, is_feasible=opts['is_feasible']) as eval_in_parallel:\n            if 1 < 3:\n                while not es.stop():  # iteration loop\n                    # X, fit = eval_in_parallel(lambda: es.ask(1)[0], es.popsize, args, repetitions=noisehandler.evaluations-1)\n                    X, fit = es.ask_and_eval(objective_function, args, gradf=gradf,\n                                             evaluations=noisehandler.evaluations,\n                                             aggregation=np.median)  # treats NaN with resampling\n                    # TODO: check args and in case use args=(noisehandler.evaluations, )\n\n                    es.tell(X, fit)  # prepare for next iteration\n                    if noise_handling:  # it would be better to also use these f-evaluations in tell\n                        es.sigma *= noisehandler(X, fit, objective_function, es.ask,\n                                                 args=args)**fmin_opts['noise_change_sigma_exponent']\n\n                        es.countevals += noisehandler.evaluations_just_done  # TODO: this is a hack, not important though\n                        # es.more_to_write.append(noisehandler.evaluations_just_done)\n                        if noisehandler.maxevals > noisehandler.minevals:\n                            es.more_to_write.append(noisehandler.get_evaluations())\n                        if 1 < 3:\n                            es.sp.cmean *= exp(-noise_kappa_exponent * np.tanh(noisehandler.noiseS))\n                            if es.sp.cmean > 1:\n                                es.sp.cmean = 1\n\n                    es.disp()\n                    logger.add(# more_data=[noisehandler.evaluations, 10**noisehandler.noiseS] if noise_handling else [],\n                               modulo=1 if es.stop() and logger.modulo else None)\n                    if (opts['verb_log'] and opts['verb_plot'] and\n                          (es.countiter % max(opts['verb_plot'], opts['verb_log']) == 0 or es.stop())):\n                        logger.plot(324)\n\n            # end while not es.stop\n            mean_pheno = es.gp.pheno(es.mean, into_bounds=es.boundary_handler.repair, archive=es.sent_solutions)\n            fmean = objective_function(mean_pheno, *args)\n            es.countevals += 1\n\n            es.best.update([mean_pheno], es.sent_solutions, [fmean], es.countevals)\n            best.update(es.best, es.sent_solutions)  # in restarted case\n            # es.best.update(best)\n\n            this_evals = es.countevals - base_evals\n            base_evals = es.countevals\n\n            # BIPOP stats update\n\n            if irun == 0:\n                popsize0 = opts['popsize']\n                maxiter0 = opts['maxiter']\n                # XXX: This might be a bug? Reproduced from Matlab\n                # small_i.append(this_evals)\n\n            if bipop:\n                if poptype == 'small':\n                    small_i.append(this_evals)\n                else:  # poptype == 'large'\n                    large_i.append(this_evals)\n\n            # final message\n            if opts['verb_disp']:\n                es.result_pretty(irun, time.asctime(time.localtime()),\n                                 best.f)\n\n            irun += 1\n            # if irun > fmin_opts['restarts'] or 'ftarget' in es.stop() \\\n            # if irun > restarts or 'ftarget' in es.stop() \\\n            if irun - runs_with_small > fmin_opts['restarts'] or 'ftarget' in es.stop() \\\n                    or 'maxfevals' in es.stop(check=False):\n                break\n            opts['verb_append'] = es.countevals\n            opts['popsize'] = fmin_opts['incpopsize'] * es.sp.popsize  # TODO: use rather options?\n            opts['seed'] += 1\n\n        # while irun\n\n        # es.out['best'] = best  # TODO: this is a rather suboptimal type for inspection in the shell\n        if 1 < 3:\n            if irun:\n                es.best.update(best)\n                # TODO: there should be a better way to communicate the overall best\n            return es.result() + (es.stop(), es, logger)\n\n        else:  # previously: to be removed\n            return (best.x.copy(), best.f, es.countevals,\n                    dict((('stopdict', _CMAStopDict(es._stopdict))\n                          , ('mean', es.gp.pheno(es.mean))\n                          , ('std', es.sigma * es.sigma_vec * sqrt(es.dC) * es.gp.scales)\n                          , ('out', es.out)\n                          , ('opts', es.opts)  # last state of options\n                          , ('cma', es)\n                          , ('inputargs', es.inputargs)\n                          ))\n                   )\n        # TODO refine output, can #args be flexible?\n        # is this well usable as it is now?\n    else:  # except KeyboardInterrupt:  # Exception, e:\n        if eval(str(options['verb_disp'])) > 0:\n            print(' in/outcomment ``raise`` in last line of cma.fmin to prevent/restore KeyboardInterrupt exception')\n        raise", "response": "Functional interface to the non - convex function minimization CMA - ES\n            model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(name=None, fig=None, abscissa=1, iteridx=None,\n         plot_mean=False,\n         foffset=1e-19, x_opt=None, fontsize=9):\n    \"\"\"\n    plot data from files written by a `CMADataLogger`,\n    the call ``cma.plot(name, **argsdict)`` is a shortcut for\n    ``cma.CMADataLogger(name).plot(**argsdict)``\n\n    Arguments\n    ---------\n        `name`\n            name of the logger, filename prefix, None evaluates to\n            the default 'outcmaes'\n        `fig`\n            filename or figure number, or both as a tuple (any order)\n        `abscissa`\n            0==plot versus iteration count,\n            1==plot versus function evaluation number\n        `iteridx`\n            iteration indices to plot\n\n    Return `None`\n\n    Examples\n    --------\n    ::\n\n       cma.plot();  # the optimization might be still\n                    # running in a different shell\n       cma.savefig('fig325.png')\n       cma.closefig()\n\n       cdl = cma.CMADataLogger().downsampling().plot()\n       # in case the file sizes are large\n\n    Details\n    -------\n    Data from codes in other languages (C, Java, Matlab, Scilab) have the same\n    format and can be plotted just the same.\n\n    :See: `CMADataLogger`, `CMADataLogger.plot()`\n\n    \"\"\"\n    global last_figure_number\n    if not fig:\n        last_figure_number += 1\n        fig = last_figure_number\n    if isinstance(fig, (int, float)):\n        last_figure_number = fig\n    CMADataLogger(name).plot(fig, abscissa, iteridx, plot_mean, foffset,\n                             x_opt, fontsize)", "response": "This function plots the data from files written by a CMADataLogger and saves the figure to disk and returns None."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay selected data from files written by the class CMADataLogger.", "response": "def disp(name=None, idx=None):\n    \"\"\"displays selected data from (files written by) the class `CMADataLogger`.\n\n    The call ``cma.disp(name, idx)`` is a shortcut for ``cma.CMADataLogger(name).disp(idx)``.\n\n    Arguments\n    ---------\n        `name`\n            name of the logger, filename prefix, `None` evaluates to\n            the default ``'outcmaes'``\n        `idx`\n            indices corresponding to rows in the data file; by\n            default the first five, then every 100-th, and the last\n            10 rows. Too large index values are removed.\n\n    Examples\n    --------\n    ::\n\n       import cma, numpy\n       # assume some data are available from previous runs\n       cma.disp(None,numpy.r_[0,-1])  # first and last\n       cma.disp(None,numpy.r_[0:1e9:100,-1]) # every 100-th and last\n       cma.disp(idx=numpy.r_[0,-10:0]) # first and ten last\n       cma.disp(idx=numpy.r_[0:1e9:1e3,-10:0])\n\n    :See: `CMADataLogger.disp()`\n\n    \"\"\"\n    return CMADataLogger(name if name else CMADataLogger.default_prefix\n                         ).disp(idx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef felli(x):\n    return sum(1e6**(np.arange(len(x)) / (len(x) - 1)) * (np.array(x, copy=False))**2)", "response": "unbound test function needed to test multiprocessor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(argv=None):\n    if argv is None:\n        argv = sys.argv  # should have better been sys.argv[1:]\n    else:\n        if isinstance(argv, list):\n            argv = ['python'] + argv  # see above\n        else:\n            argv = ['python'] + [argv]\n\n    # uncomment for unit test\n    # _test()\n    # handle input arguments, getopt might be helpful ;-)\n    if len(argv) >= 1:  # function and help\n        if len(argv) == 1 or argv[1].startswith('-h') or argv[1].startswith('--help'):\n            print(main.__doc__)\n            fun = None\n        elif argv[1].startswith('-t') or argv[1].startswith('--test'):\n            import doctest\n            if len(argv) > 2 and (argv[2].startswith('--v') or argv[2].startswith('-v')):  # verbose\n                print('doctest for cma.py: due to different platforms and python versions')\n                print('and in some cases due to a missing unique random seed')\n                print('many examples will \"fail\". This is OK, if they give a similar')\n                print('to the expected result and if no exception occurs. ')\n                # if argv[1][2] == 'v':\n                doctest.testmod(sys.modules[__name__], report=True)  # this is quite cool!\n            else:  # was: if len(argv) > 2 and (argv[2].startswith('--qu') or argv[2].startswith('-q')):\n                print('doctest for cma.py: launching...') # not anymore: (it might be necessary to close the pop up window to finish)\n                fn = '_cma_doctest_.txt'\n                stdout = sys.stdout\n                try:\n                    with open(fn, 'w') as f:\n                        sys.stdout = f\n                        clock = ElapsedTime()\n                        doctest.testmod(sys.modules[__name__], report=True)  # this is quite cool!\n                        t_elapsed = clock()\n                finally:\n                    sys.stdout = stdout\n                process_doctest_output(fn)\n                # clean up\n                try:\n                    import os\n                    for name in os.listdir('.'):\n                        if (name.startswith('bound_method_FitnessFunctions.rosen_of_cma.FitnessFunctions_object_at_')\n                            and name.endswith('.pkl')):\n                            os.remove(name)\n                except:\n                    pass\n                print('doctest for cma.py: finished (no other output should be seen after launching, more in file _cma_doctest_.txt)')\n                print('  elapsed time [s]:', t_elapsed)\n            return\n        elif argv[1] == '--doc':\n            print(__doc__)\n            print(CMAEvolutionStrategy.__doc__)\n            print(fmin.__doc__)\n            fun = None\n        elif argv[1] == '--fcts':\n            print('List of valid function names:')\n            print([d for d in dir(fcts) if not d.startswith('_')])\n            fun = None\n        elif argv[1] in ('install', '--install'):\n            from distutils.core import setup\n            setup(name=\"cma\",\n                  long_description=__doc__,\n                  version=__version__.split()[0],\n                  description=\"CMA-ES, Covariance Matrix Adaptation Evolution Strategy for non-linear numerical optimization in Python\",\n                  author=\"Nikolaus Hansen\",\n                  author_email=\"hansen at lri.fr\",\n                  maintainer=\"Nikolaus Hansen\",\n                  maintainer_email=\"hansen at lri.fr\",\n                  url=\"https://www.lri.fr/~hansen/cmaes_inmatlab.html#python\",\n                  license=\"BSD\",\n                  classifiers = [\n                    \"Intended Audience :: Science/Research\",\n                    \"Intended Audience :: Education\",\n                    \"Intended Audience :: Other Audience\",\n                    \"Topic :: Scientific/Engineering\",\n                    \"Topic :: Scientific/Engineering :: Mathematics\",\n                    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n                    \"Operating System :: OS Independent\",\n                    \"Programming Language :: Python :: 2.6\",\n                    \"Programming Language :: Python :: 2.7\",\n                    \"Programming Language :: Python :: 3\",\n                    \"Development Status :: 4 - Beta\",\n                    \"Environment :: Console\",\n                    \"License :: OSI Approved :: BSD License\",\n                    # \"License :: OSI Approved :: MIT License\",\n                  ],\n                  keywords=[\"optimization\", \"CMA-ES\", \"cmaes\"],\n                  py_modules=[\"cma\"],\n                  requires=[\"numpy\"],\n            )\n            fun = None\n        elif argv[1] in ('plot',):\n            plot(name=argv[2] if len(argv) > 2 else None)\n            raw_input('press return')\n            fun = None\n        elif len(argv) > 3:\n            fun = eval('fcts.' + argv[1])\n        else:\n            print('try -h option')\n            fun = None\n\n    if fun is not None:\n\n        if len(argv) > 2:  # dimension\n            x0 = np.ones(eval(argv[2]))\n        if len(argv) > 3:  # sigma\n            sig0 = eval(argv[3])\n\n        opts = {}\n        for i in xrange(5, len(argv), 2):\n            opts[argv[i - 1]] = eval(argv[i])\n\n        # run fmin\n        if fun is not None:\n            tic = time.time()\n            fmin(fun, x0, sig0, opts)  # ftarget=1e-9, tolfacupx=1e9, verb_log=10)\n            # plot()\n            # print ' best function value ', res[2]['es'].best[1]\n            print('elapsed time [s]: + %.2f', round(time.time() - tic, 2))\n\n    elif not len(argv):\n        fmin(fcts.elli, np.ones(6) * 0.1, 0.1, {'ftarget':1e-9})", "response": "main function for the main function of the cma. py"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for better solutions in list arx based on the smallest corresponding value in arf and update self. x and self. f.", "response": "def update(self, arx, xarchive=None, arf=None, evals=None):\n        \"\"\"checks for better solutions in list `arx`.\n\n        Based on the smallest corresponding value in `arf`,\n        alternatively, `update` may be called with a `BestSolution`\n        instance like ``update(another_best_solution)`` in which case\n        the better solution becomes the current best.\n\n        `xarchive` is used to retrieve the genotype of a solution.\n\n        \"\"\"\n        if isinstance(arx, BestSolution):\n            if self.evalsall is None:\n                self.evalsall = arx.evalsall\n            elif arx.evalsall is not None:\n                self.evalsall = max((self.evalsall, arx.evalsall))\n            if arx.f is not None and arx.f < np.inf:\n                self.update([arx.x], xarchive, [arx.f], arx.evals)\n            return self\n        assert arf is not None\n        # find failsave minimum\n        minidx = np.nanargmin(arf)\n        if minidx is np.nan:\n            return\n        minarf = arf[minidx]\n        # minarf = reduce(lambda x, y: y if y and y is not np.nan\n        #                   and y < x else x, arf, np.inf)\n        if minarf < np.inf and (minarf < self.f or self.f is None):\n            self.x, self.f = arx[minidx], arf[minidx]\n            if xarchive is not None and xarchive.get(self.x) is not None:\n                self.x_geno = xarchive[self.x].get('geno')\n            else:\n                self.x_geno = None\n            self.evals = None if not evals else evals - len(arf) + minidx + 1\n            self.evalsall = evals\n        elif evals:\n            self.evalsall = evals\n        self.last.x = arx[minidx]\n        self.last.f = minarf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrepair the values of x according to the domain bound.", "response": "def repair(self, x, copy_if_changed=True, copy_always=False):\n        \"\"\"projects infeasible values on the domain bound, might be\n        overwritten by derived class \"\"\"\n        if copy_always:\n            x = array(x, copy=True)\n            copy = False\n        else:\n            copy = copy_if_changed\n        if self.bounds is None:\n            return x\n        for ib in [0, 1]:\n            if self.bounds[ib] is None:\n                continue\n            for i in rglen(x):\n                idx = min([i, len(self.bounds[ib]) - 1])\n                if self.bounds[ib][idx] is not None and \\\n                        (-1)**ib * x[i] < (-1)**ib * self.bounds[ib][idx]:\n                    if copy:\n                        x = array(x, copy=True)\n                        copy = False\n                    x[i] = self.bounds[ib][idx]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the bounds of a single entry.", "response": "def _get_bounds(self, ib, dimension):\n        \"\"\"ib == 0/1 means lower/upper bound, return a vector of length\n        `dimension` \"\"\"\n        sign_ = 2 * ib - 1\n        assert sign_**2 == 1\n        if self.bounds is None or self.bounds[ib] is None:\n            return array(dimension * [sign_ * np.Inf])\n        res = []\n        for i in xrange(dimension):\n            res.append(self.bounds[ib][min([i, len(self.bounds[ib]) - 1])])\n            if res[-1] is None:\n                res[-1] = sign_ * np.Inf\n        return array(res)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if any variable is bounded", "response": "def has_bounds(self):\n        \"\"\"return True, if any variable is bounded\"\"\"\n        bounds = self.bounds\n        if bounds in (None, [None, None]):\n            return False\n        for ib, bound in enumerate(bounds):\n            if bound is not None:\n                sign_ = 2 * ib - 1\n                for bound_i in bound:\n                    if bound_i is not None and sign_ * bound_i < np.inf:\n                        return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_in_bounds(self, x):\n        if self.bounds is None:\n            return True\n        for ib in [0, 1]:\n            if self.bounds[ib] is None:\n                continue\n            for i in rglen(x):\n                idx = min([i, len(self.bounds[ib]) - 1])\n                if self.bounds[ib][idx] is not None and \\\n                        (-1)**ib * x[i] < (-1)**ib * self.bounds[ib][idx]:\n                    return False\n        return True", "response": "return True if the set of items is in the bounds of the set of items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the two - dimensional array of the loci in the same format as the original.", "response": "def to_dim_times_two(self, bounds):\n        \"\"\"return boundaries in format ``[[lb0, ub0], [lb1, ub1], ...]``,\n        as used by ``BoxConstraints...`` class.\n\n        \"\"\"\n        if not bounds:\n            b = [[None, None]]\n        else:\n            l = [None, None]  # figure out lenths\n            for i in [0, 1]:\n                try:\n                    l[i] = len(bounds[i])\n                except TypeError:\n                    bounds[i] = [bounds[i]]\n                    l[i] = 1\n            b = []  # bounds in different format\n            try:\n                for i in xrange(max(l)):\n                    b.append([bounds[0][i] if i < l[0] else None,\n                              bounds[1][i] if i < l[1] else None])\n            except (TypeError, IndexError):\n                print(\"boundaries must be provided in the form \" +\n                      \"[scalar_of_vector, scalar_or_vector]\")\n                raise\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrepairs the values of x into the bounded domain.", "response": "def repair(self, x, copy_if_changed=True, copy_always=False):\n        \"\"\"transforms ``x`` into the bounded domain.\n\n        ``copy_always`` option might disappear.\n\n        \"\"\"\n        copy = copy_if_changed\n        if copy_always:\n            x = array(x, copy=True)\n            copy = False\n        if self.bounds is None or (self.bounds[0] is None and\n                                   self.bounds[1] is None):\n            return x\n        return self.bounds_tf(x, copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef repair(self, x, copy_if_changed=True, copy_always=False):\n        # TODO (old data): CPU(N,lam,iter=20,200,100): 3.3s of 8s for two bounds, 1.8s of 6.5s for one bound\n        # remark: np.max([bounds[0], x]) is about 40 times slower than max((bounds[0], x))\n        copy = copy_if_changed\n        if copy_always:\n            x = array(x, copy=True)\n        bounds = self.bounds\n        if bounds not in (None, [None, None], (None, None)):  # solely for effiency\n            x = array(x, copy=True) if copy and not copy_always else x\n            if bounds[0] is not None:\n                if isscalar(bounds[0]):\n                    for i in rglen(x):\n                        x[i] = max((bounds[0], x[i]))\n                else:\n                    for i in rglen(x):\n                        j = min([i, len(bounds[0]) - 1])\n                        if bounds[0][j] is not None:\n                            x[i] = max((bounds[0][j], x[i]))\n            if bounds[1] is not None:\n                if isscalar(bounds[1]):\n                    for i in rglen(x):\n                        x[i] = min((bounds[1], x[i]))\n                else:\n                    for i in rglen(x):\n                        j = min((i, len(bounds[1]) - 1))\n                        if bounds[1][j] is not None:\n                            x[i] = min((bounds[1][j], x[i]))\n        return x", "response": "repair the out - of - bounds components of x on the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef feasible_ratio(self, solutions):\n        raise NotImplementedError('Solution class disappeared')\n        count = np.zeros(len(solutions[0]))\n        for x in solutions:\n            count += x.unrepaired == x\n        return count / float(len(solutions))", "response": "counts for each coordinate the number of feasible values in\n        and returns an array of length len ( solutions ) with the ratios."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, function_values, es):\n        if self.bounds is None or (self.bounds[0] is None and\n                                   self.bounds[1] is None):\n            return self\n\n        N = es.N\n        # ## prepare\n        # compute varis = sigma**2 * C_ii\n        varis = es.sigma**2 * array(N * [es.C] if isscalar(es.C) else (# scalar case\n                                es.C if isscalar(es.C[0]) else  # diagonal matrix case\n                                [es.C[i][i] for i in xrange(N)]))  # full matrix case\n\n        # relative violation in geno-space\n        dmean = (es.mean - es.gp.geno(self.repair(es.gp.pheno(es.mean)))) / varis**0.5\n\n        # ## Store/update a history of delta fitness value\n        fvals = sorted(function_values)\n        l = 1 + len(fvals)\n        val = fvals[3 * l // 4] - fvals[l // 4]  # exact interquartile range apart interpolation\n        val = val / np.mean(varis)  # new: val is normalized with sigma of the same iteration\n        # insert val in history\n        if isfinite(val) and val > 0:\n            self.hist.insert(0, val)\n        elif val == inf and len(self.hist) > 1:\n            self.hist.insert(0, max(self.hist))\n        else:\n            pass  # ignore 0 or nan values\n        if len(self.hist) > 20 + (3 * N) / es.popsize:\n            self.hist.pop()\n\n        # ## prepare\n        dfit = np.median(self.hist)  # median interquartile range\n        damp = min(1, es.sp.mueff / 10. / N)\n\n        # ## set/update weights\n        # Throw initialization error\n        if len(self.hist) == 0:\n            raise _Error('wrongful initialization, no feasible solution sampled. ' +\n                'Reasons can be mistakenly set bounds (lower bound not smaller than upper bound) or a too large initial sigma0 or... ' +\n                'See description of argument func in help(cma.fmin) or an example handling infeasible solutions in help(cma.CMAEvolutionStrategy). ')\n        # initialize weights\n        if dmean.any() and (not self.weights_initialized or es.countiter == 2):  # TODO\n            self.gamma = array(N * [2 * dfit])  ## BUGBUGzzzz: N should be phenotypic (bounds are in phenotype), but is genotypic\n            self.weights_initialized = True\n        # update weights gamma\n        if self.weights_initialized:\n            edist = array(abs(dmean) - 3 * max(1, N**0.5 / es.sp.mueff))\n            if 1 < 3:  # this is better, around a factor of two\n                # increase single weights possibly with a faster rate than they can decrease\n                #     value unit of edst is std dev, 3==random walk of 9 steps\n                self.gamma *= exp((edist > 0) * np.tanh(edist / 3) / 2.)**damp\n                # decrease all weights up to the same level to avoid single extremely small weights\n                #    use a constant factor for pseudo-keeping invariance\n                self.gamma[self.gamma > 5 * dfit] *= exp(-1. / 3)**damp\n                #     self.gamma[idx] *= exp(5*dfit/self.gamma[idx] - 1)**(damp/3)\n        es.more_to_write += list(self.gamma) if self.weights_initialized else N * [1.0]\n        # ## return penalty\n        # es.more_to_write = self.gamma if not isscalar(self.gamma) else N*[1]\n        return self", "response": "Updates the internal state of the internal state of the attribute object instance instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing in base class", "response": "def initialize(self):\n        \"\"\"initialize in base class\"\"\"\n        self._lb = [b[0] for b in self.bounds]  # can be done more efficiently?\n        self._ub = [b[1] for b in self.bounds]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the internal state of the object", "response": "def initialize(self, length=None):\n        \"\"\"see ``__init__``\"\"\"\n        if length is None:\n            length = len(self.bounds)\n        max_i = min((len(self.bounds) - 1, length - 1))\n        self._lb = array([self.bounds[min((i, max_i))][0]\n                          if self.bounds[min((i, max_i))][0] is not None\n                          else -np.Inf\n                          for i in xrange(length)], copy=False)\n        self._ub = array([self.bounds[min((i, max_i))][1]\n                          if self.bounds[min((i, max_i))][1] is not None\n                          else np.Inf\n                          for i in xrange(length)], copy=False)\n        lb = self._lb\n        ub = self._ub\n        # define added values for lower and upper bound\n        self._al = array([min([(ub[i] - lb[i]) / 2, (1 + np.abs(lb[i])) / 20])\n                             if isfinite(lb[i]) else 1 for i in rglen(lb)], copy=False)\n        self._au = array([min([(ub[i] - lb[i]) / 2, (1 + np.abs(ub[i])) / 20])\n                             if isfinite(ub[i]) else 1 for i in rglen(ub)], copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef idx_infeasible(self, solution_genotype):\n        res = [i for i, x in enumerate(solution_genotype)\n                                if not self.is_feasible_i(x, i)]\n        return res", "response": "return indices of infeasible variables that do not directly map into the feasible domain such that the corresponding variable is infeasible."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_feasible_i(self, x, i):\n        lb = self._lb[self._index(i)]\n        ub = self._ub[self._index(i)]\n        al = self._al[self._index(i)]\n        au = self._au[self._index(i)]\n        return lb - al < x < ub + au", "response": "return True if value x is in the invertible domain of\n        variable i"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshifts or mirroring into an inverted domain.", "response": "def shift_or_mirror_into_invertible_domain(self, solution_genotype,\n                                               copy=False):\n        \"\"\"Details: input ``solution_genotype`` is changed. The domain is\n        [lb - al, ub + au] and in [lb - 2*al - (ub - lb) / 2, lb - al]\n        mirroring is applied.\n\n        \"\"\"\n        assert solution_genotype is not None\n        if copy:\n            y = [val for val in solution_genotype]\n        else:\n            y = solution_genotype\n        if isinstance(y, np.ndarray) and not isinstance(y[0], float):\n            y = array(y, dtype=float)\n        for i in rglen(y):\n            lb = self._lb[self._index(i)]\n            ub = self._ub[self._index(i)]\n            al = self._al[self._index(i)]\n            au = self._au[self._index(i)]\n            # x is far from the boundary, compared to ub - lb\n            if y[i] < lb - 2 * al - (ub - lb) / 2.0 or y[i] > ub + 2 * au + (ub - lb) / 2.0:\n                r = 2 * (ub - lb + al + au)  # period\n                s = lb - 2 * al - (ub - lb) / 2.0  # start\n                y[i] -= r * ((y[i] - s) // r)  # shift\n            if y[i] > ub + au:\n                y[i] -= 2 * (y[i] - ub - au)\n            if y[i] < lb - al:\n                y[i] += 2 * (lb - al - y[i])\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshift into the invertible domain [ lb ub au", "response": "def _shift_or_mirror_into_invertible_i(self, x, i):\n        \"\"\"shift into the invertible domain [lb - ab, ub + au], mirror close to\n        boundaries in order to get a smooth transformation everywhere\n\n        \"\"\"\n        assert x is not None\n        lb = self._lb[self._index(i)]\n        ub = self._ub[self._index(i)]\n        al = self._al[self._index(i)]\n        au = self._au[self._index(i)]\n        # x is far from the boundary, compared to ub - lb\n        if x < lb - 2 * al - (ub - lb) / 2.0 or x > ub + 2 * au + (ub - lb) / 2.0:\n            r = 2 * (ub - lb + al + au)  # period\n            s = lb - 2 * al - (ub - lb) / 2.0  # start\n            x -= r * ((x - s) // r)  # shift\n        if x > ub + au:\n            x -= 2 * (x - ub - au)\n        if x < lb - al:\n            x += 2 * (lb - al - x)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn transform of x in component i", "response": "def _transform_i(self, x, i):\n        \"\"\"return transform of x in component i\"\"\"\n        x = self._shift_or_mirror_into_invertible_i(x, i)\n        lb = self._lb[self._index(i)]\n        ub = self._ub[self._index(i)]\n        al = self._al[self._index(i)]\n        au = self._au[self._index(i)]\n        if x < lb + al:\n            return lb + (x - (lb - al))**2 / 4 / al\n        elif x < ub - au:\n            return x\n        elif x < ub + 3 * au:\n            return ub - (x - (ub + au))**2 / 4 / au\n        else:\n            assert False  # shift removes this case\n            return ub + au - (x - (ub + au))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns inverse of y in component i", "response": "def _inverse_i(self, y, i):\n        \"\"\"return inverse of y in component i\"\"\"\n        lb = self._lb[self._index(i)]\n        ub = self._ub[self._index(i)]\n        al = self._al[self._index(i)]\n        au = self._au[self._index(i)]\n        if 1 < 3:\n            if not lb <= y <= ub:\n                raise ValueError('argument of inverse must be within the given bounds')\n        if y < lb + al:\n            return (lb - al) + 2 * (al * (y - lb))**0.5\n        elif y < ub - au:\n            return y\n        else:\n            return (ub + au) - 2 * (au * (ub - y))**0.5"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap the genotypic input argument into the phenotypic space.", "response": "def pheno(self, x, into_bounds=None, copy=True, copy_always=False,\n              archive=None, iteration=None):\n        \"\"\"maps the genotypic input argument into the phenotypic space,\n        see help for class `GenoPheno`\n\n        Details\n        -------\n        If ``copy``, values from ``x`` are copied if changed under the transformation.\n\n        \"\"\"\n        # TODO: copy_always seems superfluous, as it could be done in the calling code\n        input_type = type(x)\n        if into_bounds is None:\n            into_bounds = (lambda x, copy=False:\n                                x if not copy else array(x, copy=copy))\n        if copy_always and not copy:\n            raise ValueError('arguments copy_always=' + str(copy_always) +\n                             ' and copy=' + str(copy) + ' have inconsistent values')\n        if copy_always:\n            x = array(x, copy=True)\n            copy = False\n\n        if self.isidentity:\n            y = into_bounds(x) # was into_bounds(x, False) before (bug before v0.96.22)\n        else:\n            if self.fixed_values is None:\n                y = array(x, copy=copy)  # make a copy, in case\n            else:  # expand with fixed values\n                y = list(x)  # is a copy\n                for i in sorted(self.fixed_values.keys()):\n                    y.insert(i, self.fixed_values[i])\n                y = array(y, copy=False)\n            copy = False\n\n            if self.scales is not 1:  # just for efficiency\n                y *= self.scales\n\n            if self.typical_x is not 0:\n                y += self.typical_x\n\n            if self.tf_pheno is not None:\n                y = array(self.tf_pheno(y), copy=False)\n\n            y = into_bounds(y, copy)  # copy is False\n\n            if self.fixed_values is not None:\n                for i, k in list(self.fixed_values.items()):\n                    y[i] = k\n\n        if input_type is np.ndarray:\n            y = array(y, copy=False)\n        if archive is not None:\n            archive.insert(y, geno=x, iteration=iteration)\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding minimizer of objective_fct", "response": "def optimize(self, objective_fct, iterations=None, min_iterations=1,\n                 args=(), verb_disp=None, logger=None, call_back=None):\n        \"\"\"find minimizer of `objective_fct`.\n\n        CAVEAT: the return value for `optimize` has changed to ``self``.\n\n        Arguments\n        ---------\n\n            `objective_fct`\n                function be to minimized\n            `iterations`\n                number of (maximal) iterations, while ``not self.stop()``\n            `min_iterations`\n                minimal number of iterations, even if ``not self.stop()``\n            `args`\n                arguments passed to `objective_fct`\n            `verb_disp`\n                print to screen every `verb_disp` iteration, if ``None``\n                the value from ``self.logger`` is \"inherited\", if\n                available.\n            ``logger``\n                a `BaseDataLogger` instance, which must be compatible\n                with the type of ``self``.\n            ``call_back``\n                call back function called like ``call_back(self)`` or\n                a list of call back functions.\n\n        ``return self``, that is, the `OOOptimizer` instance.\n\n        Example\n        -------\n        >>> import cma\n        >>> es = cma.CMAEvolutionStrategy(7 * [0.1], 0.5\n        ...              ).optimize(cma.fcts.rosen, verb_disp=100)\n        (4_w,9)-CMA-ES (mu_w=2.8,w_1=49%) in dimension 7 (seed=630721393)\n        Iterat #Fevals   function value    axis ratio  sigma   minstd maxstd min:sec\n            1       9 3.163954777181882e+01 1.0e+00 4.12e-01  4e-01  4e-01 0:0.0\n            2      18 3.299006223906629e+01 1.0e+00 3.60e-01  3e-01  4e-01 0:0.0\n            3      27 1.389129389866704e+01 1.1e+00 3.18e-01  3e-01  3e-01 0:0.0\n          100     900 2.494847340045985e+00 8.6e+00 5.03e-02  2e-02  5e-02 0:0.3\n          200    1800 3.428234862999135e-01 1.7e+01 3.77e-02  6e-03  3e-02 0:0.5\n          300    2700 3.216640032470860e-04 5.6e+01 6.62e-03  4e-04  9e-03 0:0.8\n          400    3600 6.155215286199821e-12 6.6e+01 7.44e-06  1e-07  4e-06 0:1.1\n          438    3942 1.187372505161762e-14 6.0e+01 3.27e-07  4e-09  9e-08 0:1.2\n          438    3942 1.187372505161762e-14 6.0e+01 3.27e-07  4e-09  9e-08 0:1.2\n        ('termination by', {'tolfun': 1e-11})\n        ('best f-value =', 1.1189867885201275e-14)\n        ('solution =', array([ 1.        ,  1.        ,  1.        ,  0.99999999,  0.99999998,\n                0.99999996,  0.99999992]))\n        >>> print(es.result()[0])\n        array([ 1.          1.          1.          0.99999999  0.99999998  0.99999996\n          0.99999992])\n\n        \"\"\"\n        assert iterations is None or min_iterations <= iterations\n        if not hasattr(self, 'logger'):\n            self.logger = logger\n        logger = self.logger = logger or self.logger\n        if not isinstance(call_back, list):\n            call_back = [call_back]\n\n        citer = 0\n        while not self.stop() or citer < min_iterations:\n            if iterations is not None and citer >= iterations:\n                return self.result()\n            citer += 1\n\n            X = self.ask()  # deliver candidate solutions\n            fitvals = [objective_fct(x, *args) for x in X]\n            self.tell(X, fitvals)  # all the work is done here\n            self.disp(verb_disp)\n            for f in call_back:\n                f is None or f(self)\n            logger.add(self) if logger else None\n\n        # signal logger that we left the loop\n        # TODO: this is very ugly, because it assumes modulo keyword\n        #       argument *and* modulo attribute to be available\n        try:\n            logger.add(self, modulo=bool(logger.modulo)) if logger else None\n        except TypeError:\n            print('  suppressing the final call of the logger in ' +\n                  'OOOptimizer.optimize (modulo keyword parameter not ' +\n                  'available)')\n        except AttributeError:\n            print('  suppressing the final call of the logger in ' +\n                  'OOOptimizer.optimize (modulo attribute not ' +\n                  'available)')\n        if verb_disp:\n            self.disp(1)\n        if verb_disp in (1, True):\n            print('termination by', self.stop())\n            print('best f-value =', self.result()[1])\n            print('solution =', self.result()[0])\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize_base(self, es):\n        ## meta_parameters.cs_exponent == 1.0\n        b = 1.0\n        ## meta_parameters.cs_multiplier == 1.0\n        self.cs = 1.0 * (es.sp.mueff + 2)**b / (es.N**b + (es.sp.mueff + 3)**b)\n        self.ps = np.zeros(es.N)\n        self.is_initialized_base = True\n        return self", "response": "initialize parameters and state variables based on dimension and possibly further options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_ps(self, es):\n        if not self.is_initialized_base:\n            self.initialize_base(es)\n        if self._ps_updated_iteration == es.countiter:\n            return\n        if es.countiter <= es.itereigenupdated:\n            # es.B and es.D must/should be those from the last iteration\n            assert es.countiter >= es.itereigenupdated\n            _print_warning('distribution transformation (B and D) have been updated before ps could be computed',\n                          '_update_ps', 'CMAAdaptSigmaBase')\n        z = dot(es.B, (1. / es.D) * dot(es.B.T, (es.mean - es.mean_old) / es.sigma_vec))\n        z *= es.sp.mueff**0.5 / es.sigma / es.sp.cmean\n        self.ps = (1 - self.cs) * self.ps + sqrt(self.cs * (2 - self.cs)) * z\n        self._ps_updated_iteration = es.countiter", "response": "update the isotropic evolution path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn OK - signal for rank - one update", "response": "def hsig(self, es):\n        \"\"\"return \"OK-signal\" for rank-one update, `True` (OK) or `False`\n        (stall rank-one update), based on the length of an evolution path\n\n        \"\"\"\n        self._update_ps(es)\n        if self.ps is None:\n            return True\n        squared_sum = sum(self.ps**2) / (1 - (1 - self.cs)**(2 * es.countiter))\n        # correction with self.countiter seems not necessary,\n        # as pc also starts with zero\n        return squared_sum / es.N - 1 < 1 + 4. / (es.N + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the parameters and state variables based on dimension mueff and possibly further options.", "response": "def initialize(self, es):\n        \"\"\"set parameters and state variable based on dimension,\n        mueff and possibly further options.\n\n        \"\"\"\n        self.disregard_length_setting = True if es.opts['CSA_disregard_length'] else False\n        if es.opts['CSA_clip_length_value'] is not None:\n            try:\n                if len(es.opts['CSA_clip_length_value']) == 0:\n                    es.opts['CSA_clip_length_value'] = [-np.Inf, np.Inf]\n                elif len(es.opts['CSA_clip_length_value']) == 1:\n                    es.opts['CSA_clip_length_value'] = [-np.Inf, es.opts['CSA_clip_length_value'][0]]\n                elif len(es.opts['CSA_clip_length_value']) == 2:\n                    es.opts['CSA_clip_length_value'] = np.sort(es.opts['CSA_clip_length_value'])\n                else:\n                    raise ValueError('option CSA_clip_length_value should be a number of len(.) in [1,2]')\n            except TypeError:  # len(...) failed\n                es.opts['CSA_clip_length_value'] = [-np.Inf, es.opts['CSA_clip_length_value']]\n            es.opts['CSA_clip_length_value'] = list(np.sort(es.opts['CSA_clip_length_value']))\n            if es.opts['CSA_clip_length_value'][0] > 0 or es.opts['CSA_clip_length_value'][1] < 0:\n                raise ValueError('option CSA_clip_length_value must be a single positive or a negative and a positive number')\n        ## meta_parameters.cs_exponent == 1.0\n        b = 1.0\n        ## meta_parameters.cs_multiplier == 1.0\n        self.cs = 1.0 * (es.sp.mueff + 2)**b / (es.N + (es.sp.mueff + 3)**b)\n        self.damps = es.opts['CSA_dampfac'] * (0.5 +\n                                          0.5 * min([1, (es.sp.lam_mirr / (0.159 * es.sp.popsize) - 1)**2])**1 +\n                                          2 * max([0, ((es.sp.mueff - 1) / (es.N + 1))**es.opts['CSA_damp_mueff_exponent'] - 1]) +\n                                          self.cs\n                                          )\n        self.max_delta_log_sigma = 1  # in symmetric use (strict lower bound is -cs/damps anyway)\n\n        if self.disregard_length_setting:\n            es.opts['CSA_clip_length_value'] = [0, 0]\n            ## meta_parameters.cs_exponent == 1.0\n            b = 1.0 * 0.5\n            ## meta_parameters.cs_multiplier == 1.0\n            self.cs = 1.0 * (es.sp.mueff + 1)**b / (es.N**b + 2 * es.sp.mueff**b)\n            self.damps = es.opts['CSA_dampfac'] * 1  # * (1.1 - 1/(es.N+1)**0.5)\n        if es.opts['verbose'] > 1 or self.disregard_length_setting or 11 < 3:\n            print('SigmaCSA Parameters')\n            for k, v in self.__dict__.items():\n                print('  ', k, ':', v)\n        self.ps = np.zeros(es.N)\n        self._ps_updated_iteration = -1\n        self.is_initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, es, function_values, **kwargs):\n        # TODO: on the linear function, the two mirrored samples lead\n        # to a sharp increase of condition of the covariance matrix.\n        # They should not be used to update the covariance matrix,\n        # if the step-size inreases quickly. This should be fine with\n        # negative updates though.\n        if not self.initialized:\n            self.initialize(es.N, es.opts)\n        if 1 < 3:\n            # use the ranking difference of the mirrors for adaptation\n            # damp = 5 should be fine\n            z = np.where(es.fit.idx == 1)[0][0] - np.where(es.fit.idx == 0)[0][0]\n            z /= es.popsize - 1  # z in [-1, 1]\n        self.s = (1 - self.sp.c) * self.s + self.sp.c * np.sign(z) * np.abs(z)**self.sp.z_exponent\n        if self.s > 0:\n            es.sigma *= exp(self.s / self.sp.dampup)\n        else:\n            es.sigma *= exp(self.s / self.sp.dampdown)", "response": "update the covariance matrix with the new values from the function_values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop the current resource set", "response": "def stop(self, check=True):\n        \"\"\"return a dictionary with the termination status.\n        With ``check==False``, the termination conditions are not checked\n        and the status might not reflect the current situation.\n\n        \"\"\"\n        if (check and self.countiter > 0 and self.opts['termination_callback'] and\n                self.opts['termination_callback'] != str(self.opts['termination_callback'])):\n            self.callbackstop = self.opts['termination_callback'](self)\n\n        return self._stopdict(self, check)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasks the new set of candidate solutions from a multi - variate and returns the new set of candidate solutions", "response": "def ask(self, number=None, xmean=None, sigma_fac=1,\n            gradf=None, args=()):\n        \"\"\"get new candidate solutions, sampled from a multi-variate\n        normal distribution and transformed to f-representation\n        (phenotype) to be evaluated.\n\n        Arguments\n        ---------\n            `number`\n                number of returned solutions, by default the\n                population size ``popsize`` (AKA ``lambda``).\n            `xmean`\n                distribution mean, phenotyp?\n            `sigma_fac`\n                multiplier for internal sample width (standard\n                deviation)\n            `gradf`\n                gradient, ``len(gradf(x)) == len(x)``, if\n                ``gradf is not None`` the third solution in the\n                returned list is \"sampled\" in supposedly Newton\n                direction ``dot(C, gradf(xmean, *args))``.\n            `args`\n                additional arguments passed to gradf\n\n        Return\n        ------\n        A list of N-dimensional candidate solutions to be evaluated\n\n        Example\n        -------\n        >>> import cma\n        >>> es = cma.CMAEvolutionStrategy([0,0,0,0], 0.3)\n        >>> while not es.stop() and es.best.f > 1e-6:  # my_desired_target_f_value\n        ...     X = es.ask()  # get list of new solutions\n        ...     fit = [cma.fcts.rosen(x) for x in X]  # call function rosen with each solution\n        ...     es.tell(X, fit)  # feed values\n\n        :See: `ask_and_eval`, `ask_geno`, `tell`\n\n        \"\"\"\n        pop_geno = self.ask_geno(number, xmean, sigma_fac)\n\n        # N,lambda=20,200: overall CPU 7s vs 5s == 40% overhead, even without bounds!\n        #                  new data: 11.5s vs 9.5s == 20%\n        # TODO: check here, whether this is necessary?\n        # return [self.gp.pheno(x, copy=False, into_bounds=self.boundary_handler.repair) for x in pop]  # probably fine\n        # return [Solution(self.gp.pheno(x, copy=False), copy=False) for x in pop]  # here comes the memory leak, now solved\n        pop_pheno = [self.gp.pheno(x, copy=True, into_bounds=self.boundary_handler.repair) for x in pop_geno]\n\n        if gradf is not None:\n            # see Hansen (2011), Injecting external solutions into CMA-ES\n            if not self.gp.islinear:\n                _print_warning(\"\"\"\n                using the gradient (option ``gradf``) with a non-linear\n                coordinate-wise transformation (option ``transformation``)\n                has never been tested.\"\"\")\n                # TODO: check this out\n            def grad_numerical_of_coordinate_map(x, map, epsilon=None):\n                \"\"\"map is a coordinate-wise independent map, return\n                the estimated diagonal of the Jacobian.\n                \"\"\"\n                eps = 1e-8 * (1 + abs(x)) if epsilon is None else epsilon\n                return (map(x + eps) - map(x - eps)) / (2 * eps)\n            def grad_numerical_sym(x, func, epsilon=None):\n                \"\"\"return symmetric numerical gradient of func : R^n -> R.\n                \"\"\"\n                eps = 1e-8 * (1 + abs(x)) if epsilon is None else epsilon\n                grad = np.zeros(len(x))\n                ei = np.zeros(len(x))  # float is 1.6 times faster than int\n                for i in rglen(x):\n                    ei[i] = eps[i]\n                    grad[i] = (func(x + ei) - func(x - ei)) / (2*eps[i])\n                    ei[i] = 0\n                return grad\n            try:\n                if self.last_iteration_with_gradient == self.countiter:\n                    _print_warning('gradient is used several times in ' +\n                            'this iteration', iteration=self.countiter)\n                self.last_iteration_with_gradient = self.countiter\n            except AttributeError:\n                pass\n            index_for_gradient = min((2, len(pop_pheno)-1))\n            xmean = self.mean if xmean is None else xmean\n            xpheno = self.gp.pheno(xmean, copy=True,\n                                into_bounds=self.boundary_handler.repair)\n            grad_at_mean = gradf(xpheno, *args)\n            # lift gradient into geno-space\n            if not self.gp.isidentity or (self.boundary_handler is not None\n                    and self.boundary_handler.has_bounds()):\n                boundary_repair = None\n                gradpen = 0\n                if isinstance(self.boundary_handler, BoundTransform):\n                    boundary_repair = self.boundary_handler.repair\n                elif isinstance(self.boundary_handler, BoundPenalty):\n                    fpenalty = lambda x: self.boundary_handler.__call__(\n                        x, SolutionDict({tuple(x): {'geno': x}}), self.gp)\n                    gradpen = grad_numerical_sym(\n                        xmean, fpenalty)\n                elif self.boundary_handler is None or \\\n                        isinstance(self.boundary_handler, BoundNone):\n                    pass\n                else:\n                    raise NotImplementedError(\n                        \"unknown boundary handling method\" +\n                        str(self.boundary_handler) +\n                        \" when using gradf\")\n                gradgp = grad_numerical_of_coordinate_map(\n                    xmean,\n                    lambda x: self.gp.pheno(x, copy=True,\n                            into_bounds=boundary_repair))\n                grad_at_mean = grad_at_mean * gradgp + gradpen\n\n            # TODO: frozen variables brake the code (e.g. at grad of map)\n            if len(grad_at_mean) != self.N and self.opts['fixed_variables']:\n                NotImplementedError(\"\"\"\n                gradient with fixed variables is not yet implemented\"\"\")\n            v = self.D * dot(self.B.T, self.sigma_vec * grad_at_mean)\n            # newton_direction = sv * B * D * D * B^T * sv * gradient = sv * B * D * v\n            # v = D^-1 * B^T * sv^-1 * newton_direction = D * B^T * sv * gradient\n            q = sum(v**2)\n            if q:\n                # Newton direction\n                pop_geno[index_for_gradient] = xmean - self.sigma \\\n                            * (self.N / q)**0.5 \\\n                            * (self.sigma_vec * dot(self.B, self.D * v))\n            else:\n                pop_geno[index_for_gradient] = xmean\n                _print_warning('gradient zero observed',\n                               iteration=self.countiter)\n\n            pop_pheno[index_for_gradient] = self.gp.pheno(\n                pop_geno[index_for_gradient], copy=True,\n                into_bounds=self.boundary_handler.repair)\n\n        # insert solutions, this could also (better?) be done in self.gp.pheno\n        for i in rglen((pop_geno)):\n            self.sent_solutions.insert(pop_pheno[i], geno=pop_geno[i], iteration=self.countiter)\n        return pop_pheno"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ask_geno(self, number=None, xmean=None, sigma_fac=1):\n\n        if number is None or number < 1:\n            number = self.sp.popsize\n\n        # update distribution, might change self.mean\n        if self.sp.CMA_on and (\n                (self.opts['updatecovwait'] is None and\n                 self.countiter >=\n                     self.itereigenupdated + 1. / (self.sp.c1 + self.sp.cmu) / self.N / 10\n                 ) or\n                (self.opts['updatecovwait'] is not None and\n                 self.countiter > self.itereigenupdated + self.opts['updatecovwait']\n                 ) or\n                (self.sp.neg.cmuexp * (self.countiter - self.itereigenupdated) > 0.5\n                )  # TODO (minor): not sure whether this is \"the right\" criterion\n            ):\n            self.updateBD()\n        if xmean is None:\n            xmean = self.mean\n        else:\n            try:\n                xmean = self.archive[xmean]['geno']\n                # noise handling after call of tell\n            except KeyError:\n                try:\n                    xmean = self.sent_solutions[xmean]['geno']\n                    # noise handling before calling tell\n                except KeyError:\n                    pass\n\n        if self.countiter == 0:\n            self.tic = time.clock()  # backward compatible\n            self.elapsed_time = ElapsedTime()\n\n        sigma = sigma_fac * self.sigma\n\n        # update parameters for sampling the distribution\n        #        fac  0      1      10\n        # 150-D cigar:\n        #           50749  50464   50787\n        # 200-D elli:               == 6.9\n        #                  99900   101160\n        #                 100995   103275 == 2% loss\n        # 100-D elli:               == 6.9\n        #                 363052   369325  < 2% loss\n        #                 365075   365755\n\n        # sample distribution\n        if self._flgtelldone:  # could be done in tell()!?\n            self._flgtelldone = False\n            self.ary = []\n\n        # check injections from pop_injection_directions\n        arinj = []\n        if hasattr(self, 'pop_injection_directions'):\n            if self.countiter < 4 and \\\n                    len(self.pop_injection_directions) > self.popsize - 2:\n                _print_warning('  %d special injected samples with popsize %d, '\n                                  % (len(self.pop_injection_directions), self.popsize)\n                               + \"popsize %d will be used\" % (len(self.pop_injection_directions) + 2)\n                               + (\" and the warning is suppressed in the following\" if self.countiter == 3 else \"\"))\n            while self.pop_injection_directions:\n                y = self.pop_injection_directions.pop(0)\n                if self.opts['CMA_sample_on_sphere_surface']:\n                    y *= (self.N**0.5 if self.opts['CSA_squared'] else\n                          self.const.chiN) / self.mahalanobis_norm(y)\n                    arinj.append(y)\n                else:\n                    y *= self.random_rescaling_factor_to_mahalanobis_size(y) / self.sigma\n                    arinj.append(y)\n        # each row is a solution\n        # the 1 is a small safeguard which needs to be removed to implement \"pure\" adaptive encoding\n        arz = self.randn((max([1, (number - len(arinj))]), self.N))\n        if self.opts['CMA_sample_on_sphere_surface']:  # normalize the length to chiN\n            for i in rglen((arz)):\n                ss = sum(arz[i]**2)\n                if 1 < 3 or ss > self.N + 10.1:\n                    arz[i] *= (self.N**0.5 if self.opts['CSA_squared']\n                                           else self.const.chiN) / ss**0.5\n            # or to average\n            # arz *= 1 * self.const.chiN / np.mean([sum(z**2)**0.5 for z in arz])\n\n        # fac = np.mean(sum(arz**2, 1)**0.5)\n        # print fac\n        # arz *= self.const.chiN / fac\n\n        # compute ary from arz\n        if len(arz):  # should always be true\n            # apply unconditional mirroring, is pretty obsolete\n            if new_injections and self.sp.lam_mirr and self.opts['CMA_mirrormethod'] == 0:\n                for i in xrange(self.sp.lam_mirr):\n                    if 2 * (i + 1) > len(arz):\n                        if self.countiter < 4:\n                            _print_warning(\"fewer mirrors generated than given in parameter setting (%d<%d)\"\n                                            % (i, self.sp.lam_mirr))\n                        break\n                    arz[-1 - 2 * i] = -arz[-2 - 2 * i]\n            ary = self.sigma_vec * np.dot(self.B, (self.D * arz).T).T\n            if len(arinj):\n                ary = np.vstack((arinj, ary))\n        else:\n            ary = array(arinj)\n\n        # TODO: subject to removal in future\n        if not new_injections and number > 2 and self.countiter > 2:\n            if (isinstance(self.adapt_sigma, CMAAdaptSigmaTPA) or\n                self.opts['mean_shift_line_samples'] or\n                self.opts['pc_line_samples']):\n                ys = []\n                if self.opts['pc_line_samples']:\n                    ys.append(self.pc[:])  # now TPA is with pc_line_samples\n                if self.opts['mean_shift_line_samples']:\n                    ys.append(self.mean - self.mean_old)\n                if not len(ys):\n                    ys.append(self.mean - self.mean_old)\n                # assign a mirrored pair from each element of ys into ary\n                for i, y in enumerate(ys):\n                    if len(arz) > 2 * i + 1:  # at least two more samples\n                        assert y is not self.pc\n                        # y *= sum(self.randn(self.N)**2)**0.5 / self.mahalanobis_norm(y)\n                        y *= self.random_rescaling_factor_to_mahalanobis_size(y)\n                        # TODO: rescale y depending on some parameter?\n                        ary[2*i] = y / self.sigma\n                        ary[2*i + 1] = y / -self.sigma\n                    else:\n                        _print_warning('line samples omitted due to small popsize',\n                            method_name='ask_geno', iteration=self.countiter)\n\n        # print(xmean[0])\n        pop = xmean + sigma * ary\n        self.evaluations_per_f_value = 1\n        self.ary = ary\n        return pop", "response": "ask_geno returns a list of candidate solutions in genotyp representation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging x like for injection all on genotypic level", "response": "def random_rescale_to_mahalanobis(self, x):\n        \"\"\"change `x` like for injection, all on genotypic level\"\"\"\n        x -= self.mean\n        if any(x):\n            x *= sum(self.randn(len(x))**2)**0.5 / self.mahalanobis_norm(x)\n        x += self.mean\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef random_rescaling_factor_to_mahalanobis_size(self, y):\n        if len(y) != self.N:\n            raise ValueError('len(y)=%d != %d=dimension' % (len(y), self.N))\n        if not any(y):\n            _print_warning(\"input was all-zeros, which is probably a bug\",\n                           \"random_rescaling_factor_to_mahalanobis_size\",\n                           iteration=self.countiter)\n            return 1.0\n        return sum(self.randn(len(y))**2)**0.5 / self.mahalanobis_norm(y)", "response": "This function calculates the random rescaling factor of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns ``pheno(self.mean - (geno(x) - self.mean))``. >>> import cma >>> es = cma.CMAEvolutionStrategy(cma.np.random.randn(3), 1) >>> x = cma.np.random.randn(3) >>> assert cma.Mh.vequals_approximately(es.mean - (x - es.mean), es.get_mirror(x, preserve_length=True)) >>> x = es.ask(1)[0] >>> vals = (es.get_mirror(x) - es.mean) / (x - es.mean) >>> assert cma.Mh.equals_approximately(sum(vals), len(vals) * vals[0]) TODO: this implementation is yet experimental. TODO: this implementation includes geno-pheno transformation, however in general GP-transformation should be separated from specific code. Selectively mirrored sampling improves to a moderate extend but overadditively with active CMA for quite understandable reasons. Optimal number of mirrors are suprisingly small: 1,2,3 for maxlam=7,13,20 where 3,6,10 are the respective maximal possible mirrors that must be clearly suboptimal.", "response": "def get_mirror(self, x, preserve_length=False):\n        \"\"\"return ``pheno(self.mean - (geno(x) - self.mean))``.\n\n        >>> import cma\n        >>> es = cma.CMAEvolutionStrategy(cma.np.random.randn(3), 1)\n        >>> x = cma.np.random.randn(3)\n        >>> assert cma.Mh.vequals_approximately(es.mean - (x - es.mean), es.get_mirror(x, preserve_length=True))\n        >>> x = es.ask(1)[0]\n        >>> vals = (es.get_mirror(x) - es.mean) / (x - es.mean)\n        >>> assert cma.Mh.equals_approximately(sum(vals), len(vals) * vals[0])\n\n        TODO: this implementation is yet experimental.\n\n        TODO: this implementation includes geno-pheno transformation,\n        however in general GP-transformation should be separated from\n        specific code.\n\n        Selectively mirrored sampling improves to a moderate extend but\n        overadditively with active CMA for quite understandable reasons.\n\n        Optimal number of mirrors are suprisingly small: 1,2,3 for\n        maxlam=7,13,20 where 3,6,10 are the respective maximal possible\n        mirrors that must be clearly suboptimal.\n\n        \"\"\"\n        try:\n            dx = self.sent_solutions[x]['geno'] - self.mean\n        except:  # can only happen with injected solutions?!\n            dx = self.gp.geno(x, from_bounds=self.boundary_handler.inverse,\n                              copy_if_changed=True) - self.mean\n\n        if not preserve_length:\n            # dx *= sum(self.randn(self.N)**2)**0.5 / self.mahalanobis_norm(dx)\n            dx *= self.random_rescaling_factor_to_mahalanobis_size(dx)\n        x = self.mean - dx\n        y = self.gp.pheno(x, into_bounds=self.boundary_handler.repair)\n        # old measure: costs 25% in CPU performance with N,lambda=20,200\n        self.sent_solutions.insert(y, geno=x, iteration=self.countiter)\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npenalizes version of the selective mirroring function.", "response": "def _mirror_penalized(self, f_values, idx):\n        \"\"\"obsolete and subject to removal (TODO),\n        return modified f-values such that for each mirror one becomes worst.\n\n        This function is useless when selective mirroring is applied with no\n        more than (lambda-mu)/2 solutions.\n\n        Mirrors are leading and trailing values in ``f_values``.\n\n        \"\"\"\n        assert len(f_values) >= 2 * len(idx)\n        m = np.max(np.abs(f_values))\n        for i in len(idx):\n            if f_values[idx[i]] > f_values[-1 - i]:\n                f_values[idx[i]] += m\n            else:\n                f_values[-1 - i] += m\n        return f_values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmirroring version of _mirror_idx_cov.", "response": "def _mirror_idx_cov(self, f_values, idx1):  # will most likely be removed\n        \"\"\"obsolete and subject to removal (TODO),\n        return indices for negative (\"active\") update of the covariance matrix\n        assuming that ``f_values[idx1[i]]`` and ``f_values[-1-i]`` are\n        the corresponding mirrored values\n\n        computes the index of the worse solution sorted by the f-value of the\n        better solution.\n\n        TODO: when the actual mirror was rejected, it is better\n        to return idx1 instead of idx2.\n\n        Remark: this function might not be necessary at all: if the worst solution\n        is the best mirrored, the covariance matrix updates cancel (cave: weights\n        and learning rates), which seems what is desirable. If the mirror is bad,\n        as strong negative update is made, again what is desirable.\n        And the fitness--step-length correlation is in part addressed by\n        using flat weights.\n\n        \"\"\"\n        idx2 = np.arange(len(f_values) - 1, len(f_values) - 1 - len(idx1), -1)\n        f = []\n        for i in rglen((idx1)):\n            f.append(min((f_values[idx1[i]], f_values[idx2[i]])))\n            # idx.append(idx1[i] if f_values[idx1[i]] > f_values[idx2[i]] else idx2[i])\n        return idx2[np.argsort(f)][-1::-1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the distribution mean", "response": "def eval_mean(self, func, args=()):\n        \"\"\"evaluate the distribution mean, this is not (yet) effective\n        in terms of termination or display\"\"\"\n        self.fmean = func(self.mean, *args)\n        return self.fmean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ask_and_eval(self, func, args=(), gradf=None, number=None, xmean=None, sigma_fac=1,\n                     evaluations=1, aggregation=np.median, kappa=1):\n        \"\"\"samples `number` solutions and evaluates them on `func`, where\n        each solution `s` is resampled until ``self.is_feasible(s, func(s)) is True``.\n\n        Arguments\n        ---------\n            `func`\n                objective function, ``func(x)`` returns a scalar\n            `args`\n                additional parameters for `func`\n            `gradf`\n                gradient of objective function, ``g = gradf(x, *args)``\n                must satisfy ``len(g) == len(x)``\n            `number`\n                number of solutions to be sampled, by default\n                population size ``popsize`` (AKA lambda)\n            `xmean`\n                mean for sampling the solutions, by default ``self.mean``.\n            `sigma_fac`\n                multiplier for sampling width, standard deviation, for example\n                to get a small perturbation of solution `xmean`\n            `evaluations`\n                number of evaluations for each sampled solution\n            `aggregation`\n                function that aggregates `evaluations` values to\n                as single value.\n            `kappa`\n                multiplier used for the evaluation of the solutions, in\n                that ``func(m + kappa*(x - m))`` is the f-value for x.\n\n        Return\n        ------\n        ``(X, fit)``, where\n            X -- list of solutions\n            fit -- list of respective function values\n\n        Details\n        -------\n        While ``not self.is_feasible(x, func(x))``new solutions are sampled. By\n        default ``self.is_feasible == cma.feasible == lambda x, f: f not in (None, np.NaN)``.\n        The argument to `func` can be freely modified within `func`.\n\n        Depending on the ``CMA_mirrors`` option, some solutions are not sampled\n        independently but as mirrors of other bad solutions. This is a simple\n        derandomization that can save 10-30% of the evaluations in particular\n        with small populations, for example on the cigar function.\n\n        Example\n        -------\n        >>> import cma\n        >>> x0, sigma0 = 8*[10], 1  # 8-D\n        >>> es = cma.CMAEvolutionStrategy(x0, sigma0)\n        >>> while not es.stop():\n        ...     X, fit = es.ask_and_eval(cma.fcts.elli)  # handles NaN with resampling\n        ...     es.tell(X, fit)  # pass on fitness values\n        ...     es.disp(20) # print every 20-th iteration\n        >>> print('terminated on ' + str(es.stop()))\n        <output omitted>\n\n        A single iteration step can be expressed in one line, such that\n        an entire optimization after initialization becomes\n        ::\n\n            while not es.stop():\n                es.tell(*es.ask_and_eval(cma.fcts.elli))\n\n        \"\"\"\n        # initialize\n        popsize = self.sp.popsize\n        if number is not None:\n            popsize = number\n\n        selective_mirroring = self.opts['CMA_mirrormethod'] > 0\n        nmirrors = self.sp.lam_mirr\n        if popsize != self.sp.popsize:\n            nmirrors = Mh.sround(popsize * self.sp.lam_mirr / self.sp.popsize)\n            # TODO: now selective mirroring might be impaired\n        assert new_injections or self.opts['CMA_mirrormethod'] < 2 \n        if new_injections and self.opts['CMA_mirrormethod'] != 1: # otherwise mirrors are done elsewhere\n            nmirrors = 0\n        assert nmirrors <= popsize // 2\n        self.mirrors_idx = np.arange(nmirrors)  # might never be used\n        self.mirrors_rejected_idx = []  # might never be used\n        is_feasible = self.opts['is_feasible']\n\n        # do the work\n        fit = []  # or np.NaN * np.empty(number)\n        X_first = self.ask(popsize, xmean=xmean, gradf=gradf, args=args)\n        if xmean is None:\n            xmean = self.mean  # might have changed in self.ask\n        X = []\n        for k in xrange(int(popsize)):\n            x, f = X_first.pop(0), None\n            rejected = -1\n            while rejected < 0 or not is_feasible(x, f):  # rejection sampling\n                rejected += 1\n                if rejected:  # resample\n                    x = self.ask(1, xmean, sigma_fac)[0]\n                elif k >= popsize - nmirrors:  # mirrored sample\n                    if k == popsize - nmirrors and selective_mirroring:\n                        self.mirrors_idx = np.argsort(fit)[-1:-1 - nmirrors:-1]\n                    x = self.get_mirror(X[self.mirrors_idx[popsize - 1 - k]])\n                if rejected == 1 and k >= popsize - nmirrors:\n                    self.mirrors_rejected_idx.append(k)\n\n                # contraints handling test hardwired ccccccccccc\n                length_normalizer = 1\n                # zzzzzzzzzzzzzzzzzzzzzzzzz\n                f = func(x, *args) if kappa == 1 else \\\n                    func(xmean + kappa * length_normalizer * (x - xmean),\n                         *args)\n                if is_feasible(x, f) and evaluations > 1:\n                    f = aggregation([f] + [(func(x, *args) if kappa == 1 else\n                                            func(xmean + kappa * length_normalizer * (x - xmean), *args))\n                                           for _i in xrange(int(evaluations - 1))])\n                if rejected + 1 % 1000 == 0:\n                    print('  %d solutions rejected (f-value NaN or None) at iteration %d' %\n                          (rejected, self.countiter))\n            fit.append(f)\n            X.append(x)\n        self.evaluations_per_f_value = int(evaluations)\n        return X, fit", "response": "asks number solutions and evaluates them on func"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_injection_directions(self):\n        # self.pop_injection_directions is supposed to be empty here\n        if hasattr(self, 'pop_injection_directions') and self.pop_injection_directions:\n            ValueError(\"Looks like a bug in calling order/logics\")\n        ary = []\n        if (isinstance(self.adapt_sigma, CMAAdaptSigmaTPA) or\n                self.opts['mean_shift_line_samples']):\n            ary.append(self.mean - self.mean_old)\n            ary.append(self.mean_old - self.mean)  # another copy!\n            if ary[-1][0] == 0.0:\n                _print_warning('zero mean shift encountered which ',\n                               'prepare_injection_directions',\n                               'CMAEvolutionStrategy', self.countiter)\n        if self.opts['pc_line_samples']: # caveat: before, two samples were used\n            ary.append(self.pc.copy())\n        if self.sp.lam_mirr and self.opts['CMA_mirrormethod'] == 2:\n            if self.pop_sorted is None:\n                _print_warning('pop_sorted attribute not found, mirrors obmitted',\n                               'prepare_injection_directions',\n                               iteration=self.countiter)\n            else:\n                ary += self.get_selective_mirrors()\n        self.pop_injection_directions = ary\n        return ary", "response": "provide genotypic directions for TPA and selective mirroring"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_selective_mirrors(self, number=None, pop_sorted=None):\n        if pop_sorted is None:\n            if hasattr(self, 'pop_sorted'):\n                pop_sorted = self.pop_sorted\n            else:\n                return None\n        if number is None:\n            number = self.sp.lam_mirr\n        res = []\n        for i in xrange(1, number + 1):\n            res.append(self.mean_old - pop_sorted[-i])\n        return res", "response": "get selective genotypic directions of the number worst\n        solution based on pop_sorted attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tell(self, solutions, function_values, check_points=None,\n             copy=False):\n        \"\"\"pass objective function values to prepare for next\n        iteration. This core procedure of the CMA-ES algorithm updates\n        all state variables, in particular the two evolution paths, the\n        distribution mean, the covariance matrix and a step-size.\n\n        Arguments\n        ---------\n            `solutions`\n                list or array of candidate solution points (of\n                type `numpy.ndarray`), most presumably before\n                delivered by method `ask()` or `ask_and_eval()`.\n            `function_values`\n                list or array of objective function values\n                corresponding to the respective points. Beside for termination\n                decisions, only the ranking of values in `function_values`\n                is used.\n            `check_points`\n                If ``check_points is None``, only solutions that are not generated\n                by `ask()` are possibly clipped (recommended). ``False`` does not clip\n                any solution (not recommended).\n                If ``True``, clips solutions that realize long steps (i.e. also\n                those that are unlikely to be generated with `ask()`). `check_points`\n                can be a list of indices to be checked in solutions.\n            `copy`\n                ``solutions`` can be modified in this routine, if ``copy is False``\n\n        Details\n        -------\n        `tell()` updates the parameters of the multivariate\n        normal search distribution, namely covariance matrix and\n        step-size and updates also the attributes ``countiter`` and\n        ``countevals``. To check the points for consistency is quadratic\n        in the dimension (like sampling points).\n\n        Bugs\n        ----\n        The effect of changing the solutions delivered by `ask()`\n        depends on whether boundary handling is applied. With boundary\n        handling, modifications are disregarded. This is necessary to\n        apply the default boundary handling that uses unrepaired\n        solutions but might change in future.\n\n        Example\n        -------\n        ::\n\n            import cma\n            func = cma.fcts.elli  # choose objective function\n            es = cma.CMAEvolutionStrategy(cma.np.random.rand(10), 1)\n            while not es.stop():\n               X = es.ask()\n               es.tell(X, [func(x) for x in X])\n            es.result()  # where the result can be found\n\n        :See: class `CMAEvolutionStrategy`, `ask()`, `ask_and_eval()`, `fmin()`\n\n        \"\"\"\n        if self._flgtelldone:\n            raise _Error('tell should only be called once per iteration')\n\n        lam = len(solutions)\n        if lam != array(function_values).shape[0]:\n            raise _Error('for each candidate solution '\n                        + 'a function value must be provided')\n        if lam + self.sp.lam_mirr < 3:\n            raise _Error('population size ' + str(lam) + ' is too small when option CMA_mirrors * popsize < 0.5')\n\n        if not isscalar(function_values[0]):\n            if isscalar(function_values[0][0]):\n                if self.countiter <= 1:\n                    _print_warning('function values are not a list of scalars (further warnings are suppressed)')\n                function_values = [val[0] for val in function_values]\n            else:\n                raise _Error('objective function values must be a list of scalars')\n\n\n        # ## prepare\n        N = self.N\n        sp = self.sp\n        if lam < sp.mu:  # rather decrease cmean instead of having mu > lambda//2\n            raise _Error('not enough solutions passed to function tell (mu>lambda)')\n\n        self.countiter += 1  # >= 1 now\n        self.countevals += sp.popsize * self.evaluations_per_f_value\n        self.best.update(solutions, self.sent_solutions, function_values, self.countevals)\n\n        flg_diagonal = self.opts['CMA_diagonal'] is True \\\n                       or self.countiter <= self.opts['CMA_diagonal']\n        if not flg_diagonal and len(self.C.shape) == 1:  # C was diagonal ie 1-D\n            # enter non-separable phase (no easy return from here)\n            self.C = np.diag(self.C)\n            if 1 < 3:\n                self.B = np.eye(N)  # identity(N)\n                idx = np.argsort(self.D)\n                self.D = self.D[idx]\n                self.B = self.B[:, idx]\n            self._Yneg = np.zeros((N, N))\n\n        # ## manage fitness\n        fit = self.fit  # make short cut\n\n        # CPU for N,lam=20,200: this takes 10s vs 7s\n        fit.bndpen = self.boundary_handler.update(function_values, self)(solutions, self.sent_solutions, self.gp)\n        # for testing:\n        # fit.bndpen = self.boundary_handler.update(function_values, self)([s.unrepaired for s in solutions])\n        fit.idx = np.argsort(array(fit.bndpen) + array(function_values))\n        fit.fit = array(function_values, copy=False)[fit.idx]\n\n        # update output data TODO: this is obsolete!? However: need communicate current best x-value?\n        # old: out['recent_x'] = self.gp.pheno(pop[0])\n        # self.out['recent_x'] = array(solutions[fit.idx[0]])  # TODO: change in a data structure(?) and use current as identify\n        # self.out['recent_f'] = fit.fit[0]\n\n        # fitness histories\n        fit.hist.insert(0, fit.fit[0])\n        # if len(self.fit.histbest) < 120+30*N/sp.popsize or  # does not help, as tablet in the beginning is the critical counter-case\n        if ((self.countiter % 5) == 0):  # 20 percent of 1e5 gen.\n            fit.histbest.insert(0, fit.fit[0])\n            fit.histmedian.insert(0, np.median(fit.fit) if len(fit.fit) < 21\n                                    else fit.fit[self.popsize // 2])\n        if len(fit.histbest) > 2e4:  # 10 + 30*N/sp.popsize:\n            fit.histbest.pop()\n            fit.histmedian.pop()\n        if len(fit.hist) > 10 + 30 * N / sp.popsize:\n            fit.hist.pop()\n\n        # TODO: clean up inconsistency when an unrepaired solution is available and used\n        # now get the genotypes\n        pop = self.pop_sorted = []  # create pop from input argument solutions\n        for k, s in enumerate(solutions):  # use phenotype before Solution.repair()\n            if 1 < 3:\n                pop += [self.gp.geno(s,\n                            from_bounds=self.boundary_handler.inverse,\n                            repair=(self.repair_genotype if check_points not in (False, 0, [], ()) else None),\n                            archive=self.sent_solutions)]  # takes genotype from sent_solutions, if available\n                try:\n                    self.archive.insert(s, value=self.sent_solutions.pop(s), fitness=function_values[k])\n                    # self.sent_solutions.pop(s)\n                except KeyError:\n                    pass\n        # check that TPA mirrors are available, TODO: move to TPA class?\n        if isinstance(self.adapt_sigma, CMAAdaptSigmaTPA) and self.countiter > 3 and not (self.countiter % 3):\n            dm = self.mean[0] - self.mean_old[0]\n            dx0 = pop[0][0] - self.mean_old[0]\n            dx1 = pop[1][0] - self.mean_old[0]\n            for i in np.random.randint(1, self.N, 1):\n                try:\n                    if not Mh.equals_approximately(\n                                (self.mean[i] - self.mean_old[i])\n                                    / (pop[0][i] - self.mean_old[i]),\n                                 dm / dx0, 1e-8) or \\\n                        not Mh.equals_approximately(\n                                (self.mean[i] - self.mean_old[i])\n                                    / (pop[1][i] - self.mean_old[i]),\n                                dm / dx1, 1e-8):\n                        _print_warning('TPA error with mirrored samples', 'tell',\n                                       'CMAEvolutionStrategy', self.countiter)\n                except ZeroDivisionError:\n                    _print_warning('zero division encountered in TPA check\\n which should be very rare and is likely a bug',\n                                   'tell', 'CMAEvolutionStrategy', self.countiter)\n\n        try:\n            moldold = self.mean_old\n        except:\n            pass\n        self.mean_old = self.mean\n        mold = self.mean_old  # just an alias\n\n        # check and normalize each x - m\n        # check_points is a flag (None is default: check non-known solutions) or an index list\n        # should also a number possible (first check_points points)?\n        if check_points not in (None, False, 0, [], ()):  # useful in case of injected solutions and/or adaptive encoding, however is automatic with use_sent_solutions\n            try:\n                if len(check_points):\n                    idx = check_points\n            except:\n                idx = xrange(sp.popsize)\n\n            for k in idx:\n                self.repair_genotype(pop[k])\n\n        # only arrays can be multiple indexed\n        pop = array(pop, copy=False)\n\n        # sort pop\n        pop = pop[fit.idx]\n\n        # prepend best-ever solution to population, in case\n        if self.opts['CMA_elitist'] and self.best.f < fit.fit[0]:\n            if self.best.x_geno is not None:\n                xp = [self.best.x_geno]\n                # xp = [self.best.xdict['geno']]\n                # xp = [self.gp.geno(self.best.x[:])]  # TODO: remove\n                # print self.mahalanobis_norm(xp[0]-self.mean)\n            else:\n                xp = [self.gp.geno(array(self.best.x, copy=True),\n                                   self.boundary_handler.inverse,\n                                   copy_if_changed=False)]\n                print('genotype for elitist not found')\n            self.clip_or_fit_solutions(xp, [0])\n            pop = array([xp[0]] + list(pop))\n        elif self.opts['CMA_elitist'] == 'initial':  # current solution was better\n            self.opts['CMA_elitist'] = False\n\n        self.pop_sorted = pop\n        # compute new mean\n        self.mean = mold + self.sp.cmean * \\\n                    (sum(sp.weights * pop[0:sp.mu].T, 1) - mold)\n\n\n        # check Delta m (this is not default, but could become at some point)\n        # CAVE: upper_length=sqrt(2)+2 is too restrictive, test upper_length = sqrt(2*N) thoroughly.\n        # replaced by repair_geno?\n        # simple test case injecting self.mean:\n        # self.mean = 1e-4 * self.sigma * np.random.randn(N)\n        if 1 < 3:\n            cmean = self.sp.cmean\n\n        # zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n        # get learning rate constants\n        cc, c1, cmu = sp.cc, sp.c1, sp.cmu\n        if flg_diagonal:\n            cc, c1, cmu = sp.cc_sep, sp.c1_sep, sp.cmu_sep\n\n        # now the real work can start\n\n        hsig = self.adapt_sigma.hsig(self) # ps update must be done here in separable case\n\n        # hsig = sum(self.ps**2) / self.N < 2 + 4./(N+1)\n        # adjust missing variance due to hsig, in 4-D with damps=1e99 and sig0 small\n        #       hsig leads to premature convergence of C otherwise\n        # hsiga = (1-hsig**2) * c1 * cc * (2-cc)  # to be removed in future\n        c1a = c1 - (1 - hsig**2) * c1 * cc * (2 - cc)  # adjust for variance loss\n\n        self.pc = (1 - cc) * self.pc + \\\n                  hsig * (sqrt(cc * (2 - cc) * sp.mueff) / self.sigma / cmean) * \\\n                  (self.mean - mold) / self.sigma_vec\n\n        # covariance matrix adaptation/udpate\n        if sp.CMA_on:\n            # assert sp.c1 + sp.cmu < sp.mueff / N  # ??\n            assert c1 + cmu <= 1\n\n            # default full matrix case\n            if not flg_diagonal:\n                Y = (pop[0:sp.mu] - mold) / (self.sigma * self.sigma_vec)\n                Y = dot((cmu * sp.weights) * Y.T, Y)  # learning rate integrated\n                if self.sp.neg.cmuexp:\n                    tmp = (pop[-sp.neg.mu:] - mold) / (self.sigma * self.sigma_vec)\n                    # normalize to constant length (seems preferable in several aspects)\n                    for i in xrange(tmp.shape[0]):\n                        tmp[i, :] *= N**0.5 / self.mahalanobis_norm(\n                                 self.sigma_vec * tmp[i, :]) / self.sigma\n                    self._Yneg *= 1 - self.sp.neg.cmuexp  # for some reason necessary?\n                    self._Yneg += dot(sp.neg.weights * tmp.T, tmp) - self.C\n                    # self.update_exponential(dot(sp.neg.weights * tmp.T, tmp) - 1 * self.C, -1*self.sp.neg.cmuexp)\n\n                self.C *= 1 - c1a - cmu\n                self.C += np.outer(c1 * self.pc, self.pc) + Y\n                self.dC[:] = np.diag(self.C)  # for output and termination checking\n\n            else:  # separable/diagonal linear case\n                assert(c1 + cmu <= 1)\n                Z = np.zeros(N)\n                for k in xrange(sp.mu):\n                    z = (pop[k] - mold) / (self.sigma * self.sigma_vec)  # TODO see above\n                    Z += sp.weights[k] * z * z  # is 1-D\n                self.C = (1 - c1a - cmu) * self.C + c1 * self.pc * self.pc + cmu * Z\n                # TODO: self.C *= exp(cmuneg * (N - dot(sp.neg.weights,  **2)\n                self.dC = self.C\n                self.D = sqrt(self.C)  # C is a 1-D array, this is why adapt_sigma needs to prepare before\n                self.itereigenupdated = self.countiter\n\n                # idx = self._mirror_idx_cov()  # take half of mirrored vectors for negative update\n\n        # step-size adaptation, adapt sigma\n        # in case of TPA, function_values[0] and [1] must reflect samples colinear to xmean - xmean_old\n        self.adapt_sigma.update(self, function_values=function_values)\n\n        if self.sigma * min(self.sigma_vec * self.dC**0.5) < self.opts['minstd']:\n            self.sigma = self.opts['minstd'] / min(self.sigma_vec * self.dC**0.5)\n        if self.sigma * max(self.sigma_vec * self.dC**0.5) > self.opts['maxstd']:\n            self.sigma = self.opts['maxstd'] / max(self.sigma_vec * self.dC**0.5)\n        # g = self.countiter\n        # N = self.N\n        # mindx = eval(self.opts['mindx'])\n        #  if isinstance(self.opts['mindx'], basestring) else self.opts['mindx']\n        if self.sigma * min(self.D) < self.opts['mindx']:  # TODO: sigma_vec is missing here\n            self.sigma = self.opts['mindx'] / min(self.D)\n\n        if self.sigma > 1e9 * self.sigma0:\n            alpha = self.sigma / max(self.D)\n            self.multiplyC(alpha)\n            self.sigma /= alpha**0.5\n            self.opts['tolupsigma'] /= alpha**0.5  # to be compared with sigma\n\n        # TODO increase sigma in case of a plateau?\n\n        # Uncertainty noise measurement is done on an upper level\n\n        # move mean into \"feasible preimage\", leads to weird behavior on\n        # 40-D tablet with bound 0.1, not quite explained (constant\n        # dragging is problematic, but why doesn't it settle), still a bug?\n        if new_injections:\n            self.pop_injection_directions = self.prepare_injection_directions()\n        self.pop_sorted = []  # remove this in case pop is still needed\n        self._flgtelldone = True", "response": "This method updates the parameters of the multivariate\n            model with the objective function values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninject a genotypic solution. The solution is used as direction relative to the distribution mean to compute a new candidate solution returned in method ask_geno which in turn is used in method ask.", "response": "def inject(self, solutions):\n        \"\"\"inject a genotypic solution. The solution is used as direction\n        relative to the distribution mean to compute a new candidate\n        solution returned in method `ask_geno` which in turn is used in\n        method `ask`.\n\n        >>> import cma\n        >>> es = cma.CMAEvolutionStrategy(4 * [1], 2)\n        >>> while not es.stop():\n        ...     es.inject([4 * [0.0]])\n        ...     X = es.ask()\n        ...     break\n        >>> assert X[0][0] == X[0][1]\n\n        \"\"\"\n        if not hasattr(self, 'pop_injection_directions'):\n            self.pop_injection_directions = []\n        for solution in solutions:\n            if len(solution) != self.N:\n                raise ValueError('method `inject` needs a list or array'\n                    + (' each el with dimension (`len`) %d' % self.N))\n            self.pop_injection_directions.append(\n                array(solution, copy=False, dtype=float) - self.mean)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef result(self):\n        # TODO: how about xcurrent?\n        return self.best.get() + (\n            self.countevals, self.countiter, self.gp.pheno(self.mean),\n            self.gp.scales * self.sigma * self.sigma_vec * self.dC**0.5)", "response": "return the result of the current system"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking sure that solutions fit to sample distribution", "response": "def clip_or_fit_solutions(self, pop, idx):\n        \"\"\"make sure that solutions fit to sample distribution, this interface will probably change.\n\n        In particular the frequency of long vectors appearing in pop[idx] - self.mean is limited.\n\n        \"\"\"\n        for k in idx:\n            self.repair_genotype(pop[k])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef repair_genotype(self, x, copy_if_changed=False):\n        x = array(x, copy=False)\n        mold = array(self.mean, copy=False)\n        if 1 < 3:  # hard clip at upper_length\n            upper_length = self.N**0.5 + 2 * self.N / (self.N + 2)  # should become an Option, but how? e.g. [0, 2, 2]\n            fac = self.mahalanobis_norm(x - mold) / upper_length\n\n            if fac > 1:\n                if copy_if_changed:\n                    x = (x - mold) / fac + mold\n                else:  # should be 25% faster:\n                    x -= mold\n                    x /= fac\n                    x += mold\n                # print self.countiter, k, fac, self.mahalanobis_norm(pop[k] - mold)\n                # adapt also sigma: which are the trust-worthy/injected solutions?\n        else:\n            if 'checktail' not in self.__dict__:  # hasattr(self, 'checktail')\n                raise NotImplementedError\n                # from check_tail_smooth import CheckTail  # for the time being\n                # self.checktail = CheckTail()\n                # print('untested feature checktail is on')\n            fac = self.checktail.addchin(self.mahalanobis_norm(x - mold))\n\n            if fac < 1:\n                x = fac * (x - mold) + mold\n\n        return x", "response": "repair the genotypes of the class with the new values"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecomposing the covariance matrix and updates self. dC self. C and self. B.", "response": "def decompose_C(self):\n        \"\"\"eigen-decompose self.C and update self.dC, self.C, self.B.\n\n        Known bugs: this might give a runtime error with\n        CMA_diagonal / separable option on.\n\n        \"\"\"\n        if self.opts['CMA_diagonal']:\n            _print_warning(\"this might fail with CMA_diagonal option on\",\n                       iteration=self.countiter)\n            print(self.opts['CMA_diagonal'])\n\n        #  print(' %.19e' % self.C[0][0])\n        self.C = (self.C + self.C.T) / 2\n        self.dC = np.diag(self.C).copy()\n        self.D, self.B = self.opts['CMA_eigenmethod'](self.C)\n #       self.B = np.round(self.B, 10)\n #       for i in rglen(self.D):\n #           d = self.D[i]\n #           oom = np.round(np.log10(d))\n #           self.D[i] = 10**oom * np.round(d / 10**oom, 10)\n        # print(' %.19e' % self.C[0][0])\n        # print(' %.19e' % self.D[0])\n        if any(self.D <= 0):\n            _print_warning(\"ERROR\", iteration=self.countiter)\n            raise ValueError(\"covariance matrix was not positive definite,\" +\n            \" this must be considered as a bug\")\n        self.D = self.D**0.5\n        assert all(isfinite(self.D))\n        idx = np.argsort(self.D)\n        self.D = self.D[idx]\n        self.B = self.B[:, idx]  # self.B[i] is a row, columns self.B[:,i] are eigenvectors\n        self.count_eigen += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate internal variables for sampling the distribution with the current covariance matrix C. This method is O ( N^3 ).", "response": "def updateBD(self):\n        \"\"\"update internal variables for sampling the distribution with the\n        current covariance matrix C. This method is O(N^3), if C is not diagonal.\n\n        \"\"\"\n        # itereigenupdated is always up-to-date in the diagonal case\n        # just double check here\n        if self.itereigenupdated == self.countiter:\n            return\n        if self.opts['CMA_diagonal'] >= self.countiter:\n            _print_warning(\"updateBD called in CMA_diagonal mode, \" +\n                           \"this should be considered a bug\", \"updateBD\",\n                           iteration=self.countiter)\n        # C has already positive updates, here come the additional negative updates\n        if self.sp.neg.cmuexp:\n            C_shrunken = (1 - self.sp.cmu - self.sp.c1)**(self.countiter - self.itereigenupdated)\n            clip_fac = 0.60  # 0.9 is sufficient to prevent degeneration in small dimension\n            if hasattr(self.opts['vv'], '__getitem__') and self.opts['vv'][0] == 'sweep_ccov_neg':\n                clip_fac = 0.98\n            if (self.countiter - self.itereigenupdated) * self.sp.neg.cmuexp * self.N \\\n                < clip_fac * C_shrunken:\n                # pos.def. guarantied, because vectors are normalized\n                self.C -= self.sp.neg.cmuexp * self._Yneg\n            else:\n                max_warns = 1\n                try:\n                    self._updateBD_warnings += 1\n                except AttributeError:\n                    self._updateBD_warnings = 1\n                if self.opts['verbose'] > 1 and \\\n                                self._updateBD_warnings <= max_warns:\n                    _print_warning('doing two additional eigen' +\n                                   'decompositions to guarantee pos.def.',\n                                   'updateBD', 'CMAEvolutionStrategy')\n                    if self._updateBD_warnings == max_warns:\n                        _print_warning('further warnings are surpressed',\n                                       'updateBD')\n                self.decompose_C()\n                _tmp_inverse_root_C = dot(self.B / self.D, self.B.T)\n                _tmp_inverse_root_C = (_tmp_inverse_root_C + _tmp_inverse_root_C.T) / 2\n                Zneg = dot(dot(_tmp_inverse_root_C, self._Yneg), _tmp_inverse_root_C)\n                eigvals, eigvecs = self.opts['CMA_eigenmethod'](Zneg)\n                self.count_eigen += 1\n                if max(eigvals) * self.sp.neg.cmuexp <= clip_fac:\n                   self.C -= self.sp.neg.cmuexp * self._Yneg\n                elif 1 < 3:\n                    self.C -= (clip_fac / max(eigvals)) * self._Yneg\n                    _print_warning(\n                        'clipped learning rate for negative weights, ' +\n                        'maximal eigenvalue = %f, maxeig * ccov = %f > %f'\n                         % (max(eigvals), max(eigvals) * self.sp.neg.cmuexp, clip_fac),\n                        iteration=self.countiter)\n                    if 1 < 3: # let's check\n                        eigvals, eigvecs = self.opts['CMA_eigenmethod'](self.C)\n                        self.count_eigen += 1\n                        print('new min eigenval = %e, old = %e'\n                              % (min(eigvals), min(self.D)**2))\n                        if min(eigvals) > 0:\n                            print('new cond = %e, old = %e'\n                                  % (max(eigvals) / min(eigvals),\n                                     (max(self.D) / min(self.D))**2))\n                else: # guaranties pos.def. unconditionally\n                    _print_warning('exponential update for negative weights (internally more expensive)',\n                                   iteration=self.countiter)\n                    self.update_exponential(self._Yneg, -self.sp.neg.cmuexp)\n                    # self.C = self.Ypos + Cs * Mh.expms(-self.sp.neg.cmuexp*Csi*self.Yneg*Csi) * Cs\n            # Yneg = self.Yneg  # for temporary debugging, can be removed\n            self._Yneg = np.zeros((self.N, self.N))\n\n        if hasattr(self.opts['vv'], '__getitem__') and self.opts['vv'][0].startswith('sweep_ccov'):\n            self.opts['CMA_const_trace'] = True\n        if self.opts['CMA_const_trace'] in (True, 1, 2):  # normalize trace of C\n            if self.opts['CMA_const_trace'] == 2:\n                s = np.exp(2 * np.mean(np.log(self.D)))  # or geom average of dC?\n            else:\n                s = np.mean(np.diag(self.C))\n            self.C /= s\n\n        dC = np.diag(self.C)\n        if max(dC) / min(dC) > 1e8:\n            # allows for much larger condition numbers, if axis-parallel\n            self.sigma_vec *= np.diag(self.C)**0.5\n            self.C = self.correlation_matrix()\n            _print_warning('condition in coordinate system exceeded 1e8' +\n                ', rescaled to 1')\n\n        # self.C = np.triu(self.C) + np.triu(self.C,1).T  # should work as well\n        # self.D, self.B = eigh(self.C) # hermitian, ie symmetric C is assumed\n\n        self.decompose_C()\n        # assert(sum(self.D-DD) < 1e-6)\n        # assert(sum(sum(np.dot(BB, BB.T)-np.eye(self.N))) < 1e-6)\n        # assert(sum(sum(np.dot(BB * DD, BB.T) - self.C)) < 1e-6)\n        # assert(all(self.B[self.countiter % self.N] == self.B[self.countiter % self.N,:]))\n\n        # qqqqqqqqqq\n        # is O(N^3)\n        # assert(sum(abs(self.C - np.dot(self.D * self.B,  self.B.T))) < N**2*1e-11)\n\n        if 1 < 3 and max(self.D) / min(self.D) > 1e6 and self.gp.isidentity:\n            # TODO: allow to do this again\n            # dmean_prev = dot(self.B, (1. / self.D) * dot(self.B.T, (self.mean - 0*self.mean_old) / self.sigma_vec))\n            self.gp._tf_matrix = (self.sigma_vec * dot(self.B * self.D, self.B.T).T).T\n            self.gp._tf_matrix_inv = (dot(self.B / self.D, self.B.T).T / self.sigma_vec).T\n            self.gp.tf_pheno = lambda x: dot(self.gp._tf_matrix, x)\n            self.gp.tf_geno = lambda x: dot(self.gp._tf_matrix_inv, x)  # not really necessary\n            self.gp.isidentity = False\n            assert self.mean is not self.mean_old\n            self.mean = self.gp.geno(self.mean)  # same as tf_geno\n            self.mean_old = self.gp.geno(self.mean_old)  # not needed?\n            self.pc = self.gp.geno(self.pc)\n            self.D[:] = 1.0\n            self.B = np.eye(self.N)\n            self.C = np.eye(self.N)\n            self.dC[:] = 1.0\n            self.sigma_vec = 1\n            # dmean_now = dot(self.B, (1. / self.D) * dot(self.B.T, (self.mean - 0*self.mean_old) / self.sigma_vec))\n            # assert Mh.vequals_approximately(dmean_now, dmean_prev)\n            _print_warning('\\n geno-pheno transformation introduced based on current C,\\n injected solutions become \"invalid\" in this iteration',\n                           'updateBD', 'CMAEvolutionStrategy', self.countiter)\n\n        self.itereigenupdated = self.countiter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiplyC(self, alpha):\n        self.C *= alpha\n        if self.dC is not self.C:\n            self.dC *= alpha\n        self.D *= alpha**0.5", "response": "multiply C with a scalar and update all related internal variables D and C"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfeeds the history of all previous candidate solutions and their respective objective function values into a new object instance.", "response": "def feedForResume(self, X, function_values):\n        \"\"\"Given all \"previous\" candidate solutions and their respective\n        function values, the state of a `CMAEvolutionStrategy` object\n        can be reconstructed from this history. This is the purpose of\n        function `feedForResume`.\n\n        Arguments\n        ---------\n            `X`\n              (all) solution points in chronological order, phenotypic\n              representation. The number of points must be a multiple\n              of popsize.\n            `function_values`\n              respective objective function values\n\n        Details\n        -------\n        `feedForResume` can be called repeatedly with only parts of\n        the history. The part must have the length of a multiple\n        of the population size.\n        `feedForResume` feeds the history in popsize-chunks into `tell`.\n        The state of the random number generator might not be\n        reconstructed, but this would be only relevant for the future.\n\n        Example\n        -------\n        ::\n\n            import cma\n\n            # prepare\n            (x0, sigma0) = ... # initial values from previous trial\n            X = ... # list of generated solutions from a previous trial\n            f = ... # respective list of f-values\n\n            # resume\n            es = cma.CMAEvolutionStrategy(x0, sigma0)\n            es.feedForResume(X, f)\n\n            # continue with func as objective function\n            while not es.stop():\n               X = es.ask()\n               es.tell(X, [func(x) for x in X])\n\n        Credits to Dirk Bueche and Fabrice Marchal for the feeding idea.\n\n        :See: class `CMAEvolutionStrategy` for a simple dump/load to resume\n\n        \"\"\"\n        if self.countiter > 0:\n            _print_warning('feed should generally be used with a new object instance')\n        if len(X) != len(function_values):\n            raise _Error('number of solutions ' + str(len(X)) +\n                ' and number function values ' +\n                str(len(function_values)) + ' must not differ')\n        popsize = self.sp.popsize\n        if (len(X) % popsize) != 0:\n            raise _Error('number of solutions ' + str(len(X)) +\n                    ' must be a multiple of popsize (lambda) ' +\n                    str(popsize))\n        for i in rglen((X) / popsize):\n            # feed in chunks of size popsize\n            self.ask()  # a fake ask, mainly for a conditioned calling of updateBD\n                        # and secondary to get possibly the same random state\n            self.tell(X[i * popsize:(i + 1) * popsize], function_values[i * popsize:(i + 1) * popsize])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mahalanobis_norm(self, dx):\n        return sqrt(sum((self.D**-1. * np.dot(self.B.T, dx / self.sigma_vec))**2)) / self.sigma", "response": "compute the Mahalanobis norm that is induced by the adapted\n        sample distribution covariance matrix times sigma ** 2 dx is the expected Mahalanobis distance to the sample mean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _metric_when_multiplied_with_sig_vec(self, sig):\n        return dot((self.B * self.D**-1.).T * sig, self.B * self.D)", "response": "return the metric when the set of sets is multiplied with a vector"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disp(self, modulo=None):  # TODO: rather assign opt['verb_disp'] as default?\n        if modulo is None:\n            modulo = self.opts['verb_disp']\n\n        # console display\n        if modulo:\n            if (self.countiter - 1) % (10 * modulo) < 1:\n                self.disp_annotation()\n            if self.countiter > 0 and (self.stop() or self.countiter < 4\n                              or self.countiter % modulo < 1):\n                if self.opts['verb_time']:\n                    toc = self.elapsed_time()\n                    stime = str(int(toc // 60)) + ':' + str(round(toc % 60, 1))\n                else:\n                    stime = ''\n                print(' '.join((repr(self.countiter).rjust(5),\n                                repr(self.countevals).rjust(6),\n                                '%.15e' % (min(self.fit.fit)),\n                                '%4.1e' % (self.D.max() / self.D.min()),\n                                '%6.2e' % self.sigma,\n                                '%6.0e' % (self.sigma * min(self.sigma_vec * sqrt(self.dC))),\n                                '%6.0e' % (self.sigma * max(self.sigma_vec * sqrt(self.dC))),\n                                stime)))\n                # if self.countiter < 4:\n                sys.stdout.flush()\n        return self", "response": "displays some single - line infos according to the given module"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef defaults():\n        return dict((str(k), str(v)) for k, v in cma_default_options.items())", "response": "return a dictionary with default option values and description"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef versatile_options():\n        return tuple(sorted(i[0] for i in list(CMAOptions.defaults().items()) if i[1].find(' #v ') > 0))", "response": "return a tuple of all options that can be changed at any time"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(self, options=None):\n        self.check_values(options)\n        self.check_attributes(options)\n        self.check_values(options)\n        return self", "response": "check for ambiguous keys and move attributes into dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking for attributes and moves them into the dictionary", "response": "def check_attributes(self, opts=None):\n        \"\"\"check for attributes and moves them into the dictionary\"\"\"\n        if opts is None:\n            opts = self\n        if 1 < 3:\n        # the problem with merge is that ``opts['ftarget'] = new_value``\n        # would be overwritten by the old ``opts.ftarget``.\n        # The solution here is to empty opts.__dict__ after the merge\n            if hasattr(opts, '__dict__'):\n                for key in list(opts.__dict__):\n                    if key in self._attributes:\n                        continue\n                    _print_warning(\n                        \"\"\"\n        An option attribute has been merged into the dictionary,\n        thereby possibly overwriting the dictionary value, and the\n        attribute has been removed. Assign options with\n\n            ``opts['%s'] = value``  # dictionary assignment\n\n        or use\n\n            ``opts.set('%s', value)  # here isinstance(opts, CMAOptions)\n\n        instead of\n\n            ``opts.%s = value``  # attribute assignment\n                        \"\"\" % (key, key, key), 'check', 'CMAOptions')\n\n                    opts[key] = opts.__dict__[key]  # getattr(opts, key)\n                    delattr(opts, key)  # is that cosher?\n                    # delattr is necessary to prevent that the attribute\n                    # overwrites the dict entry later again\n            return opts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging the attributes of this object with the attributes of the other object", "response": "def merge(self, dict_=None):\n        \"\"\"not is use so far, see check()\"\"\"\n        if dict_ is None and hasattr(self, '__dict__'):\n            dict_ = self.__dict__  \n            # doesn't work anymore as we have _lock attribute\n        if dict_ is None:\n            return self\n        self.update(dict_)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init(self, dict_or_str, val=None, warn=True):\n        # dic = dict_or_key if val is None else {dict_or_key:val}\n        self.check(dict_or_str)\n        dic = dict_or_str\n        if val is not None:\n            dic = {dict_or_str:val}\n\n        for key, val in dic.items():\n            key = self.corrected_key(key)\n            if key not in CMAOptions.defaults():\n                # TODO: find a better solution?\n                if warn:\n                    print('Warning in cma.CMAOptions.init(): key ' +\n                        str(key) + ' ignored')\n            else:\n                self[key] = val\n\n        return self", "response": "initialize one or several options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of the key in the versatile options dictionary.", "response": "def set(self, dic, val=None, force=False):\n        \"\"\"set can assign versatile options from\n        `CMAOptions.versatile_options()` with a new value, use `init()`\n        for the others.\n\n        Arguments\n        ---------\n            `dic`\n                either a dictionary or a key. In the latter\n                case, `val` must be provided\n            `val`\n                value for `key`, approximate match is sufficient\n            `force`\n                force setting of non-versatile options, use with caution\n\n        This method will be most probably used with the ``opts`` attribute of\n        a `CMAEvolutionStrategy` instance.\n\n        \"\"\"\n        if val is not None:  # dic is a key in this case\n            dic = {dic:val}  # compose a dictionary\n        for key_original, val in list(dict(dic).items()):\n            key = self.corrected_key(key_original)\n            if not self._lock_setting or \\\n                            key in CMAOptions.versatile_options():\n                self[key] = val\n            else:\n                _print_warning('key ' + str(key_original) +\n                      ' ignored (not recognized as versatile)',\n                               'set', 'CMAOptions')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd all missing options with their default values", "response": "def complement(self):\n        \"\"\"add all missing options with their default values\"\"\"\n\n        # add meta-parameters, given options have priority\n        self.check()\n        for key in CMAOptions.defaults():\n            if key not in self:\n                self[key] = CMAOptions.defaults()[key]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the subset of those options that are settable at any time.", "response": "def settable(self):\n        \"\"\"return the subset of those options that are settable at any\n        time.\n\n        Settable options are in `versatile_options()`, but the\n        list might be incomplete.\n\n        \"\"\"\n        return CMAOptions([i for i in list(self.items())\n                                if i[0] in CMAOptions.versatile_options()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the correct key for the given key", "response": "def corrected_key(self, key):\n        \"\"\"return the matching valid key, if ``key.lower()`` is a unique\n        starting sequence to identify the valid key, ``else None``\n\n        \"\"\"\n        matching_keys = []\n        for allowed_key in CMAOptions.defaults():\n            if allowed_key.lower() == key.lower():\n                return allowed_key\n            if allowed_key.lower().startswith(key.lower()):\n                matching_keys.append(allowed_key)\n        return matching_keys[0] if len(matching_keys) == 1 else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates and sets the specified option value in environment loc.", "response": "def eval(self, key, default=None, loc=None, correct_key=True):\n        \"\"\"Evaluates and sets the specified option value in\n        environment `loc`. Many options need ``N`` to be defined in\n        `loc`, some need `popsize`.\n\n        Details\n        -------\n        Keys that contain 'filename' are not evaluated.\n        For `loc` is None, the self-dict is used as environment\n\n        :See: `evalall()`, `__call__`\n\n        \"\"\"\n        # TODO: try: loc['dim'] = loc['N'] etc\n        if correct_key:\n            # in_key = key  # for debugging only\n            key = self.corrected_key(key)\n        self[key] = self(key, default, loc)\n        return self[key]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evalall(self, loc=None, defaults=None):\n        self.check()\n        if defaults is None:\n            defaults = cma_default_options\n        # TODO: this needs rather the parameter N instead of loc\n        if 'N' in loc:  # TODO: __init__ of CMA can be simplified\n            popsize = self('popsize', defaults['popsize'], loc)\n            for k in list(self.keys()):\n                k = self.corrected_key(k)\n                self.eval(k, defaults[k],\n                          {'N':loc['N'], 'popsize':popsize})\n        self._lock_setting = True\n        return self", "response": "Evaluates all option values in environment loc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all options that match in the name or the description of the user s user s company", "response": "def match(self, s=''):\n        \"\"\"return all options that match, in the name or the description,\n        with string `s`, case is disregarded.\n\n        Example: ``cma.CMAOptions().match('verb')`` returns the verbosity\n        options.\n\n        \"\"\"\n        match = s.lower()\n        res = {}\n        for k in sorted(self):\n            s = str(k) + '=\\'' + str(self[k]) + '\\''\n            if match in s.lower():\n                res[k] = self[k]\n        return CMAOptions(res, unchecked=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest termination criteria and update dictionary with new values", "response": "def _update(self, es):\n        \"\"\"Test termination criteria and update dictionary\n\n        \"\"\"\n        if es is None:\n            es = self.es\n        assert es is not None\n\n        if es.countiter == 0:  # in this case termination tests fail\n            self.__init__()\n            return self\n\n        self.lastiter = es.countiter\n        self.es = es\n\n        self.clear()  # compute conditions from scratch\n\n        N = es.N\n        opts = es.opts\n        self.opts = opts  # a hack to get _addstop going\n\n        # fitness: generic criterion, user defined w/o default\n        self._addstop('ftarget',\n                      es.best.f < opts['ftarget'])\n        # maxiter, maxfevals: generic criteria\n        self._addstop('maxfevals',\n                      es.countevals - 1 >= opts['maxfevals'])\n        self._addstop('maxiter',\n                      ## meta_parameters.maxiter_multiplier == 1.0\n                      es.countiter >= 1.0 * opts['maxiter'])\n        # tolx, tolfacupx: generic criteria\n        # tolfun, tolfunhist (CEC:tolfun includes hist)\n        self._addstop('tolx',\n                      all([es.sigma * xi < opts['tolx'] for xi in es.sigma_vec * es.pc]) and\n                      all([es.sigma * xi < opts['tolx'] for xi in es.sigma_vec * sqrt(es.dC)]))\n        self._addstop('tolfacupx',\n                      any(es.sigma * es.sigma_vec * sqrt(es.dC) >\n                          es.sigma0 * es.sigma_vec0 * opts['tolfacupx']))\n        self._addstop('tolfun',\n                      es.fit.fit[-1] - es.fit.fit[0] < opts['tolfun'] and\n                      max(es.fit.hist) - min(es.fit.hist) < opts['tolfun'])\n        self._addstop('tolfunhist',\n                      len(es.fit.hist) > 9 and\n                      max(es.fit.hist) - min(es.fit.hist) < opts['tolfunhist'])\n\n        # worst seen false positive: table N=80,lam=80, getting worse for fevals=35e3 \\approx 50 * N**1.5\n        # but the median is not so much getting worse\n        # / 5 reflects the sparsity of histbest/median\n        # / 2 reflects the left and right part to be compared\n        ## meta_parameters.tolstagnation_multiplier == 1.0\n        l = int(max(( 1.0 * opts['tolstagnation'] / 5. / 2, len(es.fit.histbest) / 10)))\n        # TODO: why max(..., len(histbest)/10) ???\n        # TODO: the problem in the beginning is only with best ==> ???\n        # equality should handle flat fitness\n        self._addstop('tolstagnation',  # leads sometimes early stop on ftablet, fcigtab, N>=50?\n                      1 < 3 and opts['tolstagnation'] and es.countiter > N * (5 + 100 / es.popsize) and\n                      len(es.fit.histbest) > 100 and 2 * l < len(es.fit.histbest) and\n                      np.median(es.fit.histmedian[:l]) >= np.median(es.fit.histmedian[l:2 * l]) and\n                      np.median(es.fit.histbest[:l]) >= np.median(es.fit.histbest[l:2 * l]))\n        # iiinteger: stagnation termination can prevent to find the optimum\n\n        self._addstop('tolupsigma', opts['tolupsigma'] and\n                      es.sigma / np.max(es.D) > es.sigma0 * opts['tolupsigma'])\n\n        if 1 < 3:\n            # non-user defined, method specific\n            # noeffectaxis (CEC: 0.1sigma), noeffectcoord (CEC:0.2sigma), conditioncov\n            idx = np.where(es.mean == es.mean + 0.2 * es.sigma *\n                                    es.sigma_vec * es.dC**0.5)[0]\n            self._addstop('noeffectcoord', any(idx), idx)\n#                         any([es.mean[i] == es.mean[i] + 0.2 * es.sigma *\n#                                                         (es.sigma_vec if isscalar(es.sigma_vec) else es.sigma_vec[i]) *\n#                                                         sqrt(es.dC[i])\n#                              for i in xrange(N)])\n#                )\n            if opts['CMA_diagonal'] is not True and es.countiter > opts['CMA_diagonal']:\n                i = es.countiter % N\n                self._addstop('noeffectaxis',\n                             sum(es.mean == es.mean + 0.1 * es.sigma * es.D[i] * es.B[:, i]) == N)\n            self._addstop('conditioncov',\n                         es.D[-1] > 1e7 * es.D[0], 1e14)  # TODO\n\n            self._addstop('callback', es.callbackstop)  # termination_callback\n        try:\n            with open(self.opts['signals_filename'], 'r') as f:\n                for line in f.readlines():\n                    words = line.split()\n                    if len(words) < 2 or words[0].startswith(('#', '%')):\n                        continue\n                    if words[0] == 'stop' and words[1] == 'now':\n                        if len(words) > 2 and not words[2].startswith(\n                                self.opts['verb_filenameprefix']):\n                            continue\n                        self._addstop('file_signal', True, \"stop now\")\n                        break\n        except IOError:\n            pass\n\n        if len(self):\n            self._addstop('flat fitness: please (re)consider how to compute the fitness more elaborate',\n                         len(es.fit.hist) > 9 and\n                         max(es.fit.hist) == min(es.fit.hist))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the parameters of the current object based on the given options.", "response": "def set(self, opts, popsize=None, ccovfac=1, verbose=True):\n        \"\"\"Compute strategy parameters as a function\n        of dimension and population size \"\"\"\n\n        alpha_cc = 1.0  # cc-correction for mueff, was zero before\n\n        def conedf(df, mu, N):\n            \"\"\"used for computing separable learning rate\"\"\"\n            return 1. / (df + 2.*sqrt(df) + float(mu) / N)\n\n        def cmudf(df, mu, alphamu):\n            \"\"\"used for computing separable learning rate\"\"\"\n            return (alphamu + mu - 2. + 1. / mu) / (df + 4.*sqrt(df) + mu / 2.)\n\n        sp = self\n        N = sp.N\n        if popsize:\n            opts.evalall({'N':N, 'popsize':popsize})\n        else:\n            popsize = opts.evalall({'N':N})['popsize']  # the default popsize is computed in CMAOptions()\n        ## meta_parameters.lambda_exponent == 0.0\n        popsize = int(popsize + N** 0.0 - 1)\n        sp.popsize = popsize\n        if opts['CMA_mirrors'] < 0.5:\n            sp.lam_mirr = int(0.5 + opts['CMA_mirrors'] * popsize)\n        elif opts['CMA_mirrors'] > 1:\n            sp.lam_mirr = int(0.5 + opts['CMA_mirrors'])\n        else:\n            sp.lam_mirr = int(0.5 + 0.16 * min((popsize, 2 * N + 2)) + 0.29)  # 0.158650... * popsize is optimal\n            # lam = arange(2,22)\n            # mirr = 0.16 + 0.29/lam\n            # print(lam); print([int(0.5 + l) for l in mirr*lam])\n            # [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n            # [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4]\n        ## meta_parameters.parent_fraction == 0.5\n        sp.mu_f = 0.5 * sp.popsize  # float value of mu\n        if opts['CMA_mu'] is not None:\n            sp.mu_f = opts['CMA_mu']\n        sp.mu = int(sp.mu_f + 0.499999)  # round down for x.5\n        sp.mu = max((sp.mu, 1))\n        # in principle we have mu_opt = popsize/2 + lam_mirr/2,\n        # which means in particular weights should only be negative for q > 0.5+mirr_frac/2\n        if sp.mu > sp.popsize - 2 * sp.lam_mirr + 1:\n            _print_warning(\"pairwise selection is not implemented, therefore \" +\n                  \" mu = %d > %d = %d - 2*%d + 1 = popsize - 2*mirr + 1 can produce a bias\" % (\n                    sp.mu, sp.popsize - 2 * sp.lam_mirr + 1, sp.popsize, sp.lam_mirr))\n        if sp.lam_mirr > sp.popsize // 2:\n            raise _Error(\"fraction of mirrors in the population as read from option CMA_mirrors cannot be larger 0.5, \" +\n                         \"theoretically optimal is 0.159\")\n        sp.weights = log(max([sp.mu, sp.popsize / 2.0]) + 0.5) - log(1 + np.arange(sp.mu))\n        sp.weights /= sum(sp.weights)\n        sp.mueff = 1 / sum(sp.weights**2)\n        # TODO: this will disappear, as it is done in class CMAAdaptSigmaCSA\n        ## meta_parameters.cs_exponent == 1.0\n        b = 1.0\n        ## meta_parameters.cs_multiplier == 1.0\n        sp.cs = 1.0 * (sp.mueff + 2)**b / (N + (sp.mueff + 3)**b)  # TODO: this doesn't change dependency of dimension\n        # sp.cs = (sp.mueff + 2) / (N + 1.5*sp.mueff + 1)\n        ## meta_parameters.cc_exponent == 1.0\n        b = 1.0\n        ## meta_parameters.cc_multiplier == 1.0\n        sp.cc = 1.0 * \\\n                (4 + alpha_cc * sp.mueff / N)**b / \\\n                (N**b + (4 + alpha_cc * 2 * sp.mueff / N)**b)\n        sp.cc_sep = (1 + 1 / N + alpha_cc * sp.mueff / N) / (N**0.5 + 1 / N + alpha_cc * 2 * sp.mueff / N)  # \\not\\gg\\cc\n        if hasattr(opts['vv'], '__getitem__') and opts['vv'][0] == 'sweep_ccov1':\n            ## meta_parameters.cc_multiplier == 1.0\n            sp.cc = 1.0 * (4 + sp.mueff / N)**0.5 / ((N + 4)**0.5 + (2 * sp.mueff / N)**0.5)\n        sp.rankmualpha = opts['CMA_rankmualpha']\n        # sp.rankmualpha = _evalOption(opts['CMA_rankmualpha'], 0.3)\n        ## meta_parameters.c1_multiplier == 1.0\n        sp.c1 = ( 1.0 * ccovfac * min(1, sp.popsize / 6) *\n                 ## meta_parameters.c1_exponent == 2.0\n                 2 / ((N + 1.3)** 2.0 + sp.mueff))\n        # 1/0\n        sp.c1_sep = ccovfac * conedf(N, sp.mueff, N)\n        if opts['CMA_rankmu'] != 0:  # also empty\n            ## meta_parameters.cmu_multiplier == 2.0\n            alphacov, mu = 2.0 , sp.mueff\n            sp.cmu = min(1 - sp.c1, ccovfac * alphacov *\n                         ## meta_parameters.cmu_exponent == 2.0\n                         (sp.rankmualpha + mu - 2 + 1 / mu) / ((N + 2)** 2.0 + alphacov * mu / 2))\n            if hasattr(opts['vv'], '__getitem__') and opts['vv'][0] == 'sweep_ccov':\n                sp.cmu = opts['vv'][1]\n            sp.cmu_sep = min(1 - sp.c1_sep, ccovfac * cmudf(N, sp.mueff, sp.rankmualpha))\n        else:\n            sp.cmu = sp.cmu_sep = 0\n        if hasattr(opts['vv'], '__getitem__') and opts['vv'][0] == 'sweep_ccov1':\n            sp.c1 = opts['vv'][1]\n\n        sp.neg = _BlancClass()\n        if opts['CMA_active'] and opts['CMA_on']:\n            # in principle we have mu_opt = popsize/2 + lam_mirr/2,\n            # which means in particular weights should only be negative for q > 0.5+mirr_frac/2\n            if 1 < 3: # seems most natural: continuation of log(lambda/2) - log(k) qqqqqqqqqqqqqqqqqqqqqqqqqq\n                sp.neg.mu_f = popsize // 2  # not sure anymore what this is good for\n                sp.neg.weights = array([log(k) - log(popsize/2 + 1/2) for k in np.arange(np.ceil(popsize/2 + 1.1/2), popsize + .1)])\n            sp.neg.mu = len(sp.neg.weights)\n            sp.neg.weights /= sum(sp.neg.weights)\n            sp.neg.mueff = 1 / sum(sp.neg.weights**2)\n            ## meta_parameters.cact_exponent == 1.5\n            sp.neg.cmuexp = opts['CMA_active'] * 0.3 * sp.neg.mueff / ((N + 2)** 1.5 + 1.0 * sp.neg.mueff)\n            if hasattr(opts['vv'], '__getitem__') and opts['vv'][0] == 'sweep_ccov_neg':\n                sp.neg.cmuexp = opts['vv'][1]\n            # reasoning on learning rate cmuexp: with sum |w| == 1 and\n            #   length-normalized vectors in the update, the residual\n            #   variance in any direction exceeds exp(-N*cmuexp)\n            assert sp.neg.mu >= sp.lam_mirr  # not really necessary\n            # sp.neg.minresidualvariance = 0.66  # not it use, keep at least 0.66 in all directions, small popsize is most critical\n        else:\n            sp.neg.cmuexp = 0\n\n        sp.CMA_on = sp.c1 + sp.cmu > 0\n        # print(sp.c1_sep / sp.cc_sep)\n\n        if not opts['CMA_on'] and opts['CMA_on'] not in (None, [], (), ''):\n            sp.CMA_on = False\n            # sp.c1 = sp.cmu = sp.c1_sep = sp.cmu_sep = 0\n        mueff_exponent = 0.5\n        if 1 < 3:\n            mueff_exponent = opts['CSA_damp_mueff_exponent']\n        # TODO: this will disappear, as it is done in class CMAAdaptSigmaCSA\n        sp.damps = opts['CSA_dampfac'] * (0.5 +\n                                          0.5 * min([1, (sp.lam_mirr / (0.159 * sp.popsize) - 1)**2])**1 +\n                                          2 * max([0, ((sp.mueff - 1) / (N + 1))**mueff_exponent - 1]) + sp.cs\n                                          )\n        sp.cmean = float(opts['CMA_cmean'])\n        # sp.kappa = 1  # 4-D, lam=16, rank1, kappa < 4 does not influence convergence rate\n                        # in larger dim it does, 15-D with defaults, kappa=8 factor 2\n        if verbose:\n            if not sp.CMA_on:\n                print('covariance matrix adaptation turned off')\n            if opts['CMA_mu'] != None:\n                print('mu = %f' % (sp.mu_f))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns dictionary with data.", "response": "def data(self):\n        \"\"\"return dictionary with data.\n\n        If data entries are None or incomplete, consider calling\n        ``.load().data()`` to (re-)load the data from files first.\n\n        \"\"\"\n        d = {}\n        for name in self.key_names:\n            d[name] = self.__dict__.get(name, None)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(self, es, append=None, modulo=None):\n        if not isinstance(es, CMAEvolutionStrategy):\n            raise TypeError(\"only class CMAEvolutionStrategy can be \" +\n                            \"registered for logging\")\n        self.es = es\n        if append is not None:\n            self.append = append\n        if modulo is not None:\n            self.modulo = modulo\n        self.registered = True\n        return self", "response": "register a CMAEvolutionStrategy instance for logging"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, modulo=None):\n        if modulo is not None:\n            self.modulo = modulo\n        try:\n            es = self.es  # must have been registered\n        except AttributeError:\n            pass  # TODO: revise usage of es... that this can pass\n            raise _Error('call register() before initialize()')\n\n        self.counter = 0  # number of calls of add\n        self.last_iteration = 0  # some lines are only written if iteration>last_iteration\n\n        # write headers for output\n        fn = self.name_prefix + 'fit.dat'\n        strseedtime = 'seed=%d, %s' % (es.opts['seed'], time.asctime())\n\n        try:\n            with open(fn, 'w') as f:\n                f.write('% # columns=\"iteration, evaluation, sigma, axis ratio, ' +\n                        'bestever, best, median, worst objective function value, ' +\n                        'further objective values of best\", ' +\n                        strseedtime +\n                        # strftime(\"%Y/%m/%d %H:%M:%S\", localtime()) + # just asctime() would do\n                        '\\n')\n        except (IOError, OSError):\n            print('could not open file ' + fn)\n\n        fn = self.name_prefix + 'axlen.dat'\n        try:\n            with open(fn, 'w') as f:\n                f.write('%  columns=\"iteration, evaluation, sigma, ' +\n                        'max axis length, ' +\n                        ' min axis length, all principle axes lengths ' +\n                        ' (sorted square roots of eigenvalues of C)\", ' +\n                        strseedtime +\n                        '\\n')\n        except (IOError, OSError):\n            print('could not open/write file ' + fn)\n        fn = self.name_prefix + 'axlencorr.dat'\n        try:\n            with open(fn, 'w') as f:\n                f.write('%  columns=\"iteration, evaluation, min max(neg(.)) min(pos(.))' +\n                        ' max correlation, correlation matrix principle axes lengths ' +\n                        ' (sorted square roots of eigenvalues of correlation matrix)\", ' +\n                        strseedtime +\n                        '\\n')\n        except (IOError, OSError):\n            print('could not open file ' + fn)\n        fn = self.name_prefix + 'stddev.dat'\n        try:\n            with open(fn, 'w') as f:\n                f.write('% # columns=[\"iteration, evaluation, sigma, void, void, ' +\n                        ' stds==sigma*sqrt(diag(C))\", ' +\n                        strseedtime +\n                        '\\n')\n        except (IOError, OSError):\n            print('could not open file ' + fn)\n\n        fn = self.name_prefix + 'xmean.dat'\n        try:\n            with open(fn, 'w') as f:\n                f.write('% # columns=\"iteration, evaluation, void, void, void, xmean\", ' +\n                        strseedtime)\n                f.write(' # scaling_of_variables: ')\n                if np.size(es.gp.scales) > 1:\n                    f.write(' '.join(map(str, es.gp.scales)))\n                else:\n                    f.write(str(es.gp.scales))\n                f.write(', typical_x: ')\n                if np.size(es.gp.typical_x) > 1:\n                    f.write(' '.join(map(str, es.gp.typical_x)))\n                else:\n                    f.write(str(es.gp.typical_x))\n                f.write('\\n')\n        except (IOError, OSError):\n            print('could not open/write file ' + fn)\n\n        fn = self.name_prefix + 'xrecentbest.dat'\n        try:\n            with open(fn, 'w') as f:\n                f.write('% # iter+eval+sigma+0+fitness+xbest, ' +\n                        strseedtime +\n                        '\\n')\n        except (IOError, OSError):\n            print('could not open/write file ' + fn)\n\n        return self", "response": "reset logger overwrite original files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, filenameprefix=None):\n        if not filenameprefix:\n            filenameprefix = self.name_prefix\n        assert len(self.file_names) == len(self.key_names)\n        for i in rglen((self.file_names)):\n            fn = filenameprefix + self.file_names[i] + '.dat'\n            try:\n                self.__dict__[self.key_names[i]] = _fileToMatrix(fn)\n            except:\n                _print_warning('reading from file \"' + fn + '\" failed',\n                               'load', 'CMADataLogger')\n            try:\n                if self.key_names[i] in self._key_names_with_annotation:\n                    # copy last row to later fill in annotation position for display\n                    self.__dict__[self.key_names[i]].append(\n                        self.__dict__[self.key_names[i]][-1])\n                self.__dict__[self.key_names[i]] = \\\n                    array(self.__dict__[self.key_names[i]], copy=False)\n            except:\n                _print_warning('no data for %s' % fn, 'load',\n                               'CMADataLogger')\n        return self", "response": "load ( or reload ) data from output files and return self with ( added ) attributes xrecent mean f D std and correlationspec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd some data to the current CMADataLogger instance.", "response": "def add(self, es=None, more_data=[], modulo=None):\n        \"\"\"append some logging data from `CMAEvolutionStrategy` class instance `es`,\n        if ``number_of_times_called % modulo`` equals to zero, never if ``modulo==0``.\n\n        The sequence ``more_data`` must always have the same length.\n\n        When used for a different optimizer class, this function can be\n        (easily?) adapted by changing the assignments under INTERFACE\n        in the implemention.\n\n        \"\"\"\n        mod = modulo if modulo is not None else self.modulo\n        self.counter += 1\n        if mod == 0 or (self.counter > 3 and (self.counter - 1) % mod):\n            return\n        if es is None:\n            try:\n                es = self.es  # must have been registered\n            except AttributeError :\n                raise _Error('call `add` with argument `es` or ``register(es)`` before ``add()``')\n        elif not self.registered:\n            self.register(es)\n\n        if 1 < 3:\n            if self.counter == 1 and not self.append and self.modulo != 0:\n                self.initialize()  # write file headers\n                self.counter = 1\n\n        # --- INTERFACE, can be changed if necessary ---\n        if not isinstance(es, CMAEvolutionStrategy):  # not necessary\n            _print_warning('type CMAEvolutionStrategy expected, found '\n                           + str(type(es)), 'add', 'CMADataLogger')\n        evals = es.countevals\n        iteration = es.countiter\n        eigen_decompositions = es.count_eigen\n        sigma = es.sigma\n        axratio = es.D.max() / es.D.min()\n        xmean = es.mean  # TODO: should be optionally phenotype?\n        fmean_noise_free = es.fmean_noise_free\n        fmean = es.fmean\n        # TODO: find a different way to communicate current x and f?\n        try:\n            besteverf = es.best.f\n            bestf = es.fit.fit[0]\n            worstf = es.fit.fit[-1]\n            medianf = es.fit.fit[es.sp.popsize // 2]\n        except:\n            if iteration > 0:  # first call without f-values is OK\n                raise\n        try:\n            xrecent = es.best.last.x\n        except:\n            xrecent = None\n        maxD = es.D.max()\n        minD = es.D.min()\n        diagD = es.D\n        diagC = es.sigma * es.sigma_vec * sqrt(es.dC)\n        more_to_write = es.more_to_write\n        es.more_to_write = []\n        # --- end interface ---\n\n        try:\n            # fit\n            if iteration > self.last_iteration:\n                fn = self.name_prefix + 'fit.dat'\n                with open(fn, 'a') as f:\n                    f.write(str(iteration) + ' '\n                            + str(evals) + ' '\n                            + str(sigma) + ' '\n                            + str(axratio) + ' '\n                            + str(besteverf) + ' '\n                            + '%.16e' % bestf + ' '\n                            + str(medianf) + ' '\n                            + str(worstf) + ' '\n                            # + str(es.sp.popsize) + ' '\n                            # + str(10**es.noiseS) + ' '\n                            # + str(es.sp.cmean) + ' '\n                            + ' '.join(str(i) for i in more_to_write) + ' '\n                            + ' '.join(str(i) for i in more_data) + ' '\n                            + '\\n')\n            # axlen\n            fn = self.name_prefix + 'axlen.dat'\n            if 1 < 3:\n                with open(fn, 'a') as f:  # does not rely on reference counting\n                    f.write(str(iteration) + ' '\n                            + str(evals) + ' '\n                            + str(sigma) + ' '\n                            + str(maxD) + ' '\n                            + str(minD) + ' '\n                            + ' '.join(map(str, diagD))\n                            + '\\n')\n            # correlation matrix eigenvalues\n            if 1 < 3:\n                fn = self.name_prefix + 'axlencorr.dat'\n                c = es.correlation_matrix()\n                if c is not None:\n                    # accept at most 50% internal loss\n                    if self._eigen_counter < eigen_decompositions / 2:\n                        self.last_correlation_spectrum = \\\n                            sorted(es.opts['CMA_eigenmethod'](c)[0]**0.5)\n                        self._eigen_counter += 1\n                    if self.last_correlation_spectrum is None:\n                        self.last_correlation_spectrum = len(diagD) * [1]\n                    c = c[c < 1 - 1e-14]  # remove diagonal elements\n                    c[c > 1 - 1e-14] = 1 - 1e-14\n                    c[c < -1 + 1e-14] = -1 + 1e-14\n                    c_min = np.min(c)\n                    c_max = np.max(c)\n                    if np.min(abs(c)) == 0:\n                        c_medminus = 0  # thereby zero \"is negative\"\n                        c_medplus = 0  # thereby zero \"is positive\"\n                    else:\n                        c_medminus = c[np.argmin(1/c)]  # c is flat\n                        c_medplus = c[np.argmax(1/c)]  # c is flat\n\n                    with open(fn, 'a') as f:\n                        f.write(str(iteration) + ' '\n                                + str(evals) + ' '\n                                + str(c_min) + ' '\n                                + str(c_medminus) + ' ' # the one closest to 0\n                                + str(c_medplus) + ' ' # the one closest to 0\n                                + str(c_max) + ' '\n                                + ' '.join(map(str,\n                                        self.last_correlation_spectrum))\n                                + '\\n')\n\n            # stddev\n            fn = self.name_prefix + 'stddev.dat'\n            with open(fn, 'a') as f:\n                f.write(str(iteration) + ' '\n                        + str(evals) + ' '\n                        + str(sigma) + ' '\n                        + '0 0 '\n                        + ' '.join(map(str, diagC))\n                        + '\\n')\n            # xmean\n            fn = self.name_prefix + 'xmean.dat'\n            with open(fn, 'a') as f:\n                f.write(str(iteration) + ' '\n                        + str(evals) + ' '\n                        # + str(sigma) + ' '\n                        + '0 '\n                        + str(fmean_noise_free) + ' '\n                        + str(fmean) + ' '  # TODO: this does not make sense\n                        # TODO should be optional the phenotyp?\n                        + ' '.join(map(str, xmean))\n                        + '\\n')\n            # xrecent\n            fn = self.name_prefix + 'xrecentbest.dat'\n            if iteration > 0 and xrecent is not None:\n                with open(fn, 'a') as f:\n                    f.write(str(iteration) + ' '\n                            + str(evals) + ' '\n                            + str(sigma) + ' '\n                            + '0 '\n                            + str(bestf) + ' '\n                            + ' '.join(map(str, xrecent))\n                            + '\\n')\n        except (IOError, OSError):\n            if iteration <= 1:\n                _print_warning(('could not open/write file %s: ' % fn,\n                                sys.exc_info()))\n        self.last_iteration = iteration"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_to(self, nameprefix, switch=False):\n        if not nameprefix or not isinstance(nameprefix, basestring):\n            raise _Error('filename prefix must be a nonempty string')\n\n        if nameprefix == self.default_prefix:\n            raise _Error('cannot save to default name \"' + nameprefix + '...\", chose another name')\n\n        if nameprefix == self.name_prefix:\n            return\n\n        for name in self.file_names:\n            open(nameprefix + name + '.dat', 'w').write(open(self.name_prefix + name + '.dat').read())\n\n        if switch:\n            self.name_prefix = nameprefix", "response": "saves the logger data to a different set of files for the specified name prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps only data of iteration_indices", "response": "def select_data(self, iteration_indices):\n        \"\"\"keep only data of `iteration_indices`\"\"\"\n        dat = self\n        iteridx = iteration_indices\n        dat.f = dat.f[np.where([x in iteridx for x in dat.f[:, 0]])[0], :]\n        dat.D = dat.D[np.where([x in iteridx for x in dat.D[:, 0]])[0], :]\n        try:\n            iteridx = list(iteridx)\n            iteridx.append(iteridx[-1])  # last entry is artificial\n        except:\n            pass\n        dat.std = dat.std[np.where([x in iteridx\n                                    for x in dat.std[:, 0]])[0], :]\n        dat.xmean = dat.xmean[np.where([x in iteridx\n                                        for x in dat.xmean[:, 0]])[0], :]\n        try:\n            dat.xrecent = dat.x[np.where([x in iteridx for x in\n                                          dat.xrecent[:, 0]])[0], :]\n        except AttributeError:\n            pass\n        try:\n            dat.corrspec = dat.x[np.where([x in iteridx for x in\n                                           dat.corrspec[:, 0]])[0], :]\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, fig=None, iabscissa=1, iteridx=None,\n             plot_mean=False, # was: plot_mean=True\n             foffset=1e-19, x_opt=None, fontsize=9):\n        \"\"\"plot data from a `CMADataLogger` (using the files written \n        by the logger).\n\n        Arguments\n        ---------\n            `fig`\n                figure number, by default 325\n            `iabscissa`\n                ``0==plot`` versus iteration count,\n                ``1==plot`` versus function evaluation number\n            `iteridx`\n                iteration indices to plot\n\n        Return `CMADataLogger` itself.\n\n        Examples\n        --------\n        ::\n\n            import cma\n            logger = cma.CMADataLogger()  # with default name\n            # try to plot the \"default logging\" data (e.g.\n            #   from previous fmin calls, which is essentially what\n            #   also cma.plot() does)\n            logger.plot()\n            cma.savefig('fig325.png')  # save current figure\n            logger.closefig()\n\n        Dependencies: matlabplotlib/pyplot.\n\n        \"\"\"\n        try:\n            # pyplot: prodedural interface for matplotlib\n            from matplotlib.pyplot import figure, subplot, hold, gcf\n        except ImportError:\n            ImportError('could not find matplotlib.pyplot module, function plot() is not available')\n            return\n\n        if fig is None:\n            fig = 325\n        if iabscissa not in (0, 1):\n            iabscissa = 1\n\n        self.load()  # better load only conditionally?\n        dat = self\n        dat.x = dat.xmean  # this is the genotyp\n        if not plot_mean:\n            if len(dat.x) < 2:\n                print('not enough data to plot recent x')\n            else:\n                dat.x = dat.xrecent\n\n        # index out some data\n        if iteridx is not None:\n            self.select_data(iteridx)\n\n        if len(dat.f) <= 1:\n            print('nothing to plot')\n            return\n\n        # not in use anymore, see formatter above\n        # xticklocs = np.arange(5) * np.round(minxend/4., -int(np.log10(minxend/4.)))\n\n        # dfit(dfit<1e-98) = NaN;\n\n        # TODO: if abscissa==0 plot in chunks, ie loop over subsets where\n        # dat.f[:,0]==countiter is monotonous\n\n        figure(fig)\n        self._enter_plotting(fontsize)\n        self.fighandle = gcf()  # fighandle.number\n\n        subplot(2, 2, 1)\n        self.plot_divers(iabscissa, foffset)\n        pyplot.xlabel('')\n\n        # Scaling\n        subplot(2, 2, 3)\n        self.plot_axes_scaling(iabscissa)\n\n        # spectrum of correlation matrix\n        figure(fig)\n\n        subplot(2, 2, 2)\n        if plot_mean:\n            self.plot_mean(iabscissa, x_opt)\n        else:\n            self.plot_xrecent(iabscissa, x_opt)\n        pyplot.xlabel('')\n        # pyplot.xticks(xticklocs)\n\n        # standard deviations\n        subplot(2, 2, 4)\n        self.plot_stds(iabscissa)\n        self._finalize_plotting()\n        return self", "response": "plot data from a CMADataLogger"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot all data for a specific iabscissa and iteration count.", "response": "def plot_all(self, fig=None, iabscissa=1, iteridx=None,\n             foffset=1e-19, x_opt=None, fontsize=9):\n        \"\"\"\n        plot data from a `CMADataLogger` (using the files written by the logger).\n\n        Arguments\n        ---------\n            `fig`\n                figure number, by default 425\n            `iabscissa`\n                ``0==plot`` versus iteration count,\n                ``1==plot`` versus function evaluation number\n            `iteridx`\n                iteration indices to plot\n\n        Return `CMADataLogger` itself.\n\n        Examples\n        --------\n        ::\n\n            import cma\n            logger = cma.CMADataLogger()  # with default name\n            # try to plot the \"default logging\" data (e.g.\n            #   from previous fmin calls, which is essentially what\n            #   also cma.plot() does)\n            logger.plot_all()\n            cma.savefig('fig425.png')  # save current figure\n            logger.closefig()\n\n        Dependencies: matlabplotlib/pyplot.\n\n        \"\"\"\n        try:\n            # pyplot: prodedural interface for matplotlib\n            from  matplotlib.pyplot import figure, subplot, gcf\n        except ImportError:\n            ImportError('could not find matplotlib.pyplot module, function plot() is not available')\n            return\n\n        if fig is None:\n            fig = 426\n        if iabscissa not in (0, 1):\n            iabscissa = 1\n\n        self.load()\n        dat = self\n\n        # index out some data\n        if iteridx is not None:\n            self.select_data(iteridx)\n\n        if len(dat.f) == 0:\n            print('nothing to plot')\n            return\n\n        # not in use anymore, see formatter above\n        # xticklocs = np.arange(5) * np.round(minxend/4., -int(np.log10(minxend/4.)))\n\n        # dfit(dfit<1e-98) = NaN;\n\n        # TODO: if abscissa==0 plot in chunks, ie loop over subsets where dat.f[:,0]==countiter is monotonous\n\n        figure(fig)\n        self._enter_plotting(fontsize)\n        self.fighandle = gcf()  # fighandle.number\n\n        if 1 < 3:\n            subplot(2, 3, 1)\n            self.plot_divers(iabscissa, foffset)\n            pyplot.xlabel('')\n\n            # standard deviations\n            subplot(2, 3, 4)\n            self.plot_stds(iabscissa)\n\n            # Scaling\n            subplot(2, 3, 2)\n            self.plot_axes_scaling(iabscissa)\n            pyplot.xlabel('')\n\n            # spectrum of correlation matrix\n            subplot(2, 3, 5)\n            self.plot_correlations(iabscissa)\n\n            # x-vectors\n            subplot(2, 3, 3)\n            self.plot_xrecent(iabscissa, x_opt)\n            pyplot.xlabel('')\n\n            subplot(2, 3, 6)\n            self.plot_mean(iabscissa, x_opt)\n\n        self._finalize_plotting()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the spectrum of correlation matrix and largest correlation", "response": "def plot_correlations(self, iabscissa=1):\n        \"\"\"spectrum of correlation matrix and largest correlation\"\"\"\n        if not hasattr(self, 'corrspec'):\n            self.load()\n        if len(self.corrspec) < 2:\n            return self\n        x = self.corrspec[:, iabscissa]\n        y = self.corrspec[:, 6:]  # principle axes\n        ys = self.corrspec[:, :6]  # \"special\" values\n\n        from matplotlib.pyplot import semilogy, hold, text, grid, axis, title\n        self._enter_plotting()\n        semilogy(x, y, '-c')\n        hold(True)\n        semilogy(x[:], np.max(y, 1) / np.min(y, 1), '-r')\n        text(x[-1], np.max(y[-1, :]) / np.min(y[-1, :]), 'axis ratio')\n        if ys is not None:\n            semilogy(x, 1 + ys[:, 2], '-b')\n            text(x[-1], 1 + ys[-1, 2], '1 + min(corr)')\n            semilogy(x, 1 - ys[:, 5], '-b')\n            text(x[-1], 1 - ys[-1, 5], '1 - max(corr)')\n            semilogy(x[:], 1 + ys[:, 3], '-k')\n            text(x[-1], 1 + ys[-1, 3], '1 + max(neg corr)')\n            semilogy(x[:], 1 - ys[:, 4], '-k')\n            text(x[-1], 1 - ys[-1, 4], '1 - min(pos corr)')\n        grid(True)\n        ax = array(axis())\n        # ax[1] = max(minxend, ax[1])\n        axis(ax)\n        title('Spectrum (roots) of correlation matrix')\n        # pyplot.xticks(xticklocs)\n        self._xlabel(iabscissa)\n        self._finalize_plotting()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_divers(self, iabscissa=1, foffset=1e-19):\n        from matplotlib.pyplot import semilogy, hold, grid, \\\n            axis, title, text\n        fontsize = pyplot.rcParams['font.size']\n\n        if not hasattr(self, 'f'):\n            self.load()\n        dat = self\n\n        minfit = min(dat.f[:, 5])\n        dfit = dat.f[:, 5] - minfit  # why not using idx?\n        dfit[dfit < 1e-98] = np.NaN\n\n        self._enter_plotting()\n        if dat.f.shape[1] > 7:\n            # semilogy(dat.f[:, iabscissa], abs(dat.f[:,[6, 7, 10, 12]])+foffset,'-k')\n            semilogy(dat.f[:, iabscissa], abs(dat.f[:, [6, 7]]) + foffset, '-k')\n            hold(True)\n\n        # (larger indices): additional fitness data, for example constraints values\n        if dat.f.shape[1] > 8:\n            # dd = abs(dat.f[:,7:]) + 10*foffset\n            # dd = np.where(dat.f[:,7:]==0, np.NaN, dd) # cannot be\n            semilogy(dat.f[:, iabscissa], np.abs(dat.f[:, 8:]) + 10 * foffset, 'y')\n            hold(True)\n\n        idx = np.where(dat.f[:, 5] > 1e-98)[0]  # positive values\n        semilogy(dat.f[idx, iabscissa], dat.f[idx, 5] + foffset, '.b')\n        hold(True)\n        grid(True)\n\n\n        semilogy(dat.f[:, iabscissa], abs(dat.f[:, 5]) + foffset, '-b')\n        text(dat.f[-1, iabscissa], abs(dat.f[-1, 5]) + foffset,\n             r'$|f_\\mathsf{best}|$', fontsize=fontsize + 2)\n\n        # negative f-values, dots\n        sgn = np.sign(dat.f[:, 5])\n        sgn[np.abs(dat.f[:, 5]) < 1e-98] = 0\n        idx = np.where(sgn < 0)[0]\n        semilogy(dat.f[idx, iabscissa], abs(dat.f[idx, 5]) + foffset,\n                 '.m')  # , markersize=5\n\n        # lines between negative f-values\n        dsgn = np.diff(sgn)\n        start_idx = 1 + np.where((dsgn < 0) * (sgn[1:] < 0))[0]\n        stop_idx = 1 + np.where(dsgn > 0)[0]\n        if sgn[0] < 0:\n            start_idx = np.concatenate(([0], start_idx))\n        for istart in start_idx:\n            istop = stop_idx[stop_idx > istart]\n            istop = istop[0] if len(istop) else 0\n            idx = xrange(istart, istop if istop else dat.f.shape[0])\n            if len(idx) > 1:\n                semilogy(dat.f[idx, iabscissa], abs(dat.f[idx, 5]) + foffset,\n                        'm')  # , markersize=5\n            # lines between positive and negative f-values\n            # TODO: the following might plot values very close to zero\n            if istart > 0:  # line to the left of istart\n                semilogy(dat.f[istart-1:istart+1, iabscissa],\n                         abs(dat.f[istart-1:istart+1, 5]) +\n                         foffset, '--m')\n            if istop:  # line to the left of istop\n                semilogy(dat.f[istop-1:istop+1, iabscissa],\n                         abs(dat.f[istop-1:istop+1, 5]) +\n                         foffset, '--m')\n                # mark the respective first positive values\n                semilogy(dat.f[istop, iabscissa], abs(dat.f[istop, 5]) +\n                         foffset, '.b', markersize=7)\n            # mark the respective first negative values\n            semilogy(dat.f[istart, iabscissa], abs(dat.f[istart, 5]) +\n                     foffset, '.r', markersize=7)\n\n        # standard deviations std\n        semilogy(dat.std[:-1, iabscissa],\n                 np.vstack([list(map(max, dat.std[:-1, 5:])),\n                            list(map(min, dat.std[:-1, 5:]))]).T,\n                     '-m', linewidth=2)\n        text(dat.std[-2, iabscissa], max(dat.std[-2, 5:]), 'max std',\n             fontsize=fontsize)\n        text(dat.std[-2, iabscissa], min(dat.std[-2, 5:]), 'min std',\n             fontsize=fontsize)\n\n        # delta-fitness in cyan\n        idx = isfinite(dfit)\n        if 1 < 3:\n            idx_nan = np.where(idx == False)[0]  # gaps\n            if not len(idx_nan):  # should never happen\n                semilogy(dat.f[:, iabscissa][idx], dfit[idx], '-c')\n            else:\n                i_start = 0\n                for i_end in idx_nan:\n                    if i_end > i_start:\n                        semilogy(dat.f[:, iabscissa][i_start:i_end],\n                                                dfit[i_start:i_end], '-c')\n                i_start = i_end + 1\n                if len(dfit) > idx_nan[-1] + 1:\n                    semilogy(dat.f[:, iabscissa][idx_nan[-1]+1:],\n                                            dfit[idx_nan[-1]+1:], '-c')\n            text(dat.f[idx, iabscissa][-1], dfit[idx][-1],\n                 r'$f_\\mathsf{best} - \\min(f)$', fontsize=fontsize + 2)\n\n        # overall minimum\n        i = np.argmin(dat.f[:, 5])\n        semilogy(dat.f[i, iabscissa], np.abs(dat.f[i, 5]), 'ro',\n                 markersize=9)\n        semilogy(dat.f[i, iabscissa], dfit[idx][np.argmin(dfit[idx])]\n                 + 1e-98, 'ro', markersize=9)\n        # semilogy(dat.f[-1, iabscissa]*np.ones(2), dat.f[-1,4]*np.ones(2), 'rd')\n\n        # AR and sigma\n        semilogy(dat.f[:, iabscissa], dat.f[:, 3], '-r')  # AR\n        semilogy(dat.f[:, iabscissa], dat.f[:, 2], '-g')  # sigma\n        text(dat.f[-1, iabscissa], dat.f[-1, 3], r'axis ratio',\n             fontsize=fontsize)\n        text(dat.f[-1, iabscissa], dat.f[-1, 2] / 1.5, r'$\\sigma$',\n             fontsize=fontsize+3)\n        ax = array(axis())\n        # ax[1] = max(minxend, ax[1])\n        axis(ax)\n        text(ax[0] + 0.01, ax[2],  # 10**(log10(ax[2])+0.05*(log10(ax[3])-log10(ax[2]))),\n             '.min($f$)=' + repr(minfit))\n             #'.f_recent=' + repr(dat.f[-1, 5]))\n\n        # title('abs(f) (blue), f-min(f) (cyan), Sigma (green), Axis Ratio (red)')\n        # title(r'blue:$\\mathrm{abs}(f)$, cyan:$f - \\min(f)$, green:$\\sigma$, red:axis ratio',\n        #       fontsize=fontsize - 0.0)\n        title(r'$|f_{\\mathrm{best},\\mathrm{med},\\mathrm{worst}}|$, $f - \\min(f)$, $\\sigma$, axis ratio')\n\n        # if __name__ != 'cma':  # should be handled by the caller\n        self._xlabel(iabscissa)\n        self._finalize_plotting()\n        return self", "response": "plot the divers of the fitness data for the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _enter_plotting(self, fontsize=9):\n        # interactive_status = matplotlib.is_interactive()\n        self.original_fontsize = pyplot.rcParams['font.size']\n        pyplot.rcParams['font.size'] = fontsize\n        pyplot.hold(False)  # opens a figure window, if non exists\n        pyplot.ioff()", "response": "assumes that a figure is open"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the x - attributed entry of the current object.", "response": "def _plot_x(self, iabscissa=1, x_opt=None, remark=None,\n                annotations=None):\n        \"\"\"If ``x_opt is not None`` the difference to x_opt is plotted\n        in log scale\n\n        \"\"\"\n        if not hasattr(self, 'x'):\n            _print_warning('no x-attributed found, use methods ' +\n                           'plot_xrecent or plot_mean', 'plot_x',\n                           'CMADataLogger')\n            return\n        from matplotlib.pyplot import plot, semilogy, hold, text, grid, axis, title\n        dat = self  # for convenience and historical reasons\n        # modify fake last entry in x for line extension-annotation\n        if dat.x.shape[1] < 100:\n            minxend = int(1.06 * dat.x[-2, iabscissa])\n            # write y-values for individual annotation into dat.x\n            dat.x[-1, iabscissa] = minxend  # TODO: should be ax[1]\n            if x_opt is None:\n                idx = np.argsort(dat.x[-2, 5:])\n                idx2 = np.argsort(idx)\n                dat.x[-1, 5 + idx] = np.linspace(np.min(dat.x[:, 5:]),\n                            np.max(dat.x[:, 5:]), dat.x.shape[1] - 5)\n            else: # y-axis is in log\n                xdat = np.abs(dat.x[:, 5:] - np.array(x_opt, copy=False))\n                idx = np.argsort(xdat[-2, :])\n                idx2 = np.argsort(idx)\n                xdat[-1, idx] = np.logspace(np.log10(np.min(abs(xdat[xdat!=0]))),\n                            np.log10(np.max(np.abs(xdat))),\n                            dat.x.shape[1] - 5)\n        else:\n            minxend = 0\n        self._enter_plotting()\n        if x_opt is not None:  # TODO: differentate neg and pos?\n            semilogy(dat.x[:, iabscissa], abs(xdat), '-')\n        else:\n            plot(dat.x[:, iabscissa], dat.x[:, 5:], '-')\n        hold(True)\n        grid(True)\n        ax = array(axis())\n        # ax[1] = max(minxend, ax[1])\n        axis(ax)\n        ax[1] -= 1e-6  # to prevent last x-tick annotation, probably superfluous\n        if dat.x.shape[1] < 100:\n            yy = np.linspace(ax[2] + 1e-6, ax[3] - 1e-6, dat.x.shape[1] - 5)\n            # yyl = np.sort(dat.x[-1,5:])\n            if x_opt is not None:\n                # semilogy([dat.x[-1, iabscissa], ax[1]], [abs(dat.x[-1, 5:]), yy[idx2]], 'k-')  # line from last data point\n                semilogy(np.dot(dat.x[-2, iabscissa], [1, 1]),\n                         array([ax[2] * (1+1e-6), ax[3] / (1+1e-6)]), 'k-')\n            else:\n                # plot([dat.x[-1, iabscissa], ax[1]], [dat.x[-1,5:], yy[idx2]], 'k-') # line from last data point\n                plot(np.dot(dat.x[-2, iabscissa], [1, 1]),\n                     array([ax[2] + 1e-6, ax[3] - 1e-6]), 'k-')\n            # plot(array([dat.x[-1, iabscissa], ax[1]]),\n            #      reshape(array([dat.x[-1,5:], yy[idx2]]).flatten(), (2,4)), '-k')\n            for i in rglen(idx):\n                # TODOqqq: annotate phenotypic value!?\n                # text(ax[1], yy[i], 'x(' + str(idx[i]) + ')=' + str(dat.x[-2,5+idx[i]]))\n\n                text(dat.x[-1, iabscissa], dat.x[-1, 5 + i]\n                            if x_opt is None else np.abs(xdat[-1, i]),\n                     ('x(' + str(i) + ')=' if annotations is None\n                        else str(i) + ':' + annotations[i] + \"=\")\n                     + str(dat.x[-2, 5 + i]))\n        i = 2  # find smallest i where iteration count differs (in case the same row appears twice)\n        while i < len(dat.f) and dat.f[-i][0] == dat.f[-1][0]:\n            i += 1\n        title('Object Variables (' +\n                (remark + ', ' if remark is not None else '') +\n                str(dat.x.shape[1] - 5) + '-D, popsize~' +\n                (str(int((dat.f[-1][1] - dat.f[-i][1]) / (dat.f[-1][0] - dat.f[-i][0])))\n                    if len(dat.f.T[0]) > 1 and dat.f[-1][0] > dat.f[-i][0] else 'NA')\n                + ')')\n        self._finalize_plotting()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay selected data from the files written by the class CMADataLogger.", "response": "def disp(self, idx=100):  # r_[0:5,1e2:1e9:1e2,-10:0]):\n        \"\"\"displays selected data from (files written by) the class `CMADataLogger`.\n\n        Arguments\n        ---------\n           `idx`\n               indices corresponding to rows in the data file;\n               if idx is a scalar (int), the first two, then every idx-th,\n               and the last three rows are displayed. Too large index values are removed.\n\n        Example\n        -------\n        >>> import cma, numpy as np\n        >>> res = cma.fmin(cma.fcts.elli, 7 * [0.1], 1, {'verb_disp':1e9})  # generate data\n        >>> assert res[1] < 1e-9\n        >>> assert res[2] < 4400\n        >>> l = cma.CMADataLogger()  # == res[-1], logger with default name, \"points to\" above data\n        >>> l.disp([0,-1])  # first and last\n        >>> l.disp(20)  # some first/last and every 20-th line\n        >>> l.disp(np.r_[0:999999:100, -1]) # every 100-th and last\n        >>> l.disp(np.r_[0, -10:0]) # first and ten last\n        >>> cma.disp(l.name_prefix, np.r_[0::100, -10:])  # the same as l.disp(...)\n\n        Details\n        -------\n        The data line with the best f-value is displayed as last line.\n\n        :See: `disp()`\n\n        \"\"\"\n\n        filenameprefix = self.name_prefix\n\n        def printdatarow(dat, iteration):\n            \"\"\"print data of iteration i\"\"\"\n            i = np.where(dat.f[:, 0] == iteration)[0][0]\n            j = np.where(dat.std[:, 0] == iteration)[0][0]\n            print('%5d' % (int(dat.f[i, 0])) + ' %6d' % (int(dat.f[i, 1])) + ' %.14e' % (dat.f[i, 5]) +\n                  ' %5.1e' % (dat.f[i, 3]) +\n                  ' %6.2e' % (max(dat.std[j, 5:])) + ' %6.2e' % min(dat.std[j, 5:]))\n\n        dat = CMADataLogger(filenameprefix).load()\n        ndata = dat.f.shape[0]\n\n        # map index to iteration number, is difficult if not all iteration numbers exist\n        # idx = idx[np.where(map(lambda x: x in dat.f[:,0], idx))[0]] # TODO: takes pretty long\n        # otherwise:\n        if idx is None:\n            idx = 100\n        if isscalar(idx):\n            # idx = np.arange(0, ndata, idx)\n            if idx:\n                idx = np.r_[0, 1, idx:ndata - 3:idx, -3:0]\n            else:\n                idx = np.r_[0, 1, -3:0]\n\n        idx = array(idx)\n        idx = idx[idx < ndata]\n        idx = idx[-idx <= ndata]\n        iters = dat.f[idx, 0]\n        idxbest = np.argmin(dat.f[:, 5])\n        iterbest = dat.f[idxbest, 0]\n\n        if len(iters) == 1:\n            printdatarow(dat, iters[0])\n        else:\n            self.disp_header()\n            for i in iters:\n                printdatarow(dat, i)\n            self.disp_header()\n            printdatarow(dat, iterbest)\n        sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadapts self. evaluations depending on the current measurement value and return self. alphasigma", "response": "def treat(self):\n        \"\"\"adapt self.evaluations depending on the current measurement value\n        and return ``sigma_fac in (1.0, self.alphasigma)``\n\n        \"\"\"\n        if self.noiseS > 0:\n            self.evaluations = min((self.evaluations * self.alphaevals, self.maxevals))\n            return self.alphasigma\n        else:\n            self.evaluations = max((self.evaluations * self.alphaevalsdown, self.minevals))\n            return 1.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores two fitness lists fit and fitre reevaluating some solutions in X.", "response": "def reeval(self, X, fit, func, ask, args=()):\n        \"\"\"store two fitness lists, `fit` and ``fitre`` reevaluating some\n        solutions in `X`.\n        ``self.evaluations`` evaluations are done for each reevaluated\n        fitness value.\n        See `__call__()`, where `reeval()` is called.\n\n        \"\"\"\n        self.fit = list(fit)\n        self.fitre = list(fit)\n        self.idx = self.indices(fit)\n        if not len(self.idx):\n            return self.idx\n        evals = int(self.evaluations) if self.f_aggregate else 1\n        fagg = np.median if self.f_aggregate is None else self.f_aggregate\n        for i in self.idx:\n            X_i = X[i]\n            if self.epsilon:\n                if self.parallel:\n                    self.fitre[i] = fagg(func(ask(evals, X_i, self.epsilon), *args))\n                else:\n                    self.fitre[i] = fagg([func(ask(1, X_i, self.epsilon)[0], *args)\n                                            for _k in xrange(evals)])\n            else:\n                self.fitre[i] = fagg([func(X_i, *args) for _k in xrange(evals)])\n        self.evaluations_just_done = evals * len(self.idx)\n        return self.fit, self.fitre, self.idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_measure(self):\n        lam = len(self.fit)\n        idx = np.argsort(self.fit + self.fitre)\n        ranks = np.argsort(idx).reshape((2, lam))\n        rankDelta = ranks[0] - ranks[1] - np.sign(ranks[0] - ranks[1])\n\n        # compute rank change limits using both ranks[0] and ranks[1]\n        r = np.arange(1, 2 * lam)  # 2 * lam - 2 elements\n        limits = [0.5 * (Mh.prctile(np.abs(r - (ranks[0, i] + 1 - (ranks[0, i] > ranks[1, i]))),\n                                      self.theta * 50) +\n                         Mh.prctile(np.abs(r - (ranks[1, i] + 1 - (ranks[1, i] > ranks[0, i]))),\n                                      self.theta * 50))\n                    for i in self.idx]\n        # compute measurement\n        #                               max: 1 rankchange in 2*lambda is always fine\n        s = np.abs(rankDelta[self.idx]) - Mh.amax(limits, 1)  # lives roughly in 0..2*lambda\n        self.noiseS += self.cum * (np.mean(s) - self.noiseS)\n        return self.noiseS, s", "response": "update the noise level measure using two fitness lists self. fit and self. fitre return self. noiseS and all_individual_measures."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef indices(self, fit):\n        ## meta_parameters.noise_reeval_multiplier == 1.0\n        lam_reev = 1.0 * (self.lam_reeval if self.lam_reeval\n                            else 2 + len(fit) / 20)\n        lam_reev = int(lam_reev) + ((lam_reev % 1) > np.random.rand())\n        ## meta_parameters.noise_choose_reeval == 1\n        choice = 1\n        if choice == 1:\n            # take n_first first and reev - n_first best of the remaining\n            n_first = lam_reev - lam_reev // 2\n            sort_idx = np.argsort(array(fit, copy=False)[n_first:]) + n_first\n            return np.array(list(range(0, n_first)) +\n                            list(sort_idx[0:lam_reev - n_first]), copy=False)\n        elif choice == 2:\n            idx_sorted = np.argsort(array(fit, copy=False))\n            # take lam_reev equally spaced, starting with best\n            linsp = np.linspace(0, len(fit) - len(fit) / lam_reev, lam_reev)\n            return idx_sorted[[int(i) for i in linsp]]\n        # take the ``lam_reeval`` best from the first ``2 * lam_reeval + 2`` values.\n        elif choice == 3:\n            return np.argsort(array(fit, copy=False)[:2 * (lam_reev + 1)])[:lam_reev]\n        else:\n            raise ValueError('unrecognized choice value %d for noise reev'\n                             % choice)", "response": "return the set of indices to be reevaluated for noise\n            measurement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate plots and saves function values for each point in the class instance.", "response": "def do(self, repetitions=1, locations=np.arange(-0.5, 0.6, 0.2), plot=True):\n        \"\"\"generates, plots and saves function values ``func(y)``,\n        where ``y`` is 'close' to `x` (see `__init__()`). The data are stored in\n        the ``res`` attribute and the class instance is saved in a file\n        with (the weired) name ``str(func)``.\n\n        Parameters\n        ----------\n            `repetitions`\n                for each point, only for noisy functions is >1 useful. For\n                ``repetitions==0`` only already generated data are plotted.\n            `locations`\n                coordinated wise deviations from the middle point given in `__init__`\n\n        \"\"\"\n        if not repetitions:\n            self.plot()\n            return\n\n        res = self.res\n        for i in xrange(len(self.basis)):  # i-th coordinate\n            if i not in res:\n                res[i] = {}\n            # xx = np.array(self.x)\n            # TODO: store res[i]['dx'] = self.basis[i] here?\n            for dx in locations:\n                xx = self.x + dx * self.basis[i]\n                xkey = dx  # xx[i] if (self.basis == np.eye(len(self.basis))).all() else dx\n                if xkey not in res[i]:\n                    res[i][xkey] = []\n                n = repetitions\n                while n > 0:\n                    n -= 1\n                    res[i][xkey].append(self.func(xx, *self.args))\n                    if plot:\n                        self.plot()\n                    self.save()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the data we have return self", "response": "def plot(self, plot_cmd=None, tf=lambda y: y):\n        \"\"\"plot the data we have, return ``self``\"\"\"\n        if not plot_cmd:\n            plot_cmd = self.plot_cmd\n        colors = 'bgrcmyk'\n        pyplot.hold(False)\n        res = self.res\n\n        flatx, flatf = self.flattened()\n        minf = np.inf\n        for i in flatf:\n            minf = min((minf, min(flatf[i])))\n        addf = 1e-9 - minf if minf <= 1e-9 else 0\n        for i in sorted(res.keys()):  # we plot not all values here\n            if isinstance(i, int):\n                color = colors[i % len(colors)]\n                arx = sorted(res[i].keys())\n                plot_cmd(arx, [tf(np.median(res[i][x]) + addf) for x in arx], color + '-')\n                pyplot.text(arx[-1], tf(np.median(res[i][arx[-1]])), i)\n                pyplot.hold(True)\n                plot_cmd(flatx[i], tf(np.array(flatf[i]) + addf), color + 'o')\n        pyplot.ylabel('f + ' + str(addf))\n        pyplot.draw()\n        pyplot.ion()\n        pyplot.show()\n        # raw_input('press return')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flattened(self):\n        flatx = {}\n        flatf = {}\n        for i in self.res:\n            if isinstance(i, int):\n                flatx[i] = []\n                flatf[i] = []\n                for x in sorted(self.res[i]):\n                    for d in sorted(self.res[i][x]):\n                        flatx[i].append(x)\n                        flatf[i].append(d)\n        return flatx, flatf", "response": "return flattened data for the sweep through\n        coordinate i and j"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning likelihood of x for the normal density N m and sigma ** 2 Cinv ** - 1", "response": "def likelihood(x, m=None, Cinv=None, sigma=1, detC=None):\n        \"\"\"return likelihood of x for the normal density N(m, sigma**2 * Cinv**-1)\"\"\"\n        # testing: MC integrate must be one: mean(p(x_i)) * volume(where x_i are uniformely sampled)\n        # for i in xrange(3): print mean([cma.likelihood(20*r-10, dim * [0], None, 3) for r in rand(10000,dim)]) * 20**dim\n        if m is None:\n            dx = x\n        else:\n            dx = x - m  # array(x) - array(m)\n        n = len(x)\n        s2pi = (2 * np.pi)**(n / 2.)\n        if Cinv is None:\n            return exp(-sum(dx**2) / sigma**2 / 2) / s2pi / sigma**n\n        if detC is None:\n            detC = 1. / np.linalg.linalg.det(Cinv)\n        return  exp(-np.dot(dx, np.dot(Cinv, dx)) / sigma**2 / 2) / s2pi / abs(detC)**0.5 / sigma**n"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the log - likelihood of x regarding the current sample distribution", "response": "def loglikelihood(self, x, previous=False):\n        \"\"\"return log-likelihood of `x` regarding the current sample distribution\"\"\"\n        # testing of original fct: MC integrate must be one: mean(p(x_i)) * volume(where x_i are uniformely sampled)\n        # for i in xrange(3): print mean([cma.likelihood(20*r-10, dim * [0], None, 3) for r in rand(10000,dim)]) * 20**dim\n        # TODO: test this!!\n        # c=cma.fmin...\n        # c[3]['cma'].loglikelihood(...)\n\n        if previous and hasattr(self, 'lastiter'):\n            sigma = self.lastiter.sigma\n            Crootinv = self.lastiter._Crootinv\n            xmean = self.lastiter.mean\n            D = self.lastiter.D\n        elif previous and self.countiter > 1:\n            raise _Error('no previous distribution parameters stored, check options importance_mixing')\n        else:\n            sigma = self.sigma\n            Crootinv = self._Crootinv\n            xmean = self.mean\n            D = self.D\n\n        dx = array(x) - xmean  # array(x) - array(m)\n        n = self.N\n        logs2pi = n * log(2 * np.pi) / 2.\n        logdetC = 2 * sum(log(D))\n        dx = np.dot(Crootinv, dx)\n        res = -sum(dx**2) / sigma**2 / 2 - logs2pi - logdetC / 2 - n * log(sigma)\n        if 1 < 3:  # testing\n            s2pi = (2 * np.pi)**(n / 2.)\n            detC = np.prod(D)**2\n            res2 = -sum(dx**2) / sigma**2 / 2 - log(s2pi * abs(detC)**0.5 * sigma**n)\n            assert res2 < res + 1e-8 or res2 > res - 1e-8\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function applied to a rotated argument", "response": "def rot(self, x, fun, rot=1, args=()):\n        \"\"\"returns ``fun(rotation(x), *args)``, ie. `fun` applied to a rotated argument\"\"\"\n        if len(np.shape(array(x))) > 1:  # parallelized\n            res = []\n            for x in x:\n                res.append(self.rot(x, fun, rot, args))\n            return res\n\n        if rot:\n            return fun(rotate(x, *args))\n        else:\n            return fun(x)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn sometimes np. NaN otherwise fun ( x", "response": "def somenan(self, x, fun, p=0.1):\n        \"\"\"returns sometimes np.NaN, otherwise fun(x)\"\"\"\n        if np.random.rand(1) < p:\n            return np.NaN\n        else:\n            return fun(x)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef noisysphere(self, x, noise=2.10e-9, cond=1.0, noise_offset=0.10):\n        return self.elli(x, cond=cond) * (1 + noise * np.random.randn() / len(x)) + noise_offset * np.random.rand()", "response": "noise = 10 does not work with default popsize"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cigar(self, x, rot=0, cond=1e6, noise=0):\n        if rot:\n            x = rotate(x)\n        x = [x] if isscalar(x[0]) else x  # scalar into list\n        f = [(x[0]**2 + cond * sum(x[1:]**2)) * np.exp(noise * np.random.randn(1)[0] / len(x)) for x in x]\n        return f if len(f) > 1 else f[0]  # 1-element-list into scalar", "response": "Cigar test objective function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tablet(self, x, rot=0):\n        if rot and rot is not fcts.tablet:\n            x = rotate(x)\n        x = [x] if isscalar(x[0]) else x  # scalar into list\n        f = [1e6 * x[0]**2 + sum(x[1:]**2) for x in x]\n        return f if len(f) > 1 else f[0]  # 1-element-list into scalar", "response": "Tablet test objective function"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cigtab(self, y):\n        X = [y] if isscalar(y[0]) else y\n        f = [1e-4 * x[0]**2 + 1e4 * x[1]**2 + sum(x[2:]**2) for x in X]\n        return f if len(f) > 1 else f[0]", "response": "Cigtab test objective function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef twoaxes(self, y):\n        X = [y] if isscalar(y[0]) else y\n        N2 = len(X[0]) // 2\n        f = [1e6 * sum(x[0:N2]**2) + sum(x[N2:]**2) for x in X]\n        return f if len(f) > 1 else f[0]", "response": "Cigtab test objective function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef elliconstraint(self, x, cfac=1e8, tough=True, cond=1e6):\n        N = len(x)\n        f = sum(cond**(np.arange(N)[-1::-1] / (N - 1)) * x**2)\n        cvals = (x[0] + 1,\n                 x[0] + 1 + 100 * x[1],\n                 x[0] + 1 - 100 * x[1])\n        if tough:\n            f += cfac * sum(max(0, c) for c in cvals)\n        else:\n            f += cfac * sum(max(0, c + 1e-3)**2 for c in cvals)\n        return f", "response": "ellipsoid test objective function with constraints"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rosen(self, x, alpha=1e2):\n        x = [x] if isscalar(x[0]) else x  # scalar into list\n        f = [sum(alpha * (x[:-1]**2 - x[1:])**2 + (1. - x[:-1])**2) for x in x]\n        return f if len(f) > 1 else f[0]  # 1-element-list into scalar", "response": "Rosenbrock test objective function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diffpow(self, x, rot=0):\n        N = len(x)\n        if rot:\n            x = rotate(x)\n        return sum(np.abs(x)**(2. + 4.*np.arange(N) / (N - 1.)))**0.5", "response": "Diffpow test objective function"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ridgecircle(self, x, expo=0.5):\n        a = len(x)\n        s = sum(x**2)\n        return ((s - a)**2)**(expo / 2) + s / a + sum(x) / a", "response": "happy cat by HG Beyer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rastrigin(self, x):\n        if not isscalar(x[0]):\n            N = len(x[0])\n            return [10 * N + sum(xi**2 - 10 * np.cos(2 * np.pi * xi)) for xi in x]\n            # return 10*N + sum(x**2 - 10*np.cos(2*np.pi*x), axis=1)\n        N = len(x)\n        return 10 * N + sum(x**2 - 10 * np.cos(2 * np.pi * x))", "response": "Rastrigin test objective function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef schwefelmult(self, x, pen_fac=1e4):\n        y = [x] if isscalar(x[0]) else x\n        N = len(y[0])\n        f = array([418.9829 * N - 1.27275661e-5 * N - sum(x * np.sin(np.abs(x)**0.5))\n                + pen_fac * sum((abs(x) > 500) * (abs(x) - 500)**2) for x in y])\n        return f if len(f) > 1 else f[0]", "response": "multimodal Schwefel function with domain - 500.. 500"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lincon(self, x, theta=0.01):\n        if x[0] < 0:\n            return np.NaN\n        return theta * x[1] + x[0]", "response": "ridge like linear function with one linear constraint"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nneed exponential number of steps in a non - increasing f - sequence.", "response": "def rosen_nesterov(self, x, rho=100):\n        \"\"\"needs exponential number of steps in a non-increasing f-sequence.\n\n        x_0 = (-1,1,...,1)\n        See Jarre (2011) \"On Nesterov's Smooth Chebyshev-Rosenbrock Function\"\n\n        \"\"\"\n        f = 0.25 * (x[0] - 1)**2\n        f += rho * sum((x[1:] - 2 * x[:-1]**2 + 1)**2)\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_offset(self):\n\n        for d in range(self.dmps):\n            if (self.y0[d] == self.goal[d]):\n                self.goal[d] += 1e-4", "response": "Check to see if initial position and goal are the same as the current one"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimitating the path of a desired trajectory.", "response": "def imitate_path(self, y_des):\n        \"\"\"Takes in a desired trajectory and generates the set of \n        system parameters that best realize this path.\n    \n        y_des list/array: the desired trajectories of each DMP\n                          should be shaped [dmps, run_time]\n        \"\"\"\n\n        # set initial state and goal\n        if y_des.ndim == 1: \n            y_des = y_des.reshape(1,len(y_des))\n        self.y0 = y_des[:,0].copy()\n        self.y_des = y_des.copy()\n        self.goal = self.gen_goal(y_des)\n        \n        self.check_offset()\n\n        if not (self.timesteps == y_des.shape[1]):\n            # generate function to interpolate the desired trajectory\n            import scipy.interpolate\n            path = np.zeros((self.dmps, self.timesteps))\n            x = np.linspace(0, self.cs.run_time, y_des.shape[1])\n            for d in range(self.dmps):\n                path_gen = scipy.interpolate.interp1d(x, y_des[d])\n                for t in range(self.timesteps):  \n                    path[d, t] = path_gen(t * self.dt)\n            y_des = path\n\n        # calculate velocity of y_des\n        dy_des = np.diff(y_des) / self.dt\n        # add zero to the beginning of every row\n        dy_des = np.hstack((np.zeros((self.dmps, 1)), dy_des))\n\n        # calculate acceleration of y_des\n        ddy_des = np.diff(dy_des) / self.dt\n        # add zero to the beginning of every row\n        ddy_des = np.hstack((np.zeros((self.dmps, 1)), ddy_des))\n\n        f_target = np.zeros((y_des.shape[1], self.dmps))\n        # find the force required to move along this trajectory\n        for d in range(self.dmps):\n            f_target[:,d] = ddy_des[d] - self.ay[d] * \\\n                            (self.by[d] * (self.goal[d] - y_des[d]) - \\\n                            dy_des[d])\n\n        # efficiently generate weights to realize f_target\n        self.gen_weights(f_target)\n\n        '''# plot the basis function activations\n        import matplotlib.pyplot as plt\n        plt.figure()\n        plt.subplot(211)\n        plt.plot(psi_track)\n        plt.title('psi_track')\n\n        # plot the desired forcing function vs approx\n        plt.subplot(212)\n        plt.plot(f_target[:,0])\n        plt.plot(np.sum(psi_track * self.w[0], axis=1))\n        plt.legend(['f_target', 'w*psi'])\n        plt.tight_layout()\n        plt.show()'''\n\n        self.reset_state()\n        return y_des"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a system trial no feedback is incorporated.", "response": "def rollout(self, timesteps=None, **kwargs):\n        \"\"\"Generate a system trial, no feedback is incorporated.\"\"\"\n\n        self.reset_state()\n\n        if timesteps is None:\n            if kwargs.has_key('tau'):\n                timesteps = int(self.timesteps / kwargs['tau'])\n            else: \n                timesteps = self.timesteps\n\n        # set up tracking vectors\n        y_track = np.zeros((timesteps, self.dmps)) \n        dy_track = np.zeros((timesteps, self.dmps))\n        ddy_track = np.zeros((timesteps, self.dmps))\n    \n        for t in range(timesteps):\n        \n            y, dy, ddy = self.step(**kwargs)\n\n            # record timestep\n            y_track[t] = y\n            dy_track[t] = dy\n            ddy_track[t] = ddy\n\n        return y_track, dy_track, ddy_track"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the system state", "response": "def reset_state(self):\n        \"\"\"Reset the system state\"\"\"\n        self.y = self.y0.copy()\n        self.dy = np.zeros(self.dmps)   \n        self.ddy = np.zeros(self.dmps)  \n        self.cs.reset_state()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the DMP system for a single timestep.", "response": "def step(self, tau=1.0, state_fb=None):\n        \"\"\"Run the DMP system for a single timestep.\n\n       tau float: scales the timestep\n                  increase tau to make the system execute faster\n       state_fb np.array: optional system feedback\n        \"\"\"\n\n        # run canonical system\n        cs_args = {'tau':tau,\n                   'error_coupling':1.0}\n        if state_fb is not None: \n            # take the 2 norm of the overall error\n            state_fb = state_fb.reshape(1,self.dmps)\n            dist = np.sqrt(np.sum((state_fb - self.y)**2))\n            cs_args['error_coupling'] = 1.0 / (1.0 + 10*dist)\n        x = self.cs.step(**cs_args)\n\n        # generate basis function activation\n        psi = self.gen_psi(x)\n\n        for d in range(self.dmps):\n\n            # generate the forcing term\n            f = self.gen_front_term(x, d) * \\\n                (np.dot(psi, self.w[d])) / np.sum(psi) if self.bfs > 0. else 0.\n\n            # DMP acceleration\n            self.ddy[d] = (self.ay[d] * \n                     (self.by[d] * (self.goal[d] - self.y[d]) - \\\n                     self.dy[d]/tau) + f) * tau\n            self.dy[d] += self.ddy[d] * tau * self.dt * cs_args['error_coupling']\n            self.y[d] += self.dy[d] * self.dt * cs_args['error_coupling']\n\n        return self.y, self.dy, self.ddy"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninfers probable x from input y", "response": "def infer_x(self, y):\n        \"\"\"Infer probable x from input y\n        @param y  the desired output for infered x.\n        @return   a list of probable x\n        \"\"\"\n        OptimizedInverseModel.infer_x(self, y)\n        if self.fmodel.size() == 0:\n            return self._random_x()\n\n        x_guesses = [self._guess_x_simple(y)[0]]\n\n        result = []\n        for xg in x_guesses:\n            res = cma.fmin(self._error, xg, self.cmaes_sigma,\n                           options={'bounds':[self.lower, self.upper],\n                           'verb_log':0,\n                           'verb_disp':False,\n                           'maxfevals':self.maxfevals,\n                           'seed': self.seed})\n            result.append((res[1], res[0]))\n\n        return [xi for fi, xi in sorted(result)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer probable output from input x y", "response": "def infer_dims(self, x, y, dims_x, dims_y, dims_out):\n        \"\"\"Infer probable output from input x, y\n        \"\"\"\n        OptimizedInverseModel.infer_x(self, y)\n        assert len(x) == len(dims_x)\n        assert len(y) == len(dims_y)\n        if len(self.fmodel.dataset) == 0:\n            return [[0.0]*self.dim_out]\n        else:\n            _, index = self.fmodel.dataset.nn_dims(x, y, dims_x, dims_y, k=1)\n            guesses = [self.fmodel.dataset.get_dims(index[0], dims_out)]\n\n            result = []\n            for g in guesses:\n                res = cma.fmin(lambda q:self._error_dims(q, dims_x, dims_y, dims_out), g, self.cmaes_sigma,\n                               options={'bounds':[self.lower, self.upper],\n                               'verb_log':0,\n                               'verb_disp':False,\n                               'maxfevals':self.maxfevals,\n                               'seed': self.seed})\n                result.append((res[1], res[0]))\n\n            return sorted(result)[0][1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_x(self, y):\n        assert len(y) == self.fmodel.dim_y, \"Wrong dimension for y. Expected %i, got %i\" % (self.fmodel.dim_y, len(y))\n        if len(self.fmodel.dataset) == 0:\n            return [[0.0]*self.dim_x]\n        else:\n            _, index = self.fmodel.dataset.nn_y(y, k=1)\n            return [self.fmodel.dataset.get_x(index[0])]", "response": "Infer probable x from input y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer dims from input x y", "response": "def infer_dims(self, x, y, dims_x, dims_y, dims_out):\n        \"\"\"Infer probable output from input x, y\n        \"\"\"\n        assert len(x) == len(dims_x)\n        assert len(y) == len(dims_y)\n        if len(self.fmodel.dataset) == 0:\n            return [0.0]*self.dim_out\n        else:\n            _, index = self.fmodel.dataset.nn_dims(x, y, dims_x, dims_y, k=1)\n            return self.fmodel.dataset.get_dims(index[0], dims=dims_out)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_x(self, y):\n        assert len(y) == self.fmodel.dim_y, \"Wrong dimension for y. Expected %i, got %i\" % (self.fmodel.dim_y, len(y))\n        if len(self.fmodel.dataset) == 0:\n            return [[0.0]*self.dim_x]\n        else:\n            dists, index = self.fmodel.dataset.nn_y(y, k=self.k)\n            w = self._weights(dists, index)\n            idx = index[np.argmax(w)]\n            return [self.fmodel.dataset.get_x(idx)]", "response": "Infer probable x from input y"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simulate(n, x0, dt, func):\n\n    def functor(f):\n        \"\"\" This function allows us to give to :right_hand_side: function a specific input function.\n\n        It is necessary because of the format of arguments of odeint.\n\n        :param func f: input function\n\n        :rtype: differential equation that will be solved.\n        \"\"\"\n        def right_hand_side(x, t, args):\n            arguments = hstack((x, f(t), args))\n            dx = array(solve(M_func(*arguments), F_func(*arguments))).T[0]\n            return dx\n        return right_hand_side\n\n    q = dynamicsymbols('q:' + str(n + 1))\n    u = dynamicsymbols('u:' + str(n + 1))\n    f = dynamicsymbols('f')\n    m = symbols('m:' + str(n + 1))\n    l = symbols('l:' + str(n))\n    g, t = symbols('g t')\n    I = ReferenceFrame('I')\n    O = Point('O')\n    O.set_vel(I, 0)\n    P0 = Point('P0')\n    P0.set_pos(O, q[0] * I.x)\n    P0.set_vel(I, u[0] * I.x)\n    Pa0 = Particle('Pa0', P0, m[0])\n    frames = [I]\n    points = [P0]\n    particles = [Pa0]\n    forces = [(P0, f * I.x - m[0] * g * I.y)]\n    kindiffs = [q[0].diff(t) - u[0]]\n\n    for i in range(n):\n        Bi = I.orientnew('B' + str(i), 'Axis', [q[i + 1], I.z])\n        Bi.set_ang_vel(I, u[i + 1] * I.z)\n        frames.append(Bi)\n        Pi = points[-1].locatenew('P' + str(i + 1), l[i] * Bi.x)\n        Pi.v2pt_theory(points[-1], I, Bi)\n        points.append(Pi)\n        Pai = Particle('Pa' + str(i + 1), Pi, m[i + 1])\n        particles.append(Pai)\n        forces.append((Pi, -m[i + 1] * g * I.y))\n        kindiffs.append(q[i + 1].diff(t) - u[i + 1])\n\n    arm_length = 1. / n\n    bob_mass = 0.01 / n\n    parameters = [g, m[0]]\n    parameter_vals = [9.81, 0.01 / n]\n    for i in range(n):\n        parameters += [l[i], m[i + 1]]\n        parameter_vals += [arm_length, bob_mass]\n    t = linspace(0, 1000 * dt, 1000)\n\n    kane = KanesMethod(I, q_ind=q, u_ind=u, kd_eqs=kindiffs)\n    fr, frstar = kane.kanes_equations(forces, particles)\n    fr\n    frstar\n\n    dynamic = q + u\n    dynamic.append(f)\n    dummy_symbols = [Dummy() for i in dynamic]\n    dummy_dict = dict(zip(dynamic, dummy_symbols))\n    kindiff_dict = kane.kindiffdict()\n    M = kane.mass_matrix_full.subs(kindiff_dict).subs(dummy_dict)\n    F = kane.forcing_full.subs(kindiff_dict).subs(dummy_dict)\n    M_func = lambdify(dummy_symbols + parameters, M)\n    F_func = lambdify(dummy_symbols + parameters, F)\n\n    f = functor(func)\n\n    y = odeint(f, x0, t, args=(parameter_vals,))\n\n    return y", "response": "This function simulates the n - pendulum behavior of the given particle system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cartesian(n, states):\n    length = 1. / n  # arm_length\n    pos_x = hstack((states[0], zeros(n)))\n    pos_y = zeros(n + 1)\n    for j in arange(1, n + 1):\n        pos_x[j] = pos_x[j - 1] + length * cos(states[j])\n        pos_y[j] = pos_y[j - 1] + length * sin(states[j])\n    pos = hstack((pos_x, pos_y))\n    return pos", "response": "This function computes the cartesian coordinates of the states returned by the simulate function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef url_for(self, *args, **kwargs):\n        return yarl.URL(self.url(parts=kwargs))", "response": "Construct url for route with additional params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning dereference data for a node in a sequence or dict.", "response": "def deref(data, spec: dict):\n    \"\"\"\n    Return dereference data\n    :param data:\n    :param spec:\n    :return:\n    \"\"\"\n    if isinstance(data, Sequence):\n        is_dict = False\n        gen = enumerate(data)\n    elif not isinstance(data, Mapping):\n        return data\n    elif '$ref' in data:\n        return deref(get_ref(spec, data['$ref']), spec)\n    else:\n        is_dict = True\n        gen = data.items()  # type: ignore\n\n    result = None\n\n    for k, v in gen:\n        new_v = deref(v, spec)\n        if new_v is not v:\n            if result is not None:\n                pass\n            elif is_dict:\n                result = data.copy()\n            else:\n                result = data[:]\n            result[k] = new_v\n    return result or data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_swagger_data(self, loader):\n        if self.is_built:\n            return\n        self.is_built = True\n        self._required = []\n        self._parameters = {}\n        if not self._swagger_data:\n            return\n        elif loader is not None:\n            data = loader.resolve_data(self._swagger_data).copy()\n        else:\n            data = self._swagger_data\n\n        for param in data.get('parameters', ()):\n            p = param.copy()\n            if loader is None:\n                p = allOf(p)\n            name = p.pop('name')\n            self._parameters[name] = p\n            if p.pop('required', False):\n                self._required.append(name)", "response": "Builds the data for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns parameters extract from request and multidict errors", "response": "async def validate(self, request: web.Request):\n        \"\"\" Returns parameters extract from request and multidict errors\n\n        :param request: Request\n        :return: tuple of parameters and errors\n        \"\"\"\n        parameters = {}\n        files = {}\n        errors = self.errors_factory()\n        body = None\n\n        if request.method in request.POST_METHODS:\n            try:\n                body = await self._content_receiver.receive(request)\n            except ValueError as e:\n                errors[request.content_type].add(str(e))\n            except TypeError:\n                errors[request.content_type].add('Not supported content type')\n\n        for name, param in self._parameters.items():\n            where = param['in']\n            schema = param.get('schema', param)\n            vtype = schema['type']\n            is_array = vtype == 'array'\n\n            if where == 'query':\n                source = request.query\n            elif where == 'header':\n                source = request.headers\n            elif where == 'path':\n                source = request.match_info\n            elif body is None:\n                source = ()\n            elif where == 'formData':\n                source = body\n            elif where == 'body':\n                if isinstance(body, BaseException):\n                    errors[name].add(str(body))\n                else:\n                    parameters[name] = body\n                continue\n            else:\n                raise ValueError(where)\n\n            if is_array and hasattr(source, 'getall'):\n                collection_format = param.get('collectionFormat')\n                default = param.get('default', [])\n                value = get_collection(source, name,\n                                       collection_format, default)\n                if param.get('minItems') and not value \\\n                        and name not in self._required:\n                    continue\n            elif isinstance(source, Mapping) and name in source and (\n                vtype not in ('number', 'integer') or source[name] != ''\n            ):\n                value = source[name]\n            elif 'default' in param:\n                parameters[name] = param['default']\n                continue\n            elif name in self._required:\n                errors[name].add('Required')\n                if isinstance(source, BaseException):\n                    errors[name].add(str(body))\n                continue\n            else:\n                continue\n\n            if is_array:\n                vtype = schema['items']['type']\n                vformat = schema['items'].get('format')\n            else:\n                vformat = schema.get('format')\n\n            if source is body and isinstance(body, dict):\n                pass\n            elif vtype not in ('string', 'file'):\n                value = convert(name, value, vtype, vformat, errors)\n\n            if vtype == 'file':\n                files[name] = value\n            else:\n                parameters[name] = value\n\n        parameters = self._validate(parameters, errors)\n        parameters.update(files)\n        return parameters, errors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_document(request, doc_id, body, errors):\n    if doc_id in DB:\n        errors['doc_id'].add('Document already exists')\n\n    if errors:\n        raise errors\n\n    body['doc_id'] = doc_id\n    DB[doc_id] = body\n\n    # dict response for jsonify middleware\n    return body", "response": "Set the document in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the swagger UI request.", "response": "def _handler_swagger_ui(self, request, spec, version):\n        \"\"\"\n        ---\n        parameters:\n          - name: spec\n            in: query\n            type: string\n          - name: version\n            in: query\n            type: integer\n            enum: [2,3]\n        \"\"\"\n        version = version or self._version_ui\n        if self._spec_url:\n            spec_url = self._spec_url\n        else:\n            spec_url = request.url.with_path(self['swagger:spec'].url())\n            proto = request.headers.get(hdrs.X_FORWARDED_PROTO)\n            if proto:\n                spec_url = spec_url.with_scheme(proto)\n            if isinstance(spec, str):\n                spec_url = spec_url.with_query(spec=spec)\n            elif len(self._swagger_data) == 1:\n                for basePath in self._swagger_data:\n                    spec_url = spec_url.with_query(spec=basePath)\n            else:\n                spec_url = spec_url.with_query(spec='/')\n            spec_url = spec_url.human_repr()\n        return web.Response(\n            text=ui.rend_template(spec_url,\n                                  prefix=self._swagger_ui,\n                                  version=version),\n            content_type='text/html')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef include(self, spec, *,\n                basePath=None,\n                operationId_mapping=None,\n                name=None):\n        \"\"\" Adds a new specification to a router\n\n        :param spec: path to specification\n        :param basePath: override base path specify in specification\n        :param operationId_mapping: mapping for handlers\n        :param name: name to access original spec\n        \"\"\"\n\n        data = self._file_loader.load(spec)\n\n        if basePath is None:\n            basePath = data.get('basePath', '')\n\n        if name is not None:\n            d = dict(data)\n            d['basePath'] = basePath\n            self._swagger_data[name] = d\n            # TODO clear d\n\n        swagger_data = {k: v for k, v in data.items() if k != 'paths'}\n        swagger_data['basePath'] = basePath\n\n        for url, methods in data.get('paths', {}).items():\n            url = basePath + url\n            methods = dict(methods)\n            location_name = methods.pop(self.NAME, None)\n            parameters = methods.pop('parameters', [])\n            for method, body in methods.items():\n                if method == self.VIEW:\n                    view = utils.import_obj(body)\n                    view.add_routes(self, prefix=url, encoding=self._encoding)\n                    continue\n                body = dict(body)\n                if parameters:\n                    body['parameters'] = parameters + \\\n                        body.get('parameters', [])\n                handler = body.pop(self.HANDLER, None)\n                name = location_name or handler\n                if not handler:\n                    op_id = body.get('operationId')\n                    if op_id and operationId_mapping:\n                        handler = operationId_mapping.get(op_id)\n                        if handler:\n                            name = location_name or op_id\n                if handler:\n                    validate = body.pop(self.VALIDATE, self._default_validate)\n                    self.add_route(\n                        method.upper(), utils.url_normolize(url),\n                        handler=handler,\n                        name=name,\n                        swagger_data=body,\n                        validate=validate,\n                    )\n        self._swagger_data[basePath] = swagger_data\n\n        for route in self.routes():\n            if isinstance(route, SwaggerRoute) and not route.is_built:\n                route.build_swagger_data(self._file_loader)", "response": "Adds a new specification to a router."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a route to the resource with the specified name and handler.", "response": "def add_route(self, method, path, handler,\n                  *, name=None, expect_handler=None,\n                  swagger_data=None,\n                  validate=None):\n        \"\"\" Returns route\n\n        :param method: as well as in aiohttp\n        :param path: as well as in aiohttp\n        :param handler: as well as in aiohttp\n            and also must be str `mymodule.handler`\n        :param name: as well as in aiohttp\n        :param expect_handler: as well as in aiohttp\n        :param swagger_data: data\n            http://swagger.io/specification/#operationObject\n        :param validate: bool param for validate in SwaggerValidationRoute\n        :param build: bool param for build extractor and validator\n        :return: route for handler\n        \"\"\"\n        if name is None or name in self._named_resources:\n            name = ''\n\n        if validate is None:\n            validate = self._default_validate\n\n        route = super().add_route(\n            method, path, handler, name=name,\n            expect_handler=expect_handler,\n            swagger_data=swagger_data,\n            validate=validate,\n        )\n        return route"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self, app: web.Application):\n        if self.app is app:\n            raise ValueError('The router is already configured '\n                             'for this application')\n        self.app = app\n        routes = sorted(\n            ((r.name, (r, r.url_for().human_repr())) for r in self.routes()),\n            key=utils.sort_key)\n        exists = set()  # type: Set[str]\n        for name, (route, path) in routes:\n            if name and name not in exists:\n                exists.add(name)\n            else:\n                name = None\n            app.router.add_route(\n                route.method, path,\n                route.handler, name=name)", "response": "Setup the routes to the application router."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_converter(self, klass, conv, score=0):\n        if isinstance(klass, str):\n            klass = import_obj(klass)\n        item = score, klass, conv\n        self.converters.append(item)\n        self.converters.sort(key=lambda x: x[0])\n        return self", "response": "Add a converter to the list of converters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef url_normolize(url: str):\n    u = parse.urljoin('///{}/'.format(url), '.')\n    u = re.sub(r'/+', '/', u)\n    return u if url.endswith('/') else u[:-1]", "response": ">>> url_normolize('//api/1/../../status')\n    '/status'\n    >>> url_normolize('//api/1/../../status/')\n    '/status/'\n    >>> url_normolize('/api/1/../../status/')\n    '/status/'\n    >>> url_normolize('///api/1/../../status/')\n    '/status/'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort_key(x):\n    name, (r, u) = x\n    return - len(u) + u.count('}') * 100", "response": "Sort by route name and URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_collection(source, name, collection_format, default):\n    if collection_format in COLLECTION_SEP:\n        separator = COLLECTION_SEP[collection_format]\n        value = source.get(name, None)\n        if value is None:\n            return default\n        return value.split(separator)\n    if collection_format == 'brackets':\n        return source.getall(name + '[]', default)\n    else:                       # format: multi\n        return source.getall(name, default)", "response": "get collection named name from the given source that is formatted according to collection_format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _defaultExpectHandler(request):  # pragma: no cover\n    expect = request.headers.get(hdrs.EXPECT)\n    if request.version == HttpVersion11:\n        if expect.lower() == \"100-continue\":\n            request.transport.write(b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n        else:\n            raise HTTPExpectationFailed(text=\"Unknown Expect: %s\" % expect)", "response": "Default Expect header handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_name(self, name: str):\n        parts = self.NAME_SPLIT_RE.split(name)\n        for part in parts:\n            if not part.isidentifier() or keyword.iskeyword(part):\n                raise ValueError('Incorrect route name {!r}, '\n                                 'the name should be a sequence of '\n                                 'python identifiers separated '\n                                 'by dash, dot or column'.format(name))\n        if name in self._named_resources:\n            raise ValueError('Duplicate {!r}, '\n                             'already handled by {!r}'\n                             .format(name, self._named_resources[name]))", "response": "Validate the name of a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pattern_formatter(cls, location):\n        pattern = ''\n        formatter = ''\n        canon = ''\n        for part in cls.ROUTE_RE.split(location):\n            match = cls.DYN.match(part)\n            if match:\n                pattern += '(?P<{}>{})'.format(match.group('var'), cls.GOOD)\n                formatter += '{' + match.group('var') + '}'\n                continue\n\n            match = cls.DYN_WITH_RE.match(part)\n            if match:\n                pattern += '(?P<{var}>{re})'.format(**match.groupdict())\n                formatter += '{' + match.group('var') + '}'\n                canon += match.group('re')\n                continue\n\n            if '{' in part or '}' in part:\n                raise ValueError(\"Invalid path '{}'['{}']\".format(\n                    location, part))\n\n            formatter += part\n            pattern += re.escape(part)\n            canon += part\n\n        try:\n            return re.compile(pattern), formatter, canon\n        except re.error as exc:\n            raise ValueError(\n                \"Bad pattern '{}': {}\".format(pattern, exc)) from None", "response": "Return pattern formatter for a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_get(self, *args, **kwargs):\n        return self.add_route(hdrs.METH_GET, *args, **kwargs)", "response": "Shortcut for add_route with method GET"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_post(self, *args, **kwargs):\n        return self.add_route(hdrs.METH_POST, *args, **kwargs)", "response": "Shortcut for add_route with method POST"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_put(self, *args, **kwargs):\n        return self.add_route(hdrs.METH_PUT, *args, **kwargs)", "response": "Shortcut for add_route with method PUT"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_patch(self, *args, **kwargs):\n        return self.add_route(hdrs.METH_PATCH, *args, **kwargs)", "response": "Shortcut for add_route with method PATCH"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_delete(self, *args, **kwargs):\n        return self.add_route(hdrs.METH_DELETE, *args, **kwargs)", "response": "Shortcut for add_route with method DELETE"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, *args, **kwargs):\n        for arg in args:\n            if isinstance(arg, str):\n                self._operations.append(self._from_str(arg))\n            else:\n                self._operations.append(arg)\n        if kwargs:\n            self._operations.append(kwargs)", "response": "Add new mapping from args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecorating the given class with the string conversion methods for the given class.", "response": "def str_repr(klass):\n    \"\"\"\n    Implements string conversion methods for the given class.\n\n    The given class must implement the __str__ method. This decorat\n    will add __repr__ and __unicode__ (for Python 2).\n\n    \"\"\"\n    if PY2:\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    klass.__repr__ = lambda self: '<%s: %r>' % (self.__class__.__name__, str(self))\n    return klass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate and normalizes a cell range slice.", "response": "def _clean_slice(key, length):\n    \"\"\"\n    Validates and normalizes a cell range slice.\n\n    >>> _clean_slice(slice(None, None), 10)\n    (0, 10)\n    >>> _clean_slice(slice(-10, 10), 10)\n    (0, 10)\n    >>> _clean_slice(slice(-11, 11), 10)\n    (0, 10)\n    >>> _clean_slice(slice('x', 'y'), 10)\n    Traceback (most recent call last):\n    ...\n    TypeError: Cell indices must be integers, str given.\n    >>> _clean_slice(slice(0, 10, 2), 10)\n    Traceback (most recent call last):\n    ...\n    NotImplementedError: Cell slice with step is not supported.\n    >>> _clean_slice(slice(5, 5), 10)\n    Traceback (most recent call last):\n    ...\n    ValueError: Cell slice can not be empty.\n\n    \"\"\"\n    if key.step is not None:\n        raise NotImplementedError('Cell slice with step is not supported.')\n    start, stop = key.start, key.stop\n    if start is None:\n        start = 0\n    if stop is None:\n        stop = length\n    if not isinstance(start, integer_types):\n        raise TypeError('Cell indices must be integers, %s given.' % type(start).__name__)\n    if not isinstance(stop, integer_types):\n        raise TypeError('Cell indices must be integers, %s given.' % type(stop).__name__)\n    if start < 0:\n        start = start + length\n    if stop < 0:\n        stop = stop + length\n    start, stop = max(0, start), min(length, stop)\n    if start == stop:\n        raise ValueError('Cell slice can not be empty.')\n    return start, stop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _clean_index(key, length):\n    if not isinstance(key, integer_types):\n        raise TypeError('Cell indices must be integers, %s given.' % type(key).__name__)\n    if -length <= key < 0:\n        return key + length\n    elif 0 <= key < length:\n        return key\n    else:\n        raise IndexError('Cell index out of range.')", "response": "Validates and normalizes a cell index."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _col_name(index):\n    for exp in itertools.count(1):\n        limit = 26 ** exp\n        if index < limit:\n            return ''.join(chr(ord('A') + index // (26 ** i) % 26) for i in range(exp-1, -1, -1))\n        index -= limit", "response": "Converts a column index to a column name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __set_title(self, value):\n        # OpenOffice on Debian \"squeeze\" ignore value of target.XAxis.String\n        # unless target.HasXAxisTitle is set to True first. (Despite the\n        # fact that target.HasXAxisTitle is reported to be False until\n        # target.XAxis.String is set to non empty value.)\n        self._target.setPropertyValue(self._has_axis_title_property, True)\n        target = self._get_title_target()\n        target.setPropertyValue('String', text_type(value))", "response": "Sets the title of this axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ranges(self):\n        ranges = self._target.getRanges()\n        return map(SheetAddress._from_uno, ranges)", "response": "Returns a list of addresses with source data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diagram(self):\n        target = self._embedded.getDiagram()\n        target_type = target.getDiagramType()\n        cls = _DIAGRAM_TYPES.get(target_type, Diagram)\n        return cls(target)", "response": "Diagram - inner content of this chart."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_type(self, cls):\n        target_type = cls._type\n        target = self._embedded.createInstance(target_type)\n        self._embedded.setDiagram(target)\n        return cls(target)", "response": "Change type of diagram in this chart."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and inserts a new chart.", "response": "def create(self, name, position, ranges=(), col_header=False, row_header=False):\n        \"\"\"\n        Creates and inserts a new chart.\n        \"\"\"\n        rect = self._uno_rect(position)\n        ranges = self._uno_ranges(ranges)\n        self._create(name, rect, ranges, col_header, row_header)\n        return self[name]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the target from the cursor to the specified position and returns in.", "response": "def get_target(self, row, col, row_count, col_count):\n        \"\"\"\n        Moves cursor to the specified position and returns in.\n        \"\"\"\n        # This method is called for almost any operation so it should\n        # be maximally optimized.\n        #\n        # Any comparison here is negligible compared to UNO call. So we do all\n        # possible checks which can prevent an unnecessary cursor movement.\n        #\n        # Generally we need to expand or collapse selection to the desired\n        # size and move it to the desired position. But both of these actions\n        # can fail if there is not enough space. For this reason we must\n        # determine which of the actions has to be done first. In some cases\n        # we must even move the cursor twice (cursor movement is faster than\n        # selection change).\n        #\n        target = self._target\n        # If we cannot resize selection now then we must move cursor first.\n        if self.row + row_count > self.max_row_count or self.col + col_count > self.max_col_count:\n            # Move cursor to the desired position if possible.\n            row_delta = row - self.row if row + self.row_count <= self.max_row_count else 0\n            col_delta = col - self.col if col + self.col_count <= self.max_col_count else 0\n            target.gotoOffset(col_delta, row_delta)\n            self.row += row_delta\n            self.col += col_delta\n        # Resize selection\n        if (row_count, col_count) != (self.row_count, self.col_count):\n            target.collapseToSize(col_count, row_count)\n            self.row_count = row_count\n            self.col_count = col_count\n        # Move cursor to the desired position\n        if (row, col) != (self.row, self.col):\n            target.gotoOffset(col - self.col, row - self.row)\n            self.row = row\n            self.col = col\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the value of the cell to a string or number based on the given value.", "response": "def __set_value(self, value):\n        \"\"\"\n        Sets cell value to a string or number based on the given value.\n        \"\"\"\n        array = ((self._clean_value(value),),)\n        return self._get_target().setDataArray(array)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset a formula in this cell.", "response": "def __set_formula(self, formula):\n        \"\"\"\n        Sets a formula in this cell.\n\n        Any cell value can be set using this method. Actual formulas must\n        start with an equal sign.\n        \"\"\"\n        array = ((self._clean_formula(formula),),)\n        return self._get_target().setFormulaArray(array)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __set_values(self, values):\n        # Tuple of tuples is required\n        array = tuple(tuple(self._clean_value(col) for col in row) for row in values)\n        self._get_target().setDataArray(array)", "response": "Sets values in this cell range from an iterable of iterables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __set_formulas(self, formulas):\n        # Tuple of tuples is required\n        array = tuple(tuple(self._clean_formula(col) for col in row) for row in formulas)\n        self._get_target().setFormulaArray(array)", "response": "Sets the formulas in this cell range from an iterable of iterables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the values in this cell range as a tuple.", "response": "def __get_values(self):\n        \"\"\"\n        Gets values in this cell range as a tuple.\n\n        This is much more effective than reading cell values one by one.\n        \"\"\"\n        array = self._get_target().getDataArray()\n        return tuple(itertools.chain.from_iterable(array))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the values in this cell range from an iterable.", "response": "def __set_values(self, values):\n        \"\"\"\n        Sets values in this cell range from an iterable.\n\n        This is much more effective than writing cell values one by one.\n        \"\"\"\n        array = tuple((self._clean_value(v),) for v in values)\n        self._get_target().setDataArray(array)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the formulas in this cell range as a tuple.", "response": "def __get_formulas(self):\n        \"\"\"\n        Gets formulas in this cell range as a tuple.\n\n        If cells contain actual formulas then the returned values start\n        with an equal sign  but all values are returned.\n        \"\"\"\n        array = self._get_target().getFormulaArray()\n        return tuple(itertools.chain.from_iterable(array))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the formulas in this cell range from an iterable.", "response": "def __set_formulas(self, formulas):\n        \"\"\"\n        Sets formulas in this cell range from an iterable.\n\n        Any cell values can be set using this method. Actual formulas must\n        start with an equal sign.\n        \"\"\"\n        array = tuple((self._clean_formula(v),) for v in formulas)\n        self._get_target().setFormulaArray(array)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new entry in the cache with the given name.", "response": "def create(self, name, index=None):\n        \"\"\"\n        Creates a new sheet with the given name.\n\n        If an optional index argument is not provided then the created\n        sheet is appended at the end. Returns the new sheet.\n        \"\"\"\n        if index is None:\n            index = len(self)\n        self._create(name, index)\n        return self[name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self, old_name, new_name, index=None):\n        if index is None:\n            index = len(self)\n        self._copy(old_name, new_name, index)\n        return self[new_name]", "response": "Copies an old sheet with the old_name to a new sheet with new_name. Returns the new sheet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving this document to a local file system.", "response": "def save(self, path=None, filter_name=None):\n        \"\"\"\n        Saves this document to a local file system.\n\n        The optional first argument defaults to the document's path.\n\n        Accept optional second  argument which defines type of\n        the saved file. Use one of FILTER_* constants or see list of\n        available filters at http://wakka.net/archives/7 or\n        http://www.oooforum.org/forum/viewtopic.phtml?t=71294.\n        \"\"\"\n\n        if path is None:\n            try:\n                self._target.store()\n            except _IOException as e:\n                raise IOError(e.Message)\n            return\n\n        # UNO requires absolute paths\n        url = uno.systemPathToFileUrl(os.path.abspath(path))\n        if filter_name:\n            format_filter = uno.createUnoStruct('com.sun.star.beans.PropertyValue')\n            format_filter.Name = 'FilterName'\n            format_filter.Value = filter_name\n            filters = (format_filter,)\n        else:\n            filters = ()\n        # http://www.openoffice.org/api/docs/common/ref/com/sun/star/frame/XStorable.html#storeToURL\n        try:\n            self._target.storeToURL(url, filters)\n        except _IOException as e:\n            raise IOError(e.Message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_locale(self, language=None, country=None, variant=None):\n        # http://www.openoffice.org/api/docs/common/ref/com/sun/star/lang/Locale.html\n        locale = uno.createUnoStruct('com.sun.star.lang.Locale')\n        if language:\n            locale.Language = language\n        if country:\n            locale.Country = country\n        if variant:\n            locale.Variant = variant\n        formats = self._target.getNumberFormats()\n        return Locale(locale, formats)", "response": "Returns a Locale object which can be used for access to number formats."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sheets(self):\n        # http://www.openoffice.org/api/docs/common/ref/com/sun/star/sheet/XSpreadsheetDocument.html#getSheets\n        try:\n            return self._sheets\n        except AttributeError:\n            target = self._target.getSheets()\n            self._sheets = SpreadsheetCollection(self, target)\n        return self._sheets", "response": "Returns a SpreadsheetCollection object for this document."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a float value to corresponding datetime instance.", "response": "def date_from_number(self, value):\n        \"\"\"\n        Converts a float value to corresponding datetime instance.\n        \"\"\"\n        if not isinstance(value, numbers.Real):\n            return None\n        delta = datetime.timedelta(days=value)\n        return self._null_date + delta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a date or datetime instance to a corresponding float value.", "response": "def date_to_number(self, date):\n        \"\"\"\n        Converts a date or datetime instance to a corresponding float value.\n        \"\"\"\n        if isinstance(date, datetime.datetime):\n            delta = date - self._null_date\n        elif isinstance(date, datetime.date):\n            delta = date - self._null_date.date()\n        else:\n            raise TypeError(date)\n        return delta.days + delta.seconds / (24.0 * 60 * 60)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef time_from_number(self, value):\n        if not isinstance(value, numbers.Real):\n            return None\n        delta = datetime.timedelta(days=value)\n        minutes, second = divmod(delta.seconds, 60)\n        hour, minute = divmod(minutes, 60)\n        return datetime.time(hour, minute, second)", "response": "Converts a float value to a corresponding time instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef time_to_number(self, time):\n        if not isinstance(time, datetime.time):\n            raise TypeError(time)\n        return ((time.second / 60.0 + time.minute) / 60.0 + time.hour) / 24.0", "response": "Converts a time instance to a corresponding float value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the date which is represented by a integer 0.", "response": "def _null_date(self):\n        \"\"\"\n        Returns date which is represented by a integer 0.\n        \"\"\"\n        # http://www.openoffice.org/api/docs/common/ref/com/sun/star/util/NumberFormatSettings.html#NullDate\n        try:\n            return self.__null_date\n        except AttributeError:\n            number_settings = self._target.getNumberFormatSettings()\n            d = number_settings.getPropertyValue('NullDate')\n            self.__null_date = datetime.datetime(d.Year, d.Month, d.Day)\n        return self.__null_date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_spreadsheet(self, path, as_template=False, read_only=False):\n        extra = ()\n        if as_template:\n            pv = uno.createUnoStruct('com.sun.star.beans.PropertyValue')\n            pv.Name = 'AsTemplate'\n            pv.Value = True\n            extra += (pv,)\n        if read_only:\n            pv = uno.createUnoStruct('com.sun.star.beans.PropertyValue')\n            pv.Name = 'ReadOnly'\n            pv.Value = True\n            extra += (pv,)\n        # UNO requires absolute paths\n        url = uno.systemPathToFileUrl(os.path.abspath(path))\n        document = self._open_url(url, extra)\n        return SpreadsheetDocument(document)", "response": "Opens an exiting spreadsheet document on the local file system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_spreadsheet(self):\n        desktop = self.cls(self.hostname, self.port, self.pipe)\n        return desktop.create_spreadsheet()", "response": "Creates a new spreadsheet document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening an exiting spreadsheet on the local file system.", "response": "def open_spreadsheet(self, path, as_template=False):\n        \"\"\"\n        Opens an exiting spreadsheet document on the local file system.\n        \"\"\"\n        desktop = self.cls(self.hostname, self.port)\n        return desktop.open_spreadsheet(path, as_template=as_template)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the state of the object.", "response": "def plot(self, win=None, newfig=True, figsize=None, orientation='hor', topfigfrac=0.8):\n        \"\"\"Plot layout\n        \n        Parameters\n        ----------\n    \n        win : list or tuple\n            [x1, x2, y1, y2]\n            \n        \"\"\"\n        \n        if newfig:\n            plt.figure(figsize=figsize)\n            ax1 = None\n            ax2 = None\n            if orientation == 'both':\n                ax1 = plt.axes([0.125, 0.18 + (1 - topfigfrac) * 0.7, (0.9 - 0.125), topfigfrac * 0.7])\n                ax2 = plt.axes([0.125, 0.11, (0.9 - 0.125), (1 - topfigfrac) * 0.7], sharex=ax1)\n            elif orientation[:3] == 'hor':\n                ax1 = plt.subplot()\n            elif orientation[:3] == 'ver':\n                ax2 = plt.subplot()\n        else:\n            if orientation == 'both':\n                fig = plt.gcf()\n                ax1 = fig.axes[0]\n                ax2 = fig.axes[1]\n            elif orientation[:3] == 'hor':\n                fig = plt.gcf()\n                ax1 = fig.axes[0]\n                ax2 = None\n            elif orientation[:3] == 'ver':\n                fig = plt.gcf()\n                ax1 = None\n                ax2 = fig.axes[0]\n        if ax1 is not None:\n            plt.sca(ax1)\n            for e in self.elementlist:\n                e.plot()\n            if orientation[:3] == 'hor':\n                plt.axis('scaled')\n            elif orientation == 'both':\n                plt.axis('equal')  # cannot be 'scaled' when sharing axes\n            if win is not None:\n                plt.axis(win)\n        if ax2 is not None:\n            plt.sca(ax2)\n            for i in range(self.aq.nlayers):\n                if self.aq.ltype[i] == 'l':\n                    plt.axhspan(ymin=self.aq.z[i + 1], ymax=self.aq.z[i], color=[0.8, 0.8, 0.8])\n            for i in range(1, self.aq.nlayers):\n                if self.aq.ltype[i] == 'a' and self.aq.ltype[i - 1] == 'a':\n                    plt.axhspan(ymin=self.aq.z[i], ymax=self.aq.z[i], color=[0.8, 0.8, 0.8])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contour(self, win, ngr=20, layers=0, levels=20, layout=True, labels=True,\n                decimals=0, color=None, newfig=True, figsize=None, legend=True):\n        \"\"\"Contour plot\n        \n        Parameters\n        ----------\n    \n        win : list or tuple\n            [x1, x2, y1, y2]\n        ngr : scalar, tuple or list\n            if scalar: number of grid points in x and y direction\n            if tuple or list: nx, ny, number of grid points in x and y direction\n        layers : integer, list or array\n            layers for which grid is returned\n        levels : integer or array (default 20)\n            levels that are contoured\n        layout : boolean (default True)\n            plot layout of elements\n        labels : boolean (default True)\n            print labels along contours\n        decimals : integer (default 0)\n            number of decimals of labels along contours\n        color : str or list of strings\n            color of contour lines\n        newfig : boolean (default True)\n            create new figure\n        figsize : tuple of 2 values (default is mpl default)\n            size of figure\n        legend : list or boolean (default True)\n            add legend to figure\n            if list of strings: use strings as names in legend\n            \n        \"\"\"\n        \n        x1, x2, y1, y2 = win\n        if np.isscalar(ngr):\n            nx = ny = ngr\n        else:\n            nx, ny = ngr\n        layers = np.atleast_1d(layers)\n        xg = np.linspace(x1, x2, nx)\n        yg = np.linspace(y1, y2, ny)\n        h = self.headgrid(xg, yg, layers)\n        if newfig:\n            plt.figure(figsize=figsize)\n        # color\n        if color is None:\n            c = plt.rcParams['axes.prop_cycle'].by_key()['color']\n        elif type(color) is str:\n            c = len(layers) * [color]\n        elif type(color) is list:\n            c = color\n        if len(c) < len(layers):\n            n = np.ceil(self.aq.naq / len(c))\n            c = n * c \n        # contour\n        cscollectionlist = []\n        for i in range(len(layers)):\n            cs = plt.contour(xg, yg, h[i], levels, colors=c[i])\n            cscollectionlist.append(cs.collections[0])\n            if labels:\n                fmt = '%1.' + str(decimals) + 'f'\n                plt.clabel(cs, fmt=fmt)\n        if type(legend) is list:\n            plt.legend(cscollectionlist, legend)\n        elif legend:\n            legendlist = ['layer ' + str(i) for i in layers]\n            plt.legend(cscollectionlist, legendlist)\n        plt.axis('scaled')\n        if layout:\n            self.plot(win=[x1, x2, y1, y2], newfig=False)", "response": "Contour plot of the contour of the current contour."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vcontour(self, win, n, levels, labels=False, decimals=0, color=None,\n                 vinterp=True, nudge=1e-6, newfig=True, figsize=None, layout=True):\n        \"\"\"Vertical contour\n        \"\"\"\n        x1, x2, y1, y2 = win\n        h = self.headalongline(np.linspace(x1 + nudge, x2 - nudge, n),\n                               np.linspace(y1 + nudge, y2 - nudge, n))\n        L = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n        xg = np.linspace(0, L, n)\n        if vinterp:\n            zg = 0.5 * (self.aq.zaqbot + self.aq.zaqtop)\n            zg = np.hstack((self.aq.zaqtop[0], zg, self.aq.zaqbot[-1]))\n            h = np.vstack((h[0], h, h[-1]))\n        else:\n            zg = np.empty(2 * self.aq.naq)\n            for i in range(self.aq.naq):\n                zg[2 * i] = self.aq.zaqtop[i]\n                zg[2 * i + 1] = self.aq.zaqbot[i]\n            h = np.repeat(h, 2, 0)\n        if newfig:\n            plt.figure(figsize=figsize)\n        cs = plt.contour(xg, zg, h, levels, colors=color)\n        if labels:\n            fmt = '%1.' + str(decimals) + 'f'\n            plt.clabel(cs, fmt=fmt)\n        if layout:\n            self.plot(win=[x1, x2, y1, y2], orientation='ver', newfig=False)", "response": "Vertical contour of the AQ file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw trace lines for the current state of the object.", "response": "def tracelines(self, xstart, ystart, zstart, hstepmax, vstepfrac=0.2,\n                   tmax=1e12, nstepmax=100, silent='.', color=None, orientation='hor',\n                   win=[-1e30, 1e30, -1e30, 1e30], newfig=False, figsize=None):\n        \"\"\"Draw trace lines\n        \"\"\"\n        if color is None:\n            c = plt.rcParams['axes.prop_cycle'].by_key()['color']\n        elif type(color) is str:\n            c = self.aq.naq * [color]\n        elif type(color) is list:\n            c = color\n        if len(c) < self.aq.naq:\n            n = int(np.ceil(self.aq.naq / len(c)))\n            c = n * c \n        fig = plt.gcf()\n        assert len(fig.axes) > 0, 'Error: Need to specify axes in figure before invoking tracelines'\n        ax1 = None\n        ax2 = None\n        if orientation == 'both':\n            ax1 = fig.axes[0]\n            ax2 = fig.axes[1]\n        elif orientation[:3] == 'hor':\n            ax1 = fig.axes[0]\n        elif orientation[:3] == 'ver':\n            ax2 = fig.axes[1]\n            xyztlist = []\n        for i in range(len(xstart)):\n            xyzt, layerlist = timtraceline(self, xstart[i], ystart[i], zstart[i], hstepmax=hstepmax,\n                                           vstepfrac=vstepfrac, tmax=tmax, nstepmax=nstepmax,\n                                           silent=silent, win=win, returnlayers=True)\n            if silent == '.':\n                print('.', end='', flush=True)\n            if ax1 is not None:\n                #plt.axes(ax1)\n                color = [c[self.aq.layernumber[i]] if self.aq.ltype[i] == 'a' else 'k' for i in layerlist]\n                points = np.array([xyzt[:,0], xyzt[:,1]]).T.reshape(-1, 1, 2)\n                segments = np.concatenate([points[:-1], points[1:]], axis=1)\n                lc = LineCollection(segments, colors=color)\n                ax1.add_collection(lc)\n                #ax1.plot(xyzt[:, 0], xyzt[:, 1], color=color)\n            if ax2 is not None:\n                color = [c[self.aq.layernumber[i]] if self.aq.ltype[i] == 'a' else 'k' for i in layerlist]\n                points = np.array([xyzt[:,0], xyzt[:,2]]).T.reshape(-1, 1, 2)\n                segments = np.concatenate([points[:-1], points[1:]], axis=1)\n                lc = LineCollection(segments, colors=color)\n                ax2.add_collection(lc)\n                ax2.set_ylim(self.aq.z[-1], self.aq.z[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndischarge per unit length", "response": "def discharge(self):\n        \"\"\"Discharge per unit length\"\"\"\n        Q = np.zeros(self.aq.naq)\n        Q[self.layers] = self.parameters[:, 0]\n        return Q"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the potential income for a single aquifer.", "response": "def potinf(self, x, y, aq=None):\n        '''Can be called with only one x,y value'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        rv = np.zeros((self.nparam, aq.naq))\n        if aq == self.aq:\n            pot = np.zeros(aq.naq)\n            pot[:] = self.bessel.potbeslsho(float(x), float(y), self.z1, self.z2, aq.lab, 0,\n                                            aq.ilap, aq.naq)\n            rv[:] = self.aq.coef[self.layers] * pot\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disvecinf(self, x, y, aq=None):\n        '''Can be called with only one x,y value'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        rv = np.zeros((2, self.nparam, aq.naq))\n        if aq == self.aq:\n            qxqy = np.zeros((2, aq.naq))\n            qxqy[:, :] = self.bessel.disbeslsho(float(x), float(y), self.z1, self.z2, aq.lab,\n                                                0, aq.ilap, aq.naq)\n            rv[0] = self.aq.coef[self.layers] * qxqy[0]\n            rv[1] = self.aq.coef[self.layers] * qxqy[1]\n        return rv", "response": "Discrete Inverse of Discrete Inverse of Aquifer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef potinf(self, x, y, aq=None):\n        '''Can be called with only one x,y value\n        Returns array(nparam, self.aq.naq) with order\n        order 0, layer[0]\n        order 0, layer[1]\n        ...\n        order 1, layer[0]\n        order 1, layer[1]\n        etc\n        '''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        rv = np.zeros((self.nparam, aq.naq))\n        if aq == self.aq:\n            # clever way of using a reshaped rv here\n            potrv = rv.reshape((self.order + 1, self.nlayers, aq.naq))\n            pot = np.zeros((self.order + 1, aq.naq))\n            pot[:, :] = self.bessel.potbeslsv(float(x), float(y), self.z1, self.z2, aq.lab,\n                                              self.order, aq.ilap, aq.naq)\n            potrv[:] = self.aq.coef[self.layers] * pot[:, np.newaxis, :]\n        return rv", "response": "Returns array of parameters nparam x y value of the aquifer with parameters x y"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the potential inf for a single aquifer", "response": "def potinf(self, x, y, aq=None):\n        '''\n        linesink 0, order 0, layer[0]\n                    order 0, layer[1]\n                    ...\n                    order 1, layer[0]\n                    order 1, layer[1]\n                    ...\n        linesink 1, order 0, layer[0]\n                    order 0, layer[1]\n                    ...\n                    order 1, layer[0]\n                    order 1, layer[1]\n                    ...\n        '''    \n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        rv = np.zeros((self.nls, self.lslist[0].nparam, aq.naq))\n        if aq in self.aq:\n            for i, ls in enumerate(self.lslist):\n                rv[i] = ls.potinf(x, y, aq)\n        rv.shape = (self.nparam, aq.naq)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discharge(self):\n        \n        rv = np.zeros(self.aq[0].naq)\n        Qls = self.parameters[:, 0] * self.dischargeinf()\n        Qls.shape = (self.nls, self.nlayers, self.order + 1)\n        Qls = np.sum(Qls, 2)\n        for i, q in enumerate(Qls):\n            rv[self.layers[i]] += q\n            #rv[self.layers] = np.sum(Qls.reshape(self.nls * (self.order + 1), self.nlayers), 0)\n        return rv", "response": "Discharge of the element in each layer in each layer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning that computes the potentials for a single object.", "response": "def potbeslsho(x, y, z1, z2, labda, order, ilap, naq):\n    \"\"\"\n    Parameters\n    ---------- \n       x,y: Point where potential is computed\n       z1: Complex begin point of line-sink\n       z2: Complex end point of line-sink\n       labda(naq): labda's (zero for first labda if Laplace)\n       order: Order of the line-sink\n       ilap: equals 1 when first value is Laplace line-sink and first labda equals zero\n       naq: Number of aquifers\n       rv(naq): Array to store return value (must be pre-allocated)\n    Returns\n    --------\n       rv(naq): Potentials. First spot is Laplace value if ilap=1\n    \"\"\"\n\n    rv = np.zeros(naq)\n\n    # lstype = 1 means line-sink\n    lstype = 1\n\n    # Radius of convergence\n    Rconv = 7.0\n\n    # if (ilap==1) :\n    #    istart = 1\n    # else:\n    #    istart = 0\n    zin, z1in, z2in, Lin, z, zplus1, zmin1 = prepare_z(x, y, z1, z2)\n    # Laplace linesink\n    if ilap == 1:\n        power = order + 1\n        pcor = np.complex(0.0, 0.0)\n        for n in range(1, int((power + 1) / 2) + 1):\n            pcor = pcor + z ** (power - 2 * n + 1) / (2 * n - 1)\n        pcor = 2.0 * pcor\n        comega = (\n            z ** power * np.log((zmin1) / (zplus1))\n            + pcor\n            - np.log(zmin1)\n            + (-1.0) ** power * np.log(zplus1)\n        )\n        comega = -comega * Lin / (4.0 * np.pi * power)\n        rv[0] = np.real(comega)\n    # N-1 leakage factors\n    for i in range(ilap, naq):\n        # Check whether entire linesink is outside radius of convergence\n        # Outside if |z-zc|>L/2+7lab, and thus |Z|>1+7lab*2/L, or |zeta|>1/biglab+7 (zeta is called z here)\n        biglab = 2.0 * labda[i] / Lin\n        z = (2.0 * zin - (z1in + z2in)) / (z2in - z1in) / biglab\n\n        if abs(z) < (Rconv + 1.0 / biglab):\n            pot = IntegralF(zin, z1in, z2in, Lin, labda[i], order, Rconv, lstype)\n            rv[i] = -Lin / 2.0 * pot\n        else:\n            rv[i] = 0.0\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns array of size len ( layers ) only used in building equations Defined here as it is the particular solution inside a semi - conjuncted aquifer", "response": "def potentiallayers(self, x, y, layers, aq=None):\n        '''Returns array of size len(layers) only used in building equations\n        Defined here as it is the particular solution inside a semi-confined aquifer\n        and cannot be added by using eigen vectors'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        pot = np.zeros(len(layers))\n        if aq == self.aq:\n            pot[:] = self.potstar[layers]\n        return pot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmix - in class that returns matrix rows for potential - specified conditions.", "response": "def equation(self):\n        \"\"\"Mix-in class that returns matrix rows for potential-specified conditions.\n        Returns matrix part (nunknowns,neq)\n        Returns rhs part nunknowns\n        \"\"\"\n        mat = np.empty((self.nunknowns, self.model.neq))\n        # rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        rhs = self.pc.copy()\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            # rhs[istart:istart+self.nlayers] = self.pc[]\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = \\\n                        e.potinflayers(self.xc[icp], self.yc[icp], self.layers)\n                    if e == self:\n                        mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] -= self.resfac[icp]\n                    ieq += e.nunknowns\n                else:\n                    rhs[istart:istart + self.nlayers] -= \\\n                        e.potentiallayers(self.xc[icp], self.yc[icp],\n                                          self.layers)  # Pretty cool that this works, really\n        return mat, rhs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmixing - in class that returns matrix rows for mscreen condition applied at each control point separately.", "response": "def equation(self):\n        \"\"\"Mix-in class that returns matrix rows for mscreen condition.\n        Mscreen condition applied at each control point separately (so not like in ditch).\n        Returns matrix part (nunknowns,neq)\n        Returns rhs part (nunknowns)\n        \"\"\"\n        mat = np.zeros((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        rhs[0:self.nlayers - 1] = 0.0\n        rhs[self.nlayers - 1] = self.Qc\n        ieq = 0\n        for e in self.model.elementlist:\n            if e.nunknowns > 0:\n                head = e.potinflayers(self.xc, self.yc, self.layers) / self.aq.Tcol[self.layers, :]\n                mat[0:self.nlayers - 1, ieq:ieq + e.nunknowns] = head[:-1] - head[1:]\n                if e == self:\n                    for i in range(self.nlayers - 1):\n                        mat[i, ieq + i] -= self.resfac[i]\n                        mat[i, ieq + i + 1] += self.resfac[i + 1]\n                    mat[self.nlayers - 1, ieq:ieq + self.nlayers] = 1.0\n                ieq += e.nunknowns\n            else:\n                head = e.potentiallayers(self.xc, self.yc, self.layers) / self.aq.T[self.layers]\n                rhs[0:self.nlayers - 1] -= head[:-1] - head[1:]\n        return mat, rhs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmixes - in class that returns matrix rows for zero normal flux condition.", "response": "def equation(self):\n        \"\"\"Mix-in class that returns matrix rows for zero normal flux condition.\n        Using the control point on the outside\n        Returns matrix part (nunknowns,neq)\n        Returns rhs part nunknowns\n        \"\"\"\n        mat = np.empty((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    qx, qy = e.disvecinflayers(self.xcout[icp], self.ycout[icp], self.layers)\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = \\\n                        qx * self.cosnorm[icp] + qy * self.sinnorm[icp]\n                    ieq += e.nunknowns\n                else:\n                    qx, qy = e.disveclayers(self.xcout[icp], self.ycout[icp], self.layers)\n                    rhs[istart:istart + self.nlayers] -= qx * self.cosnorm[icp] + qy * self.sinnorm[icp]\n        return mat, rhs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef equation(self):\n        mat = np.empty((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    qx, qy = e.disvecinflayers(self.xc[icp], self.yc[icp], self.layers)\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = \\\n                        qx * self.cosnorm[icp] + qy * self.sinnorm[icp] - self.resfac[:, np.newaxis] * \\\n                        (e.potinflayers(self.xcin[icp], self.ycin[icp], self.layers, aq=self.aq) / self.aq.Tcol[\n                            self.layers] - \\\n                         e.potinflayers(self.xcout[icp], self.ycout[icp], self.layers, aq=self.aq) / self.aq.Tcol[\n                             self.layers])\n                    ieq += e.nunknowns\n                else:\n                    qx, qy = e.disveclayers(self.xc[icp], self.yc[icp], self.layers)\n                    rhs[istart:istart + self.nlayers] -= qx * self.cosnorm[icp] + qy * self.sinnorm[icp] + self.resfac * \\\n                                                          (e.potentiallayers(self.xcin[icp], self.ycin[icp], self.layers,\n                                                                             aq=self.aq) / self.aq.T[self.layers] -\n                                                           e.potentiallayers(self.xcout[icp], self.ycout[icp],\n                                                                             self.layers, aq=self.aq) / self.aq.T[\n                                                              self.layers])\n        return mat, rhs", "response": "Mix - in class that returns matrix rows for leaky wall condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef equation(self):\n        mat = np.empty((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = \\\n                        e.potinflayers(self.xcin[icp], self.ycin[icp], self.layers, aq=self.aqin) / self.aqin.Tcol - \\\n                        e.potinflayers(self.xcout[icp], self.ycout[icp], self.layers, aq=self.aqout) / self.aqout.Tcol\n                    ieq += e.nunknowns\n                else:\n                    rhs[istart:istart + self.nlayers] -= \\\n                        e.potentiallayers(self.xcin[icp], self.ycin[icp], self.layers, aq=self.aqin) / self.aqin.T - \\\n                        e.potentiallayers(self.xcout[icp], self.ycout[icp], self.layers, aq=self.aqout) / self.aqout.T\n        return mat, rhs", "response": "Mix - in class that returns matrix rows for difference in head between inside and\n        outside equals zeros\n        Returns matrix part nunknowns - > rhs part nunknowns - > rhs part nunknowns - > rhs part"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef equation(self):\n        mat = np.empty((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    qxin, qyin = e.disvecinflayers(self.xcin[icp], self.ycin[icp], self.layers, aq=self.aqin)\n                    qxout, qyout = e.disvecinflayers(self.xcout[icp], self.ycout[icp], self.layers, aq=self.aqout)\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = \\\n                        (qxin - qxout) * self.cosnorm[icp] + (qyin - qyout) * self.sinnorm[icp]\n                    ieq += e.nunknowns\n                else:\n                    qxin, qyin = e.disveclayers(self.xcin[icp], self.ycin[icp], self.layers, aq=self.aqin)\n                    qxout, qyout = e.disveclayers(self.xcout[icp], self.ycout[icp], self.layers, aq=self.aqout)\n                    rhs[istart:istart + self.nlayers] -= (qxin - qxout) * self.cosnorm[icp] + (qyin - qyout) * \\\n                                                         self.sinnorm[icp]\n        return mat, rhs", "response": "Mix - in class that returns matrix rows for difference in head between inside and\n        outside equals zeros\n        Returns matrix part nunknowns - > rhs part nunknowns - > rhs part"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef equation(self):\n        mat = np.empty((self.nunknowns, self.model.neq))\n        rhs = np.zeros(self.nunknowns)  # Needs to be initialized to zero\n        for icp in range(self.ncp):\n            istart = icp * self.nlayers\n            ieq = 0\n            for e in self.model.elementlist:\n                if e.nunknowns > 0:\n                    fluxin = self.intflux(e.disvecinflayers, self.xcin[icp], self.ycin[icp],\n                                          self.xcin[icp + 1], self.ycin[icp + 1], self.layers, aq=self.aqin)\n                    fluxout = self.intflux(e.disvecinflayers, self.xcout[icp], self.ycout[icp],\n                                           self.xcout[icp + 1], self.ycout[icp + 1], self.layers, aq=self.aqout)\n                    mat[istart:istart + self.nlayers, ieq:ieq + e.nunknowns] = fluxin - fluxout\n                    ieq += e.nunknowns\n                else:\n                    fluxin = self.intflux(e.disveclayers, self.xcin[icp], self.ycin[icp],\n                                          self.xcin[icp + 1], self.ycin[icp + 1], self.layers, aq=self.aqin)\n                    fluxout = self.intflux(e.disveclayers, self.xcout[icp], self.ycout[icp],\n                                           self.xcout[icp + 1], self.ycout[icp + 1], self.layers, aq=self.aqout)\n                    rhs[istart:istart + self.nlayers] -= fluxin - fluxout\n        return mat, rhs", "response": "Mix - in class that returns matrix rows for difference in head between inside and\n        outside equals zeros\n        Returns matrix part nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - > nunknowns - >"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef potinflayers(self, x, y, layers, aq=None):\n        '''Returns array of size (len(layers),nparam)\n        only used in building equations'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        pot = self.potinf(x, y, aq)  # nparam rows, naq cols\n        rv = np.sum(pot[:,np.newaxis,:] * aq.eigvec, 2).T  # Transopose as the first axes needs to be the number of layers\n        return rv[layers,:]", "response": "Returns array of size ( len layers ) only used in building equations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning array of size len ( layers ) only used in building equations", "response": "def potentiallayers(self, x, y, layers, aq=None):\n        '''Returns array of size len(layers)\n        only used in building equations'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        pot = np.sum(self.potential(x, y, aq) * aq.eigvec, 1 )\n        return pot[layers]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disvec(self, x, y, aq=None):\n        '''Returns array of size (2, nparam, naq)'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        return np.sum(self.parameters * self.disvecinf(x, y, aq), 1)", "response": "Returns array of size 2 nparam naq"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disvecinflayers(self, x, y, layers, aq=None):\n        '''Returns two arrays of size (len(layers),nparam)\n        only used in building equations'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        qxqy = self.disvecinf(x, y, aq)  # nparam rows, naq cols\n        qx = np.sum(qxqy[0,:,np.newaxis,:] * aq.eigvec, 2).T  # Transpose as the first axes needs to be the number of layers\n        qy = np.sum(qxqy[1,:,np.newaxis,:] * aq.eigvec, 2).T\n        return np.array((qx[layers], qy[layers]))", "response": "Returns two arrays of size (len ( layers ) nparam )\n        only used in building equations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn two arrays of size len ( layers ) only used in building equations", "response": "def disveclayers(self, x, y, layers, aq=None):\n        '''Returns two arrays of size len(layers)\n        only used in building equations'''\n        if aq is None: aq = self.model.aq.find_aquifer_data(x, y)\n        qxqy = self.disvec(x, y, aq)\n        rv = np.sum(qxqy[:,np.newaxis,:] * aq.eigvec, 2)\n        return rv[:,layers]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef headinside(self):\n        \n        h = self.model.head(self.xw + self.rw, self.yw, layers=self.layers)\n        return h - self.resfac * self.parameters[:, 0]", "response": "The head inside the well for each screen"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a capture zone for a set of path lines.", "response": "def capzone(self, nt=10, zstart=None, hstepmax=10, vstepfrac=0.2,\n                tmax=None, nstepmax=100, silent='.'):\n        \"\"\"Compute a capture zone\n        \n        Parameters\n        ----------\n        nt : int\n            number of path lines\n        zstart : scalar\n            starting elevation of the path lines\n        hstepmax : scalar\n            maximum step in horizontal space\n        vstepfrac : float\n            maximum fraction of aquifer layer thickness during one step\n        tmax : scalar\n            maximum time\n        nstepmax : scalar(int)\n            maximum number of steps\n        silent : boolean or string\n            True (no messages), False (all messages), or '.'\n            (print dot for each path line)\n            \n        Returns\n        -------\n        xyzt : list of arrays of x, y, z, and t values\n            \n        \"\"\"\n        xstart, ystart, zstart = self.capzonestart(nt, zstart)\n        xyzt = timtracelines(self.model, xstart, ystart, zstart, -np.abs(hstepmax), \\\n                             vstepfrac=0.2, tmax=tmax, nstepmax=100, silent='.')\n        return xyzt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plotcapzone(self, nt=10, zstart=None, hstepmax=20, vstepfrac=0.2,\n                    tmax=365, nstepmax=100, silent='.', color=None, orientation='hor',\n                   win=[-1e30, 1e30, -1e30, 1e30], newfig=False, figsize=None):\n        \"\"\"Plot a capture zone\n        \n        Parameters\n        ----------\n        nt : int\n            number of path lines\n        zstart : scalar\n            starting elevation of the path lines\n        hstepmax : scalar\n            maximum step in horizontal space\n        vstepfrac : float\n            maximum fraction of aquifer layer thickness during one step\n        tmax : scalar\n            maximum time\n        nstepmax : scalar(int)\n            maximum number of steps\n        silent : boolean or string\n            True (no messages), False (all messages), or '.'\n            (print dot for each path line)\n        color : color\n        orientation : string\n            'hor' for horizontal, 'ver' for vertical, or 'both' for both\n        win : array_like (length 4)\n            [xmin, xmax, ymin, ymax]\n        newfig : boolean (default False)\n            boolean indicating if new figure should be created\n        figsize : tuple of integers, optional, default: None\n            width, height in inches.\n            \n        \"\"\"\n        \n        xstart, ystart, zstart = self.capzonestart(nt, zstart)\n        self.model.tracelines(xstart, ystart, zstart, hstepmax=-abs(hstepmax), vstepfrac=vstepfrac,\n                   tmax=tmax, nstepmax=nstepmax, silent=silent, color=color, orientation=orientation,\n                   win=win, newfig=newfig, figsize=figsize)", "response": "Plot a capture zone for a specific aquifer layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn layer - number layer - type and model - layer - number", "response": "def findlayer(self, z):\n        '''\n        Returns layer-number, layer-type and model-layer-number'''\n        if z > self.z[0]:\n            modellayer, ltype = -1, 'above'\n            layernumber = None\n        elif z < self.z[-1]:\n            modellayer, ltype = len(self.layernumber), 'below'\n            layernumber = None\n        else:\n            modellayer = np.argwhere((z <= self.z[:-1]) & (z >= self.z[1:]))[0, 0]\n            layernumber = self.layernumber[modellayer]\n            ltype = self.ltype[modellayer] \n        return layernumber, ltype, modellayer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_element(self, e):\n        \n        if e.label is not None: self.elementdict.pop(e.label)\n        self.elementlist.remove(e)", "response": "Remove element e from model\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disvec(self, x, y, aq=None):\n        \n        if aq is None: aq = self.aq.find_aquifer_data(x, y)\n        rv = np.zeros((2, aq.naq))\n        for e in aq.elementlist:\n            rv += e.disvec(x, y, aq)\n        rv = np.sum(rv[:, np.newaxis, :] * aq.eigvec, 2)\n        return rv", "response": "Discharge vector at x y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the head of the object at x y.", "response": "def head(self, x, y, layers=None, aq=None):\n        \"\"\"Head at `x`, `y`\n        \n        Returns\n        -------\n        \n        h : array length `naq` or `len(layers)`\n            head in all `layers` (if not `None`), or all layers of aquifer (otherwise)\n        \"\"\"\n        \n        if aq is None: aq = self.aq.find_aquifer_data(x, y)\n        rv = self.potential(x, y, aq) / aq.T\n        if layers is None:\n            return rv\n        else:\n            return rv[layers]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn array of heads for each element of the array xg and yg.", "response": "def headgrid(self, xg, yg, layers=None, printrow=False):\n        \"\"\"Grid of heads\n        \n        Parameters\n        ----------\n        xg : array\n            x values of grid\n        yg : array\n            y values of grid\n        layers : integer, list or array, optional\n            layers for which grid is returned\n        printrow : boolean, optional\n            prints dot to screen for each row of grid if set to `True`\n        \n        Returns\n        -------\n        h : array size `nlayers, ny, nx`\n        \n        See also\n        --------\n        \n        :func:`~timml.model.Model.headgrid2`\n\n        \"\"\"\n        \n        nx, ny = len(xg), len(yg)\n        if layers is None:\n            Nlayers = self.aq.find_aquifer_data(xg[0], yg[0]).naq\n        else:\n            Nlayers = len(np.atleast_1d(layers))\n        h = np.empty((Nlayers, ny, nx))\n        for j in range(ny):\n            if printrow:\n                print('.', end='', flush=True)\n            for i in range(nx):\n                h[:, j, i] = self.head(xg[i], yg[j], layers)\n        if printrow:\n            print('', flush=True)\n        return h"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the head of the aquifer with the given x and y values along a line or curve.", "response": "def headalongline(self, x, y, layers=None):\n        \"\"\"Head along line or curve\n        \n        Parameters\n        ----------\n        x : array\n            x values of line\n        y : array\n            y values of line\n        layers : integer, list or array, optional\n            layers for which grid is returned\n        \n        Returns\n        -------\n        h : array size `nlayers, nx`\n\n        \"\"\"\n        \n        xg, yg = np.atleast_1d(x), np.atleast_1d(y)\n        if layers is None:\n            Nlayers = self.aq.find_aquifer_data(xg[0], yg[0]).naq\n        else:\n            Nlayers = len(np.atleast_1d(layers))\n        nx = len(xg)\n        if len(yg) == 1:\n            yg = yg * np.ones(nx)\n        h = np.zeros((Nlayers, nx))\n        for i in range(nx):\n            h[:, i] = self.head(xg[i], yg[i], layers)\n        return h"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn Qx Qy for each element of x and y.", "response": "def disvecalongline(self, x, y, layers=None):\n        '''Returns Qx[Nlayers,len(x)], Qy[Nlayers,len(x)]\n        Assumes same number of layers for each x and y\n        layers may be None or list of layers for which head is computed'''\n        xg, yg = np.atleast_1d(x), np.atleast_1d(y)\n        if layers is None:\n            Nlayers = self.aq.find_aquifer_data(xg[0], yg[0]).naq\n        else:\n            Nlayers = len(np.atleast_1d(layers))\n        nx = len(xg)\n        if len(yg) == 1:\n            yg = yg * np.ones(nx)\n        Qx = np.zeros((Nlayers, nx))\n        Qy = np.zeros((Nlayers, nx))\n        for i in range(nx):\n            Qx[:, i], Qy[:, 1] = self.disvec(xg[i], yg[i], layers)\n        return Qx, Qy"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the solution of the undirected system of elements.", "response": "def solve(self, printmat=0, sendback=0, silent=False):\n        '''Compute solution'''\n        # Initialize elements\n        self.initialize()\n        # Compute number of equations\n        self.neq = np.sum([e.nunknowns for e in self.elementlist])\n        if self.neq == 0: return\n        if silent is False:\n            print('Number of elements, Number of equations:', len(\n                self.elementlist), ',', self.neq)\n        if self.neq == 0:\n            if silent is False: print('No unknowns. Solution complete')\n            return\n        mat = np.empty((self.neq, self.neq))\n        rhs = np.empty(self.neq)\n        ieq = 0\n        for e in self.elementlist:\n            if e.nunknowns > 0:\n                mat[ieq:ieq + e.nunknowns, :], rhs[ieq:ieq + e.nunknowns] = \\\n                e.equation()\n                ieq += e.nunknowns\n            if silent is False:\n                print('.', end='', flush=True)\n        if printmat:\n            return mat, rhs\n        sol = np.linalg.solve(mat, rhs)\n        icount = 0\n        for e in self.elementlist:\n            if e.nunknowns > 0:\n                e.setparams(sol[icount:icount + e.nunknowns])\n                icount += e.nunknowns\n        if silent is False:\n            print()  # needed cause the dots are printed\n            print('solution complete')\n        elif (silent == 'dot') or (silent == '.'):\n            print('.', end='', flush=True)\n        if sendback:\n            return sol\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes solution with multiprocessing implementation.", "response": "def solve_mp(self, nproc=4, printmat=0, sendback=0, silent=False):\n        '''Compute solution, multiprocessing implementation.\n        Note: estimated speedup approximately by factor of\n        number of physical cores. Virtual cores do not improve\n        calculation time.'''\n        # Initialize elements\n        self.initialize()\n        # Compute number of equations\n        self.neq = np.sum([e.nunknowns for e in self.elementlist])\n        if self.neq == 0: return\n        if silent is False:\n            print('Number of elements, Number of equations:', len(\n                self.elementlist), ',', self.neq)\n        if self.neq == 0:\n            if silent is False: print('No unknowns. Solution complete')\n            return\n        mat = np.empty((self.neq, self.neq))\n        rhs = np.empty(self.neq)\n\n        # start multiprocessing\n        if nproc is None:\n            nproc = mp.cpu_count() - 1  # make no. of processes equal to 1 less than no. of cores\n        elif nproc > mp.cpu_count():\n            print(\"Given 'nproc' larger than no. of cores on machine. Setting 'nproc' to {}.\".format(mp.cpu_count()))\n            nproc = mp.cpu_count()\n\n        pool = mp.Pool(processes=nproc)\n        results = []\n        for e in self.elementlist:\n            if e.nunknowns > 0:\n                results.append(pool.apply_async(e.equation))\n            if silent is False:\n                print('.', end='', flush=True)\n\n        pool.close()\n        pool.join()\n\n        mat = np.empty((self.neq, self.neq))\n        rhs = np.zeros(self.neq)\n\n        ieq = 0\n\n        for p in results:\n            imat, irhs = p.get()\n            mat[ieq:ieq + imat.shape[0], :] = imat\n            rhs[ieq:ieq + irhs.shape[0]] = irhs\n            ieq += imat.shape[0]\n\n        # end multiprocessing\n\n        if printmat:\n            return mat, rhs\n        sol = np.linalg.solve(mat, rhs)\n        icount = 0\n        for e in self.elementlist:\n            if e.nunknowns > 0:\n                e.setparams(sol[icount:icount + e.nunknowns])\n                icount += e.nunknowns\n        if silent is False:\n            print()  # needed cause the dots are printed\n            print('solution complete')\n        elif (silent == 'dot') or (silent == '.'):\n            print('.', end='', flush=True)\n        if sendback:\n            return sol\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread files into setup", "response": "def read_files(*files):\n    \"\"\"Read files into setup\"\"\"\n    text = \"\"\n    for single_file in files:\n        content = read(single_file)\n        text = text + content + \"\\n\"\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a file into setup", "response": "def read(afile):\n    \"\"\"Read a file into setup\"\"\"\n    the_relative_file = os.path.join(HERE, afile)\n    with codecs.open(the_relative_file, 'r', 'utf-8') as opened_file:\n        content = filter_out_test_code(opened_file)\n        content = \"\".join(list(content))\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        prog=constants.PROGRAM_NAME, description=constants.PROGRAM_DESCRIPTION\n    )\n    parser.add_argument(\n        \"-cd\",\n        \"--%s\" % constants.LABEL_CONFIG_DIR,\n        help=\"the directory for configuration file lookup\",\n    )\n    parser.add_argument(\n        \"-c\", \"--%s\" % constants.LABEL_CONFIG, help=\"the dictionary file\"\n    )\n    parser.add_argument(\n        \"-td\",\n        \"--%s\" % constants.LABEL_TMPL_DIRS,\n        nargs=\"*\",\n        help=\"the directories for template file lookup\",\n    )\n    parser.add_argument(\n        \"-t\", \"--%s\" % constants.LABEL_TEMPLATE, help=\"the template file\"\n    )\n    parser.add_argument(\n        \"-o\", \"--%s\" % constants.LABEL_OUTPUT, help=\"the output file\"\n    )\n    parser.add_argument(\n        \"--%s\" % constants.LABEL_TEMPLATE_TYPE,\n        help=\"the template type, default is jinja2\",\n    )\n    parser.add_argument(\n        \"-f\",\n        action=\"store_true\",\n        dest=constants.LABEL_FORCE,\n        default=False,\n        help=\"force moban to template all files despite of .moban.hashes\",\n    )\n    parser.add_argument(\n        \"--%s\" % constants.LABEL_EXIT_CODE,\n        action=\"store_true\",\n        dest=constants.LABEL_EXIT_CODE,\n        default=False,\n        help=\"tell moban to change exit code\",\n    )\n    parser.add_argument(\n        \"-m\", \"--%s\" % constants.LABEL_MOBANFILE, help=\"custom moban file\"\n    )\n    parser.add_argument(\n        \"-g\", \"--%s\" % constants.LABEL_GROUP, help=\"a subset of targets\"\n    )\n    parser.add_argument(\n        constants.POSITIONAL_LABEL_TEMPLATE,\n        metavar=\"template\",\n        type=str,\n        nargs=\"?\",\n        help=\"string templates\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--%s\" % constants.LABEL_VERSION,\n        action=\"version\",\n        version=\"%(prog)s {v}\".format(v=__version__),\n    )\n    return parser", "response": "construct the command line parser for the base class"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the MOBAN file.", "response": "def handle_moban_file(moban_file, options):\n    \"\"\"\n    act upon default moban file\n    \"\"\"\n    moban_file_configurations = load_data(None, moban_file)\n    if moban_file_configurations is None:\n        raise exceptions.MobanfileGrammarException(\n            constants.ERROR_INVALID_MOBAN_FILE % moban_file\n        )\n    if (\n        constants.LABEL_TARGETS not in moban_file_configurations\n        and constants.LABEL_COPY not in moban_file_configurations\n    ):\n        raise exceptions.MobanfileGrammarException(\n            constants.ERROR_NO_TARGETS % moban_file\n        )\n    check_none(moban_file_configurations, moban_file)\n    version = moban_file_configurations.get(\n        constants.MOBAN_VERSION, constants.DEFAULT_MOBAN_VERSION\n    )\n    if version == constants.DEFAULT_MOBAN_VERSION:\n        mobanfile.handle_moban_file_v1(moban_file_configurations, options)\n    else:\n        raise exceptions.MobanfileGrammarException(\n            constants.MESSAGE_FILE_VERSION_NOT_SUPPORTED % version\n        )\n    HASH_STORE.save_hashes()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_none(data, moban_file):\n    if isinstance(data, dict):\n        for k, v in data.items():\n            if check_none(v, moban_file) is None:\n                loc = data.lc.key(k)\n                raise exceptions.MobanfileGrammarException(\n                    constants.ERROR_MALFORMED_YAML\n                    % (moban_file, loc[0] + 1)  # line number starts from 0\n                )\n    elif isinstance(data, list):\n        for i, x in enumerate(data):\n            if check_none(x, moban_file) is None:\n                loc = data.lc.item(i)\n                raise exceptions.MobanfileGrammarException(\n                    constants.ERROR_MALFORMED_YAML % (moban_file, loc[0] + 1)\n                )\n    return data", "response": "check whether the yaml data has empty value such as :\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_command_line(options):\n    options = merge(options, constants.DEFAULT_OPTIONS)\n    engine = plugins.ENGINES.get_engine(\n        options[constants.LABEL_TEMPLATE_TYPE],\n        options[constants.LABEL_TMPL_DIRS],\n        options[constants.LABEL_CONFIG_DIR],\n    )\n    if options[constants.LABEL_TEMPLATE] is None:\n        if options[constants.POSITIONAL_LABEL_TEMPLATE] is None:\n            raise exceptions.NoTemplate(constants.ERROR_NO_TEMPLATE)\n        else:\n            engine.render_string_to_file(\n                options[constants.POSITIONAL_LABEL_TEMPLATE],\n                options[constants.LABEL_CONFIG],\n                options[constants.LABEL_OUTPUT],\n            )\n    else:\n        engine.render_to_file(\n            options[constants.LABEL_TEMPLATE],\n            options[constants.LABEL_CONFIG],\n            options[constants.LABEL_OUTPUT],\n        )\n    engine.report()\n    HASH_STORE.save_hashes()\n    exit_code = reporter.convert_to_shell_exit_code(\n        engine.number_of_templated_files()\n    )\n    return exit_code", "response": "act upon command line options\n    act upon command line options\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _data():\n    global _DATA\n    if _DATA is None:\n        _DATA = {}\n        with open(resource_filename(__name__, 'data.json'), 'r', encoding='utf-8') as f:\n            _DATA['alpha3'] = {k: Currency(**v) for k, v in json.load(f).items()}\n\n        _DATA['code_num'] = {d.code_num: d for d in _DATA['alpha3'].values() if d.code_num is not None}\n        _DATA['symbol'] = defaultdict(list)\n        for d in _DATA['alpha3'].values():\n            for s in d.symbols:\n                _DATA['symbol'][s] += [d]\n\n        for s, d in _DATA['symbol'].items():\n            _DATA['symbol'][s] = sorted(d, key=lambda d: 10000 if d.code_num is None else d.code_num)\n\n        _DATA['name'] = {}\n        for d in _DATA['alpha3'].values():\n            if d.name in _DATA['name']:\n                assert 'Duplicate name \"{}\"!'.format(d.name)\n            _DATA['name'][d.name] = d\n\n        _DATA['country'] = defaultdict(list)\n        for d in _DATA['alpha3'].values():\n            for cc in d.countries:\n                _DATA['country'][cc] += [d]\n\n        for s, d in _DATA['country'].items():\n            _DATA['country'][s] = sorted(d, key=lambda d: (\n                int(d.symbols == []),  # at least one symbol\n                10000 if d.code_num is None else d.code_num,  # official first\n                len(d.countries),  # the fewer countries the more specific\n            ))\n\n    return _DATA", "response": "Lazy load index data structure for currences\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets list of possible currencies for symbol ; filter by country_code", "response": "def by_symbol(symbol, country_code=None):\n    \"\"\"Get list of possible currencies for symbol; filter by country_code\n\n    Look for all currencies that use the `symbol`. If there are currencies used\n    in the country of `country_code`, return only those; otherwise return all\n    found currencies.\n\n    Parameters:\n        symbol: unicode                  Currency symbol.\n        country_code: Optional[unicode]  Iso3166 alpha2 country code.\n\n    Returns:\n        List[Currency]: Currency objects for `symbol`; filter by country_code.\n    \"\"\"\n    res = _data()['symbol'].get(symbol)\n    if res:\n        tmp_res = []\n        for d in res:\n            if country_code in d.countries:\n                tmp_res += [d]\n\n        if tmp_res:\n            return tmp_res\n        if country_code is None:\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget list of possible currencies where the symbol is in value ; filter similar to by_symbol.", "response": "def by_symbol_match(value, country_code=None):\n    \"\"\"Get list of possible currencies where the symbol is in value; filter by country_code (iso3166 alpha2 code)\n\n    Look for first matching currency symbol in `value`. Filter similar to `by_symbol`.\n    Symbols are sorted by length and relevance of first character (see `_symbols()`).\n\n    Symbols are considered to be currency symbols, alpha3 codes or currency names.\n\n    Note: This is a [heuristic](https://en.wikipedia.org/wiki/Heuristic) !\n\n    Parameters:\n        value: unicode                   Some input string.\n        country_code: Optional[unicode]  Iso3166 alpha2 country code.\n\n    Returns:\n        List[Currency]: Currency objects found in `value`; filter by country_code.\n    \"\"\"\n    res = None\n    for s, group in _symbols():\n        if s.lower() in value.lower():\n            if group == 'symbol':\n                res = by_symbol(s, country_code)\n            if group == 'alpha3':\n                res = [by_alpha3(s)]\n            if group == 'name':\n                res = [_data()['name'][s]]\n            if res and country_code is not None:\n                res = [\n                    currency\n                    for currency in res\n                    if country_code in currency.countries\n                ]\n            if res:\n                return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(v, country_code=None):\n    if isinstance(v, int):\n        res = by_code_num(v)\n        return [] if not res else [res]\n\n    if not isinstance(v, (str, unicode)):\n        raise ValueError('`v` of incorrect type {}. Only accepts str, bytes, unicode and int.')\n\n    # check alpha3\n    if re.match('^[A-Z]{3}$', v):\n        res = by_alpha3(v)\n        if res:\n            return [res]\n\n    # check by symbol\n    res = by_symbol(v, country_code)\n    if res:\n        return res\n\n    # check by country code\n    res = by_country(v)\n    if res:\n        return res\n\n    # more or less fuzzy match by symbol\n    res = by_symbol_match(v, country_code)\n    if res:\n        return res", "response": "Try to parse v to currencies ; filter by country_code\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns json contents as string", "response": "def open_json(file_name):\n    \"\"\"\n    returns json contents as string\n    \"\"\"\n    with open(file_name, \"r\") as json_data:\n        data = json.load(json_data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshut down all connections and context and cleans up the Twisted reactor.", "response": "def shutdown(self):\n        \"\"\"\n        Shutdown factory.\n\n        This is shutting down all created connections\n        and terminating ZeroMQ context. Also cleans up\n        Twisted reactor.\n        \"\"\"\n        for connection in self.connections.copy():\n            connection.shutdown()\n\n        self.connections = None\n\n        self.context.term()\n        self.context = None\n        if self.trigger:\n            try:\n                self.reactor.removeSystemEventTrigger(self.trigger)\n            except Exception:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish(self, message, tag=b''):\n        self.send(tag + b'\\0' + message)", "response": "Publish a message with specified tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef messageReceived(self, message):\n        if len(message) == 2:\n            # compatibility receiving of tag as first part\n            # of multi-part message\n            self.gotMessage(message[1], message[0])\n        else:\n            self.gotMessage(*reversed(message[0].split(b'\\0', 1)))", "response": "Called by the protocol to process a single or multiple message from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an unique id.", "response": "def _getNextId(self):\n        \"\"\"\n        Returns an unique id.\n\n        By default, generates pool of UUID in increments\n        of ``UUID_POOL_GEN_SIZE``. Could be overridden to\n        provide custom ID generation.\n\n        :return: generated unique \"on the wire\" message ID\n        :rtype: str\n        \"\"\"\n        if not self._uuids:\n            for _ in range(self.UUID_POOL_GEN_SIZE):\n                self._uuids.append(uuid.uuid4().bytes)\n        return self._uuids.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreleasing the message ID to the pool.", "response": "def _releaseId(self, msgId):\n        \"\"\"\n        Release message ID to the pool.\n\n        @param msgId: message ID, no longer on the wire\n        @type msgId: C{str}\n        \"\"\"\n        self._uuids.append(msgId)\n        if len(self._uuids) > 2 * self.UUID_POOL_GEN_SIZE:\n            self._uuids[-self.UUID_POOL_GEN_SIZE:] = []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncancel outstanding REQ drop reply silently.", "response": "def _cancel(self, msgId):\n        \"\"\"\n        Cancel outstanding REQ, drop reply silently.\n\n        @param msgId: message ID to cancel\n        @type msgId: C{str}\n        \"\"\"\n        _, canceller = self._requests.pop(msgId, (None, None))\n\n        if canceller is not None and canceller.active():\n            canceller.cancel()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _timeoutRequest(self, msgId):\n        d, _ = self._requests.pop(msgId, (None, None))\n        if not d.called:\n            d.errback(ZmqRequestTimeoutError(msgId))", "response": "Cancel timedout request.\n        @param msgId: message ID to cancel\n        @type msgId: C{str}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sendMsg(self, *messageParts, **kwargs):\n        messageId = self._getNextId()\n        d = defer.Deferred(canceller=lambda _: self._cancel(messageId))\n\n        timeout = kwargs.pop('timeout', None)\n        if timeout is None:\n            timeout = self.defaultRequestTimeout\n        assert len(kwargs) == 0, \"Unsupported keyword argument\"\n\n        canceller = None\n        if timeout is not None:\n            canceller = reactor.callLater(timeout, self._timeoutRequest,\n                                          messageId)\n\n        self._requests[messageId] = (d, canceller)\n        self.send([messageId, b''] + list(messageParts))\n        return d", "response": "Send a message to the server and return a Deferred that will fire when the response comes back."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling on incoming message from ZeroMQ.", "response": "def messageReceived(self, message):\n        \"\"\"\n        Called on incoming message from ZeroMQ.\n\n        Dispatches message to back to the requestor.\n\n        :param message: message data\n        \"\"\"\n        msgId, msg = message[0], message[2:]\n        self._releaseId(msgId)\n        d, canceller = self._requests.pop(msgId, (None, None))\n\n        if canceller is not None and canceller.active():\n            canceller.cancel()\n\n        if d is None:\n            # reply came for timed out or cancelled request, drop it silently\n            return\n\n        d.callback(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reply(self, messageId, *messageParts):\n        routingInfo = self._routingInfo.pop(messageId)\n        self.send(routingInfo + [messageId, b''] + list(messageParts))", "response": "Send a reply to the request with specified messageId."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef messageReceived(self, message):\n        i = message.index(b'')\n        assert i > 0\n        (routingInfo, msgId, payload) = (\n            message[:i - 1], message[i - 1], message[i + 1:])\n        msgParts = payload[0:]\n        self._routingInfo[msgId] = routingInfo\n        self.gotMessage(msgId, *msgParts)", "response": "Called on incoming message from ZeroMQ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads and return the corpus from the given path.", "response": "def load(self, path):\n        \"\"\"\n        Load and return the corpus from the given path.\n\n        Args:\n            path (str): Path to the data set to load.\n\n        Returns:\n            Corpus: The loaded corpus\n\n        Raises:\n            IOError: When the data set is invalid, for example because required files (annotations, \u2026) are missing.\n        \"\"\"\n        # Check for missing files\n        missing_files = self._check_for_missing_files(path)\n\n        if len(missing_files) > 0:\n            raise IOError('Invalid data set of type {}: files {} not found at {}'.format(\n                self.type(), ' '.join(missing_files), path))\n\n        return self._load(path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all ids from the given folder which have a corresponding beamformedSignal file.", "response": "def get_ids_from_folder(path, part_name):\n        \"\"\"\n        Return all ids from the given folder, which have a corresponding beamformedSignal file.\n        \"\"\"\n        valid_ids = set({})\n\n        for xml_file in glob.glob(os.path.join(path, '*.xml')):\n            idx = os.path.splitext(os.path.basename(xml_file))[0]\n\n            if idx not in BAD_FILES[part_name]:\n                valid_ids.add(idx)\n\n        return valid_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the file with the given id and return a list of tuples.", "response": "def load_file(folder_path, idx, corpus):\n        \"\"\"\n        Load speaker, file, utterance, labels for the file with the given id.\n        \"\"\"\n        xml_path = os.path.join(folder_path, '{}.xml'.format(idx))\n        wav_paths = glob.glob(os.path.join(folder_path, '{}_*.wav'.format(idx)))\n\n        if len(wav_paths) == 0:\n            return []\n\n        xml_file = open(xml_path, 'r', encoding='utf-8')\n        soup = BeautifulSoup(xml_file, 'lxml')\n\n        transcription = soup.recording.cleaned_sentence.string\n        transcription_raw = soup.recording.sentence.string\n        gender = soup.recording.gender.string\n        is_native = soup.recording.muttersprachler.string\n        age_class = soup.recording.ageclass.string\n        speaker_idx = soup.recording.speaker_id.string\n\n        if speaker_idx not in corpus.issuers.keys():\n            start_age_class = int(age_class.split('-')[0])\n\n            if start_age_class < 12:\n                age_group = issuers.AgeGroup.CHILD\n            elif start_age_class < 18:\n                age_group = issuers.AgeGroup.YOUTH\n            elif start_age_class < 65:\n                age_group = issuers.AgeGroup.ADULT\n            else:\n                age_group = issuers.AgeGroup.SENIOR\n\n            native_lang = None\n\n            if is_native == 'Ja':\n                native_lang = 'deu'\n\n            issuer = issuers.Speaker(speaker_idx,\n                                     gender=issuers.Gender(gender),\n                                     age_group=age_group,\n                                     native_language=native_lang)\n            corpus.import_issuers(issuer)\n\n        utt_ids = []\n\n        for wav_path in wav_paths:\n            wav_name = os.path.split(wav_path)[1]\n            wav_idx = os.path.splitext(wav_name)[0]\n            corpus.new_file(wav_path, wav_idx)\n            utt = corpus.new_utterance(wav_idx, wav_idx, speaker_idx)\n            utt.set_label_list(annotations.LabelList.create_single(\n                transcription,\n                idx=audiomate.corpus.LL_WORD_TRANSCRIPT\n            ))\n            utt.set_label_list(annotations.LabelList.create_single(\n                transcription_raw,\n                idx=audiomate.corpus.LL_WORD_TRANSCRIPT_RAW\n            ))\n            utt_ids.append(wav_idx)\n\n        return utt_ids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_file(path):\n    gen = textfile.read_separated_lines_generator(path, max_columns=6,\n                                                  ignore_lines_starting_with=[';;'])\n\n    utterances = collections.defaultdict(list)\n\n    for record in gen:\n        values = record[1:len(record)]\n\n        for i in range(len(values)):\n            if i == 1 or i == 2 or i == 4:\n                values[i] = float(values[i])\n\n        utterances[record[0]].append(values)\n\n    return utterances", "response": "Reads a ctm file and returns a dictionary with utterances."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary with absolute proportions and same identifiers as keys.", "response": "def absolute_proportions(proportions, count):\n    \"\"\"\n    Split a given integer into n parts according to len(proportions) so they sum up to count and\n    match the given proportions.\n\n    Args:\n        proportions (dict): Dict of proportions, with a identifier as key.\n\n    Returns:\n        dict: Dictionary with absolute proportions and same identifiers as key.\n\n    Example::\n\n        >>> absolute_proportions({'train': 0.5, 'test': 0.5}, 100)\n        {'train': 50, 'test': 50}\n    \"\"\"\n\n    # first create absolute values by flooring non-integer portions\n    relative_sum = sum(proportions.values())\n    absolute_proportions = {idx: int(count / relative_sum * prop_value) for idx, prop_value in\n                            proportions.items()}\n\n    # Now distribute the rest value randomly over the different parts\n    absolute_sum = sum(absolute_proportions.values())\n    rest_value = count - absolute_sum\n    subset_keys = sorted(list(proportions.keys()))\n\n    for i in range(rest_value):\n        key = subset_keys[i % len(subset_keys)]\n        absolute_proportions[key] += 1\n\n    return absolute_proportions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_identifiers(identifiers=[], proportions={}):\n\n    abs_proportions = absolute_proportions(proportions, len(identifiers))\n\n    parts = {}\n    start_index = 0\n\n    for idx, proportion in abs_proportions.items():\n        parts[idx] = identifiers[start_index:start_index + proportion]\n        start_index += proportion\n\n    return parts", "response": "Splits the given identifiers by the given proportions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndividing the given identifiers based on the given proportions. But instead of randomly split the identifiers it is based on category weights. Every identifier has a weight for any number of categories. The target is, to split the identifiers in a way, so the sum of category k within part x is proportional to the sum of category x over all parts according to the given proportions. This is done by greedily insert the identifiers step by step in a part which has free space (weight). If there are no fitting parts anymore, the one with the least weight exceed is used. Args: identifiers (dict): A dictionary containing the weights for each identifier (key). Per item a dictionary of weights per category is given. proportions (dict): Dict of proportions, with a identifier as key. Returns: dict: Dictionary containing a list of identifiers per part with the same key as the proportions dict. Example:: >>> identifiers = { >>> 'a': {'music': 2, 'speech': 1}, >>> 'b': {'music': 5, 'speech': 2}, >>> 'c': {'music': 2, 'speech': 4}, >>> 'd': {'music': 1, 'speech': 4}, >>> 'e': {'music': 3, 'speech': 4} >>> } >>> proportions = { >>> \"train\" : 0.6, >>> \"dev\" : 0.2, >>> \"test\" : 0.2 >>> } >>> get_identifiers_splitted_by_weights(identifiers, proportions) { 'train': ['a', 'b', 'd'], 'dev': ['c'], 'test': ['e'] }", "response": "def get_identifiers_splitted_by_weights(identifiers={}, proportions={}):\n    \"\"\"\n    Divide the given identifiers based on the given proportions. But instead of randomly split\n    the identifiers it is based on category weights. Every identifier has a weight for any\n    number of categories. The target is, to split the identifiers in a way, so the sum of\n    category k within part x is proportional to the sum of category x over all parts\n    according to the given proportions. This is done by greedily insert the identifiers step by\n    step in a part which has free space (weight). If there are no fitting parts anymore, the one\n    with the least weight exceed is used.\n\n    Args:\n        identifiers (dict): A dictionary containing the weights for each identifier (key). Per\n                            item a dictionary of weights per category is given.\n        proportions (dict): Dict of proportions, with a identifier as key.\n\n    Returns:\n        dict: Dictionary containing a list of identifiers per part with the same key as the proportions dict.\n\n    Example::\n\n        >>> identifiers = {\n        >>>     'a': {'music': 2, 'speech': 1},\n        >>>     'b': {'music': 5, 'speech': 2},\n        >>>     'c': {'music': 2, 'speech': 4},\n        >>>     'd': {'music': 1, 'speech': 4},\n        >>>     'e': {'music': 3, 'speech': 4}\n        >>> }\n        >>> proportions = {\n        >>>     \"train\" : 0.6,\n        >>>     \"dev\" : 0.2,\n        >>>     \"test\" : 0.2\n        >>> }\n        >>> get_identifiers_splitted_by_weights(identifiers, proportions)\n        {\n            'train': ['a', 'b', 'd'],\n            'dev': ['c'],\n            'test': ['e']\n        }\n    \"\"\"\n\n    # Get total weight per category\n    sum_per_category = collections.defaultdict(int)\n\n    for identifier, cat_weights in identifiers.items():\n        for category, weight in cat_weights.items():\n            sum_per_category[category] += weight\n\n    target_weights_per_part = collections.defaultdict(dict)\n\n    # Get target weight for each part and category\n    for category, total_weight in sum_per_category.items():\n        abs_proportions = absolute_proportions(proportions, total_weight)\n\n        for idx, proportion in abs_proportions.items():\n            target_weights_per_part[idx][category] = proportion\n\n    # Distribute items greedily\n    part_ids = sorted(list(proportions.keys()))\n    current_weights_per_part = {idx: collections.defaultdict(int) for idx in part_ids}\n    result = collections.defaultdict(list)\n\n    for identifier in sorted(identifiers.keys()):\n        cat_weights = identifiers[identifier]\n\n        target_part = None\n        current_part = 0\n        weight_over_target = collections.defaultdict(int)\n\n        # Search for fitting part\n        while target_part is None and current_part < len(part_ids):\n            free_space = True\n            part_id = part_ids[current_part]\n            part_weights = current_weights_per_part[part_id]\n\n            for category, weight in cat_weights.items():\n                target_weight = target_weights_per_part[part_id][category]\n                current_weight = part_weights[category]\n                weight_diff = current_weight + weight - target_weight\n                weight_over_target[part_id] += weight_diff\n\n                if weight_diff > 0:\n                    free_space = False\n\n            # If weight doesn't exceed target, place identifier in part\n            if free_space:\n                target_part = part_id\n\n            current_part += 1\n\n        # If not found fitting part, select the part with the least overweight\n        if target_part is None:\n            target_part = sorted(weight_over_target.items(), key=lambda x: x[1])[0][0]\n\n        result[target_part].append(identifier)\n\n        for category, weight in cat_weights.items():\n            current_weights_per_part[target_part][category] += weight\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_balanced_subset(items, select_count, categories, select_count_values=None, seed=None):\n\n    rand = random.Random()\n    rand.seed(seed)\n\n    if select_count_values is None:\n        select_count_values = {item_id: 1 for item_id in items.keys()}\n\n    if sum(select_count_values.values()) < select_count:\n        return list(items.keys())\n\n    available_item_ids = sorted(list(items.keys()))\n    weight_per_category = np.zeros(len(categories))\n    selected_item_ids = []\n    available_item_weights = []\n    current_select_count = 0\n\n    rand.shuffle(available_item_ids)\n\n    # Create dict with weights as vectors\n    for item_id in available_item_ids:\n        weights = items[item_id]\n        all_weights = np.zeros(len(categories))\n\n        for category, weight in weights.items():\n            all_weights[categories.index(category)] = float(weight)\n\n        available_item_weights.append(all_weights)\n\n    # Always add best next item\n    while current_select_count < select_count:\n        best_item_index = 0\n        best_item_id = None\n        best_item_dist = float('inf')\n        current_item_index = 0\n\n        while current_item_index < len(available_item_ids) and best_item_dist > 0:\n            item_id = available_item_ids[current_item_index]\n            item_weights = available_item_weights[current_item_index]\n            temp_total_weights = weight_per_category + item_weights\n\n            dist = temp_total_weights.var()\n\n            if dist < best_item_dist:\n                best_item_index = current_item_index\n                best_item_dist = dist\n                best_item_id = item_id\n\n            current_item_index += 1\n\n        weight_per_category += available_item_weights[best_item_index]\n        selected_item_ids.append(best_item_id)\n        del available_item_ids[best_item_index]\n        del available_item_weights[best_item_index]\n        current_select_count += select_count_values[best_item_id]\n\n    return selected_item_ids", "response": "Select items so the summed category weights are balanced."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string containing a report of the result.", "response": "def get_report(self):\n        \"\"\"\n        Return a string containing a report of the result.\n        This can used to print or save to a text file.\n\n        Returns:\n            str: String containing infos about the result\n        \"\"\"\n\n        lines = [\n            self.name,\n            '=' * len(self.name)\n        ]\n\n        if self.info is not None:\n            lines.append('')\n            sorted_info = sorted(self.info.items(), key=lambda x: x[0])\n            lines.extend(['--> {}: {}'.format(k, v) for k, v in sorted_info])\n\n        lines.append('')\n        lines.append('Result: {}'.format('Passed' if self.passed else 'Failed'))\n\n        return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_report(self):\n\n        lines = [super(InvalidUtterancesResult, self).get_report()]\n\n        if len(self.invalid_utterances) > 0:\n            lines.append('\\nInvalid utterances:')\n\n            sorted_items = sorted(self.invalid_utterances.items(), key=lambda x: x[0])\n            lines.extend(['    * {} ({})'.format(x, y) for x, y in sorted_items])\n\n        return '\\n'.join(lines)", "response": "Return a string containing a report of the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef length_of_overlap(first_start, first_end, second_start, second_end):\n    if first_end <= second_start or first_start >= second_end:\n        return 0.0\n\n    if first_start < second_start:\n        if first_end < second_end:\n            return abs(first_end - second_start)\n        else:\n            return abs(second_end - second_start)\n\n    if first_start > second_start:\n        if first_end > second_end:\n            return abs(second_end - first_start)\n        else:\n            return abs(first_end - first_start)", "response": "Find the length of the overlapping part of two segments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef relabel(label_list, projections):\n    unmapped_combinations = find_missing_projections(label_list, projections)\n    if len(unmapped_combinations) > 0:\n        raise UnmappedLabelsException('Unmapped combinations: {}'.format(unmapped_combinations))\n\n    new_labels = []\n    for labeled_segment in label_list.ranges():\n        combination = tuple(sorted([label.value for label in labeled_segment[2]]))\n        label_mapping = projections[combination] if combination in projections else projections[WILDCARD_COMBINATION]\n\n        if label_mapping == '':\n            continue\n\n        new_labels.append(annotations.Label(label_mapping, labeled_segment[0], labeled_segment[1]))\n\n    return annotations.LabelList(idx=label_list.idx, labels=new_labels)", "response": "Relabel an entire segment of a tree with a set of labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_missing_projections(label_list, projections):\n    unmapped_combinations = set()\n\n    if WILDCARD_COMBINATION in projections:\n        return []\n\n    for labeled_segment in label_list.ranges():\n        combination = tuple(sorted([label.value for label in labeled_segment[2]]))\n\n        if combination not in projections:\n            unmapped_combinations.add(combination)\n\n    return sorted(unmapped_combinations)", "response": "Finds all combinations of labels in label_list that are not covered by any entry in the dictionary of projections."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_projections(projections_file):\n    projections = {}\n    for parts in textfile.read_separated_lines_generator(projections_file, '|'):\n        combination = tuple(sorted([label.strip() for label in parts[0].split(' ')]))\n        new_label = parts[1].strip()\n\n        projections[combination] = new_label\n\n    return projections", "response": "Loads the projections defined in the given file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef random_subset(self, relative_size, balance_labels=False, label_list_ids=None):\n\n        num_utterances_in_subset = round(relative_size * self.corpus.num_utterances)\n        all_utterance_ids = sorted(list(self.corpus.utterances.keys()))\n\n        if balance_labels:\n            all_label_values = self.corpus.all_label_values(label_list_ids=label_list_ids)\n            utterance_with_label_counts = collections.defaultdict(dict)\n\n            for utterance_idx, utterance in self.corpus.utterances.items():\n                utterance_with_label_counts[utterance_idx] = utterance.label_count(label_list_ids=label_list_ids)\n\n            subset_utterance_ids = utils.select_balanced_subset(utterance_with_label_counts,\n                                                                num_utterances_in_subset,\n                                                                list(all_label_values),\n                                                                seed=self.rand.random())\n\n        else:\n            subset_utterance_ids = self.rand.sample(all_utterance_ids,\n                                                    num_utterances_in_subset)\n\n        filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=set(subset_utterance_ids))\n        return subview.Subview(self.corpus, filter_criteria=[filter])", "response": "Create a random subset of the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef random_subset_by_duration(self, relative_duration, balance_labels=False, label_list_ids=None):\n        total_duration = self.corpus.total_duration\n        subset_duration = relative_duration * total_duration\n        utterance_durations = {utt_idx: utt.duration for utt_idx, utt in self.corpus.utterances.items()}\n\n        if balance_labels:\n            all_label_values = self.corpus.all_label_values(label_list_ids=label_list_ids)\n\n            label_durations = {}\n            for utt_idx, utt in self.corpus.utterances.items():\n                label_durations[utt_idx] = utt.label_total_duration(label_list_ids)\n\n            subset_utterance_ids = utils.select_balanced_subset(label_durations,\n                                                                subset_duration,\n                                                                list(all_label_values),\n                                                                select_count_values=utterance_durations,\n                                                                seed=self.rand.random())\n\n        else:\n            dummy_weights = {utt_idx: {'w': 1} for utt_idx in self.corpus.utterances.keys()}\n            subset_utterance_ids = utils.select_balanced_subset(dummy_weights,\n                                                                subset_duration,\n                                                                ['w'],\n                                                                select_count_values=utterance_durations,\n                                                                seed=self.rand.random())\n\n        filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=set(subset_utterance_ids))\n        return subview.Subview(self.corpus, filter_criteria=[filter])", "response": "Create a random subset of utterances with a approximate duration relative to the full corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef random_subsets(self, relative_sizes, by_duration=False, balance_labels=False, label_list_ids=None):\n        resulting_sets = {}\n        next_bigger_subset = self.corpus\n\n        for relative_size in reversed(relative_sizes):\n            generator = SubsetGenerator(next_bigger_subset, random_seed=self.random_seed)\n\n            if by_duration:\n                sv = generator.random_subset_by_duration(relative_size, balance_labels=balance_labels,\n                                                         label_list_ids=label_list_ids)\n            else:\n                sv = generator.random_subset(relative_size, balance_labels=balance_labels,\n                                             label_list_ids=label_list_ids)\n\n            resulting_sets[relative_size] = sv\n\n        return resulting_sets", "response": "Create a bunch of subsets with the given sizes relative to the full corpus."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef maximal_balanced_subset(self, by_duration=False, label_list_ids=None):\n        all_label_values = self.corpus.all_label_values(label_list_ids=label_list_ids)\n\n        if by_duration:\n            utterance_durations = {utt_idx: utt.duration for utt_idx, utt in self.corpus.utterances.items()}\n            total_duration_per_label = self.corpus.label_durations(label_list_ids=label_list_ids)\n            rarest_label_duration = sorted(total_duration_per_label.values())[0]\n            target_duration = len(all_label_values) * rarest_label_duration\n\n            label_durations_per_utterance = {}\n            for utt_idx, utt in self.corpus.utterances.items():\n                label_durations_per_utterance[utt_idx] = utt.label_total_duration(label_list_ids)\n\n            subset_utterance_ids = utils.select_balanced_subset(label_durations_per_utterance,\n                                                                target_duration,\n                                                                list(all_label_values),\n                                                                select_count_values=utterance_durations,\n                                                                seed=self.rand.random())\n\n        else:\n            total_count_per_label = self.corpus.label_count(label_list_ids=label_list_ids)\n            lowest_label_count = sorted(total_count_per_label.values())[0]\n            target_label_count = lowest_label_count * len(all_label_values)\n            utterance_with_label_counts = collections.defaultdict(dict)\n\n            for utterance_idx, utterance in self.corpus.utterances.items():\n                utterance_with_label_counts[utterance_idx] = utterance.label_count(label_list_ids=label_list_ids)\n\n            subset_utterance_ids = utils.select_balanced_subset(utterance_with_label_counts,\n                                                                target_label_count,\n                                                                list(all_label_values),\n                                                                seed=self.rand.random())\n\n        filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=set(subset_utterance_ids))\n        return subview.Subview(self.corpus, filter_criteria=[filter])", "response": "Create a subset of the corpus as big as possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_punctuation(text, exceptions=[]):\n\n    all_but = [\n        r'\\w',\n        r'\\s'\n    ]\n\n    all_but.extend(exceptions)\n\n    pattern = '[^{}]'.format(''.join(all_but))\n\n    return re.sub(pattern, '', text)", "response": "Removes punctuation from the input text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef starts_with_prefix_in_list(text, prefixes):\n    for prefix in prefixes:\n        if text.startswith(prefix):\n            return True\n    return False", "response": "Checks if the given string starts with one of the given prefixes in the given list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_audio_list(self, path):\n\n        result = {}\n\n        for entry in textfile.read_separated_lines_generator(path, separator='\\t', max_columns=4):\n            for i in range(len(entry)):\n                if entry[i] == '\\\\N':\n                    entry[i] = None\n\n            if len(entry) < 4:\n                entry.extend([None] * (4 - len(entry)))\n\n            if not self.include_empty_licence and entry[2] is None:\n                continue\n\n            if self.include_licenses is not None and entry[2] not in self.include_licenses:\n                continue\n\n            result[entry[0]] = entry[1:]\n\n        return result", "response": "Load and filter the audio list file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_sentence_list(self, path):\n\n        result = {}\n\n        for entry in textfile.read_separated_lines_generator(path, separator='\\t', max_columns=3):\n            if self.include_languages is None or entry[1] in self.include_languages:\n                result[entry[0]] = entry[1:]\n\n        return result", "response": "Load and filter the sentence list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads all audio files based on the given records.", "response": "def _download_audio_files(self, records, target_path):\n        \"\"\"\n        Download all audio files based on the given records.\n        \"\"\"\n\n        for record in records:\n            audio_folder = os.path.join(target_path, 'audio', record[2])\n            audio_file = os.path.join(audio_folder, '{}.mp3'.format(record[0]))\n            os.makedirs(audio_folder, exist_ok=True)\n\n            download_url = 'https://audio.tatoeba.org/sentences/{}/{}.mp3'.format(record[2], record[0])\n            download.download_file(download_url, audio_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting the corpus into subsets where the total duration of subsets are proportional to the given proportions.", "response": "def split_by_length_of_utterances(self, proportions={}, separate_issuers=False):\n        \"\"\"\n        Split the corpus into subsets where the total duration of subsets are proportional to the given proportions.\n        The corpus gets splitted into len(proportions) parts, so the number of utterances are\n        distributed according to the proportions.\n\n        Args:\n            proportions (dict): A dictionary containing the relative size of the target subsets.\n                                The key is an identifier for the subset.\n            separate_issuers (bool): If True it makes sure that all utterances of an issuer are in the same subset.\n\n        Returns:\n            (dict): A dictionary containing the subsets with the identifier from the input as key.\n\n        Example::\n\n            >>> spl = Splitter(corpus)\n            >>> corpus.num_utterances\n            100\n            >>> subsets = spl.split_by_length_of_utterances(proportions={\n            >>>     \"train\" : 0.6,\n            >>>     \"dev\" : 0.2,\n            >>>     \"test\" : 0.2\n            >>> })\n            >>> print(subsets)\n            {'dev': <audiomate.corpus.subview.Subview at 0x104ce7400>,\n            'test': <audiomate.corpus.subview.Subview at 0x104ce74e0>,\n            'train': <audiomate.corpus.subview.Subview at 0x104ce7438>}\n            >>> subsets['train'].num_utterances\n            60\n            >>> subsets['test'].num_utterances\n            20\n        \"\"\"\n\n        utterance_to_duration = {}\n\n        if separate_issuers:\n            # Count total length of utterances per issuer\n            issuer_utts_total_duration = collections.defaultdict(float)\n            issuer_utts = collections.defaultdict(list)\n\n            for utterance in self.corpus.utterances.values():\n                issuer_utts_total_duration[utterance.issuer.idx] += utterance.duration\n                issuer_utts[utterance.issuer.idx].append(utterance.idx)\n\n            issuer_utts_total_duration = {k: {'duration': int(v)} for k, v in issuer_utts_total_duration.items()}\n\n            # Split with total utt duration per issuer as weight\n            issuer_splits = utils.get_identifiers_splitted_by_weights(issuer_utts_total_duration,\n                                                                      proportions=proportions)\n\n            # Collect utterances of all issuers per split\n            splits = collections.defaultdict(list)\n\n            for split_idx, issuer_ids in issuer_splits.items():\n                for issuer_idx in issuer_ids:\n                    splits[split_idx].extend(issuer_utts[issuer_idx])\n        else:\n            for utterance in self.corpus.utterances.values():\n                utterance_to_duration[utterance.idx] = {'length': int(utterance.duration * 100)}\n\n            splits = utils.get_identifiers_splitted_by_weights(utterance_to_duration, proportions=proportions)\n\n        return self._subviews_from_utterance_splits(splits)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the corpus into subsets with the given number of utterances.", "response": "def split_by_number_of_utterances(self, proportions={}, separate_issuers=False):\n        \"\"\"\n        Split the corpus into subsets with the given number of utterances.\n        The corpus gets splitted into len(proportions) parts, so the number of utterances are\n        distributed according to the proportions.\n\n        Args:\n            proportions (dict): A dictionary containing the relative size of the target subsets.\n                                The key is an identifier for the subset.\n            separate_issuers (bool): If True it makes sure that all utterances of an issuer are in the same subset.\n\n        Returns:\n            (dict): A dictionary containing the subsets with the identifier from the input as key.\n\n        Example::\n\n            >>> spl = Splitter(corpus)\n            >>> corpus.num_utterances\n            100\n            >>> subsets = spl.split_by_number_of_utterances(proportions={\n            >>>     \"train\" : 0.6,\n            >>>     \"dev\" : 0.2,\n            >>>     \"test\" : 0.2\n            >>> })\n            >>> print(subsets)\n            {'dev': <audiomate.corpus.subview.Subview at 0x104ce7400>,\n            'test': <audiomate.corpus.subview.Subview at 0x104ce74e0>,\n            'train': <audiomate.corpus.subview.Subview at 0x104ce7438>}\n            >>> subsets['train'].num_utterances\n            60\n            >>> subsets['test'].num_utterances\n            20\n        \"\"\"\n\n        if separate_issuers:\n            # Count number of utterances per issuer\n            issuer_utt_count = collections.defaultdict(int)\n            issuer_utts = collections.defaultdict(list)\n\n            for utterance in self.corpus.utterances.values():\n                issuer_utt_count[utterance.issuer.idx] += 1\n                issuer_utts[utterance.issuer.idx].append(utterance.idx)\n\n            issuer_utt_count = {k: {'count': int(v)} for k, v in issuer_utt_count.items()}\n\n            # Split with total utt duration per issuer as weight\n            issuer_splits = utils.get_identifiers_splitted_by_weights(issuer_utt_count,\n                                                                      proportions=proportions)\n\n            # Collect utterances of all issuers per split\n            splits = collections.defaultdict(list)\n\n            for split_idx, issuer_ids in issuer_splits.items():\n                for issuer_idx in issuer_ids:\n                    splits[split_idx].extend(issuer_utts[issuer_idx])\n        else:\n            utterance_idxs = sorted(list(self.corpus.utterances.keys()))\n            self.rand.shuffle(utterance_idxs)\n            splits = utils.split_identifiers(identifiers=utterance_idxs,\n                                             proportions=proportions)\n\n        return self._subviews_from_utterance_splits(splits)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits the corpus into subsets so the occurrence of the labels is distributed amongst the target subsets according to the given proportions.", "response": "def split_by_proportionally_distribute_labels(self, proportions={}, use_lengths=True):\n        \"\"\"\n        Split the corpus into subsets, so the occurrence of the labels is distributed amongst the\n        subsets according to the given proportions.\n\n        Args:\n            proportions (dict): A dictionary containing the relative size of the target subsets.\n                                The key is an identifier for the subset.\n            use_lengths (bool): If True the lengths of the labels are considered for splitting proportionally,\n                                otherwise only the number of occurrences is taken into account.\n\n        Returns:\n            (dict): A dictionary containing the subsets with the identifier from the input as key.\n        \"\"\"\n\n        identifiers = {}\n\n        for utterance in self.corpus.utterances.values():\n            if use_lengths:\n                identifiers[utterance.idx] = {l: int(d * 100) for l, d in utterance.label_total_duration().items()}\n            else:\n                identifiers[utterance.idx] = utterance.label_count()\n\n        splits = utils.get_identifiers_splitted_by_weights(identifiers, proportions)\n\n        return self._subviews_from_utterance_splits(splits)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _subviews_from_utterance_splits(self, splits):\n        subviews = {}\n\n        for idx, subview_utterances in splits.items():\n            filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=subview_utterances)\n            split = subview.Subview(self.corpus, filter_criteria=filter)\n            subviews[idx] = split\n\n        return subviews", "response": "Create subviews from a dict containing utterance - ids for each subview."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the sampling rate of the current instance.", "response": "def sampling_rate(self):\n        \"\"\"\n        Return the sampling rate.\n        \"\"\"\n        with self.container.open_if_needed(mode='r') as cnt:\n            return cnt.get(self.key)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef num_samples(self):\n        with self.container.open_if_needed(mode='r') as cnt:\n            return cnt.get(self.key)[0].shape[0]", "response": "Return the total number of samples in the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the duration in seconds of the current instance.", "response": "def duration(self):\n        \"\"\"\n        Return the duration in seconds.\n        \"\"\"\n        with self.container.open_if_needed(mode='r') as cnt:\n            samples, sr = cnt.get(self.key)\n\n            return samples.shape[0] / sr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_samples(self, sr=None, offset=0, duration=None):\n        with self.container.open_if_needed(mode='r') as cnt:\n            samples, native_sr = cnt.get(self.key)\n\n            start_sample_index = int(offset * native_sr)\n\n            if duration is None:\n                end_sample_index = samples.shape[0]\n            else:\n                end_sample_index = int((offset + duration) * native_sr)\n\n            samples = samples[start_sample_index:end_sample_index]\n\n            if sr is not None and sr != native_sr:\n                samples = librosa.core.resample(\n                    samples,\n                    native_sr,\n                    sr,\n                    res_type='kaiser_best'\n                )\n\n            return samples", "response": "Reads the samples from the track in the container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_frames(self, frame_size, hop_size, offset=0,\n                    duration=None, block_size=None):\n        \"\"\"\n        Generator that reads and returns the samples of the track in frames.\n\n        Args:\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            offset (float): The time in seconds, from where to start\n                            reading the samples (rel. to the track start).\n            duration (float): The length of the samples to read in seconds.\n\n        Returns:\n            Generator: A generator yielding a tuple for every frame.\n            The first item is the frame,\n            the second the sampling-rate and\n            the third a boolean indicating if it is the last frame.\n        \"\"\"\n        with self.container.open_if_needed(mode='r') as cnt:\n            samples, sr = cnt.get(self.key)\n\n            current_index = 0\n\n            while current_index + frame_size < samples.shape[0]:\n                next_frame = samples[current_index:current_index+frame_size]\n                yield next_frame, False\n                current_index += hop_size\n\n            next_frame = samples[current_index:]\n\n            if next_frame.shape[0] < frame_size:\n                next_frame = np.pad(\n                    next_frame,\n                    (0, frame_size - next_frame.shape[0]),\n                    mode='constant',\n                    constant_values=0\n                )\n\n            yield next_frame, True", "response": "Generator that reads and returns the samples of the current node in the specified size."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite data as json to file.", "response": "def write_json_to_file(path, data):\n    \"\"\"\n    Writes data as json to file.\n\n    Parameters:\n        path (str): Path to write to\n        data (dict, list): Data\n    \"\"\"\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread and returns the data from the json file at the given path.", "response": "def read_json_file(path):\n    \"\"\"\n    Reads and return the data from the json file at the given path.\n\n    Parameters:\n        path (str): Path to read\n\n    Returns:\n        dict,list: The read json as dict/list.\n    \"\"\"\n\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the total number of samples in the file.", "response": "def num_samples(self):\n        \"\"\"\n        Return the total number of samples.\n        \"\"\"\n        with audioread.audio_open(self.path) as f:\n            return int(f.duration * f.samplerate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the samples from the file.", "response": "def read_samples(self, sr=None, offset=0, duration=None):\n        \"\"\"\n        Return the samples from the file.\n        Uses librosa for loading\n        (see http://librosa.github.io/librosa/generated/librosa.core.load.html).\n\n        Args:\n            sr (int): If ``None``, uses the sampling rate given by the file,\n                      otherwise resamples to the given sampling rate.\n            offset (float): The time in seconds, from where to start reading\n                            the samples (rel. to the file start).\n            duration (float): The length of the samples to read in seconds.\n\n        Returns:\n            np.ndarray: A numpy array containing the samples as a\n            floating point (numpy.float32) time series.\n        \"\"\"\n        samples, __ = librosa.core.load(\n            self.path,\n            sr=sr,\n            offset=offset,\n            duration=duration\n        )\n        return samples"}
