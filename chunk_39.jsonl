{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving results to a directory.", "response": "def _save(self) -> None:\n        \"\"\"Save output to a directory.\"\"\"\n        self._log.info(\"Saving results to '%s'\" % self.folder)\n        path: str = self.folder + \"/\"\n        for job in self.results:\n            if job['domain'] in self.saved:\n                continue\n            job['start_time'] = str_datetime(job['start_time'])\n            job['end_time'] = str_datetime(job['end_time'])\n            jid: int = random.randint(100000, 999999)\n            filename: str = \"%s_%s_%d_job.json\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            handle.write(json.dumps(job, indent=4))\n            handle.close()\n\n            filename = \"%s_%s_%d_emails.txt\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            for email in job['results']['emails']:\n                handle.write(email + \"\\n\")\n            handle.close()\n            self.saved.append(job['domain'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms the actual search based on the given jobs.", "response": "def search(self, jobs: List[Dict[str, str]]) -> None:\n        \"\"\"Perform searches based on job orders.\"\"\"\n        if not isinstance(jobs, list):\n            raise Exception(\"Jobs must be of type list.\")\n        self._log.info(\"Project: %s\" % self.project)\n        self._log.info(\"Processing jobs: %d\", len(jobs))\n        for _, job in enumerate(jobs):\n            self._unfullfilled.put(job)\n\n        for _ in range(self.PROCESSES):\n            proc: Process = Process(target=self._job_handler)\n            self._processes.append(proc)\n            proc.start()\n\n        for proc in self._processes:\n            proc.join()\n\n        while not self._fulfilled.empty():\n            output: Dict = self._fulfilled.get()\n            output.update({'project': self.project})\n            self._processed.append(output['domain'])\n            self.results.append(output)\n\n            if output['greedy']:\n                bonus_jobs: List = list()\n                observed: List = list()\n                for item in output['results']['emails']:\n                    found: str = item.split('@')[1]\n                    if found in self._processed or found in observed:\n                        continue\n                    observed.append(found)\n                    base: Dict = dict()\n                    base['limit'] = output['limit']\n                    base['modifier'] = output['modifier']\n                    base['engine'] = output['engine']\n                    base['greedy'] = False\n                    base['domain'] = found\n                    bonus_jobs.append(base)\n\n                if len(bonus_jobs) > 0:\n                    self.search(bonus_jobs)\n\n        self._log.info(\"All jobs processed\")\n        if self.output:\n            self._save()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting geocentric coordinates into ECEF", "response": "def geocentric_to_ecef(latitude, longitude, altitude):\n    \"\"\"Convert geocentric coordinates into ECEF\n    \n    Parameters\n    ----------\n    latitude : float or array_like\n        Geocentric latitude (degrees)\n    longitude : float or array_like\n        Geocentric longitude (degrees)\n    altitude : float or array_like\n        Height (km) above presumed spherical Earth with radius 6371 km.\n        \n    Returns\n    -------\n    x, y, z\n        numpy arrays of x, y, z locations in km\n        \n    \"\"\"\n\n    r = earth_geo_radius + altitude\n    x = r * np.cos(np.deg2rad(latitude)) * np.cos(np.deg2rad(longitude))\n    y = r * np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(longitude))\n    z = r * np.sin(np.deg2rad(latitude))\n\n    return x, y, z"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts ECEF coordinates into geocentric coordinates.", "response": "def ecef_to_geocentric(x, y, z, ref_height=None):\n    \"\"\"Convert ECEF into geocentric coordinates\n    \n    Parameters\n    ----------\n    x : float or array_like\n        ECEF-X in km\n    y : float or array_like\n        ECEF-Y in km\n    z : float or array_like\n        ECEF-Z in km\n    ref_height : float or array_like\n        Reference radius used for calculating height.\n        Defaults to average radius of 6371 km\n    Returns\n    -------\n    latitude, longitude, altitude\n        numpy arrays of locations in degrees, degrees, and km\n        \n    \"\"\"\n    if ref_height is None:\n        ref_height = earth_geo_radius\n\n    r = np.sqrt(x ** 2 + y ** 2 + z ** 2)\n    colatitude = np.rad2deg(np.arccos(z / r))\n    longitude = np.rad2deg(np.arctan2(y, x))\n    latitude = 90. - colatitude\n\n    return latitude, longitude, r - ref_height"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef geodetic_to_ecef(latitude, longitude, altitude):\n\n\n    ellip = np.sqrt(1. - earth_b ** 2 / earth_a ** 2)\n    r_n = earth_a / np.sqrt(1. - ellip ** 2 * np.sin(np.deg2rad(latitude)) ** 2)\n\n    # colatitude = 90. - latitude\n    x = (r_n + altitude) * np.cos(np.deg2rad(latitude)) * np.cos(np.deg2rad(longitude))\n    y = (r_n + altitude) * np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(longitude))\n    z = (r_n * (1. - ellip ** 2) + altitude) * np.sin(np.deg2rad(latitude))\n\n    return x, y, z", "response": "Convert WGS84 geodetic coordinates into ECEF"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts ECEF to Geodetic WGS84 coordinates.", "response": "def ecef_to_geodetic(x, y, z, method=None):\n    \"\"\"Convert ECEF into Geodetic WGS84 coordinates\n    \n    Parameters\n    ----------\n    x : float or array_like\n        ECEF-X in km\n    y : float or array_like\n        ECEF-Y in km\n    z : float or array_like\n        ECEF-Z in km\n    method : 'iterative' or 'closed' ('closed' is deafult)\n        String selects method of conversion. Closed for mathematical\n        solution (http://www.epsg.org/Portals/0/373-07-2.pdf , page 96 section 2.2.1)\n        or iterative (http://www.oc.nps.edu/oc2902w/coord/coordcvt.pdf).\n        \n    Returns\n    -------\n    latitude, longitude, altitude\n        numpy arrays of locations in degrees, degrees, and km\n        \n    \"\"\"\n\n    # quick notes on ECEF to Geodetic transformations \n    # http://danceswithcode.net/engineeringnotes/geodetic_to_ecef/geodetic_to_ecef.html\n    \n    method = method or 'closed'\n\n    # ellipticity of Earth    \n    ellip = np.sqrt(1. - earth_b ** 2 / earth_a ** 2)\n    # first eccentricity squared\n    e2 = ellip ** 2  # 6.6943799901377997E-3\n\n    longitude = np.arctan2(y, x)\n    # cylindrical radius\n    p = np.sqrt(x ** 2 + y ** 2)\n    \n    # closed form solution\n    # a source, http://www.epsg.org/Portals/0/373-07-2.pdf , page 96 section 2.2.1\n    if method == 'closed':\n        e_prime = np.sqrt((earth_a**2 - earth_b**2) / earth_b**2)\n        theta = np.arctan2(z*earth_a, p*earth_b)\n        latitude = np.arctan2(z + e_prime**2*earth_b*np.sin(theta)**3, p - e2*earth_a*np.cos(theta)**3)\n        r_n = earth_a / np.sqrt(1. - e2 * np.sin(latitude) ** 2)\n        h = p / np.cos(latitude) - r_n\n\n    # another possibility\n    # http://ir.lib.ncku.edu.tw/bitstream/987654321/39750/1/3011200501001.pdf\n\n    ## iterative method\n    # http://www.oc.nps.edu/oc2902w/coord/coordcvt.pdf\n    if method == 'iterative':\n        latitude = np.arctan2(p, z)\n        r_n = earth_a / np.sqrt(1. - e2*np.sin(latitude)**2)\n        for i in np.arange(6):\n            # print latitude\n            r_n = earth_a / np.sqrt(1. - e2*np.sin(latitude)**2)\n            h = p / np.cos(latitude) - r_n\n            latitude = np.arctan(z / p / (1. - e2 * (r_n / (r_n + h))))\n            # print h\n        # final ellipsoidal height update\n        h = p / np.cos(latitude) - r_n\n\n    return np.rad2deg(latitude), np.rad2deg(longitude), h"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert vector from East North Up components to ECEF x y z directions", "response": "def enu_to_ecef_vector(east, north, up, glat, glong):\n    \"\"\"Converts vector from East, North, Up components to ECEF\n    \n    Position of vector in geospace may be specified in either\n    geocentric or geodetic coordinates, with corresponding expression\n    of the vector using radial or ellipsoidal unit vectors.\n    \n    Parameters\n    ----------\n    east : float or array-like\n        Eastward component of vector\n    north : float or array-like\n        Northward component of vector\n    up : float or array-like\n        Upward component of vector\n    latitude : float or array_like\n        Geodetic or geocentric latitude (degrees)\n    longitude : float or array_like\n        Geodetic or geocentric longitude (degrees)\n    \n    Returns\n    -------\n    x, y, z\n        Vector components along ECEF x, y, and z directions\n    \n    \"\"\"\n    \n    # convert lat and lon in degrees to radians\n    rlat = np.radians(glat)\n    rlon = np.radians(glong)\n    \n    x = -east*np.sin(rlon) - north*np.cos(rlon)*np.sin(rlat) + up*np.cos(rlon)*np.cos(rlat)\n    y = east*np.cos(rlon) - north*np.sin(rlon)*np.sin(rlat) + up*np.sin(rlon)*np.cos(rlat)\n    z = north*np.cos(rlat) + up*np.sin(rlat)\n\n    return x, y, z"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project_ecef_vector_onto_basis(x, y, z, xx, xy, xz, yx, yy, yz, zx, zy, zz):\n    \n    out_x = x*xx + y*xy + z*xz\n    out_y = x*yx + y*yy + z*yz\n    out_z = x*zx + y*zy + z*zz\n    \n    return out_x, out_y, out_z", "response": "Projects vector in ecef onto different basis with components also expressed in ECEF"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_vector(x, y, z):\n    \n    mag = np.sqrt(x**2 + y**2 + z**2)\n    x = x/mag\n    y = y/mag\n    z = z/mag\n    return x, y, z", "response": "Normalizes the vector x y z to produce a unit vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cross product of two vectors v1 x v2 y2 z2", "response": "def cross_product(x1, y1, z1, x2, y2, z2):\n    \"\"\"\n    Cross product of two vectors, v1 x v2\n    \n    Parameters\n    ----------\n    x1 : float or array-like\n        X component of vector 1\n    y1 : float or array-like\n        Y component of vector 1\n    z1 : float or array-like\n        Z component of vector 1\n    x2 : float or array-like\n        X component of vector 2\n    y2 : float or array-like\n        Y component of vector 2\n    z2 : float or array-like\n        Z component of vector 2\n        \n    Returns\n    -------\n    x, y, z\n        Unit vector x,y,z components\n        \n    \"\"\"\n    x = y1*z2 - y2*z1\n    y = z1*x2 - x1*z2\n    z = x1*y2 - y1*x2\n    return x, y, z"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform field line tracing using IGRF and scipy. integrate. odeint.", "response": "def field_line_trace(init, date, direction, height, steps=None,\n                     max_steps=1E4, step_size=10., recursive_loop_count=None, \n                     recurse=True):\n    \"\"\"Perform field line tracing using IGRF and scipy.integrate.odeint.\n    \n    Parameters\n    ----------\n    init : array-like of floats\n        Position to begin field line tracing from in ECEF (x,y,z) km\n    date : datetime or float\n        Date to perform tracing on (year + day/365 + hours/24. + etc.)\n        Accounts for leap year if datetime provided.\n    direction : int\n         1 : field aligned, generally south to north. \n        -1 : anti-field aligned, generally north to south.\n    height : float\n        Altitude to terminate trace, geodetic WGS84 (km)\n    steps : array-like of ints or floats\n        Number of steps along field line when field line trace positions should \n        be reported. By default, each step is reported; steps=np.arange(max_steps).\n    max_steps : float\n        Maximum number of steps along field line that should be taken\n    step_size : float\n        Distance in km for each large integration step. Multiple substeps\n        are taken as determined by scipy.integrate.odeint\n        \n    Returns\n    -------\n    numpy array\n        2D array. [0,:] has the x,y,z location for initial point\n        [:,0] is the x positions over the integration.\n        Positions are reported in ECEF (km).\n        \n    \n    \"\"\"\n    \n    if recursive_loop_count is None:  \n        recursive_loop_count = 0\n    #     \n    if steps is None:\n        steps = np.arange(max_steps)\n    if not isinstance(date, float):\n        # recast from datetime to float, as required by IGRF12 code\n        doy = (date - datetime.datetime(date.year,1,1)).days\n        # number of days in year, works for leap years\n        num_doy_year = (datetime.datetime(date.year+1,1,1) - datetime.datetime(date.year,1,1)).days\n        date = float(date.year) + float(doy)/float(num_doy_year) + float(date.hour + date.minute/60. + date.second/3600.)/24.\n          \n    trace_north = scipy.integrate.odeint(igrf.igrf_step, init.copy(),\n                                         steps,\n                                         args=(date, step_size, direction, height),\n                                         full_output=False,\n                                         printmessg=False,\n                                         ixpr=False) #,\n                                         # mxstep=500)\n    \n    # check that we reached final altitude\n    check = trace_north[-1, :]\n    x, y, z = ecef_to_geodetic(*check)        \n    if height == 0:\n        check_height = 1.\n    else:\n        check_height = height\n    # fortran integration gets close to target height        \n    if recurse & (z > check_height*1.000001):\n        if (recursive_loop_count < 1000):\n            # When we have not reached the reference height, call field_line_trace \n            # again by taking check value as init - recursive call\n            recursive_loop_count = recursive_loop_count + 1\n            trace_north1 = field_line_trace(check, date, direction, height,\n                                            step_size=step_size, \n                                            max_steps=max_steps,\n                                            recursive_loop_count=recursive_loop_count,\n                                            steps=steps)\n        else:\n            raise RuntimeError(\"After 1000 iterations couldn't reach target altitude\")\n        return np.vstack((trace_north, trace_north1))\n    else:\n        # return results if we make it to the target altitude\n        \n        # filter points to terminate at point closest to target height\n        # code below not correct, we want the first poiint that goes below target\n        # height\n        # code also introduces a variable length return, though I suppose\n        # that already exists with the recursive functionality\n        # x, y, z = ecef_to_geodetic(trace_north[:,0], trace_north[:,1], trace_north[:,2]) \n        # idx = np.argmin(np.abs(check_height - z)) \n        return trace_north"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform field line tracing using IGRF and scipy. integrate. odeint.", "response": "def full_field_line(init, date, height, step_size=100., max_steps=1000, \n                    steps=None, **kwargs):\n    \"\"\"Perform field line tracing using IGRF and scipy.integrate.odeint.\n    \n    Parameters\n    ----------\n    init : array-like of floats\n        Position to begin field line tracing from in ECEF (x,y,z) km\n    date : datetime or float\n        Date to perform tracing on (year + day/365 + hours/24. + etc.)\n        Accounts for leap year if datetime provided.\n    height : float\n        Altitude to terminate trace, geodetic WGS84 (km)\n    max_steps : float\n        Maximum number of steps along field line that should be taken\n    step_size : float\n        Distance in km for each large integration step. Multiple substeps\n        are taken as determined by scipy.integrate.odeint\n    steps : array-like of ints or floats\n        Number of steps along field line when field line trace positions should \n        be reported. By default, each step is reported; steps=np.arange(max_steps).\n        Two traces are made, one north, the other south, thus the output array\n        could have double max_steps, or more via recursion.\n        \n    Returns\n    -------\n    numpy array\n        2D array. [0,:] has the x,y,z location for southern footpoint\n        [:,0] is the x positions over the integration.\n        Positions are reported in ECEF (km).\n        \n    \n    \"\"\"\n    \n    if steps is None:\n        steps = np.arange(max_steps)\n    # trace north, then south, and combine\n    trace_south = field_line_trace(init, date, -1., height, \n                                   steps=steps,\n                                   step_size=step_size, \n                                   max_steps=max_steps, \n                                   **kwargs)\n    trace_north = field_line_trace(init, date, 1., height, \n                                   steps=steps,\n                                   step_size=step_size, \n                                   max_steps=max_steps, \n                                   **kwargs)\n    # order of field points is generally along the field line, south to north\n    # don't want to include the initial point twice\n    trace = np.vstack((trace_south[::-1][:-1,:], trace_north))\n    return trace"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_mag_drift_unit_vectors_ecef(latitude, longitude, altitude, datetimes,\n                                          steps=None, max_steps=1000, step_size=100.,\n                                          ref_height=120., filter_zonal=True):\n    \"\"\"Calculates unit vectors expressing the ion drift coordinate system\n    organized by the geomagnetic field. Unit vectors are expressed\n    in ECEF coordinates.\n    \n    Note\n    ----\n        The zonal vector is calculated by field-line tracing from\n        the input locations toward the footpoint locations at ref_height.\n        The cross product of these two vectors is taken to define the plane of\n        the magnetic field. This vector is not always orthogonal\n        with the local field-aligned vector (IGRF), thus any component of the \n        zonal vector with the field-aligned direction is removed (optional). \n        The meridional unit vector is defined via the cross product of the \n        zonal and field-aligned directions.\n    \n    Parameters\n    ----------\n    latitude : array-like of floats (degrees)\n        Latitude of location, degrees, WGS84\n    longitude : array-like of floats (degrees)\n        Longitude of location, degrees, WGS84\n    altitude : array-like of floats (km)\n        Altitude of location, height above surface, WGS84\n    datetimes : array-like of datetimes\n        Time to calculate vectors\n    max_steps : int\n        Maximum number of steps allowed for field line tracing\n    step_size : float\n        Maximum step size (km) allowed when field line tracing\n    ref_height : float\n        Altitude used as cutoff for labeling a field line location a footpoint\n    filter_zonal : bool\n        If True, removes any field aligned component from the calculated\n        zonal unit vector. Resulting coordinate system is not-orthogonal.\n        \n    Returns\n    -------\n    zon_x, zon_y, zon_z, fa_x, fa_y, fa_z, mer_x, mer_y, mer_z\n            \n    \"\"\"\n\n    if steps is None:\n        steps = np.arange(max_steps)\n    # calculate satellite position in ECEF coordinates\n    ecef_x, ecef_y, ecef_z = geodetic_to_ecef(latitude, longitude, altitude)\n    # also get position in geocentric coordinates\n    geo_lat, geo_long, geo_alt = ecef_to_geocentric(ecef_x, ecef_y, ecef_z, \n                                                    ref_height=0.)\n    # filter longitudes (could use pysat's function here)\n    idx, = np.where(geo_long < 0)\n    geo_long[idx] = geo_long[idx] + 360.\n    # prepare output lists\n    north_x = [];\n    north_y = [];\n    north_z = []\n    south_x = [];\n    south_y = [];\n    south_z = []\n    bn = [];\n    be = [];\n    bd = []\n\n    for x, y, z, alt, colat, elong, time in zip(ecef_x, ecef_y, ecef_z, \n                                                geo_alt, np.deg2rad(90. - geo_lat),\n                                                np.deg2rad(geo_long), datetimes):\n        init = np.array([x, y, z])\n        # date = inst.yr + inst.doy / 366.\n        # trace = full_field_line(init, time, ref_height, step_size=step_size, \n        #                                                 max_steps=max_steps,\n        #                                                 steps=steps)\n        trace_north = field_line_trace(init, time, 1., ref_height, steps=steps,\n                                        step_size=step_size, max_steps=max_steps)\n        trace_south = field_line_trace(init, time, -1., ref_height, steps=steps,\n                                        step_size=step_size, max_steps=max_steps)\n        # store final location, full trace goes south to north\n        trace_north = trace_north[-1, :]\n        trace_south = trace_south[-1, :]\n        # magnetic field at spacecraft location, using geocentric inputs\n        # to get magnetic field in geocentric output\n        # recast from datetime to float, as required by IGRF12 code\n        doy = (time - datetime.datetime(time.year,1,1)).days\n        # number of days in year, works for leap years\n        num_doy_year = (datetime.datetime(time.year+1,1,1) - datetime.datetime(time.year,1,1)).days\n        date = time.year + float(doy)/float(num_doy_year) + (time.hour + time.minute/60. + time.second/3600.)/24.\n        # get IGRF field components\n        # tbn, tbe, tbd, tbmag are in nT\n        tbn, tbe, tbd, tbmag = igrf.igrf12syn(0, date, 1, alt, colat, elong)\n        # collect outputs\n        south_x.append(trace_south[0])\n        south_y.append(trace_south[1])\n        south_z.append(trace_south[2])\n        north_x.append(trace_north[0])\n        north_y.append(trace_north[1])\n        north_z.append(trace_north[2])\n\n        bn.append(tbn);\n        be.append(tbe);\n        bd.append(tbd)\n\n    north_x = np.array(north_x)\n    north_y = np.array(north_y)\n    north_z = np.array(north_z)\n    south_x = np.array(south_x)\n    south_y = np.array(south_y)\n    south_z = np.array(south_z)\n    bn = np.array(bn)\n    be = np.array(be)\n    bd = np.array(bd)\n\n    # calculate vector from satellite to northern/southern footpoints\n    north_x = north_x - ecef_x\n    north_y = north_y - ecef_y\n    north_z = north_z - ecef_z\n    north_x, north_y, north_z = normalize_vector(north_x, north_y, north_z)\n    south_x = south_x - ecef_x\n    south_y = south_y - ecef_y\n    south_z = south_z - ecef_z\n    south_x, south_y, south_z = normalize_vector(south_x, south_y, south_z)\n    # calculate magnetic unit vector\n    bx, by, bz = enu_to_ecef_vector(be, bn, -bd, geo_lat, geo_long)\n    bx, by, bz = normalize_vector(bx, by, bz)\n    \n    # take cross product of southward and northward vectors to get the zonal vector\n    zvx_foot, zvy_foot, zvz_foot = cross_product(south_x, south_y, south_z,\n                                                 north_x, north_y, north_z)  \n    # getting zonal vector utilizing magnetic field vector instead\n    zvx_north, zvy_north, zvz_north = cross_product(north_x, north_y, north_z,\n                                                    bx, by, bz)\n    # getting zonal vector utilizing magnetic field vector instead and southern point\n    zvx_south, zvy_south, zvz_south = cross_product(south_x, south_y, south_z,\n                                                    bx, by, bz)\n    # normalize the vectors\n    norm_foot = np.sqrt(zvx_foot ** 2 + zvy_foot ** 2 + zvz_foot ** 2)\n    \n    # calculate zonal vector\n    zvx = zvx_foot / norm_foot\n    zvy = zvy_foot / norm_foot\n    zvz = zvz_foot / norm_foot\n    # remove any field aligned component to the zonal vector\n    dot_fa = zvx * bx + zvy * by + zvz * bz\n    zvx -= dot_fa * bx\n    zvy -= dot_fa * by\n    zvz -= dot_fa * bz\n    zvx, zvy, zvz = normalize_vector(zvx, zvy, zvz)\n    # compute meridional vector\n    # cross product of zonal and magnetic unit vector\n    mx, my, mz = cross_product(zvx, zvy, zvz,\n                               bx, by, bz)\n    # add unit vectors for magnetic drifts in ecef coordinates\n    return zvx, zvy, zvz, bx, by, bz, mx, my, mz", "response": "Calculates the unit vectors expressing the ion drift coordinate system of the specified magnetic field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef step_until_intersect(pos, field_line, sign, time,  direction=None,\n                        step_size_goal=5., \n                        field_step_size=None):   \n    \"\"\"Starting at pos, method steps along magnetic unit vector direction \n    towards the supplied field line trace. Determines the distance of \n    closest approach to field line.\n    \n    Routine is used when calculting the mapping of electric fields along \n    magnetic field lines. Voltage remains constant along the field but the \n    distance between field lines does not.This routine may be used to form the \n    last leg when trying to trace out a closed field line loop.\n    \n    Routine will create a high resolution field line trace (.01 km step size) \n    near the location of closest approach to better determine where the \n    intersection occurs. \n    \n    Parameters\n    ----------\n    pos : array-like\n        X, Y, and Z ECEF locations to start from\n    field_line : array-like (:,3)\n        X, Y, and Z ECEF locations of field line trace, produced by the\n        field_line_trace method.\n    sign : int\n        if 1, move along positive unit vector. Negwtive direction for -1.\n    time : datetime or float\n        Date to perform tracing on (year + day/365 + hours/24. + etc.)\n        Accounts for leap year if datetime provided.\n    direction : string ('meridional', 'zonal', or 'aligned')\n        Which unit vector direction to move slong when trying to intersect\n        with supplied field line trace. See step_along_mag_unit_vector method\n        for more.\n    step_size_goal : float\n        step size goal that method will try to match when stepping towards field line. \n    \n    Returns\n    -------\n    (float, array, float)\n        Total distance taken along vector direction; the position after taking \n        the step [x, y, z] in ECEF; distance of closest approach from input pos \n        towards the input field line trace.\n         \n    \"\"\" \n                                                         \n    # work on a copy, probably not needed\n    field_copy = field_line\n    # set a high last minimum distance to ensure first loop does better than this\n    last_min_dist = 2500000.\n    # scalar is the distance along unit vector line that we are taking\n    scalar = 0.\n    # repeat boolean\n    repeat=True\n    # first run boolean\n    first=True\n    # factor is a divisor applied to the remaining distance between point and field line\n    # I slowly take steps towards the field line and I don't want to overshoot\n    # each time my minimum distance increases, I step back, increase factor, reducing\n    # my next step size, then I try again\n    factor = 1\n    while repeat:\n        # take a total step along magnetic unit vector\n        # try to take steps near user provided step_size_goal\n        unit_steps = np.abs(scalar//step_size_goal)\n        if unit_steps == 0:\n            unit_steps = 1\n        # print (unit_steps, scalar/unit_steps)\n        pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], time, \n                                              direction=direction,\n                                              num_steps=unit_steps, \n                                              step_size=np.abs(scalar)/unit_steps,\n                                              scalar=sign) \n        # find closest point along field line trace\n        diff = field_copy - pos_step\n        diff_mag = np.sqrt((diff ** 2).sum(axis=1))\n        min_idx = np.argmin(diff_mag)\n        if first:\n            # first time in while loop, create some information\n            # make a high resolution field line trace around closest distance\n            # want to take a field step size in each direction\n            # maintain accuracy of high res trace below to be .01 km\n            init = field_copy[min_idx,:]\n            field_copy = full_field_line(init, time, 0.,\n                                         step_size=0.01, \n                                         max_steps=int(field_step_size/.01),\n                                         recurse=False)\n            # difference with position\n            diff = field_copy - pos_step\n            diff_mag = np.sqrt((diff ** 2).sum(axis=1))\n            # find closest one\n            min_idx = np.argmin(diff_mag)\n            # # reduce number of elements we really need to check\n            # field_copy = field_copy[min_idx-100:min_idx+100]\n            # # difference with position\n            # diff = field_copy - pos_step\n            # diff_mag = np.sqrt((diff ** 2).sum(axis=1))\n            # # find closest one\n            # min_idx = np.argmin(diff_mag)\n            first = False\n            \n        # pull out distance of closest point \n        min_dist = diff_mag[min_idx]\n        \n        # check how the solution is doing\n        # if well, add more distance to the total step and recheck if closer\n        # if worse, step back and try a smaller step\n        if min_dist > last_min_dist:\n            # last step we took made the solution worse\n            if factor > 4:\n                # we've tried enough, stop looping\n                repeat = False\n                # undo increment to last total distance\n                scalar = scalar - last_min_dist/(2*factor)\n                # calculate latest position\n                pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], \n                                        time, \n                                        direction=direction,\n                                        num_steps=unit_steps, \n                                        step_size=np.abs(scalar)/unit_steps,\n                                        scalar=sign) \n            else:\n                # undo increment to last total distance\n                scalar = scalar - last_min_dist/(2*factor)\n                # increase the divisor used to reduce the distance \n                # actually stepped per increment\n                factor = factor + 1.\n                # try a new increment to total distance\n                scalar = scalar + last_min_dist/(2*factor)\n        else:\n            # we did better, move even closer, a fraction of remaining distance\n            # increment scalar, but only by a fraction\n            scalar = scalar + min_dist/(2*factor)\n            # we have a new standard to judge against, set it\n            last_min_dist = min_dist.copy()\n\n    # return magnitude of step\n    return scalar, pos_step, min_dist", "response": "Given a position field_line sign time and step size this routine finds the distance of the closest approach to the input field line trace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef step_along_mag_unit_vector(x, y, z, date, direction=None, num_steps=5., \n                               step_size=5., scalar=1):\n    \"\"\"\n    Move along 'lines' formed by following the magnetic unit vector directions.\n\n    Moving along the field is effectively the same as a field line trace though\n    extended movement along a field should use the specific field_line_trace \n    method.\n        \n    \n    Parameters\n    ----------\n    x : ECEF-x (km)\n        Location to step from in ECEF (km). Scalar input.\n    y : ECEF-y (km)\n        Location to step from in ECEF (km). Scalar input.\n    z : ECEF-z (km)\n        Location to step from in ECEF (km). Scalar input.\n    date : list-like of datetimes\n        Date and time for magnetic field\n    direction : string\n        String identifier for which unit vector directino to move along.\n        Supported inputs, 'meridional', 'zonal', 'aligned'\n    num_steps : int\n        Number of steps to take along unit vector direction\n    step_size = float\n        Distance taken for each step (km)\n    scalar : int\n        Scalar modifier for step size distance. Input a -1 to move along \n        negative unit vector direction.\n        \n    Returns\n    -------\n    np.array\n        [x, y, z] of ECEF location after taking num_steps along direction, \n        each step_size long.\n    \n    \"\"\"\n    \n    \n    # set parameters for the field line tracing routines\n    field_step_size = 100.\n    field_max_steps = 1000\n    field_steps = np.arange(field_max_steps)\n    \n    for i in np.arange(num_steps):\n        # x, y, z in ECEF\n        # convert to geodetic\n        lat, lon, alt = ecef_to_geodetic(x, y, z)\n        # get unit vector directions\n        zvx, zvy, zvz, bx, by, bz, mx, my, mz = calculate_mag_drift_unit_vectors_ecef(\n                                                        [lat], [lon], [alt], [date],\n                                                        steps=field_steps, \n                                                        max_steps=field_max_steps, \n                                                        step_size=field_step_size, \n                                                        ref_height=0.)\n        # pull out the direction we need\n        if direction == 'meridional':\n            ux, uy, uz = mx, my, mz\n        elif direction == 'zonal':\n            ux, uy, uz = zvx, zvy, zvz\n        elif direction == 'aligned':\n            ux, uy, uz = bx, by, bz\n            \n        # take steps along direction\n        x = x + step_size*ux[0]*scalar\n        y = y + step_size*uy[0]*scalar\n        z = z + step_size*uz[0]*scalar\n            \n    return np.array([x, y, z])", "response": "This function moves a magnetic unit vector into a single field line trace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apex_location_info(glats, glons, alts, dates):\n\n    # use input location and convert to ECEF\n    ecef_xs, ecef_ys, ecef_zs = geodetic_to_ecef(glats, glons, alts)\n    # prepare parameters for field line trace\n    step_size = 100.\n    max_steps = 1000\n    steps = np.arange(max_steps)\n    # high resolution trace parameters\n    fine_step_size = .01\n    fine_max_steps = int(step_size/fine_step_size)+10\n    fine_steps = np.arange(fine_max_steps)\n    # prepare output\n    out_x = []\n    out_y = []\n    out_z = []\n\n    for ecef_x, ecef_y, ecef_z, glat, glon, alt, date in zip(ecef_xs, ecef_ys, ecef_zs, \n                                                             glats, glons, alts, \n                                                             dates):\n        # to get the apex location we need to do a field line trace\n        # then find the highest point\n        trace = full_field_line(np.array([ecef_x, ecef_y, ecef_z]), date, 0., \n                                steps=steps,\n                                step_size=step_size, \n                                max_steps=max_steps)\n        # convert all locations to geodetic coordinates\n        tlat, tlon, talt = ecef_to_geodetic(trace[:,0], trace[:,1], trace[:,2])        \n        # determine location that is highest with respect to the geodetic Earth\n        max_idx = np.argmax(talt)\n        # repeat using a high resolution trace one big step size each \n        # direction around identified max\n        # recurse False ensures only max_steps are taken\n        trace = full_field_line(trace[max_idx,:], date, 0., \n                                steps=fine_steps,\n                                step_size=fine_step_size, \n                                max_steps=fine_max_steps, \n                                recurse=False)\n        # convert all locations to geodetic coordinates\n        tlat, tlon, talt = ecef_to_geodetic(trace[:,0], trace[:,1], trace[:,2])\n        # determine location that is highest with respect to the geodetic Earth\n        max_idx = np.argmax(talt)\n        # collect outputs\n        out_x.append(trace[max_idx,0])\n        out_y.append(trace[max_idx,1])\n        out_z.append(trace[max_idx,2])\n        \n    out_x = np.array(out_x)\n    out_y = np.array(out_y)\n    out_z = np.array(out_z)\n    glat, glon, alt = ecef_to_geodetic(out_x, out_y, out_z)\n    \n    return out_x, out_y, out_z, glat, glon, alt", "response": "Determine apex location for the field line passing through input point."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the total edge length of a closed loop integration along mag field and satrting at inputArcPoints and goes through the given footpoint.", "response": "def closed_loop_edge_lengths_via_footpoint(glats, glons, alts, dates, direction,\n                                           vector_direction, step_size=None, \n                                           max_steps=None, edge_length=25., \n                                           edge_steps=5):\n    \"\"\"\n    Forms closed loop integration along mag field, satrting at input\n    points and goes through footpoint. At footpoint, steps along vector direction\n    in both positive and negative directions, then traces back to opposite\n    footpoint. Back at input location, steps toward those new field lines \n    (edge_length) along vector direction until hitting distance of minimum\n    approach. Loops don't always close. Returns total edge distance \n    that goes through input location, along with the distances of closest approach. \n    \n    Note\n    ----\n        vector direction refers to the magnetic unit vector direction \n    \n    Parameters\n    ----------\n    glats : list-like of floats (degrees)\n        Geodetic (WGS84) latitude\n    glons : list-like of floats (degrees)\n        Geodetic (WGS84) longitude \n    alts : list-like of floats (km)\n        Geodetic (WGS84) altitude, height above surface\n    dates : list-like of datetimes\n        Date and time for determination of scalars\n    direction : string\n        'north' or 'south' for tracing through northern or\n        southern footpoint locations\n    vector_direction : string\n        'meridional' or 'zonal' unit vector directions\n    step_size : float (km)\n        Step size (km) used for field line integration\n    max_steps : int\n        Number of steps taken for field line integration\n    edge_length : float (km)\n        Half of total edge length (step) taken at footpoint location.\n        edge_length step in both positive and negative directions.\n    edge_steps : int\n        Number of steps taken from footpoint towards new field line\n        in a given direction (positive/negative) along unit vector\n        \n    Returns\n    -------\n    np.array, np.array, np.array\n        A closed loop field line path through input location and footpoint in \n        northern/southern hemisphere and back is taken. The return edge length\n        through input location is provided. \n        \n        The distances of closest approach for the positive step along vector\n        direction, and the negative step are returned.\n\n    \n    \"\"\"\n    \n    if step_size is None:\n        step_size = 100.\n    if max_steps is None:\n        max_steps = 1000\n    steps = np.arange(max_steps)\n\n    if direction == 'south':\n        direct = -1\n    elif direction == 'north':\n        direct = 1\n\n    # use spacecraft location to get ECEF\n    ecef_xs, ecef_ys, ecef_zs = geodetic_to_ecef(glats, glons, alts)\n\n    # prepare output\n    full_local_step = []\n    min_distance_plus = []\n    min_distance_minus = []\n\n    for ecef_x, ecef_y, ecef_z, glat, glon, alt, date in zip(ecef_xs, ecef_ys, ecef_zs, \n                                                             glats, glons, alts, \n                                                             dates):\n        # going to try and form close loops via field line integration\n        # start at location of interest, map down to northern or southern \n        # footpoints then take symmetric steps along meridional and zonal \n        # directions and trace back from location of interest, step along \n        # field line directions until we intersect or hit the distance of \n        # closest approach to the return field line with the known \n        # distances of footpoint steps, and the closet approach distance\n        # we can determine the scalar mapping of one location to another\n                    \n        yr, doy = pysat.utils.getyrdoy(date)\n        double_date = float(yr) + float(doy) / 366.\n\n        # print (glat, glon, alt)\n        # trace to footpoint, starting with input location\n        sc_root = np.array([ecef_x, ecef_y, ecef_z])\n        trace = field_line_trace(sc_root, double_date, direct, 120., \n                                 steps=steps,\n                                 step_size=step_size, \n                                 max_steps=max_steps)\n        # pull out footpoint location\n        ftpnt = trace[-1, :]\n        ft_glat, ft_glon, ft_alt = ecef_to_geodetic(*ftpnt)\n        \n        # take step from footpoint along + vector direction\n        plus_step = step_along_mag_unit_vector(ftpnt[0], ftpnt[1], ftpnt[2], \n                                               date, \n                                               direction=vector_direction,\n                                               num_steps=edge_steps,\n                                               step_size=edge_length/edge_steps)\n        # trace this back to other footpoint\n        other_plus = field_line_trace(plus_step, double_date, -direct, 0., \n                                      steps=steps,\n                                      step_size=step_size, \n                                      max_steps=max_steps)\n        # take half step from first footpoint along - vector direction\n        minus_step = step_along_mag_unit_vector(ftpnt[0], ftpnt[1], ftpnt[2], \n                                               date, \n                                               direction=vector_direction, \n                                               scalar=-1,\n                                               num_steps=edge_steps,\n                                               step_size=edge_length/edge_steps)\n        # trace this back to other footpoint\n        other_minus = field_line_trace(minus_step, double_date, -direct, 0., \n                                       steps=steps,\n                                       step_size=step_size, \n                                       max_steps=max_steps)\n        # need to determine where the intersection of field line coming back from\n        # footpoint through postive vector direction step and back\n        # in relation to the vector direction from the s/c location. \n        pos_edge_length, _, mind_pos = step_until_intersect(sc_root,\n                                        other_plus,\n                                        1, date, \n                                        direction=vector_direction,\n                                        field_step_size=step_size,\n                                        step_size_goal=edge_length/edge_steps)        \n        # take half step from S/C along - vector direction \n        minus_edge_length, _, mind_minus = step_until_intersect(sc_root,\n                                        other_minus,\n                                        -1, date, \n                                        direction=vector_direction,\n                                        field_step_size=step_size,\n                                        step_size_goal=edge_length/edge_steps)\n        # collect outputs\n        full_local_step.append(pos_edge_length + minus_edge_length)\n        min_distance_plus.append(mind_pos)\n        min_distance_minus.append(mind_minus)\n        \n    return np.array(full_local_step), np.array(min_distance_plus), np.array(min_distance_minus)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the distance between apex locations mapping to the input location. Using the input location, the apex location is calculated. Also from the input location, a step along both the positive and negative vector_directions is taken, and the apex locations for those points are calculated. The difference in position between these apex locations is the total centered distance between magnetic field lines at the magnetic apex when starting locally with a field line half distance of edge_length. An alternative method has been implemented, then commented out. This technique takes multiple steps from the origin apex towards the apex locations identified along vector_direction. In principle this is more accurate but more computationally intensive, similar to the footpoint model. A comparison is planned. Note ---- vector direction refers to the magnetic unit vector direction Parameters ---------- glats : list-like of floats (degrees) Geodetic (WGS84) latitude glons : list-like of floats (degrees) Geodetic (WGS84) longitude alts : list-like of floats (km) Geodetic (WGS84) altitude, height above surface dates : list-like of datetimes Date and time for determination of scalars vector_direction : string 'meridional' or 'zonal' unit vector directions step_size : float (km) Step size (km) used for field line integration max_steps : int Number of steps taken for field line integration edge_length : float (km) Half of total edge length (step) taken at footpoint location. edge_length step in both positive and negative directions. edge_steps : int Number of steps taken from footpoint towards new field line in a given direction (positive/negative) along unit vector Returns ------- np.array, ### np.array, np.array The change in field line apex locations. ## Pending ## The return edge length through input location is provided. ## Pending ## The distances of closest approach for the positive step along vector direction, and the negative step are returned.", "response": "def closed_loop_edge_lengths_via_equator(glats, glons, alts, dates,\n                                         vector_direction,\n                                         edge_length=25., \n                                         edge_steps=5):\n    \"\"\"\n    Calculates the distance between apex locations mapping to the input location.\n    \n    Using the input location, the apex location is calculated. Also from the input \n    location, a step along both the positive and negative\n    vector_directions is taken, and the apex locations for those points are calculated.\n    The difference in position between these apex locations is the total centered\n    distance between magnetic field lines at the magnetic apex when starting\n    locally with a field line half distance of edge_length.\n    \n    An alternative method has been implemented, then commented out.\n    This technique takes multiple steps from the origin apex towards the apex\n    locations identified along vector_direction. In principle this is more accurate\n    but more computationally intensive, similar to the footpoint model.\n    A comparison is planned.\n    \n    \n    Note\n    ----\n        vector direction refers to the magnetic unit vector direction \n    \n    Parameters\n    ----------\n    glats : list-like of floats (degrees)\n        Geodetic (WGS84) latitude\n    glons : list-like of floats (degrees)\n        Geodetic (WGS84) longitude \n    alts : list-like of floats (km)\n        Geodetic (WGS84) altitude, height above surface\n    dates : list-like of datetimes\n        Date and time for determination of scalars\n    vector_direction : string\n        'meridional' or 'zonal' unit vector directions\n    step_size : float (km)\n        Step size (km) used for field line integration\n    max_steps : int\n        Number of steps taken for field line integration\n    edge_length : float (km)\n        Half of total edge length (step) taken at footpoint location.\n        edge_length step in both positive and negative directions.\n    edge_steps : int\n        Number of steps taken from footpoint towards new field line\n        in a given direction (positive/negative) along unit vector\n        \n    Returns\n    -------\n    np.array, ### np.array, np.array\n        The change in field line apex locations. \n        \n        ## Pending ## The return edge length through input location is provided. \n        \n        ## Pending ## The distances of closest approach for the positive step \n                      along vector direction, and the negative step are returned.\n\n    \n    \"\"\"\n\n    # use spacecraft location to get ECEF\n    ecef_xs, ecef_ys, ecef_zs = geodetic_to_ecef(glats, glons, alts)\n\n    # prepare output\n    apex_edge_length = []\n    # outputs for alternative calculation\n    full_local_step = []\n    min_distance_plus = []\n    min_distance_minus = []\n\n    for ecef_x, ecef_y, ecef_z, glat, glon, alt, date in zip(ecef_xs, ecef_ys, ecef_zs, \n                                                             glats, glons, alts, \n                                                             dates):\n        \n        yr, doy = pysat.utils.getyrdoy(date)\n        double_date = float(yr) + float(doy) / 366.\n                    \n        # get location of apex for s/c field line\n        apex_x, apex_y, apex_z, apex_lat, apex_lon, apex_alt = apex_location_info(\n                                                                    [glat], [glon], \n                                                                    [alt], [date])\n        # apex in ecef (maps to input location)\n        apex_root = np.array([apex_x[0], apex_y[0], apex_z[0]])      \n        # take step from s/c along + vector direction\n        # then get the apex location\n        plus = step_along_mag_unit_vector(ecef_x, ecef_y, ecef_z, date, \n                                          direction=vector_direction,\n                                          num_steps=edge_steps,\n                                          step_size=edge_length/edge_steps)\n        plus_lat, plus_lon, plus_alt = ecef_to_geodetic(*plus)\n        plus_apex_x, plus_apex_y, plus_apex_z, plus_apex_lat, plus_apex_lon, plus_apex_alt = \\\n                    apex_location_info([plus_lat], [plus_lon], [plus_alt], [date])\n        # plus apex location in ECEF\n        plus_apex_root = np.array([plus_apex_x[0], plus_apex_y[0], plus_apex_z[0]])   \n\n        # take half step from s/c along - vector direction\n        # then get the apex location\n        minus = step_along_mag_unit_vector(ecef_x, ecef_y, ecef_z, date, \n                                               direction=vector_direction, \n                                               scalar=-1,\n                                               num_steps=edge_steps,\n                                               step_size=edge_length/edge_steps)\n        minus_lat, minus_lon, minus_alt = ecef_to_geodetic(*minus)\n        minus_apex_x, minus_apex_y, minus_apex_z, minus_apex_lat, minus_apex_lon, minus_apex_alt = \\\n                    apex_location_info([minus_lat], [minus_lon], [minus_alt], [date])\n        minus_apex_root = np.array([minus_apex_x[0], minus_apex_y[0], minus_apex_z[0]])   \n\n        # take difference in apex locations\n        apex_edge_length.append(np.sqrt((plus_apex_x[0]-minus_apex_x[0])**2 + \n                                        (plus_apex_y[0]-minus_apex_y[0])**2 + \n                                        (plus_apex_z[0]-minus_apex_z[0])**2))\n\n#         # take an alternative path to calculation\n#         # do field line trace around pos and neg apexes\n#         # then do intersection with field line projection thing        \n# \n#         # do a short centered field line trace around plus apex location\n#         other_trace = full_field_line(plus_apex_root, double_date, 0., \n#                                       step_size=1., \n#                                       max_steps=10,\n#                                       recurse=False)\n#         # need to determine where the intersection of apex field line \n#         # in relation to the vector direction from the s/c field apex location.\n#         pos_edge_length, _, mind_pos = step_until_intersect(apex_root,\n                                        # other_trace,\n                                        # 1, date, \n                                        # direction=vector_direction,\n                                        # field_step_size=1.,\n                                        # step_size_goal=edge_length/edge_steps)                                                                                               \n#         # do a short centered field line trace around 'minus' apex location\n#         other_trace = full_field_line(minus_apex_root, double_date, 0., \n#                                       step_size=1., \n#                                       max_steps=10,\n#                                       recurse=False)\n#         # need to determine where the intersection of apex field line \n#         # in relation to the vector direction from the s/c field apex location. \n#         minus_edge_length, _, mind_minus = step_until_intersect(apex_root,\n                                        # other_trace,\n                                        # -1, date, \n                                        # direction=vector_direction,\n                                        # field_step_size=1.,\n                                        # step_size_goal=edge_length/edge_steps)        \n        # full_local_step.append(pos_edge_length + minus_edge_length)\n        # min_distance_plus.append(mind_pos)\n        # min_distance_minus.append(mind_minus)\n        \n        # still sorting out alternative option for this calculation\n        # commented code is 'good' as far as the plan goes\n        # takes more time, so I haven't tested one vs the other yet\n        # having two live methods can lead to problems\n        # THIS IS A TODO (sort it out)\n    return np.array(apex_edge_length)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the scalars for translating ion drifts at position glat glon and alt for date and time above surface.", "response": "def scalars_for_mapping_ion_drifts(glats, glons, alts, dates, step_size=None, \n                                   max_steps=None, e_field_scaling_only=False):\n    \"\"\"\n    Calculates scalars for translating ion motions at position\n    glat, glon, and alt, for date, to the footpoints of the field line\n    as well as at the magnetic equator.\n    \n    All inputs are assumed to be 1D arrays.\n    \n    Note\n    ----\n        Directions refer to the ion motion direction e.g. the zonal\n        scalar applies to zonal ion motions (meridional E field assuming ExB ion motion)\n    \n    Parameters\n    ----------\n    glats : list-like of floats (degrees)\n        Geodetic (WGS84) latitude\n    glons : list-like of floats (degrees)\n        Geodetic (WGS84) longitude \n    alts : list-like of floats (km)\n        Geodetic (WGS84) altitude, height above surface\n    dates : list-like of datetimes\n        Date and time for determination of scalars\n    e_field_scaling_only : boolean (False)\n        If True, method only calculates the electric field scalar, ignoring \n        changes in magnitude of B. Note ion velocity related to E/B.\n        \n    Returns\n    -------\n    dict\n        array-like of scalars for translating ion drifts. Keys are,\n        'north_zonal_drifts_scalar', 'north_mer_drifts_scalar', and similarly\n        for southern locations. 'equator_mer_drifts_scalar' and \n        'equator_zonal_drifts_scalar' cover the mappings to the equator.\n    \n    \"\"\"\n    \n    if step_size is None:\n        step_size = 100.\n    if max_steps is None:\n        max_steps = 1000\n    steps = np.arange(max_steps)\n\n    # use spacecraft location to get ECEF\n    ecef_xs, ecef_ys, ecef_zs = geodetic_to_ecef(glats, glons, alts)\n\n    # prepare output\n    eq_zon_drifts_scalar = []\n    eq_mer_drifts_scalar = []\n    # magnetic field info\n    north_mag_scalar = []\n    south_mag_scalar = []\n    eq_mag_scalar = []\n    out = {}\n    # meridional e-field scalar map, can also be\n    # zonal ion drift scalar map\n    # print ('Starting Northern')\n    north_zon_drifts_scalar, mind_plus, mind_minus = closed_loop_edge_lengths_via_footpoint(glats,\n                                                        glons, alts, dates, 'north',\n                                                        'meridional',\n                                                        step_size=step_size,\n                                                        max_steps=max_steps, \n                                                        edge_length=25.,\n                                                        edge_steps=5)\n\n    north_mer_drifts_scalar, mind_plus, mind_minus = closed_loop_edge_lengths_via_footpoint(glats,\n                                                        glons, alts, dates, 'north',\n                                                        'zonal',\n                                                        step_size=step_size,\n                                                        max_steps=max_steps, \n                                                        edge_length=25.,\n                                                        edge_steps=5)\n\n    # print ('Starting Southern')\n    south_zon_drifts_scalar, mind_plus, mind_minus = closed_loop_edge_lengths_via_footpoint(glats,\n                                                        glons, alts, dates, 'south',\n                                                        'meridional',\n                                                        step_size=step_size,\n                                                        max_steps=max_steps, \n                                                        edge_length=25.,\n                                                        edge_steps=5)\n\n    south_mer_drifts_scalar, mind_plus, mind_minus = closed_loop_edge_lengths_via_footpoint(glats,\n                                                        glons, alts, dates, 'south',\n                                                        'zonal',\n                                                        step_size=step_size,\n                                                        max_steps=max_steps, \n                                                        edge_length=25.,\n                                                        edge_steps=5)\n    # print ('Starting Equatorial')                \n    # , step_zon_apex2, mind_plus, mind_minus                                 \n    eq_zon_drifts_scalar = closed_loop_edge_lengths_via_equator(glats, glons, alts, dates,\n                                                        'meridional',\n                                                        edge_length=25., \n                                                        edge_steps=5)\n    # , step_mer_apex2, mind_plus, mind_minus                                                      \n    eq_mer_drifts_scalar = closed_loop_edge_lengths_via_equator(glats, glons, alts, dates,\n                                                        'zonal',\n                                                        edge_length=25., \n                                                        edge_steps=5)\n    # print ('Done with core')\n    north_zon_drifts_scalar = north_zon_drifts_scalar/50. \n    south_zon_drifts_scalar = south_zon_drifts_scalar/50. \n    north_mer_drifts_scalar = north_mer_drifts_scalar/50. \n    south_mer_drifts_scalar = south_mer_drifts_scalar/50. \n    # equatorial \n    eq_zon_drifts_scalar = 50./eq_zon_drifts_scalar\n    eq_mer_drifts_scalar = 50./eq_mer_drifts_scalar\n\n    if e_field_scaling_only:\n        # prepare output\n        out['north_mer_fields_scalar'] = north_zon_drifts_scalar\n        out['south_mer_fields_scalar'] = south_zon_drifts_scalar\n        out['north_zon_fields_scalar'] = north_mer_drifts_scalar\n        out['south_zon_fields_scalar'] = south_mer_drifts_scalar\n        out['equator_mer_fields_scalar'] = eq_zon_drifts_scalar\n        out['equator_zon_fields_scalar'] = eq_mer_drifts_scalar\n    \n    else:\n        # figure out scaling for drifts based upon change in magnetic field\n        # strength\n        for ecef_x, ecef_y, ecef_z, glat, glon, alt, date in zip(ecef_xs, ecef_ys, ecef_zs, \n                                                                glats, glons, alts, \n                                                                dates):            \n            yr, doy = pysat.utils.getyrdoy(date)\n            double_date = float(yr) + float(doy) / 366.\n            # get location of apex for s/c field line\n            apex_x, apex_y, apex_z, apex_lat, apex_lon, apex_alt = apex_location_info(\n                                                                        [glat], [glon], \n                                                                        [alt], [date])    \n            # trace to northern footpoint\n            sc_root = np.array([ecef_x, ecef_y, ecef_z])\n            trace_north = field_line_trace(sc_root, double_date, 1., 120., \n                                        steps=steps,\n                                        step_size=step_size, \n                                        max_steps=max_steps)\n            # southern tracing\n            trace_south = field_line_trace(sc_root, double_date, -1., 120., \n                                        steps=steps,\n                                        step_size=step_size, \n                                        max_steps=max_steps)\n            # footpoint location\n            north_ftpnt = trace_north[-1, :]\n            nft_glat, nft_glon, nft_alt = ecef_to_geodetic(*north_ftpnt)\n            south_ftpnt = trace_south[-1, :]\n            sft_glat, sft_glon, sft_alt = ecef_to_geodetic(*south_ftpnt)\n    \n            # scalar for the northern footpoint electric field based on distances\n            # for drift also need to include the magnetic field, drift = E/B\n            tbn, tbe, tbd, b_sc = igrf.igrf12syn(0, double_date, 1, alt, \n                                                np.deg2rad(90.-glat), \n                                                np.deg2rad(glon))\n            # get mag field and scalar for northern footpoint\n            tbn, tbe, tbd, b_nft = igrf.igrf12syn(0, double_date, 1, nft_alt, \n                                                np.deg2rad(90.-nft_glat), \n                                                np.deg2rad(nft_glon))\n            north_mag_scalar.append(b_sc/b_nft)            \n            # equatorial values\n            tbn, tbe, tbd, b_eq = igrf.igrf12syn(0, double_date, 1, apex_alt, \n                                                 np.deg2rad(90.-apex_lat), \n                                                 np.deg2rad(apex_lon))\n            eq_mag_scalar.append(b_sc/b_eq)        \n            # scalar for the southern footpoint\n            tbn, tbe, tbd, b_sft = igrf.igrf12syn(0, double_date, 1, sft_alt, \n                                                  np.deg2rad(90.-sft_glat), \n                                                  np.deg2rad(sft_glon))\n            south_mag_scalar.append(b_sc/b_sft)\n        \n        # make E-Field scalars to drifts\n        # lists to arrays\n        north_mag_scalar = np.array(north_mag_scalar)\n        south_mag_scalar = np.array(south_mag_scalar)\n        eq_mag_scalar = np.array(eq_mag_scalar)\n        # apply to electric field scaling to get ion drift values\n        north_zon_drifts_scalar = north_zon_drifts_scalar*north_mag_scalar\n        south_zon_drifts_scalar = south_zon_drifts_scalar*south_mag_scalar \n        north_mer_drifts_scalar = north_mer_drifts_scalar*north_mag_scalar\n        south_mer_drifts_scalar = south_mer_drifts_scalar*south_mag_scalar\n        # equatorial \n        eq_zon_drifts_scalar = eq_zon_drifts_scalar*eq_mag_scalar\n        eq_mer_drifts_scalar = eq_mer_drifts_scalar*eq_mag_scalar\n        # output\n        out['north_zonal_drifts_scalar'] = north_zon_drifts_scalar\n        out['south_zonal_drifts_scalar'] = south_zon_drifts_scalar\n        out['north_mer_drifts_scalar'] = north_mer_drifts_scalar\n        out['south_mer_drifts_scalar'] = south_mer_drifts_scalar\n        out['equator_zonal_drifts_scalar'] = eq_zon_drifts_scalar\n        out['equator_mer_drifts_scalar'] = eq_mer_drifts_scalar\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying the base fabfile. py to the current working directory.", "response": "def add_fabfile():\n    \"\"\"\n    Copy the base fabfile.py to the current working directory.\n    \"\"\"\n    fabfile_src  = os.path.join(PACKAGE_ROOT, 'fabfile.py')\n    fabfile_dest = os.path.join(os.getcwd(), 'fabfile_deployer.py')\n\n    if os.path.exists(fabfile_dest):\n        print \"`fabfile.py` exists in the current directory. \" \\\n              \"Please remove or rename it and try again.\"\n        return\n\n    shutil.copyfile(fabfile_src, fabfile_dest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the item from the infoblox server.", "response": "def delete(self):\n        \"\"\"Remove the item from the infoblox server.\n\n        :rtype: bool\n        :raises: AssertionError\n        :raises: ValueError\n        :raises: infoblox.exceptions.ProtocolError\n\n        \"\"\"\n        if not self._ref:\n            raise ValueError('Object has no reference id for deletion')\n        if 'save' not in self._supports:\n            raise AssertionError('Can not save this object type')\n        response = self._session.delete(self._path)\n        if response.status_code == 200:\n            self._ref = None\n            self.clear()\n            return True\n        try:\n            error = response.json()\n            raise exceptions.ProtocolError(error['text'])\n        except ValueError:\n            raise exceptions.ProtocolError(response.content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to fetch the object from the Infoblox device.", "response": "def fetch(self):\n        \"\"\"Attempt to fetch the object from the Infoblox device. If successful\n        the object will be updated and the method will return True.\n\n        :rtype: bool\n        :raises: infoblox.exceptions.ProtocolError\n\n        \"\"\"\n        LOGGER.debug('Fetching %s, %s', self._path, self._search_values)\n        response = self._session.get(self._path, self._search_values,\n                                     {'_return_fields': self._return_fields})\n        if response.status_code == 200:\n            values = response.json()\n            self._assign(values)\n            return bool(values)\n        elif response.status_code >= 400:\n            try:\n                error = response.json()\n                raise exceptions.ProtocolError(error['text'])\n            except ValueError:\n                raise exceptions.ProtocolError(response.content)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the infoblox with new values for the specified object or add them if it s a new object all together.", "response": "def save(self):\n        \"\"\"Update the infoblox with new values for the specified object, or add\n        the values if it's a new object all together.\n\n        :raises: AssertionError\n        :raises: infoblox.exceptions.ProtocolError\n\n        \"\"\"\n        if 'save' not in self._supports:\n            raise AssertionError('Can not save this object type')\n\n        values = {}\n        for key in [key for key in self.keys() if key not in self._save_ignore]:\n            if not getattr(self, key) and getattr(self, key) != False:\n                continue\n\n            if isinstance(getattr(self, key, None), list):\n                value = list()\n                for item in getattr(self, key):\n                    if isinstance(item, dict):\n                        value.append(item)\n                    elif hasattr(item, '_save_as'):\n                        value.append(item._save_as())\n                    elif hasattr(item, '_ref') and getattr(item, '_ref'):\n                        value.append(getattr(item, '_ref'))\n                    else:\n                        LOGGER.warning('Cant assign %r', item)\n                values[key] = value\n            elif getattr(self, key, None):\n                values[key] = getattr(self, key)\n        if not self._ref:\n            response = self._session.post(self._path, values)\n        else:\n            values['_ref'] = self._ref\n            response = self._session.put(self._path, values)\n        LOGGER.debug('Response: %r, %r', response.status_code, response.content)\n        if 200 <= response.status_code <= 201:\n            self.fetch()\n            return True\n        else:\n            try:\n                error = response.json()\n                raise exceptions.ProtocolError(error['text'])\n            except ValueError:\n                raise exceptions.ProtocolError(response.content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns the values passed as a dict or list to the object if the key for each value matches an available attribute on the object.", "response": "def _assign(self, values):\n        \"\"\"Assign the values passed as either a dict or list to the object if\n        the key for each value matches an available attribute on the object.\n\n        :param dict values: The values to assign\n\n        \"\"\"\n        LOGGER.debug('Assigning values: %r', values)\n        if not values:\n            return\n        keys = self.keys()\n        if not self._ref:\n            keys.append('_ref')\n        if isinstance(values, dict):\n            for key in keys:\n                if values.get(key):\n                    if isinstance(values.get(key), list):\n                        items = list()\n                        for item in values[key]:\n                            if isinstance(item, dict):\n                                if '_ref' in item:\n                                    obj_class = get_class(item['_ref'])\n                                    if obj_class:\n                                        items.append(obj_class(self._session,\n                                                               **item))\n                            else:\n                                items.append(item)\n                        setattr(self, key, items)\n                    else:\n                        setattr(self, key, values[key])\n        elif isinstance(values, list):\n            self._assign(values[0])\n        else:\n            LOGGER.critical('Unhandled return type: %r', values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_search_values(self, kwargs):\n        criteria = {}\n        for key in self._search_by:\n            if getattr(self, key, None):\n                criteria[key] = getattr(self, key)\n            elif key in kwargs and kwargs.get(key):\n                criteria[key] = kwargs.get(key)\n        return criteria", "response": "Build the search criteria dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an IPv4 address to the host.", "response": "def add_ipv4addr(self, ipv4addr):\n        \"\"\"Add an IPv4 address to the host.\n\n        :param str ipv4addr: The IP address to add.\n        :raises: ValueError\n\n        \"\"\"\n        for addr in self.ipv4addrs:\n            if ((isinstance(addr, dict) and addr['ipv4addr'] == ipv4addr) or\n                (isinstance(addr, HostIPv4) and addr.ipv4addr == ipv4addr)):\n                raise ValueError('Already exists')\n        self.ipv4addrs.append({'ipv4addr': ipv4addr})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an IPv4 address from the host.", "response": "def remove_ipv4addr(self, ipv4addr):\n        \"\"\"Remove an IPv4 address from the host.\n\n        :param str ipv4addr: The IP address to remove\n\n        \"\"\"\n        for addr in self.ipv4addrs:\n            if ((isinstance(addr, dict) and addr['ipv4addr'] == ipv4addr) or\n                (isinstance(addr, HostIPv4) and addr.ipv4addr == ipv4addr)):\n                self.ipv4addrs.remove(addr)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an IPv6 address to the host.", "response": "def add_ipv6addr(self, ipv6addr):\n        \"\"\"Add an IPv6 address to the host.\n\n        :param str ipv6addr: The IP address to add.\n        :raises: ValueError\n\n        \"\"\"\n        for addr in self.ipv6addrs:\n            if ((isinstance(addr, dict) and addr['ipv6addr'] == ipv6addr) or\n                (isinstance(addr, HostIPv4) and addr.ipv6addr == ipv6addr)):\n                raise ValueError('Already exists')\n        self.ipv6addrs.append({'ipv6addr': ipv6addr})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_ipv6addr(self, ipv6addr):\n        for addr in self.ipv6addrs:\n            if ((isinstance(addr, dict) and addr['ipv6addr'] == ipv6addr) or\n                (isinstance(addr, HostIPv4) and addr.ipv6addr == ipv6addr)):\n                self.ipv6addrs.remove(addr)\n                break", "response": "Removes an IPv6 address from the host."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sqla_postgresql(self, uri, version=None,\n                         isolation_level=\"READ COMMITTED\"):\n        '''\n        expected uri form:\n        postgresql+psycopg2://%s:%s@%s:%s/%s' % (\n            username, password, host, port, db)\n        '''\n        isolation_level = isolation_level or \"READ COMMITTED\"\n        kwargs = dict(isolation_level=isolation_level)\n        # FIXME: version of postgresql < 9.2 don't have pg.JSON!\n        # check and use JSONTypedLite instead\n        # override default dict and list column types\n        types = {list: pg.ARRAY, tuple: pg.ARRAY, set: pg.ARRAY,\n                 dict: JSONDict, datetime: UTCEpoch}\n        self.type_map.update(types)\n        bs = self.config['batch_size']\n        # 999 batch_size is default for sqlite, postgres handles more at once\n        self.config['batch_size'] = 5000 if bs == 999 else bs\n        self._lock_required = False\n        # default schema name is 'public' for postgres\n        dsn = self.config['db_schema']\n        self.config['db_schema'] = dsn or 'public'\n        return uri, kwargs", "response": "Internal function to handle SQLAlchemy postgresql connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autoschema(self, objects, **kwargs):\n        ''' wrapper around utils.autoschema function '''\n        return autoschema(objects=objects, exclude_keys=self.RESTRICTED_KEYS,\n                          **kwargs)", "response": "wrapper around utils. autoschema function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count(self, query=None, date=None, table=None):\n        '''\n        Run a query on the given cube and return only\n        the count of resulting matches.\n\n        :param query: The query in pql\n        :param date: date (metrique date range) that should be queried\n                    If date==None then the most recent versions of the\n                    objects will be queried.\n        :param collection: cube name\n        :param owner: username of cube owner\n        '''\n        table = table or self.config.get('table')\n        sql_count = select([func.count()])\n        query = self._parse_query(table=table, query=query, date=date,\n                                  fields='id', alias='anon_x')\n\n        if query is not None:\n            query = sql_count.select_from(query)\n        else:\n            table = self.get_table(table)\n            query = sql_count\n            query = query.select_from(table)\n        return self.session_auto.execute(query).scalar()", "response": "Run a query on the given cube and return only\n        the count of resulting matches."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deptree(self, field, oids, date=None, level=None, table=None):\n        '''\n        Dependency tree builder. Recursively fetchs objects that\n        are children of the initial set of parent object ids provided.\n\n        :param field: Field that contains the 'parent of' data\n        :param oids: Object oids to build depedency tree for\n        :param date: date (metrique date range) that should be queried.\n                    If date==None then the most recent versions of the\n                    objects will be queried.\n        :param level: limit depth of recursion\n        '''\n        table = self.get_table(table)\n        fringe = str2list(oids)\n        checked = set(fringe)\n        loop_k = 0\n        while len(fringe) > 0:\n            if level and loop_k == abs(level):\n                break\n            query = '_oid in %s' % list(fringe)\n            docs = self.find(table=table, query=query, fields=[field],\n                             date=date, raw=True)\n            fringe = {oid for doc in docs for oid in (doc[field] or [])\n                      if oid not in checked}\n            checked |= fringe\n            loop_k += 1\n        return sorted(checked)", "response": "Build a dependency tree for a set of object ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index(self, fields, name=None, table=None, **kwargs):\n        '''\n        Build a new index on a cube.\n\n        Examples:\n            + index('field_name')\n\n        :param fields: A single field or a list of (key, direction) pairs\n        :param name: (optional) Custom name to use for this index\n        :param collection: cube name\n        :param owner: username of cube owner\n        '''\n        table = self.get_table(table)\n        name = self._index_default_name(fields, name)\n        fields = parse.parse_fields(fields)\n        fields = self.columns(table, fields, reflect=True)\n        session = self.session_new()\n        index = Index(name, *fields)\n        logger.info('Writing new index %s: %s' % (name, fields))\n        result = index.create(self.engine)\n        session.commit()\n        return result", "response": "Build a new index on a cube."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index_list(self):\n        '''\n        List all cube indexes\n\n        :param collection: cube name\n        :param owner: username of cube owner\n        '''\n        logger.info('Listing indexes')\n        _ix = {}\n        _i = self.inspector\n        for tbl in _i.get_table_names():\n            _ix.setdefault(tbl, [])\n            for ix in _i.get_indexes(tbl):\n                _ix[tbl].append(ix)\n        return _ix", "response": "List all cube indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all cubes available to the calling client.", "response": "def ls(self, startswith=None):\n        '''\n        List all cubes available to the calling client.\n\n        :param startswith: string to use in a simple \"startswith\" query filter\n        :returns list: sorted list of cube names\n        '''\n        logger.info('Listing cubes starting with \"%s\")' % startswith)\n        startswith = unicode(startswith or '')\n        tables = sorted(name for name in self.db_tables\n                        if name.startswith(startswith))\n        return tables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef share(self, with_user, roles=None, table=None):\n        '''\n        Give cube access rights to another user\n\n        Not, this method is NOT supported by SQLite3!\n        '''\n        table = self.get_table(table)\n        is_true(table is not None, 'invalid table: %s' % table)\n        with_user = validate_username(with_user)\n        roles = roles or ['SELECT']\n        roles = validate_roles(roles, self.VALID_SHARE_ROLES)\n        roles = list2str(roles)\n        logger.info('Sharing cube %s with %s (%s)' % (table, with_user, roles))\n        sql = 'GRANT %s ON %s TO %s' % (roles, table, with_user)\n        return self.session_auto.execute(sql)", "response": "Share a user with another user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef muscle_seqs(seqs,\n                 add_seq_names=False,\n                 out_filename=None,\n                 input_handler=None,\n                 params={},\n                 WorkingDir=tempfile.gettempdir(),\n                 SuppressStderr=None,\n                 SuppressStdout=None):\n    \"\"\"Muscle align list of sequences.\n\n    seqs: a list of sequences as strings or objects, you must set add_seq_names=True\n    or sequences in a multiline string, as read() from a fasta file\n    or sequences in a list of lines, as readlines() from a fasta file\n    or a fasta seq filename.\n\n    == for eg, testcode for guessing\n        #guess_input_handler should correctly identify input\n        gih = guess_input_handler\n        self.assertEqual(gih('abc.txt'), '_input_as_string')\n        self.assertEqual(gih('>ab\\nTCAG'), '_input_as_multiline_string')\n        self.assertEqual(gih(['ACC','TGA'], True), '_input_as_seqs')\n        self.assertEqual(gih(['>a','ACC','>b','TGA']), '_input_as_lines')\n\n    == docstring for blast_seqs, apply to muscle_seqs ==\n    seqs: either file name or list of sequence objects or list of strings or\n    single multiline string containing sequences.\n\n    WARNING: DECISION RULES FOR INPUT HANDLING HAVE CHANGED. Decision rules\n    for data are as follows. If it's s list, treat as lines, unless\n    add_seq_names is true (in which case treat as list of seqs). If it's a\n    string, test whether it has newlines. If it doesn't have newlines, assume\n    it's a filename. If it does have newlines, it can't be a filename, so\n    assume it's a multiline string containing sequences.\n\n    If you want to skip the detection and force a specific type of input\n    handler, use input_handler='your_favorite_handler'.\n\n    add_seq_names: boolean. if True, sequence names are inserted in the list\n        of sequences. if False, it assumes seqs is a list of lines of some\n        proper format that the program can handle\n\n    Addl docs coming soon\n    \"\"\"\n\n    if out_filename:\n        params[\"-out\"] = out_filename\n    #else:\n    #    params[\"-out\"] = get_tmp_filename(WorkingDir)\n\n    ih = input_handler or guess_input_handler(seqs, add_seq_names)\n    muscle_app = Muscle(\n                   params=params,\n                   InputHandler=ih,\n                   WorkingDir=WorkingDir,\n                   SuppressStderr=SuppressStderr,\n                   SuppressStdout=SuppressStdout)\n    return muscle_app(seqs)", "response": "Muscle aligns a list of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmuscling cluster list of sequences.", "response": "def cluster_seqs(seqs,\n                 neighbor_join=False,\n                 params={},\n                 add_seq_names=True,\n                 WorkingDir=tempfile.gettempdir(),\n                 SuppressStderr=None,\n                 SuppressStdout=None,\n                 max_chars=1000000,\n                 max_hours=1.0,\n                 constructor=PhyloNode,\n                 clean_up=True\n                 ):\n    \"\"\"Muscle cluster list of sequences.\n\n    seqs: either file name or list of sequence objects or list of strings or\n        single multiline string containing sequences.\n\n    Addl docs coming soon\n    \"\"\"\n    num_seqs = len(seqs)\n    if num_seqs < 2:\n        raise ValueError, \"Muscle requres 2 or more sequences to cluster.\"\n\n\n    num_chars = sum(map(len, seqs))\n    if num_chars > max_chars:\n        params[\"-maxiters\"] = 2\n        params[\"-diags1\"] = True\n        params[\"-sv\"] = True\n        #params[\"-distance1\"] = \"kmer6_6\"\n        #params[\"-distance1\"] = \"kmer20_3\"\n        #params[\"-distance1\"] = \"kbit20_3\"\n        print \"lots of chars, using fast align\", num_chars\n\n\n    params[\"-maxhours\"] = max_hours\n    #params[\"-maxiters\"] = 10\n\n    #cluster_type = \"upgmb\"\n    #if neighbor_join:\n    #    cluster_type = \"neighborjoining\"\n\n    params[\"-clusteronly\"] = True\n    params[\"-tree1\"] = get_tmp_filename(WorkingDir)\n\n    muscle_res = muscle_seqs(seqs,\n                 params=params,\n                 add_seq_names=add_seq_names,\n                 WorkingDir=WorkingDir,\n                 SuppressStderr=SuppressStderr,\n                 SuppressStdout=SuppressStdout)\n\n    tree = DndParser(muscle_res[\"Tree1Out\"], constructor=constructor)\n\n    if clean_up:\n        muscle_res.cleanUp()\n    return tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aln_tree_seqs(seqs,\n                 input_handler=None,\n                 tree_type='neighborjoining',\n                 params={},\n                 add_seq_names=True,\n                 WorkingDir=tempfile.gettempdir(),\n                 SuppressStderr=None,\n                 SuppressStdout=None,\n                 max_hours=5.0,\n                 constructor=PhyloNode,\n                 clean_up=True\n                 ):\n    \"\"\"Muscle align sequences and report tree from iteration2.\n\n    Unlike cluster_seqs, returns tree2 which is the tree made during the\n    second muscle iteration (it should be more accurate that the cluster from\n    the first iteration which is made fast based on  k-mer words)\n\n    seqs: either file name or list of sequence objects or list of strings or\n        single multiline string containing sequences.\n    tree_type: can be either neighborjoining (default) or upgmb for UPGMA\n    clean_up: When true, will clean up output files\n    \"\"\"\n\n    params[\"-maxhours\"] = max_hours\n    if tree_type:\n        params[\"-cluster2\"] = tree_type\n    params[\"-tree2\"] = get_tmp_filename(WorkingDir)\n    params[\"-out\"] = get_tmp_filename(WorkingDir)\n\n    muscle_res = muscle_seqs(seqs,\n                 input_handler=input_handler,\n                 params=params,\n                 add_seq_names=add_seq_names,\n                 WorkingDir=WorkingDir,\n                 SuppressStderr=SuppressStderr,\n                 SuppressStdout=SuppressStdout)\n    tree = DndParser(muscle_res[\"Tree2Out\"], constructor=constructor)\n    aln = [line for line in muscle_res[\"MuscleOut\"]]\n\n    if clean_up:\n        muscle_res.cleanUp()\n    return tree, aln", "response": "Muscle align sequences and report tree from iteration2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an Alignment object from seqs.", "response": "def align_unaligned_seqs(seqs, moltype=DNA, params=None):\n    \"\"\"Returns an Alignment object from seqs.\n\n    seqs: SequenceCollection object, or data that can be used to build one.\n\n    moltype: a MolType object.  DNA, RNA, or PROTEIN.\n\n    params: dict of parameters to pass in to the Muscle app controller.\n\n    Result will be an Alignment object.\n    \"\"\"\n    if not params:\n        params = {}\n    #create SequenceCollection object from seqs\n    seq_collection = SequenceCollection(seqs,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n    #get temporary filename\n    params.update({'-out':get_tmp_filename()})\n    #Create Muscle app.\n    app = Muscle(InputHandler='_input_as_multiline_string',\\\n                 params=params, WorkingDir=tempfile.gettempdir())\n    #Get results using int_map as input to app\n    res = app(int_map.toFasta())\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['MuscleOut']))\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        new_alignment[int_keys[k]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    del(seq_collection,int_map,int_keys,app,res,alignment,params)\n\n    return new_alignment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef align_and_build_tree(seqs, moltype, best_tree=False, params=None):\n    aln = align_unaligned_seqs(seqs, moltype=moltype, params=params)\n    tree = build_tree_from_alignment(aln, moltype, best_tree, params)\n    return {'Align':aln, 'Tree':tree}", "response": "Aligns sequences and builds a tree from the alignment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_tree_from_alignment(aln, moltype=DNA, best_tree=False, params=None):\n    # Create instance of app controller, enable tree, disable alignment\n    app = Muscle(InputHandler='_input_as_multiline_string', params=params, \\\n                   WorkingDir=tempfile.gettempdir())\n\n    app.Parameters['-clusteronly'].on()\n    app.Parameters['-tree1'].on(get_tmp_filename(app.WorkingDir))\n    app.Parameters['-seqtype'].on(moltype.label)\n\n    seq_collection = SequenceCollection(aln, MolType=moltype)\n\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n\n    # Collect result\n    result = app(int_map.toFasta())\n\n    # Build tree\n    tree = DndParser(result['Tree1Out'].read(), constructor=PhyloNode)\n\n    for tip in tree.tips():\n        tip.Name = int_keys[tip.Name]\n\n    # Clean up\n    result.cleanUp()\n    del(seq_collection, app, result)\n\n    return tree", "response": "Builds a tree from an alignment object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_seqs_to_alignment(seqs, aln, params=None):\n    if not params:\n        params = {}\n\n    #create SequenceCollection object from seqs\n    seqs_collection = SequenceCollection(seqs)\n    #Create mapping between abbreviated IDs and full IDs\n    seqs_int_map, seqs_int_keys = seqs_collection.getIntMap(prefix='seq_')\n    #Create SequenceCollection from int_map.\n    seqs_int_map = SequenceCollection(seqs_int_map)\n\n    #create SequenceCollection object from aln\n    aln_collection = SequenceCollection(aln)\n    #Create mapping between abbreviated IDs and full IDs\n    aln_int_map, aln_int_keys = aln_collection.getIntMap(prefix='aln_')\n    #Create SequenceCollection from int_map.\n    aln_int_map = SequenceCollection(aln_int_map)\n\n    #set output and profile options\n    params.update({'-out':get_tmp_filename(), '-profile':True})\n\n    #save seqs to tmp file\n    seqs_filename = get_tmp_filename()\n    seqs_out = open(seqs_filename,'w')\n    seqs_out.write(seqs_int_map.toFasta())\n    seqs_out.close()\n\n    #save aln to tmp file\n    aln_filename = get_tmp_filename()\n    aln_out = open(aln_filename, 'w')\n    aln_out.write(aln_int_map.toFasta())\n    aln_out.close()\n\n    #Create Muscle app and get results\n    app = Muscle(InputHandler='_input_as_multifile', params=params,\n                 WorkingDir=tempfile.gettempdir())\n    res = app((aln_filename, seqs_filename))\n\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['MuscleOut']))\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        if k in seqs_int_keys:\n            new_alignment[seqs_int_keys[k]] = v\n        else:\n            new_alignment[aln_int_keys[k]] = v\n\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment)\n\n    #Clean up\n    res.cleanUp()\n    del(seqs_collection, seqs_int_map, seqs_int_keys)\n    del(aln_collection, aln_int_map, aln_int_keys)\n    del(app, res, alignment, params)\n    remove(seqs_filename)\n    remove(aln_filename)\n\n    return new_alignment", "response": "Returns an Alignment object from seqs and existing Alignment object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef align_two_alignments(aln1, aln2, params=None):\n    if not params:\n        params = {}\n\n    #create SequenceCollection object from aln1\n    aln1_collection = SequenceCollection(aln1)\n    #Create mapping between abbreviated IDs and full IDs\n    aln1_int_map, aln1_int_keys = aln1_collection.getIntMap(prefix='aln1_')\n    #Create SequenceCollection from int_map.\n    aln1_int_map = SequenceCollection(aln1_int_map)\n\n    #create SequenceCollection object from aln2\n    aln2_collection = SequenceCollection(aln2)\n    #Create mapping between abbreviated IDs and full IDs\n    aln2_int_map, aln2_int_keys = aln2_collection.getIntMap(prefix='aln2_')\n    #Create SequenceCollection from int_map.\n    aln2_int_map = SequenceCollection(aln2_int_map)\n\n    #set output and profile options\n    params.update({'-out':get_tmp_filename(), '-profile':True})\n\n    #save aln1 to tmp file\n    aln1_filename = get_tmp_filename()\n    aln1_out = open(aln1_filename,'w')\n    aln1_out.write(aln1_int_map.toFasta())\n    aln1_out.close()\n\n    #save aln2 to tmp file\n    aln2_filename = get_tmp_filename()\n    aln2_out = open(aln2_filename, 'w')\n    aln2_out.write(aln2_int_map.toFasta())\n    aln2_out.close()\n\n    #Create Muscle app and get results\n    app = Muscle(InputHandler='_input_as_multifile', params=params,\n                 WorkingDir=tempfile.gettempdir())\n    res = app((aln1_filename, aln2_filename))\n\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['MuscleOut']))\n\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        if k in aln1_int_keys:\n            new_alignment[aln1_int_keys[k]] = v\n        else:\n            new_alignment[aln2_int_keys[k]] = v\n\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment)\n\n    #Clean up\n    res.cleanUp()\n    del(aln1_collection, aln1_int_map, aln1_int_keys)\n    del(aln2_collection, aln2_int_map, aln2_int_keys)\n    del(app, res, alignment, params)\n    remove(aln1_filename)\n    remove(aln2_filename)\n\n    return new_alignment", "response": "Returns an Alignment object from two existing Alignments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _input_as_multifile(self, data):\n        if data:\n            try:\n                filename1, filename2 = data\n            except:\n                raise ValueError, \"Expected two filenames\"\n\n            self.Parameters['-in'].off()\n            self.Parameters['-in1'].on(filename1)\n            self.Parameters['-in2'].on(filename2)\n        return ''", "response": "For use with the - profile option\n\n            This input handler expects data to be a tuple containing two filenames."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes an example for training and testing.", "response": "def get_example():\r\n    \"\"\"Make an example for training and testing.  Outputs a tuple\r\n    (label, features) where label is +1 if capital letters are the majority,\r\n    and -1 otherwise; and features is a list of letters.\r\n    \"\"\"\r\n    features = random.sample(string.ascii_letters, NUM_SAMPLES)\r\n    num_capitalized = len([ letter for letter in features if letter in string.ascii_uppercase ])\r\n    num_lowercase = len([ letter for letter in features if letter in string.ascii_lowercase ])\r\n    if num_capitalized > num_lowercase:\r\n        label = 1\r\n    else:\r\n        label = -1\r\n    return (label, features)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the static url for a file", "response": "def static_url(redis, path):\n    \"\"\"Gets the static path for a file\"\"\"\n    file_hash = get_cache_buster(redis, path)\n    return \"%s/%s?v=%s\" % (oz.settings[\"static_host\"], path, file_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the cache buster value for a given file path", "response": "def get_cache_buster(redis, path):\n    \"\"\"Gets the cache buster value for a given file path\"\"\"\n    return escape.to_unicode(redis.hget(\"cache-buster:{}:v3\".format(oz.settings[\"s3_bucket\"]), path))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the cache buster value for a given file path", "response": "def set_cache_buster(redis, path, hash):\n    \"\"\"Sets the cache buster value for a given file path\"\"\"\n    redis.hset(\"cache-buster:{}:v3\".format(oz.settings[\"s3_bucket\"]), path, hash)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bucket(s3_bucket=None, validate=False):\n    global S3Connection\n\n    if S3Connection != None:\n        settings = oz.settings\n        s3_bucket = s3_bucket or settings[\"s3_bucket\"]\n        opts = {}\n        if settings[\"s3_host\"]:\n            opts[\"host\"] = settings[\"s3_host\"]\n        if settings[\"aws_access_key\"] and settings[\"aws_secret_key\"]:\n            opts[\"aws_access_key_id\"] = settings[\"aws_access_key\"]\n            opts[\"aws_secret_access_key\"] = settings[\"aws_secret_key\"]\n        return S3Connection(**opts).get_bucket(s3_bucket, validate=validate)\n    else:\n        raise Exception(\"S3 not supported in this environment as boto is not installed\")", "response": "Gets a bucket from the specified settings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a file from the cache", "response": "def get_file(path, s3_bucket=None):\n    \"\"\"Gets a file\"\"\"\n\n    bucket_name = s3_bucket or oz.settings[\"s3_bucket\"]\n\n    if bucket_name:\n        bucket = get_bucket(bucket_name)\n        key = bucket.get_key(path)\n        if not key:\n            key = bucket.new_key(path)\n        return S3File(key)\n    else:\n        return LocalFile(oz.settings[\"static_path\"], path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies the current file to a new path.", "response": "def copy(self, new_path, replace=False):\n        \"\"\" Uses shutil to copy a file over \"\"\"\n        new_full_path = os.path.join(self.static_path, new_path)\n        if replace or not os.path.exists(new_full_path):\n            shutil.copy2(self.full_path, new_full_path)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, new_path, replace=False):\n        if replace or not get_file(new_path).exists():\n            self.key.copy(self.key.bucket, new_path)\n            return True\n        return False", "response": "Copies the file to the new path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a redis connection based on the redis settings", "response": "def create_connection():\n    \"\"\"Sets up a redis configuration\"\"\"\n\n    global _cached_connection\n    settings = oz.settings\n\n    if settings[\"redis_cache_connections\"] and _cached_connection != None:\n        return _cached_connection\n    else:\n        conn = redis.StrictRedis(\n            host=settings[\"redis_host\"],\n            port=settings[\"redis_port\"],\n            db=settings[\"redis_db\"],\n            password=settings[\"redis_password\"],\n            decode_responses=settings[\"redis_decode_responses\"],\n            ssl=settings[\"redis_use_ssl\"],\n            ssl_keyfile=settings[\"redis_ssl_keyfile\"],\n            ssl_certfile=settings[\"redis_ssl_certfile\"],\n            ssl_cert_reqs=settings[\"redis_ssl_cert_reqs\"],\n            ssl_ca_certs=settings[\"redis_ssl_ca_certs\"]\n        )\n\n        if settings[\"redis_cache_connections\"]:\n            _cached_connection = conn\n\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse out hit and miss sequences from usearch blast uc file and returns a dict of OTUs and unassigned sequences.", "response": "def clusters_from_blast_uc_file(uc_lines, otu_id_field=1):\n    \"\"\" Parses out hit/miss sequences from usearch blast uc file\n\n    All lines should be 'H'it or 'N'o hit.  Returns a dict of OTU ids: sequence\n    labels of the hits, and a list of all sequence labels that miss.\n\n    uc_lines = open file object of uc file\n\n    otu_id_field: uc field to use as the otu id. 1 is usearch's ClusterNr field,\n     and 9 is usearch's TargetLabel field\n\n    \"\"\"\n\n    hit_miss_index = 0\n    cluster_id_index = otu_id_field\n    seq_label_index = 8\n\n    otus = {}\n    unassigned_seqs = []\n\n    for line in uc_lines:\n        # skip empty, comment lines\n        if line.startswith('#') or len(line.strip()) == 0:\n            continue\n\n        curr_line = line.split('\\t')\n\n        if curr_line[hit_miss_index] == 'N':\n            # only retaining actual sequence label\n            unassigned_seqs.append(curr_line[seq_label_index].split()[0])\n\n        if curr_line[hit_miss_index] == 'H':\n\n            curr_seq_label = curr_line[seq_label_index].split()[0]\n            curr_otu_id = curr_line[cluster_id_index].split()[0]\n            # Append sequence label to dictionary, or create key\n            try:\n                otus[curr_otu_id].append(curr_seq_label)\n            except KeyError:\n                otus[curr_otu_id] = [curr_seq_label]\n\n    return otus, unassigned_seqs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate sorted fasta file from filepath.", "response": "def usearch_fasta_sort_from_filepath(\n        fasta_filepath,\n        output_filepath=None,\n        log_name=\"sortlen.log\",\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        working_dir=None):\n    \"\"\"Generates sorted fasta file via usearch --mergesort.\n\n    fasta_filepath: filepath to input fasta file\n    output_filepath: filepath for output sorted fasta file.\n    log_name: string to specify log filename\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\"\"\"\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='usearch_fasta_sort',\n                                     suffix='.fasta')\n\n    log_filepath = join(working_dir, log_name)\n\n    params = {}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    data = {'--mergesort': fasta_filepath,\n            '--output': output_filepath,\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    app_result = app(data)\n\n    return app_result, output_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef usearch_sort_by_abundance(\n        fasta_filepath,\n        output_filepath=None,\n        sizein=True,\n        sizeout=True,\n        minsize=0,\n        log_name=\"abundance_sort.log\",\n        usersort=False,\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        working_dir=None):\n    \"\"\" Sorts fasta file by abundance\n\n    fasta_filepath = input fasta file, generally a dereplicated fasta\n    output_filepath = output abundance sorted fasta filepath\n    sizein = not defined in usearch helpstring\n    sizeout = not defined in usearch helpstring\n    minsize = minimum size of cluster to retain.\n    log_name = string to specify log filename\n    usersort = Use if not sorting by abundance or usearch will raise an error\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\n    \"\"\"\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='usearch_abundance_sorted',\n                                     suffix='.fasta')\n\n    log_filepath = join(\n        working_dir,\n        \"minsize_\" + str(minsize) + \"_\" + log_name)\n\n    params = {}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    if usersort:\n        app.Parameters['--usersort'].on()\n\n    if minsize:\n        app.Parameters['--minsize'].on(minsize)\n\n    if sizein:\n        app.Parameters['--sizein'].on()\n\n    if sizeout:\n        app.Parameters['--sizeout'].on()\n\n    data = {'--sortsize': fasta_filepath,\n            '--output': output_filepath\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    # Can have no data following this filter step, which will raise an\n    # application error, try to catch it here to raise meaningful message.\n\n    try:\n        app_result = app(data)\n    except ApplicationError:\n        raise ValueError('No data following filter steps, please check ' +\n                         'parameter settings for usearch_qf.')\n\n    return app_result, output_filepath", "response": "Sort fasta file by abundance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclusters for err. correction at percent_id_err output consensus fasta_filepath output_uc_filepath output_err_corrected.", "response": "def usearch_cluster_error_correction(\n        fasta_filepath,\n        output_filepath=None,\n        output_uc_filepath=None,\n        percent_id_err=0.97,\n        sizein=True,\n        sizeout=True,\n        w=64,\n        slots=16769023,\n        maxrejects=64,\n        log_name=\"usearch_cluster_err_corrected.log\",\n        usersort=False,\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        working_dir=None):\n    \"\"\" Cluster for err. correction at percent_id_err, output consensus fasta\n\n    fasta_filepath = input fasta file, generally a dereplicated fasta\n    output_filepath = output error corrected fasta filepath\n    percent_id_err = minimum identity percent.\n    sizein = not defined in usearch helpstring\n    sizeout = not defined in usearch helpstring\n    w = Word length for U-sorting\n    slots = Size of compressed index table. Should be prime, e.g. 40000003.\n     Should also specify --w, typical is --w 16 or --w 32.\n    maxrejects = Max rejected targets, 0=ignore, default 32.\n    log_name = string specifying output log name\n    usersort = Enable if input fasta not sorted by length purposefully, lest\n     usearch will raise an error.\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\n    \"\"\"\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='usearch_cluster_err_corrected',\n                                     suffix='.fasta')\n\n    log_filepath = join(working_dir, log_name)\n\n    params = {'--sizein': sizein,\n              '--sizeout': sizeout,\n              '--id': percent_id_err,\n              '--w': w,\n              '--slots': slots,\n              '--maxrejects': maxrejects}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    if usersort:\n        app.Parameters['--usersort'].on()\n\n    data = {'--cluster': fasta_filepath,\n            '--consout': output_filepath\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    if output_uc_filepath:\n        data['--uc'] = output_uc_filepath\n\n    app_result = app(data)\n\n    return app_result, output_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef usearch_chimera_filter_de_novo(\n        fasta_filepath,\n        output_chimera_filepath=None,\n        output_non_chimera_filepath=None,\n        abundance_skew=2.0,\n        log_name=\"uchime_de_novo_chimera_filtering.log\",\n        usersort=False,\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        working_dir=None):\n    \"\"\" Chimera filter de novo, output chimeras and non-chimeras to fastas\n\n    fasta_filepath = input fasta file, generally a dereplicated fasta\n    output_chimera_filepath = output chimera filepath\n    output_non_chimera_filepath = output non chimera filepath\n    abundance_skew = abundance skew setting for de novo filtering.\n    usersort = Enable if input fasta not sorted by length purposefully, lest\n     usearch will raise an error.\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\n    \"\"\"\n    if not output_chimera_filepath:\n        _, output_chimera_filepath = mkstemp(prefix='uchime_chimeras_',\n                                             suffix='.fasta')\n\n    if not output_non_chimera_filepath:\n        _, output_non_chimera_filepath = mkstemp(prefix='uchime_non_chimeras_',\n                                                 suffix='.fasta')\n\n    log_filepath = join(working_dir, log_name)\n\n    params = {'--abskew': abundance_skew}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    if usersort:\n        app.Parameters['--usersort'].on()\n\n    data = {'--uchime': fasta_filepath,\n            '--chimeras': output_chimera_filepath,\n            '--nonchimeras': output_non_chimera_filepath\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    app_result = app(data)\n\n    if not save_intermediate_files:\n        remove_files([output_chimera_filepath])\n\n    return app_result, output_non_chimera_filepath", "response": "Chimera filter de novo."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncluster sequences at percent_id and store them in a file.", "response": "def usearch_cluster_seqs_ref(\n        fasta_filepath,\n        output_filepath=None,\n        percent_id=0.97,\n        sizein=True,\n        sizeout=True,\n        w=64,\n        slots=16769023,\n        maxrejects=64,\n        log_name=\"usearch_cluster_seqs.log\",\n        usersort=True,\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        suppress_new_clusters=False,\n        refseqs_fp=None,\n        output_dir=None,\n        working_dir=None,\n        rev=False):\n    \"\"\" Cluster seqs at percent_id, output consensus fasta\n\n    Also appends de novo clustered seqs if suppress_new_clusters is False.\n    Forced to handle reference + de novo in hackish fashion as usearch does not\n    work as listed in the helpstrings.  Any failures are clustered de novo,\n    and given unique cluster IDs.\n\n    fasta_filepath = input fasta file, generally a dereplicated fasta\n    output_filepath = output reference clustered uc filepath\n    percent_id = minimum identity percent.\n    sizein = not defined in usearch helpstring\n    sizeout = not defined in usearch helpstring\n    w = Word length for U-sorting\n    slots = Size of compressed index table. Should be prime, e.g. 40000003.\n     Should also specify --w, typical is --w 16 or --w 32.\n    maxrejects = Max rejected targets, 0=ignore, default 32.\n    log_name = string specifying output log name\n    usersort = Enable if input fasta not sorted by length purposefully, lest\n     usearch will raise an error.  In post chimera checked sequences, the seqs\n     are sorted by abundance, so this should be set to True.\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\n    suppress_new_clusters: Disables de novo OTUs when ref based OTU picking\n     enabled.\n    refseqs_fp: Filepath for ref based OTU picking\n    output_dir: output directory\n    rev = search plus and minus strands of sequences\n    \"\"\"\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='usearch_cluster_ref_based',\n                                     suffix='.uc')\n\n    log_filepath = join(working_dir, log_name)\n\n    uc_filepath = join(working_dir, \"clustered_seqs_post_chimera.uc\")\n\n    params = {'--sizein': sizein,\n              '--sizeout': sizeout,\n              '--id': percent_id,\n              '--w': w,\n              '--slots': slots,\n              '--maxrejects': maxrejects}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    if usersort:\n        app.Parameters['--usersort'].on()\n    if rev:\n        app.Parameters['--rev'].on()\n\n    data = {'--query': fasta_filepath,\n            '--uc': uc_filepath,\n            '--db': refseqs_fp\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    app_result = app(data)\n\n    files_to_remove = []\n\n    # Need to create fasta file of all hits (with reference IDs),\n    # recluster failures if new clusters allowed, and create complete fasta\n    # file, with unique fasta label IDs.\n\n    if suppress_new_clusters:\n        output_fna_filepath = join(output_dir, 'ref_clustered_seqs.fasta')\n        output_filepath, labels_hits = get_fasta_from_uc_file(fasta_filepath,\n                                                              uc_filepath, hit_type=\"H\", output_dir=output_dir,\n                                                              output_fna_filepath=output_fna_filepath)\n\n        files_to_remove.append(uc_filepath)\n    else:\n        # Get fasta of successful ref based clusters\n        output_fna_clustered = join(output_dir, 'ref_clustered_seqs.fasta')\n        output_filepath_ref_clusters,  labels_hits =\\\n            get_fasta_from_uc_file(fasta_filepath, uc_filepath, hit_type=\"H\",\n                                   output_dir=output_dir, output_fna_filepath=output_fna_clustered)\n\n        # get failures and recluster\n        output_fna_failures =\\\n            join(output_dir, 'ref_clustered_seqs_failures.fasta')\n        output_filepath_failures, labels_hits =\\\n            get_fasta_from_uc_file(fasta_filepath,\n                                   uc_filepath, hit_type=\"N\", output_dir=output_dir,\n                                   output_fna_filepath=output_fna_failures)\n\n        # de novo cluster the failures\n        app_result, output_filepath_clustered_failures =\\\n            usearch_cluster_seqs(output_fna_failures, output_filepath=\n                                 join(\n                                     output_dir,\n                                     'clustered_seqs_reference_failures.fasta'),\n                                 percent_id=percent_id, sizein=sizein, sizeout=sizeout, w=w,\n                                 slots=slots, maxrejects=maxrejects,\n                                 save_intermediate_files=save_intermediate_files,\n                                 remove_usearch_logs=remove_usearch_logs, working_dir=working_dir)\n\n        output_filepath = concatenate_fastas(output_fna_clustered,\n                                             output_fna_failures, output_concat_filepath=join(\n                                                 output_dir,\n                                                 'concatenated_reference_denovo_clusters.fasta'))\n\n        files_to_remove.append(output_fna_clustered)\n        files_to_remove.append(output_fna_failures)\n        files_to_remove.append(output_filepath_clustered_failures)\n\n    if not save_intermediate_files:\n        remove_files(files_to_remove)\n\n    return app_result, output_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconcatenating two input fastas writes to output_concat_filepath", "response": "def concatenate_fastas(output_fna_clustered,\n                       output_fna_failures,\n                       output_concat_filepath):\n    \"\"\" Concatenates two input fastas, writes to output_concat_filepath\n\n    output_fna_clustered: fasta of successful ref clusters\n    output_fna_failures: de novo fasta of cluster failures\n    output_concat_filepath: path to write combined fastas to\n    \"\"\"\n\n    output_fp = open(output_concat_filepath, \"w\")\n\n    for label, seq in parse_fasta(open(output_fna_clustered, \"U\")):\n        output_fp.write(\">%s\\n%s\\n\" % (label, seq))\n    for label, seq in parse_fasta(open(output_fna_failures, \"U\")):\n        output_fp.write(\">%s\\n%s\\n\" % (label, seq))\n\n    return output_concat_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenumerates OTUs in FASTA file.", "response": "def enumerate_otus(fasta_filepath,\n                   output_filepath=None,\n                   label_prefix=\"\",\n                   label_suffix=\"\",\n                   retain_label_as_comment=False,\n                   count_start=0):\n    \"\"\" Writes unique, sequential count to OTUs\n\n    fasta_filepath = input fasta filepath\n    output_filepath = output fasta filepath\n    label_prefix = string to place before enumeration\n    label_suffix = string to place after enumeration\n    retain_label_as_comment = if True, will place existing label in sequence\n     comment, after a tab\n    count_start = number to start enumerating OTUs with\n\n    \"\"\"\n\n    fasta_i = open(fasta_filepath, \"U\")\n\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='enumerated_seqs_',\n                                     suffix='.fasta')\n\n    fasta_o = open(output_filepath, \"w\")\n\n    for label, seq in parse_fasta(fasta_i):\n        curr_label = \">\" + label_prefix + str(count_start) + label_suffix\n        if retain_label_as_comment:\n            curr_label += '\\t' + label\n        fasta_o.write(curr_label.strip() + '\\n')\n        fasta_o.write(seq.strip() + '\\n')\n        count_start += 1\n\n    return output_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread fasta of sequences from. uc file of type hit_type", "response": "def get_fasta_from_uc_file(fasta_filepath,\n                           uc_filepath,\n                           hit_type=\"H\",\n                           output_fna_filepath=None,\n                           label_prefix=\"\",\n                           output_dir=None):\n    \"\"\" writes fasta of sequences from uc file of type hit_type\n\n    fasta_filepath:  Filepath of original query fasta file\n    uc_filepath:  Filepath of .uc file created by usearch post error filtering\n    hit_type: type to read from first field of .uc file, \"H\" for hits, \"N\" for\n     no hits.\n    output_fna_filepath = fasta output filepath\n    label_prefix = Added before each fasta label, important when doing ref\n     based OTU picking plus de novo clustering to preserve label matching.\n    output_dir: output directory\n    \"\"\"\n\n    hit_type_index = 0\n    seq_label_index = 8\n    target_label_index = 9\n\n    labels_hits = {}\n    labels_to_keep = []\n\n    for line in open(uc_filepath, \"U\"):\n        if line.startswith(\"#\") or len(line.strip()) == 0:\n            continue\n        curr_line = line.split('\\t')\n        if curr_line[0] == hit_type:\n            labels_hits[curr_line[seq_label_index]] =\\\n                curr_line[target_label_index].strip()\n            labels_to_keep.append(curr_line[seq_label_index])\n\n    labels_to_keep = set(labels_to_keep)\n\n    out_fna = open(output_fna_filepath, \"w\")\n\n    for label, seq in parse_fasta(open(fasta_filepath, \"U\")):\n        if label in labels_to_keep:\n            if hit_type == \"H\":\n                out_fna.write(\">\" + labels_hits[label] + \"\\n%s\\n\" % seq)\n            if hit_type == \"N\":\n                out_fna.write(\">\" + label + \"\\n%s\\n\" % seq)\n\n    return output_fna_filepath, labels_hits"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets union or intersection of two supplied fasta files and outputs a combined file with the set of non - chimeras.", "response": "def get_retained_chimeras(output_fp_de_novo_nonchimeras,\n                          output_fp_ref_nonchimeras,\n                          output_combined_fp,\n                          chimeras_retention='union'):\n    \"\"\" Gets union or intersection of two supplied fasta files\n\n    output_fp_de_novo_nonchimeras: filepath of nonchimeras from de novo\n     usearch detection.\n    output_fp_ref_nonchimeras: filepath of nonchimeras from reference based\n     usearch detection.\n    output_combined_fp: filepath to write retained sequences to.\n    chimeras_retention: accepts either 'intersection' or 'union'.  Will test\n     for chimeras against the full input error clustered sequence set, and\n     retain sequences flagged as non-chimeras by either (union) or\n     only those flagged as non-chimeras by both (intersection).\"\"\"\n\n    de_novo_non_chimeras = []\n    reference_non_chimeras = []\n\n    de_novo_nonchimeras_f = open(output_fp_de_novo_nonchimeras, \"U\")\n    reference_nonchimeras_f = open(output_fp_ref_nonchimeras, \"U\")\n\n    output_combined_f = open(output_combined_fp, \"w\")\n\n    for label, seq in parse_fasta(de_novo_nonchimeras_f):\n        de_novo_non_chimeras.append(label)\n    de_novo_nonchimeras_f.close()\n    for label, seq in parse_fasta(reference_nonchimeras_f):\n        reference_non_chimeras.append(label)\n    reference_nonchimeras_f.close()\n\n    de_novo_non_chimeras = set(de_novo_non_chimeras)\n    reference_non_chimeras = set(reference_non_chimeras)\n\n    if chimeras_retention == 'union':\n        all_non_chimeras = de_novo_non_chimeras.union(reference_non_chimeras)\n    elif chimeras_retention == 'intersection':\n        all_non_chimeras =\\\n            de_novo_non_chimeras.intersection(reference_non_chimeras)\n\n    de_novo_nonchimeras_f = open(output_fp_de_novo_nonchimeras, \"U\")\n    reference_nonchimeras_f = open(output_fp_ref_nonchimeras, \"U\")\n\n    # Save a list of already-written labels\n    labels_written = []\n\n    for label, seq in parse_fasta(de_novo_nonchimeras_f):\n        if label in all_non_chimeras:\n            if label not in labels_written:\n                output_combined_f.write('>%s\\n%s\\n' % (label, seq))\n                labels_written.append(label)\n    de_novo_nonchimeras_f.close()\n    for label, seq in parse_fasta(reference_nonchimeras_f):\n        if label in all_non_chimeras:\n            if label not in labels_written:\n                output_combined_f.write('>%s\\n%s\\n' % (label, seq))\n                labels_written.append(label)\n    reference_nonchimeras_f.close()\n\n    output_combined_f.close()\n\n    return output_combined_fp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign reads to the next cluster in the ONTU.", "response": "def assign_reads_to_otus(original_fasta,\n                         filtered_fasta,\n                         output_filepath=None,\n                         log_name=\"assign_reads_to_otus.log\",\n                         perc_id_blast=0.97,\n                         global_alignment=True,\n                         HALT_EXEC=False,\n                         save_intermediate_files=False,\n                         remove_usearch_logs=False,\n                         working_dir=None):\n    \"\"\" Uses original fasta file, blasts to assign reads to filtered fasta\n\n    original_fasta = filepath to original query fasta\n    filtered_fasta = filepath to enumerated, filtered fasta\n    output_filepath = output path to clusters (uc) file\n    log_name = string specifying output log name\n    perc_id_blast = percent ID for blasting original seqs against filtered set\n    usersort = Enable if input fasta not sorted by length purposefully, lest\n     usearch will raise an error.  In post chimera checked sequences, the seqs\n     are sorted by abundance, so this should be set to True.\n    HALT_EXEC: Used for debugging app controller\n    save_intermediate_files: Preserve all intermediate files created.\n    \"\"\"\n\n    # Not sure if I feel confortable using blast as a way to recapitulate\n    # original read ids....\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='assign_reads_to_otus',\n                                     suffix='.uc')\n\n    log_filepath = join(working_dir, log_name)\n\n    params = {'--id': perc_id_blast,\n              '--global': global_alignment}\n\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n\n    data = {'--query': original_fasta,\n            '--db': filtered_fasta,\n            '--uc': output_filepath\n            }\n\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n\n    app_result = app(data)\n\n    return app_result, output_filepath"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usearch_qf(\n    fasta_filepath,\n    refseqs_fp=None,\n    output_dir=None,\n    percent_id=0.97,\n    percent_id_err=0.97,\n    minsize=4,\n    abundance_skew=2.0,\n    db_filepath=None,\n    rev=False,\n    label_prefix=\"\",\n    label_suffix=\"\",\n    retain_label_as_comment=False,\n    count_start=0,\n    perc_id_blast=0.97,\n    save_intermediate_files=False,\n    HALT_EXEC=False,\n    global_alignment=True,\n    sizein=True,\n    sizeout=True,\n    w=64,\n    slots=16769023,\n    maxrejects=64,\n    minlen=64,\n    de_novo_chimera_detection=True,\n    derep_fullseq=False,\n    reference_chimera_detection=True,\n    cluster_size_filtering=True,\n    remove_usearch_logs=False,\n    usersort=True,\n    suppress_new_clusters=False,\n    chimeras_retention=\"union\",\n    verbose=False\n):\n    \"\"\" Main convenience wrapper for using usearch to filter/cluster seqs\n\n    The complete 'usearch_qf' process is a multistep process with many calls\n    to usearch with various parameters.  It is likely to change from the\n    original implementation.  A lot.\n\n    fasta_filepath = fasta filepath to filtering/clustering (e.g., output\n     seqs.fna file from split_libraries.py)\n    refseqs_fp = fasta filepath for ref-based otu picking.\n    output_dir = directory to store the otu mapping file, as well logs and\n     the intermediate files created if save_intermediate_files is True.\n    percent_ID = percent ID for clustering sequences.\n    percent_ID_err = percent ID for filtering out chimeras\n    minsize = Minimum size of cluster for retention after chimera removal.\n    abundance_skew = threshold setting for chimera removal with de novo\n     chimera detection.\n    db_filepath = filepath of reference fasta sequence set for ref based\n     chimera detection.\n    rev = search plus and minus strands of sequences, used in ref based chimera\n     detection.\n    label_prefix = optional prefix added to filtered fasta file.\n    label_suffix = optional suffix added to filtered fasta file.\n    retain_label_as_comment = option to add usearch generated label to\n     enumerated fasta labels.\n    count_start = integer to begin counting at for sequence enumeration.\n    perc_id_blast = percent identity setting for using blast algorithm to\n     assign original sequence labels to filtered fasta.\n    global_alignment = Setting for assignment of original seq labels to filtered\n     seqs.\n    sizein = not defined in usearch helpstring\n    sizeout = not defined in usearch helpstring\n    w = Word length for U-sorting\n    slots = Size of compressed index table. Should be prime, e.g. 40000003.\n     Should also specify --w, typical is --w 16 or --w 32.\n    maxrejects = Max rejected targets, 0=ignore, default 32.\n    save_intermediate_files = retain all the intermediate files created during\n     this process.\n    minlen = (not specified in usearch helpstring), but seems like a good bet\n     that this refers to the minimum length of the sequences for dereplication.\n    HALT_EXEC = used to debug app controller problems.\n    de_novo_chimera_detection = If True, will detect chimeras de novo\n    reference_chimera_detection = If True, will detect chimeras ref based\n    cluster_size_filtering = If True, will filter OTUs according to seq counts.\n    remove_usearch_logs = If True, will not call the --log function for each\n     usearch call.\n    usersort = Used for specifying custom sorting (i.e., non-length based\n     sorting) with usearch/uclust.\n    suppress_new_clusters = with reference based OTU picking, if enabled,\n     will prevent new clusters that do not match the reference from being\n     clustered.\n    chimeras_retention = accepts either 'intersection' or 'union'.  Will test\n     for chimeras against the full input error clustered sequence set, and\n     retain sequences flagged as non-chimeras by either (union) or\n     only those flagged as non-chimeras by both (intersection).\n    \"\"\"\n\n    # Save a list of intermediate filepaths in case they are to be removed.\n    intermediate_files = []\n\n    # Need absolute paths to avoid problems with app controller\n    if output_dir:\n        output_dir = abspath(output_dir) + '/'\n\n    fasta_filepath = abspath(fasta_filepath)\n\n    try:\n\n        if verbose:\n            print \"Sorting sequences by length...\"\n        # Sort seqs by length\n        app_result, output_filepath_len_sorted =\\\n            usearch_fasta_sort_from_filepath(fasta_filepath, output_filepath=\n                                             join(\n                                                 output_dir,\n                                                 'len_sorted.fasta'),\n                                             save_intermediate_files=save_intermediate_files,\n                                             remove_usearch_logs=remove_usearch_logs,\n                                             working_dir=output_dir, HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(output_filepath_len_sorted)\n\n        if verbose:\n            print \"Dereplicating sequences...\"\n        # Dereplicate sequences\n        app_result, output_filepath_dereplicated =\\\n            usearch_dereplicate_exact_subseqs(output_filepath_len_sorted,\n                                              output_filepath=join(\n                                                  output_dir,\n                                                  'dereplicated_seqs.fasta'),\n                                              minlen=minlen, w=w, slots=slots, sizeout=sizeout,\n                                              maxrejects=maxrejects, save_intermediate_files=save_intermediate_files,\n                                              remove_usearch_logs=remove_usearch_logs,\n                                              working_dir=output_dir, HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(output_filepath_dereplicated)\n\n        if verbose:\n            print \"Sorting by abundance...\"\n        # Sort by abundance, initially no filter based on seqs/otu\n        app_result, output_fp =\\\n            usearch_sort_by_abundance(output_filepath_dereplicated,\n                                      output_filepath=join(\n                                          output_dir,\n                                          'abundance_sorted.fasta'),\n                                      usersort=True, sizein=sizein, sizeout=sizeout, minsize=0,\n                                      remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                      HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(output_fp)\n\n        if verbose:\n            print \"Clustering sequences for error correction...\"\n\n        # Create .uc file of clusters file, to identify original sequences\n        # later\n        output_uc_filepath = output_dir + 'err_corrected_clusters.uc'\n\n        app_result, error_clustered_output_fp =\\\n            usearch_cluster_error_correction(output_fp,\n                                             output_filepath=join(output_dir,\n                                                                  'clustered_error_corrected.fasta'),\n                                             output_uc_filepath=output_uc_filepath,\n                                             usersort=True, percent_id_err=percent_id_err, sizein=sizein,\n                                             sizeout=sizeout, w=w, slots=slots, maxrejects=maxrejects,\n                                             remove_usearch_logs=remove_usearch_logs,\n                                             save_intermediate_files=save_intermediate_files,\n                                             working_dir=output_dir, HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(error_clustered_output_fp)\n        intermediate_files.append(output_uc_filepath)\n\n        # Series of conditional tests, using generic 'output_fp' name so the\n        # conditional filtering, if any/all are selected, do not matter.\n        if de_novo_chimera_detection:\n\n            if verbose:\n                print \"Performing de novo chimera detection...\"\n            app_result, output_fp_de_novo_nonchimeras =\\\n                usearch_chimera_filter_de_novo(error_clustered_output_fp,\n                                               abundance_skew=abundance_skew, output_chimera_filepath=\n                                               join(\n                                                   output_dir,\n                                                   'de_novo_chimeras.fasta'),\n                                               output_non_chimera_filepath=join(\n                                                   output_dir,\n                                                   'de_novo_non_chimeras.fasta'), usersort=True,\n                                               save_intermediate_files=save_intermediate_files,\n                                               remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                               HALT_EXEC=HALT_EXEC)\n\n            intermediate_files.append(output_fp_de_novo_nonchimeras)\n\n            output_fp = output_fp_de_novo_nonchimeras\n\n        if reference_chimera_detection:\n            if verbose:\n                print \"Performing reference based chimera detection...\"\n\n            app_result, output_fp_ref_nonchimeras =\\\n                usearch_chimera_filter_ref_based(error_clustered_output_fp,\n                                                 db_filepath=db_filepath, output_chimera_filepath=\n                                                 join(\n                                                     output_dir,\n                                                     'reference_chimeras.fasta'),\n                                                 output_non_chimera_filepath=\n                                                 join(output_dir, 'reference_non_chimeras.fasta'), usersort=True,\n                                                 save_intermediate_files=save_intermediate_files, rev=rev,\n                                                 remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                                 HALT_EXEC=HALT_EXEC)\n\n            intermediate_files.append(output_fp_ref_nonchimeras)\n\n            output_fp = output_fp_ref_nonchimeras\n\n        # get intersection or union if both ref and de novo chimera detection\n        if de_novo_chimera_detection and reference_chimera_detection:\n            if verbose:\n                print \"Finding %s of non-chimeras...\" % chimeras_retention\n            output_fp = get_retained_chimeras(\n                output_fp_de_novo_nonchimeras, output_fp_ref_nonchimeras,\n                output_combined_fp=\n                join(output_dir, 'combined_non_chimeras.fasta'),\n                chimeras_retention=chimeras_retention)\n\n            intermediate_files.append(output_fp)\n\n        if cluster_size_filtering:\n            # Test for empty filepath following filters, raise error if all seqs\n            # have been removed\n            if verbose:\n                print \"Filtering by cluster size...\"\n            # chimera detection was not performed, use output file of step 4 as input\n            # to filtering by cluster size\n            if not (reference_chimera_detection and de_novo_chimera_detection):\n                output_fp = error_clustered_output_fp\n            app_result, output_fp =\\\n                usearch_sort_by_abundance(output_fp, output_filepath=\n                                          join(output_dir, 'abundance_sorted_minsize_' + str(minsize) +\n                                               '.fasta'),\n                                          minsize=minsize, sizein=sizein, sizeout=sizeout,\n                                          remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                          HALT_EXEC=HALT_EXEC)\n\n            intermediate_files.append(output_fp)\n\n        # cluster seqs\n        # Should we add in option to use alternative OTU picking here?\n        # Seems like it will be a bit of a mess...maybe after we determine\n        # if usearch_qf should become standard.\n        if refseqs_fp:\n            if verbose:\n                print \"Clustering against reference sequences...\"\n            app_result, output_filepath =\\\n                usearch_cluster_seqs_ref(output_fp, output_filepath=\n                                         join(\n                                             output_dir,\n                                             'ref_clustered_seqs.uc'),\n                                         percent_id=percent_id, sizein=sizein,\n                                         sizeout=sizeout, w=w, slots=slots, maxrejects=maxrejects,\n                                         save_intermediate_files=save_intermediate_files,\n                                         remove_usearch_logs=remove_usearch_logs,\n                                         suppress_new_clusters=suppress_new_clusters, refseqs_fp=refseqs_fp,\n                                         output_dir=output_dir, working_dir=output_dir, rev=rev,\n                                         HALT_EXEC=HALT_EXEC\n                                         )\n\n        else:\n            if verbose:\n                print \"De novo clustering sequences...\"\n            app_result, output_filepath =\\\n                usearch_cluster_seqs(output_fp, output_filepath=\n                                     join(output_dir, 'clustered_seqs.fasta'),\n                                     percent_id=percent_id, sizein=sizein,\n                                     sizeout=sizeout, w=w, slots=slots, maxrejects=maxrejects,\n                                     save_intermediate_files=save_intermediate_files,\n                                     remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                     HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(output_filepath)\n\n        # Enumerate the OTUs in the clusters\n        if not suppress_new_clusters:\n            if verbose:\n                print \"Enumerating OTUs...\"\n            output_filepath =\\\n                enumerate_otus(output_filepath, output_filepath=\n                               join(output_dir, 'enumerated_otus.fasta'),\n                               label_prefix=label_prefix,\n                               label_suffix=label_suffix, count_start=count_start,\n                               retain_label_as_comment=retain_label_as_comment)\n\n            intermediate_files.append(output_filepath)\n\n        # Get original sequence label identities\n        if verbose:\n            print \"Assigning sequences to clusters...\"\n        app_result, clusters_file = assign_reads_to_otus(fasta_filepath,\n                                                         filtered_fasta=output_filepath, output_filepath=join(\n                                                             output_dir,\n                                                             'assign_reads_to_otus.uc'), perc_id_blast=percent_id,\n                                                         global_alignment=global_alignment,\n                                                         remove_usearch_logs=remove_usearch_logs, working_dir=output_dir,\n                                                         HALT_EXEC=HALT_EXEC)\n\n        intermediate_files.append(clusters_file)\n\n    except ApplicationError:\n        raise ApplicationError('Error running usearch. Possible causes are '\n                               'unsupported version (current supported version is usearch ' +\n                               'v5.2.236) is installed or improperly formatted input file was ' +\n                               'provided')\n    except ApplicationNotFoundError:\n        remove_files(files_to_remove)\n        raise ApplicationNotFoundError('usearch not found, is it properly ' +\n                                       'installed?')\n\n    # Get dict of clusters, list of failures\n    # Set OTU ID field to 9 for the case of closed reference OTU picking\n    if suppress_new_clusters:\n        otu_id_field = 9\n    else:\n        otu_id_field = 1\n    clusters, failures = clusters_from_blast_uc_file(open(clusters_file, \"U\"),\n                                                     otu_id_field)\n\n    # Remove temp files unless user specifies output filepath\n    if not save_intermediate_files:\n        remove_files(intermediate_files)\n\n    return clusters, failures", "response": "This function is a multistep process that uses arch_qf to filter and cluster sequences."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usearch61_ref_cluster(seq_path,\n                          refseqs_fp,\n                          percent_id=0.97,\n                          rev=False,\n                          save_intermediate_files=True,\n                          minlen=64,\n                          output_dir='.',\n                          remove_usearch_logs=False,\n                          verbose=False,\n                          wordlength=8,\n                          usearch_fast_cluster=False,\n                          usearch61_sort_method='abundance',\n                          otu_prefix=\"denovo\",\n                          usearch61_maxrejects=32,\n                          usearch61_maxaccepts=1,\n                          sizeorder=False,\n                          suppress_new_clusters=False,\n                          threads=1.0,\n                          HALT_EXEC=False\n                          ):\n    \"\"\" Returns dictionary of cluster IDs:seq IDs\n\n    Overall function for reference-based clustering with usearch61\n\n    seq_path:  fasta filepath to be clustered with usearch61\n    refseqs_fp: reference fasta filepath, used to cluster sequences against.\n    percent_id:  percentage id to cluster at\n    rev: enable reverse strand matching for clustering\n    save_intermediate_files: Saves intermediate files created during clustering\n    minlen: minimum sequence length\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    remove_usearch_logs: Saves usearch log files\n    verbose: print current processing step to stdout\n    wordlength: word length to use for clustering\n    usearch_fast_cluster: Use usearch61 fast cluster option, not as memory\n     efficient as the default cluster_smallmem option, requires sorting by\n     length, and does not allow reverse strand matching.\n    usearch61_sort_method:  Sort sequences by abundance or length by using\n     functionality provided by usearch61, or do not sort by using None option.\n    otu_prefix: label to place in front of OTU IDs, used to prevent duplicate\n     IDs from appearing with reference based OTU picking.\n    usearch61_maxrejects: Number of rejects allowed by usearch61\n    usearch61_maxaccepts: Number of accepts allowed by usearch61\n    sizeorder: used for clustering based upon abundance of seeds (only applies\n     when doing open reference de novo clustering)\n    suppress_new_clusters: If True, will allow de novo clustering on top of\n     reference clusters.\n    threads: Specify number of threads used per core per CPU\n    HALT_EXEC: application controller option to halt execution.\n\n    Description of analysis workflows\n    ---------------------------------\n    closed-reference approach:\n      dereplicate sequences first, do reference based clustering,\n      merge clusters/failures and dereplicated data,\n      write OTU mapping and failures file.\n\n    open-reference approach:\n      dereplicate sequences first, do reference based clustering, parse failures,\n      sort failures fasta according to chosen method, cluster failures, merge\n      reference clustering results/de novo results/dereplicated data, write\n      OTU mapping file.\n\n    Dereplication should save processing time for large datasets.\n\n    \"\"\"\n\n    files_to_remove = []\n\n    # Need absolute paths to avoid potential problems with app controller\n    if output_dir:\n        output_dir = join(abspath(output_dir), '')\n\n    seq_path = abspath(seq_path)\n\n    try:\n\n        if verbose:\n            print \"Presorting sequences according to abundance...\"\n        intermediate_fasta, dereplicated_uc, app_result =\\\n            sort_by_abundance_usearch61(seq_path, output_dir, rev,\n                                        minlen, remove_usearch_logs, HALT_EXEC,\n                                        output_fna_filepath=join(\n                                            output_dir,\n                                            'abundance_sorted.fna'),\n                                        output_uc_filepath=join(\n                                            output_dir,\n                                            'abundance_sorted.uc'),\n                                        threads=threads)\n        if not save_intermediate_files:\n            files_to_remove.append(intermediate_fasta)\n            files_to_remove.append(dereplicated_uc)\n\n        if verbose:\n            print \"Performing reference based clustering...\"\n        clusters_fp, app_result = usearch61_cluster_ref(intermediate_fasta,\n                                                        refseqs_fp, percent_id, rev, minlen, output_dir,\n                                                        remove_usearch_logs, wordlength, usearch61_maxrejects,\n                                                        usearch61_maxaccepts, HALT_EXEC,\n                                                        output_uc_filepath=join(\n                                                            output_dir,\n                                                            'ref_clustered.uc'),\n                                                        threads=threads)\n        if not save_intermediate_files:\n            files_to_remove.append(clusters_fp)\n\n        clusters, failures =\\\n            parse_usearch61_clusters(open(clusters_fp, \"U\"), otu_prefix=\"\",\n                                     ref_clustered=True)\n        dereplicated_clusters =\\\n            parse_dereplicated_uc(open(dereplicated_uc, \"U\"))\n        clusters = merge_clusters_dereplicated_seqs(clusters,\n                                                    dereplicated_clusters)\n        failures = merge_failures_dereplicated_seqs(failures,\n                                                    dereplicated_clusters)\n\n        if not suppress_new_clusters and failures:\n            if verbose:\n                print \"Parsing out sequences that failed to cluster...\"\n            failures_fasta = parse_usearch61_failures(seq_path, set(failures),\n                                                      output_fasta_fp=join(output_dir, \"failures_parsed.fna\"))\n            if not save_intermediate_files:\n                files_to_remove.append(failures_fasta)\n            denovo_clusters = usearch61_denovo_cluster(failures_fasta,\n                                                       percent_id, rev, save_intermediate_files, minlen, output_dir,\n                                                       remove_usearch_logs, verbose, wordlength, usearch_fast_cluster,\n                                                       usearch61_sort_method, otu_prefix, usearch61_maxrejects,\n                                                       usearch61_maxaccepts, sizeorder, threads, HALT_EXEC)\n            failures = []\n\n            # Merge ref and denovo clusters\n            clusters.update(denovo_clusters)\n\n    except ApplicationError:\n        raise ApplicationError('Error running usearch61. Possible causes are '\n                               'unsupported version (current supported version is usearch '\n                               'v6.1.544) is installed or improperly formatted input file was '\n                               'provided')\n\n    except ApplicationNotFoundError:\n        remove_files(files_to_remove)\n        raise ApplicationNotFoundError('usearch61 not found, is it properly '\n                                       'installed?')\n\n    if not save_intermediate_files:\n        remove_files(files_to_remove)\n\n    return clusters, failures", "response": "This function is used to cluster sequences against a reference fasta file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning dictionary of cluster IDs:seq IDs Overall function for de novo clustering with usearch61 seq_path: fasta filepath to be clustered with usearch61 percent_id: percentage id to cluster at rev: enable reverse strand matching for clustering save_intermediate_files: Saves intermediate files created during clustering minlen: minimum sequence length output_dir: directory to output log, OTU mapping, and intermediate files remove_usearch_logs: Saves usearch log files verbose: print current processing step to stdout wordlength: word length to use for clustering usearch_fast_cluster: Use usearch61 fast cluster option, not as memory efficient as the default cluster_smallmem option, requires sorting by length, and does not allow reverse strand matching. usearch61_sort_method: Sort sequences by abundance or length by using functionality provided by usearch61, or do not sort by using None option. otu_prefix: label to place in front of OTU IDs, used to prevent duplicate IDs from appearing with reference based OTU picking. usearch61_maxrejects: Number of rejects allowed by usearch61 usearch61_maxaccepts: Number of accepts allowed by usearch61 sizeorder: used for clustering based upon abundance of seeds threads: Specify number of threads used per core per CPU HALT_EXEC: application controller option to halt execution.", "response": "def usearch61_denovo_cluster(seq_path,\n                             percent_id=0.97,\n                             rev=False,\n                             save_intermediate_files=True,\n                             minlen=64,\n                             output_dir='.',\n                             remove_usearch_logs=False,\n                             verbose=False,\n                             wordlength=8,\n                             usearch_fast_cluster=False,\n                             usearch61_sort_method='abundance',\n                             otu_prefix=\"denovo\",\n                             usearch61_maxrejects=32,\n                             usearch61_maxaccepts=1,\n                             sizeorder=False,\n                             threads=1.0,\n                             HALT_EXEC=False,\n                             file_prefix=\"denovo_\"\n                             ):\n    \"\"\" Returns dictionary of cluster IDs:seq IDs\n\n    Overall function for de novo clustering with usearch61\n\n    seq_path:  fasta filepath to be clustered with usearch61\n    percent_id:  percentage id to cluster at\n    rev: enable reverse strand matching for clustering\n    save_intermediate_files: Saves intermediate files created during clustering\n    minlen: minimum sequence length\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    remove_usearch_logs: Saves usearch log files\n    verbose: print current processing step to stdout\n    wordlength: word length to use for clustering\n    usearch_fast_cluster: Use usearch61 fast cluster option, not as memory\n     efficient as the default cluster_smallmem option, requires sorting by\n     length, and does not allow reverse strand matching.\n    usearch61_sort_method:  Sort sequences by abundance or length by using\n     functionality provided by usearch61, or do not sort by using None option.\n    otu_prefix: label to place in front of OTU IDs, used to prevent duplicate\n     IDs from appearing with reference based OTU picking.\n    usearch61_maxrejects: Number of rejects allowed by usearch61\n    usearch61_maxaccepts: Number of accepts allowed by usearch61\n    sizeorder: used for clustering based upon abundance of seeds\n    threads: Specify number of threads used per core per CPU\n    HALT_EXEC: application controller option to halt execution.\n    \"\"\"\n\n    files_to_remove = []\n\n    # Need absolute paths to avoid potential problems with app controller\n    if output_dir:\n        output_dir = abspath(output_dir) + '/'\n    seq_path = abspath(seq_path)\n\n    try:\n        if verbose and usearch61_sort_method is not None and\\\n                not usearch_fast_cluster:\n            print \"Sorting sequences according to %s...\" % usearch61_sort_method\n\n        # fast sorting option automatically performs length sorting\n        if usearch61_sort_method == 'abundance' and not usearch_fast_cluster:\n            intermediate_fasta, dereplicated_uc, app_result =\\\n                sort_by_abundance_usearch61(seq_path, output_dir, rev,\n                                            minlen, remove_usearch_logs, HALT_EXEC,\n                                            output_fna_filepath=join(\n                                                output_dir,\n                                                file_prefix + 'abundance_sorted.fna'),\n                                            output_uc_filepath=join(output_dir,\n                                                                    file_prefix + 'abundance_sorted.uc'), threads=threads)\n            if not save_intermediate_files:\n                files_to_remove.append(intermediate_fasta)\n                files_to_remove.append(dereplicated_uc)\n        elif usearch61_sort_method == 'length' and not usearch_fast_cluster:\n            intermediate_fasta, app_result =\\\n                sort_by_length_usearch61(seq_path, output_dir, minlen,\n                                         remove_usearch_logs, HALT_EXEC,\n                                         output_fna_filepath=join(output_dir,\n                                                                  file_prefix + 'length_sorted.fna'))\n            if not save_intermediate_files:\n                files_to_remove.append(intermediate_fasta)\n        else:\n            intermediate_fasta = seq_path\n\n        if verbose:\n            print \"Clustering sequences de novo...\"\n\n        if usearch_fast_cluster:\n            clusters_fp, app_result = usearch61_fast_cluster(\n                intermediate_fasta,\n                percent_id, minlen, output_dir, remove_usearch_logs, wordlength,\n                usearch61_maxrejects, usearch61_maxaccepts, HALT_EXEC,\n                output_uc_filepath=join(\n                    output_dir,\n                    file_prefix + 'fast_clustered.uc'), threads=threads)\n            if not save_intermediate_files:\n                files_to_remove.append(clusters_fp)\n        else:\n            clusters_fp, app_result =\\\n                usearch61_smallmem_cluster(intermediate_fasta, percent_id,\n                                           minlen, rev, output_dir, remove_usearch_logs, wordlength,\n                                           usearch61_maxrejects, usearch61_maxaccepts, sizeorder, HALT_EXEC,\n                                           output_uc_filepath=join(output_dir,\n                                                                   file_prefix + 'smallmem_clustered.uc'))\n            if not save_intermediate_files:\n                files_to_remove.append(clusters_fp)\n\n    except ApplicationError:\n        raise ApplicationError('Error running usearch61. Possible causes are '\n                               'unsupported version (current supported version is usearch ' +\n                               'v6.1.544) is installed or improperly formatted input file was ' +\n                               'provided')\n\n    except ApplicationNotFoundError:\n        remove_files(files_to_remove)\n        raise ApplicationNotFoundError('usearch61 not found, is it properly ' +\n                                       'installed?')\n\n    if usearch61_sort_method == 'abundance' and not usearch_fast_cluster:\n        de_novo_clusters, failures =\\\n            parse_usearch61_clusters(open(clusters_fp, \"U\"), otu_prefix)\n        dereplicated_clusters =\\\n            parse_dereplicated_uc(open(dereplicated_uc, \"U\"))\n        clusters = merge_clusters_dereplicated_seqs(de_novo_clusters,\n                                                    dereplicated_clusters)\n\n    else:\n        clusters, failures =\\\n            parse_usearch61_clusters(open(clusters_fp, \"U\"), otu_prefix)\n\n    if not save_intermediate_files:\n        remove_files(files_to_remove)\n\n    return clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_by_abundance_usearch61(seq_path,\n                                output_dir='.',\n                                rev=False,\n                                minlen=64,\n                                remove_usearch_logs=False,\n                                HALT_EXEC=False,\n                                output_fna_filepath=None,\n                                output_uc_filepath=None,\n                                log_name=\"abundance_sorted.log\",\n                                threads=1.0):\n    \"\"\" usearch61 application call to sort fasta file by abundance.\n\n    seq_path:  fasta filepath to be clustered with usearch61\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    rev: enable reverse strand matching for clustering/sorting\n    minlen: minimum sequence length\n    remove_usearch_logs: Saves usearch log files\n    HALT_EXEC: application controller option to halt execution\n    output_fna_filepath: path to write sorted fasta filepath\n    output_uc_filepath: path to write usearch61 generated .uc file\n    log_name: filepath to write usearch61 generated log file\n    threads: Specify number of threads used per core per CPU\n    \"\"\"\n\n    if not output_fna_filepath:\n        _, output_fna_filepath = mkstemp(prefix='abundance_sorted',\n                                         suffix='.fna')\n\n    if not output_uc_filepath:\n        _, output_uc_filepath = mkstemp(prefix='abundance_sorted',\n                                        suffix='.uc')\n\n    log_filepath = join(output_dir, log_name)\n\n    params = {'--minseqlength': minlen,\n              '--sizeout': True,\n              '--derep_fulllength': seq_path,\n              '--output': output_fna_filepath,\n              '--uc': output_uc_filepath,\n              '--threads': threads\n              }\n\n    if rev:\n        params['--strand'] = 'both'\n    if not remove_usearch_logs:\n        params['--log'] = log_filepath\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return output_fna_filepath, output_uc_filepath, app_result", "response": "This function calls the Usearch61 application to sort a sequence by abundance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort_by_length_usearch61(seq_path,\n                             output_dir=\".\",\n                             minlen=64,\n                             remove_usearch_logs=False,\n                             HALT_EXEC=False,\n                             output_fna_filepath=None,\n                             log_name=\"length_sorted.log\"):\n    \"\"\" usearch61 application call to sort fasta file by length.\n\n    seq_path:  fasta filepath to be clustered with usearch61\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    minlen: minimum sequence length\n    remove_usearch_logs: Saves usearch log files\n    HALT_EXEC: application controller option to halt execution\n    output_fna_filepath: path to write sorted fasta filepath\n    log_name: filepath to write usearch61 generated log file\n    \"\"\"\n\n    if not output_fna_filepath:\n        _, output_fna_filepath = mkstemp(prefix='length_sorted', suffix='.fna')\n\n    log_filepath = join(output_dir, log_name)\n\n    params = {'--minseqlength': minlen,\n              '--sortbylength': seq_path,\n              '--output': output_fna_filepath\n              }\n    if not remove_usearch_logs:\n        params['--log'] = log_filepath\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return output_fna_filepath, app_result", "response": "Sort fasta file by length using usearch61."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usearch61_cluster_ref(intermediate_fasta,\n                          refseqs_fp,\n                          percent_id=0.97,\n                          rev=False,\n                          minlen=64,\n                          output_dir=\".\",\n                          remove_usearch_logs=False,\n                          wordlength=8,\n                          usearch61_maxrejects=32,\n                          usearch61_maxaccepts=1,\n                          HALT_EXEC=False,\n                          output_uc_filepath=None,\n                          log_filepath=\"ref_clustered.log\",\n                          threads=1.0\n                          ):\n    \"\"\" Cluster input fasta seqs against reference database\n\n    seq_path:  fasta filepath to be clustered with usearch61\n    refseqs_fp: reference fasta filepath, used to cluster sequences against.\n    percent_id:  percentage id to cluster at\n    rev: enable reverse strand matching for clustering\n    minlen: minimum sequence length\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    remove_usearch_logs: Saves usearch log files\n    wordlength: word length to use for clustering\n    usearch61_maxrejects: Number of rejects allowed by usearch61\n    usearch61_maxaccepts: Number of accepts allowed by usearch61\n    output_uc_filepath: path to write usearch61 generated .uc file\n    threads: Specify number of threads used per core per CPU\n    HALT_EXEC: application controller option to halt execution.\n    \"\"\"\n\n    log_filepath = join(output_dir, log_filepath)\n\n    params = {\n        '--usearch_global': intermediate_fasta,\n        '--db': refseqs_fp,\n        '--minseqlength': minlen,\n        '--id': percent_id,\n        '--uc': output_uc_filepath,\n        '--wordlength': wordlength,\n        '--maxrejects': usearch61_maxrejects,\n        '--maxaccepts': usearch61_maxaccepts,\n        '--threads': threads\n    }\n\n    if not remove_usearch_logs:\n        params['--log'] = log_filepath\n    if rev:\n        params['--strand'] = 'both'\n    else:\n        params['--strand'] = 'plus'\n\n    clusters_fp = output_uc_filepath\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return clusters_fp, app_result", "response": "Cluster input fasta file against reference database and return a list of clustered sequences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a fast clustering of a sequence using the usearch61 algorithm.", "response": "def usearch61_fast_cluster(intermediate_fasta,\n                           percent_id=0.97,\n                           minlen=64,\n                           output_dir=\".\",\n                           remove_usearch_logs=False,\n                           wordlength=8,\n                           usearch61_maxrejects=8,\n                           usearch61_maxaccepts=1,\n                           HALT_EXEC=False,\n                           output_uc_filepath=None,\n                           log_name=\"fast_clustered.log\",\n                           threads=1.0):\n    \"\"\" Performs usearch61 de novo fast clustering via cluster_fast option\n\n    Only supposed to be used with length sorted data (and performs length\n    sorting automatically) and does not support reverse strand matching\n\n    intermediate_fasta:  fasta filepath to be clustered with usearch61\n    percent_id:  percentage id to cluster at\n    minlen: minimum sequence length\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    remove_usearch_logs: Saves usearch log files\n    wordlength: word length to use for initial high probability sequence matches\n    usearch61_maxrejects: Set to 'default' or an int value specifying max\n     rejects\n    usearch61_maxaccepts: Number of accepts allowed by usearch61\n    HALT_EXEC: application controller option to halt execution\n    output_uc_filepath: Path to write clusters (.uc) file.\n    log_name: filepath to write usearch61 generated log file\n    threads: Specify number of threads used per core per CPU\n    \"\"\"\n\n    log_filepath = join(output_dir, log_name)\n\n    params = {'--minseqlength': minlen,\n              '--cluster_fast': intermediate_fasta,\n              '--id': percent_id,\n              '--uc': output_uc_filepath,\n              '--wordlength': wordlength,\n              '--maxrejects': usearch61_maxrejects,\n              '--maxaccepts': usearch61_maxaccepts,\n              '--usersort': True,\n              '--threads': threads\n              }\n\n    if not remove_usearch_logs:\n        params['--log'] = log_filepath\n\n    clusters_fp = output_uc_filepath\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return clusters_fp, app_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef usearch61_smallmem_cluster(intermediate_fasta,\n                               percent_id=0.97,\n                               minlen=64,\n                               rev=False,\n                               output_dir=\".\",\n                               remove_usearch_logs=False,\n                               wordlength=8,\n                               usearch61_maxrejects=32,\n                               usearch61_maxaccepts=1,\n                               sizeorder=False,\n                               HALT_EXEC=False,\n                               output_uc_filepath=None,\n                               log_name=\"smallmem_clustered.log\",\n                               sizeout=False,\n                               consout_filepath=None):\n    \"\"\" Performs usearch61 de novo clustering via cluster_smallmem option\n\n    Only supposed to be used with length sorted data (and performs length\n    sorting automatically) and does not support reverse strand matching\n\n    intermediate_fasta:  fasta filepath to be clustered with usearch61\n    percent_id:  percentage id to cluster at\n    minlen: minimum sequence length\n    rev: will enable reverse strand matching if True\n    output_dir: directory to output log, OTU mapping, and intermediate files\n    remove_usearch_logs: Saves usearch log files\n    wordlength: word length to use for initial high probability sequence matches\n    usearch61_maxrejects: Set to 'default' or an int value specifying max\n     rejects\n    usearch61_maxaccepts: Number of accepts allowed by usearch61\n    HALT_EXEC: application controller option to halt execution\n    output_uc_filepath: Path to write clusters (.uc) file.\n    log_name: filepath to write usearch61 generated log file\n    sizeout: If True, will save abundance data in output fasta labels.\n    consout_filepath: Needs to be set to save clustered consensus fasta\n     filepath used for chimera checking.\n    \"\"\"\n\n    log_filepath = join(output_dir, log_name)\n\n    params = {'--minseqlength': minlen,\n              '--cluster_smallmem': intermediate_fasta,\n              '--id': percent_id,\n              '--uc': output_uc_filepath,\n              '--wordlength': wordlength,\n              '--maxrejects': usearch61_maxrejects,\n              '--maxaccepts': usearch61_maxaccepts,\n              '--usersort': True\n              }\n\n    if sizeorder:\n        params['--sizeorder'] = True\n    if not remove_usearch_logs:\n        params['--log'] = log_filepath\n    if rev:\n        params['--strand'] = 'both'\n    else:\n        params['--strand'] = 'plus'\n    if sizeout:\n        params['--sizeout'] = True\n    if consout_filepath:\n        params['--consout'] = consout_filepath\n\n    clusters_fp = output_uc_filepath\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return clusters_fp, app_result", "response": "Performs usearch61 de novo clustering via cluster_smallmem"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usearch61_chimera_check_denovo(abundance_fp,\n                                   uchime_denovo_fp,\n                                   minlen=64,\n                                   output_dir=\".\",\n                                   remove_usearch_logs=False,\n                                   uchime_denovo_log_fp=\"uchime_denovo.log\",\n                                   usearch61_minh=0.28,\n                                   usearch61_xn=8.0,\n                                   usearch61_dn=1.4,\n                                   usearch61_mindiffs=3,\n                                   usearch61_mindiv=0.8,\n                                   usearch61_abundance_skew=2.0,\n                                   HALT_EXEC=False):\n    \"\"\" Does de novo, abundance based chimera checking with usearch61\n\n    abundance_fp: input consensus fasta file with abundance information for\n     each cluster.\n    uchime_denovo_fp: output uchime file for chimera results.\n    minlen: minimum sequence length for usearch input fasta seqs.\n    output_dir: output directory\n    removed_usearch_logs: suppresses creation of log file.\n    uchime_denovo_log_fp: output filepath for log file.\n    usearch61_minh: Minimum score (h) to be classified as chimera.\n     Increasing this value tends to the number of false positives (and also\n     sensitivity).\n    usearch61_xn:  Weight of \"no\" vote.  Increasing this value tends to the\n     number of false positives (and also sensitivity).\n    usearch61_dn:  Pseudo-count prior for \"no\" votes. (n). Increasing this\n     value tends to the number of false positives (and also sensitivity).\n    usearch61_mindiffs:  Minimum number of diffs in a segment. Increasing this\n     value tends to reduce the number of false positives while reducing\n     sensitivity to very low-divergence chimeras.\n    usearch61_mindiv:  Minimum divergence, i.e. 100% - identity between the\n     query and closest reference database sequence. Expressed as a percentage,\n     so the default is 0.8%, which allows chimeras that are up to 99.2% similar\n     to a reference sequence.\n    usearch61_abundance_skew: abundance skew for de novo chimera comparisons.\n    HALTEXEC: halt execution and returns command used for app controller.\n    \"\"\"\n\n    params = {'--minseqlength': minlen,\n              '--uchime_denovo': abundance_fp,\n              '--uchimeout': uchime_denovo_fp,\n              '--minh': usearch61_minh,\n              '--xn': usearch61_xn,\n              '--dn': usearch61_dn,\n              '--mindiffs': usearch61_mindiffs,\n              '--mindiv': usearch61_mindiv,\n              '--abskew': usearch61_abundance_skew\n              }\n\n    if not remove_usearch_logs:\n        params['--log'] = uchime_denovo_log_fp\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return uchime_denovo_fp, app_result", "response": "This function checks the chimera based chimera checking with the usearch61."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef usearch61_chimera_check_ref(abundance_fp,\n                                uchime_ref_fp,\n                                reference_seqs_fp,\n                                minlen=64,\n                                output_dir=\".\",\n                                remove_usearch_logs=False,\n                                uchime_ref_log_fp=\"uchime_ref.log\",\n                                usearch61_minh=0.28,\n                                usearch61_xn=8.0,\n                                usearch61_dn=1.4,\n                                usearch61_mindiffs=3,\n                                usearch61_mindiv=0.8,\n                                threads=1.0,\n                                HALT_EXEC=False):\n    \"\"\" Does reference based chimera checking with usearch61\n\n    abundance_fp: input consensus fasta file with abundance information for\n     each cluster.\n    uchime_ref_fp: output uchime filepath for reference results\n    reference_seqs_fp: reference fasta database for chimera checking.\n    minlen: minimum sequence length for usearch input fasta seqs.\n    output_dir: output directory\n    removed_usearch_logs: suppresses creation of log file.\n    uchime_denovo_log_fp: output filepath for log file.\n    usearch61_minh: Minimum score (h) to be classified as chimera.\n     Increasing this value tends to the number of false positives (and also\n     sensitivity).\n    usearch61_xn:  Weight of \"no\" vote.  Increasing this value tends to the\n     number of false positives (and also sensitivity).\n    usearch61_dn:  Pseudo-count prior for \"no\" votes. (n). Increasing this\n     value tends to the number of false positives (and also sensitivity).\n    usearch61_mindiffs:  Minimum number of diffs in a segment. Increasing this\n     value tends to reduce the number of false positives while reducing\n     sensitivity to very low-divergence chimeras.\n    usearch61_mindiv:  Minimum divergence, i.e. 100% - identity between the\n     query and closest reference database sequence. Expressed as a percentage,\n     so the default is 0.8%, which allows chimeras that are up to 99.2% similar\n     to a reference sequence.\n    threads: Specify number of threads used per core per CPU\n    HALTEXEC: halt execution and returns command used for app controller.\n    \"\"\"\n\n    params = {'--minseqlength': minlen,\n              '--uchime_ref': abundance_fp,\n              '--uchimeout': uchime_ref_fp,\n              '--db': reference_seqs_fp,\n              '--minh': usearch61_minh,\n              '--xn': usearch61_xn,\n              '--dn': usearch61_dn,\n              '--mindiffs': usearch61_mindiffs,\n              '--mindiv': usearch61_mindiv,\n              # Only works in plus according to usearch doc\n              '--strand': 'plus',\n              '--threads': threads\n              }\n\n    if not remove_usearch_logs:\n        params['--log'] = uchime_ref_log_fp\n\n    app = Usearch61(params, WorkingDir=output_dir, HALT_EXEC=HALT_EXEC)\n\n    app_result = app()\n\n    return uchime_ref_fp, app_result", "response": "This function checks if a given set of sequences for each cluster is a reference based chimera."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the dereplicated. uc file into a dict of seq ID to list of seq IDs", "response": "def parse_dereplicated_uc(dereplicated_uc_lines):\n    \"\"\" Return dict of seq ID:dereplicated seq IDs from dereplicated .uc lines\n\n    dereplicated_uc_lines: list of lines of .uc file from dereplicated seqs from\n     usearch61 (i.e. open file of abundance sorted .uc data)\n    \"\"\"\n\n    dereplicated_clusters = {}\n\n    seed_hit_ix = 0\n    seq_id_ix = 8\n    seed_id_ix = 9\n\n    for line in dereplicated_uc_lines:\n        if line.startswith(\"#\") or len(line.strip()) == 0:\n            continue\n        curr_line = line.strip().split('\\t')\n        if curr_line[seed_hit_ix] == \"S\":\n            dereplicated_clusters[curr_line[seq_id_ix]] = []\n        if curr_line[seed_hit_ix] == \"H\":\n            curr_seq_id = curr_line[seq_id_ix]\n            dereplicated_clusters[curr_line[seed_id_ix]].append(curr_seq_id)\n\n    return dereplicated_clusters"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_usearch61_clusters(clustered_uc_lines,\n                             otu_prefix='denovo',\n                             ref_clustered=False):\n    \"\"\" Returns dict of cluster ID:seq IDs\n\n    clustered_uc_lines: lines from .uc file resulting from de novo clustering\n    otu_prefix: string added to beginning of OTU ID.\n    ref_clustered: If True, will attempt to create dict keys for clusters as\n     they are read from the .uc file, rather than from seed lines.\n    \"\"\"\n\n    clusters = {}\n    failures = []\n\n    seed_hit_ix = 0\n    otu_id_ix = 1\n    seq_id_ix = 8\n    ref_id_ix = 9\n\n    for line in clustered_uc_lines:\n        if line.startswith(\"#\") or len(line.strip()) == 0:\n            continue\n        curr_line = line.strip().split('\\t')\n        if curr_line[seed_hit_ix] == \"S\":\n            # Need to split on semicolons for sequence IDs to handle case of\n            # abundance sorted data\n            clusters[otu_prefix + curr_line[otu_id_ix]] =\\\n                [curr_line[seq_id_ix].split(';')[0].split()[0]]\n        if curr_line[seed_hit_ix] == \"H\":\n            curr_id = curr_line[seq_id_ix].split(';')[0].split()[0]\n            if ref_clustered:\n                try:\n                    clusters[otu_prefix + curr_line[ref_id_ix]].append(curr_id)\n                except KeyError:\n                    clusters[otu_prefix + curr_line[ref_id_ix]] = [curr_id]\n            else:\n                clusters[otu_prefix +\n                         curr_line[otu_id_ix]].append(curr_id)\n        if curr_line[seed_hit_ix] == \"N\":\n            failures.append(curr_line[seq_id_ix].split(';')[0])\n\n    return clusters, failures", "response": "Parse the clustered. uc file into a dict of cluster ID seq IDs and list of failures."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_clusters_dereplicated_seqs(de_novo_clusters,\n                                     dereplicated_clusters):\n    \"\"\" combines de novo clusters and dereplicated seqs to OTU id:seqs dict\n\n    de_novo_clusters: dict of OTU ID:clustered sequences\n    dereplicated_clusters:  dict of seq IDs: dereplicated seq IDs\n    \"\"\"\n\n    clusters = {}\n\n    for curr_denovo_key in de_novo_clusters.keys():\n        clusters[curr_denovo_key] = de_novo_clusters[curr_denovo_key]\n        curr_clusters = []\n        for curr_denovo_id in de_novo_clusters[curr_denovo_key]:\n            curr_clusters += dereplicated_clusters[curr_denovo_id]\n        clusters[curr_denovo_key] += curr_clusters\n\n    return clusters", "response": "merges de novo clusters and dereplicated sequences into a single OTU ID - > sequence list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds failures from dereplicated seqs to failures list", "response": "def merge_failures_dereplicated_seqs(failures,\n                                     dereplicated_clusters):\n    \"\"\" Appends failures from dereplicated seqs to failures list\n\n    failures: list of failures\n    dereplicated_clusters:  dict of seq IDs: dereplicated seq IDs\n    \"\"\"\n\n    curr_failures = set(failures)\n    dereplicated_ids = set(dereplicated_clusters)\n\n    for curr_failure in curr_failures:\n        if curr_failure in dereplicated_ids:\n            failures += dereplicated_clusters[curr_failure]\n\n    return failures"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing seq IDs from failures list writes to output_fasta_fp", "response": "def parse_usearch61_failures(seq_path,\n                             failures,\n                             output_fasta_fp):\n    \"\"\" Parses seq IDs from failures list, writes to output_fasta_fp\n\n    seq_path: filepath of original input fasta file.\n    failures: list/set of failure seq IDs\n    output_fasta_fp: path to write parsed sequences\n    \"\"\"\n\n    parsed_out = open(output_fasta_fp, \"w\")\n\n    for label, seq in parse_fasta(open(seq_path), \"U\"):\n        curr_label = label.split()[0]\n        if curr_label in failures:\n            parsed_out.write(\">%s\\n%s\\n\" % (label, seq))\n    parsed_out.close()\n    return output_fasta_fp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_error_page_write_error(self, status_code, **kwargs):\n\n        if oz.settings.get('debug'):\n            exception_type, exception_value, tback = sys.exc_info()\n            is_breakpoint = isinstance(exception_value, oz.error_pages.DebugBreakException)\n\n            frames = oz.error_pages.get_frames(tback, is_breakpoint)\n            frames.reverse()\n\n            if is_breakpoint:\n                exception_type = 'Debug breakpoint'\n                exception_value = ''\n\n            self.render(oz.settings[\"error_pages_template\"],\n                exception_type=exception_type,\n                exception_value=exception_value,\n                frames=frames,\n                request_input=self.request.body,\n                request_cookies=self.cookies,\n                request_headers=self.request.headers,\n                request_path=self.request.uri,\n                request_method=self.request.method,\n                response_output=\"\".join(self._write_buffer),\n                response_headers=self._headers,\n                prettify_object=oz.error_pages.prettify_object,\n            )\n\n            return oz.break_trigger", "response": "Replaces the default Tornado error page with a Django - styled one"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a blink cookie value", "response": "def get_blink_cookie(self, name):\n        \"\"\"Gets a blink cookie value\"\"\"\n        value = self.get_cookie(name)\n\n        if value != None:\n            self.clear_cookie(name)\n            return escape.url_unescape(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the blink message that is shown on the next page load", "response": "def set_blink(self, message, type=\"info\"):\n        \"\"\"\n        Sets the blink, a one-time transactional message that is shown on the\n        next page load\n        \"\"\"\n        self.set_cookie(\"blink_message\", escape.url_escape(message), httponly=True)\n        self.set_cookie(\"blink_type\", escape.url_escape(type), httponly=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cdhit_clusters_from_seqs(seqs, moltype=DNA, params=None):\n    # keys are not remapped. Tested against seq_ids of 100char length\n    seqs = SequenceCollection(seqs, MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seqs.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n    # setup params and make sure the output argument is set\n    if params is None:\n        params = {}\n    if '-o' not in params:\n        _, params['-o'] = mkstemp()\n\n    # call the correct version of cd-hit base on moltype\n    working_dir = mkdtemp()\n    if moltype is PROTEIN:\n        app = CD_HIT(WorkingDir=working_dir, params=params)\n    elif moltype is RNA:\n        app = CD_HIT_EST(WorkingDir=working_dir, params=params)\n    elif moltype is DNA:\n        app = CD_HIT_EST(WorkingDir=working_dir, params=params)\n    else:\n        raise ValueError, \"Moltype must be either PROTEIN, RNA, or DNA\"\n\n    # grab result\n    res = app(int_map.toFasta())\n    clusters = parse_cdhit_clstr_file(res['CLSTR'])\n\n    remapped_clusters = []\n    for c in clusters:\n        curr = [int_keys[i] for i in c]\n        remapped_clusters.append(curr)\n\n    # perform cleanup\n    res.cleanUp()\n    shutil.rmtree(working_dir)\n    remove(params['-o'] + '.bak.clstr')\n\n    return remapped_clusters", "response": "Returns the CD - HIT clusters given a sequence collection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the CD - HIT results given seqs moltype and params", "response": "def cdhit_from_seqs(seqs, moltype, params=None):\n    \"\"\"Returns the CD-HIT results given seqs\n\n    seqs    : dict like collection of sequences\n    moltype : cogent.core.moltype object\n    params  : cd-hit parameters\n\n    NOTE: This method will call CD_HIT if moltype is PROTIEN,\n        CD_HIT_EST if moltype is RNA/DNA, and raise if any other\n        moltype is passed.\n    \"\"\"\n    # keys are not remapped. Tested against seq_ids of 100char length\n    seqs = SequenceCollection(seqs, MolType=moltype)\n\n    # setup params and make sure the output argument is set\n    if params is None:\n        params = {}\n    if '-o' not in params:\n        _, params['-o'] = mkstemp()\n\n    # call the correct version of cd-hit base on moltype\n    working_dir = mkdtemp()\n    if moltype is PROTEIN:\n        app = CD_HIT(WorkingDir=working_dir, params=params)\n    elif moltype is RNA:\n        app = CD_HIT_EST(WorkingDir=working_dir, params=params)\n    elif moltype is DNA:\n        app = CD_HIT_EST(WorkingDir=working_dir, params=params)\n    else:\n        raise ValueError, \"Moltype must be either PROTEIN, RNA, or DNA\"\n\n    # grab result\n    res = app(seqs.toFasta())\n    new_seqs = dict(parse_fasta(res['FASTA']))\n\n    # perform cleanup\n    res.cleanUp()\n    shutil.rmtree(working_dir)\n    remove(params['-o'] + '.bak.clstr')\n\n    return SequenceCollection(new_seqs, MolType=moltype)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cdhit_clstr_file(lines):\n    clusters = []\n    curr_cluster = []\n\n    for l in lines:\n        if l.startswith('>Cluster'):\n            if not curr_cluster:\n                continue\n            clusters.append(curr_cluster)\n            curr_cluster = []\n        else:\n            curr_cluster.append(clean_cluster_seq_id(l.split()[2]))\n\n    if curr_cluster:\n        clusters.append(curr_cluster)\n\n    return clusters", "response": "Returns a list of list of sequence ids representing clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _input_as_multiline_string(self, data):\n        if data:\n            self.Parameters['-i']\\\n                    .on(super(CD_HIT,self)._input_as_multiline_string(data))\n        return ''", "response": "Writes data to tempfile and sets - i parameter\n\n                   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite data to tempfile and sets - i parameter to data", "response": "def _input_as_lines(self, data):\n        \"\"\"Writes data to tempfile and sets -i parameter\n\n        data -- list of lines, ready to be written to file\n        \"\"\"\n        if data:\n            self.Parameters['-i']\\\n                    .on(super(CD_HIT,self)._input_as_lines(data))\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_clstr_outfile(self):\n        if self.Parameters['-o'].isOn():\n            return ''.join([self.Parameters['-o'].Value, '.clstr'])\n        else:\n            raise ValueError, \"No output file specified\"", "response": "Returns the absolute path to the clstr outfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns dict of key = > ResultPath", "response": "def _get_result_paths(self, data):\n        \"\"\"Return dict of {key: ResultPath}\"\"\"\n        result = {}\n        result['FASTA'] = ResultPath(Path=self._get_seqs_outfile())\n        result['CLSTR'] = ResultPath(Path=self._get_clstr_outfile())\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef segment(self, tokens):\n        look_ahead = LookAhead(tokens)\n\n        segments = Segment()\n\n        while not look_ahead.empty():\n\n            if look_ahead.peek().type not in self.whitespace:  # Paragraph!\n                paragraph = MatchableSegment(look_ahead.i)\n\n                while not look_ahead.empty() and \\\n                      look_ahead.peek().type not in self.paragraph_end:\n\n                    if look_ahead.peek().type == \"tab_open\":  # Table\n                        tab_depth = 1\n                        sentence = MatchableSegment(\n                            look_ahead.i, [next(look_ahead)])\n                        while not look_ahead.empty() and tab_depth > 0:\n                            tab_depth += look_ahead.peek().type == \"tab_open\"\n                            tab_depth -= look_ahead.peek().type == \"tab_close\"\n                            sentence.append(next(look_ahead))\n\n                        paragraph.append(sentence)\n\n                    elif look_ahead.peek().type not in self.whitespace:  # Sentence!\n                        sentence = MatchableSegment(\n                            look_ahead.i, [next(look_ahead)])\n                        sub_depth = int(sentence[0].type in SUB_OPEN)\n                        while not look_ahead.empty():\n\n                            sub_depth += look_ahead.peek().type in SUB_OPEN\n                            sub_depth -= look_ahead.peek().type in SUB_CLOSE\n                            sentence.append(next(look_ahead))\n\n                            if sentence[-1].type in self.sentence_end and sub_depth <= 0:\n                                non_whitespace = sum(s.type not in self.whitespace for s in sentence)\n                                if non_whitespace >= self.min_sentence:\n                                    break\n\n                        paragraph.append(sentence)\n\n                    else:  # look_ahead.peek().type in self.whitespace\n                        whitespace = Segment(look_ahead.i, [next(look_ahead)])\n                        paragraph.append(whitespace)\n\n                segments.append(paragraph)\n            else: # look_ahead.peek().type in self.whitespace\n                whitespace = Segment(look_ahead.i, [next(look_ahead)])\n                segments.append(whitespace)\n\n\n        return segments", "response": "Segments a sequence of tokens into a sequence of segments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a push message to the Apple server.", "response": "def send(self, payload, token, expiration=None, priority=None, identifier=None):\n        \"\"\"\n        Attempts to send a push message. On network failures, progagates the exception.\n        It is advised to make all text in the payload dictionary unicode objects and not\n        mix unicode objects and str objects. If str objects are used, they must be\n        in UTF-8 encoding.\n        Args:\n            payload (dict): The dictionary payload of the push to send\n            token (str): token to send the push to (raw, unencoded bytes)\n            expiration (int, seconds): When the message becomes irrelevant (time in seconds, as from time.time())\n            priority (int): Integer priority for the message as per Apple's documentation\n            identifier (any): optional identifier that will be returned if the push fails.\n                        This is opaque to the library and not limited to 4 bytes.\n        Throws:\n            BodyTooLongException: If the payload body is too long and cannot be truncated to fit\n        \"\"\"\n\n        # we only use one conn at a time currently but we may as well do this...\n        created_conn = False\n        while not created_conn:\n            if len(self.conns) == 0:\n                self.conns.append(PushConnection(self, self.address, self.certfile, self.keyfile))\n                created_conn = True\n            conn = random.choice(self.conns)\n            try:\n                conn.send(payload, token, expiration=expiration, priority=priority, identifier=identifier)\n                return\n            except:\n                logger.info(\"Connection died: removing\")\n                self.conns.remove(conn)\n        raise SendFailedException()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_feedback(self):\n        if not self.fbaddress:\n            raise Exception(\"Attempted to fetch feedback but no feedback_address supplied\")\n\n        fbconn = FeedbackConnection(self, self.fbaddress, self.certfile, self.keyfile)\n        return fbconn.get_all()", "response": "Get all feedback items from the Feedback service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the response text into an XML object.", "response": "def _parse_xml(self, response):\n        \"\"\"\n        Run our XML parser (lxml in this case) over our response text.  Lxml\n        doesn't enjoy having xml/encoding information in the header so we strip\n        that out if necessary. We return a parsed XML object that can be\n        used by the calling API method and massaged into a more appropriate\n        format.\n        \"\"\"\n        if response.startswith('\\n'):\n            response = response[1:]\n        tree = etree.fromstring(response)\n        return tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _api_rate_limit_exceeded(self, api_call, window=60):\n        current = datetime.datetime.now()\n        try:\n            previous = getattr(self, api_call.__name__ + \"_window\")\n            # Force the calling of our property so we can\n            # handle not having set it yet.\n            previous.__str__\n        except AttributeError:\n            now = datetime.datetime.now()\n            outside_window = datetime.timedelta(seconds=window+1)\n            previous = now - outside_window\n\n        if current - previous > datetime.timedelta(seconds=window):\n            setattr(self, api_call.__name__ + \"_window\", current)\n        else:\n            timeout = window - (current - previous).seconds\n            raise NewRelicApiRateLimitException(str(timeout))", "response": "Helper method to check if the rate limit for the NewRelic API is exceeded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of Application objects for the current account.", "response": "def view_applications(self):\n        \"\"\"\n        Requires: account ID (taken from Client object)\n        Returns: a list of Application objects\n        Endpoint: rpm.newrelic.com\n        Errors: 403 Invalid API Key\n        Method: Get\n        \"\"\"\n        endpoint = \"https://rpm.newrelic.com\"\n        uri = \"{endpoint}/accounts/{id}/applications.xml\".format(endpoint=endpoint, id=self.account_id)\n        response = self._make_get_request(uri)\n        applications = []\n\n        for application in response.findall('.//application'):\n            application_properties = {}\n            for field in application:\n                application_properties[field.tag] = field.text\n            applications.append(Application(application_properties))\n        return applications"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the given applications from the New Relic account.", "response": "def delete_applications(self, applications):\n        \"\"\"\n        Requires: account ID, application ID (or name).\n        Input should be a dictionary { 'app_id': 1234 , 'app': 'My Application'}\n        Returns:  list of failed deletions (if any)\n        Endpoint: api.newrelic.com\n        Errors: None Explicit, failed deletions will be in XML\n        Method: Post\n        \"\"\"\n        endpoint = \"https://api.newrelic.com\"\n        uri = \"{endpoint}/api/v1/accounts/{account_id}/applications/delete.xml\"\\\n              .format(endpoint=endpoint, account_id=self.account_id)\n        payload = applications\n        response = self._make_post_request(uri, payload)\n        failed_deletions = {}\n\n        for application in response.findall('.//application'):\n            if not 'deleted' in application.findall('.//result')[0].text:\n                failed_deletions['app_id'] = application.get('id')\n\n        return failed_deletions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnotifying NewRelic of a deployment.", "response": "def notify_deployment(self, application_id=None, application_name=None, description=None, revision=None, changelog=None, user=None):\n        \"\"\"\n        Notify NewRelic of a deployment.\n        http://newrelic.github.io/newrelic_api/NewRelicApi/Deployment.html\n\n        :param description:\n        :param revision:\n        :param changelog:\n        :param user:\n        :return: A dictionary containing all of the returned keys from the API\n        \"\"\"\n\n        endpoint = \"https://rpm.newrelic.com\"\n        uri = \"{endpoint}/deployments.xml\".format(endpoint=endpoint)\n\n        deploy_event = {}\n\n        if not application_id is None:\n            deploy_event['deployment[application_id]'] = application_id\n        elif not application_name is None:\n            deploy_event['deployment[app_name]'] = application_name\n        else:\n            raise NewRelicInvalidParameterException(\"Must specify either application_id or application_name.\")\n\n        if not description is None:\n            deploy_event['deployment[description]'] = description\n\n        if not revision is None:\n            deploy_event['deployment[revision]'] = revision\n\n        if not changelog is None:\n            deploy_event['deployment[changelog]'] = changelog\n\n        if not user is None:\n            deploy_event['deployment[user]'] = user\n\n        response = self._make_post_request(uri, deploy_event)\n        result = {}\n\n        for value in response:\n            result[value.tag] = value.text\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the names of all the metrics available for a given application ID.", "response": "def get_metric_names(self, agent_id, re=None, limit=5000):\n        \"\"\"\n        Requires: application ID\n        Optional: Regex to filter metric names, limit of results\n        Returns: A dictionary,\n                    key:  metric name,\n                    value: list of fields available for a given metric\n        Method: Get\n        Restrictions: Rate limit to 1x per minute\n        Errors: 403 Invalid API Key, 422 Invalid Parameters\n        Endpoint: api.newrelic.com\n        \"\"\"\n        # Make sure we play it slow\n        self._api_rate_limit_exceeded(self.get_metric_names)\n\n        # Construct our GET request parameters into a nice dictionary\n        parameters = {'re': re, 'limit': limit}\n\n        endpoint = \"https://api.newrelic.com\"\n        uri = \"{endpoint}/api/v1/applications/{agent_id}/metrics.xml\"\\\n              .format(endpoint=endpoint, agent_id=agent_id)\n\n        # A longer timeout is needed due to the amount of\n        # data that can be returned without a regex search\n        response = self._make_get_request(uri, parameters=parameters, timeout=max(self.timeout, 5.0))\n\n        # Parse the response. It seems clearer to return a dict of\n        # metrics/fields instead of a list of metric objects. It might be more\n        # consistent with the retrieval of metric data to make them objects but\n        # since the attributes in each type of metric object are different\n        # (and we aren't going to make heavyweight objects) we don't want to.\n        metrics = {}\n        for metric in response.findall('.//metric'):\n            fields = []\n            for field in metric.findall('.//field'):\n                fields.append(field.get('name'))\n            metrics[metric.get('name')] = fields\n        return metrics"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_metric_data(self, applications, metrics, field, begin, end, summary=False):\n        # TODO: it may be nice to have some helper methods that make it easier\n        #       to query by common time frames based off the time period folding\n        #       of the metrics returned by the New Relic API.\n\n        # Make sure we aren't going to hit an API timeout\n        self._api_rate_limit_exceeded(self.get_metric_data)\n\n        # Just in case the API needs parameters to be in order\n        parameters = {}\n\n        # Figure out what we were passed and set our parameter correctly\n        # TODO: allow querying by something other than an application name/id,\n        # such as server id or agent id\n        try:\n            int(applications[0])\n        except ValueError:\n            app_string = \"app\"\n        else:\n            app_string = \"app_id\"\n\n        if len(applications) > 1:\n            app_string = app_string + \"[]\"\n\n        # Set our parameters\n        parameters[app_string] = applications\n        parameters['metrics[]'] = metrics\n        parameters['field'] = field\n        parameters['begin'] = begin\n        parameters['end'] = end\n        parameters['summary'] = int(summary)\n\n        endpoint = \"https://api.newrelic.com\"\n        uri = \"{endpoint}/api/v1/accounts/{account_id}/metrics/data.xml\"\\\n              .format(endpoint=endpoint, account_id=self.account_id)\n        # A longer timeout is needed due to the\n        # amount of data that can be returned\n        response = self._make_get_request(uri, parameters=parameters, timeout=max(self.timeout, 5.0))\n\n        # Parsing our response into lightweight objects and creating a list.\n        # The dividing factor is the time period covered by the metric,\n        # there should be no overlaps in time.\n        metrics = []\n        for metric in response.findall('.//metric'):\n            metrics.append(Metric(metric))\n        return metrics", "response": "Get the data for a set of metrics for a given field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list of threshold values for a given application ID.", "response": "def get_threshold_values(self, application_id):\n        \"\"\"\n        Requires: account ID, list of application ID\n        Method: Get\n        Endpoint: api.newrelic.com\n        Restrictions: ???\n        Errors: 403 Invalid API key, 422 Invalid Parameters\n        Returns: A list of threshold_value objects, each will have information\n                 about its start/end time, metric name, metric value, and the\n                 current threshold\n        \"\"\"\n        endpoint = \"https://rpm.newrelic.com\"\n        remote_file = \"threshold_values.xml\"\n        uri = \"{endpoint}/accounts/{account_id}/applications/{app_id}/{xml}\".format(endpoint=endpoint, account_id=self.account_id, app_id=application_id, xml=remote_file)\n        response = self._make_get_request(uri)\n        thresholds = []\n\n        for threshold_value in response.findall('.//threshold_value'):\n            properties = {}\n            # a little ugly, but the output works fine.\n            for tag, text in threshold_value.items():\n                properties[tag] = text\n            thresholds.append(Threshold(properties))\n        return thresholds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of Server objects for the current account.", "response": "def view_servers(self):\n        \"\"\"\n        Requires: account ID (taken from Client object)\n        Returns: a list of Server objects\n        Endpoint: api.newrelic.com\n        Errors: 403 Invalid API Key\n        Method: Get\n        \"\"\"\n        endpoint = \"https://api.newrelic.com\"\n        uri = \"{endpoint}/api/v1/accounts/{id}/servers.xml\".format(endpoint=endpoint, id=self.account_id)\n        response = self._make_get_request(uri)\n        servers = []\n\n        for server in response.findall('.//server'):\n            server_properties = {}\n            for field in server:\n                server_properties[field.tag] = field.text\n            servers.append(Server(server_properties))\n        return servers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a specific server from New Relic", "response": "def delete_servers(self, server_id):\n        \"\"\"\n        Requires: account ID, server ID\n        Input should be server id\n        Returns: list of failed deletions (if any)\n        Endpoint: api.newrelic.com\n        Errors: 403 Invalid API Key\n        Method: Delete\n        \"\"\"\n        endpoint = \"https://api.newrelic.com\"\n        uri = \"{endpoint}/api/v1/accounts/{account_id}/servers/{server_id}.xml\".format(\n            endpoint=endpoint,\n            account_id=self.account_id,\n            server_id=server_id)\n        response = self._make_delete_request(uri)\n        failed_deletions = []\n        for server in response.findall('.//server'):\n            if not 'deleted' in server.findall('.//result')[0].text:\n                failed_deletions.append({'server_id': server.get('id')})\n\n        return failed_deletions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef join_paired_end_reads_fastqjoin(\n        reads1_infile_path,\n        reads2_infile_path,\n        perc_max_diff=None,  # typical default is 8\n        min_overlap=None,  # typical default is 6\n        outfile_label='fastqjoin',\n        params={},\n        working_dir=tempfile.gettempdir(),\n        SuppressStderr=True,\n        SuppressStdout=True,\n        HALT_EXEC=False):\n    \"\"\" Runs fastq-join, with default parameters to assemble paired-end reads.\n        Returns file path string.\n\n        -reads1_infile_path : reads1.fastq infile path\n        -reads2_infile_path : reads2.fastq infile path\n        -perc_max_diff : maximum % diff of overlap differences allowed\n        -min_overlap : minimum allowed overlap required to assemble reads\n        -outfile_label : base name for output files.\n        -params : dictionary of application controller parameters\n\n    \"\"\"\n    abs_r1_path = os.path.abspath(reads1_infile_path)\n    abs_r2_path = os.path.abspath(reads2_infile_path)\n\n    infile_paths = [abs_r1_path, abs_r2_path]\n\n    # check / make absolute infile paths\n    for p in infile_paths:\n        if not os.path.exists(p):\n            raise IOError('File not found at: %s' % p)\n\n    fastq_join_app = FastqJoin(params=params,\n                               WorkingDir=working_dir,\n                               SuppressStderr=SuppressStderr,\n                               SuppressStdout=SuppressStdout,\n                               HALT_EXEC=HALT_EXEC)\n\n    # set param. Helps with QIIME integration to have these values\n    # set to None by default. This way we do not have to worry\n    # about changes in default behaviour of the wrapped\n    # application\n    if perc_max_diff is not None:\n        if isinstance(perc_max_diff, int) and 0 <= perc_max_diff <= 100:\n            fastq_join_app.Parameters['-p'].on(perc_max_diff)\n        else:\n            raise ValueError(\"perc_max_diff must be int between 0-100!\")\n\n    if min_overlap is not None:\n        if isinstance(min_overlap, int) and 0 < min_overlap:\n            fastq_join_app.Parameters['-m'].on(min_overlap)\n        else:\n            raise ValueError(\"min_overlap must be an int >= 0!\")\n\n    if outfile_label is not None:\n        if isinstance(outfile_label, str):\n            fastq_join_app.Parameters['-o'].on(outfile_label + '.')\n        else:\n            raise ValueError(\"outfile_label must be a string!\")\n    else:\n        pass\n\n    # run assembler\n    result = fastq_join_app(infile_paths)\n\n    # Store output file path data to dict\n    path_dict = {}\n    path_dict['Assembled'] = result['Assembled'].name\n    path_dict['UnassembledReads1'] = result['UnassembledReads1'].name\n    path_dict['UnassembledReads2'] = result['UnassembledReads2'].name\n\n    # sanity check that files actually exist in path lcoations\n    for path in path_dict.values():\n        if not os.path.exists(path):\n            raise IOError('Output file not found at: %s' % path)\n\n    # fastq-join automatically appends: 'join', 'un1', or 'un2'\n    # to the end of the file names. But we want to rename them so\n    # they end in '.fastq'. So, we iterate through path_dict to\n    # rename the files and overwrite the dict values.\n    for key, file_path in path_dict.items():\n        new_file_path = file_path + '.fastq'\n        shutil.move(file_path, new_file_path)\n        path_dict[key] = new_file_path\n\n    return path_dict", "response": "Runs fastq - join on the specified reads1 and reads2 files and returns the path to the output files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a base file label / path is set. Returns absolute path.", "response": "def _get_output_path(self):\n        \"\"\"Checks if a base file label / path is set. Returns absolute path.\"\"\"\n        if self.Parameters['-o'].isOn():\n            output_path = self._absolute(str(self.Parameters['-o'].Value))\n        else:\n            raise ValueError(\"No output path specified.\")\n        return output_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if stitch report label / path is set. Returns absolute path.", "response": "def _get_stitch_report_path(self):\n        \"\"\"Checks if stitch report label / path is set. Returns absolute path.\"\"\"\n        if self.Parameters['-r'].isOn():\n            stitch_path = self._absolute(str(self.Parameters['-r'].Value))\n            return stitch_path\n        elif self.Parameters['-r'].isOff():\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the output paths for the next fastq - join output.", "response": "def _get_result_paths(self, data):\n        \"\"\"Capture fastq-join output.\n\n        Three output files are produced, in the form of\n            outputjoin : assembled paired reads\n            outputun1 : unassembled reads_1\n            outputun2 : unassembled reads_2\n\n        If a barcode / mate-pairs file is also provided then the following\n        additional files are output:\n            outputjoin2\n            outputun3\n\n        If a verbose stitch length report (-r) is chosen to be written by the\n        user then use a user specified filename.\n        \"\"\"\n        output_path = self._get_output_path()\n\n        result = {}\n\n        # always output:\n        result['Assembled'] = ResultPath(Path=output_path + 'join',\n                                         IsWritten=True)\n        result['UnassembledReads1'] = ResultPath(Path=output_path + 'un1',\n                                                 IsWritten=True)\n        result['UnassembledReads2'] = ResultPath(Path=output_path + 'un2',\n                                                 IsWritten=True)\n\n        # check if stitch report is requested:\n        stitch_path = self._get_stitch_report_path()\n        if stitch_path:\n            result['Report'] = ResultPath(Path=stitch_path,\n                                          IsWritten=True)\n\n        # Check if mate file / barcode file is present.\n        # If not, return result\n        # We need to check this way becuase there are no infile parameters.\n        mate_path_string = output_path + 'join2'\n        mate_unassembled_path_string = output_path + 'un3'\n        if os.path.exists(mate_path_string) and \\\n                os.path.exists(mate_unassembled_path_string):\n            result['Mate'] = ResultPath(Path=mate_path_string,\n                                        IsWritten=True)\n            result['MateUnassembled'] = ResultPath(Path=\n                                                   mate_unassembled_path_string,\n                                                   IsWritten=True)\n        else:\n            pass\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_fields(fields, as_dict=False):\n    '''\n    Given a list of fields (or several other variants of the same),\n    return back a consistent, normalized form of the same.\n\n    To forms are currently supported:\n        dictionary form: dict 'key' is the field name\n                                   and dict 'value' is either 1 (include)\n                                   or 0 (exclude).\n        list form (other): list values are field names to be included\n\n    If fields passed is one of the following values, it will be assumed\n    the user wants to include all fields and thus, we return an empty\n    dict or list to indicate this, accordingly:\n     * all fields: ['~', None, False, True, {}, []]\n\n\n    '''\n    _fields = {}\n    if fields in ['~', None, False, True, {}, []]:\n        # all these signify 'all fields'\n        _fields = {}\n    elif isinstance(fields, dict):\n        _fields.update(\n            {unicode(k).strip(): int(v) for k, v in fields.iteritems()})\n    elif isinstance(fields, basestring):\n        _fields.update({unicode(s).strip(): 1 for s in fields.split(',')})\n    elif isinstance(fields, (list, tuple)):\n        _fields.update({unicode(s).strip(): 1 for s in fields})\n    else:\n        raise ValueError(\"invalid fields value\")\n    if as_dict:\n        return _fields\n    else:\n        return sorted(_fields.keys())", "response": "Given a list of fields return back a consistent form of the same."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef date_range(date, func='date'):\n    '''\n    Return back start and end dates given date string\n\n    :param date: metrique date (range) to apply to pql query\n\n    The tilde '~' symbol is used as a date range separated.\n\n    A tilde by itself will mean 'all dates ranges possible'\n    and will therefore search all objects irrelevant of it's\n    _end date timestamp.\n\n    A date on the left with a tilde but no date on the right\n    will generate a query where the date range starts\n    at the date provide and ends 'today'.\n    ie, from date -> now.\n\n    A date on the right with a tilde but no date on the left\n    will generate a query where the date range starts from\n    the first date available in the past (oldest) and ends\n    on the date provided.\n    ie, from beginning of known time -> date.\n\n    A date on both the left and right will be a simple date\n    range query where the date range starts from the date\n    on the left and ends on the date on the right.\n    ie, from date to date.\n    '''\n    if isinstance(date, basestring):\n        date = date.strip()\n    if not date:\n        return '_end == None'\n    if date == '~':\n        return ''\n\n    # don't include objects which have start EXACTLY on the\n    # date in question, since we're looking for objects\n    # which were true BEFORE the given date, not before or on.\n    before = lambda d: '_start < %s(\"%s\")' % (func, ts2dt(d) if d else None)\n    after = lambda d: '(_end >= %s(\"%s\") or _end == None)' % \\\n        (func, ts2dt(d) if d else None)\n    split = date.split('~')\n    # replace all occurances of 'T' with ' '\n    # this is used for when datetime is passed in\n    # like YYYY-MM-DDTHH:MM:SS instead of\n    #      YYYY-MM-DD HH:MM:SS as expected\n    # and drop all occurances of 'timezone' like substring\n    # FIXME: need to adjust (to UTC) for the timezone info we're dropping!\n    split = [re.sub('\\+\\d\\d:\\d\\d', '', d.replace('T', ' ')) for d in split]\n    if len(split) == 1:  # 'dt'\n        return '%s and %s' % (before(split[0]), after(split[0]))\n    elif split[0] in ['', None]:  # '~dt'\n        return before(split[1])\n    elif split[1] in ['', None]:  # 'dt~'\n        return after(split[0])\n    else:  # 'dt~dt'\n        return '%s and %s' % (before(split[1]), after(split[0]))", "response": "Return back start and end dates given a date string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a SQLAlchemy table instance generate a SQLAlchemy Query instance with the given parameters.", "response": "def parse(table, query=None, date=None, fields=None,\n          distinct=False, limit=None, alias=None):\n    '''\n    Given a SQLAlchemy Table() instance, generate a SQLAlchemy\n    Query() instance with the given parameters.\n\n    :param table: SQLAlchemy Table() instance\n    :param query: MQL query\n    :param date: metrique date range query\n    :param date: metrique date range query element\n    :param fields: list of field names to return as columns\n    :param distinct: apply DISTINCT to this query\n    :param limit: apply LIMIT to this query\n    :param alias: apply ALIAS AS to this query\n    '''\n    date = date_range(date)\n    limit = int(limit or -1)\n    if query and date:\n        query = '%s and %s' % (query, date)\n    elif date:\n        query = date\n    elif query:\n        pass\n    else:  # date is null, query is not\n        query = None\n\n    fields = parse_fields(fields=fields) or None\n    # we must pass in the table column objects themselves to ensure\n    # our bind / result processors are mapped properly\n    fields = fields if fields else table.columns\n\n    msg = 'parse(query=%s, fields=%s)' % (query, fields)\n    #msg = re.sub(' in \\[[^\\]]+\\]', ' in [...]', msg)\n    logger.debug(msg)\n    kwargs = {}\n    if query:\n        interpreter = MQLInterpreter(table)\n        query = interpreter.parse(query)\n        kwargs['whereclause'] = query\n    if distinct:\n        kwargs['distinct'] = distinct\n    query = select(fields, from_obj=table, **kwargs)\n    if limit >= 1:\n        query = query.limit(limit)\n    if alias:\n        query = query.alias(alias)\n    return query"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def async_enqueue_sync(self, func, *func_args):\n        '''\n        Enqueue an arbitrary synchronous function.\n        '''\n        worker = self.pick_sticky(0)  # just pick first always\n        args = (func,) + func_args\n        await worker.enqueue(enums.Task.FUNC, args)", "response": "Enqueue an arbitrary synchronous function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def async_enqueue_download(self, resource):\n        '''\n        Enqueue the download of the given foreign resource.\n        '''\n        worker = self.pick_sticky(resource.url_string)\n        await worker.enqueue(enums.Task.DOWNLOAD, (resource,))", "response": "Enqueue the download of the given foreign resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enqueue_convert(self, converter, from_resource, to_resource):\n        '''\n        Enqueue use of the given converter to convert to given\n        resources.\n\n        Deprecated: Use async version instead\n        '''\n        worker = self.pick_sticky(from_resource.url_string)\n        args = (converter, from_resource, to_resource)\n        coro = worker.enqueue(enums.Task.CONVERT, args)\n        asyncio.ensure_future(coro)", "response": "Enqueue conversion of resources from one resource to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def async_enqueue_convert(self, converter, from_, to):\n        '''\n        Enqueue use of the given converter to convert to given\n        from and to resources.\n        '''\n        worker = self.pick_sticky(from_.url_string)\n        args = (converter, from_, to)\n        await worker.enqueue(enums.Task.CONVERT, args)", "response": "Enqueue conversion of from_ to resources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def async_enqueue_multiconvert(self, url_string, to_type):\n        '''\n        Enqueue a multi-step conversion process, from the given URL string\n        (which is assumed to have been downloaded / resolved)\n        '''\n        worker = self.pick_sticky(url_string)\n        args = (url_string, to_type)\n        await worker.enqueue(enums.Task.MULTICONVERT, args)", "response": "Enqueue a multi - step conversion process from the given URL string\n        to_type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the in - plane strain assuming no lattice relaxation", "response": "def strain_in_plane(self, **kwargs):\n        '''\n        Returns the in-plane strain assuming no lattice relaxation, which\n        is positive for tensile strain and negative for compressive strain.\n        '''\n        if self._strain_out_of_plane is not None:\n            return ((self._strain_out_of_plane / -2.) *\n                    (self.unstrained.c11(**kwargs) /\n                     self.unstrained.c12(**kwargs)  )  )\n        else:\n            return 1 - self.unstrained.a(**kwargs) / self.substrate.a(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the out - of - plane strain assuming no lattice relaxation.", "response": "def strain_out_of_plane(self, **kwargs):\n        '''\n        Returns the out-of-plane strain assuming no lattice relaxation, which\n        is negative for tensile strain and positive for compressive strain.\n        This is the strain measured by X-ray diffraction (XRD) symmetric\n        omega-2theta scans.\n        '''\n        if self._strain_out_of_plane is not None:\n            return self._strain_out_of_plane\n        else:\n            return (-2 * self.unstrained.c12(**kwargs) /\n                    self.unstrained.c11(**kwargs) *\n                    self.strain_in_plane(**kwargs)      )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the substrate s lattice parameter.", "response": "def substrate_a(self, **kwargs):\n        '''\n        Returns the substrate's lattice parameter.\n        '''\n        if self.substrate is not None:\n            return self.substrate.a(**kwargs)\n        else:\n            return (self.unstrained.a(**kwargs) /\n                    (1. - self.strain_in_plane(**kwargs)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the strain - shifted conduction band offset assuming is True.", "response": "def CBO(self, **kwargs):\n        '''\n        Returns the strain-shifted conduction band offset (CBO), assuming\n        the strain affects all conduction band valleys equally.\n        '''\n        return self.unstrained.CBO(**kwargs) + self.CBO_strain_shift(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the strain - shifted Gamma - valley conduction band offset ( CBO )", "response": "def CBO_Gamma(self, **kwargs):\n        '''\n        Returns the strain-shifted Gamma-valley conduction band offset (CBO),\n        assuming the strain affects all conduction band valleys equally.\n        '''\n        return (self.unstrained.CBO_Gamma(**kwargs) +\n                self.CBO_strain_shift(**kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the strain - shifted L - valley conduction band offset for the given set of conduction band items.", "response": "def CBO_L(self, **kwargs):\n        '''\n        Returns the strain-shifted L-valley conduction band offset (CBO),\n        assuming the strain affects all conduction band valleys equally.\n        '''\n        return (self.unstrained.CBO_L(**kwargs) +\n                self.CBO_strain_shift(**kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CBO_X(self, **kwargs):\n        '''\n        Returns the strain-shifted X-valley conduction band offset (CBO),\n        assuming the strain affects all conduction band valleys equally.\n        '''\n        return (self.unstrained.CBO_X(**kwargs) +\n                self.CBO_strain_shift(**kwargs))", "response": "Returns the strain - shifted X - valley conduction band offset for the given set of conduction band valleys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Eg(self, **kwargs):\n        '''\n        Returns the strain-shifted bandgap, ``Eg``.\n        '''\n        return self.unstrained.Eg(**kwargs) + self.Eg_strain_shift(**kwargs)", "response": "Returns the strain - shifted bandgap Eg."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tokens(self):\n        for subsegment_or_token in self:\n            if isinstance(subsegment_or_token, Segment):\n                subsegment = subsegment_or_token\n                for token in subsegment.tokens():\n                    yield token\n            else:\n                token = subsegment_or_token\n                yield token", "response": "generator : the tokens in this segment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle errors and renders them as a JSON message.", "response": "def _api_on_write_error(self, status_code, **kwargs):\n        \"\"\"\n        Catches errors and renders it as a JSON message. Adds the traceback if\n        debug is enabled.\n        \"\"\"\n\n        return_error = { \"code\": self.get_status() }\n        exc_info = kwargs.get(\"exc_info\")\n\n        if exc_info and isinstance(exc_info[1], oz.json_api.ApiError):\n            return_error[\"error\"] = exc_info[1].message\n        else:\n            return_error[\"error\"] = API_ERROR_CODE_MAP.get(self.get_status(), \"Unknown error\")\n\n        if oz.settings.get(\"debug\"):\n            return_error[\"trace\"] = \"\".join(traceback.format_exception(*exc_info))\n\n        self.finish(return_error)\n        return oz.break_trigger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a response JSON ( P ) message", "response": "def respond(self, obj):\n        \"\"\"Gives a response JSON(P) message\"\"\"\n\n        # Get the callback argument if JSONP is allowed\n        callback = self.get_argument(\"callback\", None) if oz.settings[\"allow_jsonp\"] else None\n\n        # We're pretty strict with what callback names are allowed, just in case\n        if callback and not CALLBACK_VALIDATOR.match(callback):\n            raise oz.json_api.ApiError(\"Invalid callback identifier - only functions with ASCII characters are allowed\")\n\n        # Provide the response in a different manner depending on whether a\n        # JSONP callback is specified\n        json = escape.json_encode(obj)\n\n        if callback:\n            self.set_header(\"Content-Type\", \"application/javascript; charset=UTF-8\")\n            self.finish(\"%s(%s)\" % (callback, json))\n        else:\n            self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n            self.finish(json)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the JSON body of the request", "response": "def body(self):\n        \"\"\"Gets the JSON body of the request\"\"\"\n\n        if self._decoded_body == None:\n            # Try to decode the JSON body. But raise an error if the\n            # content-type is unexpected, or the JSON is invalid.\n\n            raw_content_type = self.request.headers.get(\"content-type\") or \"\"\n            content_type = raw_content_type.split(\";\")[0].strip().lower()\n\n            if content_type == \"application/json\":\n                try:\n                    self._decoded_body = escape.json_decode(self.request.body)\n                except:\n                    raise oz.json_api.ApiError(\"Bad JSON body\")\n            else:\n                raise oz.json_api.ApiError(\"JSON body expected\")\n\n        return self._decoded_body"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing and return the identifier", "response": "def _get_id(self):\n        \"\"\"Construct and return the identifier\"\"\"\n        return ''.join(map(str,\n                           filter(is_not_None,\n                                  [self.Prefix, self.Name])))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on(self, val=None):\n        if val is False:\n            raise ParameterError(\"Turning the ValuedParameter on with value \"\n                                 \"False is the same as turning it off. Use \"\n                                 \"another value.\")\n        elif self.IsPath:\n            self.Value = FilePath(val)\n        else:\n            self.Value = val", "response": "Turns the MixedParameter ON by setting its Value to val\n                                ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_assets(self):\n        '''\n        Return a flat list of absolute paths to all assets required by this\n        viewer\n        '''\n        return sum([\n            [self.prefix_asset(viewer, relpath) for relpath in viewer.assets]\n            for viewer in self.viewers\n        ], [])", "response": "Return a flat list of absolute paths to all assets required by thisCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_resource(self):\n        '''\n        Returns a BytesResource to build the viewers JavaScript\n        '''\n        # Basename could be used for controlling caching\n        # basename = 'viewers_%s' % settings.get_cache_string()\n        node_packages = self.get_node_packages()\n        # sort_keys is essential to ensure resulting string is\n        # deterministic (and thus hashable)\n        viewers_data_str = json.dumps(node_packages, sort_keys=True)\n        viewers_data = viewers_data_str.encode('utf8')\n        viewers_resource = ForeignBytesResource(\n            viewers_data,\n            extension=VIEWER_EXT,\n            # basename=basename,\n        )\n        return viewers_resource", "response": "Returns a BytesResource to build the viewers JavaScript\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hmac_sha1_digest(secret, resource_url, target_type, api_key=None):\n    '''\n    Utilize hmac module to hash a secret, a string specifying a resource URL,\n    and a string specifying a target type into a (string) hex digest.\n    '''\n    # Normalize and sanitize input resource URL and target type, and then\n    # convert to bytes\n    target_type_bytes = str(TypeString(target_type)).encode('utf8')\n    resource_url_bytes = str(ResourceURL(resource_url)).encode('utf8')\n\n    # Create new hmac digest, optionally including an optional public api key\n    hm = hmac.new(secret.encode('utf8'), digestmod=hashlib.sha1)\n    if api_key:\n        hm.update(api_key.encode('utf8'))\n    hm.update(target_type_bytes)\n    hm.update(resource_url_bytes)\n    return hm.hexdigest()", "response": "Utilize hmac module to hash a secret resource URL and target type into a hex digest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init(provider=None):\n    if os.path.exists(DEPLOY_YAML):\n        _yellow(\"\\nIt looks like you've already gone through the questionnaire.\")\n        cont = prompt(\"Do you want to go through it again and overwrite the current one?\", default=\"No\")\n\n        if cont.strip().lower() == \"no\":\n            return None\n    _green(\"\\nWelcome to the django-deployer!\")\n    _green(\"\\nWe need to ask a few questions in order to set up your project to be deployed to a PaaS provider.\")\n\n    # TODO: identify the project dir based on where we find the settings.py or urls.py\n\n    django_settings = prompt(\n        \"* What is your Django settings module?\",\n        default=\"settings\",\n        validate=_validate_django_settings\n    )\n\n    managepy = prompt(\n        \"* Where is your manage.py file?\",\n        default=\"./manage.py\",\n        validate=_validate_managepy\n    )\n\n    requirements = prompt(\n        \"* Where is your requirements.txt file?\",\n        default=\"requirements.txt\",\n        validate=_validate_requirements\n    )\n    # TODO: confirm that the file exists\n    # parse the requirements file and warn the user about best practices:\n    #   Django==1.4.1\n    #   psycopg2 if they selected PostgreSQL\n    #   MySQL-python if they selected MySQL\n    #   South for database migrations\n    #   dj-database-url\n\n    pyversion = prompt(\"* What version of Python does your app need?\", default=\"Python2.7\")\n\n    # TODO: get these values by reading the settings.py file\n    static_url = prompt(\"* What is your STATIC_URL?\", default=\"/static/\")\n    media_url = prompt(\"* What is your MEDIA_URL?\", default=\"/media/\")\n\n    if not provider:\n        provider = prompt(\"* Which provider would you like to deploy to (dotcloud, appengine, stackato, openshift)?\",\n                          validate=_validate_providers)\n\n    # Where to place the provider specific questions\n    site = {}\n    additional_site = {}\n\n    if provider == \"appengine\":\n        applicationid = prompt(\"* What's your Google App Engine application ID (see https://appengine.google.com/)?\", validate=r'.+')\n        instancename = prompt(\"* What's the full instance ID of your Cloud SQL instance\\n\"\n                              \"(should be in format \\\"projectid:instanceid\\\" found at https://code.google.com/apis/console/)?\", validate=r'.+:.+')\n        databasename = prompt(\"* What's your database name?\", validate=r'.+')\n        sdk_location = prompt(\"* Where is your Google App Engine SDK location?\",\n                              default=\"/usr/local/google_appengine\",\n                              validate=r'.+'  # TODO: validate that this path exists\n                              )\n\n        additional_site.update({\n            # quotes for the yaml issue\n            'application_id': applicationid,\n            'instancename': instancename,\n            'databasename': databasename,\n            'sdk_location': sdk_location,\n        })\n\n        # only option with Google App Engine is MySQL, so we'll just hardcode it\n        site = {\n            'database': 'MySQL'\n        }\n\n    elif provider == \"openshift\":\n        application_name = prompt(\"* What is your openshift application name?\")\n\n        site = {\n            'application_name': application_name\n        }\n\n    else:\n        database = prompt(\"* What database does your app use?\", default=\"PostgreSQL\")\n        site = {\n            'database': database,\n        }\n\n    # TODO: add some validation that the admin password is valid\n    # TODO: let the user choose the admin username instead of hardcoding it to 'admin'\n    admin_password = prompt(\"* What do you want to set as the admin password?\",\n                            validate=_validate_admin_password\n                            )\n\n    import random\n    SECRET_KEY = ''.join([random.SystemRandom().choice('abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)') for i in range(50)])\n    SECRET_KEY = \"'\" + SECRET_KEY + \"'\"\n\n    site.update({\n        'pyversion': pyversion,\n        'django_settings': django_settings,\n        'managepy': managepy,\n        'requirements': requirements,\n        'static_url': static_url,\n        'media_url': media_url,\n        'provider': provider,\n        'admin_password': admin_password,\n        'secret_key': SECRET_KEY,\n    })\n\n    site.update(additional_site)\n\n    _create_deploy_yaml(site)\n\n    return site", "response": "Runs through a questionnaire to set up your project s deploy settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(provider=None):\n    site = init(provider)\n    if not site:\n        site = yaml.safe_load(_read_file(DEPLOY_YAML))\n\n    provider_class = PROVIDERS[site['provider']]\n    provider_class.init(site)", "response": "Creates the provider config files needed to deploy your project"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deploy(provider=None):\n    if os.path.exists(DEPLOY_YAML):\n        site = yaml.safe_load(_read_file(DEPLOY_YAML))\n\n    provider_class = PROVIDERS[site['provider']]\n    provider_class.deploy()", "response": "Deploy your project to the specified provider"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to an instance of IDA running our server. py.", "response": "def ida_connect(host='localhost', port=18861, retry=10):\n    \"\"\"\n    Connect to an instance of IDA running our server.py.\n\n    :param host:        The host to connect to\n    :param port:        The port to connect to\n    :param retry:       How many times to try after errors before giving up\n    \"\"\"\n    for i in range(retry):\n        try:\n            LOG.debug('Connectint to %s:%d, try %d...', host, port, i + 1)\n            link = rpyc_classic.connect(host, port)\n            link.eval('2 + 2')\n        except socket.error:\n            time.sleep(1)\n            continue\n        else:\n            LOG.debug('Connected to %s:%d', host, port)\n            return link\n\n    raise IDALinkError(\"Could not connect to %s:%d after %d tries\" % (host, port, retry))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ida_spawn(ida_binary, filename, port=18861, mode='oneshot',\n              processor_type=None, logfile=None):\n    \"\"\"\n    Open IDA on the the file we want to analyse.\n\n    :param ida_binary:  The binary name or path to ida\n    :param filename:    The filename to open in IDA\n    :param port:        The port on which to serve rpc from ida\n    :param mode:        The server mode. \"oneshot\" to close ida when the connection is closed, or\n                        \"threaded\" to run IDA visible to the user and allow multiple connections\n    :param processor_type:\n                        Which processor IDA should analyze this binary as, e.g. \"metapc\". If not\n                        provided, IDA will guess.\n    :param logfile:     The file to log IDA's output to. Default /tmp/idalink-{port}.log\n    \"\"\"\n    ida_progname = _which(ida_binary)\n    if ida_progname is None:\n        raise IDALinkError('Could not find executable %s' % ida_binary)\n\n    if mode not in ('oneshot', 'threaded'):\n        raise ValueError(\"Bad mode %s\" % mode)\n\n    if logfile is None:\n        logfile = LOGFILE.format(port=port)\n\n    ida_realpath = os.path.expanduser(ida_progname)\n    file_realpath = os.path.realpath(os.path.expanduser(filename))\n    server_script = os.path.join(MODULE_DIR, 'server.py')\n\n    LOG.info('Launching IDA (%s) on %s, listening on port %d, logging to %s',\n             ida_realpath, file_realpath, port, logfile)\n\n    env = dict(os.environ)\n    if mode == 'oneshot':\n        env['TVHEADLESS'] = '1'\n\n    if sys.platform == \"darwin\":\n        # If we are running in a virtual environment, which we should, we need\n        # to insert the python lib into the launched process in order for IDA\n        # to not default back to the Apple-installed python because of the use\n        # of paths in library identifiers on macOS.\n        if \"VIRTUAL_ENV\" in os.environ:\n            env['DYLD_INSERT_LIBRARIES'] = os.environ['VIRTUAL_ENV'] + '/.Python'\n\n    # The parameters are:\n    # -A     Automatic mode\n    # -S     Run a script (our server script)\n    # -L     Log all output to our logfile\n    # -p     Set the processor type\n\n    command = [\n        ida_realpath,\n        '-A',\n        '-S%s %d %s' % (server_script, port, mode),\n        '-L%s' % logfile,\n    ]\n    if processor_type is not None:\n        command.append('-p%s' % processor_type)\n    command.append(file_realpath)\n\n    LOG.debug('IDA command is %s', ' '.join(\"%s\" % s for s in command))\n    return subprocess.Popen(command, env=env)", "response": "Launch IDA on the given binary and return a IDA object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expect_exact(self, *args, **kwargs):\r\n        response = self._recvline()\r\n        self.before = response.strip()", "response": "This does not attempt to duplicate the expect_exact API but just sets self. before to the latest response line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        for category in orm['document_library.DocumentCategory'].objects.all():\n            for trans_old in orm['document_library.DocumentCategoryTitle'].objects.filter(category=category):\n                orm['document_library.DocumentCategoryTranslation'].objects.create(\n                    master=category,\n                    language_code=trans_old.language,\n                    title=trans_old.title,\n                )\n\n        for document in orm['document_library.Document'].objects.all():\n            for trans_old in orm['document_library.DocumentTitle'].objects.filter(document=document):\n                orm['document_library.DocumentTranslation'].objects.create(\n                    master=document,\n                    language_code=trans_old.language,\n                    title=trans_old.title,\n                    description=trans_old.description,\n                    filer_file=trans_old.filer_file,\n                    thumbnail=trans_old.thumbnail,\n                    copyright_notice=trans_old.copyright_notice,\n                    is_published=trans_old.is_published,\n                    meta_description=trans_old.meta_description,\n                )", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cache_busting_scan(*prefixes):\n\n    redis = oz.redis.create_connection()\n    pipe = redis.pipeline()\n\n    # Get all items that match any of the patterns. Put it in a set to\n    # prevent duplicates.\n    if oz.settings[\"s3_bucket\"]:\n        bucket = oz.aws_cdn.get_bucket()\n        matches = set([oz.aws_cdn.S3File(key) for prefix in prefixes for key in bucket.list(prefix)])\n    else:\n        matches = set([])\n        static_path = oz.settings[\"static_path\"]\n        \n        for root, _, filenames in os.walk(static_path):\n            for filename in filenames:\n                path = os.path.relpath(os.path.join(root, filename), static_path)\n\n                for prefix in prefixes:\n                    if path.startswith(prefix):\n                        matches.add(oz.aws_cdn.LocalFile(static_path, path))\n                        break\n\n    # Set the cache busters\n    for f in matches:\n        file_hash = f.hash()\n        print(file_hash, f.path())\n        oz.aws_cdn.set_cache_buster(pipe, f.path(), file_hash)\n\n    pipe.execute()", "response": "Re - generates the cache buster values for all files with the specified prefixes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a BWA index from an input fasta file.", "response": "def create_bwa_index_from_fasta_file(fasta_in, params=None):\n    \"\"\"Create a BWA index from an input fasta file.\n\n    fasta_in: the input fasta file from which to create the index\n    params: dict of bwa index specific paramters\n\n    This method returns a dictionary where the keys are the various\n    output suffixes (.amb, .ann, .bwt, .pac, .sa) and the values\n    are open file objects.\n\n    The index prefix will be the same as fasta_in, unless the -p parameter\n    is passed in params.\n    \"\"\"\n    if params is None:\n        params = {}\n\n    # Instantiate the app controller\n    index = BWA_index(params)\n\n    # call the application, passing the fasta file in\n    results = index({'fasta_in': fasta_in})\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assign_reads_to_database(query, database_fasta, out_path, params=None):\n    if params is None:\n        params = {}\n\n    # set the output path\n    params['-f'] = out_path\n\n    # if the algorithm is not specified in the params dict, or the algorithm\n    # is not recognized, raise an exception\n    if 'algorithm' not in params:\n        raise InvalidArgumentApplicationError(\"Must specify which algorithm to\"\n                                              \" use ('bwa-short' or 'bwasw')\")\n    elif params['algorithm'] not in ('bwa-short', 'bwasw'):\n        raise InvalidArgumentApplicationError(\"Unknown algorithm '%s' Please \"\n                                              \"enter either 'bwa-short' or \"\n                                              \"'bwasw'.\" % params['algorithm'])\n\n    # if the temp directory is not specified, assume /tmp\n    if 'temp_dir' not in params:\n        params['temp_dir'] = '/tmp'\n\n    # if the algorithm is bwa-short, we must build use bwa aln to get an sai\n    # file before calling bwa samse on that sai file, so we need to know how\n    # to run bwa aln. Therefore, we must ensure there's an entry containing\n    # those parameters\n    if params['algorithm'] == 'bwa-short':\n        if 'aln_params' not in params:\n            raise InvalidArgumentApplicationError(\"With bwa-short, need to \"\n                                                  \"specify a key 'aln_params' \"\n                                                  \"and its value, a dictionary\"\n                                                  \" to pass to bwa aln, since\"\n                                                  \" bwa aln is an intermediate\"\n                                                  \" step when doing \"\n                                                  \"bwa-short.\")\n\n    # we have this params dict, with \"algorithm\" and \"temp_dir\", etc which are\n    # not for any of the subcommands, so make a new params dict that is the\n    # same as the original minus these addendums\n    subcommand_params = {}\n    for k, v in params.iteritems():\n        if k not in ('algorithm', 'temp_dir', 'aln_params'):\n            subcommand_params[k] = v\n\n    # build index from database_fasta\n    # get a temporary file name that is not in use\n    _, index_prefix = mkstemp(dir=params['temp_dir'], suffix='')\n\n    create_bwa_index_from_fasta_file(database_fasta, {'-p': index_prefix})\n\n    # if the algorithm is bwasw, things are pretty simple. Just instantiate\n    # the proper controller and set the files\n    if params['algorithm'] == 'bwasw':\n        bwa = BWA_bwasw(params=subcommand_params)\n        files = {'prefix': index_prefix, 'query_fasta': query}\n\n    # if the algorithm is bwa-short, it's not so simple\n    elif params['algorithm'] == 'bwa-short':\n        # we have to call bwa_aln to get the sai file needed for samse\n        # use the aln_params we ensured we had above\n        bwa_aln = BWA_aln(params=params['aln_params'])\n        aln_files = {'prefix': index_prefix, 'fastq_in': query}\n        # get the path to the sai file\n        sai_file_path = bwa_aln(aln_files)['output'].name\n\n        # we will use that sai file to run samse\n        bwa = BWA_samse(params=subcommand_params)\n        files = {'prefix': index_prefix, 'sai_in': sai_file_path,\n                 'fastq_in': query}\n\n    # run which ever app controller we decided was correct on the files\n    # we set up\n    result = bwa(files)\n\n    # they both return a SAM file, so return that\n    return result['output']", "response": "Assign a set of query sequences to a reference database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assign_dna_reads_to_dna_database(query_fasta_fp, database_fasta_fp, out_fp,\n                                     params={}):\n    \"\"\"Wraps assign_reads_to_database, setting various parameters.\n\n    The default settings are below, but may be overwritten and/or added to\n    using the params dict:\n\n    algorithm:      bwasw\n    \"\"\"\n    my_params = {'algorithm': 'bwasw'}\n    my_params.update(params)\n\n    result = assign_reads_to_database(query_fasta_fp, database_fasta_fp,\n                                      out_fp, my_params)\n\n    return result", "response": "Wraps assign_reads_to_database and assigns the reads to the DNA database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_arguments(self):\n        for k, v in self.Parameters.iteritems():\n            if self.Parameters[k].isOn():\n                if k in self._valid_arguments:\n                    if not self._valid_arguments[k](v.Value):\n                        error_message = 'Invalid argument (%s) ' % v.Value\n                        error_message += 'for parameter %s\\n' % k\n                        raise InvalidArgumentApplicationError(error_message)", "response": "Sanity check the arguments passed in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the full command string for the current locale.", "response": "def _get_base_command(self):\n        \"\"\" Returns the full command string\n\n        Overridden here because there are positional arguments (specifically\n        the input and output files).\n        \"\"\"\n        command_parts = []\n        # Append a change directory to the beginning of the command to change\n        # to self.WorkingDir before running the command\n        # WorkingDir should be in quotes -- filenames might contain spaces\n        cd_command = ''.join(['cd ', str(self.WorkingDir), ';'])\n        if self._command is None:\n            raise ApplicationError('_command has not been set.')\n        command = self._command\n        # also make sure there's a subcommand!\n        if self._subcommand is None:\n            raise ApplicationError('_subcommand has not been set.')\n        subcommand = self._subcommand\n        # sorting makes testing easier, since the options will be written out\n        # in alphabetical order. Could of course use option parsing scripts\n        # in cogent for this, but this works as well.\n        parameters = sorted([str(x) for x in self.Parameters.values()\n                            if str(x)])\n        synonyms = self._synonyms\n\n        command_parts.append(cd_command)\n        command_parts.append(command)\n        # add in subcommand\n        command_parts.append(subcommand)\n        command_parts += parameters\n        # add in the positional arguments in the correct order\n        for k in self._input_order:\n            # this check is necessary to account for optional positional\n            # arguments, such as the mate file for bwa bwasw\n            # Note that the input handler will ensure that all required\n            # parameters have valid values\n            if k in self._input:\n                command_parts.append(self._input[k])\n\n        return self._command_delimiter.join(command_parts).strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _input_as_dict(self, data):\n        # clear self._input; ready to receive new input and output files\n        self._input = {}\n        # Check that the arguments to the\n        # subcommand-specific parameters are valid\n        self.check_arguments()\n\n        # Ensure that we have all required input (file I/O)\n        for k in self._input_order:\n            # N.B.: optional positional arguments begin with underscore (_)!\n            # (e.g., see _mate_in for bwa bwasw)\n            if k[0] != '_' and k not in data:\n                raise MissingRequiredArgumentApplicationError(\"Missing \"\n                                                              \"required \"\n                                                              \"input %s\" % k)\n\n        # Set values for input and output files\n        for k in data:\n            # check for unexpected keys in the dict\n            if k not in self._input_order:\n                error_message = \"Invalid input arguments (%s)\\n\" % k\n                error_message += \"Valid keys are: %s\" % repr(self._input_order)\n                raise InvalidArgumentApplicationError(error_message + '\\n')\n\n            # check for absolute paths\n            if not isabs(data[k][0]):\n                raise InvalidArgumentApplicationError(\"Only absolute paths \"\n                                                      \"allowed.\\n%s\" %\n                                                      repr(data))\n            self._input[k] = data[k]\n\n        # if there is a -f option to specify an output file, force the user to\n        # use it (otherwise things to to stdout)\n        if '-f' in self.Parameters and not self.Parameters['-f'].isOn():\n            raise InvalidArgumentApplicationError(\"Please specify an output \"\n                                                  \"file with -f\")\n\n        return ''", "response": "Takes dictionary that sets input and output files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_result_paths(self, data):\n\n        # determine the names of the files. The name will be the same as the\n        # input fasta file unless overridden with the -p option\n        if self.Parameters['-p'].isOn():\n            prefix = self.Parameters['-p'].Value\n        else:\n            prefix = data['fasta_in']\n\n        # the 5 output file suffixes\n        suffixes = ['.amb', '.ann', '.bwt', '.pac', '.sa']\n        out_files = {}\n        for suffix in suffixes:\n            out_files[suffix] = ResultPath(prefix + suffix, IsWritten=True)\n\n        return out_files", "response": "Get the output files for a run of bwa index."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all_paths_from(self, start, seen=None):\n        '''\n        Return a list of all paths to all nodes from a given start node\n        '''\n        if seen is None:\n            seen = frozenset()\n        results = [(0, (start, ))]\n        if start in seen or start not in self.edges:\n            return results\n        seen = seen | frozenset((start,))\n        for node, edge_weight in self.edges[start].items():\n            for subpath_weight, subpath in self.get_all_paths_from(node, seen):\n                total_weight = edge_weight + subpath_weight\n                full_path = (start, ) + subpath\n                results.append((total_weight, full_path))\n        return tuple(results)", "response": "Return a list of all paths to all nodes from a given start node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies a sequence of operations to tokens -- copies tokens from a_tokens and b_tokens according to operations.", "response": "def apply(operations, a_tokens, b_tokens):\n    \"\"\"\n    Applies a sequences of operations to tokens -- copies tokens from\n    `a_tokens` and `b_tokens` according to `operations`.\n\n    :Parameters:\n        operations : sequence of :~class:`deltas.Operation`\n            Operations to perform\n        a_tokens : list of `comparable`\n            Starting sequence of comparable tokens\n        b_tokens : list of `comparable`\n            Ending list of comparable tokens\n\n    :Returns:\n        A new list of tokens\n    \"\"\"\n    for operation in operations:\n\n        if isinstance(operation, Equal):\n            #print(\"Equal: {0}\".format(str(a_tokens[operation.a1:operation.a2])))\n            for t in a_tokens[operation.a1:operation.a2]: yield t\n\n        elif isinstance(operation, Insert):\n            #print(\"Insert: {0}\".format(str(b_tokens[operation.b1:operation.b2])))\n            for t in b_tokens[operation.b1:operation.b2]: yield t\n\n        elif isinstance(operation, Delete):\n            #print(\"Delete: {0}\".format(str(a_tokens[operation.a1:operation.a2])))\n            pass\n\n        else:\n            raise TypeError(\"Unexpected operation type \" + \\\n                            \"{0}\".format(type(operation)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a job block for the given CPU width and time height.", "response": "def create_job(cpu_width, time_height):\n    \"\"\"\n    :param cpu_width: number of cpus\n    :param time_height: amount of time\n    :return: the instantiated JobBlock object\n    \"\"\"\n\n    shell_command = stress_string.format(cpu_width, time_height)\n    job = JobBlock(cpu_width, time_height)\n    job.set_job(subprocess.call, shell_command, shell=True)\n    return job"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses cmbuild to construct a CM file from an alignment and a structure string.", "response": "def cmbuild_from_alignment(aln, structure_string, refine=False, \\\n    return_alignment=False,params=None):\n    \"\"\"Uses cmbuild to build a CM file given an alignment and structure string.\n\n        - aln: an Alignment object or something that can be used to construct\n            one.  All sequences must be the same length.\n        - structure_string: vienna structure string representing the consensus\n            stucture for the sequences in aln.  Must be the same length as the\n            alignment.\n        - refine: refine the alignment and realign before building the cm.\n            (Default=False)\n        - return_alignment: Return (in Stockholm format) alignment file used to\n            construct the CM file.  This will either be the original alignment\n            and structure string passed in, or the refined alignment if --refine\n            was used. (Default=False)\n            - Note.  This will be a string that can either be written to a file\n                or parsed.\n    \"\"\"\n    aln = Alignment(aln)\n    if len(structure_string) != aln.SeqLen:\n        raise ValueError, \"\"\"Structure string is not same length as alignment.  Structure string is %s long. Alignment is %s long.\"\"\"%(len(structure_string),\\\n        aln.SeqLen)\n    else:\n        struct_dict = {'SS_cons':structure_string}\n    #Make new Cmbuild app instance.\n    app = Cmbuild(InputHandler='_input_as_paths',WorkingDir='/tmp',\\\n        params=params)\n\n    #turn on refine flag if True.\n    if refine:\n        _, tmp_file = mkstemp(dir=app.WorkingDir)\n        app.Parameters['--refine'].on(tmp_file)\n\n    #Get alignment in Stockholm format\n    aln_file_string = stockholm_from_alignment(aln,GC_annotation=struct_dict)\n\n    #get path to alignment filename\n    aln_path = app._input_as_multiline_string(aln_file_string)\n    cm_path = aln_path.split('.txt')[0]+'.cm'\n    app.Parameters['-n'].on(cm_path)\n\n    filepaths = [cm_path,aln_path]\n\n    res = app(filepaths)\n\n    cm_file = res['CmFile'].read()\n\n    if return_alignment:\n        #If alignment was refined, return refined alignment and structure,\n        # otherwise return original alignment and structure.\n        if refine:\n            aln_file_string = res['Refined'].read()\n        res.cleanUp()\n        return cm_file, aln_file_string\n    #Just return cm_file\n    else:\n        res.cleanUp()\n        return cm_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmbuild_from_file(stockholm_file_path, refine=False,return_alignment=False,\\\n    params=None):\n    \"\"\"Uses cmbuild to build a CM file given a stockholm file.\n\n        - stockholm_file_path: a path to a stockholm file.  This file should\n            contain a multiple sequence alignment formated in Stockholm format.\n            This must contain a sequence structure line:\n                #=GC SS_cons <structure string>\n        - refine: refine the alignment and realign before building the cm.\n            (Default=False)\n        - return_alignment: Return alignment and structure string used to\n            construct the CM file.  This will either be the original alignment\n            and structure string passed in, or the refined alignment if\n            --refine was used. (Default=False)\n    \"\"\"\n    #get alignment and structure string from stockholm file.\n    info, aln, structure_string = \\\n        list(MinimalRfamParser(open(stockholm_file_path,'U'),\\\n            seq_constructor=ChangedSequence))[0]\n\n    #call cmbuild_from_alignment.\n    res = cmbuild_from_alignment(aln, structure_string, refine=refine, \\\n        return_alignment=return_alignment,params=params)\n    return res", "response": "Uses cmbuild to build a CM file given a stockholm file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses cmbuild to build a CM file and cmalign to build a CM file.", "response": "def cmalign_from_alignment(aln, structure_string, seqs, moltype=DNA,\\\n    include_aln=True,refine=False, return_stdout=False,params=None,\\\n    cmbuild_params=None):\n    \"\"\"Uses cmbuild to build a CM file, then cmalign to build an alignment.\n\n        - aln: an Alignment object or something that can be used to construct\n            one.  All sequences must be the same length.\n        - structure_string: vienna structure string representing the consensus\n            stucture for the sequences in aln.  Must be the same length as the\n            alignment.\n        - seqs: SequenceCollection object or something that can be used to\n            construct one, containing unaligned sequences that are to be aligned\n            to the aligned sequences in aln.\n        - moltype: Cogent moltype object.  Must be RNA or DNA.\n        - include_aln: Boolean to include sequences in aln in final alignment.\n            (Default=True)\n        - refine: refine the alignment and realign before building the cm.\n            (Default=False)\n        - return_stdout: Boolean to return standard output from infernal.  This\n            includes alignment and structure bit scores and average\n            probabilities for each sequence. (Default=False)\n    \"\"\"\n    #NOTE: Must degap seqs or Infernal well seg fault!\n    seqs = SequenceCollection(seqs,MolType=moltype).degap()\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seqs.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n    cm_file, aln_file_string = cmbuild_from_alignment(aln, structure_string,\\\n        refine=refine,return_alignment=True,params=cmbuild_params)\n\n    if params is None:\n        params = {}\n    params.update({MOLTYPE_MAP[moltype]:True})\n\n    app = Cmalign(InputHandler='_input_as_paths',WorkingDir='/tmp',\\\n        params=params)\n    app.Parameters['--informat'].on('FASTA')\n\n    #files to remove that aren't cleaned up by ResultPath object\n    to_remove = []\n    #turn on --withali flag if True.\n    if include_aln:\n        app.Parameters['--withali'].on(\\\n            app._tempfile_as_multiline_string(aln_file_string))\n        #remove this file at end\n        to_remove.append(app.Parameters['--withali'].Value)\n\n    seqs_path = app._input_as_multiline_string(int_map.toFasta())\n    cm_path = app._tempfile_as_multiline_string(cm_file)\n\n    #add cm_path to to_remove\n    to_remove.append(cm_path)\n    paths = [cm_path,seqs_path]\n\n    _, tmp_file = mkstemp(dir=app.WorkingDir)\n    app.Parameters['-o'].on(tmp_file)\n\n    res = app(paths)\n\n    info, aligned, struct_string = \\\n        list(MinimalRfamParser(res['Alignment'].readlines(),\\\n            seq_constructor=SEQ_CONSTRUCTOR_MAP[moltype]))[0]\n\n    #Make new dict mapping original IDs\n    new_alignment={}\n    for k,v in aligned.NamedSeqs.items():\n        new_alignment[int_keys.get(k,k)]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n\n    std_out = res['StdOut'].read()\n    #clean up files\n    res.cleanUp()\n    for f in to_remove: remove(f)\n\n    if return_stdout:\n        return new_alignment, struct_string, std_out\n    else:\n        return new_alignment, struct_string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing cmalign to align sequences in cm_file_path with the given moltype.", "response": "def cmalign_from_file(cm_file_path, seqs, moltype=DNA, alignment_file_path=None,\\\n    include_aln=False,return_stdout=False,params=None):\n    \"\"\"Uses cmalign to align seqs to alignment in cm_file_path.\n\n        - cm_file_path: path to the file created by cmbuild, containing aligned\n            sequences. This will be used to align sequences in seqs.\n        - seqs: unaligned sequendes that are to be aligned to the sequences in\n            cm_file.\n        - moltype: cogent.core.moltype object.  Must be DNA or RNA\n        - alignment_file_path: path to stockholm alignment file used to create\n            cm_file.\n            __IMPORTANT__: This MUST be the same file used by cmbuild\n            originally.  Only need to pass in this file if include_aln=True.\n            This helper function will NOT check if the alignment file is correct\n            so you must use it correctly.\n        - include_aln: Boolean to include sequences in aln_file in final\n            alignment. (Default=False)\n        - return_stdout: Boolean to return standard output from infernal.  This\n            includes alignment and structure bit scores and average\n            probabilities for each sequence. (Default=False)\n    \"\"\"\n    #NOTE: Must degap seqs or Infernal well seg fault!\n    seqs = SequenceCollection(seqs,MolType=moltype).degap()\n\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seqs.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n    if params is None:\n        params = {}\n    params.update({MOLTYPE_MAP[moltype]:True})\n\n    app = Cmalign(InputHandler='_input_as_paths',WorkingDir='/tmp',\\\n        params=params)\n    app.Parameters['--informat'].on('FASTA')\n\n    #turn on --withali flag if True.\n    if include_aln:\n        if alignment_file_path is None:\n            raise DataError, \"\"\"Must have path to alignment file used to build CM if include_aln=True.\"\"\"\n        else:\n            app.Parameters['--withali'].on(alignment_file_path)\n\n    seqs_path = app._input_as_multiline_string(int_map.toFasta())\n    paths = [cm_file_path,seqs_path]\n\n    _, tmp_file = mkstemp(dir=app.WorkingDir)\n    app.Parameters['-o'].on(tmp_file)\n    res = app(paths)\n\n    info, aligned, struct_string = \\\n        list(MinimalRfamParser(res['Alignment'].readlines(),\\\n            seq_constructor=SEQ_CONSTRUCTOR_MAP[moltype]))[0]\n\n\n    #Make new dict mapping original IDs\n    new_alignment={}\n    for k,v in aligned.items():\n        new_alignment[int_keys.get(k,k)]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    std_out = res['StdOut'].read()\n    res.cleanUp()\n    if return_stdout:\n        return new_alignment, struct_string, std_out\n    else:\n        return new_alignment, struct_string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse cmbuild to build a CM file and cmsearch to find homologs for the given alignment.", "response": "def cmsearch_from_alignment(aln, structure_string, seqs, moltype, cutoff=0.0,\\\n    refine=False,params=None):\n    \"\"\"Uses cmbuild to build a CM file, then cmsearch to find homologs.\n\n        - aln: an Alignment object or something that can be used to construct\n            one.  All sequences must be the same length.\n        - structure_string: vienna structure string representing the consensus\n            stucture for the sequences in aln.  Must be the same length as the\n            alignment.\n        - seqs: SequenceCollection object or something that can be used to\n            construct one, containing unaligned sequences that are to be\n            searched.\n        - moltype: cogent.core.moltype object.  Must be DNA or RNA\n        - cutoff: bitscore cutoff.  No sequences < cutoff will be kept in\n            search results. (Default=0.0).  Infernal documentation suggests\n            a cutoff of log2(number nucleotides searching) will give most\n            likely true homologs.\n        - refine: refine the alignment and realign before building the cm.\n            (Default=False)\n    \"\"\"\n    #NOTE: Must degap seqs or Infernal well seg fault!\n    seqs = SequenceCollection(seqs,MolType=moltype).degap()\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seqs.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n    cm_file, aln_file_string = cmbuild_from_alignment(aln, structure_string,\\\n        refine=refine,return_alignment=True)\n\n    app = Cmsearch(InputHandler='_input_as_paths',WorkingDir='/tmp',\\\n        params=params)\n    app.Parameters['--informat'].on('FASTA')\n    app.Parameters['-T'].on(cutoff)\n\n    to_remove = []\n\n    seqs_path = app._input_as_multiline_string(int_map.toFasta())\n    cm_path = app._tempfile_as_multiline_string(cm_file)\n    paths = [cm_path,seqs_path]\n    to_remove.append(cm_path)\n\n    _, tmp_file = mkstemp(dir=app.WorkingDir)\n    app.Parameters['--tabfile'].on(tmp_file)\n    res = app(paths)\n\n    search_results = list(CmsearchParser(res['SearchResults'].readlines()))\n    if search_results:\n        for i,line in enumerate(search_results):\n            label = line[1]\n            search_results[i][1]=int_keys.get(label,label)\n\n    res.cleanUp()\n    for f in to_remove:remove(f)\n\n    return search_results"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse cmsearch to find homologs from a CM file.", "response": "def cmsearch_from_file(cm_file_path, seqs, moltype, cutoff=0.0, params=None):\n    \"\"\"Uses cmbuild to build a CM file, then cmsearch to find homologs.\n\n        - cm_file_path: path to the file created by cmbuild, containing aligned\n            sequences. This will be used to search sequences in seqs.\n        - seqs: SequenceCollection object or something that can be used to\n            construct one, containing unaligned sequences that are to be\n            searched.\n        - moltype: cogent.core.moltype object.  Must be DNA or RNA\n        - cutoff: bitscore cutoff.  No sequences < cutoff will be kept in\n            search results. (Default=0.0).  Infernal documentation suggests\n            a cutoff of log2(number nucleotides searching) will give most\n            likely true homologs.\n    \"\"\"\n    #NOTE: Must degap seqs or Infernal well seg fault!\n    seqs = SequenceCollection(seqs,MolType=moltype).degap()\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seqs.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n\n    app = Cmsearch(InputHandler='_input_as_paths',WorkingDir='/tmp',\\\n        params=params)\n    app.Parameters['--informat'].on('FASTA')\n    app.Parameters['-T'].on(cutoff)\n\n    seqs_path = app._input_as_multiline_string(int_map.toFasta())\n\n    paths = [cm_file_path,seqs_path]\n\n    _, tmp_file = mkstemp(dir=app.WorkingDir)\n    app.Parameters['--tabfile'].on(tmp_file)\n    res = app(paths)\n\n    search_results = list(CmsearchParser(res['SearchResults'].readlines()))\n\n    if search_results:\n        for i,line in enumerate(search_results):\n            label = line[1]\n            search_results[i][1]=int_keys.get(label,label)\n\n    res.cleanUp()\n\n    return search_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a multiline string to a temp file and return the filename.", "response": "def _tempfile_as_multiline_string(self, data):\n        \"\"\"Write a multiline string to a temp file and return the filename.\n\n            data: a multiline string to be written to a file.\n\n           * Note: the result will be the filename as a FilePath object\n            (which is a string subclass).\n\n        \"\"\"\n        filename = FilePath(self.getTmpFilename(self.TmpDir))\n        data_file = open(filename,'w')\n        data_file.write(data)\n        data_file.close()\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid_sound_tuple(sound_tuple, final_form=True):\n\n    # We only work with lower case\n    sound_tuple = SoundTuple._make([s.lower() for s in sound_tuple])\n\n    # Words with no vowel are always valid\n    # FIXME: This looks like it should be toggled by a config key.\n    if not sound_tuple.vowel:\n        result = True\n    elif final_form:\n        result = \\\n            has_valid_consonants(sound_tuple) and \\\n            has_valid_vowel(sound_tuple) and \\\n            has_valid_accent(sound_tuple)\n    else:\n        result = \\\n            has_valid_consonants(sound_tuple) and \\\n            has_valid_vowel_non_final(sound_tuple)\n\n    return result", "response": "Checks if a sound tuple complies to Vietnamese phonology."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect(self, path):\n        '''\n        Guesses a TypeString from the given path\n        '''\n        typestring = UNKNOWN\n        for detector in self.detectors:\n            if typestring != UNKNOWN and not detector.can_improve(typestring):\n                continue\n            if not detector.can_detect(path):\n                continue\n            detected = detector.detect(path)\n            if detected:\n                typestring = detected\n        return typestring", "response": "Guesses a TypeString from the given path\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the cache buster value for a given file path", "response": "def set_cache_buster(self, path, hash):\n        \"\"\"Sets the cache buster value for a given file path\"\"\"\n        oz.aws_cdn.set_cache_buster(self.redis(), path, hash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a file to the given path with the given content.", "response": "def upload_file(self, path, contents, replace=False):\n        \"\"\"\n        Uplodas the file to its path with the given `content`, adding the\n        appropriate parent directories when needed. If the path already exists\n        and `replace` is `False`, the file will not be uploaded.\n        \"\"\"\n        f = self.get_file(path)\n        f.upload(contents, replace=replace)\n        self.set_cache_buster(path, f.hash())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying a file from a source path to a destination path adding the appropriate parent directories when needed.", "response": "def copy_file(self, from_path, to_path, replace=False):\n        \"\"\"\n        Copies a file from a given source path to a destination path, adding\n        appropriate parent directories when needed. If the destination path\n        already exists and `replace` is `False`, the file will not be\n        uploaded.\n        \"\"\"\n        f = self.get_file(from_path)\n        if f.copy(to_path, replace):\n            self.set_cache_buster(to_path, f.hash())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_file(self, path):\n        self.get_file(path).remove()\n        self.remove_cache_buster(path)", "response": "Removes the given file from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alignUnalignedSeqs(seqs,add_seq_names=True,WorkingDir=None,\\\n    SuppressStderr=None,SuppressStdout=None):\n    \"\"\"Aligns unaligned sequences\n\n    seqs: either list of sequence objects or list of strings\n    add_seq_names: boolean. if True, sequence names are inserted in the list\n        of sequences. if False, it assumes seqs is a list of lines of some\n        proper format that the program can handle\n    \"\"\"\n    if add_seq_names:\n        app = Clustalw(InputHandler='_input_as_seqs',\\\n            WorkingDir=WorkingDir,SuppressStderr=SuppressStderr,\\\n            SuppressStdout=SuppressStdout)\n    else:\n        app = Clustalw(InputHandler='_input_as_lines',\\\n            WorkingDir=WorkingDir,SuppressStderr=SuppressStderr,\\\n            SuppressStdout=SuppressStdout)\n    return app(seqs)", "response": "Aligns unaligned sequences in a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naligning unaligned sequences from some file", "response": "def alignUnalignedSeqsFromFile(filename,WorkingDir=None,SuppressStderr=None,\\\n    SuppressStdout=None):\n    \"\"\"Aligns unaligned sequences from some file (file should be right format)\n\n    filename: string, the filename of the file containing the sequences\n        to be aligned in a valid format.\n    \"\"\"\n    app = Clustalw(WorkingDir=WorkingDir,SuppressStderr=SuppressStderr,\\\n        SuppressStdout=SuppressStdout)\n    return app(filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naligns two alignments. Individual sequences are not realigned aln1: string, name of file containing the first alignment aln2: string, name of file containing the second alignment outfile: you're forced to specify an outfile name, because if you don't aln1 will be overwritten. So, if you want aln1 to be overwritten, you should specify the same filename. WARNING: a .dnd file is created with the same prefix as aln1. So an existing dendrogram might get overwritten.", "response": "def alignTwoAlignments(aln1,aln2,outfile,WorkingDir=None,SuppressStderr=None,\\\n    SuppressStdout=None):\n    \"\"\"Aligns two alignments. Individual sequences are not realigned\n\n    aln1: string, name of file containing the first alignment\n    aln2: string, name of file containing the second alignment\n    outfile: you're forced to specify an outfile name, because if you don't\n        aln1 will be overwritten. So, if you want aln1 to be overwritten, you\n        should specify the same filename.\n    WARNING: a .dnd file is created with the same prefix as aln1. So an\n    existing dendrogram might get overwritten.\n    \"\"\"\n    app = Clustalw({'-profile':None,'-profile1':aln1,\\\n        '-profile2':aln2,'-outfile':outfile},SuppressStderr=\\\n        SuppressStderr,WorkingDir=WorkingDir,SuppressStdout=SuppressStdout)\n    app.Parameters['-align'].off()\n    return app()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addSeqsToAlignment(aln1,seqs,outfile,WorkingDir=None,SuppressStderr=None,\\\n        SuppressStdout=None):\n    \"\"\"Aligns sequences from second profile against first profile\n\n    aln1: string, name of file containing the alignment\n    seqs: string, name of file containing the sequences that should be added\n        to the alignment.\n    opoutfile: string, name of the output file (the new alignment)\n    \"\"\"\n    app = Clustalw({'-sequences':None,'-profile1':aln1,\\\n        '-profile2':seqs,'-outfile':outfile},SuppressStderr=\\\n        SuppressStderr,WorkingDir=WorkingDir, SuppressStdout=SuppressStdout)\n\n    app.Parameters['-align'].off()\n    return app()", "response": "Aligns sequences from second profile against first profile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef buildTreeFromAlignment(filename,WorkingDir=None,SuppressStderr=None):\n    app = Clustalw({'-tree':None,'-infile':filename},SuppressStderr=\\\n        SuppressStderr,WorkingDir=WorkingDir)\n    app.Parameters['-align'].off()\n    return app()", "response": "Builds a new tree from an existing alignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_tree_from_alignment(aln, moltype=DNA, best_tree=False, params=None):\n    # Create instance of app controller, enable tree, disable alignment\n    app = Clustalw(InputHandler='_input_as_multiline_string', params=params, \\\n                   WorkingDir='/tmp')\n    app.Parameters['-align'].off()\n\n    #Set params to empty dict if None.\n    if params is None:\n        params={}\n\n    if moltype == DNA or moltype == RNA:\n        params['-type'] = 'd'\n    elif moltype == PROTEIN:\n        params['-type'] = 'p'\n    else:\n        raise ValueError, \"moltype must be DNA, RNA, or PROTEIN\"\n\n    # best_tree -> bootstrap\n    if best_tree:\n        if '-bootstrap' not in params:\n            app.Parameters['-bootstrap'].on(1000)\n        if '-seed' not in params:\n            app.Parameters['-seed'].on(randint(0,1000))\n        if '-bootlabels' not in params:\n            app.Parameters['-bootlabels'].on('nodes')\n    else:\n        app.Parameters['-tree'].on()\n\n    # Setup mapping. Clustalw clips identifiers. We will need to remap them.\n    seq_collection = SequenceCollection(aln)\n    int_map, int_keys = seq_collection.getIntMap()\n    int_map = SequenceCollection(int_map)\n\n    # Collect result\n    result = app(int_map.toFasta())\n\n    # Build tree\n    tree = DndParser(result['Tree'].read(), constructor=PhyloNode)\n    for node in tree.tips():\n        node.Name = int_keys[node.Name]\n\n    # Clean up\n    result.cleanUp()\n    del(seq_collection, app, result, int_map, int_keys)\n\n    return tree", "response": "Builds a tree from an alignment object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tree from an alignment object with bootstrap support values.", "response": "def bootstrap_tree_from_alignment(aln, seed=None, num_trees=None, params=None):\n    \"\"\"Returns a tree from Alignment object aln with bootstrap support values.\n\n    aln: an cogent.core.alignment.Alignment object, or data that can be used\n    to build one.\n\n    seed: an interger, seed value to use\n\n    num_trees: an integer, number of trees to bootstrap against\n\n    params: dict of parameters to pass in to the Clustal app controller.\n\n    The result will be an cogent.core.tree.PhyloNode object, or None if tree\n    fails.\n\n    If seed is not specifed in params, a random integer between 0-1000 is used.\n    \"\"\"\n    # Create instance of controllor, enable bootstrap, disable alignment,tree\n    app = Clustalw(InputHandler='_input_as_multiline_string', params=params, \\\n                   WorkingDir='/tmp')\n    app.Parameters['-align'].off()\n    app.Parameters['-tree'].off()\n\n    if app.Parameters['-bootstrap'].isOff():\n        if num_trees is None:\n            num_trees = 1000\n\n        app.Parameters['-bootstrap'].on(num_trees)\n\n    if app.Parameters['-seed'].isOff():\n        if seed is None:\n            seed = randint(0,1000)\n\n        app.Parameters['-seed'].on(seed)\n\n    if app.Parameters['-bootlabels'].isOff():\n        app.Parameters['-bootlabels'].on(\"node\")\n\n    # Setup mapping. Clustalw clips identifiers. We will need to remap them.\n    seq_collection = SequenceCollection(aln)\n    int_map, int_keys = seq_collection.getIntMap()\n    int_map = SequenceCollection(int_map)\n\n    # Collect result\n    result = app(int_map.toFasta())\n\n    # Build tree\n    tree = DndParser(result['Tree'].read(), constructor=PhyloNode)\n    for node in tree.tips():\n        node.Name = int_keys[node.Name]\n\n    # Clean up\n    result.cleanUp()\n    del(seq_collection, app, result, int_map, int_keys)\n\n    return tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef align_unaligned_seqs(seqs, moltype=DNA, params=None):\n    #create SequenceCollection object from seqs\n    seq_collection = SequenceCollection(seqs,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n    #Create Clustalw app.\n    app = Clustalw(InputHandler='_input_as_multiline_string',params=params)\n    #Get results using int_map as input to app\n    res = app(int_map.toFasta())\n    #Get alignment as dict out of results\n    alignment = dict(ClustalParser(res['Align'].readlines()))\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        new_alignment[int_keys[k]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    del(seq_collection,int_map,int_keys,app,res,alignment)\n\n    return new_alignment", "response": "Aligns unaligned sequences into a single cogent. core. alignment. Alignment object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_seqs_to_alignment(seqs, aln, moltype, params=None):\n    #create SequenceCollection object from seqs\n    seq_collection = SequenceCollection(seqs,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    seq_int_map, seq_int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    seq_int_map = SequenceCollection(seq_int_map,MolType=moltype)\n\n    #create Alignment object from aln\n    aln = Alignment(aln,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln_int_map, aln_int_keys = aln.getIntMap(prefix='seqn_')\n    #Create SequenceCollection from int_map.\n    aln_int_map = Alignment(aln_int_map,MolType=moltype)\n\n    #Update seq_int_keys with aln_int_keys\n    seq_int_keys.update(aln_int_keys)\n\n    #Create Mafft app.\n    app = Clustalw(InputHandler='_input_as_multiline_string',\\\n        params=params,\n        SuppressStderr=True)\n    app.Parameters['-align'].off()\n    app.Parameters['-infile'].off()\n    app.Parameters['-sequences'].on()\n\n    #Add aln_int_map as profile1\n    app.Parameters['-profile1'].on(\\\n        app._tempfile_as_multiline_string(aln_int_map.toFasta()))\n\n    #Add seq_int_map as profile2\n    app.Parameters['-profile2'].on(\\\n        app._tempfile_as_multiline_string(seq_int_map.toFasta()))\n    #Get results using int_map as input to app\n    res = app()\n\n    #Get alignment as dict out of results\n    alignment = dict(ClustalParser(res['Align'].readlines()))\n\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        new_alignment[seq_int_keys[k]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    remove(app.Parameters['-profile1'].Value)\n    remove(app.Parameters['-profile2'].Value)\n    del(seq_collection,seq_int_map,seq_int_keys,\\\n        aln,aln_int_map,aln_int_keys,app,res,alignment)\n\n    return new_alignment", "response": "Returns an Alignment object from seqs and existing Alignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef align_two_alignments(aln1, aln2, moltype, params=None):\n    #create SequenceCollection object from seqs\n    aln1 = Alignment(aln1,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln1_int_map, aln1_int_keys = aln1.getIntMap()\n    #Create SequenceCollection from int_map.\n    aln1_int_map = Alignment(aln1_int_map,MolType=moltype)\n\n    #create Alignment object from aln\n    aln2 = Alignment(aln2,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln2_int_map, aln2_int_keys = aln2.getIntMap(prefix='seqn_')\n    #Create SequenceCollection from int_map.\n    aln2_int_map = Alignment(aln2_int_map,MolType=moltype)\n\n    #Update aln1_int_keys with aln2_int_keys\n    aln1_int_keys.update(aln2_int_keys)\n\n    #Create Mafft app.\n    app = Clustalw(InputHandler='_input_as_multiline_string',\\\n        params=params,\n        SuppressStderr=True)\n    app.Parameters['-align'].off()\n    app.Parameters['-infile'].off()\n    app.Parameters['-profile'].on()\n\n    #Add aln_int_map as profile1\n    app.Parameters['-profile1'].on(\\\n        app._tempfile_as_multiline_string(aln1_int_map.toFasta()))\n\n    #Add seq_int_map as profile2\n    app.Parameters['-profile2'].on(\\\n        app._tempfile_as_multiline_string(aln2_int_map.toFasta()))\n    #Get results using int_map as input to app\n    res = app()\n\n    #Get alignment as dict out of results\n    alignment = dict(ClustalParser(res['Align'].readlines()))\n\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        new_alignment[aln1_int_keys[k]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    remove(app.Parameters['-profile1'].Value)\n    remove(app.Parameters['-profile2'].Value)\n    del(aln1,aln1_int_map,aln1_int_keys,\\\n        aln2,aln2_int_map,aln2_int_keys,app,res,alignment)\n\n    return new_alignment", "response": "Returns an Alignment object from two existing Alignments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _input_as_multiline_string(self, data):\n        if data:\n            self.Parameters['-infile']\\\n                .on(super(Clustalw,self)._input_as_multiline_string(data))\n        return ''", "response": "Writes data to tempfile and sets - infile parameter\nMimeType"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite data to tempfile and sets - infile parameter MimeType", "response": "def _input_as_lines(self,data):\n        \"\"\"Writes data to tempfile and sets -infile parameter\n\n        data -- list of lines, ready to be written to file\n        \"\"\"\n        if data:\n            self.Parameters['-infile']\\\n                .on(super(Clustalw,self)._input_as_lines(data))\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning appropriate suffix for alignment file", "response": "def _suffix(self):\n        \"\"\"Return appropriate suffix for alignment file\"\"\"\n        _output_formats={'GCG':'.msf',\n                        'GDE':'.gde',\n                        'PHYLIP':'.phy',\n                        'PIR':'.pir',\n                        'NEXUS':'.nxs'}\n\n        if self.Parameters['-output'].isOn():\n            return _output_formats[self.Parameters['-output'].Value]\n        else:\n            return '.aln'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn name of file containing the alignment", "response": "def _aln_filename(self,prefix):\n        \"\"\"Return name of file containing the alignment\n\n        prefix -- str, prefix of alignment file.\n        \"\"\"\n        if self.Parameters['-outfile'].isOn():\n            aln_filename = self._absolute(self.Parameters['-outfile'].Value)\n        else:\n            aln_filename = prefix + self._suffix()\n        return aln_filename"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_result_paths(self,data):\n\n        #clustalw .aln is used when no or unkown output type specified\n        _treeinfo_formats = {'nj':'.nj',\n                            'dist':'.dst',\n                            'nexus':'.tre'}\n\n        result = {}\n        par = self.Parameters\n        abs = self._absolute\n\n        if par['-align'].isOn():\n            prefix = par['-infile'].Value.rsplit('.', 1)[0]\n            #prefix = par['-infile'].Value.split('.')[0]\n            aln_filename = self._aln_filename(prefix)\n            if par['-newtree'].isOn():\n                dnd_filename = abs(par['-newtree'].Value)\n            elif par['-usetree'].isOn():\n                dnd_filename = abs(par['-usetree'].Value)\n            else:\n                dnd_filename = abs(prefix + '.dnd')\n            result['Align'] = ResultPath(Path=aln_filename,IsWritten=True)\n            result['Dendro'] = ResultPath(Path=dnd_filename,IsWritten=True)\n        elif par['-profile'].isOn():\n            prefix1 = par['-profile1'].Value.rsplit('.', 1)[0]\n            prefix2 = par['-profile2'].Value.rsplit('.', 1)[0]\n            #prefix1 = par['-profile1'].Value.split('.')[0]\n            #prefix2 = par['-profile2'].Value.split('.')[0]\n            aln_filename = ''; aln_written = True\n            dnd1_filename = ''; tree1_written = True\n            dnd2_filename = ''; tree2_written = True\n            aln_filename = self._aln_filename(prefix1)\n            #usetree1\n            if par['-usetree1'].isOn():\n                tree1_written = False\n            #usetree2\n            if par['-usetree2'].isOn():\n                tree2_written = False\n            if par['-newtree1'].isOn():\n                dnd1_filename = abs(par['-newtree1'].Value)\n                aln_written=False\n            else:\n                dnd1_filename = abs(prefix1 + '.dnd')\n            if par['-newtree2'].isOn():\n                dnd2_filename = abs(par['-newtree2'].Value)\n                aln_written=False\n            else:\n                dnd2_filename = abs(prefix2 + '.dnd')\n            result['Align'] = ResultPath(Path=aln_filename,\n                IsWritten=aln_written)\n            result['Dendro1'] = ResultPath(Path=dnd1_filename,\n                IsWritten=tree1_written)\n            result['Dendro2'] = ResultPath(Path=dnd2_filename,\n                IsWritten=tree2_written)\n        elif par['-sequences'].isOn():\n            prefix1 = par['-profile1'].Value.rsplit('.', 1)[0]\n            prefix2 = par['-profile2'].Value.rsplit('.', 1)[0]\n            #prefix1 = par['-profile1'].Value.split('.')[0] #alignment\n            #prefix2 = par['-profile2'].Value.split('.')[0] #sequences\n            aln_filename = ''; aln_written = True\n            dnd_filename = ''; dnd_written = True\n\n            aln_filename = self._aln_filename(prefix2)\n            if par['-usetree'].isOn():\n                dnd_written = False\n            elif par['-newtree'].isOn():\n                aln_written = False\n                dnd_filename = abs(par['-newtree'].Value)\n            else:\n                dnd_filename = prefix2 + '.dnd'\n            result['Align'] = ResultPath(Path=aln_filename,\\\n                IsWritten=aln_written)\n            result['Dendro'] = ResultPath(Path=dnd_filename,\\\n                IsWritten=dnd_written)\n        elif par['-tree'].isOn():\n            prefix = par['-infile'].Value.rsplit('.', 1)[0]\n            #prefix = par['-infile'].Value.split('.')[0]\n            tree_filename = ''; tree_written = True\n            treeinfo_filename = ''; treeinfo_written = False\n            tree_filename = prefix + '.ph'\n            if par['-outputtree'].isOn() and\\\n                par['-outputtree'].Value != 'phylip':\n                treeinfo_filename = prefix +\\\n                    _treeinfo_formats[par['-outputtree'].Value]\n                treeinfo_written = True\n            result['Tree'] = ResultPath(Path=tree_filename,\\\n                IsWritten=tree_written)\n            result['TreeInfo'] = ResultPath(Path=treeinfo_filename,\\\n                IsWritten=treeinfo_written)\n\n        elif par['-bootstrap'].isOn():\n            prefix = par['-infile'].Value.rsplit('.', 1)[0]\n            #prefix = par['-infile'].Value.split('.')[0]\n            boottree_filename = prefix + '.phb'\n            result['Tree'] = ResultPath(Path=boottree_filename,IsWritten=True)\n\n        return result", "response": "Returns a dict of the result paths for the current set of clustalw files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nraise a HTTPException for the given http_status_code.", "response": "def abort(http_status_code, exc=None, **kwargs):\n    \"\"\"Raise a HTTPException for the given http_status_code. Attach any keyword\n    arguments to the exception for later processing.\n\n    From Flask-Restful. See NOTICE file for license information.\n    \"\"\"\n    try:\n        sanic.exceptions.abort(http_status_code, exc)\n    except sanic.exceptions.SanicException as err:\n        err.data = kwargs\n        err.exc = exc\n        raise err"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_view_args(self, req, name, field):\n        return core.get_value(req.match_info, name, field)", "response": "Pull a value from the request s view_args."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_request_from_view_args(self, view, args, kwargs):\n        if len(args) > 1 and isinstance(args[1], sanic.request.Request):\n            req = args[1]\n        else:\n            req = args[0]\n        assert isinstance(\n            req, sanic.request.Request\n        ), \"Request argument not found for handler\"\n        return req", "response": "Get request object from a view function or method. Used internally by\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_json(self, req, name, field):\n        if not (req.body and is_json_request(req)):\n            return core.missing\n        json_data = req.json\n        if json_data is None:\n            return core.missing\n        return core.get_value(json_data, name, field, allow_many_nested=True)", "response": "Pull a json value from the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_error(self, error, req, schema):\n\n        status_code = getattr(error, \"status_code\", self.DEFAULT_VALIDATION_STATUS)\n        abort(status_code, exc=error, messages=error.messages, schema=schema)", "response": "Handles errors during parsing. Aborts the current HTTP request and\n        responds with a 422 error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef directory_walk(source_d, destination_d):\n    '''\n    Walk a directory structure and yield full parallel source and destination\n    files, munging filenames as necessary\n    '''\n    for dirpath, dirnames, filenames in os.walk(source_d):\n        relpath = os.path.relpath(dirpath, source_d)\n        if relpath == '.':\n            relpath = ''  # remove implied '.'\n        for filename in filenames:\n            suffix = filename\n            if relpath:\n                suffix = os.path.join(relpath, filename)\n            full_source_path = os.path.join(source_d, suffix)\n            full_destination_path = os.path.join(destination_d, suffix)\n            yield full_source_path, full_destination_path", "response": "Walk a directory structure and yield full source and destination directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recursive_symlink_dirs(source_d, destination_d):\n    '''\n    Create dirs and symlink all files recursively from source_d, ignoring\n    errors (e.g. existing files)\n    '''\n    func = os.symlink\n    if os.name == 'nt':\n        # NOTE: need to verify that default perms only allow admins to create\n        # symlinks on Windows\n        func = shutil.copy\n    if os.path.exists(destination_d):\n        os.rmdir(destination_d)\n    shutil.copytree(source_d, destination_d, copy_function=func)", "response": "Create dirs and symlink all files recursively from source_d to destination_d ignoring any errors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recursive_hardlink_dirs(source_d, destination_d):\n    '''\n    Same as above, except creating hardlinks for all files\n    '''\n    func = os.link\n    if os.name == 'nt':\n        func = shutil.copy\n    if os.path.exists(destination_d):\n        os.rmdir(destination_d)\n    shutil.copytree(source_d, destination_d, copy_function=func)", "response": "Same as above except creating hardlinks for all files\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive an array of git files and directories and a prefix returns a nested dictionary that represents the tree structure.", "response": "def flat_git_tree_to_nested(flat_tree, prefix=''):\n    '''\n    Given an array in format:\n        [\n            [\"100644\", \"blob\", \"ab3ce...\", \"748\", \".gitignore\" ],\n            [\"100644\", \"blob\", \"ab3ce...\", \"748\", \"path/to/thing\" ],\n            ...\n        ]\n\n    Outputs in a nested format:\n        {\n            \"path\": \"/\",\n            \"type\": \"directory\",\n            \"children\": [\n                {\n                    \"type\": \"blob\",\n                    \"size\": 748,\n                    \"sha\": \"ab3ce...\",\n                    \"mode\": \"100644\",\n                },\n                ...\n            ],\n            ...\n        }\n    '''\n    root = _make_empty_dir_dict(prefix if prefix else '/')\n\n    # Filter all descendents of this prefix\n    descendent_files = [\n        info for info in flat_tree\n        if os.path.dirname(info[PATH]).startswith(prefix)\n    ]\n\n    # Figure out strictly leaf nodes of this tree (can be immediately added as\n    # children)\n    children_files = [\n        info for info in descendent_files\n        if os.path.dirname(info[PATH]) == prefix\n    ]\n\n    # Figure out all descendent directories\n    descendent_dirs = set(\n        os.path.dirname(info[PATH]) for info in descendent_files\n        if os.path.dirname(info[PATH]).startswith(prefix)\n        and not os.path.dirname(info[PATH]) == prefix\n    )\n\n    # Figure out all descendent directories\n    children_dirs = set(\n        dir_path for dir_path in descendent_dirs\n        if os.path.dirname(dir_path) == prefix\n    )\n\n    # Recurse into children dirs, constructing file trees for each of them,\n    # then appending those\n    for dir_path in children_dirs:\n        info = flat_git_tree_to_nested(descendent_files, prefix=dir_path)\n        root['children'].append(info)\n\n    # Append direct children files\n    for info in children_files:\n        root['children'].append(_make_child(info))\n\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_all(self, key, default=None):\n        '''\n        Import settings key as a dict or list with values of importable paths\n        If a default constructor is specified, and a path is not importable, it\n        falls back to running the given constructor.\n        '''\n        value = getattr(self, key)\n        if default is not None:\n            def loader(path): return self.load_path_with_default(path, default)\n        else:\n            loader = self.load_path\n        if isinstance(value, dict):\n            return {key: loader(value) for key, value in value.items()}\n        elif isinstance(value, list):\n            return [loader(value) for value in value]\n        else:\n            raise ValueError('load_all must be list or dict')", "response": "Load all settings for a given key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_path(self, path):\n        '''\n        Load and return a given import path to a module or class\n        '''\n        containing_module, _, last_item = path.rpartition('.')\n        if last_item[0].isupper():\n            # Is a class definition, should do an \"import from\"\n            path = containing_module\n        imported_obj = importlib.import_module(path)\n        if last_item[0].isupper():\n            try:\n                imported_obj = getattr(imported_obj, last_item)\n            except AttributeError:\n                msg = 'Cannot import \"%s\". ' \\\n                    '(Hint: CamelCase is only for classes)' % last_item\n                raise ConfigurationError(msg)\n        return imported_obj", "response": "Load and return a given import path to a module or class\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides existing settings taking precedence over both user settings object and default settings. Useful for specific runtime requirements.", "response": "def set(self, **kwargs):\n        '''\n        Override existing settings, taking precedence over both user settings\n        object and default settings. Useful for specific runtime requirements,\n        such as overriding PORT or HOST.\n        '''\n        for lower_key, value in kwargs.items():\n            if lower_key.lower() != lower_key:\n                raise ValueError('Requires lowercase: %s' % lower_key)\n            key = lower_key.upper()\n            try:\n                getattr(self, key)\n            except (AttributeError, ConfigurationError):\n                raise AttributeError('Cannot override %s' % key)\n            self.overridden_settings[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses the specified settings module to override the current settings.", "response": "def use_settings(self, settings_module):\n        '''\n        Useful for tests for overriding current settings manually\n        '''\n        self._previous_settings = self.settings_module\n        self.settings_module = settings_module\n        self.reconfigure()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef use_settings_dict(self, settings_dict):\n        '''\n        Slightly cleaner interface to override settings that autogenerates a\n        settings module based on a given dict.\n        '''\n        class SettingsDictModule:\n            __slots__ = tuple(key.upper() for key in settings_dict.keys())\n        settings_obj = SettingsDictModule()\n        for key, value in settings_dict.items():\n            setattr(settings_obj, key.upper(), value)\n        self.use_settings(settings_obj)", "response": "Use settings that autogenerates a specific settings module based on a given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_path_to_absolute_path(import_path, file_marker):\n        '''\n        Given a Python import path, convert to a likely absolute filesystem\n        path, by searching for the given filename marker (such as\n        'package.json' or '__init__.py') through the Python system path. Do not\n        return given filename.\n        '''\n        path_fragment = import_path.replace('.', os.path.sep)\n        path_suffix = os.path.join(path_fragment, file_marker)\n        for path_base in sys.path:\n            path = os.path.join(path_base, path_suffix)\n            if os.path.exists(path):\n                return os.path.join(path_base, path_fragment)\n        msg = 'Cannot find import path: %s, %s'\n        raise ConfigurationError(msg % (import_path, file_marker))", "response": "Given a Python import path convert to a likely absolute filesystem\n        path by searching through sys. path and looking for the given filename marker. Do not\n        return given filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_telex_definition(w_shorthand=True, brackets_shorthand=True):\n    telex = {\n        \"a\": \"a^\",\n        \"o\": \"o^\",\n        \"e\": \"e^\",\n        \"w\": [\"u*\", \"o*\", \"a+\"],\n        \"d\": \"d-\",\n        \"f\": \"\\\\\",\n        \"s\": \"/\",\n        \"r\": \"?\",\n        \"x\": \"~\",\n        \"j\": \".\",\n    }\n\n    if w_shorthand:\n        telex[\"w\"].append('<\u01b0')\n\n    if brackets_shorthand:\n        telex.update({\n            \"]\": \"<\u01b0\",\n            \"[\": \"<\u01a1\",\n            \"}\": \"<\u01af\",\n            \"{\": \"<\u01a0\"\n        })\n\n    return telex", "response": "Create a TELEX definition dictionary for the TELEX input method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_key(string, key,\n                fallback_sequence=\"\", rules=None,\n                skip_non_vietnamese=True):\n    \"\"\"Process a keystroke.\n\n    Args:\n        string: The previously processed string or \"\".\n        key: The keystroke.\n        fallback_sequence: The previous keystrokes.\n        rules (optional): A dictionary listing\n            transformation rules. Defaults to get_telex_definition().\n        skip_non_vietnamese (optional): Whether to skip results that\n            doesn't seem like Vietnamese. Defaults to True.\n\n    Returns a tuple. The first item of which is the processed\n    Vietnamese string, the second item is the next fallback sequence.\n    The two items are to be fed back into the next call of process_key()\n    as `string` and `fallback_sequence`. If `skip_non_vietnamese` is\n    True and the resulting string doesn't look like Vietnamese,\n    both items contain the `fallback_sequence`.\n\n    >>> process_key('a', 'a', 'a')\n    (\u00e2, aa)\n\n    Note that when a key is an undo key, it won't get appended to\n    `fallback_sequence`.\n\n    >>> process_key('\u00e2', 'a', 'aa')\n    (aa, aa)\n\n    `rules` is a dictionary that maps keystrokes to\n    their effect string. The effects can be one of the following:\n\n    'a^': a with circumflex (\u00e2), only affect an existing 'a family'\n    'a+': a with breve (\u0103), only affect an existing 'a family'\n    'e^': e with circumflex (\u00ea), only affect an existing 'e family'\n    'o^': o with circumflex (\u00f4), only affect an existing 'o family'\n    'o*': o with horn (\u01a1), only affect an existing 'o family'\n    'd-': d with bar (\u0111), only affect an existing 'd'\n    '/': acute (s\u1eafc), affect an existing vowel\n    '\\': grave (huy\u1ec1n), affect an existing vowel\n    '?': hook (h\u1ecfi), affect an existing vowel\n    '~': tilde (ng\u00e3), affect an existing vowel\n    '.': dot (n\u1eb7ng), affect an existing vowel\n    '<\u01b0': append \u01b0\n    '<\u01a1': append \u01a1\n\n    A keystroke entry can have multiple effects, in which case the\n    dictionary entry's value should be a list of the possible\n    effect strings. Although you should try to avoid this if\n    you are defining a custom input method rule.\n    \"\"\"\n    # TODO Figure out a way to remove the `string` argument. Perhaps only the\n    #      key sequence is needed?\n    def default_return():\n        return string + key, fallback_sequence + key\n\n    if rules is None:\n        rules = get_telex_definition()\n\n    comps = utils.separate(string)\n\n    # if not _is_processable(comps):\n    #     return default_return()\n\n    # Find all possible transformations this keypress can generate\n    trans_list = _get_transformation_list(\n        key, rules, fallback_sequence)\n\n    # Then apply them one by one\n    new_comps = list(comps)\n    for trans in trans_list:\n        new_comps = _transform(new_comps, trans)\n\n    if new_comps == comps:\n        tmp = list(new_comps)\n\n        # If none of the transformations (if any) work\n        # then this keystroke is probably an undo key.\n        if _can_undo(new_comps, trans_list):\n            # The prefix \"_\" means undo.\n            for trans in map(lambda x: \"_\" + x, trans_list):\n                new_comps = _transform(new_comps, trans)\n\n            # Undoing the w key with the TELEX input method with the\n            # w:<\u01b0 extension requires some care.\n            #\n            # The input (\u01b0, w) should be undone as w\n            # on the other hand, (\u01b0, uw) should return uw.\n            #\n            # _transform() is not aware of the 2 ways to generate\n            # \u01b0 in TELEX and always think \u01b0 was created by uw.\n            # Therefore, after calling _transform() to undo \u01b0,\n            # we always get ['', 'u', ''].\n            #\n            # So we have to clean it up a bit.\n            def is_telex_like():\n                return '<\u01b0' in rules[\"w\"]\n\n            def undone_vowel_ends_with_u():\n                return new_comps[1] and new_comps[1][-1].lower() == \"u\"\n\n            def not_first_key_press():\n                return len(fallback_sequence) >= 1\n\n            def user_typed_ww():\n                return (fallback_sequence[-1:]+key).lower() == \"ww\"\n\n            def user_didnt_type_uww():\n                return not (len(fallback_sequence) >= 2 and\n                            fallback_sequence[-2].lower() == \"u\")\n\n            if is_telex_like() and \\\n                    not_first_key_press() and \\\n                    undone_vowel_ends_with_u() and \\\n                    user_typed_ww() and \\\n                    user_didnt_type_uww():\n                # The vowel part of new_comps is supposed to end with\n                # u now. That u should be removed.\n                new_comps[1] = new_comps[1][:-1]\n\n        if tmp == new_comps:\n            fallback_sequence += key\n        new_comps = utils.append_comps(new_comps, key)\n    else:\n        fallback_sequence += key\n\n    if skip_non_vietnamese is True and key.isalpha() and \\\n            not is_valid_combination(new_comps, final_form=False):\n        result = fallback_sequence, fallback_sequence\n    else:\n        result = utils.join(new_comps), fallback_sequence\n\n    return result", "response": "Process a keystroke and return a tuple of the keystrokes in the keystroke string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of transformations inferred from the entered key.", "response": "def _get_transformation_list(key, im, fallback_sequence):\n    \"\"\"\n    Return the list of transformations inferred from the entered key. The\n    map between transform types and keys is given by module\n    bogo_config (if exists) or by variable simple_telex_im\n\n    if entered key is not in im, return \"+key\", meaning appending\n    the entered key to current text\n    \"\"\"\n    # if key in im:\n    #     lkey = key\n    # else:\n    #     lkey = key.lower()\n    lkey = key.lower()\n\n    if lkey in im:\n        if isinstance(im[lkey], list):\n            trans_list = im[lkey]\n        else:\n            trans_list = [im[lkey]]\n\n        for i, trans in enumerate(trans_list):\n            if trans[0] == '<' and key.isalpha():\n                trans_list[i] = trans[0] + \\\n                    utils.change_case(trans[1], int(key.isupper()))\n\n        if trans_list == ['_']:\n            if len(fallback_sequence) >= 2:\n                # TODO Use takewhile()/dropwhile() to process the last IM keypress\n                # instead of assuming it's the last key in fallback_sequence.\n                t = list(map(lambda x: \"_\" + x,\n                             _get_transformation_list(fallback_sequence[-2], im,\n                                                     fallback_sequence[:-1])))\n                # print(t)\n                trans_list = t\n            # else:\n            #     trans_list = ['+' + key]\n\n        return trans_list\n    else:\n        return ['+' + key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the action inferred from the transformation trans and the parameter going with this action", "response": "def _get_action(trans):\n    \"\"\"\n    Return the action inferred from the transformation `trans`.\n    and the parameter going with this action\n    An _Action.ADD_MARK goes with a Mark\n    while an _Action.ADD_ACCENT goes with an Accent\n    \"\"\"\n    # TODO: VIQR-like convention\n    mark_action = {\n        '^': (_Action.ADD_MARK, Mark.HAT),\n        '+': (_Action.ADD_MARK, Mark.BREVE),\n        '*': (_Action.ADD_MARK, Mark.HORN),\n        '-': (_Action.ADD_MARK, Mark.BAR),\n    }\n\n    accent_action = {\n        '\\\\': (_Action.ADD_ACCENT, Accent.GRAVE),\n        '/': (_Action.ADD_ACCENT, Accent.ACUTE),\n        '?': (_Action.ADD_ACCENT, Accent.HOOK),\n        '~': (_Action.ADD_ACCENT, Accent.TIDLE),\n        '.': (_Action.ADD_ACCENT, Accent.DOT),\n    }\n\n    if trans[0] in ('<', '+'):\n        return _Action.ADD_CHAR, trans[1]\n    if trans[0] == \"_\":\n        return _Action.UNDO, trans[1:]\n    if len(trans) == 2:\n        return mark_action[trans[1]]\n    else:\n        return accent_action[trans[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms the given string with transform type trans", "response": "def _transform(comps, trans):\n    \"\"\"\n    Transform the given string with transform type trans\n    \"\"\"\n    logging.debug(\"== In _transform(%s, %s) ==\", comps, trans)\n    components = list(comps)\n\n    action, parameter = _get_action(trans)\n    if action == _Action.ADD_MARK and \\\n            components[2] == \"\" and \\\n            mark.strip(components[1]).lower() in ['oe', 'oa'] and trans == \"o^\":\n        action, parameter = _Action.ADD_CHAR, trans[0]\n\n    if action == _Action.ADD_ACCENT:\n        logging.debug(\"add_accent(%s, %s)\", components, parameter)\n        components = accent.add_accent(components, parameter)\n    elif action == _Action.ADD_MARK and mark.is_valid_mark(components, trans):\n        logging.debug(\"add_mark(%s, %s)\", components, parameter)\n        components = mark.add_mark(components, parameter)\n\n        # Handle u\u01a1 in \"hu\u01a1\", \"thu\u1edf\", \"qu\u1edf\"\n        # If the current word has no last consonant and the first consonant\n        # is one of \"h\", \"th\" and the vowel is \"\u01b0\u01a1\" then change the vowel into\n        # \"u\u01a1\", keeping case and accent. If an alphabet character is then added\n        # into the word then change back to \"\u01b0\u01a1\".\n        #\n        # NOTE: In the dictionary, these are the only words having this strange\n        # vowel so we don't need to worry about other cases.\n        if accent.remove_accent_string(components[1]).lower() == \"\u01b0\u01a1\" and \\\n                not components[2] and components[0].lower() in [\"\", \"h\", \"th\", \"kh\"]:\n            # Backup accents\n            ac = accent.get_accent_string(components[1])\n            components[1] = (\"u\", \"U\")[components[1][0].isupper()] + components[1][1]\n            components = accent.add_accent(components, ac)\n\n    elif action == _Action.ADD_CHAR:\n        if trans[0] == \"<\":\n            if not components[2]:\n                # Only allow \u01b0, \u01a1 or \u01b0\u01a1 sitting alone in the middle part\n                # and ['g', 'i', '']. If we want to type giowf = 'gi\u1edd', separate()\n                # will create ['g', 'i', '']. Therefore we have to allow\n                # components[1] == 'i'.\n                if (components[0].lower(), components[1].lower()) == ('g', 'i'):\n                    components[0] += components[1]\n                    components[1] = ''\n                if not components[1] or \\\n                        (components[1].lower(), trans[1].lower()) == ('\u01b0', '\u01a1'):\n                    components[1] += trans[1]\n        else:\n            components = utils.append_comps(components, parameter)\n            if parameter.isalpha() and \\\n                    accent.remove_accent_string(components[1]).lower().startswith(\"u\u01a1\"):\n                ac = accent.get_accent_string(components[1])\n                components[1] = ('\u01b0',  '\u01af')[components[1][0].isupper()] + \\\n                    ('\u01a1', '\u01a0')[components[1][1].isupper()] + components[1][2:]\n                components = accent.add_accent(components, ac)\n    elif action == _Action.UNDO:\n        components = _reverse(components, trans[1:])\n\n    if action == _Action.ADD_MARK or (action == _Action.ADD_CHAR and parameter.isalpha()):\n        # If there is any accent, remove and reapply it\n        # because it is likely to be misplaced in previous transformations\n        ac = accent.get_accent_string(components[1])\n\n        if ac != accent.Accent.NONE:\n            components = accent.add_accent(components, Accent.NONE)\n            components = accent.add_accent(components, ac)\n\n    logging.debug(\"After transform: %s\", components)\n    return components"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreverse the effect of transformation trans on components.", "response": "def _reverse(components, trans):\n    \"\"\"\n    Reverse the effect of transformation 'trans' on 'components'\n    If the transformation does not affect the components, return the original\n    string.\n    \"\"\"\n\n    action, parameter = _get_action(trans)\n    comps = list(components)\n    string = utils.join(comps)\n\n    if action == _Action.ADD_CHAR and string[-1].lower() == parameter.lower():\n        if comps[2]:\n            i = 2\n        elif comps[1]:\n            i = 1\n        else:\n            i = 0\n        comps[i] = comps[i][:-1]\n    elif action == _Action.ADD_ACCENT:\n        comps = accent.add_accent(comps, Accent.NONE)\n    elif action == _Action.ADD_MARK:\n        if parameter == Mark.BAR:\n            comps[0] = comps[0][:-1] + \\\n                mark.add_mark_char(comps[0][-1:], Mark.NONE)\n        else:\n            if mark.is_valid_mark(comps, trans):\n                comps[1] = \"\".join([mark.add_mark_char(c, Mark.NONE)\n                                    for c in comps[1]])\n    return comps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn whether a list of components can be undone with one of the transformation in trans_list.", "response": "def _can_undo(comps, trans_list):\n    \"\"\"\n    Return whether a components can be undone with one of the transformation in\n    trans_list.\n    \"\"\"\n    comps = list(comps)\n    accent_list = list(map(accent.get_accent_char, comps[1]))\n    mark_list = list(map(mark.get_mark_char, utils.join(comps)))\n    action_list = list(map(lambda x: _get_action(x), trans_list))\n\n    def atomic_check(action):\n        \"\"\"\n        Check if the `action` created one of the marks, accents, or characters\n        in `comps`.\n        \"\"\"\n        return (action[0] == _Action.ADD_ACCENT and action[1] in accent_list) \\\n                or (action[0] == _Action.ADD_MARK and action[1] in mark_list) \\\n                or (action[0] == _Action.ADD_CHAR and action[1] == \\\n                    accent.remove_accent_char(comps[1][-1]))  # \u01a1, \u01b0\n\n    return any(map(atomic_check, action_list))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling backspace in the raw_sequence.", "response": "def handle_backspace(converted_string, raw_sequence, im_rules=None):\n    \"\"\"\n    Returns a new raw_sequence after a backspace. This raw_sequence should\n    be pushed back to process_sequence().\n    \"\"\"\n    # I can't find a simple explanation for this, so\n    # I hope this example can help clarify it:\n    #\n    # handle_backspace(th\u01b0\u01a1ng, thuwongw) -> thuwonw\n    # handle_backspace(th\u01b0\u01a1n, thuwonw) -> thuwow\n    # handle_backspace(th\u01b0\u01a1, thuwow) -> thuw\n    # handle_backspace(th\u01b0\u01a1, thuw) -> th\n    #\n    # The algorithm for handle_backspace was contributed by @hainp.\n\n    if im_rules == None:\n        im_rules = get_telex_definition()\n\n    deleted_char = converted_string[-1]\n\n    _accent = accent.get_accent_char(deleted_char)\n    _mark = mark.get_mark_char(deleted_char)\n\n    if _mark or _accent:\n        # Find a sequence of IM keys at the end of\n        # raw_sequence\n\n        ime_keys_at_end = \"\"\n        len_raw_sequence = len(raw_sequence)\n        i = len_raw_sequence - 1\n\n        while i >= 0:\n            if raw_sequence[i] not in im_rules and \\\n                    raw_sequence[i] not in \"aeiouyd\":\n                i += 1\n                break\n            else:\n                ime_keys_at_end = raw_sequence[i] + ime_keys_at_end\n            i -= 1\n\n        # Try to find a subsequence from that sequence\n        # that can be converted to the deleted_char\n        k = 0\n        while k < len_raw_sequence:\n            if process_sequence(raw_sequence[i + k:], im_rules) == deleted_char:\n                # Delete that subsequence\n                raw_sequence = raw_sequence[:i + k]\n                break\n            k += 1\n    else:\n        index = raw_sequence.rfind(deleted_char)\n        raw_sequence = raw_sequence[:index] + raw_sequence[(index + 1):]\n\n    return raw_sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the mark of a single character.", "response": "def get_mark_char(char):\n    \"\"\"\n    Get the mark of a single char, if any.\n    \"\"\"\n    char = accent.remove_accent_char(char.lower())\n    if char == \"\":\n        return Mark.NONE\n    if char == \"\u0111\":\n        return Mark.BAR\n    if char in \"\u0103\":\n        return Mark.BREVE\n    if char in \"\u01a1\u01b0\":\n        return Mark.HORN\n    if char in \"\u00e2\u00ea\u00f4\":\n        return Mark.HAT\n    return Mark.NONE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a mark to the index - th character of the given string.", "response": "def add_mark_at(string, index, mark):\n    \"\"\"\n    Add mark to the index-th character of the given string. Return the new string after applying change.\n    Notice: index > 0\n    \"\"\"\n    if index == -1:\n        return string\n    # Python can handle the case which index is out of range of given string\n    return string[:index] + add_mark_char(string[index], mark) + string[index+1:]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_mark_char(char, mark):\n    if char == \"\":\n        return \"\"\n    case = char.isupper()\n    ac = accent.get_accent_char(char)\n    char = accent.add_accent_char(char.lower(), Accent.NONE)\n    new_char = char\n    if mark == Mark.HAT:\n        if char in FAMILY_A:\n            new_char = \"\u00e2\"\n        elif char in FAMILY_O:\n            new_char = \"\u00f4\"\n        elif char in FAMILY_E:\n            new_char = \"\u00ea\"\n    elif mark == Mark.HORN:\n        if char in FAMILY_O:\n            new_char = \"\u01a1\"\n        elif char in FAMILY_U:\n            new_char = \"\u01b0\"\n    elif mark == Mark.BREVE:\n        if char in FAMILY_A:\n            new_char = \"\u0103\"\n    elif mark == Mark.BAR:\n        if char in FAMILY_D:\n            new_char = \"\u0111\"\n    elif mark == Mark.NONE:\n        if char in FAMILY_A:\n            new_char = \"a\"\n        elif char in FAMILY_E:\n            new_char = \"e\"\n        elif char in FAMILY_O:\n            new_char = \"o\"\n        elif char in FAMILY_U:\n            new_char = \"u\"\n        elif char in FAMILY_D:\n            new_char = \"d\"\n\n    new_char = accent.add_accent_char(new_char, ac)\n    return utils.change_case(new_char, case)", "response": "Add mark to a single char."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether the mark given by mark_trans is valid to add to the components", "response": "def is_valid_mark(comps, mark_trans):\n    \"\"\"\n    Check whether the mark given by mark_trans is valid to add to the components\n    \"\"\"\n    if mark_trans == \"*_\":\n        return True\n    components = list(comps)\n\n    if mark_trans[0] == 'd' and components[0] \\\n            and components[0][-1].lower() in (\"d\", \"\u0111\"):\n        return True\n    elif components[1] != \"\" and \\\n            strip(components[1]).lower().find(mark_trans[0]) != -1:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls the Infoblox device to delete the ref", "response": "def delete(self, path):\n        \"\"\"Call the Infoblox device to delete the ref\n\n        :param str ref: The reference id\n        :rtype: requests.Response\n\n        \"\"\"\n        return self.session.delete(self._request_url(path),\n                                   auth=self.auth, verify=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall Infoblox device to get the object for the data passed in data", "response": "def get(self, path, data=None, return_fields=None):\n        \"\"\"Call the Infoblox device to get the obj for the data passed in\n\n        :param str obj_reference: The object reference data\n        :param dict data: The data for the get request\n        :rtype: requests.Response\n\n        \"\"\"\n        return self.session.get(self._request_url(path, return_fields),\n                                data=json.dumps(data),\n                                auth=self.auth, verify=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls Infoblox device to post the object for the data passed in data", "response": "def post(self, path, data):\n        \"\"\"Call the Infoblox device to post the obj for the data passed in\n\n        :param str obj: The object type\n        :param dict data: The data for the post\n        :rtype: requests.Response\n\n        \"\"\"\n        LOGGER.debug('Posting data: %r', data)\n        return self.session.post(self._request_url(path),\n                                 data=json.dumps(data or {}),\n                                 headers=self.HEADERS, auth=self.auth,\n                                 verify=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n\n    # Hack to make user code available for import\n    sys.path.append(\".\")\n\n    # Run the specified action\n    oz.initialize()\n    retr = optfn.run(list(oz._actions.values()))\n\n    if retr == optfn.ERROR_RETURN_CODE:\n        sys.exit(-1)\n    elif retr == None:\n        sys.exit(0)\n    elif isinstance(retr, int):\n        sys.exit(retr)\n    else:\n        raise Exception(\"Unexpected return value from action: %s\" % retr)", "response": "Entry - point for oz s cli"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_tree_from_alignment(aln, moltype=DNA, best_tree=False, params={},\\\n    working_dir='/tmp'):\n    \"\"\"Returns a tree from Alignment object aln.\n\n    aln: an cogent.core.alignment.Alignment object, or data that can be used\n    to build one.\n        -  Clearcut only accepts aligned sequences.  Alignment object used to\n        handle unaligned sequences.\n\n    moltype: a cogent.core.moltype object.\n        - NOTE: If moltype = RNA, we must convert to DNA since Clearcut v1.0.8\n        gives incorrect results if RNA is passed in.  'U' is treated as an\n        incorrect character and is excluded from distance calculations.\n\n    best_tree: if True (default:False), uses a slower but more accurate\n    algorithm to build the tree.\n\n    params: dict of parameters to pass in to the Clearcut app controller.\n\n    The result will be an cogent.core.tree.PhyloNode object, or None if tree\n    fails.\n    \"\"\"\n    params['--out'] = get_tmp_filename(working_dir)\n\n    # Create instance of app controller, enable tree, disable alignment\n    app = Clearcut(InputHandler='_input_as_multiline_string', params=params, \\\n                   WorkingDir=working_dir, SuppressStdout=True,\\\n                   SuppressStderr=True)\n    #Input is an alignment\n    app.Parameters['-a'].on()\n    #Turn off input as distance matrix\n    app.Parameters['-d'].off()\n\n    #If moltype = RNA, we must convert to DNA.\n    if moltype == RNA:\n        moltype = DNA\n\n    if best_tree:\n        app.Parameters['-N'].on()\n\n    #Turn on correct moltype\n    moltype_string = moltype.label.upper()\n    app.Parameters[MOLTYPE_MAP[moltype_string]].on()\n\n    # Setup mapping. Clearcut clips identifiers. We will need to remap them.\n    # Clearcut only accepts aligned sequences.  Let Alignment object handle\n    # unaligned sequences.\n    seq_aln = Alignment(aln,MolType=moltype)\n    #get int mapping\n    int_map, int_keys = seq_aln.getIntMap()\n    #create new Alignment object with int_map\n    int_map = Alignment(int_map)\n\n    # Collect result\n    result = app(int_map.toFasta())\n\n    # Build tree\n    tree = DndParser(result['Tree'].read(), constructor=PhyloNode)\n    for node in tree.tips():\n        node.Name = int_keys[node.Name]\n\n    # Clean up\n    result.cleanUp()\n    del(seq_aln, app, result, int_map, int_keys, params)\n\n    return tree", "response": "Builds a tree from an alignment object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_tree_from_distance_matrix(matrix, best_tree=False, params={},\\\n    working_dir='/tmp'):\n    \"\"\"Returns a tree from a distance matrix.\n\n    matrix: a square Dict2D object (cogent.util.dict2d)\n\n    best_tree: if True (default:False), uses a slower but more accurate\n    algorithm to build the tree.\n\n    params: dict of parameters to pass in to the Clearcut app controller.\n\n    The result will be an cogent.core.tree.PhyloNode object, or None if tree\n    fails.\n    \"\"\"\n    params['--out'] = get_tmp_filename(working_dir)\n\n    # Create instance of app controller, enable tree, disable alignment\n    app = Clearcut(InputHandler='_input_as_multiline_string', params=params, \\\n                   WorkingDir=working_dir, SuppressStdout=True,\\\n                   SuppressStderr=True)\n    #Turn off input as alignment\n    app.Parameters['-a'].off()\n    #Input is a distance matrix\n    app.Parameters['-d'].on()\n\n    if best_tree:\n        app.Parameters['-N'].on()\n\n    # Turn the dict2d object into the expected input format\n    matrix_input, int_keys = _matrix_input_from_dict2d(matrix)\n\n    # Collect result\n    result = app(matrix_input)\n\n    # Build tree\n    tree = DndParser(result['Tree'].read(), constructor=PhyloNode)\n\n    # reassign to original names\n    for node in tree.tips():\n        node.Name = int_keys[node.Name]\n\n    # Clean up\n    result.cleanUp()\n    del(app, result, params)\n\n    return tree", "response": "Builds a tree from a distance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _matrix_input_from_dict2d(matrix):\n    #clearcut truncates names to 10 char- need to rename before and\n    #reassign after\n\n    #make a dict of env_index:full name\n    int_keys = dict([('env_' + str(i), k) for i,k in \\\n            enumerate(sorted(matrix.keys()))])\n    #invert the dict\n    int_map = {}\n    for i in int_keys:\n        int_map[int_keys[i]] = i\n\n    #make a new dict2D object with the integer keys mapped to values instead of\n    #the original names\n    new_dists = []\n    for env1 in matrix:\n        for env2 in matrix[env1]:\n            new_dists.append((int_map[env1], int_map[env2], matrix[env1][env2]))\n    int_map_dists = Dict2D(new_dists)\n\n    #names will be fed into the phylipTable function - it is the int map names\n    names = sorted(int_map_dists.keys())\n    rows = []\n    #populated rows with values based on the order of names\n    #the following code will work for a square matrix only\n    for index, key1 in enumerate(names):\n        row = []\n        for key2 in names:\n            row.append(str(int_map_dists[key1][key2]))\n        rows.append(row)\n    input_matrix = phylipMatrix(rows, names)\n    #input needs a trailing whitespace or it will fail!\n    input_matrix += '\\n'\n\n    return input_matrix, int_keys", "response": "makes input for running clearcut on a matrix from a dict2D object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _input_as_multiline_string(self, data):\n        if data:\n            self.Parameters['--in']\\\n                .on(super(Clearcut,self)._input_as_multiline_string(data))\n        return ''", "response": "Writes data to tempfile and sets - infile parameter\nMimeType data -- list of lines to be read"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites data to tempfile and sets - infile parameter MimeType data -- list of lines ready to be written to file", "response": "def _input_as_lines(self,data):\n        \"\"\"Writes data to tempfile and sets -infile parameter\n\n        data -- list of lines, ready to be written to file\n        \"\"\"\n        if data:\n            self.Parameters['--in']\\\n                .on(super(Clearcut,self)._input_as_lines(data))\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tree_filename(self):\n        if self.Parameters['--out']:\n            aln_filename = self._absolute(self.Parameters['--out'].Value)\n        else:\n            raise ValueError, \"No tree output file specified.\"\n        return aln_filename", "response": "Return name of file containing the alignment\n            prefix -- str prefix of alignment file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_request(self, request, uri, **kwargs):\n        attempts = 0\n        response = None\n        while attempts <= self.retries:\n            try:\n                response = request(uri, headers=self.headers, proxies=self.proxy, **kwargs)\n\n            except (requests.ConnectionError, requests.HTTPError) as ce:\n                attempts += 1\n                msg = \"Attempting retry {attempts} after {delay} seconds\".format(attempts=attempts, delay=self.retry_delay)\n                logger.error(ce.__doc__)\n                logger.error(msg)\n                sleep(self.retry_delay)\n            else:\n                break\n        if response is not None:\n            try:\n                response.raise_for_status()\n            except Exception as e:\n                self._handle_api_error(e)\n        else:\n            raise requests.RequestException\n        return self._parser(response.text)", "response": "This method is called by the client to make a request to the New Relic API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_get_request(self, uri, parameters=None, timeout=None):\n        if not timeout:\n            timeout = self.timeout\n        return self._make_request(requests.get, uri, params=parameters, timeout=timeout)", "response": "Make a GET request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_post_request(self, uri, payload, timeout=None):\n        if not timeout:\n            timeout = self.timeout\n        return self._make_request(requests.post, uri, data=payload, timeout=timeout)", "response": "Make a POST request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a request to delete the object with the given uri.", "response": "def _make_delete_request(self, uri, timeout=None):\n        \"\"\"\n        Given a request add in the required parameters and return the parsed\n        XML object.\n        \"\"\"\n        if not timeout:\n            timeout = self.timeout\n        return self._make_request(requests.delete, uri, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing aiohttp session and all open file descriptors and all open files.", "response": "def _close(self):\n        '''\n        Closes aiohttp session and all open file descriptors\n        '''\n        if hasattr(self, 'aiohttp'):\n            if not self.aiohttp.closed:\n                self.aiohttp.close()\n        if hasattr(self, 'file_descriptors'):\n            for fd in self.file_descriptors.values():\n                if not fd.closed:\n                    fd.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checkAndRaise(pageNum, itemsPerPage):\n        if pageNum < 1:\n            raise ErrPaginationLimits(ErrPaginationLimits.ERR_PAGE_NUM)\n        \n        if itemsPerPage < Settings.itemsPerPageMin or itemsPerPage > Settings.itemsPerPageMax:\n            raise ErrPaginationLimits(ErrPaginationLimits.ERR_ITEMS_PER_PAGE)", "response": "Check and Raise an Exception if needed\n            is not a valid entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_packet(data, format=None):\n    unpacked = struct.unpack(BASE_FORMAT, data[:_FORMAT_SIZE])\n    psize, protocol, mac, gateway, time, ptype = unpacked\n    header = Header(psize, protocol, mac, gateway, time, ptype)\n    return header, data[_FORMAT_SIZE:]", "response": "Parses a Lifx data packet into a Header object and a bytestring of the payload data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a bytestring of Lifx payload data into a dictionary of keys and values.", "response": "def parse_payload(data, payload_fmt, *payload_names):\n    \"\"\"\n    Parses a bytestring of Lifx payload data (the bytes after the common\n    fields), as returned by `parse_packet`. Returns a dictionary where the keys\n    are from `payload_names` and the values are the corresponding values from\n    the bytestring.\n    \"\"\"\n    payload = struct.unpack(payload_fmt, data)\n    return dict(zip(payload_names, payload))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_packet(packet_type, gateway, bulb, payload_fmt, *payload_args,\n                 **kwargs):\n    \"\"\"\n    Constructs a Lifx packet, returning a bytestring. The arguments are as\n    follows:\n\n    - `packet_type`, an integer\n    - `gateway`, a 6-byte string containing the mac address of the gateway bulb\n    - `bulb`, a 6-byte string containing either the mac address of the target\n      bulb or `ALL_BULBS`\n    - `payload_fmt`, a `struct`-compatible string that describes the format\n      of the payload part of the packet\n    - `payload_args`, the values to use to build the payload part of the packet\n\n    Additionally, the `protocol` keyword argument can be used to override the\n    protocol field in the packet.\n    \"\"\"\n    protocol = kwargs.get('protocol', COMMAND_PROTOCOL)\n\n    packet_fmt = BASE_FORMAT + payload_fmt\n    packet_size = struct.calcsize(packet_fmt)\n    return struct.pack(packet_fmt,\n                       packet_size,\n                       protocol,\n                       bulb,\n                       gateway,\n                       0,  # timestamp\n                       packet_type,\n                       *payload_args)", "response": "Builds a Lifx packet from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _unbytes(bytestr):\n    return ''.join(chr(int(bytestr[k:k + 2], 16))\n                   for k in range(0, len(bytestr), 2))", "response": "Returns a bytestring from the human - friendly string returned by _bytes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _spawn(func, *args, **kwargs):\n    thr = Thread(target=func, args=args, kwargs=kwargs)\n    thr.daemon = True\n    thr.start()\n    return thr", "response": "Spawns a thread and returns the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a callback function to be run when a message of type event is received.", "response": "def register(self, event, fn):\n        \"\"\"\n        Tell the object to run `fn` whenever a message of type `event` is\n        received.\n        \"\"\"\n        self._callbacks.setdefault(event, []).append(fn)\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self, event, *args, **kwargs):\n        self._queue.put((event, args, kwargs))", "response": "Schedule a callback for event passing args and kwargs to each\n        registered callback handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        while True:\n            msg = self._queue.get()\n            if msg is _SHUTDOWN:\n                break\n            event, args, kwargs = msg\n            self._logger('<< %s', event)\n            for func in self._callbacks.get(event, []):\n                func(*args, **kwargs)", "response": "Process all callbacks until stop is called."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing all incoming packets until stop is called.", "response": "def run(self):\n        \"\"\"\n        Process all incoming packets, until `stop()` is called. Intended to run\n        in its own thread.\n        \"\"\"\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        sock.bind(self._addr)\n        sock.settimeout(self._timeout)\n        with closing(sock):\n            while not self._shutdown.is_set():\n                try:\n                    data, addr = sock.recvfrom(self._buffer_size)\n                except socket.timeout:\n                    continue\n\n                header, rest = parse_packet(data)\n                if header.packet_type in _PAYLOADS:\n                    payload = parse_payload(rest,\n                                            *_PAYLOADS[header.packet_type])\n                    self._callbacks.put(header.packet_type,\n                                        header, payload, None, addr)\n                else:\n                    self._callbacks.put(EVENT_UNKNOWN,\n                                        header, None, rest, addr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        while True:\n            to_send = self._queue.get()\n            if to_send is _SHUTDOWN:\n                break\n\n            # If we get a gateway object, connect to it. Otherwise, assume\n            # it's a bytestring and send it out on the socket.\n            if isinstance(to_send, Gateway):\n                self._gateway = to_send\n                self._connected.set()\n            else:\n                if not self._gateway:\n                    raise SendException('no gateway')\n                dest = (self._gateway.addr, self._gateway.port)\n                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n                sock.sendto(to_send, dest)", "response": "Process all outgoing packets until stop is called."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        while True:\n            msg = self._queue.get()\n            if msg is _SHUTDOWN:\n                break\n            if self._enabled:\n                print msg", "response": "Process all log messages until stop is called."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a gateway is discovered.", "response": "def _on_gateway(self, header, payload, rest, addr):\n        \"\"\"\n        Records a discovered gateway, for connecting to later.\n        \"\"\"\n        if payload.get('service') == SERVICE_UDP:\n            self.gateway = Gateway(addr[0], payload['port'], header.gateway)\n            self.gateway_found_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_power_state(self, header, payload, rest, addr):\n        with self.lock:\n            self.power_state[header.mac] = payload\n            if len(self.power_state) >= self.num_bulbs:\n                self.power_state_event.set()\n\n        self.callbacks.put(EVENT_POWER_STATE, self.get_bulb(header.mac),\n                           is_on=bool(payload['is_on']))", "response": "Called when a power state is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_light_state(self, header, payload, rest, addr):\n        with self.lock:\n            label = payload['label'].strip('\\x00')\n            self.bulbs[header.mac] = bulb = Bulb(label, header.mac)\n            if len(self.bulbs) >= self.num_bulbs:\n                self.bulbs_found_event.set()\n\n            self.light_state[header.mac] = payload\n            if len(self.light_state) >= self.num_bulbs:\n                self.light_state_event.set()\n\n        self.callbacks.put(EVENT_LIGHT_STATE, bulb,\n                           raw=payload,\n                           hue=(payload['hue'] / float(0xffff) * 360) % 360.0,\n                           saturation=payload['sat'] / float(0xffff),\n                           brightness=payload['bright'] / float(0xffff),\n                           kelvin=payload['kelvin'],\n                           is_on=bool(payload['power']))", "response": "Called when a light state is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Bulb object corresponding to the bulb with the given mac address.", "response": "def get_bulb(self, mac):\n        \"\"\"\n        Returns a Bulb object corresponding to the bulb with the mac address\n        `mac` (a 6-byte bytestring).\n        \"\"\"\n        return self.bulbs.get(mac, Bulb('Bulb %s' % _bytes(mac), mac))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, packet_type, bulb, packet_fmt, *packet_args):\n        packet = build_packet(packet_type, self.gateway.mac, bulb,\n                              packet_fmt, *packet_args)\n        self.logger('>> %s', _bytes(packet))\n        self.sender.put(packet)", "response": "Builds and sends a packet to one or more bulbs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_power_state(self, is_on, bulb=ALL_BULBS, timeout=None):\n        with _blocking(self.lock, self.power_state, self.light_state_event,\n                       timeout):\n            self.send(REQ_SET_POWER_STATE,\n                      bulb, '2s', '\\x00\\x01' if is_on else '\\x00\\x00')\n            self.send(REQ_GET_LIGHT_STATE, ALL_BULBS, '')\n        return self.power_state", "response": "Sets the power state of one or more bulbs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the light state of one or more bulbs.", "response": "def set_light_state_raw(self, hue, saturation, brightness, kelvin,\n                            bulb=ALL_BULBS, timeout=None):\n        \"\"\"\n        Sets the (low-level) light state of one or more bulbs.\n        \"\"\"\n        with _blocking(self.lock, self.light_state, self.light_state_event,\n                       timeout):\n            self.send(REQ_SET_LIGHT_STATE, bulb, 'xHHHHI',\n                      hue, saturation, brightness, kelvin, 0)\n            self.send(REQ_GET_LIGHT_STATE, ALL_BULBS, '')\n        return self.light_state"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_light_state(self, hue, saturation, brightness, kelvin,\n                        bulb=ALL_BULBS, timeout=None):\n        \"\"\"\n        Sets the light state of one or more bulbs.\n\n        Hue is a float from 0 to 360, saturation and brightness are floats from\n        0 to 1, and kelvin is an integer.\n        \"\"\"\n        raw_hue = int((hue % 360) / 360.0 * 0xffff) & 0xffff\n        raw_sat = int(saturation * 0xffff) & 0xffff\n        raw_bright = int(brightness * 0xffff) & 0xffff\n        return self.set_light_state_raw(raw_hue, raw_sat, raw_bright, kelvin,\n                                        bulb, timeout)", "response": "Sets the light state of one or more bulbs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a function to be called when packet data is received with a specific type.", "response": "def on_packet(self, packet_type):\n        \"\"\"\n        Registers a function to be called when packet data is received with a\n        specific type.\n        \"\"\"\n        def _wrapper(fn):\n            return self.callbacks.register(packet_type, fn)\n        return _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to a gateway blocking until a connection is made and bulbs are found.", "response": "def connect(self, attempts=20, delay=0.5):\n        \"\"\"\n        Connects to a gateway, blocking until a connection is made and bulbs\n        are found.\n\n        Step 1: send a gateway discovery packet to the broadcast address, wait\n        until we've received some info about the gateway.\n\n        Step 2: connect to a discovered gateway, wait until the connection has\n        been completed.\n\n        Step 3: ask for info about bulbs, wait until we've found the number of\n        bulbs we expect.\n\n        Raises a ConnectException if any of the steps fail.\n        \"\"\"\n        # Broadcast discovery packets until we find a gateway.\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        with closing(sock):\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n            discover_packet = build_packet(REQ_GATEWAY,\n                                           ALL_BULBS, ALL_BULBS, '',\n                                           protocol=DISCOVERY_PROTOCOL)\n\n            for _, ok in _retry(self.gateway_found_event, attempts, delay):\n                sock.sendto(discover_packet, BROADCAST_ADDRESS)\n        if not ok:\n            raise ConnectException('discovery failed')\n        self.callbacks.put(EVENT_DISCOVERED)\n\n        # Tell the sender to connect to the gateway until it does.\n        for _, ok in _retry(self.sender.is_connected, 1, 3):\n            self.sender.put(self.gateway)\n        if not ok:\n            raise ConnectException('connection failed')\n        self.callbacks.put(EVENT_CONNECTED)\n\n        # Send light state packets to the gateway until we find bulbs.\n        for _, ok in _retry(self.bulbs_found_event, attempts, delay):\n            self.send(REQ_GET_LIGHT_STATE, ALL_BULBS, '')\n        if not ok:\n            raise ConnectException('only found %d of %d bulbs' % (\n                                   len(self.bulbs), self.num_bulbs))\n        self.callbacks.put(EVENT_BULBS_FOUND)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the multiconvert task.", "response": "async def run_multiconvert(self, url_string, to_type):\n        '''\n        Enqueues in succession all conversions steps necessary to take the\n        given URL and convert it to to_type, storing the result in the cache\n        '''\n        async def enq_convert(*args):\n            await self.enqueue(Task.CONVERT, args)\n        await tasks.multiconvert(url_string, to_type, enq_convert)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_otu_list(lines, precision=0.0049):\n    for line in lines:\n        if is_empty(line):\n            continue\n        tokens = line.strip().split('\\t')\n\n        distance_str = tokens.pop(0)\n        if distance_str.lstrip().lower().startswith('u'):\n            distance = 0.0\n        elif distance_str == '0.0':\n            distance = float(precision)\n        else:\n            distance = float(distance_str)\n\n        num_otus = int(tokens.pop(0))\n        otu_list = [t.split(',') for t in tokens]\n\n        yield (distance, otu_list)", "response": "Parser for mothur. list file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclassify a set of sequences using Mothur s naive bayes method.", "response": "def mothur_classify_file(\n        query_file, ref_fp, tax_fp, cutoff=None, iters=None, ksize=None,\n        output_fp=None, tmp_dir=None):\n    \"\"\"Classify a set of sequences using Mothur's naive bayes method\n\n    Dashes are used in Mothur to provide multiple filenames.  A\n    filepath with a dash typically breaks an otherwise valid command\n    in Mothur.  This wrapper script makes a copy of both files, ref_fp\n    and tax_fp, to ensure that the path has no dashes.\n\n    For convenience, we also ensure that each taxon list in the\n    id-to-taxonomy file ends with a semicolon.\n    \"\"\"\n    if tmp_dir is None:\n        tmp_dir = gettempdir()\n\n    ref_seq_ids = set()\n\n    user_ref_file = open(ref_fp)\n    tmp_ref_file = NamedTemporaryFile(dir=tmp_dir, suffix=\".ref.fa\")\n    for seq_id, seq in parse_fasta(user_ref_file):\n        id_token = seq_id.split()[0]\n        ref_seq_ids.add(id_token)\n        tmp_ref_file.write(\">%s\\n%s\\n\" % (seq_id, seq))\n    tmp_ref_file.seek(0)\n\n    user_tax_file = open(tax_fp)\n    tmp_tax_file = NamedTemporaryFile(dir=tmp_dir, suffix=\".tax.txt\")\n    for line in user_tax_file:\n        line = line.rstrip()\n        if not line:\n            continue\n\n        # MOTHUR is particular that each assignment end with a semicolon.\n        if not line.endswith(\";\"):\n            line = line + \";\"\n\n        id_token, _, _ = line.partition(\"\\t\")\n        if id_token in ref_seq_ids:\n            tmp_tax_file.write(line)\n            tmp_tax_file.write(\"\\n\")\n    tmp_tax_file.seek(0)\n\n    params = {\"reference\": tmp_ref_file.name, \"taxonomy\": tmp_tax_file.name}\n    if cutoff is not None:\n        params[\"cutoff\"] = cutoff\n    if ksize is not None:\n        params[\"ksize\"] = ksize\n    if iters is not None:\n        params[\"iters\"] = iters\n\n    # Create a temporary working directory to accommodate mothur's output\n    # files, which are generated automatically based on the input\n    # file.\n    work_dir = mkdtemp(dir=tmp_dir)\n\n    app = MothurClassifySeqs(\n        params, InputHandler='_input_as_lines', WorkingDir=work_dir,\n        TmpDir=tmp_dir)\n    result = app(query_file)\n\n    # Force evaluation so we can safely clean up files\n    assignments = list(parse_mothur_assignments(result['assignments']))\n    result.cleanUp()\n    rmtree(work_dir)\n\n    if output_fp is not None:\n        f = open(output_fp, \"w\")\n        for query_id, taxa, conf in assignments:\n            taxa_str = \";\".join(taxa)\n            f.write(\"%s\\t%s\\t%.2f\\n\" % (query_id, taxa_str, conf))\n        f.close()\n        return None\n    return dict((a, (b, c)) for a, b, c in assignments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compile_mothur_script(self):\n        def format_opts(*opts):\n            \"\"\"Formats a series of options for a Mothur script\"\"\"\n            return ', '.join(filter(None, map(str, opts)))\n        vars = {\n            'in': self._input_filename,\n            'unique': self._derive_unique_path(),\n            'dist': self._derive_dist_path(),\n            'names': self._derive_names_path(),\n            'cluster_opts': format_opts(\n                self.Parameters['method'],\n                self.Parameters['cutoff'],\n                self.Parameters['precision'],\n            ),\n        }\n        script = (\n            '#'\n            'unique.seqs(fasta=%(in)s); '\n            'dist.seqs(fasta=%(unique)s); '\n            'read.dist(column=%(dist)s, name=%(names)s); '\n            'cluster(%(cluster_opts)s)' % vars\n        )\n        return script", "response": "Returns a Mothur batch script as a string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _derive_log_path(self):\n        filenames = listdir(self.WorkingDir)\n        lognames = [\n            x for x in filenames if re.match(\n                \"^mothur\\.\\d+\\.logfile$\",\n                x)]\n        if not lognames:\n            raise ApplicationError(\n                'No log file detected in directory %s. Contents: \\n\\t%s' % (\n                    input_dir, '\\n\\t'.join(possible_logfiles)))\n        most_recent_logname = sorted(lognames, reverse=True)[0]\n        return path.join(self.WorkingDir, most_recent_logname)", "response": "Guess the logfile path produced by Mothur."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _derive_unique_path(self):\n        base, ext = path.splitext(self._input_filename)\n        return '%s.unique%s' % (base, ext)", "response": "Guess unique sequences path produced by Mothur"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nabbreviate form of clustering method parameter.", "response": "def __get_method_abbrev(self):\n        \"\"\"Abbreviated form of clustering method parameter.\n\n        Used to guess output filenames for MOTHUR.\n        \"\"\"\n        abbrevs = {\n            'furthest': 'fn',\n            'nearest': 'nn',\n            'average': 'an',\n        }\n        if self.Parameters['method'].isOn():\n            method = self.Parameters['method'].Value\n        else:\n            method = self.Parameters['method'].Default\n        return abbrevs[method]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nguesses otu list file path produced by Mothur", "response": "def _derive_list_path(self):\n        \"\"\"Guess otu list file path produced by Mothur\"\"\"\n        base, ext = path.splitext(self._input_filename)\n        return '%s.unique.%s.list' % (base, self.__get_method_abbrev())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nguessing rank abundance file path produced by Mothur", "response": "def _derive_rank_abundance_path(self):\n        \"\"\"Guess rank abundance file path produced by Mothur\"\"\"\n        base, ext = path.splitext(self._input_filename)\n        return '%s.unique.%s.rabund' % (base, self.__get_method_abbrev())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nguessing species abundance file path produced by Mothur", "response": "def _derive_species_abundance_path(self):\n        \"\"\"Guess species abundance file path produced by Mothur\"\"\"\n        base, ext = path.splitext(self._input_filename)\n        return '%s.unique.%s.sabund' % (base, self.__get_method_abbrev())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getTmpFilename(self, tmp_dir=None, prefix='tmp', suffix='.txt'):\n        # Override to change default constructor to str(). FilePath\n        # objects muck up the Mothur script.\n        return super(Mothur, self).getTmpFilename(\n            tmp_dir=tmp_dir, prefix=prefix, suffix=suffix,\n            result_constructor=str)", "response": "Returns a temporary filename that can be used for storing the log file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a multiline string to a temp file and return the filename", "response": "def _input_as_multiline_string(self, data):\n        \"\"\"Write multiline string to temp file, return filename\n\n        data: a multiline string to be written to a file.\n        \"\"\"\n        self._input_filename = self.getTmpFilename(\n            self.WorkingDir, suffix='.fasta')\n        with open(self._input_filename, 'w') as f:\n            f.write(data)\n        return self._input_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites sequence of lines to temp file", "response": "def _input_as_lines(self, data):\n        \"\"\"Write sequence of lines to temp file, return filename\n\n        data: a sequence to be written to a file, each element of the\n            sequence will compose a line in the file\n\n        * Note: '\\n' will be stripped off the end of each sequence\n            element before writing to a file in order to avoid\n            multiple new lines accidentally be written to a file\n        \"\"\"\n        self._input_filename = self.getTmpFilename(\n            self.WorkingDir, suffix='.fasta')\n        with open(self._input_filename, 'w') as f:\n            # Use lazy iteration instead of list comprehension to\n            # prevent reading entire file into memory\n            for line in data:\n                f.write(str(line).strip('\\n'))\n                f.write('\\n')\n        return self._input_filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies the provided file to WorkingDir and returns the new filename", "response": "def _input_as_path(self, data):\n        \"\"\"Copys the provided file to WorkingDir and returns the new filename\n\n        data: path or filename\n        \"\"\"\n        self._input_filename = self.getTmpFilename(\n            self.WorkingDir, suffix='.fasta')\n        copyfile(data, self._input_filename)\n        return self._input_filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_WorkingDir(self, path):\n        self._curr_working_dir = path\n        try:\n            mkdir(self.WorkingDir)\n        except OSError:\n            # Directory already exists\n            pass", "response": "Sets the working directory for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting a series of function arguments in a Mothur script.", "response": "def _format_function_arguments(self, opts):\n        \"\"\"Format a series of function arguments in a Mothur script.\"\"\"\n        params = [self.Parameters[x] for x in opts]\n        return ', '.join(filter(None, map(str, params)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Mothur batch script as a string", "response": "def _compile_mothur_script(self):\n        \"\"\"Returns a Mothur batch script as a string\"\"\"\n        fasta = self._input_filename\n\n        required_params = [\"reference\", \"taxonomy\"]\n        for p in required_params:\n            if self.Parameters[p].Value is None:\n                raise ValueError(\"Must provide value for parameter %s\" % p)\n        optional_params = [\"ksize\", \"cutoff\", \"iters\"]\n        args = self._format_function_arguments(\n            required_params + optional_params)\n        script = '#classify.seqs(fasta=%s, %s)' % (fasta, args)\n        return script"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a Parameter object to the instance.", "response": "def _add_parameter(self, parameter):\n        '''\n        Force adds a `Parameter` object to the instance.\n        '''\n        if isinstance(parameter, MethodParameter):\n            # create a bound instance of the MethodParameter\n            parameter = parameter.bind(alloy=self)\n        self._parameters[parameter.name] = parameter\n        for alias in parameter.aliases:\n            self._aliases[alias] = parameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_parameter(self, parameter, overload=False):\n        '''\n        Adds a `Parameter` object to the instance.\n        \n        If a `Parameter` with the same name or alias has already been added\n        and `overload` is False (the default), a `ValueError` is thrown.\n        \n        If a class member or method with the same name or alias is already\n        defined, a `ValueError` is thrown, regardless of the value of overload.\n        '''\n        if not isinstance(parameter, Parameter):\n            raise TypeError('`parameter` must be an instance of `Parameter`')\n\n        if hasattr(self, parameter.name):\n            item = getattr(self, parameter.name)\n            if not isinstance(item, Parameter):\n                raise ValueError('\"{}\" is already a class member or method.'\n                                 ''.format(parameter.name))\n            elif not overload:\n                raise ValueError('Parameter \"{}\" has already been added'\n                                 ' and overload is False.'\n                                 ''.format(parameter.name))\n        if parameter.name in self._parameters and not overload:\n            raise ValueError('Parameter \"{}\" has already been added'\n                             ' and overload is False.'\n                             ''.format(parameter.name))\n        for alias in parameter.aliases:\n            if alias in self._aliases and not overload:\n                raise ValueError('Alias \"{}\" has already been added'\n                                 ' and overload is False.'\n                                 ''.format(parameter.name))\n        self._add_parameter(parameter)", "response": "Adds a parameter object to the instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the value of the named parameter if present or the value of default otherwise.", "response": "def get_parameter(self, name, default=None):\n        '''\n        Returns the named parameter if present, or the value of `default`,\n        otherwise.\n        '''\n        if hasattr(self, name):\n            item = getattr(self, name)\n            if isinstance(item, Parameter):\n                return item\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_unique_parameters(self):\n        '''\n        Returns a list of the unique parameters (no duplicates).\n        '''\n        # start with parameters in the `_parameters` dictionary\n        parameters = self._parameters.values()\n        # add parameters defined with the class\n        for name in dir(self):\n            item = getattr(self, name)\n            if isinstance(item, Parameter):\n                if item.name not in self._parameters:\n                    parameters.append(item)\n        return parameters", "response": "Returns a list of the unique parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_log_level(self, level: str) -> None:\n        if level == 'info':\n            to_set = logging.INFO\n        if level == 'debug':\n            to_set = logging.DEBUG\n        if level == 'error':\n            to_set = logging.ERROR\n        self.log.setLevel(to_set)", "response": "Override the default log level of the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _request_bulk(self, urls: List[str]) -> List:\n        if not urls:\n            raise Exception(\"No results were found\")\n        session: FuturesSession = FuturesSession(max_workers=len(urls))\n        self.log.info(\"Bulk requesting: %d\" % len(urls))\n        futures = [session.get(u, headers=gen_headers(), timeout=3) for u in urls]\n        done, incomplete = wait(futures)\n        results: List = list()\n        for response in done:\n            try:\n                results.append(response.result())\n            except Exception as err:\n                self.log.warn(\"Failed result: %s\" % err)\n        return results", "response": "Batch the requests going out."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbind this MethodParameter to an alloy.", "response": "def bind(self, alloy):\n        '''\n        Shallow copies this MethodParameter, and binds it to an alloy.\n        This is required before calling.\n        '''\n        param = MethodParameter(self.name, self.method, self.dependencies,\n                                self.units, self.aliases, self._references)\n        param.alloy = alloy\n        return param"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naligns unaligned sequences in a new order.", "response": "def align_unaligned_seqs(seqs,moltype=DNA,params=None,accurate=False):\n    \"\"\"Aligns unaligned sequences\n\n    seqs: either list of sequence objects or list of strings\n    add_seq_names: boolean. if True, sequence names are inserted in the list\n        of sequences. if False, it assumes seqs is a list of lines of some\n        proper format that the program can handle\n    \"\"\"\n    #create SequenceCollection object from seqs\n    seq_collection = SequenceCollection(seqs,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    int_map, int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    int_map = SequenceCollection(int_map,MolType=moltype)\n    #Create Mafft app.\n    app = Mafft(InputHandler='_input_as_multiline_string',params=params)\n\n    #Turn on correct moltype\n    moltype_string = moltype.label.upper()\n    app.Parameters[MOLTYPE_MAP[moltype_string]].on()\n\n    #Do not report progress\n    app.Parameters['--quiet'].on()\n\n    #More accurate alignment, sacrificing performance.\n    if accurate:\n        app.Parameters['--globalpair'].on()\n        app.Parameters['--maxiterate'].Value=1000\n\n    #Get results using int_map as input to app\n    res = app(int_map.toFasta())\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['StdOut']))\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        new_alignment[int_keys[k]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    del(seq_collection,int_map,int_keys,app,res,alignment)\n\n    return new_alignment"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an Alignment object from seqs and existing Alignment object.", "response": "def add_seqs_to_alignment(seqs, aln, moltype, params=None, accurate=False):\n    \"\"\"Returns an Alignment object from seqs and existing Alignment.\n\n    seqs: a cogent.core.sequence.Sequence object, or data that can be used\n    to build one.\n\n    aln: an cogent.core.alignment.Alignment object, or data that can be used\n    to build one\n\n    params: dict of parameters to pass in to the Mafft app controller.\n    \"\"\"\n    #create SequenceCollection object from seqs\n    seq_collection = SequenceCollection(seqs,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    seq_int_map, seq_int_keys = seq_collection.getIntMap()\n    #Create SequenceCollection from int_map.\n    seq_int_map = SequenceCollection(seq_int_map,MolType=moltype)\n\n    #create Alignment object from aln\n    aln = Alignment(aln,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln_int_map, aln_int_keys = aln.getIntMap(prefix='seqn_')\n    #Create SequenceCollection from int_map.\n    aln_int_map = Alignment(aln_int_map,MolType=moltype)\n\n    #Update seq_int_keys with aln_int_keys\n    seq_int_keys.update(aln_int_keys)\n\n    #Create Mafft app.\n    app = Mafft(InputHandler='_input_as_multiline_string',\\\n        params=params,\n        SuppressStderr=True)\n\n    #Turn on correct moltype\n    moltype_string = moltype.label.upper()\n    app.Parameters[MOLTYPE_MAP[moltype_string]].on()\n\n    #Do not report progress\n    app.Parameters['--quiet'].on()\n\n    #Add aln_int_map as seed alignment\n    app.Parameters['--seed'].on(\\\n        app._tempfile_as_multiline_string(aln_int_map.toFasta()))\n\n    #More accurate alignment, sacrificing performance.\n    if accurate:\n        app.Parameters['--globalpair'].on()\n        app.Parameters['--maxiterate'].Value=1000\n\n    #Get results using int_map as input to app\n    res = app(seq_int_map.toFasta())\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['StdOut']))\n\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        key = k.replace('_seed_','')\n        new_alignment[seq_int_keys[key]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    remove(app.Parameters['--seed'].Value)\n    del(seq_collection,seq_int_map,seq_int_keys,\\\n        aln,aln_int_map,aln_int_keys,app,res,alignment)\n\n    return new_alignment"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef align_two_alignments(aln1, aln2, moltype, params=None):\n    #create SequenceCollection object from seqs\n    aln1 = Alignment(aln1,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln1_int_map, aln1_int_keys = aln1.getIntMap()\n    #Create SequenceCollection from int_map.\n    aln1_int_map = Alignment(aln1_int_map,MolType=moltype)\n\n    #create Alignment object from aln\n    aln2 = Alignment(aln2,MolType=moltype)\n    #Create mapping between abbreviated IDs and full IDs\n    aln2_int_map, aln2_int_keys = aln2.getIntMap(prefix='seqn_')\n    #Create SequenceCollection from int_map.\n    aln2_int_map = Alignment(aln2_int_map,MolType=moltype)\n\n    #Update aln1_int_keys with aln2_int_keys\n    aln1_int_keys.update(aln2_int_keys)\n\n    #Create Mafft app.\n    app = Mafft(InputHandler='_input_as_paths',\\\n        params=params,\n        SuppressStderr=False)\n    app._command = 'mafft-profile'\n\n    aln1_path = app._tempfile_as_multiline_string(aln1_int_map.toFasta())\n    aln2_path = app._tempfile_as_multiline_string(aln2_int_map.toFasta())\n    filepaths = [aln1_path,aln2_path]\n\n    #Get results using int_map as input to app\n    res = app(filepaths)\n\n    #Get alignment as dict out of results\n    alignment = dict(parse_fasta(res['StdOut']))\n\n    #Make new dict mapping original IDs\n    new_alignment = {}\n    for k,v in alignment.items():\n        key = k.replace('_seed_','')\n        new_alignment[aln1_int_keys[key]]=v\n    #Create an Alignment object from alignment dict\n    new_alignment = Alignment(new_alignment,MolType=moltype)\n    #Clean up\n    res.cleanUp()\n    remove(aln1_path)\n    remove(aln2_path)\n    remove('pre')\n    remove('trace')\n    del(aln1,aln1_int_map,aln1_int_keys,\\\n        aln2,aln2_int_map,aln2_int_keys,app,res,alignment)\n\n    return new_alignment", "response": "Aligns two Alignments and returns a cogent. core. alignment. Alignment object from those alignments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a request to the server and return the result", "response": "def doQuery(self, url, method='GET', getParmeters=None, postParameters=None, files=None, extraHeaders={}, session={}):\n        \"\"\"Send a request to the server and return the result\"\"\"\n\n        # Build headers\n        headers = {}\n\n        if not postParameters:\n            postParameters = {}\n\n        for key, value in extraHeaders.iteritems():\n            # Fixes #197 for values with utf-8 chars to be passed into plugit\n            if isinstance(value, basestring):\n                headers['X-Plugit-' + key] = value.encode('utf-8')\n            else:\n                headers['X-Plugit-' + key] = value\n\n        for key, value in session.iteritems():\n            headers['X-Plugitsession-' + key] = value\n            if 'Cookie' not in headers:\n                headers['Cookie'] = ''\n            headers['Cookie'] += key + '=' + str(value) + '; '\n\n        if method == 'POST':\n            if not files:\n                r = requests.post(self.baseURI + '/' + url, params=getParmeters, data=postParameters, stream=True, headers=headers)\n            else:\n                # Special way, for big files\n                # Requests is not usable: https://github.com/shazow/urllib3/issues/51\n\n                from poster.encode import multipart_encode, MultipartParam\n                from poster.streaminghttp import register_openers\n                import urllib2\n                import urllib\n\n                # Register the streaming http handlers with urllib2\n                register_openers()\n\n                # headers contains the necessary Content-Type and Content-Length\n                # datagen is a generator object that yields the encoded parameters\n                data = []\n                for x in postParameters:\n                    if isinstance(postParameters[x], list):\n                        for elem in postParameters[x]:\n                            data.append((x, elem))\n                    else:\n                        data.append((x, postParameters[x]))\n\n                for f in files:\n                    data.append((f, MultipartParam(f, fileobj=open(files[f].temporary_file_path(), 'rb'), filename=files[f].name)))\n\n                datagen, headers_multi = multipart_encode(data)\n\n                headers.update(headers_multi)\n\n                if getParmeters:\n                    get_uri = '?' + urllib.urlencode(getParmeters)\n                else:\n                    get_uri = ''\n\n                # Create the Request object\n                request = urllib2.Request(self.baseURI + '/' + url + get_uri, datagen, headers)\n\n                re = urllib2.urlopen(request)\n\n                from requests import Response\n\n                r = Response()\n                r.status_code = re.getcode()\n                r.headers = dict(re.info())\n                r.encoding = \"application/json\"\n                r.raw = re.read()\n                r._content = r.raw\n\n                return r\n\n        else:\n            # Call the function based on the method.\n            r = requests.request(method.upper(), self.baseURI + '/' + url, params=getParmeters, stream=True, headers=headers, allow_redirects=True)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ping(self):\n\n        randomToken = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for x in range(32))\n\n        r = self.doQuery('ping?data=' + randomToken)\n\n        if r.status_code == 200:  # Query ok ?\n            if r.json()['data'] == randomToken:  # Token equal ?\n                return True\n        return False", "response": "Return true if the server successfully pinged"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the server use the same version of our protocol", "response": "def checkVersion(self):\n        \"\"\"Check if the server use the same version of our protocol\"\"\"\n\n        r = self.doQuery('version')\n\n        if r.status_code == 200:  # Query ok ?\n            data = r.json()\n\n            if data['result'] == 'Ok' and data['version'] == self.PI_API_VERSION and data['protocol'] == self.PI_API_NAME:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a new mail to a plugit server", "response": "def newMail(self, data, message):\n        \"\"\"Send a mail to a plugit server\"\"\"\n        r = self.doQuery('mail', method='POST', postParameters={'response_id': str(data), 'message': str(message)})\n\n        if r.status_code == 200:  # Query ok ?\n            data = r.json()\n\n            return data['result'] == 'Ok'\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getMedia(self, uri):\n\n        r = self.doQuery('media/' + uri)\n\n        if r.status_code == 200:\n            content_type = 'application/octet-stream'\n\n            if 'content-type' in r.headers:\n                content_type = r.headers['content-type']\n\n            cache_control = None\n\n            if 'cache-control' in r.headers:\n                cache_control = r.headers['cache-control']\n\n            return (r.content, content_type, cache_control)\n        else:\n            return (None, None, None)", "response": "Return a tuple with a media and his content - type. Don t cache anything!"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getMeta(self, uri):\n\n        action = urlparse(uri).path\n\n        mediaKey = self.cacheKey + '_meta_' + action\n        mediaKey = mediaKey.replace(' ', '__')\n\n        meta = cache.get(mediaKey, None)\n\n        # Nothing found -> Retrieve it from the server and cache it\n        if not meta:\n\n            r = self.doQuery('meta/' + uri)\n\n            if r.status_code == 200:  # Get the content if there is not problem. If there is, template will stay to None\n                meta = r.json()\n\n            if 'expire' not in r.headers:\n                expire = 5 * 60  # 5 minutes of cache if the server didn't specified anything\n            else:\n                expire = int((parser.parse(r.headers['expire']) - datetime.datetime.now(tzutc())).total_seconds())  # Use the server value for cache\n\n            if expire > 0:  # Do the server want us to cache ?\n                cache.set(mediaKey, meta, expire)\n\n        return meta", "response": "Return meta information about an action. Cache the result as specified by the server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the template for an action. Cache the result. Can use an optional meta parameter with meta information.", "response": "def getTemplate(self, uri, meta=None):\n        \"\"\"Return the template for an action. Cache the result. Can use an optional meta parameter with meta information\"\"\"\n\n        if not meta:\n\n            metaKey = self.cacheKey + '_templatesmeta_cache_' + uri\n\n            meta = cache.get(metaKey, None)\n\n            if not meta:\n                meta = self.getMeta(uri)\n                cache.set(metaKey, meta, 15)\n\n        if not meta:  # No meta, can return a template\n            return None\n\n        # Let's find the template in the cache\n        action = urlparse(uri).path\n\n        templateKey = self.cacheKey + '_templates_' + action + '_' + meta['template_tag']\n        template = cache.get(templateKey, None)\n\n        # Nothing found -> Retrieve it from the server and cache it\n        if not template:\n\n            r = self.doQuery('template/' + uri)\n\n            if r.status_code == 200:  # Get the content if there is not problem. If there is, template will stay to None\n                template = r.content\n\n            cache.set(templateKey, template, None)  # None = Cache forever\n\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an iterable of offset vectors return the shortest list of offset vectors that can be constructed from which all the offset vectors can be constructed.", "response": "def component_offsetvectors(offsetvectors, n):\n\t\"\"\"\n\tGiven an iterable of offset vectors, return the shortest list of\n\tthe unique n-instrument offset vectors from which all the vectors\n\tin the input iterable can be constructed.  This can be used to\n\tdetermine the minimal set of n-instrument coincs required to\n\tconstruct all of the coincs for all of the requested instrument and\n\toffset combinations in a set of offset vectors.\n\n\tIt is assumed that the coincs for the vector {\"H1\": 0, \"H2\": 10,\n\t\"L1\": 20} can be constructed from the coincs for the vectors {\"H1\":\n\t0, \"H2\": 10} and {\"H2\": 0, \"L1\": 10}, that is only the relative\n\toffsets are significant in determining if two events are\n\tcoincident, not the absolute offsets.  This assumption is not true\n\tfor the standard inspiral pipeline, where the absolute offsets are\n\tsignificant due to the periodic wrapping of triggers around rings.\n\t\"\"\"\n\t#\n\t# collect unique instrument set / deltas combinations\n\t#\n\n\tdelta_sets = {}\n\tfor vect in offsetvectors:\n\t\tfor instruments in iterutils.choices(sorted(vect), n):\n\t\t\t# NOTE:  the arithmetic used to construct the\n\t\t\t# offsets *must* match the arithmetic used by\n\t\t\t# offsetvector.deltas so that the results of the\n\t\t\t# two can be compared to each other without worry\n\t\t\t# of floating-point round off confusing things.\n\t\t\tdelta_sets.setdefault(instruments, set()).add(tuple(vect[instrument] - vect[instruments[0]] for instrument in instruments))\n\n\t#\n\t# translate into a list of normalized n-instrument offset vectors\n\t#\n\n\treturn [offsetvector(zip(instruments, deltas)) for instruments, delta_set in delta_sets.items() for deltas in delta_set]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deltas(self):\n\t\t# FIXME:  instead of raising ValueError when the\n\t\t# offsetvector is empty this should return an empty\n\t\t# dictionary.  the inverse, .fromdeltas() accepts\n\t\t# empty dictionaries\n\t\t# NOTE:  the arithmetic used to construct the offsets\n\t\t# *must* match the arithmetic used by\n\t\t# time_slide_component_vectors() so that the results of the\n\t\t# two functions can be compared to each other without worry\n\t\t# of floating-point round off confusing things.\n\t\trefkey = self.refkey\n\t\trefoffset = self[refkey]\n\t\treturn dict(((refkey, key), self[key] - refoffset) for key in self)", "response": "Returns a dictionary of relative offsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contains(self, other):\n\t\treturn offsetvector((key, offset) for key, offset in self.items() if key in other).deltas == other.deltas", "response": "Returns True if offset vector self contains other."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the offset vector so that the instrument has the desired offset.", "response": "def normalize(self, **kwargs):\n\t\t\"\"\"\n\t\tAdjust the offsetvector so that a particular instrument has\n\t\tthe desired offset.  All other instruments have their\n\t\toffsets adjusted so that the relative offsets are\n\t\tpreserved.  The instrument to noramlize, and the offset one\n\t\twishes it to have, are provided as a key-word argument.\n\t\tThe return value is the time slide dictionary, which is\n\t\tmodified in place.\n\n\t\tIf more than one key-word argument is provided the keys are\n\t\tsorted and considered in order until a key is found that is\n\t\tin the offset vector.  The offset vector is normalized to\n\t\tthat value.  This function is a no-op if no key-word\n\t\targument is found that applies.\n\n\t\tExample:\n\n\t\t>>> a = offsetvector({\"H1\": -10, \"H2\": -10, \"L1\": -10})\n\t\t>>> a.normalize(L1 = 0)\n\t\toffsetvector({'H2': 0, 'H1': 0, 'L1': 0})\n\t\t>>> a = offsetvector({\"H1\": -10, \"H2\": -10})\n\t\t>>> a.normalize(L1 = 0, H2 = 5)\n\t\toffsetvector({'H2': 5, 'H1': 5})\n\t\t\"\"\"\n\t\t# FIXME:  should it be performed in place?  if it should\n\t\t# be, the should there be no return value?\n\t\tfor key, offset in sorted(kwargs.items()):\n\t\t\tif key in self:\n\t\t\t\tdelta = offset - self[key]\n\t\t\t\tfor key in self.keys():\n\t\t\t\t\tself[key] += delta\n\t\t\t\tbreak\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing an offsetvector from a dictionary of offsetvector deltas.", "response": "def fromdeltas(cls, deltas):\n\t\t\"\"\"\n\t\tConstruct an offsetvector from a dictionary of offset\n\t\tdeltas as returned by the .deltas attribute.\n\n\t\tExample:\n\n\t\t>>> x = offsetvector({\"H1\": 0, \"L1\": 10, \"V1\": 20})\n\t\t>>> y = offsetvector.fromdeltas(x.deltas)\n\t\t>>> y\n\t\toffsetvector({'V1': 20, 'H1': 0, 'L1': 10})\n\t\t>>> y == x\n\t\tTrue\n\n\t\tSee also .deltas, .fromkeys()\n\t\t\"\"\"\n\t\treturn cls((key, value) for (refkey, key), value in deltas.items())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfeeding data into HTML.", "response": "def feed(self, data):\n        \"\"\"\n        Main method for purifying HTML (overrided)\n        \"\"\"\n        self.reset_purified()\n        HTMLParser.feed(self, data)\n        return self.html()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_starttag(self, tag, attrs):\n        self.log.debug( u'Encountered a start tag: {0} {1}'.format(tag, attrs) )\n        if tag in self.sanitizelist:\n            self.level += 1\n            return\n        if self.isNotPurify or tag in self.whitelist_keys:\n            attrs = self.__attrs_str(tag, attrs)\n            attrs = ' ' + attrs if attrs else ''\n            tmpl = u'<%s%s />' if tag in self.unclosedTags and self.isStrictHtml else u'<%s%s>'\n            self.data.append( tmpl % (tag, attrs,) )", "response": "Handle a start tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles an end tag.", "response": "def handle_endtag(self, tag):\n        \"\"\"\n        Handler of ending tag processing (overrided, private)\n        \"\"\"\n        self.log.debug( u'Encountered an end tag : {0}'.format(tag) )\n        if tag in self.sanitizelist:\n            self.level -= 1\n            return\n        if tag in self.unclosedTags:\n            return\n        if self.isNotPurify or tag in self.whitelist_keys:\n            self.data.append(u'</%s>' % tag)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_data(self, data):\n        self.log.debug( u'Encountered some data  : {0}'.format(data) )\n        if not self.level:\n            self.data.append(data)", "response": "Handle data inside a tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling an entity reference.", "response": "def handle_entityref(self, name):\n        \"\"\"\n        Handler of processing entity (overrided, private)\n        \"\"\"\n        self.log.debug( u'Encountered entity  : {0}'.format(name) )\n        if not self.removeEntity:\n            self.data.append('&%s;' % name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __set_whitelist(self, whitelist=None):\n        # add tag's names as key and list of enabled attributes as value for defaults\n        self.whitelist = {}\n        # tags that removed with contents\n        self.sanitizelist = ['script', 'style']\n        if isinstance(whitelist, dict) and '*' in whitelist.keys():\n            self.isNotPurify = True\n            self.whitelist_keys = []\n            return\n        else:\n            self.isNotPurify = False\n        self.whitelist.update(whitelist or {})\n        self.whitelist_keys = self.whitelist.keys()", "response": "Update default white list by customer white list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding string of attributes list for tag.", "response": "def __attrs_str(self, tag, attrs):\n        \"\"\"\n        Build string of attributes list for tag\n        \"\"\"\n        enabled = self.whitelist.get(tag, ['*'])\n        all_attrs = '*' in enabled\n        items = []\n        for attr in attrs:\n            key = attr[0]\n            value = attr[1] or ''\n            if all_attrs or key in enabled:\n                items.append( u'%s=\"%s\"' % (key, value,) )\n        return u' '.join(items)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hex_from(val):\n    if isinstance(val, integer_types):\n        hex_str = '%x' % val\n        if len(hex_str) % 2:\n            hex_str = '0' + hex_str\n        return hex_str\n\n    return hexlify(val)", "response": "Returns hex string representation of a given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef b64_from(val):\n    if isinstance(val, integer_types):\n        val = int_to_bytes(val)\n    return b64encode(val).decode('ascii')", "response": "Returns base64 encoded bytes for a given int or long or bytes value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_output_format( output_format ):\n    if output_format == 'wiki':\n        ttx = '== '\n        xtt = ' =='\n        tx = ''\n        xt = ''\n        capx = \"'''\"\n        xcap = \"'''\"\n        rx = '|'\n        xr = '|'\n        rspx = '|<|'\n        xrsp = '>'\n        cx = '|'\n        xc = '|'\n        hlx = '['\n        hxl = ' '\n        xhl = ']'\n\n    elif output_format == \"html\":\n        ttx = '<b>'\n        xtt = '</b><hr>'\n        tx = '<table border = \"1\">'\n        xt = '</table><br><br>'\n        capx = '<caption>'\n        xcap = '</caption>'\n        rx = '<tr>'\n        xr = '</tr>'\n        rspx = '<td rowspan='\n        xrsp = '>'\n        cx = '<td>'\n        xc = '</td>'\n        hlx = '<a href=\"'\n        hxl = '\">'\n        xhl = \"</a>\"\n\n    else:\n        raise ValueError(\"unrecognized output_format %s\" % output_format)\n\n    return ttx, xtt, tx, xt, capx, xcap, rx, xr, cx, xc, rspx, xrsp, hlx, hxl, xhl", "response": "Sets the output format of the current set of tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smart_round( val, decimal_places = 2 ):\n    if isinstance(val, float) and val != 0.0:\n        if val >= 10.**-(decimal_places - 1):\n            conv_str = ''.join([ '%.', str(decimal_places), 'f' ])\n        else:\n            conv_str = ''.join([ '%.', str(decimal_places), 'e' ])\n        val = float( conv_str % val )\n\n    return val", "response": "Round a number to the specified number of decimal places."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats an html hyperlink into other forms.", "response": "def format_hyperlink( val, hlx, hxl, xhl ):\n    \"\"\"\n    Formats an html hyperlink into other forms.\n\n    @hlx, hxl, xhl: values returned by set_output_format\n    \"\"\"\n    if '<a href=\"' in str(val) and hlx != '<a href=\"':\n        val = val.replace('<a href=\"', hlx).replace('\">', hxl, 1).replace('</a>', xhl) \n\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_cell(val, round_floats = False, decimal_places = 2, format_links = False, \n    hlx = '', hxl = '', xhl = ''):\n    \"\"\"\n    Applys smart_round and format_hyperlink to values in a cell if desired.\n    \"\"\"\n    if round_floats:\n        val = smart_round(val, decimal_places = decimal_places)\n    if format_links:\n        val = format_hyperlink(val, hlx, hxl, xhl)\n\n    return val", "response": "Formats the value in a cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_header_cell(val):\n    return re.sub('_', ' ', re.sub(r'(_Px_)', '(', re.sub(r'(_xP_)', ')', str(val) )))", "response": "Formats given header column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_row_data(row, column_name, cat_time_ns = True):\n    column_name_ns = re.sub(r'_time', r'_time_ns', column_name)\n    try:\n        rowattrs = [attr for attr in row.__slots__]\n    except AttributeError:\n        rowattrs = [attr for attr in row.__dict__.iterkeys()]\n\n    if cat_time_ns and \"_time\" in column_name and column_name_ns in rowattrs:\n        return int(getattr(row, column_name)) + 10**(-9.)*int(getattr(row, column_name_ns))\n    else:\n        return getattr(row, column_name)", "response": "Retrieves the requested column s data from the given row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_build_info():\n    date = branch = tag = author = committer = status = builder_name = build_date = \"\"\n    id = \"1.1.0\"\n    \n    try:\n        v = gvcsi.generate_git_version_info()\n        id, date, branch, tag, author = v.id, v.date, b.branch, v.tag, v.author\n        committer, status = v.committer, v.status\n\n        # determine current time and treat it as the build time\n        build_date = time.strftime('%Y-%m-%d %H:%M:%S +0000', time.gmtime())\n\n        # determine builder\n        retcode, builder_name = gvcsi.call_out(('git', 'config', 'user.name'))\n        if retcode:\n            builder_name = \"Unknown User\"\n        retcode, builder_email = gvcsi.call_out(('git', 'config', 'user.email'))\n        if retcode:\n            builder_email = \"\"\n        builder = \"%s <%s>\" % (builder_name, builder_email)\n    except:\n        pass\n\n    sed_cmd = ('sed',\n        '-e', 's/@ID@/%s/' % id,\n        '-e', 's/@DATE@/%s/' % date,\n        '-e', 's/@BRANCH@/%s/' % branch,\n        '-e', 's/@TAG@/%s/' % tag,\n        '-e', 's/@AUTHOR@/%s/' % author,\n        '-e', 's/@COMMITTER@/%s/' % committer,\n        '-e', 's/@STATUS@/%s/' % status,\n        '-e', 's/@BUILDER@/%s/' % builder_name,\n        '-e', 's/@BUILD_DATE@/%s/' % build_date,\n        'misc/git_version.py.in')\n\n    # FIXME: subprocess.check_call becomes available in Python 2.5\n    sed_retcode = subprocess.call(sed_cmd,\n        stdout=open('pycbc_glue/git_version.py', 'w'))\n    if sed_retcode:\n        raise gvcsi.GitInvocationError\n    return id", "response": "Write build information to pycbc_glue. py. in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pad(self, val):\n        padding = len(int_to_bytes(self._prime))\n        padded = int_to_bytes(val).rjust(padding, b'\\x00')\n        return padded", "response": "Returns a padded version of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhashing the given list of items and return the hash value.", "response": "def hash(self, *args, **kwargs):\n        \"\"\"\n        :param args:\n        :param kwargs:\n            joiner - string to join values (args)\n            as_bytes - bool to return hash bytes instead of default int\n        :rtype: int|bytes\n        \"\"\"\n        joiner = kwargs.get('joiner', '').encode('utf-8')\n        as_bytes = kwargs.get('as_bytes', False)\n\n        def conv(arg):\n            if isinstance(arg, integer_types):\n                arg = int_to_bytes(arg)\n\n            if PY3:\n                if isinstance(arg, str):\n                    arg = arg.encode('utf-8')\n                return arg\n\n            return str(arg)\n\n        digest = joiner.join(map(conv, args))\n\n        hash_obj = self._hash_func(digest)\n\n        if as_bytes:\n            return hash_obj.digest()\n\n        return int_from_hex(hash_obj.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_random(self, bits_len=None):\n        bits_len = bits_len or self._bits_random\n        return random().getrandbits(bits_len)", "response": "Generates a random value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_common_secret(self, server_public, client_public):\n        return self.hash(self.pad(client_public), self.pad(server_public))", "response": "u = H ( A | B )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_client_premaster_secret(self, password_hash, server_public, client_private, common_secret):\n        password_verifier = self.get_common_password_verifier(password_hash)\n        return pow(\n            (server_public - (self._mult * password_verifier)),\n            (client_private + (common_secret * password_hash)), self._prime)", "response": "Get client premaster secret."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_server_premaster_secret(self, password_verifier, server_private, client_public, common_secret):\n        return pow((client_public * pow(password_verifier, common_secret, self._prime)), server_private, self._prime)", "response": "Get the server premaster secret for the current server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_server_public(self, password_verifier, server_private):\n        return ((self._mult * password_verifier) + pow(self._gen, server_private, self._prime)) % self._prime", "response": "Get the public key of the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_common_password_hash(self, salt):\n        password = self._password\n        if password is None:\n            raise SRPException('User password should be in context for this scenario.')\n\n        return self.hash(salt, self.hash(self._user, password, joiner=':'))", "response": "Get common password hash for the user and the salt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_common_session_key_proof(self, session_key, salt, server_public, client_public):\n        h = self.hash\n        prove = h(\n            h(self._prime) ^ h(self._gen),\n            h(self._user),\n            salt,\n            client_public,\n            server_public,\n            session_key,\n            as_bytes=True\n        )\n        return prove", "response": "Get the common session key proof for the given session key and salt."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_common_session_key_proof_hash(self, session_key, session_key_proof, client_public):\n        return self.hash(client_public, session_key_proof, session_key, as_bytes=True)", "response": "This function returns the common session key proof hash for the given session key and client public key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the user data triplet for the current user.", "response": "def get_user_data_triplet(self, base64=False):\n        \"\"\"( <_user>, <_password verifier>, <salt> )\n\n        :param base64:\n        :rtype: tuple\n        \"\"\"\n        salt = self.generate_salt()\n        verifier = self.get_common_password_verifier(self.get_common_password_hash(salt))\n\n        verifier = value_encode(verifier, base64)\n        salt = value_encode(salt, base64)\n\n        return self._user, verifier, salt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_input_file(self, filename):\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)", "response": "Add filename as a necessary input file for this DAG node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds filename as a output file for this DAG node.", "response": "def add_output_file(self, filename):\n    \"\"\"\n    Add filename as a output file for this DAG node.\n\n    @param filename: output filename to add\n    \"\"\"\n    if filename not in self.__output_files:\n      self.__output_files.append(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_checkpoint_file(self, filename):\n    if filename not in self.__checkpoint_files:\n        self.__checkpoint_files.append(filename)", "response": "Add filename as a checkpoint file for this DAG job."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a file argument to the executable.", "response": "def add_file_arg(self, filename):\n    \"\"\"\n    Add a file argument to the executable. Arguments are appended after any\n    options and their order is guaranteed. Also adds the file name to the\n    list of required input data for this job.\n    @param filename: file to add as argument.\n    \"\"\"\n    self.__arguments.append(filename)\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value associated with the given command line option.", "response": "def get_opt( self, opt):\n    \"\"\"\n    Returns the value associated with the given command line option.\n    Returns None if the option does not exist in the options list.\n    @param opt: command line option\n    \"\"\"\n    if self.__options.has_key(opt):\n      return self.__options[opt]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_file_opt(self, opt, filename):\n    self.__options[opt] = filename\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)", "response": "Add a command line option to the executable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_ini_opts(self, cp, section):\n    for opt in cp.options(section):\n      arg = string.strip(cp.get(section,opt))\n      self.__options[opt] = arg", "response": "Parse command line options from a given section in an ini file and add them to the executable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a submit file for this Condor job.", "response": "def write_sub_file(self):\n    \"\"\"\n    Write a submit file for this Condor job.\n    \"\"\"\n    if not self.__log_file:\n      raise CondorSubmitError, \"Log file not specified.\"\n    if not self.__err_file:\n      raise CondorSubmitError, \"Error file not specified.\"\n    if not self.__out_file:\n      raise CondorSubmitError, \"Output file not specified.\"\n\n    if not self.__sub_file_path:\n      raise CondorSubmitError, 'No path for submit file.'\n    try:\n      subfile = open(self.__sub_file_path, 'w')\n    except:\n      raise CondorSubmitError, \"Cannot open file \" + self.__sub_file_path\n\n    if self.__universe == 'grid':\n      if self.__grid_type == None:\n        raise CondorSubmitError, 'No grid type specified.'\n      elif self.__grid_type == 'gt2':\n        if self.__grid_server == None:\n          raise CondorSubmitError, 'No server specified for grid resource.'\n      elif self.__grid_type == 'gt4':\n        if self.__grid_server == None:\n          raise CondorSubmitError, 'No server specified for grid resource.'\n        if self.__grid_scheduler == None:\n          raise CondorSubmitError, 'No scheduler specified for grid resource.'\n      else:\n        raise CondorSubmitError, 'Unsupported grid resource.'\n\n    subfile.write( 'universe = ' + self.__universe + '\\n' )\n    subfile.write( 'executable = ' + self.__executable + '\\n' )\n\n    if self.__universe == 'grid':\n      if self.__grid_type == 'gt2':\n        subfile.write('grid_resource = %s %s\\n' % (self.__grid_type,\n          self.__grid_server))\n      if self.__grid_type == 'gt4':\n        subfile.write('grid_resource = %s %s %s\\n' % (self.__grid_type,\n          self.__grid_server, self.__grid_scheduler))\n\n    if self.__universe == 'grid':\n      subfile.write('when_to_transfer_output = ON_EXIT\\n')\n      subfile.write('transfer_output_files = $(macrooutput)\\n')\n      subfile.write('transfer_input_files = $(macroinput)\\n')\n\n    if self.__options.keys() or self.__short_options.keys() or self.__arguments:\n      subfile.write( 'arguments = \"' )\n      for c in self.__options.keys():\n        if self.__options[c]:\n          subfile.write( ' --' + c + ' ' + self.__options[c] )\n        else:\n          subfile.write( ' --' + c )\n      for c in self.__short_options.keys():\n        if self.__short_options[c]:\n          subfile.write( ' -' + c + ' ' + self.__short_options[c] )\n        else:\n          subfile.write( ' -' + c )\n      for c in self.__arguments:\n        subfile.write( ' ' + c )\n      subfile.write( ' \"\\n' )\n\n    for cmd in self.__condor_cmds.keys():\n      subfile.write( str(cmd) + \" = \" + str(self.__condor_cmds[cmd]) + '\\n' )\n\n    subfile.write( 'log = ' + self.__log_file + '\\n' )\n    if self.__in_file is not None:\n      subfile.write( 'input = ' + self.__in_file + '\\n' )\n    subfile.write( 'error = ' + self.__err_file + '\\n' )\n    subfile.write( 'output = ' + self.__out_file + '\\n' )\n    if self.__notification:\n      subfile.write( 'notification = ' + self.__notification + '\\n' )\n    subfile.write( 'queue ' + str(self.__queue) + '\\n' )\n\n    subfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_grid_site(self,site):\n    self.__grid_site=str(site)\n    if site != 'local':\n      self.set_executable_installed(False)", "response": "Sets the grid site to run on."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a variable option to the condor job.", "response": "def add_var_opt(self, opt, short=False):\n    \"\"\"\n    Add a variable (or macro) option to the condor job. The option is added\n    to the submit file and a different argument to the option can be set for\n    each node in the DAG.\n    @param opt: name of option to add.\n    \"\"\"\n    if opt not in self.__var_opts:\n      self.__var_opts.append(opt)\n      macro = self.__bad_macro_chars.sub( r'', opt )\n      if short:\n        self.add_short_opt(opt,'$(macro' + macro + ')')\n      else:\n        self.add_opt(opt,'$(macro' + macro + ')')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_var_condor_cmd(self, command):\n    if command not in self.__var_cmds:\n        self.__var_cmds.append(command)\n        macro = self.__bad_macro_chars.sub( r'', command )\n        self.add_condor_cmd(command, '$(macro' + macro + ')')", "response": "Add a condor command to the submit file that allows variable ( macro ) arguments to be passed to the executable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_var_arg(self,arg_index):\n    try:\n      self.__var_args[arg_index]\n    except IndexError:\n      if arg_index != self.__arg_index:\n        raise CondorDAGJobError, \"mismatch between job and node var_arg index\"\n      self.__var_args.append('$(macroargument%s)' % str(arg_index))\n      self.add_arg(self.__var_args[self.__arg_index])\n      self.__arg_index += 1", "response": "Adds a command to the submit file to allow variable arguments to be passed to the executable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_pegasus_profile(self, namespace, key, value):\n    self.__pegasus_profile.append((str(namespace),str(key),str(value)))", "response": "Adds a Pegasus profile to this job which will be written to the dax as\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_input_file(self, filename):\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)\n      if not isinstance(self.job(), CondorDAGManJob):\n        if self.job().get_universe() == 'grid':\n          self.add_input_macro(filename)", "response": "Add filename as a necessary input file for this DAG node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd filename as a output file for this DAG node.", "response": "def add_output_file(self, filename):\n    \"\"\"\n    Add filename as a output file for this DAG node.\n\n    @param filename: output filename to add\n    \"\"\"\n    if filename not in self.__output_files:\n      self.__output_files.append(filename)\n      if not isinstance(self.job(), CondorDAGManJob):\n        if self.job().get_universe() == 'grid':\n          self.add_output_macro(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_checkpoint_file(self,filename):\n    if filename not in self.__checkpoint_files:\n        self.__checkpoint_files.append(filename)\n        if not isinstance(self.job(), CondorDAGManJob):\n            if self.job().get_universe() == 'grid':\n                self.add_checkpoint_macro(filename)", "response": "Add filename as a checkpoint file for this DAG node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning list of input files for this DAG node and its job.", "response": "def get_input_files(self):\n    \"\"\"\n    Return list of input files for this DAG node and its job.\n    \"\"\"\n    input_files = list(self.__input_files)\n    if isinstance(self.job(), CondorDAGJob):\n      input_files = input_files + self.job().get_input_files()\n    return input_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_output_files(self):\n    output_files = list(self.__output_files)\n    if isinstance(self.job(), CondorDAGJob):\n      output_files = output_files + self.job().get_output_files()\n    return output_files", "response": "Returns a list of output files for this DAG node and its job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of checkpoint files for this DAG node and its job.", "response": "def get_checkpoint_files(self):\n    \"\"\"\n    Return a list of checkpoint files for this DAG node and its job.\n    \"\"\"\n    checkpoint_files = list(self.__checkpoint_files)\n    if isinstance(self.job(), CondorDAGJob):\n        checkpoint_files = checkpoint_files + self.job().get_checkpoint_files()\n    return checkpoint_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_macro(self,name,value):\n    macro = self.__bad_macro_chars.sub( r'', name )\n    self.__opts[macro] = value", "response": "Add a macro to the options dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a macro for storing the input and output files associated with this node.", "response": "def add_io_macro(self,io,filename):\n    \"\"\"\n    Add a variable (macro) for storing the input/output files associated\n    with this node.\n    @param io: macroinput or macrooutput\n    @param filename: filename of input/output file\n    \"\"\"\n    io = self.__bad_macro_chars.sub( r'', io )\n    if io not in self.__opts:\n      self.__opts[io] = filename\n    else:\n      if filename not in self.__opts[io]:\n        self.__opts[io] += ',%s' % filename"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_var_condor_cmd(self, command, value):\n    macro = self.__bad_macro_chars.sub( r'', command )\n    self.__macros['macro' + macro] = value\n    self.__job.add_var_condor_cmd(command)", "response": "Add a variable condor command for this node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_var_opt(self,opt,value,short=False):\n    macro = self.__bad_macro_chars.sub( r'', opt )\n    self.__opts['macro' + macro] = value\n    self.__job.add_var_opt(opt,short)", "response": "Add a variable option for this node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_file_opt(self,opt,filename,file_is_output_file=False):\n    self.add_var_opt(opt,filename)\n    if file_is_output_file: self.add_output_file(filename)\n    else: self.add_input_file(filename)", "response": "Add a file option for this node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_var_arg(self, arg):\n    self.__args.append(arg)\n    self.__job.add_var_arg(self.__arg_index)\n    self.__arg_index += 1", "response": "Adds a variable argument to the condor job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_job(self,fh):\n    if isinstance(self.job(),CondorDAGManJob):\n      # create an external subdag from this dag\n      fh.write( ' '.join(\n        ['SUBDAG EXTERNAL', self.__name, self.__job.get_sub_file()]) )\n      if self.job().get_dag_directory():\n        fh.write( ' DIR ' + self.job().get_dag_directory() )\n    else:\n      # write a regular condor job\n      fh.write( 'JOB ' + self.__name + ' ' + self.__job.get_sub_file() )\n    fh.write( '\\n')\n\n    fh.write( 'RETRY ' + self.__name + ' ' + str(self.__retry) + '\\n' )", "response": "Writes the entry for this node s job to the DAG file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_vars(self,fh):\n    if self.__macros.keys() or self.__opts.keys() or self.__args:\n      fh.write( 'VARS ' + self.__name )\n    for k in self.__macros.keys():\n      fh.write( ' ' + str(k) + '=\"' + str(self.__macros[k]) + '\"' )\n    for k in self.__opts.keys():\n      fh.write( ' ' + str(k) + '=\"' + str(self.__opts[k]) + '\"' )\n    if self.__args:\n      for i in range(self.__arg_index):\n        fh.write( ' macroargument' + str(i) + '=\"' + self.__args[i] + '\"' )\n    fh.write( '\\n' )", "response": "Writes the variable options and arguments to the DAG file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the parent and child relations for this job to the DAG file descriptor fh.", "response": "def write_parents(self,fh):\n    \"\"\"\n    Write the parent/child relations for this job to the DAG file descriptor.\n    @param fh: descriptor of open DAG file.\n    \"\"\"\n    for parent in self.__parents:\n      fh.write( 'PARENT ' + str(parent) + ' CHILD ' + str(self) + '\\n' )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the pre script for the job if there is one", "response": "def write_pre_script(self,fh):\n    \"\"\"\n    Write the pre script for the job, if there is one\n    @param fh: descriptor of open DAG file.\n    \"\"\"\n    if self.__pre_script:\n      fh.write( 'SCRIPT PRE ' + str(self) + ' ' + self.__pre_script + ' ' +\n        ' '.join(self.__pre_script_args) + '\\n' )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the post script for the job if there is one", "response": "def write_post_script(self,fh):\n    \"\"\"\n    Write the post script for the job, if there is one\n    @param fh: descriptor of open DAG file.\n    \"\"\"\n    if self.__post_script:\n      fh.write( 'SCRIPT POST ' + str(self) + ' ' + self.__post_script + ' ' +\n        ' '.join(self.__post_script_args) + '\\n' )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite as a comment into the DAG file the list of input files that this DAG node requires.", "response": "def write_input_files(self, fh):\n    \"\"\"\n    Write as a comment into the DAG file the list of input files\n    for this DAG node.\n\n    @param fh: descriptor of open DAG file.\n    \"\"\"\n    for f in self.__input_files:\n        print >>fh, \"## Job %s requires input file %s\" % (self.__name, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_output_files(self, fh):\n    for f in self.__output_files:\n        print >>fh, \"## Job %s generates output file %s\" % (self.__name, f)", "response": "Writes the list of output files for this DAG node to fh"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a parent to this node.", "response": "def add_parent(self,node):\n    \"\"\"\n    Add a parent to this node. This node will not be executed until the\n    parent node has run sucessfully.\n    @param node: CondorDAGNode to add as a parent.\n    \"\"\"\n    if not isinstance(node, (CondorDAGNode,CondorDAGManNode) ):\n      raise CondorDAGNodeError, \"Parent must be a CondorDAGNode or a CondorDAGManNode\"\n    self.__parents.append( node )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cmd_tuple_list(self):\n\n    # pattern to find DAGman macros\n    pat = re.compile(r'\\$\\((.+)\\)')\n    argpat = re.compile(r'\\d+')\n\n    # first parse the options and replace macros with values\n    options = self.job().get_opts()\n    macros = self.get_opts()\n\n    cmd_list = []\n\n    for k in options:\n      val = options[k]\n      m = pat.match(val)\n      if m:\n        key = m.group(1)\n        value = macros[key]\n\n        cmd_list.append((\"--%s\" % k, str(value)))\n      else:\n        cmd_list.append((\"--%s\" % k, str(val)))\n\n    # second parse the short options and replace macros with values\n    options = self.job().get_short_opts()\n\n    for k in options:\n      val = options[k]\n      m = pat.match(val)\n      if m:\n        key = m.group(1)\n        value = macros[key]\n\n        cmd_list.append((\"-%s\" % k, str(value)))\n      else:\n        cmd_list.append((\"-%s\" % k, str(val)))\n\n    # lastly parse the arguments and replace macros with values\n    args = self.job().get_args()\n    macros = self.get_args()\n\n    for a in args:\n      m = pat.match(a)\n      if m:\n        arg_index = int(argpat.findall(a)[0])\n        try:\n          cmd_list.append((\"%s\" % macros[arg_index], \"\"))\n        except IndexError:\n          cmd_list.append(\"\")\n      else:\n        cmd_list.append((\"%s\" % a, \"\"))\n\n    return cmd_list", "response": "Returns a list of tuples containg the command line arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full command line that will be used when this node is run by DAGman.", "response": "def get_cmd_line(self):\n    \"\"\"\n    Return the full command line that will be used when this node\n    is run by DAGman.\n    \"\"\"\n\n    cmd = \"\"\n    cmd_list = self.get_cmd_tuple_list()\n    for argument in cmd_list:\n      cmd += ' '.join(argument) + \" \"\n\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_maxjobs_category(self,categoryName,maxJobsNum):\n    self.__maxjobs_categories.append((str(categoryName),str(maxJobsNum)))", "response": "Adds a category with a maxjobs of maxJobsNum to this DAG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a CondorJob to the CondorDAG.", "response": "def add_node(self,node):\n    \"\"\"\n    Add a CondorDAGNode to this DAG. The CondorJob that the node uses is\n    also added to the list of Condor jobs in the DAG so that a list of the\n    submit files needed by the DAG can be maintained. Each unique CondorJob\n    will be added once to prevent duplicate submit files being written.\n    @param node: CondorDAGNode to add to the CondorDAG.\n    \"\"\"\n    if not isinstance(node, CondorDAGNode):\n      raise CondorDAGError, \"Nodes must be class CondorDAGNode or subclass\"\n    if not isinstance(node.job(), CondorDAGManJob):\n      node.set_log_file(self.__log_file_path)\n    self.__nodes.append(node)\n    if self.__integer_node_names:\n      node.set_name(str(self.__node_count))\n    self.__node_count += 1\n    if node.job() not in self.__jobs:\n      self.__jobs.append(node.job())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_maxjobs(self,fh,category):\n    fh.write( 'MAXJOBS ' + str(category[0]) + ' ' + str(category[1]) +  '\\n' )", "response": "Writes the DAG entry for this category s maxjobs to fh."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites all the submit files used by this CondorJob to disk.", "response": "def write_sub_files(self):\n    \"\"\"\n    Write all the submit files used by the dag to disk. Each submit file is\n    written to the file name set in the CondorJob.\n    \"\"\"\n    if not self.__nodes_finalized:\n      for node in self.__nodes:\n        node.finalize()\n    if not self.is_dax():\n      for job in self.__jobs:\n        job.write_sub_file()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the DAG to the file.", "response": "def write_concrete_dag(self):\n    \"\"\"\n    Write all the nodes in the DAG to the DAG file.\n    \"\"\"\n    if not self.__dag_file_path:\n      raise CondorDAGError, \"No path for DAG file\"\n    try:\n      dagfile = open( self.__dag_file_path, 'w' )\n    except:\n      raise CondorDAGError, \"Cannot open file \" + self.__dag_file_path\n    for node in self.__nodes:\n      node.write_job(dagfile)\n      node.write_vars(dagfile)\n      if node.get_category():\n        node.write_category(dagfile)\n      if node.get_priority():\n        node.write_priority(dagfile)\n      node.write_pre_script(dagfile)\n      node.write_post_script(dagfile)\n      node.write_input_files(dagfile)\n      node.write_output_files(dagfile)\n    for node in self.__nodes:\n      node.write_parents(dagfile)\n    for category in self.__maxjobs_categories:\n      self.write_maxjobs(dagfile, category)\n    dagfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_abstract_dag(self):\n\n    # keep track of if we are using stampede at TACC\n    using_stampede = False\n\n    if not self.__dax_file_path:\n      # this workflow is not dax-compatible, so don't write a dax\n      return\n\n    import Pegasus.DAX3\n    # create the workflow object\n    dax_name = os.path.split(self.__dax_file_path)[-1]\n    dax_basename = '.'.join(dax_name.split('.')[0:-1])\n    workflow = Pegasus.DAX3.ADAG( dax_basename )\n\n    # we save the ID number to DAG node name mapping so that\n    # we can easily write out the child/parent relationship\n    # later\n    node_job_object_dict = {}\n\n    # FIXME disctionary of executables and pfns in the workflow\n    # Pegasus should take care of this so we don't have to\n    workflow_executable_dict = {}\n    workflow_pfn_dict = {}\n\n    # Add PFN caches for this workflow\n    for pfn_tuple in self.get_pfn_cache():\n        workflow_pfn_dict[pfn_tuple[0]] = pfn_tuple\n\n    if self.get_pegasus_worker():\n      # write the executable into the dax\n      worker_package = Pegasus.DAX3.Executable(\n        namespace=\"pegasus\", name=\"worker\",\n        os=\"linux\", arch=\"x86_64\", installed=False)\n      worker_package.addPFN(Pegasus.DAX3.PFN(self.get_pegasus_worker(),\"local\"))\n      workflow_executable_dict['pegasus-pegasus_worker'] = worker_package\n\n    # check for the pegasus-cluster package\n    for path in os.environ[\"PATH\"].split(\":\"):\n      cluster_path = os.path.join(path,\"pegasus-cluster\")\n      if os.path.exists(cluster_path):\n        # and add to the dax if it exists\n        seqexec_package = Pegasus.DAX3.Executable(\n          namespace=\"pegasus\", name=\"seqexec\",\n          os=\"linux\", arch=\"x86_64\", installed=True)\n        seqexec_package.addPFN(Pegasus.DAX3.PFN(cluster_path,\"local\"))\n        workflow_executable_dict['pegasus-pegasus_seqexec'] = seqexec_package\n\n    id = 0\n    for node in self.__nodes:\n      if self.is_dax() and isinstance(node, LSCDataFindNode):\n        pass\n\n      elif isinstance(node.job(), CondorDAGManJob):\n        id += 1\n        id_tag = \"ID%06d\" % id\n        node_name = node._CondorDAGNode__name\n\n        if node.job().get_dax() is None:\n          # write this node as a sub-dag\n          subdag_name = os.path.split(node.job().get_dag())[-1]\n          try:\n            subdag_exec_path = os.path.join(\n              os.getcwd(),node.job().get_dag_directory())\n          except AttributeError:\n            subdag_exec_path = os.getcwd()\n\n          subdag = Pegasus.DAX3.DAG(subdag_name,id=id_tag)\n          subdag.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"DIR\",subdag_exec_path))\n\n          subdag_file = Pegasus.DAX3.File(subdag_name)\n          subdag_file.addPFN(Pegasus.DAX3.PFN(os.path.join(subdag_exec_path,subdag_name),\"local\"))\n          workflow.addFile(subdag_file)\n          workflow.addDAG(subdag)\n          node_job_object_dict[node_name] = subdag\n\n        else:\n          # write this node as a sub-dax\n          subdax_name = os.path.split(node.job().get_dax())[-1]\n          dax_subdir = node.job().get_dag_directory()\n          if dax_subdir:\n            subdax_path = os.path.join(\n              os.getcwd(),node.job().get_dag_directory(),subdax_name)\n          else:\n            subdax_path = os.path.join(os.getcwd(),subdax_name)\n            dax_subdir = '.'\n\n          subdax = Pegasus.DAX3.DAX(subdax_name,id=id_tag)\n\n          # FIXME pegasus should ensure these are unique\n          for pfn_tuple in node.job().get_pfn_cache():\n            workflow_pfn_dict[pfn_tuple[0]] = pfn_tuple\n\n          # set the storage, execute, and output directory locations\n          pegasus_args = \"\"\"--dir %s \"\"\" % dax_subdir\n          pegasus_args += \"\"\"--output-dir %s \"\"\" % dax_subdir\n\n          # set the maxjobs categories for the subdax\n          # FIXME pegasus should expose this in the dax, so it can\n          # be handled like the MAXJOBS keyword in dag files\n          for maxjobcat in node.get_maxjobs_categories():\n            pegasus_args += \"-Dpegasus.dagman.\" + maxjobcat[0] + \".maxjobs=\" + maxjobcat[1] + \" \"\n\n          if not self.is_dax():\n            pegasus_args += \"--nocleanup \"\n\n          if node.get_cluster_jobs():\n            pegasus_args += \"--cluster \" + node.get_cluster_jobs() + \" \"\n\n          if node.get_reduce_dax() is False:\n            pegasus_args += \" --force \"\n\n          if node.get_static_pfn_cache():\n            pegasus_args += \" --cache \" + node.get_static_pfn_cache() + \" \"\n\n          pegasus_args += \"--output-site local -vvvvvv\"\n          subdax.addArguments(pegasus_args)\n\n          subdax_file = Pegasus.DAX3.File(subdax_name)\n          subdax_file.addPFN(Pegasus.DAX3.PFN(subdax_path,\"local\"))\n          workflow.addFile(subdax_file)\n          workflow.addDAX(subdax)\n          node_job_object_dict[node_name] = subdax\n\n      else:\n        # write this job as a regular node\n        executable = node.job()._CondorJob__executable\n        node_name = node._CondorDAGNode__name\n\n        id += 1\n        id_tag = \"ID%06d\" % id\n        node_job_object_dict[node_name] = id_tag\n\n        # get the name of the executable\n        executable_namespace = 'ligo-' + str(node.job().__class__.__name__).lower()\n        executable_base = os.path.basename(executable)\n\n        workflow_job = Pegasus.DAX3.Job( namespace=executable_namespace,\n          name=executable_base, version=\"1.0\", id=id_tag)\n\n        cmd_line = node.get_cmd_tuple_list()\n\n        # loop through all filenames looking for them in the command\n        # line so that they can be replaced appropriately by xml tags\n        input_node_file_dict = {}\n        for f in node.get_input_files():\n          input_node_file_dict[f] = 1\n\n        for f in input_node_file_dict.keys():\n          workflow_job.uses(Pegasus.DAX3.File(os.path.basename(f)),link=Pegasus.DAX3.Link.INPUT,register=False,transfer=True)\n\n        output_node_file_dict = {}\n        for f in node.get_output_files():\n          output_node_file_dict[f] = 1\n\n        checkpoint_node_file_dict = {}\n        for f in node.get_checkpoint_files():\n            checkpoint_node_file_dict[f] = 1\n\n        for f in output_node_file_dict.keys():\n          workflow_job.uses(Pegasus.DAX3.File(os.path.basename(f)),link=Pegasus.DAX3.Link.OUTPUT,register=False,transfer=True)\n\n        for f in checkpoint_node_file_dict.keys():\n          workflow_job.uses(Pegasus.DAX3.File(os.path.basename(f)),link=Pegasus.DAX3.Link.CHECKPOINT,register=False,transfer=True)\n\n        node_file_dict = dict( input_node_file_dict.items() + output_node_file_dict.items() + checkpoint_node_file_dict.items() )\n\n        for job_arg in cmd_line:\n          try:\n            if node_file_dict.has_key(job_arg[0]):\n              workflow_job.addArguments(Pegasus.DAX3.File(os.path.basename(job_arg[0])))\n            elif node_file_dict.has_key(job_arg[1]):\n              workflow_job.addArguments(job_arg[0], Pegasus.DAX3.File(os.path.basename(job_arg[1])))\n            elif len(job_arg[1].split(' ')) != 1:\n                args = [job_arg[0]]\n                for arg in job_arg[1].split(' '):\n                    if node_file_dict.has_key(arg):\n                        args.append(Pegasus.DAX3.File(os.path.basename(arg)))\n                    else:\n                        args.append(arg)\n                workflow_job.addArguments(*args)\n            else:\n              workflow_job.addArguments(job_arg[0], job_arg[1])\n          except IndexError:\n            pass\n\n        # Check for desired grid site\n        if node.job().get_grid_site():\n            this_grid_site = node.job().get_grid_site()\n            workflow_job.addProfile(Pegasus.DAX3.Profile('hints','execution.site',this_grid_site))\n            if this_grid_site == 'stampede-dev' or this_grid_site=='stampede':\n              using_stampede = True\n\n        # write the executable into the dax\n        job_executable = Pegasus.DAX3.Executable(\n          namespace=executable_namespace,\n          name=executable_base, version=\"1.0\",\n          os=\"linux\", arch=\"x86_64\",\n          installed=node.job().get_executable_installed())\n\n        executable_path = os.path.join(os.getcwd(),executable)\n        job_executable.addPFN(Pegasus.DAX3.PFN(executable_path,\"local\"))\n\n        workflow_executable_dict[executable_namespace + executable_base] = job_executable\n\n        # write the mpi cluster parameter for the job\n        if node.job().get_dax_mpi_cluster():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"pegasus\",\"job.aggregator\",\"mpiexec\"))\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"pegasus\",\"clusters.size\",str(node.job().get_dax_mpi_cluster())))\n\n        # write the grid start parameter for this node\n        # if the grid start is not None\n        if node.get_grid_start():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"pegasus\",\"gridstart\",node.get_grid_start()))\n\n        # write the bundle parameter if this node has one\n        if node.get_dax_collapse():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"pegasus\",\"clusters.size\",str(node.get_dax_collapse())))\n\n        # write number of times the node should be retried\n        if node.get_retry():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"retry\",str(node.get_retry())))\n\n        # write the post script for this node\n        if node.get_post_script():\n          post_script_base = os.path.basename(node.get_post_script())\n          post_script_path = os.path.join(os.getcwd(),node.get_post_script())\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"post\",post_script_base))\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"post.path.\" + post_script_base,post_script_path))\n\n        # write the post script for this node\n        if node.get_post_script_arg():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"post.arguments\",' '.join(node.get_post_script_arg())))\n\n        # write the dag node category if this node has one\n        if node.get_category():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"dagman\",\"category\",str(node.get_category())))\n\n        # write the dag node priority if this node has one\n        if node.get_priority():\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"condor\",\"priority\",str(node.get_priority())))\n\n        # write the universe that this job should run in to the dax\n        if node.get_dax_collapse():\n          # collapsed jobs must run in the vanilla universe\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"condor\",\"universe\",\"vanilla\"))\n        else:\n          workflow_job.addProfile(Pegasus.DAX3.Profile(\"condor\",\"universe\",node.job().get_universe()))\n\n        # add any other user specified condor commands or classads\n        for p in node.get_pegasus_profile():\n            workflow_job.addProfile(Pegasus.DAX3.Profile(p[0],p[1],p[2]))\n\n        # finally add this job to the workflow\n        workflow.addJob(workflow_job)\n        node_job_object_dict[node_name] = workflow_job\n\n\n    # print parent-child relationships to DAX\n    for node in self.__nodes:\n      if self.is_dax() and isinstance(node, LSCDataFindNode):\n        pass\n      elif self.is_dax() and ( len(node._CondorDAGNode__parents) == 1 ) and isinstance(node._CondorDAGNode__parents[0], LSCDataFindNode):\n        pass\n      else:\n        child_job_object = node_job_object_dict[str(node)]\n        if node._CondorDAGNode__parents:\n          for parent in node._CondorDAGNode__parents:\n            if self.is_dax() and isinstance(parent, LSCDataFindNode):\n              pass\n            else:\n              parent_job_object = node_job_object_dict[str(parent)]\n              workflow.addDependency(Pegasus.DAX3.Dependency(parent=parent_job_object, child=child_job_object))\n\n    # FIXME put all the executables in the workflow\n    for exec_key in workflow_executable_dict.keys():\n      workflow.addExecutable(workflow_executable_dict[exec_key])\n\n    # FIXME if we are running on stampede, add the mpi wrapper job\n    if using_stampede:\n      prod_mpiexec = Pegasus.DAX3.Executable(namespace=\"pegasus\",\n        name=\"mpiexec\", os=\"linux\", arch=\"x86_64\", installed=\"true\")\n      prod_mpiexec.addPFN(Pegasus.DAX3.PFN(\"file:///home1/02796/dabrown/bin/mpi-cluster-wrapper-impi.sh\",\"stampede\"))\n      workflow.addExecutable(prod_mpiexec)\n\n      dev_mpiexec = Pegasus.DAX3.Executable(namespace=\"pegasus\",\n        name=\"mpiexec\", os=\"linux\", arch=\"x86_64\", installed=\"true\")\n      dev_mpiexec.addPFN(Pegasus.DAX3.PFN(\"file:///home1/02796/dabrown/bin/mpi-cluster-wrapper-impi.sh\",\"stampede-dev\"))\n      workflow.addExecutable(dev_mpiexec)\n\n    # FIXME put all the pfns in the workflow\n    for pfn_key in workflow_pfn_dict.keys():\n      f = Pegasus.DAX3.File(workflow_pfn_dict[pfn_key][0])\n      f.addPFN(Pegasus.DAX3.PFN(workflow_pfn_dict[pfn_key][1],workflow_pfn_dict[pfn_key][2]))\n      workflow.addFile(f)\n\n    f = open(self.__dax_file_path,\"w\")\n    workflow.writeXML(f)\n    f.close()", "response": "Writes the abstract DAG to the DAX file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_dag(self):\n    if not self.__nodes_finalized:\n      for node in self.__nodes:\n        node.finalize()\n    self.write_concrete_dag()\n    self.write_abstract_dag()", "response": "Write either a dag or a dax."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the workflow to a script.", "response": "def write_script(self):\n    \"\"\"\n    Write the workflow to a script (.sh instead of .dag).\n\n    Assuming that parents were added to the DAG before their children,\n    dependencies should be handled correctly.\n    \"\"\"\n    if not self.__dag_file_path:\n      raise CondorDAGError, \"No path for DAG file\"\n    try:\n      dfp = self.__dag_file_path\n      outfilename = \".\".join(dfp.split(\".\")[:-1]) + \".sh\"\n      outfile = open(outfilename, \"w\")\n    except:\n      raise CondorDAGError, \"Cannot open file \" + self.__dag_file_path\n\n    for node in self.__nodes:\n        outfile.write(\"# Job %s\\n\" % node.get_name())\n        # Check if this is a DAGMAN Node\n        if isinstance(node,CondorDAGManNode):\n          outfile.write(\"condor_submit_dag %s\\n\\n\" % (node.job().get_dag()))\n        else:\n          outfile.write(\"%s %s\\n\\n\" % (node.job().get_executable(),\n              node.get_cmd_line()))\n    outfile.close()\n\n    os.chmod(outfilename, os.stat(outfilename)[0] | stat.S_IEXEC)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_dax(self,grid_site=None,tmp_exec_dir='.',peg_frame_cache=None):\n    dag=self\n    log_path=self.__log_file_path\n\n    # this function creates the following three files needed by pegasus\n    peg_fh = open(\"pegasus_submit_dax\", \"w\")\n    pegprop_fh = open(\"pegasus.properties\", \"w\")\n    sitefile = open( 'sites.xml', 'w' )\n\n    # write the default properties\n    print >> pegprop_fh, PEGASUS_PROPERTIES % (os.getcwd())\n\n    # set up site and dir options for pegasus-submit-dax\n    dirs_entry='--relative-dir .'\n    if grid_site:\n      exec_site=grid_site\n      exec_ssite_list = exec_site.split(',')\n      for site in exec_ssite_list:\n        # if virgo sites are being used, then we don't have a shared fs\n        if site == 'nikhef':\n          dirs_entry += ' --staging-site nikhef=nikhef-srm'\n        else:\n          dirs_entry += ' --staging-site %s=%s' % (site,site)\n        if site == 'nikhef' or site == 'bologna':\n          print >> pegprop_fh, \\\n\"\"\"\n###############################################################################\n# Data Staging Configuration\n\n# Pegasus will be setup to execute jobs on an execution site without relying\n# on a shared filesystem between the head node and the worker nodes. If this\n# is set, specify staging site ( using --staging-site option to pegasus-plan)\n# to indicate the site to use as a central storage location for a workflow.\npegasus.data.configuration=nonsharedfs\n\n\n\"\"\"\n    else:\n      exec_site='local'\n\n    # write the pegasus_submit_dax and make it executable\n    print >> peg_fh,PEGASUS_SCRIPT % ( tmp_exec_dir, os.getcwd(),\n          dag.get_dax_file().replace('.dax','') + '-0.dag',\n          dag.get_dax_file(), dirs_entry, exec_site )\n    peg_fh.close()\n    os.chmod(\"pegasus_submit_dax\",0755)\n\n    # if a frame cache has been specified, write it to the properties\n    # however note that this is overridden by the --cache option to pegasus\n    if peg_frame_cache:\n      print >> pegprop_fh, \"pegasus.catalog.replica.file=%s\" % (os.path.join(os.getcwd(),os.path.basename(peg_frame_cache)))\n    pegprop_fh.close()\n\n    # write a shell script that can return the basedir and uber-concrete-dag\n    basedir_fh = open(\"pegasus_basedir\", \"w\")\n    print >> basedir_fh, PEGASUS_BASEDIR_SCRIPT % ( tmp_exec_dir, dag.get_dax_file().replace('.dax','') + '-0.dag' )\n    basedir_fh.close()\n    os.chmod(\"pegasus_basedir\",0755)\n\n    # write the site catalog file which is needed by pegasus\n    pwd = os.getcwd()\n    try:\n      hostname = socket.gethostbyaddr(socket.gethostname())[0]\n    except:\n      hostname = 'localhost'\n\n    print >> sitefile, \"\"\"\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<sitecatalog xmlns=\"http://pegasus.isi.edu/schema/sitecatalog\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-4.0.xsd\" version=\"4.0\">\n  <site handle=\"local\" arch=\"x86_64\" os=\"LINUX\">\n    <grid type=\"gt2\" contact=\"%s/jobmanager-fork\" scheduler=\"Fork\" jobtype=\"auxillary\" total-nodes=\"50\"/>\n    <grid type=\"gt2\" contact=\"%s/jobmanager-condor\" scheduler=\"Condor\" jobtype=\"compute\" total-nodes=\"50\"/>\n    <directory  path=\"%s\" type=\"shared-scratch\" free-size=\"null\" total-size=\"null\">\n        <file-server  operation=\"all\" url=\"file://%s\">\n        </file-server>\n    </directory>\n    <directory  path=\"%s\" type=\"shared-storage\" free-size=\"null\" total-size=\"null\">\n        <file-server  operation=\"all\" url=\"file://%s\">\n        </file-server>\n    </directory>\n    <replica-catalog  type=\"LRC\" url=\"rlsn://smarty.isi.edu\">\n    </replica-catalog>\n\"\"\" % (hostname,hostname,pwd,pwd,pwd,pwd)\n\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"GLOBUS_LOCATION\">%s</profile>\"\"\" % os.environ['GLOBUS_LOCATION']\n    except:\n      pass\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"LD_LIBRARY_PATH\">%s</profile>\"\"\" % os.environ['LD_LIBRARY_PATH']\n    except:\n      pass\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"PYTHONPATH\">%s</profile>\"\"\" % os.environ['PYTHONPATH']\n    except:\n      pass\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"PEGASUS_HOME\">%s</profile>\"\"\" % os.environ['PEGASUS_HOME']\n    except:\n      pass\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"LIGO_DATAFIND_SERVER\">%s</profile>\"\"\" % os.environ['LIGO_DATAFIND_SERVER']\n    except:\n      pass\n    try:\n      print >> sitefile, \"\"\"    <profile namespace=\"env\" key=\"S6_SEGMENT_SERVER\">%s</profile>\"\"\" % os.environ['S6_SEGMENT_SERVER']\n    except:\n      pass\n\n    print >> sitefile, \"\"\"\\\n    <profile namespace=\"env\" key=\"JAVA_HEAPMAX\">4096</profile>\n    <profile namespace=\"pegasus\" key=\"style\">condor</profile>\n    <profile namespace=\"condor\" key=\"getenv\">True</profile>\n    <profile namespace=\"condor\" key=\"should_transfer_files\">YES</profile>\n    <profile namespace=\"condor\" key=\"when_to_transfer_output\">ON_EXIT_OR_EVICT</profile>\n  </site>\n\"\"\"\n\n    print >> sitefile, \"\"\"\\\n  <!-- Bologna cluster -->\n  <site handle=\"bologna\" arch=\"x86_64\" os=\"LINUX\">\n    <grid type=\"cream\" contact=\"https://ce01-lcg.cr.cnaf.infn.it:8443/ce-cream/services/CREAM2\" scheduler=\"LSF\" jobtype=\"compute\" />\n    <grid type=\"cream\" contact=\"https://ce01-lcg.cr.cnaf.infn.it:8443/ce-cream/services/CREAM2\" scheduler=\"LSF\" jobtype=\"auxillary\" />\n    <directory type=\"shared-scratch\" path=\"/storage/gpfs_virgo4/virgo4/%s/\">\n        <file-server operation=\"all\" url=\"srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/virgo4/%s/\"/>\n    </directory>\n    <profile namespace=\"pegasus\" key=\"style\">cream</profile>\n    <profile namespace=\"globus\" key=\"queue\">virgo</profile>\n  </site>\n\"\"\" % (os.path.basename(tmp_exec_dir),os.path.basename(tmp_exec_dir))\n\n    print >> sitefile, \"\"\"\\\n  <!-- Nikhef Big Grid -->\n  <site handle=\"nikhef\" arch=\"x86_64\" os=\"LINUX\">\n    <grid type=\"cream\" contact=\"https://klomp.nikhef.nl:8443/ce-cream/services/CREAM2\" scheduler=\"PBS\" jobtype=\"compute\" />\n    <grid type=\"cream\" contact=\"https://klomp.nikhef.nl:8443/ce-cream/services/CREAM2\" scheduler=\"PBS\" jobtype=\"auxillary\" />\n    <profile namespace=\"pegasus\" key=\"style\">cream</profile>\n    <profile namespace=\"globus\" key=\"queue\">medium</profile>\n  </site>\n  <!-- Nikhef Stage in Site -->\n  <site handle=\"nikhef-srm\" arch=\"x86_64\" os=\"LINUX\">\n    <directory type=\"shared-scratch\" path=\"/%s/\">\n      <file-server operation=\"all\" url=\"srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/virgo/%s/\" />\n    </directory>\n  </site>\n\"\"\" % (os.path.basename(tmp_exec_dir),os.path.basename(tmp_exec_dir))\n\n    try:\n      stampede_home = subprocess.check_output(\n        ['gsissh','-o','BatchMode=yes','-p','2222','stampede.tacc.xsede.org','pwd'])\n      stampede_home = stampede_home.split('/')\n      stampede_magic_number = stampede_home[2]\n      stampede_username = stampede_home[3]\n      shared_scratch = \"/work/%s/%s/ihope-workflow/%s\" % (\n        stampede_magic_number,stampede_username,os.path.basename(tmp_exec_dir))\n\n      print >> sitefile, \"\"\"\\\n  <!-- XSEDE Stampede Cluster at TACC Development Queue -->\n  <site handle=\"stampede-dev\" arch=\"x86_64\" os=\"LINUX\">\n     <grid type=\"gt5\" contact=\"login5.stampede.tacc.utexas.edu/jobmanager-fork\" scheduler=\"Fork\" jobtype=\"auxillary\"/>\n     <grid type=\"gt5\" contact=\"login5.stampede.tacc.utexas.edu/jobmanager-slurm\" scheduler=\"unknown\" jobtype=\"compute\"/>\n     <directory type=\"shared-scratch\" path=\"%s\">\n       <file-server operation=\"all\" url=\"gsiftp://gridftp.stampede.tacc.xsede.org%s\"/>\n     </directory>\n     <profile namespace=\"env\" key=\"PEGASUS_HOME\">/usr</profile>\n     <profile namespace=\"globus\" key=\"queue\">development</profile>\n     <profile namespace=\"globus\" key=\"maxwalltime\">180</profile>\n     <profile namespace=\"globus\" key=\"host_count\">1</profile>\n     <profile namespace=\"globus\" key=\"count\">16</profile>\n     <profile namespace=\"globus\" key=\"jobtype\">single</profile>\n     <profile namespace=\"globus\" key=\"project\">TG-PHY140012</profile>\n   </site>\n\"\"\" % (shared_scratch,shared_scratch)\n\n      print >> sitefile, \"\"\"\\\n  <!-- XSEDE Stampede Cluster at TACC Development Queue -->\n  <site handle=\"stampede\" arch=\"x86_64\" os=\"LINUX\">\n     <grid type=\"gt5\" contact=\"login5.stampede.tacc.utexas.edu/jobmanager-fork\" scheduler=\"Fork\" jobtype=\"auxillary\"/>\n     <grid type=\"gt5\" contact=\"login5.stampede.tacc.utexas.edu/jobmanager-slurm\" scheduler=\"unknown\" jobtype=\"compute\"/>\n     <directory type=\"shared-scratch\" path=\"%s\">\n       <file-server operation=\"all\" url=\"gsiftp://gridftp.stampede.tacc.xsede.org%s\"/>\n     </directory>\n     <profile namespace=\"env\" key=\"PEGASUS_HOME\">/usr</profile>\n     <profile namespace=\"globus\" key=\"queue\">development</profile>\n     <profile namespace=\"globus\" key=\"maxwalltime\">540</profile>\n     <profile namespace=\"globus\" key=\"host_count\">32</profile>\n     <profile namespace=\"globus\" key=\"count\">512</profile>\n     <profile namespace=\"globus\" key=\"jobtype\">single</profile>\n     <profile namespace=\"globus\" key=\"project\">TG-PHY140012</profile>\n   </site>\n\"\"\" % (shared_scratch,shared_scratch)\n\n    except:\n      print >> sitefile, \"\"\"\\\n  <!-- XSEDE Stampede Cluster disabled as gsissh to TACC failed-->\n\"\"\"\n\n    print >> sitefile, \"\"\"\\\n</sitecatalog>\"\"\"\n    sitefile.close()\n\n    # Write a help message telling the user how to run the workflow\n    print\n    print \"Created a workflow file which can be submitted by executing\"\n    print \"\"\"\n\n      ./pegasus_submit_dax\n\n    in the analysis directory on a condor submit machine.\n\n    From the analysis directory on the condor submit machine, you can run the\n    command\n\n      pegasus-status --long -t -i `./pegasus_basedir`\n\n    to check the status of your workflow. Once the workflow has finished you\n    can run the command\n\n      pegasus-analyzer -t -i `./pegasus_basedir`\n\n    to debug any failed jobs.\n    \"\"\"", "response": "Prepares the DAX for the given dag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config(self,sec,opt):\n    return string.strip(self.__cp.get(sec,opt))", "response": "Get the configration variable in a particular section of this jobs ini\n    file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_start(self,time,pass_to_command_line=True):\n    if pass_to_command_line:\n      self.add_var_opt('gps-start-time',time)\n    self.__start = time\n    self.__data_start = time", "response": "Set the start time of the node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_end(self,time,pass_to_command_line=True):\n    if pass_to_command_line:\n      self.add_var_opt('gps-end-time',time)\n    self.__end = time\n    self.__data_end = time", "response": "Set the end time of the node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_trig_start(self,time,pass_to_command_line=True):\n    if pass_to_command_line:\n      self.add_var_opt('trig-start-time',time)\n    self.__trig_start = time", "response": "Set the trig start time of the analysis node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_trig_end(self,time,pass_to_command_line=True):\n    if pass_to_command_line:\n      self.add_var_opt('trig-end-time',time)\n    self.__trig_end = time", "response": "Set the end time of the trig - end - time of the analysis node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_input(self,filename,pass_to_command_line=True):\n    self.__input = filename\n    if pass_to_command_line:\n      self.add_var_opt('input', filename)\n    self.add_input_file(filename)", "response": "Add an input file to the node by adding a variable option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an output file to the node by adding a variable option.", "response": "def set_output(self,filename,pass_to_command_line=True):\n    \"\"\"\n    Add an output to the node by adding a --output option.\n    @param filename: option argument to pass as output.\n    @bool pass_to_command_line: add output as a variable option.\n    \"\"\"\n    self.__output = filename\n    if pass_to_command_line:\n      self.add_var_opt('output', filename)\n    self.add_output_file(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the ifo name to analyze.", "response": "def set_ifo(self,ifo):\n    \"\"\"\n    Set the ifo name to analyze. If the channel name for the job is defined,\n    then the name of the ifo is prepended to the channel name obtained\n    from the job configuration file and passed with a --channel-name option.\n    @param ifo: two letter ifo code (e.g. L1, H1 or H2).\n    \"\"\"\n    self.__ifo = ifo\n    if self.job().channel():\n      self.add_var_opt('channel-name', ifo + ':' + self.job().channel())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the ifo - tag that is passed to the analysis code.", "response": "def set_ifo_tag(self,ifo_tag,pass_to_command_line=True):\n    \"\"\"\n    Set the ifo tag that is passed to the analysis code.\n    @param ifo_tag: a string to identify one or more IFOs\n    @bool pass_to_command_line: add ifo-tag as a variable option.\n    \"\"\"\n    self.__ifo_tag = ifo_tag\n    if pass_to_command_line:\n      self.add_var_opt('ifo-tag', ifo_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_user_tag(self,usertag,pass_to_command_line=True):\n    self.__user_tag = usertag\n    if pass_to_command_line:\n      self.add_var_opt('user-tag', usertag)", "response": "Sets the user tag that is passed to the analysis code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_cache(self,filename):\n    if isinstance( filename, str ):\n      # the name of a lal cache file created by a datafind node\n      self.add_var_opt('frame-cache', filename)\n      self.add_input_file(filename)\n    elif isinstance( filename, list ):\n      # we have an LFN list\n      self.add_var_opt('glob-frame-data',' ')\n      # only add the LFNs that actually overlap with this job\n      # XXX FIXME this is a very slow algorithm\n      if len(filename) == 0:\n        raise CondorDAGNodeError, \\\n          \"LDR did not return any LFNs for query: check ifo and frame type\"\n      for lfn in filename:\n        a, b, c, d = lfn.split('.')[0].split('-')\n        t_start = int(c)\n        t_end = int(c) + int(d)\n        if (t_start <= (self.get_data_end()+self.get_pad_data()+int(d)+1) \\\n          and t_end >= (self.get_data_start()-self.get_pad_data()-int(d)-1)):\n          self.add_input_file(lfn)\n      # set the frame type based on the LFNs returned by datafind\n      self.add_var_opt('frame-type',b)\n    else:\n      raise CondorDAGNodeError, \"Unknown LFN cache format\"", "response": "Set the frame cache to use."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calibration_cache_path(self):\n    if self.__ifo and self.__start > 0:\n        cal_path = self.job().get_config('calibration','path')\n\n        # check if this is S2: split calibration epochs\n        if ( self.__LHO2k.match(self.__ifo) and\n          (self.__start >= 729273613) and (self.__start <= 734367613) ):\n          if self.__start < int(\n            self.job().get_config('calibration','H2-cal-epoch-boundary')):\n            cal_file = self.job().get_config('calibration','H2-1')\n          else:\n            cal_file = self.job().get_config('calibration','H2-2')\n        else:\n            # if not: just add calibration cache\n            cal_file = self.job().get_config('calibration',self.__ifo)\n\n        cal = os.path.join(cal_path,cal_file)\n        self.__calibration_cache = cal\n    else:\n       msg = \"IFO and start-time must be set first\"\n       raise CondorDAGNodeError, msg", "response": "Determine the path to the correct calibration cache file to use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calibration(self):\n    # figure out the name of the calibration cache files\n    # as specified in the ini-file\n    self.calibration_cache_path()\n\n    if self.job().is_dax():\n      # new code for DAX\n      self.add_var_opt('glob-calibration-data','')\n      cache_filename=self.get_calibration()\n      pat = re.compile(r'(file://.*)')\n      f = open(cache_filename, 'r')\n      lines = f.readlines()\n\n      # loop over entries in the cache-file...\n      for line in lines:\n        m = pat.search(line)\n        if not m:\n          raise IOError\n        url = m.group(1)\n        # ... and add files to input-file list\n        path = urlparse.urlparse(url)[2]\n        calibration_lfn = os.path.basename(path)\n        self.add_input_file(calibration_lfn)\n    else:\n      # old .calibration for DAG's\n      self.add_var_opt('calibration-cache', self.__calibration_cache)\n      self.__calibration = self.__calibration_cache\n      self.add_input_file(self.__calibration)", "response": "Set the path to the calibration cache file for the given IFO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a list of chunks from the science segment.", "response": "def make_chunks(self,length=0,overlap=0,play=0,sl=0,excl_play=0,pad_data=0):\n    \"\"\"\n    Divides the science segment into chunks of length seconds overlapped by\n    overlap seconds. If the play option is set, only chunks that contain S2\n    playground data are generated. If the user has a more complicated way\n    of generating chunks, this method should be overriden in a sub-class.\n    Any data at the end of the ScienceSegment that is too short to contain a\n    chunk is ignored. The length of this unused data is stored and can be\n    retrieved with the unused() method.\n    @param length: length of chunk in seconds.\n    @param overlap: overlap between chunks in seconds.\n    @param play: 1 : only generate chunks that overlap with S2 playground data.\n                 2 : as play = 1 plus compute trig start and end times to\n                     coincide with the start/end of the playground\n    @param sl: slide by sl seconds before determining playground data.\n    @param excl_play: exclude the first excl_play second from the start and end\n    of the chunk when computing if the chunk overlaps with playground.\n    @param pad_data: exclude the first and last pad_data seconds of the segment\n    when generating chunks\n    \"\"\"\n    time_left = self.dur() - (2 * pad_data)\n    start = self.start() + pad_data\n    increment = length - overlap\n    while time_left >= length:\n      end = start + length\n      if (not play) or (play and (((end-sl-excl_play-729273613) % 6370) <\n        (600+length-2*excl_play))):\n        if (play == 2):\n        # calculate the start of the playground preceeding the chunk end\n          play_start = 729273613 + 6370 * \\\n           math.floor((end-sl-excl_play-729273613) / 6370)\n          play_end = play_start + 600\n          trig_start = 0\n          trig_end = 0\n          if ( (play_end - 6370) > start ):\n            print \"Two playground segments in this chunk:\",\n            print \"  Code to handle this case has not been implemented\"\n            sys.exit(1)\n          else:\n            if play_start > start:\n              trig_start = int(play_start)\n            if play_end < end:\n              trig_end = int(play_end)\n          self.__chunks.append(AnalysisChunk(start,end,trig_start,trig_end))\n        else:\n          self.__chunks.append(AnalysisChunk(start,end))\n      start += increment\n      time_left -= increment\n    self.__unused = time_left - overlap"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an AnalysisChunk to the list associated with this ScienceSegment.", "response": "def add_chunk(self,start,end,trig_start=0,trig_end=0):\n    \"\"\"\n    Add an AnalysisChunk to the list associated with this ScienceSegment.\n    @param start: GPS start time of chunk.\n    @param end: GPS end time of chunk.\n    @param trig_start: GPS start time for triggers from chunk\n    \"\"\"\n    self.__chunks.append(AnalysisChunk(start,end,trig_start,trig_end))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noverride the start time of this ScienceSegment and set the duration of the current GPS segment", "response": "def set_start(self,t):\n    \"\"\"\n    Override the GPS start time (and set the duration) of this ScienceSegment.\n    @param t: new GPS start time.\n    \"\"\"\n    self.__dur += self.__start - t\n    self.__start = t"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides the end time of this ScienceSegment and set the duration of this ScienceSegment.", "response": "def set_end(self,t):\n    \"\"\"\n    Override the GPS end time (and set the duration) of this ScienceSegment.\n    @param t: new GPS end time.\n    \"\"\"\n    self.__dur -= self.__end - t\n    self.__end = t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the output of segwizard into a list of ScienceSegments.", "response": "def read(self,filename,min_length,slide_sec=0,buffer=0):\n    \"\"\"\n    Parse the science segments from the segwizard output contained in file.\n    @param filename: input text file containing a list of science segments generated by\n    segwizard.\n    @param min_length: only append science segments that are longer than min_length.\n    @param slide_sec: Slide each ScienceSegment by::\n\n      delta > 0:\n        [s,e] -> [s+delta,e].\n      delta < 0:\n        [s,e] -> [s,e-delta].\n\n    @param buffer: shrink the ScienceSegment::\n\n      [s,e] -> [s+buffer,e-buffer]\n    \"\"\"\n    self.__filename = filename\n    octothorpe = re.compile(r'\\A#')\n    for line in open(filename):\n      if not octothorpe.match(line) and int(line.split()[3]) >= min_length:\n        (id,st,en,du) = map(int,line.split())\n\n        # slide the data if doing a background estimation\n        if slide_sec > 0:\n          st += slide_sec\n        elif slide_sec < 0:\n          en += slide_sec\n        du -= abs(slide_sec)\n\n        # add a buffer\n        if buffer > 0:\n          st += buffer\n          en -= buffer\n          du -= 2*abs(buffer)\n\n        x = ScienceSegment(tuple([id,st,en,du]))\n        self.__sci_segs.append(x)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the science segments from a tama list of locked segments contained in file.", "response": "def tama_read(self,filename):\n    \"\"\"\n    Parse the science segments from a tama list of locked segments contained in\n                file.\n    @param filename: input text file containing a list of tama segments.\n    \"\"\"\n    self.__filename = filename\n    for line in open(filename):\n      columns = line.split()\n      id = int(columns[0])\n      start = int(math.ceil(float(columns[3])))\n      end = int(math.floor(float(columns[4])))\n      dur = end - start\n\n      x = ScienceSegment(tuple([id, start, end, dur]))\n      self.__sci_segs.append(x)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndividing each ScienceSegment contained in this object into AnalysisChunks.", "response": "def make_chunks(self,length,overlap=0,play=0,sl=0,excl_play=0,pad_data=0):\n    \"\"\"\n    Divide each ScienceSegment contained in this object into AnalysisChunks.\n    @param length: length of chunk in seconds.\n    @param overlap: overlap between segments.\n    @param play: if true, only generate chunks that overlap with S2 playground\n    data.\n    @param sl: slide by sl seconds before determining playground data.\n    @param excl_play: exclude the first excl_play second from the start and end\n    of the chunk when computing if the chunk overlaps with playground.\n    \"\"\"\n    for seg in self.__sci_segs:\n      seg.make_chunks(length,overlap,play,sl,excl_play,pad_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_chunks_from_unused(self,length,trig_overlap,play=0,min_length=0,\n    sl=0,excl_play=0,pad_data=0):\n    \"\"\"\n    Create an extra chunk that uses up the unused data in the science segment.\n    @param length: length of chunk in seconds.\n    @param trig_overlap: length of time start generating triggers before the\n    start of the unused data.\n    @param play:\n                - 1 : only generate chunks that overlap with S2 playground data.\n                - 2 : as 1 plus compute trig start and end times to coincide\n                        with the start/end of the playground\n    @param min_length: the unused data must be greater than min_length to make a\n    chunk.\n    @param sl: slide by sl seconds before determining playground data.\n    @param excl_play: exclude the first excl_play second from the start and end\n    of the chunk when computing if the chunk overlaps with playground.\n    @param pad_data: exclude the first and last pad_data seconds of the segment\n    when generating chunks\n\n    \"\"\"\n    for seg in self.__sci_segs:\n      # if there is unused data longer than the minimum chunk length\n      if seg.unused() > min_length:\n        end = seg.end() - pad_data\n        start = end - length\n        if (not play) or (play and (((end-sl-excl_play-729273613)%6370) <\n          (600+length-2*excl_play))):\n          trig_start = end - seg.unused() - trig_overlap\n          if (play == 2):\n            # calculate the start of the playground preceeding the chunk end\n            play_start = 729273613 + 6370 * \\\n              math.floor((end-sl-excl_play-729273613) / 6370)\n            play_end = play_start + 600\n            trig_end = 0\n            if ( (play_end - 6370) > start ):\n              print \"Two playground segments in this chunk\"\n              print \"  Code to handle this case has not been implemented\"\n              sys.exit(1)\n            else:\n              if play_start > trig_start:\n                trig_start = int(play_start)\n              if (play_end < end):\n                trig_end = int(play_end)\n              if (trig_end == 0) or (trig_end > trig_start):\n                seg.add_chunk(start, end, trig_start, trig_end)\n          else:\n            seg.add_chunk(start, end, trig_start)\n        seg.set_unused(0)", "response": "This function creates a list of extra chunks that use up the unused data in the science segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a short chunk that uses up the unused data in the science segment .", "response": "def make_short_chunks_from_unused(\n    self,min_length,overlap=0,play=0,sl=0,excl_play=0):\n    \"\"\"\n    Create a chunk that uses up the unused data in the science segment\n    @param min_length: the unused data must be greater than min_length to make a\n    chunk.\n    @param overlap: overlap between chunks in seconds.\n    @param play: if true, only generate chunks that overlap with S2 playground data.\n    @param sl: slide by sl seconds before determining playground data.\n    @param excl_play: exclude the first excl_play second from the start and end\n    of the chunk when computing if the chunk overlaps with playground.\n    \"\"\"\n    for seg in self.__sci_segs:\n      if seg.unused() > min_length:\n        start = seg.end() - seg.unused() - overlap\n        end = seg.end()\n        length = start - end\n        if (not play) or (play and (((end-sl-excl_play-729273613)%6370) <\n        (600+length-2*excl_play))):\n          seg.add_chunk(start, end, start)\n        seg.set_unused(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting the ScienceSegments into chunks of a given maximum length and appends the data to the last two chunks.", "response": "def make_optimised_chunks(self, min_length, max_length, pad_data=0):\n    \"\"\"\n    Splits ScienceSegments up into chunks, of a given maximum length.\n    The length of the last two chunks are chosen so that the data\n    utilisation is optimised.\n    @param min_length: minimum chunk length.\n    @param max_length: maximum chunk length.\n    @param pad_data: exclude the first and last pad_data seconds of the\n    segment when generating chunks\n    \"\"\"\n    for seg in self.__sci_segs:\n      # pad data if requested\n      seg_start = seg.start() + pad_data\n      seg_end = seg.end() - pad_data\n\n      if seg.unused() > max_length:\n        # get number of max_length chunks\n        N = (seg_end - seg_start)/max_length\n\n        # split into chunks of max_length\n        for i in range(N-1):\n          start = seg_start + (i * max_length)\n          stop = start + max_length\n          seg.add_chunk(start, stop)\n\n        # optimise data usage for last 2 chunks\n        start = seg_start + ((N-1) * max_length)\n        middle = (start + seg_end)/2\n        seg.add_chunk(start, middle)\n        seg.add_chunk(middle, seg_end)\n        seg.set_unused(0)\n      elif seg.unused() > min_length:\n        # utilise as single chunk\n        seg.add_chunk(seg_start, seg_end)\n      else:\n        # no chunk of usable length\n        seg.set_unused(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace the ScienceSegments contained in this instance of ScienceData with the intersection of those in the other instance of ScienceData. Returns the number of segments in the intersection.", "response": "def intersection(self, other):\n    \"\"\"\n    Replaces the ScienceSegments contained in this instance of ScienceData\n    with the intersection of those in the instance other. Returns the number\n    of segments in the intersection.\n    @param other: ScienceData to use to generate the intersection\n    \"\"\"\n\n    # we only deal with the case of two lists here\n    length1 = len(self)\n    length2 = len(other)\n\n    # initialize list of output segments\n    ostart = -1\n    outlist = []\n    iseg2 = -1\n    start2 = -1\n    stop2 = -1\n\n    for seg1 in self:\n      start1 = seg1.start()\n      stop1 = seg1.end()\n      id = seg1.id()\n\n      # loop over segments from the second list which overlap this segment\n      while start2 < stop1:\n        if stop2 > start1:\n          # these overlap\n\n          # find the overlapping range\n          if start1 < start2:\n            ostart = start2\n          else:\n            ostart = start1\n          if stop1 > stop2:\n            ostop = stop2\n          else:\n            ostop = stop1\n\n          x = ScienceSegment(tuple([id, ostart, ostop, ostop-ostart]))\n          outlist.append(x)\n\n          if stop2 > stop1:\n            break\n\n        # step forward\n        iseg2 += 1\n        if iseg2 < len(other):\n          seg2 = other[iseg2]\n          start2 = seg2.start()\n          stop2 = seg2.end()\n        else:\n          # pseudo-segment in the far future\n          start2 = 2000000000\n          stop2 = 2000000000\n\n    # save the intersection and return the length\n    self.__sci_segs = outlist\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of ScienceSegments contained in this instance of ScienceData with the union of those in the other instance of ScienceData.", "response": "def union(self, other):\n    \"\"\"\n    Replaces the ScienceSegments contained in this instance of ScienceData\n    with the union of those in the instance other. Returns the number of\n    ScienceSegments in the union.\n    @param other: ScienceData to use to generate the intersection\n    \"\"\"\n\n    # we only deal with the case of two lists here\n    length1 = len(self)\n    length2 = len(other)\n\n    # initialize list of output segments\n    ostart = -1\n    seglist = []\n\n    i1 = -1\n    i2 = -1\n    start1 = -1\n    start2 = -1\n    id = -1\n\n    while 1:\n      # if necessary, get a segment from list 1\n      if start1 == -1:\n        i1 += 1\n        if i1 < length1:\n          start1 = self[i1].start()\n          stop1 = self[i1].end()\n          id = self[i1].id()\n        elif i2 == length2:\n          break\n\n      # if necessary, get a segment from list 2\n      if start2 == -1:\n        i2 += 1\n        if i2 < length2:\n          start2 = other[i2].start()\n          stop2 = other[i2].end()\n        elif i1 == length1:\n          break\n\n      # pick the earlier segment from the two lists\n      if start1 > -1 and ( start2 == -1 or start1 <= start2):\n        ustart = start1\n        ustop = stop1\n        # mark this segment has having been consumed\n        start1 = -1\n      elif start2 > -1:\n        ustart = start2\n        ustop = stop2\n        # mark this segment has having been consumed\n        start2 = -1\n      else:\n        break\n\n      # if the output segment is blank, initialize it; otherwise, see\n      # whether the new segment extends it or is disjoint\n      if ostart == -1:\n        ostart = ustart\n        ostop = ustop\n      elif ustart <= ostop:\n        if ustop > ostop:\n          # this extends the output segment\n          ostop = ustop\n        else:\n          # This lies entirely within the current output segment\n          pass\n      else:\n         # flush the current output segment, and replace it with the\n         # new segment\n         x = ScienceSegment(tuple([id,ostart,ostop,ostop-ostart]))\n         seglist.append(x)\n         ostart = ustart\n         ostop = ustop\n\n    # flush out the final output segment (if any)\n    if ostart != -1:\n      x = ScienceSegment(tuple([id,ostart,ostop,ostop-ostart]))\n      seglist.append(x)\n\n    self.__sci_segs = seglist\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef coalesce(self):\n\n    # check for an empty list\n    if len(self) == 0:\n      return 0\n\n    # sort the list of science segments\n    self.__sci_segs.sort()\n\n    # coalesce the list, checking each segment for validity as we go\n    outlist = []\n    ostop = -1\n\n    for seg in self:\n      start = seg.start()\n      stop = seg.end()\n      id = seg.id()\n      if start > ostop:\n        # disconnected, so flush out the existing segment (if any)\n        if ostop >= 0:\n          x = ScienceSegment(tuple([id,ostart,ostop,ostop-ostart]))\n          outlist.append(x)\n        ostart = start\n        ostop = stop\n      elif stop > ostop:\n        # extend the current segment\n        ostop = stop\n\n    # flush out the final segment (if any)\n    if ostop >= 0:\n      x = ScienceSegment(tuple([id,ostart,ostop,ostop-ostart]))\n      outlist.append(x)\n\n    self.__sci_segs = outlist\n    return len(self)", "response": "Coalesces any adjacent ScienceSegments. Returns the number of adjacent ScienceSegments in the coalesced list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvert the ScienceSegments in the class. Returns the number of ScienceSegments after inversion.", "response": "def invert(self):\n    \"\"\"\n    Inverts the ScienceSegments in the class (i.e. set NOT).  Returns the\n    number of ScienceSegments after inversion.\n    \"\"\"\n\n    # check for an empty list\n    if len(self) == 0:\n      # return a segment representing all time\n      self.__sci_segs = ScienceSegment(tuple([0,0,1999999999,1999999999]))\n\n    # go through the list checking for validity as we go\n    outlist = []\n    ostart = 0\n    for seg in self:\n      start = seg.start()\n      stop = seg.end()\n      if start < 0 or stop < start or start < ostart:\n        raise SegmentError, \"Invalid list\"\n      if start > 0:\n        x = ScienceSegment(tuple([0,ostart,start,start-ostart]))\n        outlist.append(x)\n      ostart = stop\n\n    if ostart < 1999999999:\n      x = ScienceSegment(tuple([0,ostart,1999999999,1999999999-ostart]))\n      outlist.append(x)\n\n    self.__sci_segs = outlist\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the length of the list of playground segments.", "response": "def play(self):\n    \"\"\"\n    Keep only times in ScienceSegments which are in the playground\n    \"\"\"\n\n    length = len(self)\n\n    # initialize list of output segments\n    ostart = -1\n    outlist = []\n    begin_s2 = 729273613\n    play_space = 6370\n    play_len = 600\n\n    for seg in self:\n      start = seg.start()\n      stop = seg.end()\n      id = seg.id()\n\n      # select first playground segment which ends after start of seg\n      play_start = begin_s2+play_space*( 1 +\n        int((start - begin_s2 - play_len)/play_space) )\n\n      while play_start < stop:\n        if play_start > start:\n          ostart = play_start\n        else:\n          ostart = start\n\n\n        play_stop = play_start + play_len\n\n        if play_stop < stop:\n          ostop = play_stop\n        else:\n          ostop = stop\n\n        x = ScienceSegment(tuple([id, ostart, ostop, ostop-ostart]))\n        outlist.append(x)\n\n        # step forward\n        play_start = play_start + play_space\n\n    # save the playground segs and return the length\n    self.__sci_segs = outlist\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nintersects two set of vertices.", "response": "def intersect_3(self, second, third):\n    \"\"\"\n    Intersection routine for three inputs.  Built out of the intersect,\n    coalesce and play routines\n    \"\"\"\n    self.intersection(second)\n    self.intersection(third)\n    self.coalesce()\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nintersecting two set of vertices.", "response": "def intersect_4(self, second, third, fourth):\n    \"\"\"\n     Intersection routine for four inputs.\n    \"\"\"\n    self.intersection(second)\n    self.intersection(third)\n    self.intersection(fourth)\n    self.coalesce()\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the segments in the list at least as long as dt", "response": "def split(self, dt):\n    \"\"\"\n      Split the segments in the list is subsegments at least as long as dt\n    \"\"\"\n    outlist=[]\n    for seg in self:\n      start = seg.start()\n      stop = seg.end()\n      id = seg.id()\n\n      while start < stop:\n        tmpstop = start + dt\n        if tmpstop > stop:\n          tmpstop = stop\n        elif tmpstop + dt > stop:\n          tmpstop = int( (start + stop)/2 )\n        x = ScienceSegment(tuple([id,start,tmpstop,tmpstop-start]))\n        outlist.append(x)\n        start = tmpstop\n\n    # save the split list and return length\n    self.__sci_segs = outlist\n    return len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef group(self, lst, n):\n    return itertools.izip(*[itertools.islice(lst, i, None, n) for i in range(n)])", "response": "Group an iterable into an n - tuples iterable. Incomplete\n    tuples are discarded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the frame cache file and stores the result in a dictionary.", "response": "def parse(self,type_regex=None):\n    \"\"\"\n    Each line of the frame cache file is like the following:\n\n    /frames/E13/LHO/frames/hoftMon_H1/H-H1_DMT_C00_L2-9246,H,H1_DMT_C00_L2,1,16 1240664820 6231 {924600000 924646720 924646784 924647472 924647712 924700000}\n\n    The description is as follows:\n\n    1.1) Directory path of files\n    1.2) Site\n    1.3) Type\n    1.4) Number of frames in the files (assumed to be 1)\n    1.5) Duration of the frame files.\n\n    2) UNIX timestamp for directory modification time.\n\n    3) Number of files that that match the above pattern in the directory.\n\n    4) List of time range or segments [start, stop)\n\n    We store the cache for each site and frameType combination\n    as a dictionary where the keys are (directory, duration)\n    tuples and the values are segment lists.\n\n    Since the cache file is already coalesced we do not\n    have to call the coalesce method on the segment lists.\n    \"\"\"\n    path = self.__path\n    cache = self.cache\n    if type_regex:\n      type_filter = re.compile(type_regex)\n    else:\n      type_filter = None\n\n    f = open(path, 'r')\n\n    # holds this iteration of the cache\n    gwfDict = {}\n\n    # parse each line in the cache file\n    for line in f:\n      # ignore lines that don't match the regex\n      if type_filter and type_filter.search(line) is None:\n        continue\n\n      # split on spaces and then comma to get the parts\n      header, modTime, fileCount, times = line.strip().split(' ', 3)\n      dir, site, frameType, frameCount, duration = header.split(',')\n      duration = int(duration)\n\n      # times string has form { t1 t2 t3 t4 t5 t6 ... tN t(N+1) }\n      # where the (ti, t(i+1)) represent segments\n      #\n      # first turn the times string into a list of integers\n      times = [ int(s) for s in times[1:-1].split(' ') ]\n\n      # group the integers by two and turn those tuples into segments\n      segments = [ pycbc_glue.segments.segment(a) for a in self.group(times, 2) ]\n\n      # initialize if necessary for this site\n      if not gwfDict.has_key(site):\n        gwfDict[site] = {}\n\n      # initialize if necessary for this frame type\n      if not gwfDict[site].has_key(frameType):\n        gwfDict[site][frameType] = {}\n\n      # record segment list as value indexed by the (directory, duration) tuple\n      key = (dir, duration)\n      if gwfDict[site][frameType].has_key(key):\n        msg = \"The combination %s is not unique in the frame cache file\" \\\n          % str(key)\n        raise RuntimeError, msg\n\n      gwfDict[site][frameType][key] = pycbc_glue.segments.segmentlist(segments)\n    f.close()\n\n    cache['gwf'] = gwfDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __set_output(self):\n    if self.__start and self.__end and self.__observatory and self.__type:\n      self.__output = os.path.join(self.__job.get_cache_dir(), self.__observatory + '-' + self.__type +'_CACHE' + '-' + str(self.__start) + '-' + str(self.__end - self.__start) + '.lcf')\n      self.set_output(self.__output)", "response": "Private method to set the output for the cache file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the start time of the datafind query.", "response": "def set_start(self,time,pad = None):\n    \"\"\"\n    Set the start time of the datafind query.\n    @param time: GPS start time of query.\n    \"\"\"\n    if pad:\n      self.add_var_opt('gps-start-time', int(time)-int(pad))\n    else:\n      self.add_var_opt('gps-start-time', int(time))\n    self.__start = time\n    self.__set_output()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_end(self,time):\n    self.add_var_opt('gps-end-time', time)\n    self.__end = time\n    self.__set_output()", "response": "Set the end time of the datafind query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_observatory(self,obs):\n    self.add_var_opt('observatory',obs)\n    self.__observatory = str(obs)\n    self.__set_output()", "response": "Sets the IFO to retrieve data for."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the type of the sequence", "response": "def set_type(self,type):\n    \"\"\"\n    sets the frame type that we are querying\n    \"\"\"\n    self.add_var_opt('type',str(type))\n    self.__type = str(type)\n    self.__set_output()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the output file for the current frame cache object.", "response": "def get_output(self):\n    \"\"\"\n    Return the output file, i.e. the file containing the frame cache data.\n    or the files itself as tuple (for DAX)\n    \"\"\"\n    if self.__dax:\n      # we are a dax running in grid mode so we need to resolve the\n      # frame file metadata into LFNs so pegasus can query the RLS\n      if self.__lfn_list is None:\n\n        if self.job().lsync_cache():\n          # get the lfns from the lsync cache object\n          if self.__lfn_list is None:\n            self.__lfn_list = self.job().lsync_cache().get_lfns(\n              self.get_observatory(), self.get_type(),\n              self.get_start(), self.get_end())\n\n        else:\n          # try querying the ligo_data_find server\n          try:\n            server = os.environ['LIGO_DATAFIND_SERVER']\n          except KeyError:\n            raise RuntimeError, \\\n              \"Environment variable LIGO_DATAFIND_SERVER is not set\"\n\n          try:\n            h = httplib.HTTPConnection(server)\n          except:\n            # try and get a proxy or certificate\n            # FIXME this doesn't check that it is valid, though\n            cert = None\n            key = None\n            try:\n              proxy = os.environ['X509_USER_PROXY']\n              cert = proxy\n              key = proxy\n            except:\n              try:\n                cert = os.environ['X509_USER_CERT']\n                key = os.environ['X509_USER_KEY']\n              except:\n                uid = os.getuid()\n                proxy_path = \"/tmp/x509up_u%d\" % uid\n                if os.access(path, os.R_OK):\n                  cert = proxy_path\n                  key = proxy_path\n\n            h = httplib.HTTPSConnection(server, key_file = key, cert_file = cert)\n\n          # construct the URL for a simple data find query\n          url = \"/LDR/services/data/v1/gwf/%s/%s/%s,%s.json\" % (\n            self.get_observatory(), self.get_type(),\n            str(self.get_start()), str(self.get_end()))\n\n          # query the server\n          h.request(\"GET\", url)\n          response = h.getresponse()\n\n          if response.status != 200:\n            msg = \"Server returned code %d: %s\" % (response.status, response.reason)\n            body = response.read()\n            msg += body\n            raise RuntimeError, msg\n\n          # since status is 200 OK read the URLs\n          body = response.read()\n\n          # decode the JSON\n          urlList = cjson.decode(body)\n          lfnDict = {}\n          for url in urlList:\n            path = urlparse.urlparse(url)[2]\n            lfn = os.path.split(path)[1]\n            lfnDict[lfn] = 1\n\n          self.__lfn_list = lfnDict.keys()\n          self.__lfn_list.sort()\n\n      return self.__lfn_list\n    else:\n      return self.__output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the xml output of the database.", "response": "def set_xml_output(self, xml_file):\n    \"\"\"\n    Tell ligolw_sqlite to dump the contents of the database to a file.\n    \"\"\"\n    if self.get_database() is None:\n      raise ValueError, \"no database specified\"\n    self.add_file_opt('extract', xml_file)\n    self.__xml_output = xml_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_output(self):\n    if self.__xml_output:\n      return self.__xml_output\n    elif self.get_database():\n      return self.get_database()\n    else:\n      raise ValueError, \"no output xml file or database specified\"", "response": "Override get_output to return xml - file or database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_version():\n    contents = read(os.path.join(PATH_BASE, 'srptools', '__init__.py'))\n    version = re.search('VERSION = \\(([^)]+)\\)', contents)\n    version = version.group(1).replace(', ', '.').strip()\n    return version", "response": "Reads version number from the contents of the\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for the get_coinc_def_id method of the CoincDefiner table in pycbc_glue. ligolw. lsctables. CoincDefTable class in pycbc_glue. ligolw. lsctables.", "response": "def get_coinc_def_id(xmldoc, search, coinc_type, create_new = True, description = u\"\"):\n\t\"\"\"\n\tWrapper for the get_coinc_def_id() method of the CoincDefiner table\n\tclass in pycbc_glue.ligolw.lsctables.  This wrapper will optionally\n\tcreate a new coinc_definer table in the document if one does not\n\talready exist.\n\t\"\"\"\n\ttry:\n\t\tcoincdeftable = lsctables.CoincDefTable.get_table(xmldoc)\n\texcept ValueError:\n\t\t# table not found\n\t\tif not create_new:\n\t\t\traise\n\t\t# FIXME:  doesn't work if the document is stored in a\n\t\t# database.\n\t\tcoincdeftable = lsctables.New(lsctables.CoincDefTable)\n\t\txmldoc.childNodes[0].appendChild(coincdeftable)\n\t# make sure the next_id attribute is correct\n\tcoincdeftable.sync_next_id()\n\t# get the id\n\treturn coincdeftable.get_coinc_def_id(search, coinc_type, create_new = create_new, description = description)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fromlalcache(cachefile, coltype = int):\n\treturn segments.segmentlist(lal.CacheEntry(l, coltype = coltype).segment for l in cachefile)", "response": "Construct a segmentlist representing the times spanned by the files in the LAL cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fromsegwizard(file, coltype = int, strict = True):\n\tcommentpat = re.compile(r\"\\s*([#;].*)?\\Z\", re.DOTALL)\n\ttwocolsegpat = re.compile(r\"\\A\\s*([\\d.+-eE]+)\\s+([\\d.+-eE]+)\\s*\\Z\")\n\tthreecolsegpat = re.compile(r\"\\A\\s*([\\d.+-eE]+)\\s+([\\d.+-eE]+)\\s+([\\d.+-eE]+)\\s*\\Z\")\n\tfourcolsegpat = re.compile(r\"\\A\\s*([\\d]+)\\s+([\\d.+-eE]+)\\s+([\\d.+-eE]+)\\s+([\\d.+-eE]+)\\s*\\Z\")\n\tformat = None\n\tl = segments.segmentlist()\n\tfor line in file:\n\t\tline = commentpat.split(line)[0]\n\t\tif not line:\n\t\t\tcontinue\n\t\ttry:\n\t\t\t[tokens] = fourcolsegpat.findall(line)\n\t\t\tnum = int(tokens[0])\n\t\t\tseg = segments.segment(map(coltype, tokens[1:3]))\n\t\t\tduration = coltype(tokens[3])\n\t\t\tthis_line_format = 4\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\t[tokens] = threecolsegpat.findall(line)\n\t\t\t\tseg = segments.segment(map(coltype, tokens[0:2]))\n\t\t\t\tduration = coltype(tokens[2])\n\t\t\t\tthis_line_format = 3\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\t[tokens] = twocolsegpat.findall(line)\n\t\t\t\t\tseg = segments.segment(map(coltype, tokens[0:2]))\n\t\t\t\t\tduration = abs(seg)\n\t\t\t\t\tthis_line_format = 2\n\t\t\t\texcept ValueError:\n\t\t\t\t\tbreak\n\t\tif strict:\n\t\t\tif abs(seg) != duration:\n\t\t\t\traise ValueError(\"segment '%s' has incorrect duration\" % line)\n\t\t\tif format is None:\n\t\t\t\tformat = this_line_format\n\t\t\telif format != this_line_format:\n\t\t\t\traise ValueError(\"segment '%s' format mismatch\" % line)\n\t\tl.append(seg)\n\treturn l", "response": "Read a segmentlist from a segwizard file containing a segwizard compatible segment list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tosegwizard(file, seglist, header = True, coltype = int):\n\tif header:\n\t\tprint >>file, \"# seg\\tstart    \\tstop     \\tduration\"\n\tfor n, seg in enumerate(seglist):\n\t\tprint >>file, \"%d\\t%s\\t%s\\t%s\" % (n, str(coltype(seg[0])), str(coltype(seg[1])), str(coltype(abs(seg))))", "response": "Writes the segmentlist seglist to the file object file in segwizard compatible format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a segmentlist from a TAMA locked - segments file.", "response": "def fromtama(file, coltype = lal.LIGOTimeGPS):\n\t\"\"\"\n\tRead a segmentlist from the file object file containing TAMA\n\tlocked-segments data.  Parsing stops on the first line that cannot\n\tbe parsed (which is consumed).  The segmentlist will be created\n\twith segments whose boundaries are of type coltype, which should\n\traise ValueError if it cannot convert its string argument.\n\n\tNOTE:  TAMA locked-segments files contain non-integer start and end\n\ttimes, so the default column type is set to LIGOTimeGPS.\n\n\tNOTE:  the output is a segmentlist as described by the file;  if\n\tthe segments in the input file are not coalesced or out of order,\n\tthen thusly shall be the output of this function.  It is\n\trecommended that this function's output be coalesced before use.\n\t\"\"\"\n\tsegmentpat = re.compile(r\"\\A\\s*\\S+\\s+\\S+\\s+\\S+\\s+([\\d.+-eE]+)\\s+([\\d.+-eE]+)\")\n\tl = segments.segmentlist()\n\tfor line in file:\n\t\ttry:\n\t\t\t[tokens] = segmentpat.findall(line)\n\t\t\tl.append(segments.segment(map(coltype, tokens[0:2])))\n\t\texcept ValueError:\n\t\t\tbreak\n\treturn l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a list of ranges expressed as strings in the form value first last or infinity.", "response": "def from_range_strings(ranges, boundtype = int):\n\t\"\"\"\n\tParse a list of ranges expressed as strings in the form \"value\" or\n\t\"first:last\" into an equivalent pycbc_glue.segments.segmentlist.  In the\n\tlatter case, an empty string for \"first\" and(or) \"last\" indicates a\n\t(semi)infinite range.  A typical use for this function is in\n\tparsing command line options or entries in configuration files.\n\n\tNOTE:  the output is a segmentlist as described by the strings;  if\n\tthe segments in the input file are not coalesced or out of order,\n\tthen thusly shall be the output of this function.  It is\n\trecommended that this function's output be coalesced before use.\n\n\tExample:\n\n\t>>> text = \"0:10,35,100:\"\n\t>>> from_range_strings(text.split(\",\"))\n\t[segment(0, 10), segment(35, 35), segment(100, infinity)]\n\t\"\"\"\n\t# preallocate segmentlist\n\tsegs = segments.segmentlist([None] * len(ranges))\n\n\t# iterate over strings\n\tfor i, range in enumerate(ranges):\n\t\tparts = range.split(\":\")\n\t\tif len(parts) == 1:\n\t\t\tparts = boundtype(parts[0])\n\t\t\tsegs[i] = segments.segment(parts, parts)\n\t\t\tcontinue\n\t\tif len(parts) != 2:\n\t\t\traise ValueError(range)\n\t\tif parts[0] == \"\":\n\t\t\tparts[0] = segments.NegInfinity\n\t\telse:\n\t\t\tparts[0] = boundtype(parts[0])\n\t\tif parts[1] == \"\":\n\t\t\tparts[1] = segments.PosInfinity\n\t\telse:\n\t\t\tparts[1] = boundtype(parts[1])\n\t\tsegs[i] = segments.segment(parts[0], parts[1])\n\n\t# success\n\treturn segs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_range_strings(seglist):\n\t# preallocate the string list\n\tranges = [None] * len(seglist)\n\n\t# iterate over segments\n\tfor i, seg in enumerate(seglist):\n\t\tif not seg:\n\t\t\tranges[i] = str(seg[0])\n\t\telif (seg[0] is segments.NegInfinity) and (seg[1] is segments.PosInfinity):\n\t\t\tranges[i] = \":\"\n\t\telif (seg[0] is segments.NegInfinity) and (seg[1] is not segments.PosInfinity):\n\t\t\tranges[i] = \":%s\" % str(seg[1])\n\t\telif (seg[0] is not segments.NegInfinity) and (seg[1] is segments.PosInfinity):\n\t\t\tranges[i] = \"%s:\" % str(seg[0])\n\t\telif (seg[0] is not segments.NegInfinity) and (seg[1] is not segments.PosInfinity):\n\t\t\tranges[i] = \"%s:%s\" % (str(seg[0]), str(seg[1]))\n\t\telse:\n\t\t\traise ValueError(seg)\n\n\t# success\n\treturn ranges", "response": "Turn a segment list into a list of range strings as could be parsed\n\tby from_range_strings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representation of a segmentlistdict object. Each segmentlist in the dictionary is encoded using to_range_strings() with \",\" used to delimit segments. The keys are converted to strings and paired with the string representations of their segmentlists using \"=\" as a delimiter. Finally the key=value strings are combined using \"/\" to delimit them. Example: >>> from pycbc_glue.segments import * >>> segs = segmentlistdict({\"H1\": segmentlist([segment(0, 10), segment(35, 35), segment(100, infinity())]), \"L1\": segmentlist([segment(5, 15), segment(45, 60)])}) >>> segmentlistdict_to_short_string(segs) 'H1=0:10,35,100:/L1=5:15,45:60' This function, and its inverse segmentlistdict_from_short_string(), are intended to be used to allow small segmentlistdict objects to be encoded in command line options and config files. For large segmentlistdict objects or when multiple sets of segmentlists are required, the LIGO Light Weight XML encoding available through the pycbc_glue.ligolw library should be used.", "response": "def segmentlistdict_to_short_string(seglists):\n\t\"\"\"\n\tReturn a string representation of a segmentlistdict object.  Each\n\tsegmentlist in the dictionary is encoded using to_range_strings()\n\twith \",\" used to delimit segments.  The keys are converted to\n\tstrings and paired with the string representations of their\n\tsegmentlists using \"=\" as a delimiter.  Finally the key=value\n\tstrings are combined using \"/\" to delimit them.\n\n\tExample:\n\n\t>>> from pycbc_glue.segments import *\n\t>>> segs = segmentlistdict({\"H1\": segmentlist([segment(0, 10), segment(35, 35), segment(100, infinity())]), \"L1\": segmentlist([segment(5, 15), segment(45, 60)])})\n\t>>> segmentlistdict_to_short_string(segs)\n\t'H1=0:10,35,100:/L1=5:15,45:60'\n\n\tThis function, and its inverse segmentlistdict_from_short_string(),\n\tare intended to be used to allow small segmentlistdict objects to\n\tbe encoded in command line options and config files.  For large\n\tsegmentlistdict objects or when multiple sets of segmentlists are\n\trequired, the LIGO Light Weight XML encoding available through the\n\tpycbc_glue.ligolw library should be used.\n\t\"\"\"\n\treturn \"/\".join([\"%s=%s\" % (str(key), \",\".join(to_range_strings(value))) for key, value in seglists.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef segmentlistdict_from_short_string(s, boundtype = int):\n\td = segments.segmentlistdict()\n\tfor token in s.strip().split(\"/\"):\n\t\tkey, ranges = token.strip().split(\"=\")\n\t\td[key.strip()] = from_range_strings(ranges.strip().split(\",\"), boundtype = boundtype)\n\treturn d", "response": "This function parses a string representation of a set of named segmentlists into a segmentlistdict object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_bitstream(bitstream, start, dt, minlen = 1):\n\tbitstream = iter(bitstream)\n\ti = 0\n\twhile 1:\n\t\tif bitstream.next():\n\t\t\t# found start of True block; find the end\n\t\t\tj = i + 1\n\t\t\ttry:\n\t\t\t\twhile bitstream.next():\n\t\t\t\t\tj += 1\n\t\t\tfinally:  # make sure StopIteration doesn't kill final segment\n\t\t\t\tif j - i >= minlen:\n\t\t\t\t\tyield segments.segment(start + i * dt, start + j * dt)\n\t\t\ti = j  # advance to end of block\n\t\ti += 1", "response": "Convert consecutive True values in a bit stream to a stream of segments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a segmentlist identifying the S2 playground times within the specified segment extent.", "response": "def S2playground(extent):\n\t\"\"\"\n\tReturn a segmentlist identifying the S2 playground times within the\n\tinterval defined by the segment extent.\n\n\tExample:\n\n\t>>> from pycbc_glue import segments\n\t>>> S2playground(segments.segment(874000000, 874010000))\n\t[segment(874000013, 874000613), segment(874006383, 874006983)]\n\t\"\"\"\n\tlo = int(extent[0])\n\tlo -= (lo - 729273613) % 6370\n\thi = int(extent[1]) + 1\n\treturn segments.segmentlist(segments.segment(t, t + 600) for t in range(lo, hi, 6370)) & segments.segmentlist([extent])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef segmentlist_range(start, stop, period):\n\tn = 1\n\tb = start\n\twhile True:\n\t\ta, b = b, start + n * period\n\t\tif b > stop:\n\t\t\tbreak\n\t\tyield segments.segment(a, b)\n\t\tn += 1", "response": "This generator yields a coalesced segmentlist of continuous adjacent segments each of length period starting at start and ending at stop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Fold(seglist1, seglist2):\n\tfor seg in seglist2:\n\t\tyield (seglist1 & segments.segmentlist([seg])).shift(-seg[0])", "response": "An iterator that yields the results of taking the intersection of seglist1 with each segment in seglist2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a sequence of segmentlists returns the intervals during which at least n of them intersect.", "response": "def vote(seglists, n):\n\t\"\"\"\n\tGiven a sequence of segmentlists, returns the intervals during\n\twhich at least n of them intersect.  The input segmentlists must be\n\tcoalesced, the output is coalesced.\n\n\tExample:\n\n\t>>> from pycbc_glue.segments import *\n\t>>> w = segmentlist([segment(0, 15)])\n\t>>> x = segmentlist([segment(5, 20)])\n\t>>> y = segmentlist([segment(10, 25)])\n\t>>> z = segmentlist([segment(15, 30)])\n\t>>> vote((w, x, y, z), 3)\n\t[segment(10, 20)]\n\n\tThe sequence of segmentlists is only iterated over once, and the\n\tsegmentlists within it are only iterated over once;  they can all\n\tbe generators.  If there are a total of N segments in M segment\n\tlists and the final result has L segments the algorithm is O(N M) +\n\tO(L).\n\t\"\"\"\n\t# check for no-op\n\n\tif n < 1:\n\t\treturn segments.segmentlist()\n\n\t# digest the segmentlists into an ordered sequence of off-on and\n\t# on-off transitions with the vote count for each transition\n\t# FIXME:  this generator is declared locally for now, is it useful\n\t# as a stand-alone generator?\n\n\tdef pop_min(l):\n\t\t# remove and return the smallest value from a list\n\t\tval = min(l)\n\t\tfor i in xrange(len(l) - 1, -1, -1):\n\t\t\tif l[i] is val:\n\t\t\t\treturn l.pop(i)\n\t\tassert False\t# cannot get here\n\n\tdef vote_generator(seglists):\n\t\tqueue = []\n\t\tfor seglist in seglists:\n\t\t\tsegiter = iter(seglist)\n\t\t\ttry:\n\t\t\t\tseg = segiter.next()\n\t\t\texcept StopIteration:\n\t\t\t\tcontinue\n\t\t\t# put them in so that the smallest boundary is\n\t\t\t# closest to the end of the list\n\t\t\tqueue.append((seg[1], -1, segiter))\n\t\t\tqueue.append((seg[0], +1, None))\n\t\tif not queue:\n\t\t\treturn\n\t\tqueue.sort(reverse = True)\n\t\tbound = queue[-1][0]\n\t\tvotes = 0\n\t\twhile queue:\n\t\t\tthis_bound, delta, segiter = pop_min(queue)\n\t\t\tif this_bound == bound:\n\t\t\t\tvotes += delta\n\t\t\telse:\n\t\t\t\tyield bound, votes\n\t\t\t\tbound = this_bound\n\t\t\t\tvotes = delta\n\t\t\tif segiter is not None:\n\t\t\t\ttry:\n\t\t\t\t\tseg = segiter.next()\n\t\t\t\texcept StopIteration:\n\t\t\t\t\tcontinue\n\t\t\t\tqueue.append((seg[1], -1, segiter))\n\t\t\t\tqueue.append((seg[0], +1, None))\n\t\tyield bound, votes\n\n\t# compute the cumulative sum of votes, and assemble a segmentlist\n\t# from the intervals when the vote count is equal to or greater\n\t# than n\n\n\tresult = segments.segmentlist()\n\tvotes = 0\n\tfor bound, delta in vote_generator(seglists):\n\t\tif delta > 0 and n - delta <= votes < n:\n\t\t\tstart = bound\n\t\telif delta < 0 and n <= votes < n - delta:\n\t\t\tresult.append(segments.segment(start, bound))\n\t\t\tdel start\t# detect stops that aren't preceded by starts\n\t\tvotes += delta\n\tassert votes == 0\t# detect failed cumulative sum\n\n\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_proxy(path):\n    # load the proxy from path\n    try:\n        proxy = M2Crypto.X509.load_cert(path)\n    except Exception, e:\n        msg = \"Unable to load proxy from path %s : %s\" % (path, e)\n        raise RuntimeError(msg)\n\n    # make sure the proxy is RFC 3820 compliant\n    try:\n        proxy.get_ext(\"proxyCertInfo\")\n    except LookupError:\n        subject = proxy.get_subject().as_text()\n        if re.search(r'.+CN=proxy$', subject):\n            raise RuntimeError(\"Could not find a RFC 3820 compliant proxy \"\n                               \"credential. Please run 'grid-proxy-init -rfc' \"\n                               \"and try again.\")\n\n    # attempt to make sure the proxy is still good for more than 15 minutes\n    try:\n        expireASN1 = proxy.get_not_after().__str__()\n        expireGMT  = time.strptime(expireASN1, \"%b %d %H:%M:%S %Y %Z\")\n        expireUTC  = calendar.timegm(expireGMT)\n        now = int(time.time())\n        secondsLeft = expireUTC - now\n    except Exception, e:\n        # problem getting or parsing time so just let the client\n        # continue and pass the issue along to the server\n        secondsLeft = 3600\n\n    if secondsLeft <= 0:\n        raise RuntimeError(\"Your proxy certificate is expired.\\n\"\n                           \"Please generate a new proxy certificate and \"\n                           \"try again. \")\n    if secondsLeft < (60 * 15):\n        raise RuntimeError(\"Your proxy certificate expires in less than 15 \"\n                           \"minutes.\\nPlease generate a new proxy \"\n                           \"certificate and try again.\")\n\n    # return True to indicate validated proxy\n    return True", "response": "Validate the users X509 proxy certificate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_credential():\n\n    rfc_proxy_msg = (\"Could not find a RFC 3820 compliant proxy credential.\"\n                     \"Please run 'grid-proxy-init -rfc' and try again.\")\n\n    # use X509_USER_PROXY from environment if set\n    if os.environ.has_key('X509_USER_PROXY'):\n        filePath = os.environ['X509_USER_PROXY']\n        if validate_proxy(filePath):\n            return filePath, filePath\n        else:\n            raise RuntimeError(rfc_proxy_msg)\n\n    # use X509_USER_CERT and X509_USER_KEY if set\n    if (os.environ.has_key('X509_USER_CERT') and\n        os.environ.has_key('X509_USER_KEY')):\n        certFile = os.environ['X509_USER_CERT']\n        keyFile = os.environ['X509_USER_KEY']\n        return certFile, keyFile\n\n    # search for proxy file on disk\n    uid = os.getuid()\n    path = \"/tmp/x509up_u%d\" % uid\n\n    if os.access(path, os.R_OK):\n        if validate_proxy(path):\n            return path, path\n        else:\n            raise RuntimeError(rfc_proxy_msg)\n\n    # if we get here could not find a credential\n    raise RuntimeError(rfc_proxy_msg)", "response": "Locate the users X509 certificate and key files and return the path and key of the proxy file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the default server host from the environment variable _server_env.", "response": "def find_server():\n    \"\"\"Find the default server host from the environment\n\n    This method uses the C{LIGO_DATAFIND_SERVER} variable to construct\n    a C{(host, port)} tuple.\n\n    @returns: C{(host, port)}: the L{str} host name and L{int} port number\n\n    @raises RuntimeError: if the C{LIGO_DATAFIND_SERVER} environment variable\n                          is not set\n    \"\"\"\n\n    if os.environ.has_key(_server_env):\n        host = os.environ[_server_env]\n        port = None\n        if re.search(':', host):\n            host, port = host.split(':', 1)\n            if port:\n                port = int(port)\n        return host, port\n    else:\n        raise RuntimeError(\"Environment variable %s is not set\" % _server_env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the LDR host for observatories. Use match to restrict returned observatories to those matching the regular expression.", "response": "def find_observatories(self, match=None):\n        \"\"\"Query the LDR host for observatories. Use match to\n        restrict returned observatories to those matching the\n        regular expression.\n\n        Example:\n\n        >>> connection.find_observatories()\n        ['AGHLT', 'G', 'GHLTV', 'GHLV', 'GHT', 'H', 'HL', 'HLT',\n         'L', 'T', 'V', 'Z']\n        >>> connection.find_observatories(\"H\")\n        ['H', 'HL', 'HLT']\n\n        @type  match: L{str}\n        @param match:\n            name to match return observatories against\n\n        @returns: L{list} of observatory prefixes\n        \"\"\"\n        url = \"%s/gwf.json\" % _url_prefix\n        response = self._requestresponse(\"GET\", url)\n        sitelist = sorted(set(decode(response.read())))\n        if match:\n            regmatch = re.compile(match)\n            sitelist = [site for site in sitelist if regmatch.search(site)]\n        return sitelist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the LDR host for frame types. Use site to restrict query to given observatory prefix and use match to restrict query to given regular expression.", "response": "def find_types(self, site=None, match=None):\n        \"\"\"Query the LDR host for frame types. Use site to restrict\n        query to given observatory prefix, and use match to restrict\n        returned types to those matching the regular expression.\n\n        Example:\n\n        >>> connection.find_types(\"L\", \"RDS\")\n        ['L1_RDS_C01_LX',\n         'L1_RDS_C02_LX',\n         'L1_RDS_C03_L2',\n         'L1_RDS_R_L1',\n         'L1_RDS_R_L3',\n         'L1_RDS_R_L4',\n         'PEM_RDS_A6',\n         'RDS_R_L1',\n         'RDS_R_L2',\n         'RDS_R_L3',\n         'TESTPEM_RDS_A6']\n\n        @param  site: single-character name of site to match\n        @param match: type-name to match against\n\n        @type  site: L{str}\n        @type match: L{str}\n\n        @returns: L{list} of frame types\n        \"\"\"\n        if site:\n            url = \"%s/gwf/%s.json\" % (_url_prefix, site[0])\n        else:\n            url = \"%s/gwf/all.json\" % _url_prefix\n        response = self._requestresponse(\"GET\", url)\n        typelist = sorted(set(decode(response.read())))\n        if match:\n            regmatch = re.compile(match)\n            typelist = [type for type in typelist if regmatch.search(type)]\n        return typelist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the LDR for which frames are avaliable Use gpsstart and gpsend to restrict the returned times to this semiopen interval.", "response": "def find_times(self, site, frametype, gpsstart=None, gpsend=None):\n        \"\"\"Query the LDR for times for which frames are avaliable\n\n        Use gpsstart and gpsend to restrict the returned times to\n        this semiopen interval.\n\n        @returns: L{segmentlist<pycbc_glue.segments.segmentlist>}\n\n        @param site:\n            single-character name of site to match\n        @param frametype:\n            name of frametype to match\n        @param gpsstart:\n            integer GPS start time of query\n        @param gpsend:\n            integer GPS end time of query\n\n        @type       site: L{str}\n        @type  frametype: L{str}\n        @type   gpsstart: L{int}\n        @type     gpsend: L{int}\n        \"\"\"\n        if gpsstart and gpsend:\n            url = (\"%s/gwf/%s/%s/segments/%s,%s.json\"\n                   % (_url_prefix, site, frametype, gpsstart, gpsend))\n        else:\n            url = (\"%s/gwf/%s/%s/segments.json\"\n                   % (_url_prefix, site, frametype))\n\n        response = self._requestresponse(\"GET\", url)\n        segmentlist = decode(response.read())\n        return segments.segmentlist(map(segments.segment, segmentlist))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_frame(self, framefile, urltype=None, on_missing=\"warn\"):\n        if on_missing not in (\"warn\", \"error\", \"ignore\"):\n            raise ValueError(\"on_missing must be 'warn', 'error', or 'ignore'.\")\n        framefile = os.path.basename(framefile)\n        # parse file name for site, frame type\n        try:\n            site,frametype,_,_ = framefile.split(\"-\")\n        except Exception, e:\n            raise RuntimeError(\"Error parsing filename %s: %s\" % (framefile, e))\n        url = (\"%s/gwf/%s/%s/%s.json\"\n              % (_url_prefix, site, frametype, framefile))\n        response = self._requestresponse(\"GET\", url)\n        urllist  = decode(response.read())\n        if len(urllist) == 0:\n            if on_missing == \"warn\":\n                sys.stderr.write(\"No files found!\\n\")\n            elif on_missing == \"error\":\n                raise RuntimeError(\"No files found!\")\n        # verify urltype is what we want\n        cache = lal.Cache(e for e in\n                 [lal.CacheEntry.from_T050017(x, coltype=self.LIGOTimeGPSType)\n                  for x in urllist] if not urltype or e.scheme == urltype)\n        return cache", "response": "Query the LDR host for a single framefile and return the corresponding cache object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_frame_urls(self, site, frametype, gpsstart, gpsend,\n                        match=None, urltype=None, on_gaps=\"warn\"):\n        \"\"\"Find the framefiles for the given type in the [start, end) interval\n        frame\n\n        @param site:\n            single-character name of site to match\n        @param frametype:\n            name of frametype to match\n        @param gpsstart:\n            integer GPS start time of query\n        @param gpsend:\n            integer GPS end time of query\n        @param match:\n            regular expression to match against\n        @param urltype:\n            file scheme to search for (e.g. 'file')\n        @param on_gaps:\n            what to do when the requested frame isn't found, one of:\n                - C{'warn'} (default): print a warning,\n                - C{'error'}: raise an L{RuntimeError}, or\n                - C{'ignore'}: do nothing\n\n        @type       site: L{str}\n        @type  frametype: L{str}\n        @type   gpsstart: L{int}\n        @type     gpsend: L{int}\n        @type      match: L{str}\n        @type    urltype: L{str}\n        @type    on_gaps: L{str}\n\n        @returns: L{Cache<pycbc_glue.lal.Cache>}\n\n        @raises RuntimeError: if gaps are found and C{on_gaps='error'}\n        \"\"\"\n        if on_gaps not in (\"warn\", \"error\", \"ignore\"):\n            raise ValueError(\"on_gaps must be 'warn', 'error', or 'ignore'.\")\n        url = (\"%s/gwf/%s/%s/%s,%s\"\n               % (_url_prefix, site, frametype, gpsstart, gpsend))\n        # if a URL type is specified append it to the path\n        if urltype:\n            url += \"/%s\" % urltype\n        # request JSON output\n        url += \".json\"\n        # append a regex if input\n        if match:\n            url += \"?match=%s\" % match\n        # make query\n        response = self._requestresponse(\"GET\", url)\n        urllist  = decode(response.read())\n\n        out = lal.Cache([lal.CacheEntry.from_T050017(x,\n                         coltype=self.LIGOTimeGPSType) for x in urllist])\n\n        if on_gaps == \"ignore\":\n            return out\n        else:\n            span    = segments.segment(gpsstart, gpsend)\n            seglist = segments.segmentlist(e.segment for e in out).coalesce()\n            missing = (segments.segmentlist([span]) - seglist).coalesce()\n            if span in seglist:\n                return out\n            else:\n                msg = \"Missing segments: \\n%s\" % \"\\n\".join(map(str, missing))\n                if on_gaps==\"warn\":\n                    sys.stderr.write(\"%s\\n\" % msg)\n                    return out\n                else:\n                    raise RuntimeError(msg)", "response": "Find the framefiles for the given type in the given start and end time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwalks the XML tree of children below elem returning each in order.", "response": "def WalkChildren(elem):\n\t\"\"\"\n\tWalk the XML tree of children below elem, returning each in order.\n\t\"\"\"\n\tfor child in elem.childNodes:\n\t\tyield child\n\t\tfor elem in WalkChildren(child):\n\t\t\tyield elem"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a child to this element.", "response": "def appendChild(self, child):\n\t\t\"\"\"\n\t\tAdd a child to this element.  The child's parentNode\n\t\tattribute is updated, too.\n\t\t\"\"\"\n\t\tself.childNodes.append(child)\n\t\tchild.parentNode = self\n\t\tself._verifyChildren(len(self.childNodes) - 1)\n\t\treturn child"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts a new child node before an existing child node.", "response": "def insertBefore(self, newchild, refchild):\n\t\t\"\"\"\n\t\tInsert a new child node before an existing child. It must\n\t\tbe the case that refchild is a child of this node; if not,\n\t\tValueError is raised. newchild is returned.\n\t\t\"\"\"\n\t\tfor i, childNode in enumerate(self.childNodes):\n\t\t\tif childNode is refchild:\n\t\t\t\tself.childNodes.insert(i, newchild)\n\t\t\t\tnewchild.parentNode = self\n\t\t\t\tself._verifyChildren(i)\n\t\t\t\treturn newchild\n\t\traise ValueError(refchild)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removeChild(self, child):\n\t\tfor i, childNode in enumerate(self.childNodes):\n\t\t\tif childNode is child:\n\t\t\t\tdel self.childNodes[i]\n\t\t\t\tchild.parentNode = None\n\t\t\t\treturn child\n\t\traise ValueError(child)", "response": "Removes a child from this element."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unlink(self):\n\t\tself.parentNode = None\n\t\tfor child in self.childNodes:\n\t\t\tchild.unlink()\n\t\tdel self.childNodes[:]", "response": "Unlink this element and all its children."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replaceChild(self, newchild, oldchild):\n\t\t# .index() would use compare-by-value, we want\n\t\t# compare-by-id because we want to find the exact object,\n\t\t# not something equivalent to it.\n\t\tfor i, childNode in enumerate(self.childNodes):\n\t\t\tif childNode is oldchild:\n\t\t\t\tself.childNodes[i].parentNode = None\n\t\t\t\tself.childNodes[i] = newchild\n\t\t\t\tnewchild.parentNode = self\n\t\t\t\tself._verifyChildren(i)\n\t\t\t\treturn newchild\n\t\traise ValueError(oldchild)", "response": "Replace an existing node with a new node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of elements below and including this element for which filter(element) returns True.", "response": "def getElements(self, filter):\n\t\t\"\"\"\n\t\tReturn a list of elements below and including this element\n\t\tfor which filter(element) returns True.\n\t\t\"\"\"\n\t\tl = reduce(lambda l, e: l + e.getElements(filter), self.childNodes, [])\n\t\tif filter(self):\n\t\t\tl.append(self)\n\t\treturn l"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend the given content to the element s pcdata.", "response": "def appendData(self, content):\n\t\t\"\"\"\n\t\tAdd characters to the element's pcdata.\n\t\t\"\"\"\n\t\tif self.pcdata is not None:\n\t\t\tself.pcdata += content\n\t\telse:\n\t\t\tself.pcdata = content"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the string for the element s start tag.", "response": "def start_tag(self, indent):\n\t\t\"\"\"\n\t\tGenerate the string for the element's start tag.\n\t\t\"\"\"\n\t\treturn u\"%s<%s%s/>\" % (indent, self.tagName, u\"\".join(u\" %s=\\\"%s\\\"\" % keyvalue for keyvalue in self.attributes.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite an element to a file.", "response": "def write(self, fileobj = sys.stdout, indent = u\"\"):\n\t\t\"\"\"\n\t\tRecursively write an element and it's children to a file.\n\t\t\"\"\"\n\t\tfileobj.write(self.start_tag(indent))\n\t\tfileobj.write(u\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef now(cls, Name = None):\n\t\tself = cls()\n\t\tif Name is not None:\n\t\t\tself.Name = Name\n\t\tself.pcdata = datetime.datetime.utcnow()\n\t\treturn self", "response": "Returns a new Time element initialized to the current UTCDate."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, fileobj = sys.stdout, xsl_file = None):\n\t\tfileobj.write(Header)\n\t\tfileobj.write(u\"\\n\")\n\t\tif xsl_file is not None:\n\t\t\tfileobj.write(u'<?xml-stylesheet type=\"text/xsl\" href=\"%s\" ?>\\n' % xsl_file)\n\t\tfor c in self.childNodes:\n\t\t\tif c.tagName not in self.validchildren:\n\t\t\t\traise ElementError(\"invalid child %s for %s\" % (c.tagName, self.tagName))\n\t\t\tc.write(fileobj)", "response": "Write the document to the file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_element(self, name, attrs):\n    if name.lower() == \"table\":\n      for attr in attrs.keys():\n        if attr.lower() == \"name\":\n          if self.__ignore_pat.search(attrs[attr]):\n            self.__in_table = 1", "response": "Callback for opening an XML element. Checks to see if we are about to start a table that matches the ignore pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_line(self, line):\n    self.__p.Parse(line)\n    if self.__in_table:\n      self.__silent = 1\n    if not self.__silent:\n      ret = line\n    else:\n      ret = \"\"\n    if not self.__in_table:\n      self.__silent = 0\n    return ret", "response": "Parse a line of LIGO_LW XML file into a sequence of strings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npinging the LDBD Server and return any message received back as a string.", "response": "def ping(self):\n    \"\"\"\n    Ping the LDBD Server and return any message received back as a string.\n\n    @return: message received (may be empty) from LDBD Server as a string\n    \"\"\"\n\n    msg = \"PING\\0\"\n    self.sfile.write(msg)\n\n    ret, output = self.__response__()\n    reply = str(output[0])\n\n    if ret:\n      msg = \"Error pinging server %d:%s\" % (ret, reply)\n      raise LDBDClientException, msg\n\n    return reply"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(self,sql):\n\n    msg = \"QUERY\\0\" + sql + \"\\0\"\n    self.sfile.write(msg)\n\n    ret, output = self.__response__()\n    reply = str(output[0])\n\n    if ret:\n      msg = \"Error executing query on server %d:%s\" % (ret, reply)\n      raise LDBDClientException, msg\n\n    return reply", "response": "Execute an SQL query on the server and fetch the resulting XML file and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert(self,xmltext):\n\n    msg = \"INSERT\\0\" + xmltext + \"\\0\"\n    self.sfile.write(msg)\n\n    ret, output = self.__response__()\n    reply = str(output[0])\n\n    if ret:\n      msg = \"Error executing insert on server %d:%s\" % (ret, reply)\n      raise LDBDClientException, msg\n\n    return reply", "response": "Insert the LIGO_LW metadata in the xmltext string into the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insertmap(self,xmltext,lfnpfn_dict):\n\n    pmsg = cPickle.dumps(lfnpfn_dict)\n\n    msg = \"INSERTMAP\\0\" + xmltext + \"\\0\" + pmsg + \"\\0\"\n    self.sfile.write(msg)\n\n    ret, output = self.__response__()\n    reply = str(output[0])\n\n    if ret:\n      msg = \"Error executing insert on server %d:%s\" % (ret, reply)\n      raise LDBDClientException, msg\n\n    return reply", "response": "Insert the LIGO_LW metadata in the xmltext string into the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dayOfWeek(year, month, day):\n    \"returns day of week: 0=Sun, 1=Mon, .., 6=Sat\"\n    hr = 12  #make sure you fall into right day, middle is save\n    t = time.mktime((year, month, day, hr, 0, 0.0, 0, 0, -1))\n    pyDow = time.localtime(t)[6]\n    gpsDow = (pyDow + 1) % 7\n    return gpsDow", "response": "returns day of week 0 = Sun 1 = Mon.. 6 = Sat"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gpsWeek(year, month, day):\n    \"returns (full) gpsWeek for given date (in UTC)\"\n    hr = 12  #make sure you fall into right day, middle is save\n    return gpsFromUTC(year, month, day, hr, 0, 0.0)[0]", "response": "returns ( full ) gpsWeek for given date ( in UTC"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning julian day = day since Jan 1 of year", "response": "def julianDay(year, month, day):\n    \"returns julian day=day since Jan 1 of year\"\n    hr = 12  #make sure you fall into right day, middle is save\n    t = time.mktime((year, month, day, hr, 0, 0.0, 0, 0, -1))\n    julDay = time.localtime(t)[7]\n    return julDay"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wtFromUTCpy(pyUTC, leapSecs=14):\n    ymdhms = ymdhmsFromPyUTC(pyUTC)\n    wSowDSoD = apply(gpsFromUTC, ymdhms + (leapSecs,))\n    return wSowDSoD[0:2]", "response": "convenience function that allows to use python UTC times and\n         returns only week and tow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gpsFromUTC(year, month, day, hour, min, sec, leapSecs=14):\n    secFract = sec % 1\n    epochTuple = gpsEpoch + (-1, -1, 0)\n    t0 = time.mktime(epochTuple)\n    t = time.mktime((year, month, day, hour, min, sec, -1, -1, 0)) \n    # Note: time.mktime strictly works in localtime and to yield UTC, it should be\n    #       corrected with time.timezone\n    #       However, since we use the difference, this correction is unnecessary.\n    # Warning:  trouble if daylight savings flag is set to -1 or 1 !!!\n    t = t + leapSecs   \n    tdiff = t - t0\n    gpsSOW = (tdiff % secsInWeek)  + secFract\n    gpsWeek = int(math.floor(tdiff/secsInWeek)) \n    gpsDay = int(math.floor(gpsSOW/secsInDay))\n    gpsSOD = (gpsSOW % secsInDay) \n    return (gpsWeek, gpsSOW, gpsDay, gpsSOD)", "response": "converts UTC to GPS week day secsOfDay gpsSOW"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert GPS week and seconds to UTC", "response": "def UTCFromGps(gpsWeek, SOW, leapSecs=14):\n    \"\"\"converts gps week and seconds to UTC\n\n    see comments of inverse function!\n\n    SOW = seconds of week\n    gpsWeek is the full number (not modulo 1024)\n    \"\"\"\n    secFract = SOW % 1\n    epochTuple = gpsEpoch + (-1, -1, 0) \n    t0 = time.mktime(epochTuple) - time.timezone  #mktime is localtime, correct for UTC\n    tdiff = (gpsWeek * secsInWeek) + SOW - leapSecs\n    t = t0 + tdiff\n    (year, month, day, hh, mm, ss, dayOfWeek, julianDay, daylightsaving) = time.gmtime(t)\n    #use gmtime since localtime does not allow to switch off daylighsavings correction!!!\n    return (year, month, day, hh, mm, ss + secFract)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GpsSecondsFromPyUTC( pyUTC, leapSecs=14 ):\n    t = t=gpsFromUTC(*ymdhmsFromPyUTC( pyUTC ))\n    return int(t[0] * 60 * 60 * 24 * 7 + t[1])", "response": "converts the python epoch to gps seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a meme from the list given by inptStr. Returns None if it can t find a meme", "response": "def findMeme(inptStr):\n    '''\n    inptStr may be a string of the following forms:\n    * 'text0 | text1'\n    * 'text0'\n\n    Returns None if it can't find find a meme from the list given above\n    '''\n\n    global meme_id_dict\n\n    testStr = inptStr\n    testStr.lower()\n\n    template_id = 0\n\n    '''\n    meme_id_dict[i] is of form:\n    [meme_tagline, meme_name, template_id]\n    '''\n    for i in range(len(meme_id_dict)):\n        test_words = testStr.strip('|.,?!').split(' ')\n\n        meme_words = meme_id_dict[i][0].split(' ')\n        common_words = len(list(set(meme_words).intersection(test_words)))\n\n        if (len(meme_words) >= 4 and common_words >= 3) or (len(meme_words) < 4 and common_words >= 1):\n            template_id = meme_id_dict[i][2]\n            return template_id\n\n    if template_id == 0:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping function for genMeme() and findMeme() imgParams may be a string of the following forms: * 'text0 | text1' * 'text0' * ' | text1' Fails gracefully when it can't find or generate a meme by returning an appropriate image url with the failure message on it.", "response": "def processMeme(imgParams):\n    '''\n    Wrapper function for genMeme() and findMeme()\n    imgParams may be a string of the following forms:\n    * 'text0 | text1'\n    * 'text0'\n    * ' | text1'\n\n    Fails gracefully when it can't find or generate a meme\n    by returning an appropriate image url with the failure\n    message on it.\n    '''\n\n    template_id = findMeme(imgParams)\n\n    if template_id is None:\n        print(\"Couldn't find a suitable match for meme :(\")\n        return meme_not_supported\n\n    # if template_id exists\n    imgParams = imgParams.split('|')\n\n    if len(imgParams) == 2 or len(imgParams) == 1:\n        text0 = imgParams[0]\n\n        if len(imgParams) == 2:\n            text1 = imgParams[1]    # Bottom text text1 exists\n        elif len(imgParams) == 1:\n            text1 = ''              # No bottom text\n\n        imgURL = genMeme(template_id, text0, text1)\n\n        if imgURL is None:          # Couldn't generate meme\n            print(\"Couldn't generate meme :(\")\n            return couldnt_create_meme\n        else:                       # Success!\n            # print(imgURL)\n            return imgURL\n\n    elif len(imgParams) > 2:\n        print(\"Too many lines of captions! Cannot create meme.\")\n        return too_many_lines\n\n    elif len(imgParams) < 1:        # No top text text0 exists\n        print(\"Too few lines of captions! Cannot create meme.\")\n        return too_few_lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef string_format_func(s):\n\treturn u\"\\\"%s\\\"\" % unicode(s).replace(u\"\\\\\", u\"\\\\\\\\\").replace(u\"\\\"\", u\"\\\\\\\"\")", "response": "This function is used internally to format string data for XML output to XML. It is used internally to format back - slashes and quotes and wraps the resulting string in a double quote."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mk_complex_format_func(fmt):\n\tfmt = fmt + u\"+i\" + fmt\n\tdef complex_format_func(z):\n\t\treturn fmt % (z.real, z.imag)\n\treturn complex_format_func", "response": "Function used internally to generate functions to format complex\n\tvalued data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield a set of fields for each item in iterable.", "response": "def fields(iterable, fields=None):\n    \"\"\"\n    Add a set of fields to each item in ``iterable``. The set of fields have a\n    key=value format. '@' are added to the front of each key.\n    \"\"\"\n    if not fields:\n        for item in iterable:\n            yield item\n\n    prepared_fields = _prepare_fields(fields)\n\n    for item in iterable:\n        yield _process_fields(item, prepared_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield the tags of each item in iterable.", "response": "def tag(iterable, tags=None, key='@tags'):\n    \"\"\"\n    Add tags to each dict or dict-like object in ``iterable``. Tags are added\n    to each dict with a key set by ``key``. If a key already exists under the\n    key given by ``key``, this function will attempt to ``.extend()``` it, but\n    will fall back to replacing it in the event of error.\n    \"\"\"\n    if not tags:\n        for item in iterable:\n            yield item\n\n    else:\n        for item in iterable:\n            yield _tag(item, tags, key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a request on the plugit api", "response": "def _request(self, uri, params=None, postParams=None, verb='GET'):\n        \"\"\"Execute a request on the plugit api\"\"\"\n        return getattr(requests, verb.lower())(self.url + uri, params=params, data=postParams, stream=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user(self, userPk):\n        r = self._request('user/' + str(userPk))\n        if r:\n            # Set base properties and copy data inside the user\n            u = User()\n            u.pk = u.id = userPk\n            u.__dict__.update(r.json())\n            return u\n        return None", "response": "Returns the user specified with the user s Pk or UUID"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list with all the labels the user is subscribed to", "response": "def get_subscription_labels(self, userPk):\n        \"\"\"Returns a list with all the labels the user is subscribed to\"\"\"\n        r = self._request('subscriptions/' + str(userPk))\n        if r:\n            s = r.json()\n            return s\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of pk for all orgas", "response": "def get_orgas(self):\n        \"\"\"Return the list of pk for all orgas\"\"\"\n\n        r = self._request('orgas/')\n        if not r:\n            return None\n\n        retour = []\n\n        for data in r.json()['data']:\n            o = Orga()\n            o.__dict__.update(data)\n            o.pk = o.id\n\n            retour.append(o)\n\n        return retour"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an organization speficied with orgaPk", "response": "def get_orga(self, orgaPk):\n        \"\"\"Return an organization speficied with orgaPk\"\"\"\n        r = self._request('orga/' + str(orgaPk))\n        if r:\n            # Set base properties and copy data inside the orga\n            o = Orga()\n            o.pk = o.id = orgaPk\n            o.__dict__.update(r.json())\n            return o\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of members in the project", "response": "def get_project_members(self):\n        \"\"\"Return the list of members in the project\"\"\"\n\n        r = self._request('members/')\n        if not r:\n            return None\n\n        retour = []\n\n        for data in r.json()['members']:\n            # Base properties\n            u = User()\n            u.__dict__.update(data)\n\n            retour.append(u)\n\n        return retour"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend an email using EBUio features.", "response": "def send_mail(self, sender, subject, recipients, message, response_id=None, html_message=False):\n        \"\"\"Send an email using EBUio features. If response_id is set, replies will be send back to the PlugIt server.\"\"\"\n\n        params = {\n            'sender': sender,\n            'subject': subject,\n            'dests': recipients,\n            'message': message,\n            'html_message': html_message,\n        }\n\n        if response_id:\n            params['response_id'] = response_id\n\n        return self._request('mail/', postParams=params, verb='POST')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a topic using EBUio features.", "response": "def forum_create_topic(self, subject, author, message, tags=\"\"):\n        \"\"\"Create a topic using EBUio features.\"\"\"\n\n        params = {'subject': subject, 'author': author, 'message': message, 'tags': tags}\n\n        return self._request('ebuio/forum/', postParams=params, verb='POST')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all forum topics with a specific tag for a specific user", "response": "def forum_topic_get_by_tag_for_user(self, tag=None, author=None):\n        \"\"\"Get all forum topics with a specific tag\"\"\"\n\n        if not tag:\n            return None\n\n        if author:\n            r = self._request('ebuio/forum/search/bytag/' + tag + '?u=' + author)\n        else:\n            r = self._request('ebuio/forum/search/bytag/' + tag)\n        if not r:\n            return None\n\n        retour = []\n\n        for data in r.json().get('data', []):\n            retour.append(data)\n\n        return retour"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getArraysByName(elem, name):\n\tname = StripArrayName(name)\n\treturn elem.getElements(lambda e: (e.tagName == ligolw.Array.tagName) and (e.Name == name))", "response": "getArraysByName - Get a list of arrays with name under elem.\n"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_array(name, array, dim_names = None):\n\t# Type must be set for .__init__();  easier to set Name afterwards\n\t# to take advantage of encoding handled by attribute proxy\n\tdoc = Array(Attributes({u\"Type\": ligolwtypes.FromNumPyType[str(array.dtype)]}))\n\tdoc.Name = name\n\tfor n, dim in enumerate(reversed(array.shape)):\n\t\tchild = ligolw.Dim()\n\t\tif dim_names is not None:\n\t\t\tchild.Name = dim_names[n]\n\t\tchild.pcdata = unicode(dim)\n\t\tdoc.appendChild(child)\n\tchild = ArrayStream(Attributes({u\"Type\": ArrayStream.Type.default, u\"Delimiter\": ArrayStream.Delimiter.default}))\n\tdoc.appendChild(child)\n\tdoc.array = array\n\treturn doc", "response": "Construct a LIGO Light Weight XML Array document subtree from a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the first array with the given name in the xmldoc. Raises ValueError if not exactly 1 such array is found.", "response": "def get_array(xmldoc, name):\n\t\"\"\"\n\tScan xmldoc for an array named name.  Raises ValueError if not\n\texactly 1 such array is found.\n\t\"\"\"\n\tarrays = getArraysByName(xmldoc, name)\n\tif len(arrays) != 1:\n\t\traise ValueError(\"document must contain exactly one %s array\" % StripArrayName(name))\n\treturn arrays[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmodifies a ContentHandler to cause it to use the Array and ArrayStream classes defined in this module when parsing XML documents.", "response": "def use_in(ContentHandler):\n\t\"\"\"\n\tModify ContentHandler, a sub-class of\n\tpycbc_glue.ligolw.LIGOLWContentHandler, to cause it to use the Array and\n\tArrayStream classes defined in this module when parsing XML\n\tdocuments.\n\n\tExample:\n\n\t>>> from pycbc_glue.ligolw import ligolw\n\t>>> class MyContentHandler(ligolw.LIGOLWContentHandler):\n\t...\tpass\n\t...\n\t>>> use_in(MyContentHandler)\n\t<class 'pycbc_glue.ligolw.array.MyContentHandler'>\n\t\"\"\"\n\tdef startStream(self, parent, attrs, __orig_startStream = ContentHandler.startStream):\n\t\tif parent.tagName == ligolw.Array.tagName:\n\t\t\treturn ArrayStream(attrs).config(parent)\n\t\treturn __orig_startStream(self, parent, attrs)\n\n\tdef startArray(self, parent, attrs):\n\t\treturn Array(attrs)\n\n\tContentHandler.startStream = startStream\n\tContentHandler.startArray = startArray\n\n\treturn ContentHandler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple of this array s dimensions. This is done by examining the Dim children of this array.", "response": "def get_shape(self):\n\t\t\"\"\"\n\t\tReturn a tuple of this array's dimensions.  This is done by\n\t\tquerying the Dim children.  Note that once it has been\n\t\tcreated, it is also possible to examine an Array object's\n\t\t.array attribute directly, and doing that is much faster.\n\t\t\"\"\"\n\t\treturn tuple(int(c.pcdata) for c in self.getElementsByTagName(ligolw.Dim.tagName))[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_conversejs_settings():\n    converse_settings = {\n        'CONVERSEJS_AUTO_LIST_ROOMS': False,\n        'CONVERSEJS_AUTO_SUBSCRIBE': False,\n        'CONVERSEJS_BOSH_SERVICE_URL': 'https://bind.opkode.im',\n        'CONVERSEJS_HIDE_MUC_SERVER': False,\n        'CONVERSEJS_PREBIND': True,\n        'CONVERSEJS_SHOW_CONTROLBOX_BY_DEFAULT': False,\n        'CONVERSEJS_XHR_USER_SEARCH': False,\n        'CONVERSEJS_DEBUG': settings.DEBUG,\n        'CONVERSEJS_SHOW_ONLY_ONLINE_USERS': False,\n        'CONVERSEJS_ALLOW_CONTACT_REQUESTS': True,\n    }\n\n    for key, value in converse_settings.items():\n        conf = getattr(settings, key, value)\n\n        if isinstance(conf, bool):\n            conf = unicode(conf).lower()\n\n        converse_settings[key] = conf\n\n    return converse_settings", "response": "This helper function returns all the configuration needed by django - conversejs frontend."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all files in dirname and all subdirectories whose seqnum is in the range starttime to endtime", "response": "def get_all_files_in_range(dirname, starttime, endtime, pad=64):\n    \"\"\"Returns all files in dirname and all its subdirectories whose\n    names indicate that they contain segments in the range starttime\n    to endtime\"\"\"\n    \n    ret = []\n\n    # Maybe the user just wants one file...\n    if os.path.isfile(dirname):\n        if re.match('.*-[0-9]*-[0-9]*\\.xml$', dirname):\n            return [dirname]\n        else:\n            return ret\n\n    first_four_start = starttime / 100000\n    first_four_end   = endtime   / 100000\n\n    for filename in os.listdir(dirname):\n        if re.match('.*-[0-9]{5}$', filename):\n            dirtime = int(filename[-5:])\n            if dirtime >= first_four_start and dirtime <= first_four_end:\n                ret += get_all_files_in_range(os.path.join(dirname,filename), starttime, endtime, pad=pad)\n        elif re.match('.*-[0-9]{4}$', filename):\n            dirtime = int(filename[-4:])\n            if dirtime >= first_four_start and dirtime <= first_four_end:\n                ret += get_all_files_in_range(os.path.join(dirname,filename), starttime, endtime, pad=pad)\n        elif re.match('.*-[0-9]*-[0-9]*\\.xml$', filename):\n            file_time = int(filename.split('-')[-2])\n            if file_time >= (starttime-pad) and file_time <= (endtime+pad):\n                ret.append(os.path.join(dirname,filename))\n        else:\n            # Keep recursing, we may be looking at directories of\n            # ifos, each of which has directories with times\n            ret += get_all_files_in_range(os.path.join(dirname,filename), starttime, endtime, pad=pad)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures that the DB represented by connection posses a segment table. If not creates one and prints a warning to stderr", "response": "def ensure_segment_table(connection):\n    \"\"\"Ensures that the DB represented by connection posses a segment table.\n    If not, creates one and prints a warning to stderr\"\"\"\n\n    count = connection.cursor().execute(\"SELECT count(*) FROM sqlite_master WHERE name='segment'\").fetchone()[0]\n\n    if count == 0:\n        print >>sys.stderr, \"WARNING: None of the loaded files contain a segment table\"\n        theClass  = lsctables.TableByName['segment']\n        statement = \"CREATE TABLE IF NOT EXISTS segment (\" + \", \".join(map(lambda key: \"%s %s\" % (key, ligolwtypes.ToSQLiteType[theClass.validcolumns[key]]), theClass.validcolumns)) + \")\"\n\n        connection.cursor().execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_segment_list(engine, gps_start_time, gps_end_time, ifo, segment_name, version = None, start_pad = 0, end_pad = 0):\n    if version is not None:\n        return build_segment_list_one(engine, gps_start_time, gps_end_time, ifo, segment_name, version, start_pad, end_pad)\n\n    # This needs more sophisticated logic, for the moment just return the latest\n    # available version\n    sql  = \"SELECT max(version) FROM segment_definer \"\n    sql += \"WHERE  segment_definer.ifos = '%s' \" % ifo\n    sql += \"AND   segment_definer.name = '%s' \" % segment_name\n\n    rows = engine.query(sql)\n    version = len(rows[0]) and rows[0][0] or 1\n\n    return build_segment_list_one(engine, gps_start_time, gps_end_time, ifo, segment_name, version, start_pad, end_pad)", "response": "Optains a list of segments for the given ifo name and version between the given time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_segment_list_one(engine, gps_start_time, gps_end_time, ifo, segment_name, version = None, start_pad = 0, end_pad = 0):\n    seg_result = segmentlist([])\n    sum_result = segmentlist([])\n\n    # Is there any way to get segment and segement summary in one query?\n    # Maybe some sort of outer join where we keep track of which segment\n    # summaries we've already seen.\n    sql = \"SELECT segment_summary.start_time, segment_summary.end_time \"\n    sql += \"FROM segment_definer, segment_summary \"\n    sql += \"WHERE segment_summary.segment_def_id = segment_definer.segment_def_id \"\n    sql += \"AND   segment_definer.ifos = '%s' \" % ifo\n    if engine.__class__ == query_engine.LdbdQueryEngine:\n       sql += \"AND segment_summary.segment_def_cdb = segment_definer.creator_db \"\n    sql += \"AND   segment_definer.name = '%s' \" % segment_name\n    sql += \"AND   segment_definer.version = %s \" % version\n    sql += \"AND NOT (%s > segment_summary.end_time OR segment_summary.start_time > %s)\" % (gps_start_time, gps_end_time)\n\n    rows = engine.query(sql)\n\n    for sum_start_time, sum_end_time in rows:\n        sum_start_time = (sum_start_time < gps_start_time) and gps_start_time or sum_start_time\n        sum_end_time = (sum_end_time > gps_end_time) and gps_end_time or sum_end_time\n\n        sum_result |= segmentlist([segment(sum_start_time, sum_end_time)])\n\n    # We can't use queries paramaterized with ? since the ldbd protocol doesn't support it...\n    sql = \"SELECT segment.start_time + %d, segment.end_time + %d \" % (start_pad, end_pad)\n    sql += \"FROM segment, segment_definer \"\n    sql += \"WHERE segment.segment_def_id = segment_definer.segment_def_id \"\n\n    if engine.__class__ == query_engine.LdbdQueryEngine:\n       sql += \"AND segment.segment_def_cdb = segment_definer.creator_db \"\n    sql += \"AND   segment_definer.ifos = '%s' \" % ifo\n    sql += \"AND   segment_definer.name = '%s' \" % segment_name\n    sql += \"AND   segment_definer.version = %s \" % version\n    sql += \"AND NOT (%s > segment.end_time OR segment.start_time > %s)\" % (gps_start_time, gps_end_time)\n\n    rows = engine.query(sql)\n    \n    for seg_start_time, seg_end_time in rows:\n        seg_start_time = (seg_start_time < gps_start_time) and gps_start_time or seg_start_time\n        seg_end_time = (seg_end_time > gps_end_time) and gps_end_time or seg_end_time\n\n        seg_result |= segmentlist([segment(seg_start_time, seg_end_time)])\n\n    engine.close()\n\n    return sum_result, seg_result", "response": "Builds a list of segments satisfying the given criteria"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a segment query on the specified segment list.", "response": "def run_query_segments(doc, proc_id, engine, gps_start_time, gps_end_time, included_segments_string, excluded_segments_string = None, write_segments = True, start_pad = 0, end_pad = 0):\n    \"\"\"Runs a segment query.  This was originally part of ligolw_query_segments, but now is also\n    used by ligolw_segments_from_cats.\n\n    The write_segments option is provided so callers can coalesce segments obtained over\n    sever invocations (as segments_from_cats does).\n    \"\"\"\n    \n    if write_segments:\n        all_ifos = {}\n\n        for ifo, segment_name, version in split_segment_ids(included_segments_string.split(',')):\n            all_ifos[ifo] = True\n\n\n        new_seg_def_id = add_to_segment_definer(doc, proc_id, ''.join(all_ifos.keys()), 'result', 0)\n        add_to_segment_summary(doc, proc_id, new_seg_def_id, [[gps_start_time, gps_end_time]])\n\n    result = segmentlist([])\n\n    for ifo, segment_name, version in split_segment_ids(included_segments_string.split(',')):\n        sum_segments, seg_segments = build_segment_list(engine, gps_start_time, gps_end_time, ifo, segment_name, version, start_pad, end_pad)\n        seg_def_id                 = add_to_segment_definer(doc, proc_id, ifo, segment_name, version)\n\n        add_to_segment_summary(doc, proc_id, seg_def_id, sum_segments)\n\n        # and accumulate segments \n        result |= seg_segments\n\n    # Excluded segments are not required\n    if excluded_segments_string:\n        excluded_segments = segmentlist([])\n\n        for ifo, segment_name, version in split_segment_ids(excluded_segments_string.split(',')):\n            sum_segments, seg_segments = build_segment_list(engine, gps_start_time, gps_end_time, ifo, segment_name, version)\n            excluded_segments |= seg_segments\n\n        result = result - excluded_segments\n\n    result.coalesce()\n    \n    # Add the segments\n    if write_segments:\n        add_to_segment(doc, proc_id, new_seg_def_id, result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an array of strings of the form ifo name and version returns an array of tuples where name and version are None.", "response": "def split_segment_ids(segment_ids):\n    \"\"\"Given an array of strings of the form ifo:name and\n    ifo:name:version, returns an array of tuples of the form (ifo,\n    name, version) where version may be None\"\"\"\n    \n    def split_segment_id(segment_id):\n        temp = segment_id.split(':')\n        if len(temp) == 2:\n            temp.append(None)\n        elif temp[2] == '*':\n            temp[2] = None\n        else:\n            temp[2] = int(temp[2])\n            \n        return temp\n\n    return map(split_segment_id, segment_ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path to the file or file on the local host.", "response": "def url2path(url):\n\t\"\"\"\n\tIf url identifies a file on the local host, return the path to the\n\tfile otherwise raise ValueError.\n\t\"\"\"\n\tscheme, host, path, nul, nul, nul = urlparse(url)\n\tif scheme.lower() in (\"\", \"file\") and host.lower() in (\"\", \"localhost\"):\n\t\treturn path\n\traise ValueError(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_input(urls, preserves, verbose = False):\n\tfor path in map(url2path, urls):\n\t\tif any(os.path.samefile(path, preserve) for preserve in preserves):\n\t\t\tcontinue\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"removing \\\"%s\\\" ...\" % path\n\t\ttry:\n\t\t\tos.remove(path)\n\t\texcept:\n\t\t\tpass", "response": "Remove all files identified by the URLs in urls except those in preserves."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign new IDs to all rows in all LSC tables in the document.", "response": "def reassign_ids(doc, verbose = False):\n\t\"\"\"\n\tAssign new IDs to all rows in all LSC tables in doc so that there\n\tare no collisions when the LIGO_LW elements are merged.\n\t\"\"\"\n\t# Can't simply run reassign_ids() on doc because we need to\n\t# construct a fresh old --> new mapping within each LIGO_LW block.\n\tfor n, elem in enumerate(doc.childNodes):\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"reassigning row IDs: %.1f%%\\r\" % (100.0 * (n + 1) / len(doc.childNodes)),\n\t\tif elem.tagName == ligolw.LIGO_LW.tagName:\n\t\t\ttable.reassign_ids(elem)\n\tif verbose:\n\t\tprint >>sys.stderr, \"reassigning row IDs: 100.0%\"\n\treturn doc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_ligolws(elem):\n\tligolws = [child for child in elem.childNodes if child.tagName == ligolw.LIGO_LW.tagName]\n\tif ligolws:\n\t\tdest = ligolws.pop(0)\n\t\tfor src in ligolws:\n\t\t\t# copy children;  LIGO_LW elements have no attributes\n\t\t\tmap(dest.appendChild, src.childNodes)\n\t\t\t# unlink from parent\n\t\t\tif src.parentNode is not None:\n\t\t\t\tsrc.parentNode.removeChild(src)\n\treturn elem", "response": "Merge all LIGO_LW elements that are immediate children of elem by appending their children to the first."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_table_cols(a, b):\n\treturn cmp(sorted((col.Name, col.Type) for col in a.getElementsByTagName(ligolw.Column.tagName)), sorted((col.Name, col.Type) for col in b.getElementsByTagName(ligolw.Column.tagName)))", "response": "Compare the columns of two tables a and b."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_compatible_tables(elem):\n\tfor name in lsctables.TableByName.keys():\n\t\ttables = table.getTablesByName(elem, name)\n\t\tif tables:\n\t\t\tdest = tables.pop(0)\n\t\t\tfor src in tables:\n\t\t\t\tif src.Name != dest.Name:\n\t\t\t\t\t# src and dest have different names\n\t\t\t\t\tcontinue\n\t\t\t\t# src and dest have the same names\n\t\t\t\tif compare_table_cols(dest, src):\n\t\t\t\t\t# but they have different columns\n\t\t\t\t\traise ValueError(\"document contains %s tables with incompatible columns\" % dest.Name)\n\t\t\t\t# and the have the same columns\n\t\t\t\t# copy src rows to dest\n\t\t\t\tfor row in src:\n\t\t\t\t\tdest.append(row)\n\t\t\t\t# unlink src from parent\n\t\t\t\tif src.parentNode is not None:\n\t\t\t\t\tsrc.parentNode.removeChild(src)\n\treturn elem", "response": "This function will find all tables whose structure is\n\tdescribed in lsctables and merge compatible ones into one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ligolw_add(xmldoc, urls, non_lsc_tables_ok = False, verbose = False, contenthandler = DefaultContentHandler):\n\t# Input\n\tfor n, url in enumerate(urls):\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"%d/%d:\" % (n + 1, len(urls)),\n\t\tutils.load_url(url, verbose = verbose, xmldoc = xmldoc, contenthandler = contenthandler)\n\n\t# ID reassignment\n\tif not non_lsc_tables_ok and lsctables.HasNonLSCTables(xmldoc):\n\t\traise ValueError(\"non-LSC tables found.  Use --non-lsc-tables-ok to force\")\n\treassign_ids(xmldoc, verbose = verbose)\n\n\t# Document merge\n\tif verbose:\n\t\tprint >>sys.stderr, \"merging elements ...\"\n\tmerge_ligolws(xmldoc)\n\tmerge_compatible_tables(xmldoc)\n\n\treturn xmldoc", "response": "An implementation of the LIGO LW add algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        if self._skip_delims:\n            delims = readline.get_completer_delims()\n            for delim in self._skip_delims:\n                delims = delims.replace(delim, '')\n            readline.set_completer_delims(delims)\n\n        readline.parse_and_bind(\"tab: complete\")\n        readline.set_completer(self._completer.complete)\n\n        if self._history_file:\n            # Ensure history file exists\n            if not os.path.isfile(self._history_file):\n                open(self._history_file, 'w').close()\n\n            readline.read_history_file(self._history_file)\n\n        self._running = True\n        try:\n            while self._running:\n                try:\n                    command = input(self._format_prompt())\n                    if command:\n                        result = self.execute(*shlex.split(command))\n                        if result:\n                            print(result)\n                except CLIException as exc:\n                    print(exc)\n                except (KeyboardInterrupt, EOFError):\n                    self._running = False\n                    print()\n                except Exception as exc:\n                    if self._verbose:\n                        traceback.print_exc()\n                    else:\n                        print(exc)\n        finally:\n            if self._history_file:\n                readline.write_history_file(self._history_file)", "response": "Loops and executes commands in interactive mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, *args):\n        command, kwargs = self.parse(*args)\n        return self._commands.execute(command, **kwargs)", "response": "Executes single command and returns result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gcommer_claim(address=None):\n    if not address:\n        # get token for any world\n        # this is only useful for testing,\n        # because that is exactly what m.agar.io does\n        url = 'http://at.gcommer.com/status'\n        text = urllib.request.urlopen(url).read().decode()\n        j = json.loads(text)\n        for address, num in j['status'].items():\n            if num > 0:\n                break  # address is now one of the listed servers with tokens\n    url = 'http://at.gcommer.com/claim?server=%s' % address\n    text = urllib.request.urlopen(url).read().decode()\n    j = json.loads(text)\n    token = j['token']\n    return address, token", "response": "Try to get a token for this server address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gcommer_donate(address, token, *_):\n    token = urllib.request.quote(token)\n    url = 'http://at.gcommer.com/donate?server=%s&token=%s' % (address, token)\n    response = urllib.request.urlopen(url).read().decode()\n    return json.loads(response)['msg']", "response": "This function is used to donate a token for this server address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a daemon thread that requests and donates a token every interval seconds.", "response": "def gcommer_donate_threaded(interval=5, region='EU-London', mode=None):\n    \"\"\"\n    Run a daemon thread that requests and\n    donates a token every `interval` seconds.\n    \"\"\"\n    def donate_thread():\n        while 1:\n            gcommer_donate(*find_server(region, mode))\n            time.sleep(interval)\n\n    Thread(target=donate_thread, daemon=True).start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, src_project=None, path_to_zip_file=None):\n        if path_to_zip_file:\n            code = self.set_artefact_path(path_to_zip_file)\n        elif not self.config[\"deploy\"].get(\"deploy_file\", False):\n            code = self.build_artefact(src_project)\n        else:\n            code = self.set_artefact_path(self.config[\"deploy\"].get(\"deploy_file\"))\n\n        self.set_artefact(code=code)\n        # Reload conf because each lambda conf need to read again the global conf\n        self.config.reload_conf()\n\n        self.deploy()\n\n        return True", "response": "Run deploy the lambdas defined in our project."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the route to the local file to deploy", "response": "def set_artefact_path(self, path_to_zip_file):\n        \"\"\"\n        Set the route to the local file to deploy\n        :param path_to_zip_file: \n        :return: \n        \"\"\"\n        self.config[\"deploy\"][\"deploy_file\"] = path_to_zip_file\n        return {'ZipFile': self.build.read(self.config[\"deploy\"][\"deploy_file\"])}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the artefact file.", "response": "def build_artefact(self, src_project=None):\n        \"\"\"Run deploy the lambdas defined in our project.\n        Steps:\n        * Build Artefact\n        * Read file or deploy to S3. It's defined in config[\"deploy\"][\"deploy_method\"]\n\n        :param src_project: str. Name of the folder or path of the project where our code lives\n        :return: bool\n        \"\"\"\n        path_to_zip_file = self.build.run(src_project or self.config.get_projectdir())\n        self.set_artefact_path(path_to_zip_file)\n\n        deploy_method = self.config[\"deploy\"][\"deploy_method\"]\n\n        if deploy_method == \"S3\":\n            deploy_bucket = self.config[\"deploy\"][\"deploy_bucket\"]\n            bucket = self.awss3.Bucket(deploy_bucket)\n            try:\n                self.awss3.meta.client.head_bucket(Bucket=deploy_bucket)\n            except ClientError as e:\n                if e.response['Error']['Code'] == \"404\" or e.response['Error']['Code'] == \"NoSuchBucket\":\n                    region = self.config.get(\"aws_credentials\", {}).get(\"region\", None)\n\n                    logger.info(\n                        \"Bucket not exist. Creating new one with name {} in region {}\".format(deploy_bucket, region))\n                    bucket_conf = {}\n                    if region:\n                        bucket_conf = {\"CreateBucketConfiguration\": {'LocationConstraint': region}}\n                    bucket.wait_until_not_exists()\n                    bucket.create(**bucket_conf)\n                    bucket.wait_until_exists()\n\n                else:\n                    # TODO: handle other errors there\n                    pass\n            s3_keyfile = self.config[\"deploy\"][\"deploy_file\"].split(os.path.sep)[-1]\n            bucket.put_object(\n                Key=s3_keyfile,\n                Body=self.build.read(self.config[\"deploy\"][\"deploy_file\"])\n\n            )\n            code = {'S3Bucket': deploy_bucket, 'S3Key': s3_keyfile, }\n\n        elif deploy_method == \"FILE\":\n            code = {'ZipFile': self.build.read(self.config[\"deploy\"][\"deploy_file\"])}\n        else:\n            raise Exception(\"No deploy_method in config\")\n\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading code to AWS Lambda. To use this method, first, must set the zip file with code with `self.set_artefact(code=code)`. Check all lambdas in our config file or the functions passed in command line and exist in our config file. If the function is upload correctly, update/create versions, alias and triggers :return: True", "response": "def deploy(self):\n        \"\"\"Upload code to AWS Lambda. To use this method, first, must set the zip file with code with\n         `self.set_artefact(code=code)`. Check all lambdas in our config file or the functions passed in command line\n         and exist in our config file. If the function is upload correctly, update/create versions, alias and\n         triggers\n\n        :return: True\n        \"\"\"\n        lambdas_deployed = []\n        for lambda_funcion in self.config.get_lambdas():\n            start_deploy = not len(self.lambdas_to_deploy) or \\\n                           lambda_funcion[\"FunctionNameOrigin\"] in self.lambdas_to_deploy\n\n            if start_deploy:\n                lambdas_deployed.append(lambda_funcion[\"FunctionName\"])\n                conf = lambda_funcion.get_deploy_conf()\n                response = self.remote_get_lambda(**conf)\n                if response:\n                    remote_conf = response[\"Configuration\"]\n\n                    # TODO: Diferences sometimes not return all values, check it!\n                    logger.info(\"Diferences:\")\n                    diffkeys = [k for k in remote_conf if\n                                conf.get(k, False) != remote_conf.get(k, True) and k not in ['Code', ]]\n                    for k in diffkeys:\n                        logger.info((k, ':', conf.get(k, \"\"), '->', remote_conf.get(k, \"\")))\n\n                    logger.info(\"START to update funcion {}\".format(conf[\"FunctionName\"]))\n                    self.remote_update_conf_lambada(**conf)\n                    result = self.remote_update_code_lambada(**conf)\n                    logger.debug(\"Funcion {} updated {}\".format(conf[\"FunctionName\"], result))\n\n                else:\n                    logger.info(\"START to create funcion {}\".format(lambda_funcion[\"FunctionName\"]))\n                    result = self.remote_create_lambada(**conf)\n                    logger.debug(\"Funcion {} created {}\".format(conf[\"FunctionName\"], result))\n\n                if self.is_client_result_ok(result):\n\n                    # Check and publish version\n                    version = \"LATEST\"\n                    if self.config[\"deploy\"].get(\"use_version\", False):\n                        logger.info(\"Publish new version of {} with conf {}\".format(\n                            lambda_funcion[\"FunctionName\"],\n                            json.dumps(conf, indent=4, sort_keys=True)\n                        ))\n                        result = self.remote_publish_version(**conf)\n                        version = result[\"Version\"]\n                        logger.info(\"Published version {}: {}\".format(\n                            version,\n                            json.dumps(result, indent=4, sort_keys=True)\n                        ))\n\n                    # Check and publish alias\n                    if self.config[\"deploy\"].get(\"use_alias\", False):\n                        alias_conf = {\n                            \"FunctionName\": conf[\"FunctionName\"],\n                            \"Description\": conf[\"Description\"],\n                            \"FunctionVersion\": version,\n                        }\n                        if self.config.get_environment():\n                            alias_conf.update({\"Name\": self.config.get_environment()})\n                        else:\n                            alias_conf.update({\"Name\": conf[\"FunctionName\"]})\n\n                        logger.info(\"Update alias of {} with conf {}\".format(\n                            lambda_funcion[\"FunctionName\"],\n                            json.dumps(alias_conf, indent=4, sort_keys=True)\n                        ))\n                        result = self.remote_update_alias(**alias_conf)\n                        logger.info(\"Updated alias {}: {}\".format(conf[\"FunctionName\"],\n                                                                  json.dumps(result, indent=4, sort_keys=True)\n                                                                  ))\n\n                    # Check and publish triggers\n                    logger.info(\"Updating Triggers for fuction {}\".format(lambda_funcion[\"FunctionName\"]))\n                    if lambda_funcion.get(\"triggers\", False):\n                        for trigger in lambda_funcion[\"triggers\"].keys():\n                            trigger_object = get_trigger(trigger, lambda_funcion, result[\"FunctionArn\"])\n                            trigger_object.put()\n\n        if lambdas_deployed:\n            logger.info(\"Deploy finished. Created/updated lambdas {}\".format(\", \".join(lambdas_deployed)))\n        else:\n            logger.info(\"No lambdas found to deploy\")\n\n        # TODO: check errors to return correct value\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_regex(self):\n    return re.compile(\"|\".join(map(re.escape, self.keys())))", "response": "Build a re object based on keys in the current dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the unique id for the given string.", "response": "def lookup(self,istring):\n    \"\"\"\n    istring = the ilwd:char string corresponding to a unique id\n    \"\"\"\n    try:\n      return self.uqids[istring]\n    except KeyError:\n      curs = self.curs\n      curs.execute('VALUES BLOB(GENERATE_UNIQUE())')\n      self.uqids[istring] = curs.fetchone()[0]\n      return self.uqids[istring]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a parsed lstring by stripping out whitespace and double quotes at the beginning or end of the string.", "response": "def __lstring(self,lstr):\n    \"\"\"\n    Returns a parsed lstring by stripping out and instances of\n    the escaped delimiter. Sometimes the raw lstring has whitespace\n    and a double quote at the beginning or end. If present, these\n    are removed.\n    \"\"\"\n    lstr = self.llsrx.sub('',lstr.encode('ascii'))\n    lstr = self.rlsrx.sub('',lstr)\n    lstr = self.xmltostr.xlat(lstr)\n    lstr = self.dlmrx.sub(',',lstr)\n    return lstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an XML document into a form read for insertion into the database.", "response": "def parse(self,xml):\n    \"\"\"\n    Parses an XML document into a form read for insertion into the database\n\n    xml = the xml document to be parsed\n    \"\"\"\n    if not self.xmlparser:\n      raise LIGOLwParseError, \"pyRXP parser not initialized\"\n    if not self.lwtparser:\n      raise LIGOLwParseError, \"LIGO_LW tuple parser not initialized\"\n    xml = \"\".join([x.strip() for x in xml.split('\\n')])\n    ligolwtup = self.xmlparser(xml)\n    if self.curs:\n      self.lwtparser.unique = UniqueIds(self.curs)\n    self.table = self.lwtparser.parsetuple(ligolwtup)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_lfn(self,lfn):\n    if len(self.table['process']['stream']) > 1:\n      msg = \"cannot add lfn to table with more than one process\"\n      raise LIGOLwParseError, msg\n    # get the process_id from the process table\n    pid_col = self.table['process']['orderedcol'].index('process_id')\n    pid = self.table['process']['stream'][0][pid_col]\n    try:\n      self.table['lfn']['stream'].append((pid,lfn))\n    except KeyError:\n      self.table['lfn'] = {\n        'pos' : 0,\n        'column' : {'process_id' : 'ilwd:char', 'name' : 'lstring'},\n        'stream' : [(pid, lfn)],\n        'query' : '',\n        'orderedcol' : ['process_id', 'name' ]\n        }", "response": "Add a LFN table to a parsed LIGO_LW XML document."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_dn(self,dn):\n    try:\n      domain_col = self.table['process']['orderedcol'].index('domain')\n      for row_idx in range(len(self.table['process']['stream'])):\n        row_list = list(self.table['process']['stream'][row_idx])\n        row_list[domain_col] = dn\n        self.table['process']['stream'][row_idx] = tuple(row_list)\n    except ValueError:\n      self.table['process']['column']['domain'] = 'lstring'\n      self.table['process']['orderedcol'].append('domain')\n      for row_idx in range(len(self.table['process']['stream'])):\n        row_list = list(self.table['process']['stream'][row_idx])\n        row_list.append(dn)\n        self.table['process']['stream'][row_idx] = tuple(row_list)", "response": "Set the DN of the current process table to be added to the DN"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert(self):\n    if not self.curs:\n      raise LIGOLwDBError, \"Database connection not initalized\"\n    if len(self.table) == 0:\n      raise LIGOLwDBError, 'attempt to insert empty table'\n    for tab in self.table.keys():\n      # find and add any missing unique ids\n      generate = []\n      missingcols = [k for k in self.ldb.uniqueids[tab] \n        if k not in self.table[tab]['column']]\n      for m in missingcols:\n        generate.append(',BLOB(GENERATE_UNIQUE())')\n        self.table[tab]['orderedcol'].append(m)\n      # and construct the sql query\n      self.table[tab]['query'] = ' '.join( \n        ['INSERT INTO', tab, '(', ','.join(self.table[tab]['orderedcol']), \n        ') VALUES (', ','.join(['?' for x in self.table[tab]['column']]) , \n        ''.join(generate), ')'])\n    for tabtup in self.ldb.tables:\n      tab = tabtup[0].lower()\n      try:\n        try: \n          self.curs.executemany(self.table[tab]['query'],\n            self.table[tab]['stream'])\n          rowcount = self.curs.rowcount\n        except DB2.Error, e:\n          self.curs.execute('rollback')\n          msg = e[2] \n          msg += self.xml() + '\\n' \n          msg += str(self.table[tab]['query']) + '\\n' \n          msg += str(self.table[tab]['stream']) + '\\n'\n          raise LIGOLwDBError, msg\n        except DB2.Warning, e:\n          self.curs.execute('rollback')\n          raise LIGOLwDBError, e[2]\n        #except Exception, e:\n        #  self.curs.execute('rollback')\n        #  raise LIGOLwDBError, e[2]\n      except KeyError:\n        pass\n    self.curs.execute('commit')\n    return rowcount", "response": "Insert the object into the database"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes an SQL SELECT statement and stuff the results into a dictionary.", "response": "def select(self,sql):\n    \"\"\"\n    Execute an SQL select statement and stuff the results into a\n    dictionary.\n\n    sql = the (case sensitve) SQL statment to execute\n    \"\"\"\n    if not self.curs:\n      raise LIGOLwDBError, \"Database connection not initalized\"\n    if len(self.table) != 0:\n      raise LIGOLwDBError, 'attempt to fill non-empty table from database'\n    ligolw = ''\n    self.table = {}\n    sqltypes = {\n      -2 : 'ilwd:char_u',\n      1 : 'lstring',\n      3 : 'real_8',\n      4  : 'int_4s',\n      5 : 'int_2s',\n      7 : 'real_4',\n      8 : 'real_8',\n      12 : 'lstring',\n      93 : 'lstring', \n      }\n    try:\n      tab = re.compile(r'[Ff][Rr][Oo][Mm]\\s+([A-Za-z0-0_]+)([,\\s]+|$)').search(sql).group(1)\n    except AttributeError:\n      raise LIGOLwDBError, 'could not find table name in query ' + str(sql)\n    self.table[tab] = {\n      'pos' : 0,\n      'column' : {},\n      'stream' : (), \n      'query' : sql\n      }\n    try:\n      self.curs.execute(sql)\n    except DB2.Error, e:\n      raise LIGOLwDBError, e[2]\n    desc = self.curs.description\n    for col,typ,disp,intsz,prec,sca,nul in desc:\n      try:\n        self.table[tab]['column'][col] = sqltypes[typ]\n      except KeyError:\n        raise LIGOLwDBError, 'unknown type returned by database ' + str(typ)\n      self.table[tab].setdefault('orderedcol',[]).append(col)\n\n    try:\n      self.table[tab]['stream'] = self.curs.fetchall()\n    except DB2.Error, e:\n      raise LIGOLwDBError, e[2]\n\n    return len(self.table[tab]['stream'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xml(self, ilwdchar_to_hex = True):\n    if len(self.table) == 0:\n      raise LIGOLwDBError, 'attempt to convert empty table to xml'\n    ligolw = \"\"\"\\\n<?xml version='1.0' encoding='utf-8' ?>\n<?xml-stylesheet type=\"text/xsl\" href=\"ligolw.xsl\"?>\n<!DOCTYPE LIGO_LW SYSTEM \"http://ldas-sw.ligo.caltech.edu/doc/ligolwAPI/html/ligolw_dtd.txt\">\n<LIGO_LW>\n\"\"\"\n\n    for tab in self.table.keys():\n      try:\n        ligolw += '   <Comment>'+self.strtoxml.xlat(self.table[tab]['query'])+'</Comment>\\n'\n      except KeyError:\n        pass\n      ligolw += '   <Table Name=\"'+tab+':table\">\\n'\n      for col in self.table[tab]['orderedcol']:\n        ligolw +='      <Column Name=\"'+tab.lower()+':'+col.lower()+'\" Type=\"'+self.table[tab]['column'][col].lower()+'\"/>\\n'\n      ligolw += '      <Stream Name=\"'+tab.lower()+':table\" Type=\"Local\" Delimiter=\",\">\\n'\n      stridx = 0\n      ligolw += '      '\n      for tup in self.table[tab]['stream']:\n        if stridx != 0:\n          ligolw += ',\\n      '\n        colidx = 0\n        for tupi in tup:\n          if tupi is not None:\n            coltype = self.table[tab]['column'][self.table[tab]['orderedcol'][colidx]]\n            if re.match(r'\\Ailwd:char_u\\Z',coltype):\n              ligolw += '\"'\n              for ch in str(tupi):\n                # NOTE: escape the backslash in the ilwd:char_u octal string\n                ligolw += '\\\\\\\\%.3o' % (ord(ch))\n              ligolw += '\"'\n            elif re.match(r'\\Ailwd:char\\Z',coltype):\n              if ilwdchar_to_hex is True:\n                # encode in DB2-style hex (e.g., \"x'deadbeef'\")\n                ligolw += '\"x\\''\n                for ch in str(tupi):\n                  ligolw += \"%02x\" % ord(ch)\n                ligolw += '\\'\"'\n              else:\n                ligolw += '\"' + str(tupi) + '\"'\n            elif re.match(r'\\Alstring\\Z',coltype):\n              # this santizes the contents of tupi in several ways: \n              # strtoxml.xlat escapes any double-quote and\n              # backslash chars (with a preceding blackslash); and\n              # then replaces <>& chars with their html\n              # code equivalents\n              # NOTE: string_format_func was removed so the enclosing \"\"\n              # chars need to be added ourselves\n              ligolw += '\"'+self.strtoxml.xlat(tupi)+'\"' \n            elif re.match(r'\\Areal_4\\Z',coltype):\n              ligolw += '%13.7e' % tupi\n            elif re.match(r'\\Areal_8\\Z',coltype):\n              ligolw += '%22.16e' % tupi\n            else:\n              ligolw += str(tupi)\n          else:\n            ligolw += ''\n          if colidx < (len(self.table[tab]['column']) - 1):\n            ligolw += ','\n          colidx += 1\n        stridx += 1\n      ligolw += '\\n      </Stream>\\n'\n      ligolw += '   </Table>\\n'\n    ligolw += '</LIGO_LW>'\n\n    return ligolw", "response": "Convert a dictionary to LIGO lightweight XML"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(url):\n\n    config = {}\n\n    url = urlparse.urlparse(url)\n\n    # Remove query strings.\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n\n    if url.scheme in SCHEMES:\n        config[\"ENGINE\"] = SCHEMES[url.scheme]\n\n    if url.scheme in USES_URL:\n        config[\"URL\"] = urlparse.urlunparse((\"http\",) + url[1:])\n\n    if url.scheme in USES_INDEX:\n        if path.endswith(\"/\"):\n            path = path[:-1]\n\n        split = path.rsplit(\"/\", 1)\n\n        if len(split) > 1:\n            path = split[:-1]\n            index = split[-1]\n        else:\n            path = \"\"\n            index = split[0]\n\n        config.update({\n            \"URL\": urlparse.urlunparse((\"http\",) + url[1:2] + (path,) + url[3:]),\n            \"INDEX_NAME\": index,\n        })\n\n    if url.scheme in USES_PATH:\n        config.update({\n            \"PATH\": path,\n        })\n\n    return config", "response": "Parses a search URL and returns a dictionary of configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds new item to the model.", "response": "def add(self, name, priority=3, comment=\"\", parent=\"\"):\n        \"\"\"Adds new item to the model.\n\n        Name argument may contain (ref:) syntax, which will be\n        stripped down as needed.\n\n        :parent: should have a form \"<itemref>.<subitemref...>\" (e.g. \"1.1\").\n\n        :name: Name (with refs).\n        :priority: Item's priority.\n        :comment: Comment.\n        :parent: Item's parent (\"\" for top-level item).\n\n        \"\"\"\n        item = [name, priority, comment, False, []]\n        data = self.data\n        for c in self._split(parent):\n            data = data[int(c) - 1][4]\n        data.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nedit the entry in the internal data structure.", "response": "def edit(\n        self, index, name=None, priority=None,\n        comment=None, done=None, parent=None\n    ):\n        \"\"\"Modifies :index: to specified data.\n\n        Every argument, which is not None, will get changed.\n\n        If parent is not None, the item will get reparented.\n        Use parent=-1 or parent='' for reparenting to top-level.\n\n        :index: Index of the item to edit.\n        :name: New name.\n        :priority: New priority.\n        :comment: New comment.\n        :done: Done mark.\n        :parent: New parent.\n\n        \"\"\"\n        if parent == -1:\n            parent = ''\n        parent = self._split(parent)\n        index = self._split(index)\n        item = self.data\n        for j, c in enumerate(index):\n            item = item[int(c) - 1]\n            if j + 1 != len(index):\n                item = item[4]\n        if name is not None:\n            item[0] = name\n        if priority is not None:\n            item[1] = priority\n        if comment is not None:\n            item[2] = comment\n        if done is not None:\n            item[3] = done\n        if parent is not None and parent != index[:-1]:\n            parentitem = self.data\n            for c in parent:\n                parentitem = parentitem[int(c) - 1][4]\n            parentitem.append(item)\n            parent = index[:-1]\n            parentitem = self.data\n            for c in parent:\n                parentitem = parentitem[int(c) - 1][4]\n            parentitem.remove(item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove specified item from the model.", "response": "def remove(self, index):\n        \"\"\"Removes specified item from the model.\n\n        :index: Should have a form \"<itemref>.<subitemref...>\" (e.g. \"1.1\").\n\n        :index: Item's index.\n\n        \"\"\"\n        data = self.data\n        index = self._split(index)\n        for j, c in enumerate(index):\n            i = int(c) - 1\n            if j + 1 == len(index):\n                try:\n                    del data[i]\n                except IndexError:\n                    raise NoItemError('.'.join(index))\n            else:\n                data = data[i][4]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists(self, index):\n        data = self.data\n        try:\n            for c in self._split(index):\n                i = int(c) - 1\n                data = data[i][4]\n        except Exception:\n            return False\n        return True", "response": "Checks whether the index is in the Model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets data values for specified index.", "response": "def get(self, index):\n        \"\"\"Gets data values for specified :index:.\n\n        :index: Index for which to get data.\n        :returns: A list in form\n        [parent, name, priority, comment, done, children].\n\n        \"\"\"\n        data = self.data\n        index2 = self._split(index)\n        for c in index2[:-1]:\n            i = int(c) - 1\n            data = data[i][4]\n        return [index[:-2] or \"\"] + data[int(index[-1]) - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _modifyInternal(self, *, sort=None, purge=False, done=None):\n        sortAll, sortLevels = sort is not None and sort or ([], {})\n        doneAll, doneLevels = done is not None and done or ([], {})\n\n        def _mark(v, i):\n            if done is None:\n                return v[:4]\n\n            def _mark_(index, regexp, du):\n                if du is None:\n                    return v[:4]\n                if index is None:\n                    for v_ in v[:3]:\n                        if regexp is None or re.match(regexp, str(v_)):\n                            return v[:3] + [du]\n                    return v[:4]\n                if regexp is None or re.match(regexp, str(v[index])):\n                    return v[:3] + [du]\n            try:\n                for doneLevel in doneLevels[i]:\n                    result = _mark_(*doneLevel)\n                if result is not None:\n                    return result\n            except KeyError:\n                pass\n            for doneAll_ in doneAll:\n                result = _mark_(*doneAll_)\n            if result is None:\n                return v[:4]\n            return result\n\n        def _modify(submodel, i):\n            _new = list()\n            for v in submodel:\n                if purge:\n                    if not v[3]:\n                        _new.append(_mark(v, i) + [_modify(v[4], i + 1)])\n                else:\n                    _new.append(_mark(v, i) + [_modify(v[4], i + 1)])\n            levels = sortLevels.get(i) or sortLevels.get(str(i))\n            for index, reverse in levels or sortAll:\n                _new = sorted(_new, key=lambda e: e[index], reverse=reverse)\n            return _new\n        return _modify(self.data, 1)", "response": "Creates a whole new database from existing items based on given modifiers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modify(self, *, sort=None, purge=False, done=None):\n        return self._modifyInternal(sort=sort, purge=purge, done=done)", "response": "Calls _modifyInternal after loading the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modifyInPlace(self, *, sort=None, purge=False, done=None):\n        self.data = self.modify(sort=sort, purge=purge, done=done)", "response": "Like Model. modify but changes existing database instead of\n        returning a new one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the options of the current instance.", "response": "def setOptions(self, glob=False, **kwargs):\n        \"\"\"Set option(s).\n\n        :glob: If True, stores specified options globally.\n        :kwargs: Dictionary of options and values to set.\n\n        \"\"\"\n        if glob:\n            self.globalOptions.update(kwargs)\n        else:\n            self.options.update(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_line(self, string):\n        self.code_strings.append(string)\n        code = ''\n        if len(self.code_strings) == 1:\n            code = '(setv result ' + self.code_strings[0] + ')'\n        if len(self.code_strings) > 1:\n            code = '(setv result (and ' + ' '.join(self.code_strings) + '))'\n        self._compiled_ast_and_expr = self.__compile_code(code_string=code)", "response": "Adds a line to the LISP code to execute"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the variables name to the namespace of the LISP code", "response": "def add_graph_to_namespace(self, graph):\n        \"\"\"\n        Adds the variables name to the namespace of the local LISP code\n\n        :param graph: the graph to add to the namespace\n        :return: None\n        \"\"\"\n        for node in graph.vs:\n            attributes = node.attributes()\n            self.namespace[node['name']] = attributes\n        for node in graph.es:\n            attributes = node.attributes()\n            self.namespace[node['name']] = attributes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the code and returns the result of the code", "response": "def execute(self, vertices_substitution_dict={}):\n        \"\"\"\n        Executes the code\n\n        :param vertices_substitution_dict: aliases of the variables in the code\n        :return: True/False, depending on the result of the code (default is True)\n        \"\"\"\n\n        if not self.code_strings:\n            return True\n\n        if vertices_substitution_dict:\n            namespace = self.__substitute_names_in_namespace(self.namespace, vertices_substitution_dict)\n        else:\n            namespace = self.namespace\n        try:\n            self.__execute_code(self._compiled_ast_and_expr, namespace)\n        except:\n            pass\n        return namespace['result']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef substitute_namespace_into_graph(self, graph):\n        for key, value in self.namespace.items():\n            try:\n                nodes = graph.vs.select(name=key)\n                for node in nodes:\n                    for k, v in value.items():\n                        node[k] = v\n            except:\n                pass\n            try:\n                nodes = graph.es.select(name=key)\n                for node in nodes:\n                    for k, v in value.items():\n                        node[k] = v\n            except:\n                pass\n        return graph", "response": "Substitute the namespace of the code into a graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef droplets(self):\n        json = self.request('/droplets/', method='GET')\n        status = json.get('status')\n        if status == 'OK':\n            droplet_json = json.get('droplets', [])\n            droplets = [Droplet.from_json(droplet) for droplet in droplet_json]\n            return droplets\n        else:\n            message = json.get('message', None)\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method returns the list of droplets in the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef power_cycle_droplet(self, droplet_id):\n        if not droplet_id:\n            msg = 'droplet_id is required to power cycle a droplet!'\n            raise DOPException(msg)\n        json = self.request('/droplets/%s/power_cycle' % droplet_id, method='GET')\n        status = json.get('status')\n        if status == 'OK':\n            return json.get('event_id')\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method allows you to power cycle a droplet. This method will power off the droplet and turn it back on."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restore_droplet(self, droplet_id, image_id):\n        if not droplet_id:\n            raise DOPException('droplet_id is required to restore a droplet!')\n        if not image_id:\n            raise DOPException('image_id is required to rebuild a droplet!')\n        params = {'image_id': image_id}\n        json = self.request('/droplets/%s/restore' % droplet_id, method='GET',\n                            params=params)\n        status = json.get('status')\n        if status == 'OK':\n            return json.get('event_id')\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method allows you to restore an image or snapshot of a droplet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_droplet(self, droplet_id, name):\n        if not droplet_id:\n            raise DOPException('droplet_id is required to rebuild a droplet!')\n        if not name:\n            raise DOPException('name is required to rebuild a droplet!')\n        params = {'name': name}\n        json = self.request('/droplets/%s/rename' % droplet_id, method='GET',\n                            params=params)\n        status = json.get('status')\n        if status == 'OK':\n            return json.get('event_id')\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method allows you to reinstall a droplet with a default image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef destroy_droplet(self, droplet_id, scrub_data=False):\n        params = {}\n\n        if scrub_data:\n            params['scrub_data'] = True\n\n        json = self.request('/droplets/%s/destroy' % droplet_id, method='GET',\n                            params=params)\n        status = json.get('status')\n        if status == 'OK':\n            return json.get('event_id')\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method destroys one of your droplets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show_image(self, image_id_or_slug):\n        if not image_id_or_slug:\n            msg = 'image_id_or_slug is required to destroy an image!'\n            raise DOPException(msg)\n\n        json = self.request('/images/%s' % image_id_or_slug, method='GET')\n        image_json = json.get('image')\n        status = json.get('status')\n        if status == 'OK':\n            image = Image.from_json(image_json)\n            return image\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method displays the attributes of an image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef destroy_image(self, image_id_or_slug):\n\n        if not image_id_or_slug:\n            msg = 'image_id_or_slug is required to destroy an image!'\n            raise DOPException(msg)\n\n        json = self.request('/images/%s/destroy' % image_id_or_slug, method='GET')\n        status = json.get('status')\n        return status", "response": "This method allows you to destroy an image. You can use this method to destroy an image."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transfer_image(self, image_id_or_slug, region_id):\n        if not image_id_or_slug:\n            msg = 'image_id_or_slug is required to transfer an image!'\n            raise DOPException(msg)\n\n        if not region_id:\n            raise DOPException('region_id is required to transfer an image!')\n        params = {'region_id': region_id}\n        json = self.request('/images/%s/transfer' % image_id_or_slug,\n                            method='GET', params=params)\n        status = json.get('status')\n        if status == 'OK':\n            return json.get('event_id')\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method allows you to transfer an image to a specified region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ssh_keys(self):\n        params = {}\n        json = self.request('/ssh_keys', method='GET', params=params)\n        status = json.get('status')\n        if status == 'OK':\n            ssh_keys_json = json.get('ssh_keys', [])\n            keys = [SSHKey.from_json(ssh_key) for ssh_key in ssh_keys_json]\n            return keys\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method lists all the available public SSH keys in your account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef destroy_ssh_key(self, ssh_key_id):\n        json = self.request('/ssh_keys/%s/destroy' % ssh_key_id, method='GET')\n        status = json.get('status')\n        return status", "response": "This method will delete an SSH key from your account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef domains(self):\n        json = self.request('/domains', method='GET')\n        status = json.get('status')\n        if status == 'OK':\n            domains_json = json.get('domains', [])\n            domains = [Domain.from_json(domain) for domain in domains_json]\n            return domains\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method returns all of your current domains."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show_domain_record(self, domain_id, record_id):\n        json = self.request('/domains/%s/records/%s' % (domain_id, record_id),\n                            method='GET')\n        status = json.get('status')\n        if status == 'OK':\n            domain_record_json = json.get('record')\n            domain_record = Record.from_json(domain_record_json)\n            return domain_record\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method returns the specified domain record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef destroy_domain_record(self, domain_id, record_id):\n        json = self.request('/domains/%s/records/%s/destroy' % (domain_id, record_id),\n                            method='GET')\n        status = json.get('status')\n        return status", "response": "This method deletes the specified domain record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef events(self, event_id):\n        json = self.request('/events/%s' % event_id, method='GET')\n        status = json.get('status')\n        if status == 'OK':\n            event_json = json.get('event')\n            event = Event.from_json(event_json)\n            return event\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))", "response": "This method returns the details of an event in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict with ebuio headers", "response": "def get_ebuio_headers(request):\n    \"\"\"Return a dict with ebuio headers\"\"\"\n\n    retour = {}\n\n    for (key, value) in request.headers:\n        if key.startswith('X-Plugit-'):\n            key = key[9:]\n\n            retour[key] = value\n\n    return retour"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getTerminalSize():\n\n    def ioctl_GWINSZ(fd):\n        # These two imports are only present on POSIX systems, so they must be\n        # guarded by a try block.\n        import fcntl\n        import termios\n        return struct.unpack(\"hh\", fcntl.ioctl(fd, termios.TIOCGWINSZ, \"1234\"))\n    # try stdin, stdout, stderr\n    for fd in (0, 1, 2):\n        try:\n            return ioctl_GWINSZ(fd)\n        except:\n            pass\n    # try os.ctermid()\n    try:\n        fd = os.open(os.ctermid(), os.O_RDONLY)\n        try:\n            return ioctl_GWINSZ(fd)\n        finally:\n            os.close(fd)\n    except:\n        pass\n    # try environment variables\n    try:\n        return tuple(int(os.getenv(var)) for var in (\"LINES\", \"COLUMNS\"))\n    except:\n        pass\n    # i give up. return default.\n    return (25, 80)", "response": "Returns the size of the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over the items in the given iterable and update the progress bar.", "response": "def iterate(self, iterable, format=\"%s\"):\n        \"\"\"Use as a target of a for-loop to issue a progress update for every\n        iteration. For example:\n\n        progress = ProgressBar()\n        for text in progress.iterate([\"foo\", \"bar\", \"bat\"]):\n            ...\n        \"\"\"\n\n        # If iterable has a definite length, then set the maximum value of the\n        # progress bar. Else, set the maximum value to -1 so that the progress\n        # bar displays indeterminate progress (scrolling dots).\n        try:\n            length = len(iterable)\n        except TypeError:\n            self.max = -1\n        else:\n            self.max = length\n\n        # Iterate over the input, updating the progress bar for each element.\n        for i, item in enumerate(iterable):\n            self.update(i, format % item)\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nredraw the text progress bar.", "response": "def show(self):\n        \"\"\"Redraw the text progress bar.\"\"\"\n\n        if len(self.text) > self.textwidth:\n            label = self.text[0:self.textwidth]\n        else:\n            label = self.text.rjust(self.textwidth)\n\n        terminalSize = getTerminalSize()\n        if terminalSize is None:\n            terminalSize = 80\n        else:\n            terminalSize = terminalSize[1]\n\n        barWidth = terminalSize - self.textwidth - 10\n\n        if self.value is None or self.value < 0:\n            pattern = self.twiddle_sequence[\n                self.twiddle % len(self.twiddle_sequence)]\n            self.twiddle += 1\n            barSymbols = (pattern * int(math.ceil(barWidth/3.0)))[0:barWidth]\n            progressFractionText = '   . %'\n        else:\n            progressFraction = float(self.value) / self.max\n\n            nBlocksFrac, nBlocksInt = math.modf(\n                max(0.0, min(1.0, progressFraction)) * barWidth)\n            nBlocksInt = int(nBlocksInt)\n\n            partialBlock = self.sequence[\n                int(math.floor(nBlocksFrac * len(self.sequence)))]\n\n            nBlanks = barWidth - nBlocksInt - 1\n            barSymbols = (self.sequence[-1] * nBlocksInt) + partialBlock + \\\n                (self.sequence[0] * nBlanks)\n            barSymbols = barSymbols[:barWidth]\n            progressFractionText = ('%.1f%%' % (100*progressFraction)).rjust(6)\n\n        print >>self.fid, '\\r\\x1B[1m' + label + '\\x1B[0m [' + barSymbols + \\\n            ']' + progressFractionText,\n        self.fid.flush()\n        self.linefed = False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nredraw the progress bar optionally changing the value and text and return the new value.", "response": "def update(self, value=None, text=None):\n        \"\"\"Redraw the progress bar, optionally changing the value and text\n        and return the (possibly new) value.  For I/O performance, the\n        progress bar might not be written to the terminal if the text does\n        not change and the value changes by too little.  Use .show() to\n        force a redraw.\"\"\"\n        redraw = False\n        if text is not None:\n            redraw = text != self.text\n            self.text = text\n        if value is not None:\n            redraw |= self.max == 0 or round(value/(0.0003*self.max)) != \\\n                round(self.value/(0.0003*self.max))\n            self.value = value\n        if redraw:\n            if self.isatty:\n                self.show()\n            else:\n                print >>self.fid, self.text\n        return self.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef increment(self, delta=1, text=None):\n        return self.update(value=min(self.max, self.value + delta), text=text)", "response": "Redraw the progress bar incrementing the value by delta by changing the text. Returns the new value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a x and y of dest, determine the ratio and return an (x,y,w,h) for a fitted image (note x or y could be neg). >>> adjust_fit(4,3,5,5) (0.5, 0, 3.0, 3.0) >>> adjust_fit(8,6,5,5) (1.0, 0, 6.0, 6.0) >>> adjust_fit(4,3,5,2) (0, 0.69999999999999996, 4.0, 1.6000000000000001) >>> adjust_fit(8,6,5,2) (0, 1.3999999999999999, 8.0, 3.2000000000000002)", "response": "def adjust_fit(dst_w, dst_h, img_w, img_h):\n    \"\"\"\n    given a x and y of dest, determine the ratio and return\n    an (x,y,w,h) for a fitted image (note x or y could be neg).\n    >>> adjust_fit(4,3,5,5)\n    (0.5, 0, 3.0, 3.0)\n    >>> adjust_fit(8,6,5,5)\n    (1.0, 0, 6.0, 6.0)\n    >>> adjust_fit(4,3,5,2)\n    (0, 0.69999999999999996, 4.0, 1.6000000000000001)\n    >>> adjust_fit(8,6,5,2)\n    (0, 1.3999999999999999, 8.0, 3.2000000000000002)\n    \"\"\"\n    dst_w = float(dst_w)\n    dst_h = float(dst_h)\n    img_w = float(img_w)\n    img_h = float(img_h)\n\n    dst_ratio = float(dst_w) / dst_h\n\n    img_ratio = float(img_w) / img_h\n\n    if dst_ratio > img_ratio:\n        # image is narrower, use height\n        y = 0\n        h = dst_h\n        w = h * img_ratio\n        x = dst_w / 2 - w / 2\n    else:\n        scale = dst_h / img_h\n        x = 0\n        w = dst_w\n        h = w / img_ratio\n        y = dst_h / 2 - h / 2\n    return x, y, w, h"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadjust the size of the image", "response": "def adjust_size(self, dst_x, dst_y, mode=FIT):\n        \"\"\"\n        given a x and y of dest, determine the ratio and return\n        an (x,y,w,h) for a output image.\n        \"\"\"\n        # get image size\n        image = Image.open(self.path)\n        width, height = image.size\n        if mode == FIT:\n            return adjust_crop(dst_x, dst_y, width, height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit(self, P):\n        P = np.asarray(P)\n        assert np.all(P >= 0)\n        assert P.ndim == 2\n        assert P.shape[0] == P.shape[1]\n\n        N = P.shape[0]\n        max_thresh = 1 + self._epsilon\n        min_thresh = 1 - self._epsilon\n\n        # Initialize r and c, the diagonals of D1 and D2\n        # and warn if the matrix does not have support.\n        r = np.ones((N, 1))\n        pdotr = P.T.dot(r)\n        total_support_warning_str = (\n            \"Matrix P must have total support. \"\n            \"See documentation\"\n        )\n        if not np.all(pdotr != 0):\n            warnings.warn(total_support_warning_str, UserWarning)\n\n        c = 1 / pdotr\n        pdotc = P.dot(c)\n        if not np.all(pdotc != 0):\n            warnings.warn(total_support_warning_str, UserWarning)\n\n        r = 1 / pdotc\n        del pdotr, pdotc\n\n        P_eps = np.copy(P)\n        while np.any(np.sum(P_eps, axis=1) < min_thresh) \\\n                or np.any(np.sum(P_eps, axis=1) > max_thresh) \\\n                or np.any(np.sum(P_eps, axis=0) < min_thresh) \\\n                or np.any(np.sum(P_eps, axis=0) > max_thresh):\n\n            c = 1 / P.T.dot(r)\n            r = 1 / P.dot(c)\n\n            self._D1 = np.diag(np.squeeze(r))\n            self._D2 = np.diag(np.squeeze(c))\n            P_eps = self._D1.dot(P).dot(self._D2)\n\n            self._iterations += 1\n\n            if self._iterations >= self._max_iter:\n                self._stopping_condition = \"max_iter\"\n                break\n\n        if not self._stopping_condition:\n            self._stopping_condition = \"epsilon\"\n\n        self._D1 = np.diag(np.squeeze(r))\n        self._D2 = np.diag(np.squeeze(c))\n        P_eps = self._D1.dot(P).dot(self._D2)\n\n        return P_eps", "response": "Fit the diagonal matrices in Sinkhorn Knopp s algorithm to a 2d array - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting the given URL using LIGO.ORG SAML authentication.", "response": "def request_ligodotorg(url, debug=False):\n    \"\"\"Request the given URL using LIGO.ORG SAML authentication.\n\n    This requires an active Kerberos ticket for the user, to get one:\n\n    $ kinit albert.einstein@LIGO.ORG\n\n    Parameters\n    ----------\n    url : `str`\n        URL path for request\n    debug : `bool`, optional\n        Query in verbose debuggin mode, default `False`\n\n    Returns\n    -------\n    urllib.addinfourl\n        file object containing output data, use .read() to extract\n        text content\n    \"\"\"\n    # set debug to 1 to see all HTTP(s) traffic\n    debug = int(debug)\n\n    # need an instance of HTTPS handler to do HTTPS\n    httpsHandler = urllib2.HTTPSHandler(debuglevel = debug)\n\n    # use a cookie jar to store session cookies\n    jar = cookielib.LWPCookieJar()\n\n    # if a cookier jar exists open it and read the cookies\n    # and make sure it has the right permissions\n    if os.path.exists(COOKIE_JAR):\n        os.chmod(COOKIE_JAR, stat.S_IRUSR | stat.S_IWUSR)\n\n        # set ignore_discard so that session cookies are preserved\n        jar.load(COOKIE_JAR, ignore_discard = True)\n\n    # create a cookie handler from the cookier jar\n    cookie_handler = urllib2.HTTPCookieProcessor(jar)\n    # need a redirect handler to follow redirects\n    redirectHandler = urllib2.HTTPRedirectHandler()\n\n    # need an auth handler that can do negotiation.\n    # input parameter is the Kerberos service principal.\n    auth_handler = HTTPNegotiateAuthHandler(service_principal='HTTP@%s'\n                                                            % (LIGO_LOGIN_URL))\n\n    # create the opener.\n    opener = urllib2.build_opener(auth_handler, cookie_handler, httpsHandler,\n                                  redirectHandler)\n\n    # prepare the request object\n    request = urllib2.Request(url)\n\n    # use the opener and the request object to make the request.\n    response = opener.open(request)\n\n    # save the session cookies to a file so that they can\n    # be used again without having to authenticate\n    jar.save(COOKIE_JAR, ignore_discard=True)\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef publish_cat1(method, con, token, cat, kwargs):\n        req_str = \"/\"+str( kwargs['id'] )+\"/\"+cat+'?'                #/id/category?\n        del kwargs['id']\n        kwargs['access_token'] = token                               #add access token to kwwargs\n        res = wiring.send_request(method, con, req_str, kwargs)    \n        return res", "response": "This function is used by the publish and delete functions. It is used by the publish and delete functions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object_cat1(con, token, cat, kwargs):\n        req_str = \"/\"+kwargs['id']+\"?\"               #/id?\n        req_str += \"access_token=\"+token             #/id?@acces_token=......\n        del kwargs['id']\n        \n        key = settings.get_object_cat1_param[cat]    #get the param name for the category(single, multiple)\n        req_str += \"&\"+key+\"=\"                       #/id?@acces_token=......key=\n        if key in kwargs.keys():\n                length = len( kwargs[key] )\n                for i in range(length):\n                        if i == 0:\n                                req_str +=  kwargs[key][i]\n                        else:\n                                req_str +=  \",\"+kwargs[key][i]        \n        else:\n                return \"Parameter Error\"\n        \n        res = wiring.send_request(\"GET\", con, req_str, '')\n        return res", "response": "This function is used to get the object category 1."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_variables_substitution_dictionaries(self, lhs_graph, rhs_graph):\n        if not rhs_graph:\n            return {}, {}, {}\n        self.matching_code_container.add_graph_to_namespace(lhs_graph)\n        self.matching_code_container.add_graph_to_namespace(rhs_graph)\n        return self.__collect_variables_that_match_graph(lhs_graph, rhs_graph)", "response": "This method returns a list of dictionaries that are used to substitute the names of the variable names in lhs with the names of the variable names in rhs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the file bundle.", "response": "def run(self, src_folder, requirements=\"requirements.txt\", local_package=None):\n        \"\"\"Builds the file bundle.\n        :param str src:\n           The path to your Lambda ready project (folder must contain a valid\n            config.yaml and handler module (e.g.: service.py).\n        :param str local_package:\n            The path to a local package with should be included in the deploy as\n            well (and/or is not available on PyPi)\n        \"\"\"\n        self.set_src_path(src_folder)\n\n        if not os.path.isdir(self.get_src_path()):\n            raise ArdyNoFileError(\"File {} not exist\".format(self.get_src_path()))\n        # Get the absolute path to the output directory and create it if it doesn't\n        # already exist.\n        dist_directory = 'dist'\n        path_to_dist = os.path.join(self.get_src_path(), dist_directory)\n        self.mkdir(path_to_dist)\n\n        # Combine the name of the Lambda function with the current timestamp to use\n        # for the output filename.\n        output_filename = \"{0}.zip\".format(self.timestamp())\n\n        path_to_temp = mkdtemp(prefix='aws-lambda')\n        self.pip_install_to_target(path_to_temp,\n                                   requirements=requirements,\n                                   local_package=local_package)\n\n        if os.path.isabs(src_folder):\n            src_folder = src_folder.split(os.sep)[-1]\n\n        self.copytree(self.get_src_path(), os.path.join(path_to_temp, src_folder))\n\n        # Zip them together into a single file.\n        # TODO: Delete temp directory created once the archive has been compiled.\n        path_to_zip_file = self.create_artefact(path_to_temp, path_to_dist, output_filename)\n        return path_to_zip_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _install_packages(self, path, packages):\n\n        def _filter_blacklist(package):\n            blacklist = [\"-i\", \"#\", \"Python==\", \"ardy==\"]\n            return all(package.startswith(entry.encode()) is False for entry in blacklist)\n\n        filtered_packages = filter(_filter_blacklist, packages)\n        # print([package for package in filtered_packages])\n        for package in filtered_packages:\n            package = str(package, \"utf-8\")\n            if package.startswith('-e '):\n                package = package.replace('-e ', '')\n\n            logger.info('Installing {package}'.format(package=package))\n            pip.main(['install', package, '-t', path, '--ignore-installed', '-q'])", "response": "Install all packages listed to the target directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pip_install_to_target(self, path, requirements=\"\", local_package=None):\n        packages = []\n        if not requirements:\n            logger.debug('Gathering pip packages')\n            # packages.extend(pip.operations.freeze.freeze())\n            pass\n        else:\n            requirements_path = os.path.join(self.get_src_path(), requirements)\n            logger.debug('Gathering packages from requirements: {}'.format(requirements_path))\n            if os.path.isfile(requirements_path):\n                data = self.read(requirements_path)\n                packages.extend(data.splitlines())\n            else:\n                logger.debug('No requirements file in {}'.format(requirements_path))\n\n        if local_package is not None:\n            if not isinstance(local_package, (list, tuple)):\n                local_package = [local_package]\n            for l_package in local_package:\n                packages.append(l_package)\n        self._install_packages(path, packages)", "response": "This function installs all installed pip packages to the target directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readTableFromDelimited(f, separator=\"\\t\"):\n    rowNames = []\n    columnNames = []\n    matrix = []\n\n    first = True\n    for line in f.readlines():\n        line = line.rstrip()\n        if len(line) == 0:\n            continue\n\n        row = line.split(separator)\n        if first:\n            columnNames = row[1:]\n            first = False\n        else:\n            rowNames.append(row[0])\n            matrix.append([float(c) for c in row[1:]])\n\n    return Table(rowNames, columnNames, matrix)", "response": "Reads a table object from a plain delimited file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef readTableFromCSV(f, dialect=\"excel\"):\n    rowNames = []\n    columnNames = []\n    matrix = []\n\n    first = True\n    for row in csv.reader(f, dialect):\n        if first:\n            columnNames = row[1:]\n            first = False\n        else:\n            rowNames.append(row[0])\n            matrix.append([float(c) for c in row[1:]])\n\n    return Table(rowNames, columnNames, matrix)", "response": "Reads a table object from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cell(self, rowName, columnName):\n        return self.matrix[self.rowIndices[rowName], self.columnIndices[columnName]]", "response": "Returns the value of the cell on the given row and column."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of dicts. Each dictionary contains a row with the keys rowName and columnName.", "response": "def rows(self):\n        \"\"\"\n        Returns a list of dicts.\n        \"\"\"\n        rows = []\n        for rowName in self.rowNames:\n            row = {columnName: self[rowName, columnName] for columnName in self.columnNames}\n            row[\"_\"] = rowName\n            rows.append(row)\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndrop and re - creates the SQL schema", "response": "def init_db():\n    \"\"\"\n    Drops and re-creates the SQL schema\n    \"\"\"\n    db.drop_all()\n    db.configure_mappers()\n    db.create_all()\n    db.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse gunicorn to run gunicorn", "response": "def gunicorn(host, port, workers):\n    \"\"\"use gunicorn\"\"\"\n    from gunicorn.app.base import Application\n\n    class FlaskApplication(Application):\n        def init(self, parser, opts, args):\n            return {'bind': '{0}:{1}'.format(host, port), 'workers': workers}\n\n        def load(self):\n            return app\n\n    application = FlaskApplication()\n    return application.run()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_zipfile(self, path):\n        # try to add as zipfile\n        zin = zipfile.ZipFile(path)\n        for zinfo in zin.infolist():\n            name = zinfo.filename\n            if name.endswith(\"/\"):\n                self.mkdir(name)\n            else:\n                content = zin.read(name)\n                self.touch(name, content)", "response": "Load a zip file into the cache"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the contents of a directory and creates the directories that are not yet loaded.", "response": "def load_dir(self, path):\n        \"\"\"\n        import contents of a directory\n        \"\"\"\n\n        def visit_path(arg, dirname, names):\n            for name in names:\n                fpath = os.path.join(dirname, name)\n                new_path = fpath[len(path):]\n                if os.path.isfile(fpath):\n                    content = open(fpath, \"rb\").read()\n                    self.touch(new_path, content)\n                else:\n                    self.mkdir(new_path)\n\n        os.path.walk(path, visit_path, None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rel_path(self, path, basepath=None):\n        basepath = basepath or self.src_dir\n        return path[len(basepath) + 1:]", "response": "Return the relative path of the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nunzips the contents of the source directory into the given directory.", "response": "def unzip(self, directory):\n        \"\"\"\n        Write contents of zipfile to directory\n        \"\"\"\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        shutil.copytree(self.src_dir, directory)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the database for the given string using the given builder", "response": "def __query_with_builder(self, string, builder):\n        \"\"\"\n        Uses the builder in the argument to modify the graph, according to the commands in the string\n\n        :param string: The single query to the database\n        :return: The result of the RETURN operation\n        \"\"\"\n        action_graph_pairs = self.__get_action_graph_pairs_from_query(string)\n        for action, graph_str in action_graph_pairs:\n            if action == 'RETURN' or action == '':\n                return self.__return(graph_str, builder)\n            try:\n                self.action_dict[action](graph_str, builder)\n            except MatchException:\n                break\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit the query into command and argument pairs.", "response": "def __get_action_graph_pairs_from_query(self, query):\n        \"\"\"\n        Splits the query into command/argument pairs, for example [(\"MATCH\",\"{}(_a))\", (\"RETURN\",\"_a\")]\n\n        :param query: The string with the list of commands\n        :return: the command/argument pairs\n        \"\"\"\n        import re\n\n        query = convert_special_characters_to_spaces(query)\n        graph_list = re.split('|'.join(self.action_list), query)\n        query_list_positions = [query.find(graph) for graph in graph_list]\n        query_list_positions = query_list_positions\n        query_list_positions = query_list_positions\n        action_list = [query[query_list_positions[i] + len(graph_list[i]):query_list_positions[i + 1]].strip()\n                       for i in range(len(graph_list) - 1)]\n        graph_list = graph_list[1:]\n        return zip(action_list, graph_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_secret(*args, **kwargs):\n\n    to_sign = '-!'.join(args) + '$$'.join(kwargs.values())\n\n    key = settings.SECRET_FOR_SIGNS\n\n    hashed = hmac.new(key, to_sign, sha1)\n    return re.sub(r'[\\W_]+', '', binascii.b2a_base64(hashed.digest()))", "response": "Create a secure key generated from the user and the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a string of the form meme text1 text2 or search_keywords and return a url to the image that is used to generate the image.", "response": "def processString(inptStr):\n    ''' \n    inptStr may be a string of the following forms:\n    * 'meme: text0 | text1'\n    * 'gif: search_keywords'\n\n    If not, it returns an appropriate error message,\n    stating an improperly formatted <magic> tag.\n    \n    Fails gracefully when it can't find or generate a meme\n    or a gif, by returning an appropriate image url with the\n    failure message on it.\n\n    TODO: Find a way to efficiently search for xkcd comics\n    '''\n\n    inptStr.strip(' ')\n    imgParamList = inptStr.split(':')\n\n    if len(imgParamList) < 2:\n        print(\"Not enough information for searching for image.\")\n        return not_enough_info\n\n    else:\n\n        imgType = imgParamList[0]\n        imgParams = imgParamList[1]\n        \n        if imgType == 'meme':\n            imgURL = processMeme(imgParams)\n            # print(imgURL)\n            return imgURL\n\n        elif imgType == 'gif':\n            gifURL = processGif(imgParams)\n            # print(gifURL)\n            return gifURL\n\n        else:\n            print(\"Improperly formatted <magic> tag.\")\n            return improperly_formatted_tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n  '''i am winston wolfe, i solve problems'''\n  arguments = docopt(__doc__, version=__version__)\n  if arguments['on']:\n    print 'Mr. Wolfe is at your service'\n    print 'If any of your programs run into an error'\n    print 'use wolfe $l'\n    print 'To undo the changes made by mr wolfe in your  bashrc, do wolfe off'\n    on()\n  elif arguments['off']:\n    off()\n    print 'Mr. Wolfe says goodbye!'\n  elif arguments['QUERY']:\n  \tlast(arguments['QUERY'], arguments['-g'] or arguments['--google'])\n  else:\n    print __doc__", "response": "i am winston wolfe i solve problems"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_transactions(self, **kwargs):\n        kw_map = {\n            'to_date': 'query(period).end',\n            'from_account_id': 'query(member)',\n            'from_date': 'query(period).begin',\n            'txn_ref': 'query(transactionNumber)'}\n\n        if not self.TRANSACTIONS_FORM:\n            try:\n                self.get_url(self.TRANSACTIONS_URL)\n            except AuthRequiredException:\n                self._auth()\n                self.get_url(self.TRANSACTIONS_URL)\n            self.br.select_form(\"accountHistoryForm\")\n            self.br.form.method = 'POST'\n            self.br.form.action = self.TRANSACTIONS_EXPORT_URL\n            self.TRANSACTIONS_FORM = self.br.form\n            _form = deepcopy(self.TRANSACTIONS_FORM)\n        else:\n            _form = deepcopy(self.TRANSACTIONS_FORM)\n\n        # make all hidden and readonly fields writable\n        _form.set_all_readonly(False)\n\n        for key, field_name in kw_map.items():\n            if key in kwargs:\n                # if the field is a date, format accordingly\n                if key.endswith('_date'):\n                    _form[field_name] = kwargs.get(key).strftime('%d/%m/%Y')\n                else:\n                    _form[field_name] = kwargs.get(key)\n\n        try:\n            r = self.post_url(self.TRANSACTIONS_EXPORT_URL, form=_form)\n            return self._parse_transactions(r)\n        except AuthRequiredException:\n            self._auth()\n            r = self.post_url(self.TRANSACTIONS_EXPORT_URL, form=_form)\n            return self._parse_transactions(r)", "response": "This method returns a list of all transactions for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the balance for the configured account.", "response": "def get_balance(self):\n        \"\"\"\n        Retrieves the balance for the configured account\n        \"\"\"\n        self.br.open(self.MOBILE_WEB_URL % {'accountno': self.account})\n        try:\n            # Search for the existence of the Register link - indicating a new account\n            self.br.find_link(text='Register')\n            raise InvalidAccountException\n        except mechanize.LinkNotFoundError:\n            pass\n\n        self.br.follow_link(text='My sarafu')\n        self.br.follow_link(text='Balance Inquiry')\n        self.br.select_form(nr=0)\n        self.br['pin'] = self.pin\n        r = self.br.submit().read()\n\n        # Pin valid?\n        if re.search(r'Invalid PIN', r):\n            raise AuthDeniedException\n\n        # An error could occur for other reasons\n        if re.search(r'Error occured', r):\n            raise RequestErrorException\n\n        # If it was successful, we extract the balance\n        if re.search(r'Your balance is TSH (?P<balance>[\\d\\.]+)', r):\n            match = re.search(r'Your balance is TSH (?P<balance>[\\d\\.]+)', r)\n            return match.group('balance')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_url(self, url, form):\n        _r = self.br.open(url, form.click_request_data()[1])\n\n        # check that we've not been redirected to the login page or an error occured\n        if self.br.geturl().startswith(self.AUTH_URL):\n            raise AuthRequiredException\n        elif self.br.geturl().startswith(self.ERROR_URL):\n            raise RequestErrorException\n        else:\n            return _r.read()", "response": "This method is used to retrieve the contents of a URL using the POST method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the plugit object and the baseURI to use if not in standalone mode", "response": "def getPlugItObject(hproPk):\n    \"\"\"Return the plugit object and the baseURI to use if not in standalone mode\"\"\"\n\n    from hprojects.models import HostedProject\n\n    try:\n        hproject = HostedProject.objects.get(pk=hproPk)\n    except (HostedProject.DoesNotExist, ValueError):\n        try:\n            hproject = HostedProject.objects.get(plugItCustomUrlKey=hproPk)\n        except HostedProject.DoesNotExist:\n            raise Http404\n\n    if hproject.plugItURI == '' and not hproject.runURI:\n        raise Http404\n    plugIt = PlugIt(hproject.plugItURI)\n\n    # Test if we should use custom key\n    if hasattr(hproject, 'plugItCustomUrlKey') and hproject.plugItCustomUrlKey:\n        baseURI = reverse('plugIt.views.main', args=(hproject.plugItCustomUrlKey, ''))\n    else:\n        baseURI = reverse('plugIt.views.main', args=(hproject.pk, ''))\n\n    return (plugIt, baseURI, hproject)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_user(mode=None, pk=None):\n\n    user = None\n\n    if mode == 'log' or pk == \"-1\":\n        user = DUser(pk=-1, username='Logged', first_name='Logged', last_name='Hector', email='logeedin@plugit-standalone.ebuio')\n        user.gravatar = 'https://www.gravatar.com/avatar/ebuio1?d=retro'\n        user.ebuio_member = False\n        user.ebuio_admin = False\n        user.subscription_labels = []\n    elif mode == 'mem' or pk == \"-2\":\n        user = DUser(pk=-2, username='Member', first_name='Member', last_name='Luc', email='memeber@plugit-standalone.ebuio')\n        user.gravatar = 'https://www.gravatar.com/avatar/ebuio2?d=retro'\n        user.ebuio_member = True\n        user.ebuio_admin = False\n        user.subscription_labels = []\n    elif mode == 'adm' or pk == \"-3\":\n        user = DUser(pk=-3, username='Admin', first_name='Admin', last_name='Charles', email='admin@plugit-standalone.ebuio')\n        user.gravatar = 'https://www.gravatar.com/avatar/ebuio3?d=retro'\n        user.ebuio_member = True\n        user.ebuio_admin = True\n        user.subscription_labels = []\n    elif mode == 'ano':\n        user = AnonymousUser()\n        user.email = 'nobody@plugit-standalone.ebuio'\n        user.first_name = 'Ano'\n        user.last_name = 'Nymous'\n        user.ebuio_member = False\n        user.ebuio_admin = False\n        user.subscription_labels = []\n    elif settings.PIAPI_STANDALONE and pk >= 0:\n        # Generate an unknown user for compatibility reason in standalone mode\n        user = DUser(pk=pk, username='Logged', first_name='Unknown', last_name='Other User', email='unknown@plugit-standalone.ebuio')\n        user.gravatar = 'https://www.gravatar.com/avatar/unknown?d=retro'\n        user.ebuio_member = False\n        user.ebuio_admin = False\n        user.subscription_labels = []\n\n    if user:\n        user.ebuio_orga_member = user.ebuio_member\n        user.ebuio_orga_admin = user.ebuio_admin\n\n    return user", "response": "Generate a user for a standalone mode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen404(request, baseURI, reason, project=None):\n    return HttpResponseNotFound(\n        render_to_response('plugIt/404.html', {'context':\n            {\n                'reason': reason,\n                'ebuio_baseUrl': baseURI,\n                'ebuio_userMode': request.session.get('plugit-standalone-usermode', 'ano'),\n            },\n            'project': project\n        }, context_instance=RequestContext(request)))", "response": "Return a 404 error"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a 500 error", "response": "def gen500(request, baseURI, project=None):\n    \"\"\"Return a 500 error\"\"\"\n    return HttpResponseServerError(\n        render_to_response('plugIt/500.html', {\n            'context': {\n                'ebuio_baseUrl': baseURI,\n                'ebuio_userMode': request.session.get('plugit-standalone-usermode', 'ano'),\n            },\n            'project': project\n        }, context_instance=RequestContext(request)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a 403 error", "response": "def gen403(request, baseURI, reason, project=None):\n    \"\"\"Return a 403 error\"\"\"\n    orgas = None\n    public_ask = False\n\n    if not settings.PIAPI_STANDALONE:\n        from organizations.models import Organization\n\n        if project and project.plugItLimitOrgaJoinable:\n            orgas = project.plugItOrgaJoinable.order_by('name').all()\n        else:\n            orgas = Organization.objects.order_by('name').all()\n\n        rorgas = []\n\n        # Find and exclude the visitor orga\n        for o in orgas:\n            if str(o.pk) == settings.VISITOR_ORGA_PK:\n                public_ask = True\n            else:\n                rorgas.append(o)\n\n        orgas = rorgas\n\n    return HttpResponseForbidden(render_to_response('plugIt/403.html', {'context':\n        {\n            'reason': reason,\n            'orgas': orgas,\n            'public_ask': public_ask,\n            'ebuio_baseUrl': baseURI,\n            'ebuio_userMode': request.session.get('plugit-standalone-usermode', 'ano'),\n        },\n        'project': project\n    }, context_instance=RequestContext(request)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the cache key to use for the current page.", "response": "def get_cache_key(request, meta, orgaMode, currentOrga):\n    \"\"\"Return the cache key to use\"\"\"\n\n    # Caching\n    cacheKey = None\n\n    if 'cache_time' in meta:\n        if meta['cache_time'] > 0:\n\n            # by default, no cache by user\n            useUser = False\n\n            # If a logged user in needed, cache the result by user\n            if ('only_logged_user' in meta and meta['only_logged_user']) or \\\n                    ('only_member_user' in meta and meta['only_member_user']) or \\\n                    ('only_admin_user' in meta and meta['only_admin_user']) or \\\n                    ('only_orga_member_user' in meta and meta['only_orga_member_user']) or \\\n                    ('only_orga_admin_user' in meta and meta['only_orga_admin_user']):\n                useUser = True\n\n            # If a value if present in meta, use it\n            if 'cache_by_user' in meta:\n                useUser = meta['cache_by_user']\n\n            cacheKey = '-'\n\n            # Add user info if needed\n            if useUser:\n                cacheKey += str(request.user.pk) + 'usr-'\n\n            # Add orga\n            if orgaMode:\n                cacheKey += str(currentOrga.pk) + 'org-'\n\n            # Add current query\n            cacheKey += request.get_full_path()\n\n            # Add current template (if the template changed, cache must be invalided)\n            cacheKey += meta['template_tag']\n\n    return cacheKey"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_rights_and_access(request, meta, project=None):\n    # User must be logged ?\n    if ('only_logged_user' in meta and meta['only_logged_user']):\n        if not request.user.is_authenticated():\n            return gen403(request, baseURI, 'only_logged_user', project)\n\n    # User must be member of the project ?\n    if ('only_member_user' in meta and meta['only_member_user']):\n        if not request.user.ebuio_member:\n            return gen403(request, baseURI, 'only_member_user', project)\n\n    # User must be administrator of the project ?\n    if ('only_admin_user' in meta and meta['only_admin_user']):\n        if not request.user.ebuio_admin:\n            return gen403(request, baseURI, 'only_admin_user', project)\n\n    # User must be member of the orga ?\n    if ('only_orga_member_user' in meta and meta['only_orga_member_user']):\n        if not request.user.ebuio_orga_member:\n            return gen403(request, baseURI, 'only_orga_member_user', project)\n\n    # User must be administrator of the orga ?\n    if ('only_orga_admin_user' in meta and meta['only_orga_admin_user']):\n        if not request.user.ebuio_orga_admin:\n            return gen403(request, baseURI, 'only_orga_admin_user', project)\n\n    # Remote IP must be in range ?\n    if ('address_in_networks' in meta):\n        if not is_requestaddress_in_networks(request, meta['address_in_networks']):\n            return gen403(request, baseURI, 'address_in_networks', project)", "response": "Check if the user can access the page"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_requestaddress_in_networks(request, networks):\n    from ipware.ip import get_real_ip, get_ip\n\n    # Get the real IP, i.e. no reverse proxy, no nginx\n    ip = get_real_ip(request)\n    if not ip:\n        ip = get_ip(request)\n        if not ip:\n            return False\n\n    # For all networks\n    for network in networks:\n        if is_address_in_network(ip, network):\n            return True\n\n    return False", "response": "Helper method to check if the remote real ip of a request is in a network"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_address_in_network(ip, net):\n    # http://stackoverflow.com/questions/819355/how-can-i-check-if-an-ip-is-in-a-network-in-python\n    import socket\n    import struct\n    ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n    netaddr, bits = net.split('/')\n    if int(bits) == 0:\n        return True\n    net = struct.unpack('=L', socket.inet_aton(netaddr))[0]\n\n    mask = ((2L << int(bits) - 1) - 1)\n\n    return (ipaddr & mask) == (net & mask)", "response": "Is an address in a network?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the content exists in cache and return it", "response": "def find_in_cache(cacheKey):\n    \"\"\"Check if the content exists in cache and return it\"\"\"\n    # If we have to use cache, we try to find the result in cache\n    if cacheKey:\n\n        data = cache.get('plugit-cache-' + cacheKey, None)\n\n        # We found a result, we can return it\n        if data:\n            return (data['result'], data['menu'], data['context'])\n    return (None, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_base_parameters(request):\n\n    getParameters = {}\n    postParameters = {}\n    files = {}\n\n    # Copy GET parameters, excluding ebuio_*\n    for v in request.GET:\n        if v[:6] != 'ebuio_':\n            val = request.GET.getlist(v)\n\n            if len(val) == 1:\n                getParameters[v] = val[0]\n            else:\n                getParameters[v] = val\n\n    # If using post, copy post parameters and files. Excluding ebuio_*\n    if request.method == 'POST':\n        for v in request.POST:\n            if v[:6] != 'ebuio_':\n                val = request.POST.getlist(v)\n\n                if len(val) == 1:\n                    postParameters[v] = val[0]\n                else:\n                    postParameters[v] = val\n\n        for v in request.FILES:\n            if v[:6] != 'ebuio_':\n                files[v] = request.FILES[v]  # .chunks()\n\n    return (getParameters, postParameters, files)", "response": "Build the list of parameters to forward from the post and get parameters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_user_requested_parameters(request, meta):\n\n    postParameters = {}\n    getParameters = {}\n    files = {}\n\n    # Add parameters requested by the server\n    if 'user_info' in meta:\n        for prop in meta['user_info']:\n\n            # Test if the value exist, otherwise return None\n            value = None\n            if hasattr(request.user, prop) and prop in settings.PIAPI_USERDATA:\n                value = getattr(request.user, prop)\n            else:\n                raise Exception('requested user attribute \"%s\", '\n                                'does not exist or requesting is not allowed' % prop)\n\n            # Add informations to get or post parameters, depending on the current method\n            if request.method == 'POST':\n                postParameters['ebuio_u_' + prop] = value\n            else:\n                getParameters['ebuio_u_' + prop] = value\n\n    return (getParameters, postParameters, files)", "response": "Build the list of parameters requested by the plugit server"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the list of get post and file parameters to send", "response": "def build_parameters(request, meta, orgaMode, currentOrga):\n    \"\"\"Return the list of get, post and file parameters to send\"\"\"\n\n    postParameters = {}\n    getParameters = {}\n    files = {}\n\n    def update_parameters(data):\n        tmp_getParameters, tmp_postParameters, tmp_files = data\n\n        getParameters.update(tmp_getParameters)\n        postParameters.update(tmp_postParameters)\n        files.update(tmp_files)\n\n    update_parameters(build_base_parameters(request))\n    update_parameters(build_user_requested_parameters(request, meta))\n    update_parameters(build_orga_parameters(request, orgaMode, currentOrga))\n\n    return (getParameters, postParameters, files)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the list of extra headers that should be added to the headers of the current page.", "response": "def build_extra_headers(request, proxyMode, orgaMode, currentOrga):\n    \"\"\"Build the list of extra headers\"\"\"\n\n    things_to_add = {}\n\n    # If in proxymode, add needed infos to headers\n    if proxyMode:\n\n        # User\n        for prop in settings.PIAPI_USERDATA:\n            if hasattr(request.user, prop):\n                things_to_add['user_' + prop] = getattr(request.user, prop)\n\n        # Orga\n        if orgaMode:\n            things_to_add['orga_pk'] = currentOrga.pk\n            things_to_add['orga_name'] = currentOrga.name\n            things_to_add['orga_codops'] = currentOrga.ebu_codops\n\n        # General\n        things_to_add['base_url'] = baseURI\n\n    if request and hasattr(request, 'META'):\n        if 'REMOTE_ADDR' in request.META:\n            things_to_add['remote-addr'] = request.META['REMOTE_ADDR']\n\n        if 'HTTP_X_FORWARDED_FOR' in request.META and getattr(settings, 'HONOR_X_FORWARDED_FOR'):\n            things_to_add['remote-addr'] = request.META['HTTP_X_FORWARDED_FOR']\n\n        for meta_header, dest_header in [('HTTP_IF_NONE_MATCH', 'If-None-Match'), ('HTTP_ORIGIN', 'Origin'), ('HTTP_ACCESS_CONtROL_REQUEST_METHOD', 'Access-Control-Request-Method'), ('HTTP_ACCESS_CONTROL_REQUEST_HEADERS', 'Access-Control-Request-Headers')]:\n            if meta_header in request.META:\n                things_to_add[dest_header] = request.META[meta_header]\n\n    return things_to_add"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_special_cases(request, data, baseURI, meta):\n\n    if request.method == 'OPTIONS':\n        r = HttpResponse('')\n        return r\n\n    if data is None:\n        return gen404(request, baseURI, 'data')\n\n    if data.__class__.__name__ == 'PlugIt500':\n        return gen500(request, baseURI)\n\n    if data.__class__.__name__ == 'PlugItSpecialCode':\n        r = HttpResponse('')\n        r.status_code = data.code\n        return r\n\n    if data.__class__.__name__ == 'PlugItRedirect':\n        url = data.url\n        if not data.no_prefix:\n            url = baseURI + url\n\n        return HttpResponseRedirect(url)\n\n    if data.__class__.__name__ == 'PlugItFile':\n        response = HttpResponse(data.content, content_type=data.content_type)\n        response['Content-Disposition'] = data.content_disposition\n\n        return response\n\n    if data.__class__.__name__ == 'PlugItNoTemplate':\n        response = HttpResponse(data.content)\n        return response\n\n    if meta.get('json_only', None):  # Just send the json back\n        # Return application/json if requested\n        if 'HTTP_ACCEPT' in request.META and request.META['HTTP_ACCEPT'].find('json') != -1:\n            return JsonResponse(data)\n\n        # Return json data without html content type, since json was not\n        # requiered\n        result = json.dumps(data)\n        return HttpResponse(result)\n\n    if meta.get('xml_only', None):  # Just send the xml back\n        return HttpResponse(data['xml'], content_type='application/xml')", "response": "Handle special cases for return values by the doAction function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_final_response(request, meta, result, menu, hproject, proxyMode, context):\n\n    if 'no_template' in meta and meta['no_template']:  # Just send the json back\n        return HttpResponse(result)\n\n    # TODO this breaks pages not using new template\n    # Add sidebar toggler if plugit did not add by itself\n    # if not \"sidebar-toggler\" in result:\n    #     result = \"<div class=\\\"menubar\\\"><div class=\\\"sidebar-toggler visible-xs\\\"><i class=\\\"ion-navicon\\\"></i></div></div>\" + result\n\n    # render the template into the whole page\n    if not settings.PIAPI_STANDALONE:\n        return render_to_response('plugIt/' + hproject.get_plugItTemplate_display(),\n                                  {\"project\": hproject,\n                                   \"plugit_content\": result,\n                                   \"plugit_menu\": menu,\n                                   'context': context},\n                                  context_instance=RequestContext(request))\n\n    if proxyMode:  # Force inclusion inside template\n        return render_to_response('plugIt/base.html',\n                                  {'plugit_content': result,\n                                   \"plugit_menu\": menu,\n                                   'context': context},\n                                  context_instance=RequestContext(request))\n\n    renderPlugItTemplate = 'plugItBase.html'\n    if settings.PIAPI_PLUGITTEMPLATE:\n        renderPlugItTemplate = settings.PIAPI_PLUGITTEMPLATE\n\n    return render_to_response('plugIt/' + renderPlugItTemplate,\n                              {\"plugit_content\": result,\n                               \"plugit_menu\": menu,\n                               'context': context},\n                              context_instance=RequestContext(request))", "response": "Build the final response to send back to the browser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_node(template, context, name):\n    '''\n    taken originally from\n    http://stackoverflow.com/questions/2687173/django-how-can-i-get-a-block-from-a-template\n    '''\n    for node in template:\n        if isinstance(node, BlockNode) and node.name == name:\n            return node.nodelist.render(context)\n        elif isinstance(node, ExtendsNode):\n            return _get_node(node.nodelist, context, name)\n\n    # raise Exception(\"Node '%s' could not be found in template.\" % name)\n    return \"\"", "response": "Get a node from a template."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the template content and return the menu and the result", "response": "def render_data(context, templateContent, proxyMode, rendered_data, menukey='menubar'):\n    \"\"\"Render the template\"\"\"\n\n    if proxyMode:\n        # Update csrf_tokens\n        csrf = unicode(context['csrf_token'])\n        tag = u'{~__PLUGIT_CSRF_TOKEN__~}'\n        rendered_data = unicode(rendered_data, 'utf-8').replace(tag, csrf)\n\n        result = rendered_data  # Render in proxy mode\n        menu = None  # Proxy mode plugit do not have menu\n\n    else:\n        # Render it\n        template = Template(templateContent)\n        result = template.render(context)\n        menu = _get_node(template, context, menukey)\n\n    return (result, menu)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaches the result if needed", "response": "def cache_if_needed(cacheKey, result, menu, context, meta):\n    \"\"\"Cache the result, if needed\"\"\"\n\n    if cacheKey:\n\n        # This will be a method in django 1.7\n        flat_context = {}\n        for d in context.dicts:\n            flat_context.update(d)\n\n        del flat_context['csrf_token']\n\n        data = {'result': result, 'menu': menu, 'context': flat_context}\n\n        cache.set('plugit-cache-' + cacheKey, data, meta['cache_time'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the template to use", "response": "def get_template(request, query, meta, proxyMode):\n    \"\"\"Return (if needed) the template to use\"\"\"\n\n    templateContent = None\n\n    if not proxyMode:\n\n        templateContent = plugIt.getTemplate(query, meta)\n\n        if not templateContent:\n            return (None, gen404(request, baseURI, 'template'))\n\n    return (templateContent, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_current_orga(request, hproject, availableOrga):\n\n    # If nothing available return 404\n    if len(availableOrga) == 0:\n        raise Http404\n\n    # Find the current orga\n    currentOrgaId = request.session.get('plugit-orgapk-' + str(hproject.pk), None)\n\n    # If we don't have a current one select the first available\n    if currentOrgaId is None:\n        (tmpOrga, _) = availableOrga[0]\n        currentOrgaId = tmpOrga.pk\n    else:\n        # If the current Orga is not among the available ones reset to the first one\n        availableOrgaIds = [o.pk for (o, r) in availableOrga]\n        if currentOrgaId not in availableOrgaIds:\n            (tmpOrga, _) = availableOrga[0]\n            currentOrgaId = tmpOrga.pk\n\n    from organizations.models import Organization\n\n    realCurrentOrga = get_object_or_404(Organization, pk=currentOrgaId)\n\n    return realCurrentOrga", "response": "Return the current orga to use"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_session(request, session_to_set, hproPk):\n\n    for key, value in session_to_set.items():\n        request.session['plugit_' + str(hproPk) + '_' + key] = value", "response": "Update the session with the users - realted values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_current_session(request, hproPk):\n\n    retour = {}\n\n    base_key = 'plugit_' + str(hproPk) + '_'\n\n    for key, value in request.session.iteritems():\n        if key.startswith(base_key):\n            retour[key[len(base_key):]] = value\n\n    return retour", "response": "Get the current session value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nask the server for a media and return it to the client browser. Forward cache headers", "response": "def media(request, path, hproPk=None):\n    \"\"\"Ask the server for a media and return it to the client browser. Forward cache headers\"\"\"\n\n    if not settings.PIAPI_STANDALONE:\n        (plugIt, baseURI, _) = getPlugItObject(hproPk)\n    else:\n        global plugIt, baseURI\n\n    try:\n        (media, contentType, cache_control) = plugIt.getMedia(path)\n    except Exception as e:\n        report_backend_error(request, e, 'meta', hproPk)\n        return gen500(request, baseURI)\n\n    if not media:  # No media returned\n        raise Http404\n\n    response = HttpResponse(media)\n    response['Content-Type'] = contentType\n    response['Content-Length'] = len(media)\n\n    if cache_control:\n        response['Cache-Control'] = cache_control\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the current orga", "response": "def setOrga(request, hproPk=None):\n    \"\"\"Change the current orga\"\"\"\n\n    if settings.PIAPI_STANDALONE:\n        request.session['plugit-standalone-organame'] = request.GET.get('name')\n        request.session['plugit-standalone-orgapk'] = request.GET.get('pk')\n    else:\n\n        (_, _, hproject) = getPlugItObject(hproPk)\n\n        from organizations.models import Organization\n\n        orga = get_object_or_404(Organization, pk=request.GET.get('orga'))\n\n        if request.user.is_superuser or orga.isMember(request.user) or orga.isOwner(request.user):\n            request.session['plugit-orgapk-' + str(hproject.pk)] = orga.pk\n\n    return HttpResponse('')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_api_key(request, key, hproPk):\n\n    if settings.PIAPI_STANDALONE:\n        return True\n\n    (_, _, hproject) = getPlugItObject(hproPk)\n\n    if not hproject:\n        return False\n\n    if hproject.plugItApiKey is None or hproject.plugItApiKey == '':\n        return False\n\n    return hproject.plugItApiKey == key", "response": "Check if an API key is valid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nroute the request to runURI if defined otherwise go to plugIt", "response": "def home(request, hproPk):\n    \"\"\" Route the request to runURI if defined otherwise go to plugIt \"\"\"\n\n    if settings.PIAPI_STANDALONE:\n        return main(request, '', hproPk)\n\n    (plugIt, baseURI, hproject) = getPlugItObject(hproPk)\n    if hproject.runURI:\n        return HttpResponseRedirect(hproject.runURI)\n    else:\n        # Check if a custom url key is used\n        if hasattr(hproject, 'plugItCustomUrlKey') and hproject.plugItCustomUrlKey:\n            return HttpResponseRedirect(reverse('plugIt.views.main', args=(hproject.plugItCustomUrlKey, '')))\n\n        return main(request, '', hproPk)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef api_home(request, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    return render_to_response('plugIt/api.html', {}, context_instance=RequestContext(request))", "response": "Show the home page for the API with all methods"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef api_user(request, userPk, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    if settings.PIAPI_STANDALONE:\n        if not settings.PIAPI_REALUSERS:\n            user = generate_user(pk=userPk)\n            if user is None:\n                return HttpResponseNotFound()\n        else:\n            user = get_object_or_404(DUser, pk=userPk)\n\n        hproject = None\n    else:\n        from users.models import TechUser\n\n        user = get_object_or_404(TechUser, pk=userPk)\n\n        (_, _, hproject) = getPlugItObject(hproPk)\n\n        user.ebuio_member = hproject.isMemberRead(user)\n        user.ebuio_admin = hproject.isMemberWrite(user)\n        user.subscription_labels = _get_subscription_labels(user, hproject)\n\n    retour = {}\n\n    # Append properties for the user data\n    for prop in settings.PIAPI_USERDATA:\n        if hasattr(user, prop):\n            retour[prop] = getattr(user, prop)\n\n    retour['id'] = str(retour['pk'])\n\n    # Append the users organisation and access levels\n    orgas = {}\n    if user:\n        limitedOrgas = []\n\n        if hproject and hproject.plugItLimitOrgaJoinable:\n            # Get List of Plugit Available Orgas first\n            projectOrgaIds = hproject.plugItOrgaJoinable.order_by('name').values_list('pk', flat=True)\n            for (orga, isAdmin) in user.getOrgas(distinct=True):\n                if orga.pk in projectOrgaIds:\n                    limitedOrgas.append((orga, isAdmin))\n        elif hasattr(user, 'getOrgas'):\n            limitedOrgas = user.getOrgas(distinct=True)\n\n        # Create List\n        orgas = [{'id': orga.pk, 'name': orga.name, 'codops': orga.ebu_codops, 'is_admin': isAdmin} for (orga, isAdmin)\n                 in limitedOrgas]\n    retour['orgas'] = orgas\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")", "response": "Return information about an user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn information about a user based on uuid", "response": "def api_user_uuid(request, userUuid, key=None, hproPk=None):\n    \"\"\"Return information about an user based on uuid\"\"\"\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    # From UUID to Pk\n    from users.models import TechUser\n\n    user = get_object_or_404(TechUser, uuid=userUuid)\n\n    (_, _, hproject) = getPlugItObject(hproPk)\n\n    user.ebuio_member = hproject.isMemberRead(user)\n    user.ebuio_admin = hproject.isMemberWrite(user)\n    user.subscription_labels = _get_subscription_labels(user, hproject)\n\n    return api_user(request, user.pk, key, hproPk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef api_subscriptions(request, userPk, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    # From UUID to Pk\n    from users.models import TechUser\n\n    user = get_object_or_404(TechUser, pk=userPk)\n\n    (_, _, hproject) = getPlugItObject(hproPk)\n\n    retour = user.getActiveSubscriptionLabels(hproject)\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")", "response": "Return information about an user based on uuid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_orga(request, orgaPk, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    retour = {}\n\n    if settings.PIAPI_STANDALONE:\n        retour['pk'] = orgaPk\n        if orgaPk == \"-1\":\n            retour['name'] = 'EBU'\n            retour['codops'] = 'zzebu'\n        if orgaPk == \"-2\":\n            retour['name'] = 'RTS'\n            retour['codops'] = 'chrts'\n        if orgaPk == \"-3\":\n            retour['name'] = 'BBC'\n            retour['codops'] = 'gbbbc'\n        if orgaPk == \"-4\":\n            retour['name'] = 'CNN'\n            retour['codops'] = 'uscnn'\n\n    else:\n        from organizations.models import Organization\n\n        orga = get_object_or_404(Organization, pk=orgaPk)\n\n        retour['pk'] = orga.pk\n        retour['name'] = orga.name\n        retour['codops'] = orga.ebu_codops\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")", "response": "Return information about an organization"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_get_project_members(request, key=None, hproPk=True):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    if settings.PIAPI_STANDALONE:\n        if not settings.PIAPI_REALUSERS:\n            users = [generate_user(pk=\"-1\"), generate_user(pk=\"-2\"), generate_user(pk=\"-3\")]\n        else:\n            users = DUser.object.all()\n    else:\n\n        (_, _, hproject) = getPlugItObject(hproPk)\n\n        users = []\n\n        for u in hproject.getMembers():\n            u.ebuio_member = True\n            u.ebuio_admin = hproject.isMemberWrite(u)\n            u.subscription_labels = _get_subscription_labels(u, hproject)\n            users.append(u)\n\n    liste = []\n\n    for u in users:\n\n        retour = {}\n\n        for prop in settings.PIAPI_USERDATA:\n            if hasattr(u, prop):\n                retour[prop] = getattr(u, prop)\n\n        retour['id'] = str(retour['pk'])\n\n        liste.append(retour)\n\n    return HttpResponse(json.dumps({'members': liste}), content_type=\"application/json\")", "response": "Return the list of project members"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of techgroups", "response": "def api_techgroup_list(request, key, hproPk):\n    \"\"\"Return the list of techgroup\"\"\"\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    from users.models import TechGroup\n\n    retour = [{\n        'uuid': t.uuid,\n        'uid': t.uid,\n        'name': t.name,\n    } for t in TechGroup.objects.filter(is_enabled=True)]\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef api_user_techgroup_list(request, userPk, key, hproPk):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    # From UUID to Pk\n    from users.models import TechUser\n\n    user = get_object_or_404(TechUser, pk=userPk)\n\n    retour = [t.uuid for t in user.techgroup_set.filter(is_enabled=True)]\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")", "response": "Return the list of techgroups of a user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generic_send_mail(sender, dests, subject, message, key, origin='', html_message=False):\n\n    # If no EBUIO Mail settings have been set, then no e-mail shall be sent\n    if settings.EBUIO_MAIL_SECRET_KEY and settings.EBUIO_MAIL_SECRET_HASH:\n        headers = {}\n\n        if key:\n            from Crypto.Cipher import AES\n\n            hash_key = hashlib.sha512(key + settings.EBUIO_MAIL_SECRET_HASH).hexdigest()[30:42]\n\n            encrypter = AES.new(((settings.EBUIO_MAIL_SECRET_KEY) * 32)[:32], AES.MODE_CFB, '87447JEUPEBU4hR!')\n            encrypted_key = encrypter.encrypt(hash_key + ':' + key)\n\n            base64_key = base64.urlsafe_b64encode(encrypted_key)\n\n            headers = {'Reply-To': settings.MAIL_SENDER.replace('@', '+' + base64_key + '@')}\n\n        msg = EmailMessage(subject, message, sender, dests, headers=headers)\n        if html_message:\n            msg.content_subtype = \"html\"  # Main content is now text/html\n        msg.send(fail_silently=False)\n\n        try:\n            from main.models import MailSend\n\n            MailSend(dest=','.join(dests), subject=subject, sender=sender, message=message, origin=origin).save()\n\n        except ImportError:\n            pass\n    else:\n        logger.debug(\n            \"E-Mail notification not sent, since no EBUIO_MAIL_SECRET_KEY and EBUIO_MAIL_SECRET_HASH set in settingsLocal.py.\")", "response": "Generic mail sending function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef api_send_mail(request, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    sender = request.POST.get('sender', settings.MAIL_SENDER)\n    dests = request.POST.getlist('dests')\n    subject = request.POST['subject']\n    message = request.POST['message']\n    html_message = request.POST.get('html_message')\n\n    if html_message and html_message.lower() == 'false':\n        html_message = False\n\n    if 'response_id' in request.POST:\n        key = hproPk + ':' + request.POST['response_id']\n    else:\n        key = None\n\n    generic_send_mail(sender, dests, subject, message, key, 'PlugIt API (%s)' % (hproPk or 'StandAlone',), html_message)\n\n    return HttpResponse(json.dumps({}), content_type=\"application/json\")", "response": "Send a email. Posts parameters are used"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef api_orgas(request, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    list_orgas = []\n\n    if settings.PIAPI_STANDALONE:\n        list_orgas = [{'id': -1, 'name': 'EBU', 'codops': 'ZZEBU'},\n                      {'id': -2, 'name': 'RTS', 'codops': 'CHRTS'},\n                      {'id': -3, 'name': 'BBC', 'codops': 'GBEBU'},\n                      {'id': -4, 'name': 'CNN', 'codops': 'USCNN'}]\n\n    else:\n        from organizations.models import Organization\n\n        (_, _, hproject) = getPlugItObject(hproPk)\n\n        if hproject and hproject.plugItLimitOrgaJoinable:\n            orgas = hproject.plugItOrgaJoinable.order_by('name').all()\n        else:\n            orgas = Organization.objects.order_by('name').all()\n\n        list_orgas = [{'id': orga.pk, 'name': orga.name, 'codops': orga.ebu_codops} for orga in orgas]\n\n    retour = {'data': list_orgas}\n\n    return HttpResponse(json.dumps(retour), content_type=\"application/json\")", "response": "Return the list of organizations pk"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef api_ebuio_forum(request, key=None, hproPk=None):\n\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    if settings.PIAPI_STANDALONE:\n        return HttpResponse(json.dumps({'error': 'no-on-ebuio'}), content_type=\"application/json\")\n\n    (_, _, hproject) = getPlugItObject(hproPk)\n\n    error = ''\n\n    subject = request.POST.get('subject')\n    author_pk = request.POST.get('author')\n    message = request.POST.get('message')\n    tags = request.POST.get('tags', '')\n\n    if not subject:\n        error = 'no-subject'\n    if not author_pk:\n        error = 'no-author'\n    else:\n        try:\n            from users.models import TechUser\n\n            author = TechUser.objects.get(pk=author_pk)\n        except TechUser.DoesNotExist:\n            error = 'author-no-found'\n\n    if not message:\n        error = 'no-message'\n\n    if error:\n        return HttpResponse(json.dumps({'error': error}), content_type=\"application/json\")\n\n    # Create the topic\n    from discuss.models import Post, PostTag\n\n    if tags:\n        real_tags = []\n        for tag in tags.split(','):\n            (pt, __) = PostTag.objects.get_or_create(tag=tag)\n            real_tags.append(str(pt.pk))\n\n        tags = ','.join(real_tags)\n\n    post = Post(content_object=hproject, who=author, score=0, title=subject, text=message)\n    post.save()\n\n    from app.tags_utils import update_object_tag\n\n    update_object_tag(post, PostTag, tags)\n\n    post.send_email()\n\n    # Return the URL\n    return HttpResponse(json.dumps({'result': 'ok',\n                                    'url': settings.EBUIO_BASE_URL + reverse('hprojects.views.forum_topic',\n                                                                             args=(hproject.pk, post.pk))}),\n                        content_type=\"application/json\")", "response": "Create a topic on the forum of the ioproject. EBUIo only!"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of topics related to a specific tag for a specific user", "response": "def api_ebuio_forum_get_topics_by_tag_for_user(request, key=None, hproPk=None, tag=None, userPk=None):\n    \"\"\"Return the list of topics using the tag pk\"\"\"\n\n    # Check API key (in order to be sure that we have a valid one and that's correspond to the project\n    if not check_api_key(request, key, hproPk):\n        return HttpResponseForbidden\n\n    if settings.PIAPI_STANDALONE:\n        return HttpResponse(json.dumps({'error': 'no-on-ebuio'}), content_type=\"application/json\")\n\n    # We get the plugit object representing the project\n    (_, _, hproject) = getPlugItObject(hproPk)\n\n    # We get the user and we check his rights\n    author_pk = request.GET.get('u')\n    if author_pk and author_pk.isdigit():\n        try:\n            from users.models import TechUser\n\n            user = TechUser.objects.get(pk=author_pk)\n        except TechUser.DoesNotExist:\n            error = 'user-no-found'\n            user = generate_user(mode='ano')\n    else:\n        user = generate_user(mode='ano')\n\n    if not hproject.discuss_can_display_posts(user):\n        return HttpResponseForbidden\n\n    # Verify the existence of the tag\n    if not tag:\n        raise Http404\n\n    # We get the posts (only topics ones-the parent) related to the project and to the tag.\n    # We dont' take the deleted ones.\n    from discuss.models import Post\n\n    posts = Post.objects.filter(is_deleted=False).filter(object_id=hproPk).filter(tags__tag=tag).order_by('-when')\n\n    # We convert the posts list to json\n    posts_json = [\n        {'id': post.id, 'link': post.discuss_get_forum_topic_link(), 'subject': post.title, 'author': post.who_id,\n         'when': post.when.strftime('%a, %d %b %Y %H:%M GMT'), 'score': post.score,\n         'replies_number': post.direct_subposts_size()} for post in posts]\n\n    return HttpResponse(json.dumps({'data': posts_json}), content_type=\"application/json\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef auto_complete(self, term, state=None, postcode=None, max_results=None):\n        self._validate_state(state)\n        params = {\"term\": term, \"state\": state, \"postcode\": postcode,\n                  \"max_results\": max_results or self.max_results}\n        return self._make_request('/address/autoComplete', params)", "response": "Returns a list of addresses that begin with the given term."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the given address into its individual address fields.", "response": "def parse_address(self, address_line):\n        \"\"\"\n        Parses the given address into it's individual address fields.\n        \"\"\"\n        params = {\"term\": address_line}\n        json = self._make_request('/address/getParsedAddress', params)\n        if json is None:\n            return None\n        return Address.from_json(json)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of valid addresses that are similar to the given term.", "response": "def similar(self, address_line, max_results=None):\n        \"\"\"\n        Gets a list of valid addresses that are similar to the given term, can\n        be used to match invalid addresses to valid addresses.\n        \"\"\"\n        params = {\"term\": address_line,\n                  \"max_results\": max_results or self.max_results}\n        return self._make_request('/address/getSimilar', params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect the underlying websocket to the address and send a handshake and optionally a token packet.", "response": "def connect(self, address, token=None):\n        \"\"\"\n        Connect the underlying websocket to the address,\n        send a handshake and optionally a token packet.\n\n        Returns `True` if connected, `False` if the connection failed.\n\n        :param address: string, `IP:PORT`\n        :param token: unique token, required by official servers,\n                      acquired through utils.find_server()\n        :return: True if connected, False if not\n        \"\"\"\n        if self.connected:\n            self.subscriber.on_connect_error(\n                'Already connected to \"%s\"' % self.address)\n            return False\n\n        self.address = address\n        self.server_token = token\n        self.ingame = False\n\n        self.ws.settimeout(1)\n        self.ws.connect('ws://%s' % self.address, origin='http://agar.io')\n        if not self.connected:\n            self.subscriber.on_connect_error(\n                'Failed to connect to \"%s\"' % self.address)\n            return False\n\n        self.subscriber.on_sock_open()\n        # allow handshake canceling\n        if not self.connected:\n            self.subscriber.on_connect_error(\n                'Disconnected before sending handshake')\n            return False\n\n        self.send_handshake()\n        if self.server_token:\n            self.send_token(self.server_token)\n\n        old_nick = self.player.nick\n        self.player.reset()\n        self.world.reset()\n        self.player.nick = old_nick\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisconnect from the server.", "response": "def disconnect(self):\n        \"\"\"\n        Disconnect from server.\n\n        Closes the websocket, sets `ingame = False`, and emits on_sock_closed.\n        \"\"\"\n        self.ws.close()\n        self.ingame = False\n        self.subscriber.on_sock_closed()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlistens for new messages from the server. Returns on disconnect.", "response": "def listen(self):\n        \"\"\"\n        Set up a quick connection. Returns on disconnect.\n\n        After calling `connect()`, this waits for messages from the server\n        using `select`, and notifies the subscriber of any events.\n        \"\"\"\n        import select\n        while self.connected:\n            r, w, e = select.select((self.ws.sock, ), (), ())\n            if r:\n                self.on_message()\n            elif e:\n                self.subscriber.on_sock_error(e)\n        self.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_message(self, msg=None):\n        if msg is None:\n            try:\n                msg = self.ws.recv()\n            except Exception as e:\n                self.subscriber.on_message_error(\n                    'Error while receiving packet: %s' % str(e))\n                self.disconnect()\n                return False\n\n        if not msg:\n            self.subscriber.on_message_error('Empty message received')\n            return False\n\n        buf = BufferStruct(msg)\n        opcode = buf.pop_uint8()\n        try:\n            packet_name = packet_s2c[opcode]\n        except KeyError:\n            self.subscriber.on_message_error('Unknown packet %s' % opcode)\n            return False\n\n        if not self.ingame and packet_name in ingame_packets:\n            self.subscriber.on_ingame()\n            self.ingame = True\n\n        parser = getattr(self, 'parse_%s' % packet_name)\n        try:\n            parser(buf)\n        except BufferUnderflowError as e:\n            msg = 'Parsing %s packet failed: %s' % (packet_name, e.args[0])\n            self.subscriber.on_message_error(msg)\n\n        if len(buf.buffer) != 0:\n            msg = 'Buffer not empty after parsing \"%s\" packet' % packet_name\n            self.subscriber.on_message_error(msg)\n\n        return packet_name", "response": "Parse a new packet from the websocket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the data to a struct and sends it to the server.", "response": "def send_struct(self, fmt, *data):\n        \"\"\"\n        If connected, formats the data to a struct and sends it to the server.\n        Used internally by all other `send_*()` methods.\n        \"\"\"\n        if self.connected:\n            self.ws.send(struct.pack(fmt, *data))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_token(self, token):\n        self.send_struct('<B%iB' % len(token), 80, *map(ord, token))\n        self.server_token = token", "response": "Send a token to the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_facebook(self, token):\n        self.send_struct('<B%iB' % len(token), 81, *map(ord, token))\n        self.facebook_token = token", "response": "Send the Facebook token to the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_respawn(self):\n        nick = self.player.nick\n        self.send_struct('<B%iH' % len(nick), 0, *map(ord, nick))", "response": "Send a respawn message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_target(self, x, y, cid=0):\n        self.send_struct('<BiiI', 16, int(x), int(y), cid)", "response": "Send a target to all cells."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an explode message to the player.", "response": "def send_explode(self):\n        \"\"\"\n        In earlier versions of the game, sending this caused your cells\n        to split into lots of small cells and die.\n        \"\"\"\n        self.send_struct('<B', 20)\n        self.player.own_ids.clear()\n        self.player.cells_changed()\n        self.ingame = False\n        self.subscriber.on_death()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_slide_id(xmldoc, time_slide, create_new = None, superset_ok = False, nonunique_ok = False):\n\ttry:\n\t\ttisitable = lsctables.TimeSlideTable.get_table(xmldoc)\n\texcept ValueError:\n\t\t# table not found\n\t\tif create_new is None:\n\t\t\traise\n\t\ttisitable = lsctables.New(lsctables.TimeSlideTable)\n\t\txmldoc.childNodes[0].appendChild(tisitable)\n\t# make sure the next_id attribute is correct\n\ttisitable.sync_next_id()\n\t# get the id\n\treturn tisitable.get_time_slide_id(time_slide, create_new = create_new, superset_ok = superset_ok, nonunique_ok = nonunique_ok)", "response": "This function returns the time_slide_id corresponding to the offset vector of the time_slide."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time_slides_vacuum(time_slides, verbose = False):\n\t# convert offsets to deltas\n\ttime_slides = dict((time_slide_id, offsetvect.deltas) for time_slide_id, offsetvect in time_slides.items())\n\tif verbose:\n\t\tprogressbar = ProgressBar(max = len(time_slides))\n\telse:\n\t\tprogressbar = None\n\t# old --> new mapping\n\tmapping = {}\n\t# while there are time slide offset dictionaries remaining\n\twhile time_slides:\n\t\t# pick an ID/offset dictionary pair at random\n\t\tid1, deltas1 = time_slides.popitem()\n\t\t# for every other ID/offset dictionary pair in the time\n\t\t# slides\n\t\tids_to_delete = []\n\t\tfor id2, deltas2 in time_slides.items():\n\t\t\t# if the relative offset dictionaries are\n\t\t\t# equivalent record in the old --> new mapping\n\t\t\tif deltas2 == deltas1:\n\t\t\t\tmapping[id2] = id1\n\t\t\t\tids_to_delete.append(id2)\n\t\tfor id2 in ids_to_delete:\n\t\t\ttime_slides.pop(id2)\n\t\tif progressbar is not None:\n\t\t\tprogressbar.update(progressbar.max - len(time_slides))\n\t# done\n\tdel progressbar\n\treturn mapping", "response": "This function is used to create a dictionary mapping time slide IDs to instrument IDs to offset dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging two lists of offset dictionaries into a single list with no duplicate ( equivalent ) time slides.", "response": "def time_slide_list_merge(slides1, slides2):\n\t\"\"\"\n\tMerges two lists of offset dictionaries into a single list with\n\tno duplicate (equivalent) time slides.\n\t\"\"\"\n\tdeltas1 = set(frozenset(offsetvect1.deltas.items()) for offsetvect1 in slides1)\n\treturn slides1 + [offsetvect2 for offsetvect2 in slides2 if frozenset(offsetvect2.deltas.items()) not in deltas1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_request(req_cat, con, req_str, kwargs):\n        try:\n                kwargs = parse.urlencode(kwargs)    #python3x\n        except:\n                kwargs = urllib.urlencode(kwargs)   #python2x\n        \n        \"\"\"\n        Wrapper to keep TCP connection ESTABLISHED. Rather the connection go to\n        CLOSE_WAIT and raise errors CannotSendRequest or the server reply with\n        empty and it raise BadStatusLine\n        \"\"\"        \n        try:\n            con.request(req_cat, req_str, kwargs)      #send request to facebook graph\n        except httplib.CannotSendRequest:\n            con = create()\n            con.request(req_cat, req_str, kwargs)\n\n        try:\n            res = con.getresponse().read()\t\t   #read response\n        except (IOError, httplib.BadStatusLine):\n            con = create()\n            con.request(req_cat, req_str, kwargs) \n            res = con.getresponse().read()  \n\n        t = type(res)\n        if type(res) == t:\n                res = bytes.decode(res)\n        return json.loads(res)", "response": "Sends a request to the facebook - json graph and returns the facebook - json response converted to python object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef action(route, template='', methods=['GET']):\n    def real_decorator(function):\n        function.pi_api_action = True\n        function.pi_api_route = route\n        function.pi_api_template = template\n        function.pi_api_methods = methods\n\n        if hasattr(function, 'pi_api_crossdomain'):\n            if not function.pi_api_crossdomain_data['methods']:\n                function.pi_api_crossdomain_data['methods'] = methods\n\n            if 'OPTIONS' not in function.pi_api_methods:\n                function.pi_api_methods += ['OPTIONS']\n\n        return function\n    return real_decorator", "response": "Decorator to create an action"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cache(time=0, byUser=None):\n    def real_decorator(function):\n        function.pi_api_cache_time = time\n        function.pi_api_cache_by_user = byUser\n        return function\n    return real_decorator", "response": "Decorator to specify the number of seconds the result should be cached in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a postfix to the file name to make unique.", "response": "def add_unique_postfix(fn):\n    \"\"\"__source__ = 'http://code.activestate.com/recipes/577200-make-unique-file-name/'\"\"\"\n    if not os.path.exists(fn):\n        return fn\n\n    path, name = os.path.split(fn)\n    name, ext = os.path.splitext(name)\n\n    make_fn = lambda i: os.path.join(path, '%s(%d)%s' % (name, i, ext))\n\n    for i in xrange(2, sys.maxint):\n        uni_fn = make_fn(i)\n        if not os.path.exists(uni_fn):\n            return uni_fn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self, *args, **options):\n        database = getattr(\n                settings, 'LIQUIMIGRATE_DATABASE', options['database'])\n\n        try:\n            dbsettings = databases[database]\n        except KeyError:\n            raise CommandError(\"don't know such a connection: %s\" % database)\n\n        verbosity = int(options.get('verbosity'))\n\n        # get driver\n        driver_class = (\n                options.get('driver')\n                or dbsettings.get('ENGINE').split('.')[-1])\n        dbtag, driver, classpath = LIQUIBASE_DRIVERS.get(\n                            driver_class, (None, None, None))\n\n        classpath = options.get('classpath') or classpath\n\n        if driver is None:\n            raise CommandError(\n                \"unsupported db driver '%s'\\n\"\n                \"available drivers: %s\" % (\n                    driver_class, ' '.join(LIQUIBASE_DRIVERS.keys())))\n\n        # command options\n        changelog_file = (\n                options.get('changelog_file')\n                or _get_changelog_file(options['database']))\n        username = options.get('username') or dbsettings.get('USER') or ''\n        password = options.get('password') or dbsettings.get('PASSWORD') or ''\n        url = options.get('url') or _get_url_for_db(dbtag, dbsettings)\n\n        command = options['command']\n        cmdargs = {\n            'jar': LIQUIBASE_JAR,\n            'changelog_file': changelog_file,\n            'username': username,\n            'password': password,\n            'command': command,\n            'driver': driver,\n            'classpath': classpath,\n            'url': url,\n            'args': ' '.join(args),\n        }\n\n        cmdline = \"java -jar %(jar)s --changeLogFile %(changelog_file)s \\\n--username=%(username)s --password=%(password)s \\\n--driver=%(driver)s --classpath=%(classpath)s --url=%(url)s \\\n%(command)s %(args)s\" % (cmdargs)\n\n        if verbosity > 0:\n            print(\"changelog file: %s\" % (changelog_file,))\n            print(\"executing: %s\" % (cmdline,))\n\n        created_models = None   # we dont know it\n\n        if emit_pre_migrate_signal and not options.get('no_signals'):\n            if django_19_or_newer:\n                emit_pre_migrate_signal(\n                        1, options.get('interactive'), database)\n            else:\n                emit_pre_migrate_signal(\n                    created_models, 1, options.get('interactive'), database)\n\n        rc = os.system(cmdline)\n\n        if rc == 0:\n\n            try:\n                if not options.get('no_signals'):\n                    if emit_post_migrate_signal:\n                        if django_19_or_newer:\n                            emit_post_migrate_signal(\n                                0, options.get('interactive'), database)\n                        else:\n                            emit_post_migrate_signal(\n                                created_models, 0,\n                                options.get('interactive'), database)\n                    elif emit_post_sync_signal:\n                        emit_post_sync_signal(\n                                created_models, 0,\n                                options.get('interactive'), database)\n\n                if not django_19_or_newer:\n                    call_command(\n                        'loaddata', 'initial_data', verbosity=1,\n                        database=database)\n            except TypeError:\n                # singledb (1.1 and older)\n                emit_post_sync_signal(\n                        created_models, 0, options.get('interactive'))\n\n                call_command(\n                        'loaddata', 'initial_data', verbosity=0)\n        else:\n            raise CommandError('Liquibase returned an error code %s' % rc)", "response": "Handle liquibase command parameters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading lines of UTF - 8 from the file - like object fp and yield dicts with the key as the key.", "response": "def messages(fp, key='@message'):\n    \"\"\"\n    Read lines of UTF-8 from the file-like object given in ``fp``, with the\n    same fault-tolerance as :function:`tagalog.io.lines`, but instead yield\n    dicts with the line data stored in the key given by ``key`` (default:\n    \"@message\").\n    \"\"\"\n    for line in lines(fp):\n        txt = line.rstrip('\\n')\n        yield {key: txt}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading lines of UTF - 8 from the file - like object fp making sure that the file - like object fp is at most line - buffered.", "response": "def lines(fp):\n    \"\"\"\n    Read lines of UTF-8 from the file-like object given in ``fp``, making sure\n    that when reading from STDIN, reads are at most line-buffered.\n\n    UTF-8 decoding errors are handled silently. Invalid characters are\n    replaced by U+FFFD REPLACEMENT CHARACTER.\n\n    Line endings are normalised to newlines by Python's universal newlines\n    feature.\n\n    Returns an iterator yielding lines.\n    \"\"\"\n    if fp.fileno() == sys.stdin.fileno():\n        close = True\n\n        try: # Python 3\n            fp = open(fp.fileno(), mode='r', buffering=BUF_LINEBUFFERED, errors='replace')\n            decode = False\n        except TypeError:\n            fp = os.fdopen(fp.fileno(), 'rU', BUF_LINEBUFFERED)\n            decode = True\n\n    else:\n        close = False\n\n        try:\n            # only decode if the fp doesn't already have an encoding\n            decode = (fp.encoding != UTF8)\n        except AttributeError:\n            # fp has been opened in binary mode\n            decode = True\n\n    try:\n        while 1:\n            l = fp.readline()\n            if l:\n                if decode:\n                    l = l.decode(UTF8, 'replace')\n                yield l\n            else:\n                break\n    finally:\n        if close:\n            fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npublish a category of objects in the facebook graph.", "response": "def publish(self,  cat, **kwargs):\n                \"\"\"\n                This method is used for creating objects in the facebook graph.\n                The first paramter is \"cat\", the category of publish. In addition to \"cat\"\n                \"id\" must also be passed and is catched by \"kwargs\"\n                \"\"\"\n                res=request.publish_cat1(\"POST\", self.con, self.token,  cat, kwargs)    \n                return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_object(self,  cat, **kwargs):\n                if 'id' not in kwargs.keys():\n                        kwargs['id']=''\n                res=request.get_object_cat1(self.con, self.token, cat,  kwargs)\n                return res", "response": "This method is used for retrieving objects from the facebook API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse for deleting objects from the facebook graph", "response": "def delete(self,  **kwargs):       \n                \"\"\"\n                Used for deleting objects from the facebook graph. Just pass the id of the object to be \n                deleted. But in case of like, have to pass the cat (\"likes\") and object id as a like has no id\n                itself in the facebook graph\n                \"\"\"\n                if 'cat' not in kwargs.keys():\n                        kwargs['cat']=''\n                cat=kwargs['cat']\n                del kwargs['cat']\n                res=request.publish_cat1(\"DELETE\", self.con, self.token,  cat, kwargs)\n                return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of Column elements named name under elem. The name comparison is done with CompareColumnNames().", "response": "def getColumnsByName(elem, name):\n\t\"\"\"\n\tReturn a list of Column elements named name under elem.  The name\n\tcomparison is done with CompareColumnNames().\n\t\"\"\"\n\tname = StripColumnName(name)\n\treturn elem.getElements(lambda e: (e.tagName == ligolw.Column.tagName) and (e.Name == name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstrips the significant portion of a table name according to LIGO LW naming conventions.", "response": "def StripTableName(name):\n\t\"\"\"\n\tReturn the significant portion of a table name according to LIGO LW\n\tnaming conventions.\n\n\tExample:\n\n\t>>> StripTableName(\"sngl_burst_group:sngl_burst:table\")\n\t'sngl_burst'\n\t>>> StripTableName(\"sngl_burst:table\")\n\t'sngl_burst'\n\t>>> StripTableName(\"sngl_burst\")\n\t'sngl_burst'\n\t\"\"\"\n\tif name.lower() != name:\n\t\twarnings.warn(\"table name \\\"%s\\\" is not lower case\" % name)\n\ttry:\n\t\treturn TablePattern.search(name).group(\"Name\")\n\texcept AttributeError:\n\t\treturn name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of Table elements named name under elem. The name comparison is done using CompareTableNames().", "response": "def getTablesByName(elem, name):\n\t\"\"\"\n\tReturn a list of Table elements named name under elem.  The name\n\tcomparison is done using CompareTableNames().\n\t\"\"\"\n\tname = StripTableName(name)\n\treturn elem.getElements(lambda e: (e.tagName == ligolw.Table.tagName) and (e.Name == name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_table(xmldoc, name):\n\ttables = getTablesByName(xmldoc, name)\n\tif len(tables) != 1:\n\t\traise ValueError(\"document must contain exactly one %s table\" % StripTableName(name))\n\treturn tables[0]", "response": "This function returns a Table element from the XML document with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreassign all the IDs of all the tables in the specified element to their corresponding rows.", "response": "def reassign_ids(elem):\n\t\"\"\"\n\tRecurses over all Table elements below elem whose next_id\n\tattributes are not None, and uses the .get_next_id() method of each\n\tof those Tables to generate and assign new IDs to their rows.  The\n\tmodifications are recorded, and finally all ID attributes in all\n\trows of all tables are updated to fix cross references to the\n\tmodified IDs.\n\n\tThis function is used by ligolw_add to assign new IDs to rows when\n\tmerging documents in order to make sure there are no ID collisions.\n\tUsing this function in this way requires the .get_next_id() methods\n\tof all Table elements to yield unused IDs, otherwise collisions\n\twill result anyway.  See the .sync_next_id() method of the Table\n\tclass for a way to initialize the .next_id attributes so that\n\tcollisions will not occur.\n\n\tExample:\n\n\t>>> import ligolw\n\t>>> import lsctables\n\t>>> xmldoc = ligolw.Document()\n\t>>> xmldoc.appendChild(ligolw.LIGO_LW()).appendChild(lsctables.New(lsctables.SnglInspiralTable))\n\t[]\n\t>>> reassign_ids(xmldoc)\n\t\"\"\"\n\tmapping = {}\n\tfor tbl in elem.getElementsByTagName(ligolw.Table.tagName):\n\t\tif tbl.next_id is not None:\n\t\t\ttbl.updateKeyMapping(mapping)\n\tfor tbl in elem.getElementsByTagName(ligolw.Table.tagName):\n\t\ttbl.applyKeyMapping(mapping)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the next_id attribute of all the classes in the list.", "response": "def reset_next_ids(classes):\n\t\"\"\"\n\tFor each class in the list, if the .next_id attribute is not None\n\t(meaning the table has an ID generator associated with it), set\n\t.next_id to 0.  This has the effect of reseting the ID generators,\n\tand is useful in applications that process multiple documents and\n\tadd new rows to tables in those documents.  Calling this function\n\tbetween documents prevents new row IDs from growing continuously\n\tfrom document to document.  There is no need to do this, it's\n\tpurpose is merely aesthetic, but it can be confusing to open a\n\tdocument and find process ID 300 in the process table and wonder\n\twhat happened to the other 299 processes.\n\n\tExample:\n\n\t>>> import lsctables\n\t>>> reset_next_ids(lsctables.TableByName.values())\n\t\"\"\"\n\tfor cls in classes:\n\t\tif cls.next_id is not None:\n\t\t\tcls.set_next_id(type(cls.next_id)(0))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef use_in(ContentHandler):\n\tdef startColumn(self, parent, attrs):\n\t\treturn Column(attrs)\n\n\tdef startStream(self, parent, attrs, __orig_startStream = ContentHandler.startStream):\n\t\tif parent.tagName == ligolw.Table.tagName:\n\t\t\tparent._end_of_columns()\n\t\t\treturn TableStream(attrs).config(parent)\n\t\treturn __orig_startStream(self, parent, attrs)\n\n\tdef startTable(self, parent, attrs):\n\t\treturn Table(attrs)\n\n\tContentHandler.startColumn = startColumn\n\tContentHandler.startStream = startStream\n\tContentHandler.startTable = startTable\n\n\treturn ContentHandler", "response": "Modify ContentHandler to cause it to use the TableColumn and Stream classes defined in this module when parsing XML\n\tdocuments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count(self, value):\n\t\treturn sum(getattr(row, self.Name) == value for row in self.parentNode)", "response": "Return the number of rows with this column equal to value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the smallest index of the row with this column with this value.", "response": "def index(self, value):\n\t\t\"\"\"\n\t\tReturn the smallest index of the row(s) with this column\n\t\tequal to value.\n\t\t\"\"\"\n\t\tfor i in xrange(len(self.parentNode)):\n\t\t\tif getattr(self.parentNode[i], self.Name) == value:\n\t\t\t\treturn i\n\t\traise ValueError(value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef asarray(self):\n\t\t# most codes don't use this feature, this is the only place\n\t\t# numpy is used here, and importing numpy can be\n\t\t# time-consuming, so we derfer the import until needed.\n\t\timport numpy\n\t\ttry:\n\t\t\tdtype = ligolwtypes.ToNumPyType[self.Type]\n\t\texcept KeyError as e:\n\t\t\traise TypeError(\"cannot determine numpy dtype for Column '%s': %s\" % (self.getAttribute(\"Name\"), e))\n\t\treturn numpy.fromiter(self, dtype = dtype)", "response": "Construct a numpy array from this column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove this element from the document tree.", "response": "def unlink(self):\n\t\t\"\"\"\n\t\tBreak internal references within the document tree rooted\n\t\ton this element to promote garbage collection.\n\t\t\"\"\"\n\t\tself._tokenizer = None\n\t\tself._rowbuilder = None\n\t\tsuper(TableStream, self).unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self):\n\t\tnew = copy.copy(self)\n\t\tnew.childNodes = map(copy.copy, self.childNodes)\n\t\tfor child in new.childNodes:\n\t\t\tchild.parentNode = new\n\t\tdel new[:]\n\t\tnew._end_of_columns()\n\t\tnew._end_of_rows()\n\t\treturn new", "response": "Returns a copy of the current Table document subtree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CheckProperties(cls, tagname, attrs):\n\t\treturn tagname == cls.tagName and not CompareTableNames(attrs[u\"Name\"], cls.tableName)", "response": "Checks if the tag name and attributes match the table name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getColumnByName(self, name):\n\t\ttry:\n\t\t\tcol, = getColumnsByName(self, name)\n\t\texcept ValueError:\n\t\t\t# did not find exactly 1 matching child\n\t\t\traise KeyError(name)\n\t\treturn col", "response": "Retrieve and return the Column child element named name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a Column element named name to the table. Returns the new child.", "response": "def appendColumn(self, name):\n\t\t\"\"\"\n\t\tAppend a Column element named \"name\" to the table.  Returns\n\t\tthe new child.  Raises ValueError if the table already has\n\t\ta column by that name, and KeyError if the validcolumns\n\t\tattribute of this table does not contain an entry for a\n\t\tcolumn by that name.\n\n\t\tNote that the name string is assumed to be \"pre-stripped\",\n\t\tthat is it is the significant portion of the elements Name\n\t\tattribute.  The Column element's Name attribute will be\n\t\tconstructed by pre-pending the stripped Table element's\n\t\tname and a colon.\n\n\t\tExample:\n\n\t\t>>> import lsctables\n\t\t>>> process_table = lsctables.New(lsctables.ProcessTable, [])\n\t\t>>> col = process_table.appendColumn(\"program\")\n\t\t>>> col.getAttribute(\"Name\")\n\t\t'process:program'\n\t\t>>> col.Name\n\t\t'program'\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.getColumnByName(name)\n\t\t\t# if we get here the table already has that column\n\t\t\traise ValueError(\"duplicate Column '%s'\" % name)\n\t\texcept KeyError:\n\t\t\tpass\n\t\tcolumn = Column(AttributesImpl({u\"Name\": \"%s:%s\" % (StripTableName(self.tableName), name), u\"Type\": self.validcolumns[name]}))\n\t\tstreams = self.getElementsByTagName(ligolw.Stream.tagName)\n\t\tif streams:\n\t\t\tself.insertBefore(column, streams[0])\n\t\telse:\n\t\t\tself.appendChild(column)\n\t\treturn column"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef appendRow(self, *args, **kwargs):\n\t\trow = self.RowType(*args, **kwargs)\n\t\tself.append(row)\n\t\treturn row", "response": "Append a new row to this table and return it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_column_info(self):\n\t\tdel self.columnnames[:]\n\t\tdel self.columntypes[:]\n\t\tdel self.columnpytypes[:]\n\t\tfor child in self.getElementsByTagName(ligolw.Column.tagName):\n\t\t\tif self.validcolumns is not None:\n\t\t\t\ttry:\n\t\t\t\t\tif self.validcolumns[child.Name] != child.Type:\n\t\t\t\t\t\traise ligolw.ElementError(\"invalid type '%s' for Column '%s' in Table '%s', expected type '%s'\" % (child.Type, child.getAttribute(\"Name\"), self.getAttribute(\"Name\"), self.validcolumns[child.Name]))\n\t\t\t\texcept KeyError:\n\t\t\t\t\traise ligolw.ElementError(\"invalid Column '%s' for Table '%s'\" % (child.getAttribute(\"Name\"), self.getAttribute(\"Name\")))\n\t\t\tif child.Name in self.columnnames:\n\t\t\t\traise ligolw.ElementError(\"duplicate Column '%s' in Table '%s'\" % (child.getAttribute(\"Name\"), self.getAttribute(\"Name\")))\n\t\t\tself.columnnames.append(child.Name)\n\t\t\tself.columntypes.append(child.Type)\n\t\t\ttry:\n\t\t\t\tself.columnpytypes.append(ligolwtypes.ToPyType[child.Type])\n\t\t\texcept KeyError:\n\t\t\t\traise ligolw.ElementError(\"unrecognized Type '%s' for Column '%s' in Table '%s'\" % (child.Type, child.getAttribute(\"Name\"), self.getAttribute(\"Name\")))", "response": "Update the column names columntypes and columnpytypes attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify that the children of the table are valid.", "response": "def _verifyChildren(self, i):\n\t\t\"\"\"\n\t\tUsed for validation during parsing, and additional\n\t\tbook-keeping.  For internal use only.\n\t\t\"\"\"\n\t\tsuper(Table, self)._verifyChildren(i)\n\t\tchild = self.childNodes[i]\n\t\tif child.tagName == ligolw.Column.tagName:\n\t\t\tself._update_column_info()\n\t\telif child.tagName == ligolw.Stream.tagName:\n\t\t\t# require agreement of non-stripped strings\n\t\t\tif child.getAttribute(\"Name\") != self.getAttribute(\"Name\"):\n\t\t\t\traise ligolw.ElementError(\"Stream name '%s' does not match Table name '%s'\" % (child.getAttribute(\"Name\"), self.getAttribute(\"Name\")))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removeChild(self, child):\n\t\tsuper(Table, self).removeChild(child)\n\t\tif child.tagName == ligolw.Column.tagName:\n\t\t\tself._update_column_info()\n\t\treturn child", "response": "Removes a child from this element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the highest-numbered ID in this table, and sets the table's .next_id attribute to the next highest ID in sequence. If the .next_id attribute is already set to a value greater than the highest value found, then it is left unmodified. The return value is the ID identified by this method. If the table's .next_id attribute is None, then this function is a no-op. Note that tables of the same name typically share a common .next_id attribute (it is a class attribute, not an attribute of each instance) so that IDs can be generated that are unique across all tables in the document. Running sync_next_id() on all the tables in a document that are of the same type will have the effect of setting the ID to the next ID higher than any ID in any of those tables. Example: >>> import lsctables >>> tbl = lsctables.New(lsctables.ProcessTable) >>> print tbl.sync_next_id() process:process_id:0", "response": "def sync_next_id(self):\n\t\t\"\"\"\n\t\tDetermines the highest-numbered ID in this table, and sets\n\t\tthe table's .next_id attribute to the next highest ID in\n\t\tsequence.  If the .next_id attribute is already set to a\n\t\tvalue greater than the highest value found, then it is left\n\t\tunmodified.  The return value is the ID identified by this\n\t\tmethod.  If the table's .next_id attribute is None, then\n\t\tthis function is a no-op.\n\n\t\tNote that tables of the same name typically share a common\n\t\t.next_id attribute (it is a class attribute, not an\n\t\tattribute of each instance) so that IDs can be generated\n\t\tthat are unique across all tables in the document.  Running\n\t\tsync_next_id() on all the tables in a document that are of\n\t\tthe same type will have the effect of setting the ID to the\n\t\tnext ID higher than any ID in any of those tables.\n\n\t\tExample:\n\n\t\t>>> import lsctables\n\t\t>>> tbl = lsctables.New(lsctables.ProcessTable)\n\t\t>>> print tbl.sync_next_id()\n\t\tprocess:process_id:0\n\t\t\"\"\"\n\t\tif self.next_id is not None:\n\t\t\tif len(self):\n\t\t\t\tn = max(self.getColumnByName(self.next_id.column_name)) + 1\n\t\t\telse:\n\t\t\t\tn = type(self.next_id)(0)\n\t\t\tif n > self.next_id:\n\t\t\t\tself.set_next_id(n)\n\t\treturn self.next_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the mapping of the row keys in this table.", "response": "def updateKeyMapping(self, mapping):\n\t\t\"\"\"\n\t\tUsed as the first half of the row key reassignment\n\t\talgorithm.  Accepts a dictionary mapping old key --> new\n\t\tkey.  Iterates over the rows in this table, using the\n\t\ttable's next_id attribute to assign a new ID to each row,\n\t\trecording the changes in the mapping.  Returns the mapping.\n\t\tRaises ValueError if the table's next_id attribute is None.\n\t\t\"\"\"\n\t\tif self.next_id is None:\n\t\t\traise ValueError(self)\n\t\ttry:\n\t\t\tcolumn = self.getColumnByName(self.next_id.column_name)\n\t\texcept KeyError:\n\t\t\t# table is missing its ID column, this is a no-op\n\t\t\treturn mapping\n\t\tfor i, old in enumerate(column):\n\t\t\tif old is None:\n\t\t\t\traise ValueError(\"null row ID encountered in Table '%s', row %d\" % (self.getAttribute(\"Name\"), i))\n\t\t\tif old in mapping:\n\t\t\t\tcolumn[i] = mapping[old]\n\t\t\telse:\n\t\t\t\tcolumn[i] = mapping[old] = self.get_next_id()\n\t\treturn mapping"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply the mapping to the keys of the rows in the table.", "response": "def applyKeyMapping(self, mapping):\n\t\t\"\"\"\n\t\tUsed as the second half of the key reassignment algorithm.\n\t\tLoops over each row in the table, replacing references to\n\t\told row keys with the new values from the mapping.\n\t\t\"\"\"\n\t\tfor coltype, colname in zip(self.columntypes, self.columnnames):\n\t\t\tif coltype in ligolwtypes.IDTypes and (self.next_id is None or colname != self.next_id.column_name):\n\t\t\t\tcolumn = self.getColumnByName(colname)\n\t\t\t\tfor i, old in enumerate(column):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tcolumn[i] = mapping[old]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tpass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _part(self, name, func, args, help, **kwargs):\n        while self.argv:\n            arg = self.argv.popleft()\n            if arg == \"-h\" or arg == \"--help\":\n                print(help)\n                return\n            try:\n                argname, argarg = args[arg]\n                kwargs[argname] = argarg and self.argv.popleft() or True\n            except KeyError:\n                raise UnrecognizedArgumentError(name, arg)\n            except IndexError:\n                valids = [\"-s\", \"--sort\", \"-d\", \"--done\", \"-D\", \"--undone\"]\n                if arg not in valids:\n                    raise NotEnoughArgumentsError(name)\n                kwargs[argname] = True\n        func(**kwargs)", "response": "Parses arguments of a single command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rock(self):\n        if not self.argv:\n            self.arg.view()\n        while(self.argv):\n            arg = self.argv.popleft()\n            if arg == \"-h\" or arg == \"--help\":\n                print(\n                    \"\"\"Usage: td [-h (--help)] [-v (--version)] [command]\"\"\"\n                    \"\"\", where [command] is one of:\\n\\n\"\"\"\n                    \"\"\"v (view)\\tChanges the way next output\"\"\"\n                    \"\"\" will look like. See [td v -h].\\n\"\"\"\n                    \"\"\"m (modify)\\tApplies one time changes to\"\"\"\n                    \"\"\" the database. See [td m -h].\\n\"\"\"\n                    \"\"\"o (options)\\tSets persistent options, applied\"\"\"\n                    \"\"\" on every next execution. See [td o -h].\\n\"\"\"\n                    \"\"\"a (add)\\t\\tAdds new item. See [td a -h].\\n\"\"\"\n                    \"\"\"e (edit)\\tEdits existing item. See [td e -h].\\n\"\"\"\n                    \"\"\"r (rm)\\t\\tRemoves existing item. See [td r -h].\\n\"\"\"\n                    \"\"\"d (done)\\tMarks items as done. See [td d -h].\\n\"\"\"\n                    \"\"\"D (undone)\\tMarks items as not done. See [td D -h].\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\tShows this screen.\\n\"\"\"\n                    \"\"\"  -v (--version)Shows version number.\"\"\"\n                )\n            elif arg == \"-v\" or arg == \"--version\":\n                print(\"td :: {}\".format(__version__))\n            elif arg == \"v\" or arg == \"view\":\n                self._part(\"view\", self.arg.view, {\n                    \"--no-color\": (\"nocolor\", False),\n                    \"-s\": (\"sort\", True), \"--sort\": (\"sort\", True),\n                    \"-p\": (\"purge\", False), \"--purge\": (\"purge\", False),\n                    \"-d\": (\"done\", True), \"--done\": (\"done\", True),\n                    \"-D\": (\"undone\", True), \"--undone\": (\"undone\", True)\n                },\n                    \"\"\"Usage: td v [-h (--help)] [command(s)]\"\"\"\n                    \"\"\", where [command(s)] are any of:\\n\\n\"\"\"\n                    \"\"\"-s (--sort) <pattern>\\tSorts the output using\"\"\"\n                    \"\"\" <pattern>.\\n\"\"\"\n                    \"\"\"-p (--purge)\\t\\tHides items marked as done.\\n\"\"\"\n                    \"\"\"-d (--done) <pattern>\\tDisplays items matching\"\"\"\n                    \"\"\" <pattern> as done.\\n\"\"\"\n                    \"\"\"-D (--undone) <pattern>\\tDisplays items matching\"\"\"\n                    \"\"\" <pattern> as not done.\\n\"\"\"\n                    \"\"\"--no-color\\t\\tDo not add color codes to the output.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\t\\tShows this screen.\"\"\"\n                )\n            elif arg == \"m\" or arg == \"modify\":\n                self._part(\"modify\", self.arg.modify, {\n                    \"-s\": (\"sort\", True), \"--sort\": (\"sort\", True),\n                    \"-p\": (\"purge\", False), \"--purge\": (\"purge\", False),\n                    \"-d\": (\"done\", True), \"--done\": (\"done\", True),\n                    \"-D\": (\"undone\", True), \"--undone\": (\"undone\", True)\n                },\n                    \"\"\"Usage: td m [-h (--help)] [command(s)]\"\"\"\n                    \"\"\", where [command(s)] are any of:\\n\\n\"\"\"\n                    \"\"\"-s (--sort) <pattern>\\tSorts database using\"\"\"\n                    \"\"\" <pattern>.\\n\"\"\"\n                    \"\"\"-p (--purge)\\t\\tRemoves items marked as done.\\n\"\"\"\n                    \"\"\"-d (--done) <pattern>\\tMarks items matching\"\"\"\n                    \"\"\" <pattern> as done.\\n\"\"\"\n                    \"\"\"-D (--undone) <pattern>\\tMarks items matching\"\"\"\n                    \"\"\" <pattern> as not done.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\t\\tShows this screen.\"\"\"\n                )\n            elif arg == \"a\" or arg == \"add\":\n                args = dict()\n                if self.argv and self.arg.model.exists(self.argv[0]):\n                    args[\"parent\"] = self.argv.popleft()\n                self._part(\"add\", self.arg.add, {\n                    \"-n\": (\"name\", True), \"--name\": (\"name\", True),\n                    \"-p\": (\"priority\", True), \"--priority\": (\"priority\", True),\n                    \"-c\": (\"comment\", True), \"--comment\": (\"comment\", True)\n                },\n                    \"\"\"Usage: td a [-h (--help)] [parent] [command(s)]\"\"\"\n                    \"\"\", where [command(s)] are any of:\\n\\n\"\"\"\n                    \"\"\"-n (--name) <text>\\t\\tSets item's name.\\n\"\"\"\n                    \"\"\"-p (--priority) <no|name>\\tSets item's priority.\\n\"\"\"\n                    \"\"\"-c (--comment) <text>\\t\\tSets item's comment.\\n\"\"\"\n                    \"\"\"\\nIf [parent] index is specified, new item will\"\"\"\n                    \"\"\" become it's child.\\n\"\"\"\n                    \"\"\"If any of the arguments is omitted,\"\"\"\n                    \"\"\" this command will launch an interactive session\"\"\"\n                    \"\"\" letting the user supply the rest of them.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\t\\t\\tShows this screen.\"\"\",\n                    **args\n                )\n            elif arg == \"e\" or arg == \"edit\":\n                if not self.argv:\n                    raise NotEnoughArgumentsError(\"edit\")\n                args = dict()\n                if self.argv[0] not in [\"-h\", \"--help\"]:\n                    args[\"index\"] = self.argv.popleft()\n                self._part(\"edit\", self.arg.edit, {\n                    \"--parent\": (\"parent\", True),\n                    \"-n\": (\"name\", True), \"--name\": (\"name\", True),\n                    \"-p\": (\"priority\", True), \"--priority\": (\"priority\", True),\n                    \"-c\": (\"comment\", True), \"--comment\": (\"comment\", True)\n                },\n                    \"\"\"Usage: td e [-h (--help)] <index> [command(s)]\"\"\"\n                    \"\"\", where [command(s)] are any of:\\n\\n\"\"\"\n                    \"\"\"--parent <index>\\t\\tChanges item's parent.\\n\"\"\"\n                    \"\"\"-n (--name) <text>\\t\\tChanges item's name.\\n\"\"\"\n                    \"\"\"-p (--priority) <no|name>\\tChanges item's priority.\\n\"\"\"\n                    \"\"\"-c (--comment) <text>\\t\\tChanges item's comment.\\n\"\"\"\n                    \"\"\"\\nIndex argument is required and has to point at\"\"\"\n                    \"\"\" an existing item.\\n\"\"\"\n                    \"\"\"If any of the arguments is omitted, it will launch\"\"\"\n                    \"\"\" an interactive session letting the user supply the\"\"\"\n                    \"\"\" rest of them.\\n\"\"\"\n                    \"\"\"\\nAdditions options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\t\\t\\tShows this screen.\"\"\",\n                    **args\n                )\n            elif arg == \"r\" or arg == \"rm\":\n                args = dict()\n                if not self.argv:\n                    raise NotEnoughArgumentsError(\"rm\")\n                elif self.argv[0] not in [\"-h\", \"--help\"]:\n                    args[\"index\"] = self.argv.popleft()\n                self._part(\"rm\", self.arg.rm, {\n                },\n                    \"\"\"Usage: td r [-h (--help)] <index>\\n\\n\"\"\"\n                    \"\"\"Index argument is required and has to point at\"\"\"\n                    \"\"\" an existing item.\\n\"\"\"\n                    \"\"\"\\nAdditions options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\tShows this screen.\"\"\",\n                    **args\n                )\n            elif arg == \"d\" or arg == \"done\":\n                args = dict()\n                if not self.argv:\n                    raise NotEnoughArgumentsError(\"done\")\n                elif self.argv[0] not in [\"-h\", \"--help\"]:\n                    args[\"index\"] = self.argv.popleft()\n                self._part(\"done\", self.arg.done, {\n                },\n                    \"\"\"Usage: td d [-h (--help)] <index>\\n\\n\"\"\"\n                    \"\"\"Index argument is required and has to point at\"\"\"\n                    \"\"\" an existing item.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\tShows this screen.\"\"\",\n                    **args\n                )\n            elif arg == \"D\" or arg == \"undone\":\n                args = dict()\n                if not self.argv:\n                    raise NotEnoughArgumentsError(\"undone\")\n                elif self.argv[0] not in [\"-h\", \"--help\"]:\n                    args[\"index\"] = self.argv.popleft()\n                self._part(\"undone\", self.arg.undone, {\n                },\n                    \"\"\"Usage: td D [-h (--help)] <index>\\n\\n\"\"\"\n                    \"\"\"Index argument is required and has to point at\"\"\"\n                    \"\"\" an existing item.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\tShows this screen.\"\"\",\n                    **args\n                )\n            elif arg == \"o\" or arg == \"options\":\n                self._part(\"options\", self.arg.options, {\n                    \"-g\": (\"glob\", False), \"--global\": (\"glob\", False),\n                    \"-s\": (\"sort\", True), \"--sort\": (\"sort\", True),\n                    \"-p\": (\"purge\", False), \"--purge\": (\"purge\", False),\n                    \"-d\": (\"done\", True), \"--done\": (\"done\", True),\n                    \"-D\": (\"undone\", True), \"--undone\": (\"undone\", True)\n                },\n                    \"\"\"Usage: td o [-h (--help)] [command(s)]\"\"\"\n                    \"\"\", where [command(s)] are any of:\\n\\n\"\"\"\n                    \"\"\"-g (--global)\\t\\tApply specified options to all\"\"\"\n                    \"\"\" ToDo lists (store in ~/.tdrc).\\n\"\"\"\n                    \"\"\"-s (--sort) <pattern>\\tAlways sorts using\"\"\"\n                    \"\"\" <pattern>.\\n\"\"\"\n                    \"\"\"-p (--purge)\\t\\tAlways removes items marked\"\"\"\n                    \"\"\"as done.\\n\"\"\"\n                    \"\"\"-d (--done) <pattern>\\tAlways marks items maching\"\"\"\n                    \"\"\" <pattern> as done.\\n\"\"\"\n                    \"\"\"-D (--undone) <pattern>\\tAlways marks items maching\"\"\"\n                    \"\"\" <pattern> as not done.\\n\"\"\"\n                    \"\"\"\\nAdditional options:\\n\"\"\"\n                    \"\"\"  -h (--help)\\t\\tShows this screen.\"\"\"\n                )\n            else:\n                raise UnrecognizedCommandError(\"td\", arg)", "response": "Starts and does the parsing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget user input for given field. Can be interrupted with ^C.", "response": "def input(self, field):\n        \"\"\"Gets user input for given field.\n\n        Can be interrupted with ^C.\n\n        :field: Field name.\n        :returns: User input.\n\n        \"\"\"\n        try:\n            desc = Get.TYPES[field]\n            return input(\"{}|{}[{}]> \".format(\n                field, \"-\" * (Get._LEN - len(field) - len(desc)), desc\n            ))\n        except KeyboardInterrupt:\n            print()\n            exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, field, value=None):\n        self.value = value\n        val = self.input(field)\n        if field == 'name':\n            while True:\n                if val != '':\n                    break\n                print(\"Name cannot be empty.\")\n                val = self.input(field)\n        elif field == 'priority':\n            if val == '':  # Use default priority\n                return None\n            while True:\n                if val in Get.PRIORITIES.values():\n                    break\n                c, val = val, Get.PRIORITIES.get(val)\n                if val:\n                    break\n                print(\"Unrecognized priority number or name [{}].\".format(c))\n                val = self.input(field)\n            val = int(val)\n        return val", "response": "Gets user input for given field and checks if it is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a sort pattern and returns a list of tuples.", "response": "def _getPattern(self, ipattern, done=None):\n        \"\"\"Parses sort pattern.\n\n        :ipattern: A pattern to parse.\n        :done:  If :ipattern: refers to done|undone,\n        use this to indicate proper state.\n        :returns: A pattern suitable for Model.modify.\n\n        \"\"\"\n        if ipattern is None:\n            return None\n        if ipattern is True:\n            if done is not None:\n                return ([(None, None, done)], {})\n            # REMEMBER: This False is for sort reverse!\n            return ([(0, False)], {})\n\n        def _getReverse(pm):\n            return pm == '-'\n\n        def _getIndex(k):\n            try:\n                return int(k)\n            except ValueError:\n                raise InvalidPatternError(k, \"Invalid level number\")\n\n        def _getDone(p):\n            v = p.split('=')\n            if len(v) == 2:\n                try:\n                    return (Model.indexes[v[0]], v[1], done)\n                except KeyError:\n                    raise InvalidPatternError(v[0], 'Invalid field name')\n            return (None, v[0], done)\n        ipattern1 = list()\n        ipattern2 = dict()\n        for s in ipattern.split(','):\n            if done is not None:\n                v = done\n            else:\n                v = _getReverse(s[-1])\n            k = s.split(':')\n            if len(k) == 1:\n                if done is not None:\n                    ipattern1.append(_getDone(k[0]))\n                    continue\n                ko = k[0][:-1]\n                try:\n                    if len(k[0]) == 1:\n                        k = 0\n                    else:\n                        k = Model.indexes[ko]\n                except KeyError:\n                    k = _getIndex(k[0][:-1])\n                else:\n                    ipattern1.append((k, v))\n                    continue\n                v = (0, v)\n            elif len(k) == 2:\n                try:\n                    if done is not None:\n                        v = _getDone(k[1])\n                    else:\n                        v = (Model.indexes[k[1][:-1]], v)\n                    k = _getIndex(k[0])\n                except KeyError:\n                    raise InvalidPatternError(k[1][:-1], 'Invalid field name')\n            else:\n                raise InvalidPatternError(s, 'Unrecognized token in')\n            ipattern2.setdefault(k, []).append(v)\n        return (ipattern1, ipattern2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getDone(self, done, undone):\n        if done:\n            return self._getPattern(done, True)\n        if undone:\n            return self._getPattern(undone, False)", "response": "Parses the done | undone state and returns the pattern for done or undone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the v command.", "response": "def view(self, sort=None, purge=False, done=None, undone=None, **kwargs):\n        \"\"\"Handles the 'v' command.\n\n        :sort: Sort pattern.\n        :purge: Whether to purge items marked as 'done'.\n        :done: Done pattern.\n        :undone: Not done pattern.\n        :kwargs: Additional arguments to pass to the View object.\n\n        \"\"\"\n        View(self.model.modify(\n            sort=self._getPattern(sort),\n            purge=purge,\n            done=self._getDone(done, undone)\n        ), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the m command.", "response": "def modify(self, sort=None, purge=False, done=None, undone=None):\n        \"\"\"Handles the 'm' command.\n\n        :sort: Sort pattern.\n        :purge: Whether to purge items marked as 'done'.\n        :done: Done pattern.\n        :undone: Not done pattern.\n\n        \"\"\"\n        self.model.modifyInPlace(\n            sort=self._getPattern(sort),\n            purge=purge,\n            done=self._getDone(done, undone)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, **args):\n        kwargs = self.getKwargs(args)\n        if kwargs:\n            self.model.add(**kwargs)", "response": "Handles the a command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef edit(self, **args):\n        if self.model.exists(args[\"index\"]):\n            values = dict(zip(\n                ['parent', 'name', 'priority', 'comment', 'done'],\n                self.model.get(args[\"index\"])\n            ))\n            kwargs = self.getKwargs(args, values)\n            if kwargs:\n                self.model.edit(args[\"index\"], **kwargs)", "response": "Handles the e command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rm(self, index):\n        if self.model.exists(index):\n            self.model.remove(index)", "response": "Handles the r command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef done(self, index):\n        if self.model.exists(index):\n            self.model.edit(index, done=True)", "response": "Handles the d command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef undone(self, index):\n        if self.model.exists(index):\n            self.model.edit(index, done=False)", "response": "Handles the D command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the o command.", "response": "def options(self, glob=False, **args):\n        \"\"\"Handles the 'o' command.\n\n        :glob: Whether to store specified options globally.\n        :args: Arguments supplied to the 'o' command (excluding '-g').\n\n        \"\"\"\n        kwargs = {}\n        for argname, argarg in args.items():\n            if argname == \"sort\":\n                argarg = self._getPattern(argarg)\n            if argname not in [\"done\", \"undone\"]:\n                kwargs[argname] = argarg\n        if \"done\" in args or \"undone\" in args:\n            kwargs[\"done\"] = self._getDone(\n                args.get(\"done\"), args.get(\"undone\")\n            )\n\n        self.model.setOptions(glob=glob, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary containing the necessary data from user input.", "response": "def getKwargs(self, args, values={}, get=Get()):\n        \"\"\"Gets necessary data from user input.\n\n        :args: Dictionary of arguments supplied in command line.\n        :values: Default values dictionary, supplied for editing.\n        :get: Object used to get values from user input.\n        :returns: A dictionary containing data gathered from user input.\n\n        \"\"\"\n        kwargs = dict()\n        for field in ['name', 'priority', 'comment', 'parent']:\n            fvalue = args.get(field) or get.get(field, values.get(field))\n            if fvalue is not None:\n                kwargs[field] = fvalue\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url(self):\n\t\treturn urlparse.urlunparse((self.scheme, self.host, self.path, None, None, None))", "response": "Returns the URL of the cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef segmentlistdict(self):\n\t\t# the import has to be done here to break the cyclic\n\t\t# dependancy\n\t\tfrom pycbc_glue.ligolw.lsctables import instrument_set_from_ifos\n\t\tinstruments = instrument_set_from_ifos(self.observatory) or (None,)\n\t\treturn segments.segmentlistdict((instrument, segments.segmentlist(self.segment is not None and [self.segment] or [])) for instrument in instruments)", "response": "A segmentlistdict object describing the instruments and the time spanned by this CacheEntry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_T050017(cls, url, coltype = LIGOTimeGPS):\n\t\tmatch = cls._url_regex.search(url)\n\t\tif not match:\n\t\t\traise ValueError(\"could not convert %s to CacheEntry\" % repr(url))\n\t\tobservatory = match.group(\"obs\")\n\t\tdescription = match.group(\"dsc\")\n\t\tstart = match.group(\"strt\")\n\t\tduration = match.group(\"dur\")\n\t\tif start == \"-\" and duration == \"-\":\n\t\t\t# no segment information\n\t\t\tsegment = None\n\t\telse:\n\t\t\tsegment = segments.segment(coltype(start), coltype(start) + coltype(duration))\n\t\treturn cls(observatory, description, segment, url)", "response": "Parse a URL in the style of T050017 - 00 into a CacheEntry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Cache object whose entries are read from a file - like object.", "response": "def fromfile(cls, fileobj, coltype=LIGOTimeGPS):\n\t\t\"\"\"\n\t\tReturn a Cache object whose entries are read from an open file.\n\t\t\"\"\"\n\t\tc = [cls.entry_class(line, coltype=coltype) for line in fileobj]\n\t\treturn cls(c)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromfilenames(cls, filenames, coltype=LIGOTimeGPS):\n\t\tcache = cls()\n\t\tfor filename in filenames:\n\t\t\tcache.extend(cls.fromfile(open(filename), coltype=coltype))\n\t\treturn cache", "response": "Create a new Cache object from the files named and concatenate the results into a single Cache object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_urls(cls, urllist, coltype=LIGOTimeGPS):\n\t\tdef pfn_to_url(url):\n\t\t\tscheme, host, path, dummy, dummy = urlparse.urlsplit(url)\n\t\t\tif scheme == \"\": path = os.path.abspath(path)\n\t\t\treturn urlparse.urlunsplit((scheme or \"file\", host or \"localhost\",\n\t\t\t                            path, \"\", \"\"))\n\t\treturn cls([cls.entry_class.from_T050017(pfn_to_url(f), coltype=coltype) \\\n\t\t            for f in urllist])", "response": "Return a Cache object from a list of URLs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new Cache which has every element of self but without duplication.", "response": "def unique(self):\n\t\t\"\"\"\n\t\tReturn a Cache which has every element of self, but without\n\t\tduplication.  Preserve order.  Does not hash, so a bit slow.\n\t\t\"\"\"\n\t\tnew = self.__class__([])\n\t\tfor elem in self:\n\t\t\tif elem not in new:\n\t\t\t\tnew.append(elem)\n\t\treturn new"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a cache object to filename as a plain text pfn file", "response": "def topfnfile(self, fileobj):\n\t\t\"\"\"\n\t\twrite a cache object to filename as a plain text pfn file\n\t\t\"\"\"\n\t\tfor entry in self:\n\t\t\tprint >>fileobj, entry.path\n\t\tfileobj.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_segmentlistdict(self):\n\t\td = segments.segmentlistdict()\n\t\tfor entry in self:\n\t\t\td |= entry.segmentlistdict\n\t\treturn d", "response": "Return a segmentlistdict object describing the instruments\n\tand times spanned by the entries in this Cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sieve(self, ifos=None, description=None, segment=None,\n\t\tsegmentlist=None, exact_match=False):\n\t\t\"\"\"\n\t\tReturn a Cache object with those CacheEntries that\n\t\tcontain the given patterns (or overlap, in the case of\n\t\tsegment or segmentlist).  If exact_match is True, then\n\t\tnon-None ifos, description, and segment patterns must\n\t\tmatch exactly, and a non-None segmentlist must contain\n\t\ta segment which matches exactly).\n\n\t\tIt makes little sense to specify both segment and\n\t\tsegmentlist arguments, but it is not prohibited.\n\n\t\tBash-style wildcards (*?) are allowed for ifos and description.\n\t\t\"\"\"\n\t\tif exact_match:\n\t\t\tsegment_func = lambda e: e.segment == segment\n\t\t\tsegmentlist_func = lambda e: e.segment in segmentlist\n\t\telse:\n\t\t\tif ifos is not None: ifos = \"*\" + ifos + \"*\"\n\t\t\tif description is not None: description = \"*\" + description + \"*\"\n\t\t\tsegment_func = lambda e: segment.intersects(e.segment)\n\t\t\tsegmentlist_func = lambda e: segmentlist.intersects_segment(e.segment)\n\t\t\n\t\tc = self\n\t\t\n\t\tif ifos is not None:\n\t\t\tifos_regexp = re.compile(fnmatch.translate(ifos))\n\t\t\tc = [entry for entry in c if ifos_regexp.match(entry.observatory) is not None]\n\t\t\n\t\tif description is not None:\n\t\t\tdescr_regexp = re.compile(fnmatch.translate(description))\n\t\t\tc = [entry for entry in c if descr_regexp.match(entry.description) is not None]\n\t\t\n\t\tif segment is not None:\n\t\t\tc = [entry for entry in c if segment_func(entry)]\n\t\t\n\t\tif segmentlist is not None:\n\t\t\t# must coalesce for intersects_segment() to work\n\t\t\tsegmentlist.coalesce()\n\t\t\tc = [entry for entry in c if segmentlist_func(entry)]\n\n\t\treturn self.__class__(c)", "response": "Return a new Cache object with those CacheEntries that contain the given patterns or overlap."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun through the entries of the Cache() object and checks each entry if the file which it points to exists or not. If the file does exist then it adds the entry to the Cache() object containing found files, otherwise it adds the entry to the Cache() object containing all entries that are missing. It returns both in the follwing order: Cache_Found, Cache_Missed. Pass on_missing to control how missing files are handled: \"warn\": print a warning message saying how many files are missing out of the total checked. \"error\": raise an exception if any are missing \"ignore\": do nothing", "response": "def checkfilesexist(self, on_missing=\"warn\"):\n\t\t'''\n\t\tRuns through the entries of the Cache() object and checks each entry\n\t\tif the file which it points to exists or not. If the file does exist then \n\t\tit adds the entry to the Cache() object containing found files, otherwise it\n\t\tadds the entry to the Cache() object containing all entries that are missing. \n\t\tIt returns both in the follwing order: Cache_Found, Cache_Missed.\n\t\t\n\t\tPass on_missing to control how missing files are handled:\n\t\t  \"warn\": print a warning message saying how many files\n\t\t          are missing out of the total checked.\n\t\t  \"error\": raise an exception if any are missing\n\t\t  \"ignore\": do nothing\n\t\t'''  \n\t\tif on_missing not in (\"warn\", \"error\", \"ignore\"):\n\t\t\traise ValueError(\"on_missing must be \\\"warn\\\", \\\"error\\\", or \\\"ignore\\\".\")\n\t\t\n\t\tc_found = []\n\t\tc_missed = []\n\t\tfor entry in self:\n\t\t\tif os.path.isfile(entry.path):\n\t\t\t\tc_found.append(entry)\n\t\t\telse:\n\t\t\t\tc_missed.append(entry)\n\t\t\n\t\tif len(c_missed) > 0:\n\t\t\tmsg = \"%d of %d files in the cache were not found \"\\\n\t\t\t    \"on disk\" % (len(c_missed), len(self))\n\t\t\tif on_missing == \"warn\":\n\t\t\t\tprint >>sys.stderr, \"warning: \" + msg\n\t\t\telif on_missing == \"error\":\n\t\t\t\traise ValueError(msg)\n\t\t\telif on_missing == \"ignore\":\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise ValueError(\"Why am I here? \"\\\n\t\t\t\t      \"Please file a bug report!\")\n\t\treturn self.__class__(c_found), self.__class__(c_missed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ilwdchar_class(tbl_name, col_name, namespace = globals()):\n\t#\n\t# if the class already exists, retrieve and return it\n\t#\n\n\tkey = (str(tbl_name), str(col_name))\n\tcls_name = \"%s_%s_class\" % key\n\tassert cls_name != \"get_ilwdchar_class\"\n\ttry:\n\t\treturn namespace[cls_name]\n\texcept KeyError:\n\t\tpass\n\n\t#\n\t# otherwise define a new class, and add it to the cache\n\t#\n\n\tclass new_class(_ilwd.ilwdchar):\n\t\t__slots__ = ()\n\t\ttable_name, column_name = key\n\t\tindex_offset = len(\"%s:%s:\" % key)\n\n\tnew_class.__name__ = cls_name\n\n\tnamespace[cls_name] = new_class\n\n\t#\n\t# pickle support\n\t#\n\n\tcopy_reg.pickle(new_class, lambda x: (ilwdchar, (unicode(x),)))\n\n\t#\n\t# return the new class\n\t#\n\n\treturn new_class", "response": "Returns the class name of the _ilwd. ilwdchar class that matches the provided table and column names."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to retrieve the username from a variety of sources.", "response": "def get_username():\n\t\"\"\"\n\tTry to retrieve the username from a variety of sources.  First the\n\tenvironment variable LOGNAME is tried, if that is not set the\n\tenvironment variable USERNAME is tried, if that is not set the\n\tpassword database is consulted (only on Unix systems, if the import\n\tof the pwd module succeeds), finally if that fails KeyError is\n\traised.\n\t\"\"\"\n\ttry:\n\t\treturn os.environ[\"LOGNAME\"]\n\texcept KeyError:\n\t\tpass\n\ttry:\n\t\treturn os.environ[\"USERNAME\"]\n\texcept KeyError:\n\t\tpass\n\ttry:\n\t\timport pwd\n\t\treturn pwd.getpwuid(os.getuid())[0]\n\texcept (ImportError, KeyError):\n\t\traise KeyError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nappends a process to the XML document.", "response": "def append_process(xmldoc, program = None, version = None, cvs_repository = None, cvs_entry_time = None, comment = None, is_online = False, jobid = 0, domain = None, ifos = None):\n\t\"\"\"\n\tAdd an entry to the process table in xmldoc.  program, version,\n\tcvs_repository, comment, and domain should all be strings or\n\tunicodes.  cvs_entry_time should be a string or unicode in the\n\tformat \"YYYY/MM/DD HH:MM:SS\".  is_online should be a boolean, jobid\n\tan integer.  ifos should be an iterable (set, tuple, etc.) of\n\tinstrument names.\n\n\tSee also register_to_xmldoc().\n\t\"\"\"\n\ttry:\n\t\tproctable = lsctables.ProcessTable.get_table(xmldoc)\n\texcept ValueError:\n\t\tproctable = lsctables.New(lsctables.ProcessTable)\n\t\txmldoc.childNodes[0].appendChild(proctable)\n\n\tproctable.sync_next_id()\n\n\tprocess = proctable.RowType()\n\tprocess.program = program\n\tprocess.version = version\n\tprocess.cvs_repository = cvs_repository\n\t# FIXME:  remove the \"\" case when the git versioning business is\n\t# sorted out\n\tif cvs_entry_time is not None and cvs_entry_time != \"\":\n\t\ttry:\n\t\t\t# try the git_version format first\n\t\t\tprocess.cvs_entry_time = _UTCToGPS(time.strptime(cvs_entry_time, \"%Y-%m-%d %H:%M:%S +0000\"))\n\t\texcept ValueError:\n\t\t\t# fall back to the old cvs format\n\t\t\tprocess.cvs_entry_time = _UTCToGPS(time.strptime(cvs_entry_time, \"%Y/%m/%d %H:%M:%S\"))\n\telse:\n\t\tprocess.cvs_entry_time = None\n\tprocess.comment = comment\n\tprocess.is_online = int(is_online)\n\tprocess.node = socket.gethostname()\n\ttry:\n\t\tprocess.username = get_username()\n\texcept KeyError:\n\t\tprocess.username = None\n\tprocess.unix_procid = os.getpid()\n\tprocess.start_time = _UTCToGPS(time.gmtime())\n\tprocess.end_time = None\n\tprocess.jobid = jobid\n\tprocess.domain = domain\n\tprocess.instruments = ifos\n\tprocess.process_id = proctable.get_next_id()\n\tproctable.append(process)\n\treturn process"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend process parameters to the XML document tree.", "response": "def append_process_params(xmldoc, process, params):\n\t\"\"\"\n\txmldoc is an XML document tree, process is the row in the process\n\ttable for which these are the parameters, and params is a list of\n\t(name, type, value) tuples one for each parameter.\n\n\tSee also process_params_from_dict(), register_to_xmldoc().\n\t\"\"\"\n\ttry:\n\t\tparamtable = lsctables.ProcessParamsTable.get_table(xmldoc)\n\texcept ValueError:\n\t\tparamtable = lsctables.New(lsctables.ProcessParamsTable)\n\t\txmldoc.childNodes[0].appendChild(paramtable)\n\n\tfor name, typ, value in params:\n\t\trow = paramtable.RowType()\n\t\trow.program = process.program\n\t\trow.process_id = process.process_id\n\t\trow.param = unicode(name)\n\t\tif typ is not None:\n\t\t\trow.type = unicode(typ)\n\t\t\tif row.type not in ligolwtypes.Types:\n\t\t\t\traise ValueError(\"invalid type '%s' for parameter '%s'\" % (row.type, row.param))\n\t\telse:\n\t\t\trow.type = None\n\t\tif value is not None:\n\t\t\trow.value = unicode(value)\n\t\telse:\n\t\t\trow.value = None\n\t\tparamtable.append(row)\n\treturn process"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_process_params(xmldoc, program, param, require_unique_program = True):\n\tprocess_ids = lsctables.ProcessTable.get_table(xmldoc).get_ids_by_program(program)\n\tif len(process_ids) < 1:\n\t\traise ValueError(\"process table must contain at least one program named '%s'\" % program)\n\telif require_unique_program and len(process_ids) != 1:\n\t\traise ValueError(\"process table must contain exactly one program named '%s'\" % program)\n\treturn [row.pyvalue for row in lsctables.ProcessParamsTable.get_table(xmldoc) if (row.process_id in process_ids) and (row.param == param)]", "response": "Returns a list of the values stored in the process_params table for the given program and param."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the process table in xmldoc includes entries for a program.", "response": "def doc_includes_process(xmldoc, program):\n\t\"\"\"\n\tReturn True if the process table in xmldoc includes entries for a\n\tprogram named program.\n\t\"\"\"\n\treturn program in lsctables.ProcessTable.get_table(xmldoc).getColumnByName(u\"program\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_params_from_dict(paramdict):\n\tfor name, values in paramdict.items():\n\t\t# change the name back to the form it had on the command line\n\t\tname = u\"--%s\" % name.replace(\"_\", \"-\")\n\n\t\tif values is True or values is False:\n\t\t\tyield (name, None, None)\n\t\telif values is not None:\n\t\t\tif not isinstance(values, list):\n\t\t\t\tvalues = [values]\n\t\t\tfor value in values:\n\t\t\t\tyield (name, ligolwtypes.FromPyType[type(value)], value)", "response": "This function processes the parameters in the command - line from a dictionary of name value pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters the current process and params to an XML document.", "response": "def register_to_xmldoc(xmldoc, program, paramdict, **kwargs):\n\t\"\"\"\n\tRegister the current process and params to an XML document.\n\tprogram is the name of the program.  paramdict is a dictionary of\n\tname/value pairs that will be used to populate the process_params\n\ttable;  see process_params_from_dict() for information on how these\n\tname/value pairs are interpreted.  Any additional keyword arguments\n\tare passed to append_process().  Returns the new row from the\n\tprocess table.\n\t\"\"\"\n\tprocess = append_process(xmldoc, program = program, **kwargs)\n\tappend_process_params(xmldoc, process, process_params_from_dict(paramdict))\n\treturn process"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister the current process and params to a LDBD client.", "response": "def register_to_ldbd(client, program, paramdict, version = u'0', cvs_repository = u'-', cvs_entry_time = 0, comment = u'-', is_online = False, jobid = 0, domain = None, ifos = u'-'):\n\t\"\"\"\n\tRegister the current process and params to a database via a\n\tLDBDClient.  The program and paramdict arguments and any additional\n\tkeyword arguments are the same as those for register_to_xmldoc().\n\tReturns the new row from the process table.\n\t\"\"\"\n\txmldoc = ligolw.Document()\n\txmldoc.appendChild(ligolw.LIGO_LW())\n\tprocess = register_to_xmldoc(xmldoc, program, paramdict, version = version, cvs_repository = cvs_repository, cvs_entry_time = cvs_entry_time, comment = comment, is_online = is_online, jobid = jobid, domain = domain, ifos = ifos)\n\n\tfake_file = StringIO.StringIO()\n\txmldoc.write(fake_file)\n\tclient.insert(fake_file.getvalue())\n\n\treturn process"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload and render a template using the same context of a specific action.", "response": "def plugitInclude(parser, token):\n    \"\"\"\n        Load and render a template, using the same context of a specific action.\n\n        Example: {% plugitInclude \"/menuBar\" %}\n    \"\"\"\n    bits = token.split_contents()\n\n    if len(bits) != 2:\n        raise TemplateSyntaxError(\"'plugitInclude' tag takes one argument: the tempalte's action to use\")\n\n    action = parser.compile_filter(bits[1])\n\n    return PlugItIncludeNode(action)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall a signal handler to erase temporary scratch files when the process exits.", "response": "def install_signal_trap(signums = (signal.SIGTERM, signal.SIGTSTP), retval = 1):\n\t\"\"\"\n\tInstalls a signal handler to erase temporary scratch files when a\n\tsignal is received.  This can be used to help ensure scratch files\n\tare erased when jobs are evicted by Condor.  signums is a squence\n\tof the signals to trap, the default value is a list of the signals\n\tused by Condor to kill and/or evict jobs.\n\n\tThe logic is as follows.  If the current signal handler is\n\tsignal.SIG_IGN, i.e. the signal is being ignored, then the signal\n\thandler is not modified since the reception of that signal would\n\tnot normally cause a scratch file to be leaked.  Otherwise a signal\n\thandler is installed that erases the scratch files.  If the\n\toriginal signal handler was a Python callable, then after the\n\tscratch files are erased the original signal handler will be\n\tinvoked.  If program control returns from that handler, i.e.  that\n\thandler does not cause the interpreter to exit, then sys.exit() is\n\tinvoked and retval is returned to the shell as the exit code.\n\n\tNote:  by invoking sys.exit(), the signal handler causes the Python\n\tinterpreter to do a normal shutdown.  That means it invokes\n\tatexit() handlers, and does other garbage collection tasks that it\n\tnormally would not do when killed by a signal.\n\n\tNote:  this function will not replace a signal handler more than\n\tonce, that is if it has already been used to set a handler\n\ton a signal then it will be a no-op when called again for that\n\tsignal until uninstall_signal_trap() is used to remove the handler\n\tfrom that signal.\n\n\tNote:  this function is called by get_connection_filename()\n\twhenever it creates a scratch file.\n\t\"\"\"\n\t# NOTE:  this must be called with the temporary_files_lock held.\n\t# ignore signums we've already replaced\n\tsignums = set(signums) - set(origactions)\n\n\tdef temporary_file_cleanup_on_signal(signum, frame):\n\t\twith temporary_files_lock:\n\t\t\ttemporary_files.clear()\n\t\tif callable(origactions[signum]):\n\t\t\t# original action is callable, chain to it\n\t\t\treturn origactions[signum](signum, frame)\n\t\t# original action was not callable or the callable\n\t\t# returned.  invoke sys.exit() with retval as exit code\n\t\tsys.exit(retval)\n\n\tfor signum in signums:\n\t\torigactions[signum] = signal.getsignal(signum)\n\t\tif origactions[signum] != signal.SIG_IGN:\n\t\t\t# signal is not being ignored, so install our\n\t\t\t# handler\n\t\t\tsignal.signal(signum, temporary_file_cleanup_on_signal)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uninstall_signal_trap(signums = None):\n\t# NOTE:  this must be called with the temporary_files_lock held.\n\tif signums is None:\n\t\tsignums = origactions.keys()\n\tfor signum in signums:\n\t\tsignal.signal(signum, origactions.pop(signum))", "response": "Uninstalls the original signal handler for the specified signums."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_temp_store_directory(connection, temp_store_directory, verbose = False):\n\tif temp_store_directory == \"_CONDOR_SCRATCH_DIR\":\n\t\ttemp_store_directory = os.getenv(\"_CONDOR_SCRATCH_DIR\")\n\tif verbose:\n\t\tprint >>sys.stderr, \"setting the temp_store_directory to %s ...\" % temp_store_directory,\n\tcursor = connection.cursor()\n\tcursor.execute(\"PRAGMA temp_store_directory = '%s'\" % temp_store_directory)\n\tcursor.close()\n\tif verbose:\n\t\tprint >>sys.stderr, \"done\"", "response": "Sets the temp_store_directory parameter in sqlite."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discard_connection_filename(filename, working_filename, verbose = False):\n\tif working_filename == filename:\n\t\treturn\n\twith temporary_files_lock:\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"removing '%s' ...\" % working_filename,\n\t\t# remove reference to tempfile.TemporaryFile object\n\t\tdel temporary_files[working_filename]\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"done.\"\n\t\t# if there are no more temporary files in place, remove the\n\t\t# temporary-file signal traps\n\t\tif not temporary_files:\n\t\t\tuninstall_signal_trap()", "response": "This function is used to discard a file from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef idmap_sync(connection):\n\txmldoc = get_xml(connection)\n\tfor tbl in xmldoc.getElementsByTagName(DBTable.tagName):\n\t\ttbl.sync_next_id()\n\txmldoc.unlink()", "response": "Synchronize ID values in the database with the ID values in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a new ID string from the old ID string and the Table instance.", "response": "def idmap_get_new(connection, old, tbl):\n\t\"\"\"\n\tFrom the old ID string, obtain a replacement ID string by either\n\tgrabbing it from the _idmap_ table if one has already been assigned\n\tto the old ID, or by using the current value of the Table\n\tinstance's next_id class attribute.  In the latter case, the new ID\n\tis recorded in the _idmap_ table, and the class attribute\n\tincremented by 1.\n\n\tThis function is for internal use, it forms part of the code used\n\tto re-map row IDs when merging multiple documents.\n\t\"\"\"\n\tcursor = connection.cursor()\n\tcursor.execute(\"SELECT new FROM _idmap_ WHERE old == ?\", (old,))\n\tnew = cursor.fetchone()\n\tif new is not None:\n\t\t# a new ID has already been created for this old ID\n\t\treturn ilwd.ilwdchar(new[0])\n\t# this ID was not found in _idmap_ table, assign a new ID and\n\t# record it\n\tnew = tbl.get_next_id()\n\tcursor.execute(\"INSERT INTO _idmap_ VALUES (?, ?)\", (old, new))\n\treturn new"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef idmap_get_max_id(connection, id_class):\n\tcursor = connection.cursor()\n\tcursor.execute(\"SELECT MAX(CAST(SUBSTR(%s, %d, 10) AS INTEGER)) FROM %s\" % (id_class.column_name, id_class.index_offset + 1, id_class.table_name))\n\tmaxid = cursor.fetchone()[0]\n\tcursor.close()\n\tif maxid is None:\n\t\treturn None\n\treturn id_class(maxid)", "response": "Get the highest ID from the table\n\t for whose IDs that are the class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the table names in the database.", "response": "def get_table_names(connection):\n\t\"\"\"\n\tReturn a list of the table names in the database.\n\t\"\"\"\n\tcursor = connection.cursor()\n\tcursor.execute(\"SELECT name FROM sqlite_master WHERE type == 'table'\")\n\treturn [name for (name,) in cursor]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an in order list of name type tuples describing the the columns in the given table.", "response": "def get_column_info(connection, table_name):\n\t\"\"\"\n\tReturn an in order list of (name, type) tuples describing the\n\tcolumns in the given table.\n\t\"\"\"\n\tcursor = connection.cursor()\n\tcursor.execute(\"SELECT sql FROM sqlite_master WHERE type == 'table' AND name == ?\", (table_name,))\n\tstatement, = cursor.fetchone()\n\tcoldefs = re.match(_sql_create_table_pattern, statement).groupdict()[\"coldefs\"]\n\treturn [(coldef.groupdict()[\"name\"], coldef.groupdict()[\"type\"]) for coldef in re.finditer(_sql_coldef_pattern, coldefs) if coldef.groupdict()[\"name\"].upper() not in (\"PRIMARY\", \"UNIQUE\", \"CHECK\")]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing an XML document tree wrapping around the contents of the database.", "response": "def get_xml(connection, table_names = None):\n\t\"\"\"\n\tConstruct an XML document tree wrapping around the contents of the\n\tdatabase.  On success the return value is a ligolw.LIGO_LW element\n\tcontaining the tables as children.  Arguments are a connection to\n\tto a database, and an optional list of table names to dump.  If\n\ttable_names is not provided the set is obtained from get_table_names()\n\t\"\"\"\n\tligo_lw = ligolw.LIGO_LW()\n\n\tif table_names is None:\n\t\ttable_names = get_table_names(connection)\n\n\tfor table_name in table_names:\n\t\t# build the table document tree.  copied from\n\t\t# lsctables.New()\n\t\ttry:\n\t\t\tcls = TableByName[table_name]\n\t\texcept KeyError:\n\t\t\tcls = DBTable\n\t\ttable_elem = cls(AttributesImpl({u\"Name\": u\"%s:table\" % table_name}), connection = connection)\n\t\tfor column_name, column_type in get_column_info(connection, table_elem.Name):\n\t\t\tif table_elem.validcolumns is not None:\n\t\t\t\t# use the pre-defined column type\n\t\t\t\tcolumn_type = table_elem.validcolumns[column_name]\n\t\t\telse:\n\t\t\t\t# guess the column type\n\t\t\t\tcolumn_type = ligolwtypes.FromSQLiteType[column_type]\n\t\t\ttable_elem.appendChild(table.Column(AttributesImpl({u\"Name\": u\"%s:%s\" % (table_name, column_name), u\"Type\": column_type})))\n\t\ttable_elem._end_of_columns()\n\t\ttable_elem.appendChild(table.TableStream(AttributesImpl({u\"Name\": u\"%s:table\" % table_name, u\"Delimiter\": table.TableStream.Delimiter.default, u\"Type\": table.TableStream.Type.default})))\n\t\tligo_lw.appendChild(table_elem)\n\treturn ligo_lw"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a set of indexes for the database at the given connection.", "response": "def build_indexes(connection, verbose = False):\n\t\"\"\"\n\tUsing the how_to_index annotations in the table class definitions,\n\tconstruct a set of indexes for the database at the given\n\tconnection.\n\t\"\"\"\n\tcursor = connection.cursor()\n\tfor table_name in get_table_names(connection):\n\t\t# FIXME:  figure out how to do this extensibly\n\t\tif table_name in TableByName:\n\t\t\thow_to_index = TableByName[table_name].how_to_index\n\t\telif table_name in lsctables.TableByName:\n\t\t\thow_to_index = lsctables.TableByName[table_name].how_to_index\n\t\telse:\n\t\t\tcontinue\n\t\tif how_to_index is not None:\n\t\t\tif verbose:\n\t\t\t\tprint >>sys.stderr, \"indexing %s table ...\" % table_name\n\t\t\tfor index_name, cols in how_to_index.iteritems():\n\t\t\t\tcursor.execute(\"CREATE INDEX IF NOT EXISTS %s ON %s (%s)\" % (index_name, table_name, \",\".join(cols)))\n\tconnection.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef use_in(ContentHandler):\n\tContentHandler = lsctables.use_in(ContentHandler)\n\n\tdef startTable(self, parent, attrs):\n\t\tname = table.StripTableName(attrs[u\"Name\"])\n\t\tif name in TableByName:\n\t\t\treturn TableByName[name](attrs, connection = self.connection)\n\t\treturn DBTable(attrs, connection = self.connection)\n\n\tContentHandler.startTable = startTable\n\n\treturn ContentHandler", "response": "Modify ContentHandler to cause it to use the DBTable\n\tclass defined in this module when parsing XML documents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remapping_append(self, row):\n\t\tif self.next_id is not None:\n\t\t\t# assign (and record) a new ID before inserting the\n\t\t\t# row to avoid collisions with existing rows\n\t\t\tsetattr(row, self.next_id.column_name, idmap_get_new(self.connection, getattr(row, self.next_id.column_name), self))\n\t\tself._append(row)", "response": "This method is used to re - map a row to a new ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef row_from_cols(self, values):\n\t\trow = self.RowType()\n\t\tfor c, t, v in zip(self.dbcolumnnames, self.dbcolumntypes, values):\n\t\t\tif t in ligolwtypes.IDTypes:\n\t\t\t\tv = ilwd.ilwdchar(v)\n\t\t\tsetattr(row, c, v)\n\t\treturn row", "response": "This is a convenience function for creating a row object from an iterable of values in the order of columns in the database. This is a convenience function for turning the results of the database\n\tqueries into Python objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef applyKeyMapping(self):\n\t\tassignments = \", \".join(\"%s = (SELECT new FROM _idmap_ WHERE old == %s)\" % (colname, colname) for coltype, colname in zip(self.dbcolumntypes, self.dbcolumnnames) if coltype in ligolwtypes.IDTypes and (self.next_id is None or colname != self.next_id.column_name))\n\t\tif assignments:\n\t\t\t# SQLite documentation says ROWID is monotonically\n\t\t\t# increasing starting at 1 for the first row unless\n\t\t\t# it ever wraps around, then it is randomly\n\t\t\t# assigned.  ROWID is a 64 bit integer, so the only\n\t\t\t# way it will wrap is if somebody sets it to a very\n\t\t\t# high number manually.  This library does not do\n\t\t\t# that, so I don't bother checking.\n\t\t\tself.cursor.execute(\"UPDATE %s SET %s WHERE ROWID > %d\" % (self.Name, assignments, self.last_maxrowid))\n\t\t\tself.last_maxrowid = self.maxrowid() or 0", "response": "Applies the key mapping to the table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_dict(self):\n\t\treturn dict((ilwd.ilwdchar(id), offsetvector.offsetvector((instrument, offset) for id, instrument, offset in values)) for id, values in itertools.groupby(self.cursor.execute(\"SELECT time_slide_id, instrument, offset FROM time_slide ORDER BY time_slide_id\"), lambda (id, instrument, offset): id))", "response": "Return a ditionary mapping time slide IDs to offsetvectors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_time_slide_id(self, offsetdict, create_new = None, superset_ok = False, nonunique_ok = False):\n\t\t# look for matching offset vectors\n\t\tif superset_ok:\n\t\t\tids = [id for id, slide in self.as_dict().items() if offsetdict == dict((instrument, offset) for instrument, offset in slide.items() if instrument in offsetdict)]\n\t\telse:\n\t\t\tids = [id for id, slide in self.as_dict().items() if offsetdict == slide]\n\t\tif len(ids) > 1:\n\t\t\t# found more than one\n\t\t\tif nonunique_ok:\n\t\t\t\t# and that's OK\n\t\t\t\treturn ids[0]\n\t\t\t# and that's not OK\n\t\t\traise KeyError(offsetdict)\n\t\tif len(ids) == 1:\n\t\t\t# found one\n\t\t\treturn ids[0]\n\t\t# offset vector not found in table\n\t\tif create_new is None:\n\t\t\t# and that's not OK\n\t\t\traise KeyError(offsetdict)\n\t\t# that's OK, create new vector\n\t\tid = self.get_next_id()\n\t\tfor instrument, offset in offsetdict.items():\n\t\t\trow = self.RowType()\n\t\t\trow.process_id = create_new.process_id\n\t\t\trow.time_slide_id = id\n\t\t\trow.instrument = instrument\n\t\t\trow.offset = offset\n\t\t\tself.append(row)\n\n\t\t# return new ID\n\t\treturn id", "response": "This method returns the time_slide_id corresponding to the offset vector in the given offset dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findCredential():\n\n    # use X509_USER_PROXY from environment if set\n    if os.environ.has_key('X509_USER_PROXY'):\n        filePath = os.environ['X509_USER_PROXY']\n        if validateProxy(filePath):\n            return filePath, filePath\n        else:\n            RFCproxyUsage()\n            sys.exit(1)\n\n    # use X509_USER_CERT and X509_USER_KEY if set\n    if os.environ.has_key('X509_USER_CERT'):\n        if os.environ.has_key('X509_USER_KEY'):\n            certFile = os.environ['X509_USER_CERT']\n            keyFile = os.environ['X509_USER_KEY']\n            return certFile, keyFile\n\n    # search for proxy file on disk\n    uid = os.getuid()\n    path = \"/tmp/x509up_u%d\" % uid\n\n    if os.access(path, os.R_OK):\n        if validateProxy(path):\n            return path, path\n        else:\n            RFCproxyUsage()\n            sys.exit(1)\n\n    # if we get here could not find a credential\n    RFCproxyUsage()\n    sys.exit(1)", "response": "Find a valid proxy credential."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest that the proxy certificate is RFC 3820 compliant and that it is valid for at least 15 minutes.", "response": "def validateProxy(path):\n    \"\"\"\n    Test that the proxy certificate is RFC 3820\n    compliant and that it is valid for at least\n    the next 15 minutes.\n    \"\"\"\n\n    # load the proxy from path\n    try:\n        proxy = M2Crypto.X509.load_cert(path)\n    except Exception, e:\n        msg = \"Unable to load proxy from path %s : %s\" % (path, e)\n        print >>sys.stderr, msg\n        sys.exit(1)\n\n    # make sure the proxy is RFC 3820 compliant\n    # or is an end-entity X.509 certificate\n    try:\n        proxy.get_ext(\"proxyCertInfo\")\n    except LookupError:\n        # it is not an RFC 3820 proxy so check\n        # if it is an old globus legacy proxy\n        subject = proxy.get_subject().as_text()\n        if re.search(r'.+CN=proxy$', subject):\n            # it is so print warning and exit\n            RFCproxyUsage()\n            sys.exit(1)\n\n    # attempt to make sure the proxy is still good for more than 15 minutes\n    try:\n        expireASN1 = proxy.get_not_after().__str__()\n        expireGMT  = time.strptime(expireASN1, \"%b %d %H:%M:%S %Y %Z\")\n        expireUTC  = calendar.timegm(expireGMT)\n        now = int(time.time())\n        secondsLeft = expireUTC - now\n    except Exception, e:\n        # problem getting or parsing time so just let the client\n        # continue and pass the issue along to the server\n        secondsLeft = 3600\n\n    if secondsLeft <= 0:\n        msg = \"\"\"\\\nYour proxy certificate is expired.\n\nPlease generate a new proxy certificate and\ntry again.\n\"\"\"\n        print >>sys.stderr, msg\n        sys.exit(1)\n\n    if secondsLeft < (60 * 15):\n        msg = \"\"\"\\\nYour proxy certificate expires in less than\n15 minutes.\n\nPlease generate a new proxy certificate and\ntry again.\n\"\"\"\n        print >>sys.stderr, msg\n        sys.exit(1)\n\n    # return True to indicate validated proxy\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the customer profile remotely and locally", "response": "def delete(self):\n        \"\"\"Delete the customer profile remotely and locally\"\"\"\n        response = delete_profile(self.profile_id)\n        response.raise_if_error()\n        super(CustomerProfile, self).delete()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_to_server(self, data):\n        output = add_profile(self.customer.pk, data, data)\n        output['response'].raise_if_error()\n        self.profile_id = output['profile_id']\n        self.payment_profile_ids = output['payment_profile_ids']", "response": "Create customer profile on Authorize. NET and store it in self. profile_id and self. payment_profile_ids."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sync(self):\n        output = get_profile(self.profile_id)\n        output['response'].raise_if_error()\n        for payment_profile in output['payment_profiles']:\n            instance, created = CustomerPaymentProfile.objects.get_or_create(\n                customer_profile=self,\n                payment_profile_id=payment_profile['payment_profile_id']\n            )\n            instance.sync(payment_profile)", "response": "Synchronize local customer profile data with remote data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the customer payment profile on Authorize. NET", "response": "def save(self, *args, **kwargs):\n        \"\"\"Sync payment profile on Authorize.NET if sync kwarg is not False\"\"\"\n        if kwargs.pop('sync', True):\n            self.push_to_server()\n        self.card_code = None\n        self.card_number = \"XXXX%s\" % self.card_number[-4:]\n        super(CustomerPaymentProfile, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_to_server(self):\n        if not self.customer_profile_id:\n            try:\n                self.customer_profile = CustomerProfile.objects.get(\n                    customer=self.customer)\n            except CustomerProfile.DoesNotExist:\n                pass\n        if self.payment_profile_id:\n            response = update_payment_profile(\n                self.customer_profile.profile_id,\n                self.payment_profile_id,\n                self.raw_data,\n                self.raw_data,\n            )\n            response.raise_if_error()\n        elif self.customer_profile_id:\n            output = create_payment_profile(\n                self.customer_profile.profile_id,\n                self.raw_data,\n                self.raw_data,\n            )\n            response = output['response']\n            response.raise_if_error()\n            self.payment_profile_id = output['payment_profile_id']\n        else:\n            output = add_profile(\n                self.customer.id,\n                self.raw_data,\n                self.raw_data,\n            )\n            response = output['response']\n            response.raise_if_error()\n            self.customer_profile = CustomerProfile.objects.create(\n                customer=self.customer,\n                profile_id=output['profile_id'],\n                sync=False,\n            )\n            self.payment_profile_id = output['payment_profile_ids'][0]", "response": "Push payment profile to the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sync(self, data):\n        for k, v in data.get('billing', {}).items():\n            setattr(self, k, v)\n        self.card_number = data.get('credit_card', {}).get('card_number',\n                                                           self.card_number)\n        self.save(sync=False)", "response": "Overwrite local customer payment profile data with remote data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self):\n        response = delete_payment_profile(self.customer_profile.profile_id,\n                                          self.payment_profile_id)\n        response.raise_if_error()\n        return super(CustomerPaymentProfile, self).delete()", "response": "Delete the customer payment profile remotely and locally"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget changelog filename for given database", "response": "def get_changelog_file_for_database(database=DEFAULT_DB_ALIAS):\n    \"\"\"get changelog filename for given `database` DB alias\"\"\"\n\n    from django.conf import settings\n\n    try:\n        return settings.LIQUIMIGRATE_CHANGELOG_FILES[database]\n    except AttributeError:\n        if database == DEFAULT_DB_ALIAS:\n            try:\n                return settings.LIQUIMIGRATE_CHANGELOG_FILE\n            except AttributeError:\n                raise ImproperlyConfigured(\n                        'Please set LIQUIMIGRATE_CHANGELOG_FILE or '\n                        'LIQUIMIGRATE_CHANGELOG_FILES in your '\n                        'project settings')\n        else:\n            raise ImproperlyConfigured(\n                'LIQUIMIGRATE_CHANGELOG_FILES dictionary setting '\n                'is required for multiple databases support')\n    except KeyError:\n        raise ImproperlyConfigured(\n            \"Liquibase changelog file is not set for database: %s\" % database)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_target_migration_file(database=DEFAULT_DB_ALIAS, changelog_file=None):\n\n    if not database:\n        database = DEFAULT_DB_ALIAS\n\n    if not changelog_file:\n        changelog_file = get_changelog_file_for_database(database)\n\n    try:\n        doc = minidom.parse(changelog_file)\n    except ExpatError as ex:\n        raise InvalidChangelogFile(\n                'Could not parse XML file %s: %s' % (changelog_file, ex))\n\n    try:\n        dbchglog = doc.getElementsByTagName('databaseChangeLog')[0]\n    except IndexError:\n        raise InvalidChangelogFile(\n            'Missing <databaseChangeLog> node in file %s' % (\n                                                    changelog_file))\n    else:\n        nodes = list(filter(lambda x: x.nodeType is x.ELEMENT_NODE,\n                            dbchglog.childNodes))\n        if not nodes:\n            return changelog_file\n\n        last_node = nodes[-1]\n\n        if last_node.tagName == 'include':\n            last_file = last_node.attributes.get('file').firstChild.data\n            return find_target_migration_file(\n                    database=database, changelog_file=last_file)\n        else:\n            return changelog_file", "response": "Finds best matching target migration file for a given database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an initialized connection object", "response": "def connection(self):\n        \"\"\"Returns an stablished connection\"\"\"\n\n        if self._connection:\n            return self._connection\n\n        self.log.debug('Initializing connection to %s' % (self.bosh_service.\n                                                          netloc))\n        if self.bosh_service.scheme == 'http':\n            Connection = httplib.HTTPConnection\n        elif self.bosh_service.scheme == 'https':\n            Connection = httplib.HTTPSConnection\n        else:\n            # TODO: raise proper exception\n            raise Exception('Invalid URL scheme %s' % self.bosh_service.scheme)\n\n        self._connection = Connection(self.bosh_service.netloc, timeout=10)\n        self.log.debug('Connection initialized')\n        # TODO add exceptions handler there (URL not found etc)\n\n        return self._connection"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request_sid(self):\n        if self._sid:\n            return self._sid\n\n        self.log.debug('Prepare to request BOSH session')\n\n        data = self.send_request(self.get_body(sid_request=True))\n        if not data:\n            return None\n\n        # This is XML. response_body contains the <body/> element of the\n        # response.\n        response_body = ET.fromstring(data)\n\n        # Get the remote Session ID\n        self._sid = response_body.get('sid')\n        self.log.debug('sid = %s' % self._sid)\n\n        # Get the longest time (s) that the XMPP server will wait before\n        # responding to any request.\n        self.server_wait = response_body.get('wait')\n        self.log.debug('wait = %s' % self.server_wait)\n\n        # Get the authid\n        self.authid = response_body.get('authid')\n\n        # Get the allowed authentication methods using xpath\n        search_for = '{{{0}}}features/{{{1}}}mechanisms/{{{2}}}mechanism'.format(\n            JABBER_STREAMS_NS, XMPP_SASL_NS, XMPP_SASL_NS\n        )\n        self.log.debug('Looking for \"%s\" into response body', search_for)\n        mechanisms = response_body.findall(search_for)\n        self.server_auth = []\n\n        for mechanism in mechanisms:\n            self.server_auth.append(mechanism.text)\n            self.log.debug('New AUTH method: %s' % mechanism.text)\n\n        if not self.server_auth:\n            self.log.debug(('The server didn\\'t send the allowed '\n                            'authentication methods'))\n            self._sid = None\n\n        return self._sid", "response": "Request a BOSH session according to the XMPP server and return the new SID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_challenge_response(self, response_plain):\n\n        # Get a basic stanza body\n        body = self.get_body()\n\n        # Create a response tag and add the response content on it\n        #   using base64 encoding\n        response_node = ET.SubElement(body, 'response')\n        response_node.set('xmlns', XMPP_SASL_NS)\n        response_node.text = base64.b64encode(response_plain)\n\n        # Send the challenge response to server\n        resp_root = ET.fromstring(self.send_request(body))\n        return resp_root", "response": "Send a challenge response to the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate_xmpp(self):\n\n        self.request_sid()\n\n        self.log.debug('Prepare the XMPP authentication')\n\n        # Instantiate a sasl object\n        sasl = SASLClient(\n            host=self.to,\n            service='xmpp',\n            username=self.jid,\n            password=self.password\n        )\n\n        # Choose an auth mechanism\n        sasl.choose_mechanism(self.server_auth, allow_anonymous=False)\n\n        # Request challenge\n        challenge = self.get_challenge(sasl.mechanism)\n\n        # Process challenge and generate response\n        response = sasl.process(base64.b64decode(challenge))\n\n        # Send response\n        resp_root = self.send_challenge_response(response)\n\n        success = self.check_authenticate_success(resp_root)\n        if success is None and\\\n                resp_root.find('{{{0}}}challenge'.format(XMPP_SASL_NS)) is not None:\n            resp_root = self.send_challenge_response('')\n            return self.check_authenticate_success(resp_root)\n        return success", "response": "Authenticate the user to the XMPP server via the BOSH connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getParamsByName(elem, name):\n\tname = StripParamName(name)\n\treturn elem.getElements(lambda e: (e.tagName == ligolw.Param.tagName) and (e.Name == name))", "response": "getParamsByName - Get a list of params with name under elem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a new LIGO Light Weight XML Param document subtree.", "response": "def new_param(name, type, value, start = None, scale = None, unit = None, dataunit = None, comment = None):\n\t\"\"\"\n\tConstruct a LIGO Light Weight XML Param document subtree.  FIXME:\n\tdocument keyword arguments.\n\t\"\"\"\n\telem = Param()\n\telem.Name = name\n\telem.Type = type\n\telem.pcdata = value\n\t# FIXME:  I have no idea how most of the attributes should be\n\t# encoded, I don't even know what they're supposed to be.\n\tif dataunit is not None:\n\t\telem.DataUnit = dataunit\n\tif scale is not None:\n\t\telem.Scale = scale\n\tif start is not None:\n\t\telem.Start = start\n\tif unit is not None:\n\t\telem.Unit = unit\n\tif comment is not None:\n\t\telem.appendChild(ligolw.Comment())\n\t\telem.childNodes[-1].pcdata = comment\n\treturn elem"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_param(xmldoc, name):\n\tparams = getParamsByName(xmldoc, name)\n\tif len(params) != 1:\n\t\traise ValueError(\"document must contain exactly one %s param\" % StripParamName(name))\n\treturn params[0]", "response": "Returns the parameter with the given name from the xmldoc. Raises ValueError if there is not exactly one such param."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap for new_param that constructs a Param element from a Python value.", "response": "def from_pyvalue(name, value, **kwargs):\n\t\"\"\"\n\tConvenience wrapper for new_param() that constructs a Param element\n\tfrom an instance of a Python builtin type.  See new_param() for a\n\tdescription of the valid keyword arguments.\n\t\"\"\"\n\treturn new_param(name, ligolwtypes.FromPyType[type(value)], value, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the top - level element of a document sub - tree containing the pickled serialization of a Python object.", "response": "def pickle_to_param(obj, name):\n\t\"\"\"\n\tReturn the top-level element of a document sub-tree containing the\n\tpickled serialization of a Python object.\n\t\"\"\"\n\treturn from_pyvalue(u\"pickle:%s\" % name, unicode(pickle.dumps(obj)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pickle_from_param(elem, name):\n\treturn pickle.loads(str(get_pyvalue(elem, u\"pickle:%s\" % name)))", "response": "Retrieve a pickled Python object from the document tree rooted at\n\telem."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef yaml_to_param(obj, name):\n\treturn from_pyvalue(u\"yaml:%s\" % name, unicode(yaml.dump(obj)))", "response": "Returns the top - level element of a document sub - tree containing the\n\tYAML serialization of a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef use_in(ContentHandler):\n\tdef startParam(self, parent, attrs):\n\t\treturn Param(attrs)\n\n\tContentHandler.startParam = startParam\n\n\treturn ContentHandler", "response": "Modify ContentHandler to cause it to use the Param object defined in this module when parsing XML documents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_connections(self):\n        for i in _xrange(self.num_patterns):\n            for c in self._available_connections[i]:\n                yield c\n            for c in self._in_use_connections[i]:\n                yield c", "response": "Returns a generator over all current connection objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a connection from the pool", "response": "def get_connection(self, command_name, *keys, **options):\n        \"\"\"Get a connection from the pool\"\"\"\n        self._checkpid()\n        try:\n            connection = self._available_connections[self._pattern_idx].pop()\n        except IndexError:\n            connection = self.make_connection()\n        self._in_use_connections[self._pattern_idx].add(connection)\n        self._next_pattern()\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new connection", "response": "def make_connection(self):\n        \"\"\"Create a new connection\"\"\"\n        if self._created_connections[self._pattern_idx] >= self.max_connections_per_pattern:\n            raise ConnectionError(\"Too many connections\")\n        self._created_connections[self._pattern_idx] += 1\n        conn = self.connection_class(**self.patterns[self._pattern_idx])\n        conn._pattern_idx = self._pattern_idx\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef release(self, connection):\n        self._checkpid()\n        if connection.pid == self.pid:\n            idx = connection._pattern_idx\n            self._in_use_connections[idx].remove(connection)\n            self._available_connections[idx].append(connection)", "response": "Releases the connection back to the pool"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the connection from rotation", "response": "def purge(self, connection):\n        \"\"\"Remove the connection from rotation\"\"\"\n        self._checkpid()\n        if connection.pid == self.pid:\n            idx = connection._pattern_idx\n            if connection in self._in_use_connections[idx]:\n                self._in_use_connections[idx].remove(connection)\n            else:\n                self._available_connections[idx].remove(connection)\n            connection.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a command and return a parsed response", "response": "def execute_command(self, *args, **options):\n        \"\"\"Execute a command and return a parsed response\"\"\"\n        pool = self.connection_pool\n        command_name = args[0]\n        for i in _xrange(self.execution_attempts):\n            connection = pool.get_connection(command_name, **options)\n            try:\n                connection.send_command(*args)\n                res = self.parse_response(connection, command_name, **options)\n                pool.release(connection)\n                return res\n            except ConnectionError:\n                pool.purge(connection)\n                if i >= self.execution_attempts - 1:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort_files_by_size(filenames, verbose = False, reverse = False):\n\tif verbose:\n\t\tif reverse:\n\t\t\tprint >>sys.stderr, \"sorting files from largest to smallest ...\"\n\t\telse:\n\t\t\tprint >>sys.stderr, \"sorting files from smallest to largest ...\"\n\treturn sorted(filenames, key = (lambda filename: os.stat(filename)[stat.ST_SIZE] if filename is not None else 0), reverse = reverse)", "response": "Sort a list of filenames in order from smallest file to largest file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef local_path_from_url(url):\n\tif url is None:\n\t\treturn None\n\tscheme, host, path = urlparse.urlparse(url)[:3]\n\tif scheme.lower() not in (\"\", \"file\") or host.lower() not in (\"\", \"localhost\"):\n\t\traise ValueError(\"%s is not a local file\" % repr(url))\n\treturn path", "response": "Extract the filesystem path of the object to which the given URL points to."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_fileobj(fileobj, gz = None, xmldoc = None, contenthandler = None):\n\tfileobj = MD5File(fileobj)\n\tmd5obj = fileobj.md5obj\n\tif gz or gz is None:\n\t\tfileobj = RewindableInputFile(fileobj)\n\t\tmagic = fileobj.read(2)\n\t\tfileobj.seek(0, os.SEEK_SET)\n\t\tif gz or magic == '\\037\\213':\n\t\t\tfileobj = gzip.GzipFile(mode = \"rb\", fileobj = fileobj)\n\tif xmldoc is None:\n\t\txmldoc = ligolw.Document()\n\tligolw.make_parser(contenthandler(xmldoc)).parse(fileobj)\n\treturn xmldoc, md5obj.hexdigest()", "response": "Load the contents of a file object fileobj and return the contents of the file object as a LIGO Light Weight document tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a file identified by filename and return a LIGO Light Weight document tree.", "response": "def load_filename(filename, verbose = False, **kwargs):\n\t\"\"\"\n\tParse the contents of the file identified by filename, and return\n\tthe contents as a LIGO Light Weight document tree.  stdin is parsed\n\tif filename is None.  Helpful verbosity messages are printed to\n\tstderr if verbose is True.  All other keyword arguments are passed\n\tto load_fileobj(), see that function for more information.  In\n\tparticular note that a content handler must be specified.\n\n\tExample:\n\n\t>>> from pycbc_glue.ligolw import ligolw\n\t>>> xmldoc = load_filename(\"demo.xml\", contenthandler = ligolw.LIGOLWContentHandler, verbose = True)\n\t\"\"\"\n\tif verbose:\n\t\tprint >>sys.stderr, \"reading %s ...\" % ((\"'%s'\" % filename) if filename is not None else \"stdin\")\n\tif filename is not None:\n\t\tfileobj = open(filename, \"rb\")\n\telse:\n\t\tfileobj = sys.stdin\n\txmldoc, hexdigest = load_fileobj(fileobj, **kwargs)\n\tif verbose:\n\t\tprint >>sys.stderr, \"md5sum: %s  %s\" % (hexdigest, (filename if filename is not None else \"\"))\n\treturn xmldoc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_url(url, verbose = False, **kwargs):\n\tif verbose:\n\t\tprint >>sys.stderr, \"reading %s ...\" % ((\"'%s'\" % url) if url is not None else \"stdin\")\n\tif url is not None:\n\t\tscheme, host, path = urlparse.urlparse(url)[:3]\n\t\tif scheme.lower() in (\"\", \"file\") and host.lower() in (\"\", \"localhost\"):\n\t\t\tfileobj = open(path)\n\t\telse:\n\t\t\tfileobj = urllib2.urlopen(url)\n\telse:\n\t\tfileobj = sys.stdin\n\txmldoc, hexdigest = load_fileobj(fileobj, **kwargs)\n\tif verbose:\n\t\tprint >>sys.stderr, \"md5sum: %s  %s\" % (hexdigest, (url if url is not None else \"\"))\n\treturn xmldoc", "response": "Load the contents of a file at the given URL and return the contents as a LIGO Light Weight document tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_filename(xmldoc, filename, verbose = False, gz = False, **kwargs):\n\tif verbose:\n\t\tprint >>sys.stderr, \"writing %s ...\" % ((\"'%s'\" % filename) if filename is not None else \"stdout\")\n\tif filename is not None:\n\t\tif not gz and filename.endswith(\".gz\"):\n\t\t\twarnings.warn(\"filename '%s' ends in '.gz' but file is not being gzip-compressed\" % filename, UserWarning)\n\t\tfileobj = open(filename, \"w\")\n\telse:\n\t\tfileobj = sys.stdout\n\thexdigest = write_fileobj(xmldoc, fileobj, gz = gz, **kwargs)\n\tif not fileobj is sys.stdout:\n\t\tfileobj.close()\n\tif verbose:\n\t\tprint >>sys.stderr, \"md5sum: %s  %s\" % (hexdigest, (filename if filename is not None else \"\"))", "response": "Writes the LIGO Light Weight document tree rooted at xmldoc to the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle user input into magic tag processes it and inserts the returned URL into an Element.", "response": "def handleMatch(self, m):\n        \"\"\"\n        Handles user input into [magic] tag, processes it,\n        and inserts the returned URL into an <img> tag\n        through a Python ElementTree <img> Element.\n        \"\"\"\n        userStr = m.group(3)\n        # print(userStr)\n        imgURL = processString(userStr)\n        # print(imgURL)\n        el = etree.Element('img')\n        # Sets imgURL to 'src' attribute of <img> tag element\n        el.set('src', imgURL)       \n        el.set('alt', userStr)\n        el.set('title', userStr)\n        return el"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef home(request):\n\n    polls = []\n\n    for row in curDB.execute('SELECT id, title FROM Poll ORDER BY title'):\n        polls.append({'id': row[0], 'name': row[1]})\n\n    return {'polls': polls}", "response": "Show the home page. Send the list of polls"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows a poll. We send informations about votes only if the user is an administrator", "response": "def show(request, pollId):\n    \"\"\"Show a poll. We send informations about votes only if the user is an administrator\"\"\"\n\n    # Get the poll\n    curDB.execute('SELECT id, title, description FROM Poll WHERE id = ? ORDER BY title', (pollId,))\n    poll = curDB.fetchone()\n\n    if poll is None:\n        return {}\n\n    responses = []\n    totalVotes = 0\n    votedFor = 0\n\n    # Compute the list of responses\n    curDB.execute('SELECT id, title FROM Response WHERE pollId = ? ORDER BY title ', (poll[0],))\n    resps = curDB.fetchall()\n\n    for rowRep in resps:\n\n        votes = []\n        nbVotes = 0\n\n        # List each votes\n        for rowVote in curDB.execute('SELECT username FROM Vote WHERE responseId = ?', (rowRep[0],)):\n\n            nbVotes += 1\n\n            # If the user is and administrator, saves each votes\n            if request.args.get('ebuio_u_ebuio_admin') == 'True':\n                votes.append(rowVote[0])\n\n            # Save the vote of the current suer\n            if request.args.get('ebuio_u_username') == rowVote[0]:\n                votedFor = rowRep[0]\n\n        totalVotes += nbVotes\n        responses.append({'id': rowRep[0], 'title': rowRep[1], 'nbVotes': nbVotes, 'votes': votes})\n\n    return {'id': poll[0], 'name': poll[1], 'description': poll[2], 'responses': responses, 'totalVotes': totalVotes, 'votedFor': votedFor}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vote(request, pollId, responseId):\n\n    username = request.args.get('ebuio_u_username')\n\n    # Remove old votes from the same user on the same poll\n    curDB.execute('DELETE FROM Vote WHERE username = ?  AND responseId IN (SELECT id FROM Response WHERE pollId = ?) ', (username, pollId))\n\n    # Save the vote\n    curDB.execute('INSERT INTO Vote (username, responseID) VALUES (?, ?) ', (username, responseId))\n\n    coxDB.commit()\n\n    # To display a success page, comment this line\n    return PlugItRedirect(\"show/\" + str(pollId))\n\n    # Line to display the success page\n    return {'id': pollId}", "response": "Vote for a poll"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new poll", "response": "def create(request):\n    \"\"\"Create a new poll\"\"\"\n\n    errors = []\n    success = False\n    listOfResponses = ['', '', '']  # 3 Blank lines by default\n    title = ''\n    description = ''\n    id = ''\n\n    if request.method == 'POST':  # User saved the form\n        # Retrieve parameters\n        title = request.form.get('title')\n        description = request.form.get('description')\n\n        listOfResponses = []\n        for rep in request.form.getlist('rep[]'):\n            if rep != '':\n                listOfResponses.append(rep)\n\n        # Test if everything is ok\n        if title == \"\":\n            errors.append(\"Please set a title !\")\n\n        if len(listOfResponses) == 0:\n            errors.append(\"Please set at least one response !\")\n\n        # Can we save the new question ?\n        if len(errors) == 0:\n            # Yes. Let save data\n            curDB.execute(\"INSERT INTO Poll (title, description) VALUES (?, ?)\", (title, description))\n\n            # The id of the poll\n            id = curDB.lastrowid\n\n            # Insert responses\n            for rep in listOfResponses:\n                curDB.execute(\"INSERT INTO Response (pollId, title) VALUES (?, ?)\", (id, rep))\n\n            coxDB.commit()\n\n            success = True\n\n        # Minimum of 3 lines of questions\n        while len(listOfResponses) < 3:\n            listOfResponses.append('')\n\n    return {'errors': errors, 'success': success, 'listOfResponses': listOfResponses, 'title': title, 'description': description, 'id': id}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef New(Type, columns = None, **kwargs):\n\tnew = Type(sax.xmlreader.AttributesImpl({u\"Name\": Type.tableName}), **kwargs)\n\tcolnamefmt = u\":\".join(Type.tableName.split(\":\")[:-1]) + u\":%s\"\n\tif columns is not None:\n\t\tfor key in columns:\n\t\t\tif key not in new.validcolumns:\n\t\t\t\traise ligolw.ElementError(\"invalid Column '%s' for Table '%s'\" % (key, new.tableName))\n\t\t\tnew.appendChild(table.Column(sax.xmlreader.AttributesImpl({u\"Name\": colnamefmt % key, u\"Type\": new.validcolumns[key]})))\n\telse:\n\t\tfor key, value in new.validcolumns.items():\n\t\t\tnew.appendChild(table.Column(sax.xmlreader.AttributesImpl({u\"Name\": colnamefmt % key, u\"Type\": value})))\n\tnew._end_of_columns()\n\tnew.appendChild(table.TableStream(sax.xmlreader.AttributesImpl({u\"Name\": Type.tableName, u\"Delimiter\": table.TableStream.Delimiter.default, u\"Type\": table.TableStream.Type.default})))\n\treturn new", "response": "Construct a pre - defined LSC table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef HasNonLSCTables(elem):\n\treturn any(t.Name not in TableByName for t in elem.getElementsByTagName(ligolw.Table.tagName))", "response": "Return True if the document tree below elem contains non - LSC\n\ttables otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef instrument_set_from_ifos(ifos):\n\tif ifos is None:\n\t\treturn None\n\tif u\",\" in ifos:\n\t\tresult = set(ifo.strip() for ifo in ifos.split(u\",\"))\n\t\tresult.discard(u\"\")\n\t\treturn result\n\tif u\"+\" in ifos:\n\t\tresult = set(ifo.strip() for ifo in ifos.split(u\"+\"))\n\t\tresult.discard(u\"\")\n\t\treturn result\n\tifos = ifos.strip()\n\tif len(ifos) > 2 and not len(ifos) % 2:\n\t\t# if ifos is a string with an even number of characters\n\t\t# greater than two, split it into two-character pieces.\n\t\t# FIXME:  remove this when the inspiral codes don't write\n\t\t# ifos strings like this anymore\n\t\treturn set(ifos[n:n+2] for n in range(0, len(ifos), 2))\n\tif ifos:\n\t\treturn set([ifos])\n\treturn set()", "response": "This function parses the values stored in the ifos column and returns a set containing the values that are found in the columns in the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ifos_from_instrument_set(instruments):\n\tif instruments is None:\n\t\treturn None\n\t_instruments = sorted(set(instrument.strip() for instrument in instruments))\n\t# safety check:  refuse to accept blank names, or names with commas\n\t# or pluses in them as they cannot survive the encode/decode\n\t# process\n\tif not all(_instruments) or any(u\",\" in instrument or u\"+\" in instrument for instrument in _instruments):\n\t\traise ValueError(instruments)\n\tif len(_instruments) == 1 and len(_instruments[0]) > 2 and not len(_instruments[0]) % 2:\n\t\t# special case disambiguation.  FIXME:  remove when\n\t\t# everything uses the comma-delimited encoding\n\t\treturn u\"%s,\" % _instruments[0]\n\treturn u\",\".join(_instruments)", "response": "This function converts an iterable of instrument names into a value suitable for writing into the output file. The input is a single string containing the unique instrument names in the input. The output is a single string containing the value of the last possible entry in the output file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef use_in(ContentHandler):\n\tContentHandler = table.use_in(ContentHandler)\n\n\tdef startTable(self, parent, attrs, __orig_startTable = ContentHandler.startTable):\n\t\tname = table.StripTableName(attrs[u\"Name\"])\n\t\tif name in TableByName:\n\t\t\treturn TableByName[name](attrs)\n\t\treturn __orig_startTable(self, parent, attrs)\n\n\tContentHandler.startTable = startTable\n\n\treturn ContentHandler", "response": "Modify ContentHandler to cause it to use the Table\n\tclasses defined in this module when parsing XML documents."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a set containing the process IDs from rows whose program string equals the given program.", "response": "def get_ids_by_program(self, program):\n\t\t\"\"\"\n\t\tReturn a set containing the process IDs from rows whose\n\t\tprogram string equals the given program.\n\t\t\"\"\"\n\t\treturn set(row.process_id for row in self if row.program == program)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_in_segmentlistdict(self, process_ids = None):\n\t\tseglists = segments.segmentlistdict()\n\t\tfor row in self:\n\t\t\tifos = row.instruments or (None,)\n\t\t\tif process_ids is None or row.process_id in process_ids:\n\t\t\t\tseglists.extend(dict((ifo, segments.segmentlist([row.in_segment])) for ifo in ifos))\n\t\treturn seglists", "response": "Return a segmentlistdict mapping instrument to in segmentlist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_out_segmentlistdict(self, process_ids = None):\n\t\tseglists = segments.segmentlistdict()\n\t\tfor row in self:\n\t\t\tifos = row.instruments or (None,)\n\t\t\tif process_ids is None or row.process_id in process_ids:\n\t\t\t\tseglists.extend(dict((ifo, segments.segmentlist([row.out_segment])) for ifo in ifos))\n\t\treturn seglists", "response": "Return a segmentlistdict mapping instrument to out segmentlist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_expr_id(self, search_group, search, lars_id, instruments, gps_start_time, gps_end_time, comments = None):\n\t\t# create string from instrument set\n\t\tinstruments = ifos_from_instrument_set(instruments)\n\n\t\t# look for the ID\n\t\tfor row in self:\n\t\t\tif (row.search_group, row.search, row.lars_id, row.instruments, row.gps_start_time, row.gps_end_time, row.comments) == (search_group, search, lars_id, instruments, gps_start_time, gps_end_time, comments):\n\t\t\t\t# found it\n\t\t\t\treturn row.experiment_id\n\n\t\t# experiment not found in table\n\t\treturn None", "response": "Return the expr_def_id for the row in the table whose\n\tvalues match the givens."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_new_expr_id(self, search_group, search, lars_id, instruments, gps_start_time, gps_end_time, comments = None):\n\t\t\n\t\t# check if id already exists\n\t\tcheck_id = self.get_expr_id( search_group, search, lars_id, instruments, gps_start_time, gps_end_time, comments = comments )\n\t\tif check_id:\n\t\t\treturn check_id\n\n\t\t# experiment not found in table\n\t\trow = self.RowType()\n\t\trow.experiment_id = self.get_next_id()\n\t\trow.search_group = search_group\n\t\trow.search = search\n\t\trow.lars_id = lars_id\n\t\trow.instruments = ifos_from_instrument_set(instruments)\n\t\trow.gps_start_time = gps_start_time\n\t\trow.gps_end_time = gps_end_time\n\t\trow.comments = comments\n\t\tself.append(row)\n\n\t\t# return new ID\n\t\treturn row.experiment_id", "response": "This function creates a new ID for the given arguments and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns row in matching the given experiment_id.", "response": "def get_row_from_id(self, experiment_id):\n\t\t\"\"\"\n\t\tReturns row in matching the given experiment_id.\n\t\t\"\"\"\n\t\trow = [row for row in self if row.experiment_id == experiment_id]\n\t\tif len(row) > 1:\n\t\t\traise ValueError(\"duplicate ids in experiment table\")\n\t\tif len(row) == 0:\n\t\t\traise ValueError(\"id '%s' not found in table\" % experiment_id)\n\n\t\treturn row[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_id_dict(self):\n\t\td = {}\n\t\tfor row in self:\n\t\t\tif row.experiment_id not in d:\n\t\t\t\td[row.experiment_id] = {}\n\t\t\tif (row.time_slide_id, row.veto_def_name, row.datatype, row.sim_proc_id) in d[row.experiment_id]:\n\t\t\t\t# entry already exists, raise error\n\t\t\t\traise KeyError(\"duplicate entries in experiment_summary table\")\n\t\t\td[row.experiment_id][(row.time_slide_id, row.veto_def_name, row.datatype, row.sim_proc_id)] = row.experiment_summ_id\n\n\t\treturn d", "response": "Return table as a dictionary mapping experiment_id time_slide_id veto_def_name and sim_proc_id to expr_summ_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_expr_summ_id(self, experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id = None):\n\n\t\t# look for the ID\n\t\tfor row in self:\n\t\t\tif (row.experiment_id, row.time_slide_id, row.veto_def_name, row.datatype, row.sim_proc_id) == (experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id):\n\t\t\t\t# found it\n\t\t\t\treturn row.experiment_summ_id\n\n\t\t# if get to here, experiment not found in table\n\t\treturn None", "response": "returns the expr_summ_id for the row in the table whose experiment_id time_slide_id veto_def_name and datatype match the given."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites an entry to the experiment_summ table.", "response": "def write_experiment_summ(self, experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id = None ):\n\t\t\"\"\"\n\t\tWrites a single entry to the experiment_summ table. This can be used\n\t\tfor either injections or non-injection experiments. However, it is\n\t\trecommended that this only be used for injection experiments; for\n\t\tnon-injection experiments write_experiment_summ_set should be used to\n\t\tensure that an entry gets written for every time-slide performed.\n\t\t\"\"\"\n\t\t# check if entry alredy exists; if so, return value\n\t\tcheck_id = self.get_expr_summ_id(experiment_id, time_slide_id, veto_def_name, datatype, sim_proc_id = sim_proc_id)\n\t\tif check_id:\n\t\t\treturn check_id\n\n\t\trow = self.RowType()\n\t\trow.experiment_summ_id = self.get_next_id()\n\t\trow.experiment_id = experiment_id\n\t\trow.time_slide_id = time_slide_id\n\t\trow.veto_def_name = veto_def_name\n\t\trow.datatype = datatype\n\t\trow.sim_proc_id = sim_proc_id\n\t\trow.nevents = None\n\t\trow.duration = None\n\t\tself.append(row)\n\t\t\n\t\treturn row.experiment_summ_id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_non_injection_summary(self, experiment_id, time_slide_dict, veto_def_name, write_all_data = True, write_playground = True, write_exclude_play = True, return_dict = False):\n\t\tfor slide_id in time_slide_dict:\n\t\t\t# check if it's zero_lag or not\n\t\t\tif not any( time_slide_dict[slide_id].values() ):\n\t\t\t\tif write_all_data:\n\t\t\t\t\tself.write_experiment_summ( experiment_id, slide_id, veto_def_name, 'all_data', sim_proc_id = None )\n\t\t\t\tif write_playground:\n\t\t\t\t\tself.write_experiment_summ( experiment_id, slide_id, veto_def_name, 'playground', sim_proc_id = None )\n\t\t\t\tif write_exclude_play:\n\t\t\t\t\tself.write_experiment_summ( experiment_id, slide_id, veto_def_name, 'exclude_play', sim_proc_id = None )\n\t\t\telse:\n\t\t\t\tself.write_experiment_summ( experiment_id, slide_id, veto_def_name, 'slide', sim_proc_id = None )\n\n\t\tif return_dict:\n\t\t\treturn self.as_id_dict()", "response": "Method for writing a new set of non - injection experiments to the experiment\n\tsummary table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds num_events to the nevents column in a specific entry in the table.", "response": "def add_nevents(self, experiment_summ_id, num_events, add_to_current = True):\n\t\t\"\"\"\n\t\tAdd num_events to the nevents column in a specific entry in the table. If\n\t\tadd_to_current is set to False, will overwrite the current nevents entry in\n\t\tthe row with num_events. Otherwise, default is to add num_events to\n\t\tthe current value.\n\n\t\tNote: Can subtract events by passing a negative number to num_events.\n\t\t\"\"\"\n\t\tfor row in self:\n\t\t\tif row.experiment_summ_id != experiment_summ_id:\n\t\t\t\tcontinue\n\t\t\tif row.nevents is None:\n\t\t\t\trow.nevents = 0\n\t\t\tif add_to_current:\n\t\t\t\trow.nevents += num_events\n\t\t\t\treturn row.nevents\n\t\t\telse:\n\t\t\t\trow.nevents = num_events\n\t\t\t\treturn row.nevents\n\t\t\t\t\n\t\t# if get to here, couldn't find experiment_summ_id in the table\n\t\traise ValueError(\"'%s' could not be found in the table\" % (str(experiment_summ_id)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_experiment_summ_ids( self, coinc_event_id ):\n\t\texperiment_summ_ids = []\n\t\tfor row in self:\n\t\t\tif row.coinc_event_id == coinc_event_id:\n\t\t\t\texperiment_summ_ids.append(row.experiment_summ_id)\n\t\tif len(experiment_summ_ids) == 0:\n\t\t\traise ValueError(\"'%s' could not be found in the experiment_map table\" % coinc_event_id)\n\t\treturn experiment_summ_ids", "response": "Get all the experiment_summ_ids that map to a given coinc_event_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_column(self, column):\n\t\tif column.lower() == 'q':\n\t\t\treturn self.get_q\n\t\telse:\n\t\t\treturn self.getColumnByName(column).asarray()", "response": "returns the values of each row in the table\n\t"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a SnglInspiralTable with rows from self having IFO equal to the given ifo.", "response": "def ifocut(self, ifo, inplace=False):\n\t\t\"\"\"\n\t\tReturn a SnglInspiralTable with rows from self having IFO equal\n\t\tto the given ifo. If inplace, modify self directly, else create\n\t\ta new table and fill it.\n\t\t\"\"\"\n\t\tif inplace:\n\t\t\titerutils.inplace_filter(lambda row: row.ifo == ifo, self)\n\t\t\treturn self\n\t\telse:\n\t\t\tifoTrigs = self.copy()\n\t\t\tifoTrigs.extend([row for row in self if row.ifo == ifo])\n\t\t\treturn ifoTrigs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getslide(self,slide_num):\n\t\tslideTrigs = self.copy()\n\t\tslideTrigs.extend(row for row in self if row.get_slide_number() == slide_num)\n\t\treturn slideTrigs", "response": "get_slide - Returns the triggers with a specific slide number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_id_parts(self):\n\t\tint_event_id = int(self.event_id)\n\t\ta = int_event_id // 1000000000\n\t\tslidenum = (int_event_id % 1000000000) // 100000\n\t\tb = int_event_id % 100000\n\t\treturn int(a), int(slidenum), int(b)", "response": "Return the three pieces of the int_8s - style sngl_inspiral\n\tevent_id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the slide - number for this trigger", "response": "def get_slide_number(self):\n\t\t\"\"\"\n\t\tReturn the slide-number for this trigger\n\t\t\"\"\"\n\t\ta, slide_number, b = self.get_id_parts()\n\t\tif slide_number > 5000:\n\t\t\tslide_number = 5000 - slide_number\n\t\treturn slide_number"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_null_snr(self):\n\t\tnull_snr_sq = self.get_coinc_snr()**2 - self.get_column('snr')**2\n\t\tnull_snr_sq[null_snr_sq < 0] = 0.\n\t\treturn null_snr_sq**(1./2.)", "response": "Get the coherent Null SNR for each row in the table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of single - detector sigmas for each row in the table.", "response": "def get_sigmasqs(self, instruments=None):\n\t\t\"\"\"\n\t\tReturn dictionary of single-detector sigmas for each row in the\n\t\ttable.\n\t\t\"\"\"\n\t\tif len(self):\n\t\t\tif not instruments:\n\t\t\t\tinstruments = map(str, \\\n\t\t\t\t\tinstrument_set_from_ifos(self[0].ifos))\n\t\t\treturn dict((ifo, self.get_sigmasq(ifo))\\\n\t\t\t\t    for ifo in instruments)\n\t\telse:\n\t\t\treturn dict()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sngl_snrs(self, instruments=None):\n\t\tif len(self) and instruments is None:\n\t\t\tinstruments = map(str, \\\n\t\t\t                instrument_set_from_ifos(self[0].ifos))\n\t\telif instruments is None:\n\t\t\tinstruments = []\n\t\treturn dict((ifo, self.get_sngl_snr(ifo))\\\n\t\t            for ifo in instruments)", "response": "Get the single - detector SNRs for each row in the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the single - detector chisqs for each row in the table.", "response": "def get_sngl_chisqs(self, instruments=None):\n\t\t\"\"\"\n\t\tGet the single-detector \\chi^2 for each row in the table.\n\t\t\"\"\"\n\t\tif len(self) and instruments is None:\n\t\t\tinstruments = map(str, \\\n\t\t\t                instrument_set_from_ifos(self[0].ifos))\n\t\telif instruments is None:\n\t\t\tinstruments = []\n\t\treturn dict((ifo, self.get_sngl_chisq(ifo))\\\n\t\t            for ifo in instruments)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the single - detector chisqs for each row in the table.", "response": "def get_sngl_bank_chisqs(self, instruments=None):\n\t\t\"\"\"\n\t\tGet the single-detector \\chi^2 for each row in the table.\n\t\t\"\"\"\n\t\tif len(self) and instruments is None:\n\t\t\tinstruments = map(str, \\\n\t\t\t                instrument_set_from_ifos(self[0].ifos))\n\t\telif instruments is None:\n\t\t\tinstruments = []\n\t\treturn dict((ifo, self.get_sngl_bank_chisq(ifo))\\\n\t\t            for ifo in instruments)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the single - detector chisq for each row in the table.", "response": "def get_sngl_cont_chisqs(self, instruments=None):\n\t\t\"\"\"\n\t\tGet the single-detector \\chi^2 for each row in the table.\n\t\t\"\"\"\n\t\tif len(self) and instruments is None:\n\t\t\tinstruments = map(str, \\\n\t\t\t                instrument_set_from_ifos(self[0].ifos))\n\t\telif instruments is None:\n\t\t\tinstruments = []\n\t\treturn dict((ifo, self.get_sngl_cont_chisq(ifo))\\\n\t\t            for ifo in instruments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bestnr(self, index=4.0, nhigh=3.0, null_snr_threshold=4.25,\\\n\t\t           null_grad_thresh=20., null_grad_val = 1./5.):\n\t\t\"\"\"\n\t\tGet the BestNR statistic for each row in the table\n\t\t\"\"\"\n\t\treturn [row.get_bestnr(index=index, nhigh=nhigh,\n\t\t                       null_snr_threshold=null_snr_threshold,\n\t\t                       null_grad_thresh=null_grad_thresh,\n\t\t                       null_grad_val=null_grad_val)\n\t\t        for row in self]", "response": "Get the BestNR statistic for each row in the table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_null_snr(self):\n\t\tnull_snr_sq = (numpy.asarray(self.get_sngl_snrs().values())**2)\\\n                             .sum() - self.snr**2\n\t\tif null_snr_sq < 0:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn null_snr_sq**(1./2.)", "response": "Get the coherent Null SNR for this row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sngl_snrs(self):\n\t\treturn dict((ifo, self.get_sngl_snr(ifo)) for ifo in\\\n                            instrument_set_from_ifos(self.ifos))", "response": "Return a dictionary of single - detector SNRs for this row."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bestnr(self, index=4.0, nhigh=3.0, null_snr_threshold=4.25,\\\n\t\t           null_grad_thresh=20., null_grad_val = 1./5.):\n\t\t\"\"\"\n\t\tReturn the BestNR statistic for this row.\n\t\t\"\"\"\n\t\t# weight SNR by chisq\n\t\tbestnr = self.get_new_snr(index=index, nhigh=nhigh,\n\t\t                          column=\"chisq\")\n\t\tif len(self.get_ifos()) < 3:\n\t\t\treturn bestnr\n\t\t# recontour null SNR threshold for higher SNRs\n\t\tif self.snr > null_grad_thresh:\n\t\t\tnull_snr_threshold += (self.snr - null_grad_thresh) * null_grad_val\n\t\t# weight SNR by null SNR\n\t\tif self.get_null_snr() > null_snr_threshold:\n\t\t\tbestnr /= 1 + self.get_null_snr() - null_snr_threshold\n\t\treturn bestnr", "response": "Get the BestNR statistic for this row."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a segmentlist object describing the times spanned by the given segment_def_id.", "response": "def get(self, segment_def_id = None):\n\t\t\"\"\"\n\t\tReturn a segmentlist object describing the times spanned by\n\t\tthe segments carrying the given segment_def_id.  If\n\t\tsegment_def_id is None then all segments are returned.\n\n\t\tNote:  the result is not coalesced, the segmentlist\n\t\tcontains the segments as they appear in the table.\n\t\t\"\"\"\n\t\tif segment_def_id is None:\n\t\t\treturn segments.segmentlist(row.segment for row in self)\n\t\treturn segments.segmentlist(row.segment for row in self if row.segment_def_id == segment_def_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a ditionary mapping time slide IDs to offsetvector s.", "response": "def as_dict(self):\n\t\t\"\"\"\n\t\tReturn a ditionary mapping time slide IDs to offset\n\t\tdictionaries.\n\t\t\"\"\"\n\t\td = {}\n\t\tfor row in self:\n\t\t\tif row.time_slide_id not in d:\n\t\t\t\td[row.time_slide_id] = offsetvector.offsetvector()\n\t\t\tif row.instrument in d[row.time_slide_id]:\n\t\t\t\traise KeyError(\"'%s': duplicate instrument '%s'\" % (row.time_slide_id, row.instrument))\n\t\t\td[row.time_slide_id][row.instrument] = row.offset\n\t\treturn d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nappending rows describing an instrument --> offset mapping to this table.", "response": "def append_offsetvector(self, offsetvect, process):\n\t\t\"\"\"\n\t\tAppend rows describing an instrument --> offset mapping to\n\t\tthis table.  offsetvect is a dictionary mapping instrument\n\t\tto offset.  process should be the row in the process table\n\t\ton which the new time_slide table rows will be blamed (or\n\t\tany object with a process_id attribute).  The return value\n\t\tis the time_slide_id assigned to the new rows.\n\t\t\"\"\"\n\t\ttime_slide_id = self.get_next_id()\n\t\tfor instrument, offset in offsetvect.items():\n\t\t\trow = self.RowType()\n\t\t\trow.process_id = process.process_id\n\t\t\trow.time_slide_id = time_slide_id\n\t\t\trow.instrument = instrument\n\t\t\trow.offset = offset\n\t\t\tself.append(row)\n\t\treturn time_slide_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the time_slide_id corresponding to the offset vector described by offsetdict, a dictionary of instrument/offset pairs. If the optional create_new argument is None (the default), then the table must contain a matching offset vector. The return value is the ID of that vector. If the table does not contain a matching offset vector then KeyError is raised. If the optional create_new argument is set to a Process object (or any other object with a process_id attribute), then if the table does not contain a matching offset vector a new one will be added to the table and marked as having been created by the given process. The return value is the ID of the (possibly newly created) matching offset vector. If the optional superset_ok argument is False (the default) then an offset vector in the table is considered to \"match\" the requested offset vector only if they contain the exact same set of instruments. If the superset_ok argument is True, then an offset vector in the table is considered to match the requested offset vector as long as it provides the same offsets for the same instruments as the requested vector, even if it provides offsets for other instruments as well. More than one offset vector in the table might match the requested vector. If the optional nonunique_ok argument is False (the default), then KeyError will be raised if more than one offset vector in the table is found to match the requested vector. If the optional nonunique_ok is True then the return value is the ID of one of the matching offset vectors selected at random.", "response": "def get_time_slide_id(self, offsetdict, create_new = None, superset_ok = False, nonunique_ok = False):\n\t\t\"\"\"\n\t\tReturn the time_slide_id corresponding to the offset vector\n\t\tdescribed by offsetdict, a dictionary of instrument/offset\n\t\tpairs.\n\n\t\tIf the optional create_new argument is None (the default),\n\t\tthen the table must contain a matching offset vector.  The\n\t\treturn value is the ID of that vector.  If the table does\n\t\tnot contain a matching offset vector then KeyError is\n\t\traised.\n\n\t\tIf the optional create_new argument is set to a Process\n\t\tobject (or any other object with a process_id attribute),\n\t\tthen if the table does not contain a matching offset vector\n\t\ta new one will be added to the table and marked as having\n\t\tbeen created by the given process.  The return value is the\n\t\tID of the (possibly newly created) matching offset vector.\n\n\t\tIf the optional superset_ok argument is False (the default)\n\t\tthen an offset vector in the table is considered to \"match\"\n\t\tthe requested offset vector only if they contain the exact\n\t\tsame set of instruments.  If the superset_ok argument is\n\t\tTrue, then an offset vector in the table is considered to\n\t\tmatch the requested offset vector as long as it provides\n\t\tthe same offsets for the same instruments as the requested\n\t\tvector, even if it provides offsets for other instruments\n\t\tas well.\n\n\t\tMore than one offset vector in the table might match the\n\t\trequested vector.  If the optional nonunique_ok argument is\n\t\tFalse (the default), then KeyError will be raised if more\n\t\tthan one offset vector in the table is found to match the\n\t\trequested vector.  If the optional nonunique_ok is True\n\t\tthen the return value is the ID of one of the matching\n\t\toffset vectors selected at random.\n\t\t\"\"\"\n\t\t# look for matching offset vectors\n\t\tif superset_ok:\n\t\t\tids = [id for id, slide in self.as_dict().items() if offsetdict == dict((instrument, offset) for instrument, offset in slide.items() if instrument in offsetdict)]\n\t\telse:\n\t\t\tids = [id for id, slide in self.as_dict().items() if offsetdict == slide]\n\t\tif len(ids) > 1:\n\t\t\t# found more than one\n\t\t\tif nonunique_ok:\n\t\t\t\t# and that's OK\n\t\t\t\treturn ids[0]\n\t\t\t# and that's not OK\n\t\t\traise KeyError(\"%s not unique\" % repr(offsetdict))\n\t\tif len(ids) == 1:\n\t\t\t# found one\n\t\t\treturn ids[0]\n\t\t# offset vector not found in table\n\t\tif create_new is None:\n\t\t\t# and that's not OK\n\t\t\traise KeyError(\"%s not found\" % repr(offsetdict))\n\t\t# that's OK, create new vector, return its ID\n\t\treturn self.append_offsetvector(offsetdict, create_new)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_coinc_def_id(self, search, search_coinc_type, create_new = True, description = None):\n\t\t# look for the ID\n\t\trows = [row for row in self if (row.search, row.search_coinc_type) == (search, search_coinc_type)]\n\t\tif len(rows) > 1:\n\t\t\traise ValueError(\"(search, search coincidence type) = ('%s', %d) is not unique\" % (search, search_coinc_type))\n\t\tif len(rows) > 0:\n\t\t\treturn rows[0].coinc_def_id\n\n\t\t# coinc type not found in table\n\t\tif not create_new:\n\t\t\traise KeyError((search, search_coinc_type))\n\t\trow = self.RowType()\n\t\trow.coinc_def_id = self.get_next_id()\n\t\trow.search = search\n\t\trow.search_coinc_type = search_coinc_type\n\t\trow.description = description\n\t\tself.append(row)\n\n\t\t# return new ID\n\t\treturn row.coinc_def_id", "response": "This method returns the ID for the row whose search string and search_coinc_type integer have the values\n\tgiven. If create_new is False then the ID is assigned to the new row. If create_new is True then the ID is assigned to the new row. If create_new is False then the ID is assigned to the new row."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies our low and high windows to the segments in a segmentlist.", "response": "def apply_to_segmentlist(self, seglist):\n\t\t\"\"\"\n\t\tApply our low and high windows to the segments in a\n\t\tsegmentlist.\n\t\t\"\"\"\n\t\tfor i, seg in enumerate(seglist):\n\t\t\tseglist[i] = seg.__class__(seg[0] - self.low_window, seg[1] + self.high_window)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef synchronizeLayout(primary, secondary, surface_size):\n    primary.configure_bound(surface_size)\n    secondary.configure_bound(surface_size)\n    # Check for key size.\n    if (primary.key_size < secondary.key_size):\n        logging.warning('Normalizing key size from secondary to primary')\n        secondary.key_size = primary.key_size\n    elif (primary.key_size > secondary.key_size):\n        logging.warning('Normalizing key size from primary to secondary')\n        primary.key_size = secondary.key_size\n    if (primary.size[1] > secondary.size[1]):\n        logging.warning('Normalizing layout size from secondary to primary')\n        secondary.set_size(primary.size, surface_size)\n    elif (primary.size[1] < secondary.size[1]):\n        logging.warning('Normalizing layout size from primary to secondary')\n        primary.set_size(secondary.size, surface_size)", "response": "Synchronizes given layouts by using max height of given layouts to avoid transistion dirty effects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_key(self, surface, key):\n        if isinstance(key, VSpaceKey):\n            self.draw_space_key(surface, key)\n        elif isinstance(key, VBackKey):\n            self.draw_back_key(surface, key)\n        elif isinstance(key, VUppercaseKey):\n            self.draw_uppercase_key(surface, key)\n        elif isinstance(key, VSpecialCharKey):\n            self.draw_special_char_key(surface, key)\n        else:\n            self.draw_character_key(surface, key)", "response": "Default drawing method for key. \n\n        Draws the key according to the type of key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefaults drawing method for key.", "response": "def draw_character_key(self, surface, key, special=False):\n        \"\"\"Default drawing method for key. \n\n        Key is drawn as a simple rectangle filled using this\n        cell style background color attribute. Key value is printed\n        into drawn cell using internal font.\n\n        :param surface: Surface background should be drawn in.\n        :param key: Target key to be drawn.\n        :param special: BOolean flag that indicates if the drawn key should use special background color if available.\n        \"\"\"\n        background_color = self.key_background_color\n        if special and self.special_key_background_color is not None:\n            background_color = self.special_key_background_color\n        pygame.draw.rect(surface, background_color[key.state], key.position + key.size)\n        size = self.font.size(key.value)\n        x = key.position[0] + ((key.size[0] - size[0]) / 2)\n        y = key.position[1] + ((key.size[1] - size[1]) / 2)\n        surface.blit(self.font.render(key.value, 1, self.text_color[key.state], None), (x, y))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_uppercase_key(self, surface, key):\n        key.value = u'\\u21e7' \n        if key.is_activated():\n            key.value = u'\\u21ea'\n        self.draw_character_key(surface, key, True)", "response": "Default drawing method for uppercase key. Drawn as character key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefault drawing method for special char key. Drawn as character key.", "response": "def draw_special_char_key(self, surface, key):\n        \"\"\"Default drawing method for special char key. Drawn as character key.\n\n        :param surface: Surface background should be drawn in.\n        :param key: Target key to be drawn.\n        \"\"\"\n        key.value = u'#' \n        if key.is_activated():\n            key.value = u'Ab'\n        self.draw_character_key(surface, key, True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_touched(self, position):\n        return position[0] >= self.position[0] and position[0] <= self.position[0]+ self.size[0]", "response": "Hit detection method.\n        \n        Indicates if this key is touched by touch or click event at the given position."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_key(self, key, first=False):\n        if first:\n            self.keys = [key] + self.keys\n        else:\n            self.keys.append(key)\n        if isinstance(key, VSpaceKey):\n            self.space = key", "response": "Adds the given key to this row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the size of the keyboard in the specified position and size.", "response": "def set_size(self, position, size, padding):\n        \"\"\"Row size setter.\n\n        The size correspond to the row height, since the row width is constraint\n        to the surface width the associated keyboard belongs. Once size is settled,\n        the size for each child keys is associated.\n        \n        :param position: Position of this row.\n        :param size: Size of the row (height)\n        :param padding: Padding between key.\n        \"\"\"\n        self.height = size\n        self.position = position\n        x = position[0]\n        for key in self.keys:\n            key.set_size(size)\n            key.position = (x, position[1])\n            x += padding + key.size[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures specials key if needed.", "response": "def configure_specials_key(self, keyboard):\n        \"\"\"Configures specials key if needed.\n\n        :param keyboard: Keyboard instance this layout belong.\n        \"\"\"\n        special_row = VKeyRow()\n        max_length = self.max_length\n        i = len(self.rows) - 1\n        current_row = self.rows[i]\n        special_keys = [VBackKey()]\n        if self.allow_uppercase: special_keys.append(VUppercaseKey(keyboard))\n        if self.allow_special_chars: special_keys.append(VSpecialCharKey(keyboard))\n        while len(special_keys) > 0:\n            first = False\n            while len(special_keys) > 0 and len(current_row) < max_length:\n                current_row.add_key(special_keys.pop(0), first=first)\n                first = not first\n            if i > 0:\n                i -= 1\n                current_row = self.rows[i]\n            else:\n                break\n        if self.allow_space:\n            space_length = len(current_row) - len(special_keys)\n            special_row.add_key(VSpaceKey(space_length))\n        first = True\n        # Adding left to the special bar.\n        while len(special_keys) > 0:\n            special_row.add_key(special_keys.pop(0), first=first)\n            first = not first\n        if len(special_row) > 0:\n            self.rows.append(special_row)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure_bound(self, surface_size):\n        r = len(self.rows)\n        max_length = self.max_length\n        if self.key_size is None:\n            self.key_size = (surface_size[0] - (self.padding * (max_length + 1))) / max_length\n        height = self.key_size * r + self.padding * (r + 1)\n        if height >= surface_size[1] / 2:\n            logger.warning('Computed keyboard height outbound target surface, reducing key_size to match')\n            self.key_size = ((surface_size[1] / 2) - (self.padding * (r + 1))) / r\n            height = self.key_size * r + self.padding * (r + 1)\n            logger.warning('Normalized key_size to %spx' % self.key_size)\n        self.set_size((surface_size[0], height), surface_size)", "response": "Compute keyboard bound regarding of this layout."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_size(self, size, surface_size):\n        self.size = size\n        self.position = (0, surface_size[1] - self.size[1])\n        y = self.position[1] + self.padding\n        max_length = self.max_length\n        for row in self.rows:\n            r = len(row)\n            width = (r * self.key_size) + ((r + 1) * self.padding)\n            x = (surface_size[0] - width) / 2\n            if row.space is not None:\n                x -= ((row.space.length - 1) * self.key_size) / 2\n            row.set_size((x, y), self.key_size, self.padding)\n            y += self.padding + self.key_size", "response": "Sets the size of this layout and updates the position and rows accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invalidate(self):\n        for row in self.rows:\n            for key in row.keys:\n                key.state = 0", "response": "Restores all keys states."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_uppercase(self, uppercase):\n        for row in self.rows:\n            for key in row.keys:\n                if type(key) == VKey:\n                    if uppercase:\n                        key.value = key.value.upper()\n                    else:\n                        key.value = key.value.lower()", "response": "Sets layout uppercase state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_key_at(self, position):\n        for row in self.rows:\n            if position in row:\n                for key in row.keys:\n                    if key.is_touched(position):\n                        return key\n        return None", "response": "Retrieves if any key is located at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing the virtual keyboard into the delegate surface object if enabled.", "response": "def draw(self):\n        \"\"\" Draw the virtual keyboard into the delegate surface object if enabled. \"\"\"\n        if self.state > 0:\n            self.renderer.draw_background(self.surface, self.layout.position, self.layout.size)\n            for row in self.layout.rows:\n                for key in row.keys:\n                    self.renderer.draw_key(self.surface, key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_special_char(self):\n        self.special_char = not self.special_char\n        if self.special_char:\n            self.set_layout(self.special_char_layout)\n        else:\n            self.set_layout(self.original_layout)\n        self.invalidate()", "response": "Special char key press handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_event(self, event):\n        if self.state > 0:\n            if event.type == MOUSEBUTTONDOWN:\n                key = self.layout.get_key_at(pygame.mouse.get_pos())\n                if key is not None:\n                    self.on_key_down(key)\n            elif event.type == MOUSEBUTTONUP:\n                self.on_key_up()\n            elif event.type == KEYDOWN:\n                value = pygame.key.name(event.key)\n                # TODO : Find from layout (consider checking layout key space ?)\n            elif event.type == KEYUP:\n                value = pygame.key.name(event.key)", "response": "Pygame event processing callback method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the state of the key and redraws it.", "response": "def set_key_state(self, key, state):\n        \"\"\"Sets the key state and redraws it.\n\n        :param key: Key to update state for.\n        :param state: New key state.\n        \"\"\"\n        key.state = state\n        self.renderer.draw_key(self.surface, key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_key_up(self):\n        if (self.last_pressed is not None):\n            self.set_key_state(self.last_pressed, 0)\n            self.buffer = self.last_pressed.update_buffer(self.buffer)\n            self.text_consumer(self.buffer)\n            self.last_pressed = None", "response": "Process key up event by updating buffer and release key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if both names and colors are owned by the same player.", "response": "def same_player(self, other):\n        \"\"\"\n        Compares name and color.\n        Returns True if both are owned by the same player.\n        \"\"\"\n        return self.name == other.name \\\n            and self.color == other.color"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        self.cells.clear()\n        self.leaderboard_names.clear()\n        self.leaderboard_groups.clear()\n        self.top_left.set(0, 0)\n        self.bottom_right.set(0, 0)", "response": "Clears the cells leaderboards and leaderboards and sets all corners to 0."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the internal state of the object to its initial state.", "response": "def reset(self):\n        \"\"\"\n        Clears `nick` and `own_ids`, sets `center` to `world.center`,\n        and then calls `cells_changed()`.\n        \"\"\"\n        self.own_ids.clear()\n        self.nick = ''\n        self.center = self.world.center\n        self.cells_changed()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating total_size total_mass scale and center.", "response": "def cells_changed(self):\n        \"\"\"\n        Calculates `total_size`, `total_mass`, `scale`, and `center`.\n\n        Has to be called when the controlled cells (`own_ids`) change.\n        \"\"\"\n        self.total_size = sum(cell.size for cell in self.own_cells)\n        self.total_mass = sum(cell.mass for cell in self.own_cells)\n        self.scale = pow(min(1.0, 64.0 / self.total_size), 0.4) \\\n            if self.total_size > 0 else 1.0\n\n        if self.own_ids:\n            left = min(cell.pos.x for cell in self.own_cells)\n            right = max(cell.pos.x for cell in self.own_cells)\n            top = min(cell.pos.y for cell in self.own_cells)\n            bottom = max(cell.pos.y for cell in self.own_cells)\n            self.center = Vec(left + right, top + bottom) / 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visible_area(self):\n        # looks like zeach has a nice big screen\n        half_viewport = Vec(1920, 1080) / 2 / self.scale\n        top_left = self.world.center - half_viewport\n        bottom_right = self.world.center + half_viewport\n        return top_left, bottom_right", "response": "Calculate the area of the visible area of the current system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_segment_tables(xmldoc, name = None):\n\ttry:\n\t\tnames = lsctables.SegmentDefTable.get_table(xmldoc).getColumnByName(\"name\")\n\t\tlsctables.SegmentTable.get_table(xmldoc)\n\t\tlsctables.SegmentSumTable.get_table(xmldoc)\n\texcept (ValueError, KeyError):\n\t\treturn False\n\treturn name is None or name in names", "response": "Returns True if the document contains a complete set of segment tables. Returns False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort(self, *args):\n\t\tself.valid.sort(*args)\n\t\tself.active.sort(*args)", "response": "Sort the internal segment lists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_from_segwizard(self, fileobj, instruments, name, version = None, comment = None):\n\t\tself.add(LigolwSegmentList(active = segmentsUtils.fromsegwizard(fileobj, coltype = LIGOTimeGPS), instruments = instruments, name = name, version = version, comment = comment))", "response": "Parse the contents of the file object fileobj as a segwizard - format segment list and insert the result as a LigolwSegments\n\tobject."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert the segments from the segmentlistdict object into the LIGO_LIGO_SEGMENTS table.", "response": "def insert_from_segmentlistdict(self, seglists, name, version = None, comment = None, valid=None):\n\t\t\"\"\"\n\t\tInsert the segments from the segmentlistdict object\n\t\tseglists as a new list of \"active\" segments into this\n\t\tLigolwSegments object.  The dictionary's keys are assumed\n\t\tto provide the instrument name for each segment list.  A\n\t\tnew entry will be created in the segment_definer table for\n\t\tthe segment lists, and the dictionary's keys, the name, and\n\t\tcomment will be used to populate the entry's metadata.\n\t\t\"\"\"\n\t\tfor instrument, segments in seglists.items():\n\t\t\tif valid is None:\n\t\t\t\tcurr_valid = ()\n\t\t\telse:\n\t\t\t\tcurr_valid = valid[instrument]\t\t\n\t\t\tself.add(LigolwSegmentList(active = segments, instruments = set([instrument]), name = name, version = version, comment = comment, valid = curr_valid))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef optimize(self):\n\t\tself.sort()\n\t\tsegment_lists = dict(enumerate(self))\n\t\tfor target, source in [(idx_a, idx_b) for (idx_a, seglist_a), (idx_b, seglist_b) in iterutils.choices(segment_lists.items(), 2) if seglist_a.valid == seglist_b.valid and seglist_a.active == seglist_b.active and seglist_a.name == seglist_b.name and seglist_a.version == seglist_b.version and seglist_a.comment == seglist_b.comment]:\n\t\t\ttry:\n\t\t\t\tsource = segment_lists.pop(source)\n\t\t\texcept KeyError:\n\t\t\t\tcontinue\n\t\t\tsegment_lists[target].instruments |= source.instruments\n\t\tself.clear()\n\t\tself.update(segment_lists.values())", "response": "This method is used to optimize the segment lists that differ only in their\n\tinstruments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_by_name(self, name, clip_to_valid = False):\n\t\tresult = segments.segmentlistdict()\n\t\tfor seglist in self:\n\t\t\tif seglist.name != name:\n\t\t\t\tcontinue\n\t\t\tsegs = seglist.active\n\t\t\tif clip_to_valid:\n\t\t\t\t# do not use in-place intersection\n\t\t\t\tsegs = segs & seglist.valid\n\t\t\tfor instrument in seglist.instruments:\n\t\t\t\tif instrument in result:\n\t\t\t\t\traise ValueError(\"multiple '%s' segmentlists for instrument '%s'\" % (name, instrument))\n\t\t\t\tresult[instrument] = segs.copy()\n\t\treturn result", "response": "Retrieve the active segmentlists whose name equals name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrestore the LigolwSegmentList objects to the XML tables in preparation for output. All segments from all segment lists are inserted into the tables in time order, but this is NOT behaviour external applications should rely on. This is done simply in the belief that it might assist in constructing well balanced indexed databases from the resulting files. If that proves not to be the case, or for some reason this behaviour proves inconvenient to preserve, then it might be discontinued without notice. You've been warned.", "response": "def finalize(self, process_row = None):\n\t\t\"\"\"\n\t\tRestore the LigolwSegmentList objects to the XML tables in\n\t\tpreparation for output.  All segments from all segment\n\t\tlists are inserted into the tables in time order, but this\n\t\tis NOT behaviour external applications should rely on.\n\t\tThis is done simply in the belief that it might assist in\n\t\tconstructing well balanced indexed databases from the\n\t\tresulting files.  If that proves not to be the case, or for\n\t\tsome reason this behaviour proves inconvenient to preserve,\n\t\tthen it might be discontinued without notice.  You've been\n\t\twarned.\n\t\t\"\"\"\n\t\tif process_row is not None:\n\t\t\tprocess_id = process_row.process_id\n\t\telif self.process is not None:\n\t\t\tprocess_id = self.process.process_id\n\t\telse:\n\t\t\traise ValueError(\"must supply a process row to .__init__()\")\n\n\t\t#\n\t\t# ensure ID generators are synchronized with table contents\n\t\t#\n\n\t\tself.segment_def_table.sync_next_id()\n\t\tself.segment_table.sync_next_id()\n\t\tself.segment_sum_table.sync_next_id()\n\n\t\t#\n\t\t# put all segment lists in time order\n\t\t#\n\n\t\tself.sort()\n\n\t\t#\n\t\t# generator function to convert segments into row objects,\n\t\t# each paired with the table to which the row is to be\n\t\t# appended\n\t\t#\n\n\t\tdef row_generator(segs, target_table, process_id, segment_def_id):\n\t\t\tid_column = target_table.next_id.column_name\n\t\t\tfor seg in segs:\n\t\t\t\trow = target_table.RowType()\n\t\t\t\trow.segment = seg\n\t\t\t\trow.process_id = process_id\n\t\t\t\trow.segment_def_id = segment_def_id\n\t\t\t\tsetattr(row, id_column, target_table.get_next_id())\n\t\t\t\tif 'comment' in target_table.validcolumns:\n\t\t\t\t\trow.comment = None\n\t\t\t\tyield row, target_table\n\n\t\t#\n\t\t# populate the segment_definer table from the list of\n\t\t# LigolwSegmentList objects and construct a matching list\n\t\t# of table row generators.  empty ourselves to prevent this\n\t\t# process from being repeated\n\t\t#\n\n\t\trow_generators = []\n\t\twhile self:\n\t\t\tligolw_segment_list = self.pop()\n\t\t\tsegment_def_row = self.segment_def_table.RowType()\n\t\t\tsegment_def_row.process_id = process_id\n\t\t\tsegment_def_row.segment_def_id = self.segment_def_table.get_next_id()\n\t\t\tsegment_def_row.instruments = ligolw_segment_list.instruments\n\t\t\tsegment_def_row.name = ligolw_segment_list.name\n\t\t\tsegment_def_row.version = ligolw_segment_list.version\n\t\t\tsegment_def_row.comment = ligolw_segment_list.comment\n\t\t\tself.segment_def_table.append(segment_def_row)\n\n\t\t\trow_generators.append(row_generator(ligolw_segment_list.valid, self.segment_sum_table, process_id, segment_def_row.segment_def_id))\n\t\t\trow_generators.append(row_generator(ligolw_segment_list.active, self.segment_table, process_id, segment_def_row.segment_def_id))\n\n\t\t#\n\t\t# populate segment and segment_summary tables by pulling\n\t\t# rows from the generators in time order\n\t\t#\n\n\t\tfor row, target_table in iterutils.inorder(*row_generators):\n\t\t\ttarget_table.append(row)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a graph to the current graph.", "response": "def add_graph(self, rhs_graph):\n        \"\"\"\n        Adds a graph to self.g\n\n        :param rhs_graph: the graph to add\n        :return: itself\n        \"\"\"\n        rhs_graph = self.__substitute_names_in_graph(rhs_graph)\n        self.g = self.__merge_graphs(self.g, rhs_graph)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, code):\n        if self.update:\n            self.vertices_substitution_dict, self.edges_substitution_dict, self.match_info\\\n                = self.match.get_variables_substitution_dictionaries(self.g, self.matching_graph)\n        try:\n            self.matching_graph = self.__apply_code_to_graph(code, self.matching_graph)\n        except:\n            pass\n        try:\n            code = self.__substitute_names_in_code(code)\n            self.g = self.__apply_code_to_graph(code, self.g)\n        except:\n            pass\n        return True", "response": "Executes the code and applies it to the self. g"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_list(self, variables):\n        variables = set(self.__substitute_names_in_list(variables))\n        self.update = False\n        self.g.delete_vertices(self.g.vs.select(name_in=variables))\n        self.g.delete_edges(self.g.es.select(name_in=variables))", "response": "Deletes a list of vertices and edges from self. g\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the variables that are defined in the variable_placeholders list and returns the values of the variables that are defined in the match_info dictionary.", "response": "def build_variables(self, variable_placeholders):\n        \"\"\"\n        :param variables: The list of vertices/edges to return\n        :return: a dict where the keys are the names of the variables to return,\n                 the values are the JSON of the properties of these variables\n        \"\"\"\n        variables = self.__substitute_names_in_list(variable_placeholders)\n        attributes = {}\n        for i, variable in enumerate(variables):\n            placeholder_name = variable_placeholders[i]\n            try:\n                vertices = self.g.vs.select(name=variable)\n                attributes[placeholder_name] = vertices[0].attributes()\n            except:\n                pass\n        for i, variable in enumerate(variables):\n            placeholder_name = variable_placeholders[i]\n            try:\n                edges = self.g.es.select(name=variable)\n                edge_attr = edges[0].attributes()\n                attributes[placeholder_name] = edge_attr\n            except:\n                pass\n        for i, variable in enumerate(variables):\n            placeholder_name = variable_placeholders[i]\n            try:\n                attributes[placeholder_name] = self.match_info[placeholder_name]\n            except:\n                pass\n        return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to write a lockfile containing the current pid.", "response": "def get_lock(lockfile):\n    \"\"\"\n    Tries to write a lockfile containing the current pid.  Excepts if\n    the lockfile already contains the pid of a running process.\n\n    Although this should prevent a lock from being granted twice, it\n    can theoretically deny a lock unjustly in the unlikely event that\n    the original process is gone but another unrelated process has\n    been assigned the same pid by the OS.\n    \"\"\"\n\n    pidfile = open(lockfile, \"a+\")\n\n    # here we do some meta-locking by getting an exclusive lock on the\n    # pidfile before reading it, to prevent two daemons from seeing a\n    # stale lock at the same time, and both trying to run\n    try:\n        fcntl.flock(pidfile.fileno(), fcntl.LOCK_EX|fcntl.LOCK_NB)\n    except IOError,e:\n        raise RuntimeError, \"failed to lock %s: %s\" % (lockfile, e)\n\n    # we got the file lock, so check for a pid therein\n    pidfile.seek(0)\n    pidfile_pid = pidfile.readline().strip()\n\n    if pidfile_pid.isdigit():\n        if pycbc_glue.utils.pid_exists(int(pidfile_pid)):\n            raise RuntimeError, (\"pidfile %s contains pid (%s) of a running \"\n                                 \"process\" % (lockfile, pidfile_pid))\n        else:\n            print (\"pidfile %s contains stale pid %s; writing new lock\" %\n                   (lockfile, pidfile_pid))\n\n    # the pidfile didn't exist or was stale, so grab a new lock\n    pidfile.truncate(0)\n    pidfile.write(\"%d\\n\" % os.getpid())\n    pidfile.close()\n\n    # should be entirely unecessary, but paranoia always served me well\n    confirm_lock(lockfile)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef confirm_lock(lockfile):\n    pidfile = open(lockfile, \"r\")\n    pidfile_pid = pidfile.readline().strip()\n    pidfile.close()\n    if int(pidfile_pid) != os.getpid():\n        raise RuntimeError, (\"pidfile %s contains pid %s; expected pid %s!\" %\n                             (lockfile, os.getpid(), pidfile_pid))\n    return True", "response": "Confirm that the given lockfile contains our pid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _totuple( x ):\n\n    if isinstance( x, basestring ):\n        out = x,\n    elif isinstance( x, ( int, long, float ) ):\n        out = str( x ),\n    elif x is None:\n        out = None,\n    else:\n        out = tuple( x )\n\n    return out", "response": "Utility stuff to convert string int long float None or anything to a usable tuple."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nescaping special html characters.", "response": "def escape( text, newline=False ):\n    \"\"\"Escape special html characters.\"\"\"\n\n    if isinstance( text, basestring ):\n        if '&' in text:\n            text = text.replace( '&', '&amp;' )\n        if '>' in text:\n            text = text.replace( '>', '&gt;' )\n        if '<' in text:\n            text = text.replace( '<', '&lt;' )\n        if '\\\"' in text:\n            text = text.replace( '\\\"', '&quot;' )\n        if '\\'' in text:\n            text = text.replace( '\\'', '&quot;' )\n        if newline:\n            if '\\n' in text:\n                text = text.replace( '\\n', '<br>' )\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender the actual tags to the content.", "response": "def render( self, tag, single, between, kwargs ):\n        \"\"\"Append the actual tags to content.\"\"\"\n\n        out = \"<%s\" % tag\n        for key, value in list( kwargs.items( ) ):\n            if value is not None:               # when value is None that means stuff like <... checked>\n                key = key.strip('_')            # strip this so class_ will mean class, etc.\n                if key == 'http_equiv':         # special cases, maybe change _ to - overall?\n                    key = 'http-equiv'\n                elif key == 'accept_charset':\n                    key = 'accept-charset'\n                out = \"%s %s=\\\"%s\\\"\" % ( out, key, escape( value ) )\n            else:\n                out = \"%s %s\" % ( out, key )\n        if between is not None:\n            out = \"%s>%s</%s>\" % ( out, between, tag )\n        else:\n            if single:\n                out = \"%s />\" % out\n            else:\n                out = \"%s>\" % out\n        if self.parent is not None:\n            self.parent.content.append( out )\n        else:\n            return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close( self ):\n\n        if self.tag in self.parent.twotags:\n            self.parent.content.append( \"</%s>\" % self.tag )\n        elif self.tag in self.parent.onetags:\n            raise ClosingError( self.tag )\n        elif self.parent.mode == 'strict_html' and self.tag in self.parent.deptags:\n            raise DeprecationError( self.tag )", "response": "Append a closing tag unless element has only opening tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend an opening tag.", "response": "def open( self, **kwargs ):\n        \"\"\"Append an opening tag.\"\"\"\n\n        if self.tag in self.parent.twotags or self.tag in self.parent.onetags:\n            self.render( self.tag, False, None, kwargs )\n        elif self.mode == 'strict_html' and self.tag in self.parent.deptags:\n            raise DeprecationError( self.tag )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scripts( self, mydict ):\n\n        if isinstance( mydict, dict ):\n            for src, type in list( mydict.items( ) ):\n                self.script( '', src=src, type='text/%s' % type )\n        else:\n            try:\n                for src in mydict:\n                    self.script( '', src=src, type='text/javascript' )\n            except:\n                raise TypeError( \"Script should be given a dictionary of src:type pairs or a list of javascript src's.\" )", "response": "Only useful in html mydict is dictionary of src type pairs or a list of javascript src s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef MultiIter(*sequences):\n\tif len(sequences) > 1:\n\t\t# FIXME:  this loop is about 5% faster if done the other\n\t\t# way around, if the last list is iterated over in the\n\t\t# inner loop.  but there is code, like snglcoinc.py in\n\t\t# pylal, that has been optimized for the current order and\n\t\t# would need to be reoptimized if this function were to be\n\t\t# reversed.\n\t\thead = tuple((x,) for x in sequences[0])\n\t\tfor t in MultiIter(*sequences[1:]):\n\t\t\tfor h in head:\n\t\t\t\tyield h + t\n\telif sequences:\n\t\tfor t in sequences[0]:\n\t\t\tyield (t,)", "response": "A generator for iterating over the elements of multiple sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef choices(vals, n):\n\tif n == len(vals):\n\t\tyield tuple(vals)\n\telif n > 1:\n\t\tn -= 1\n\t\tfor i, v in enumerate(vals[:-n]):\n\t\t\tv = (v,)\n\t\t\tfor c in choices(vals[i+1:], n):\n\t\t\t\tyield v + c\n\telif n == 1:\n\t\tfor v in vals:\n\t\t\tyield (v,)\n\telif n == 0:\n\t\tyield ()\n\telse:\n\t\t# n < 0\n\t\traise ValueError(n)", "response": "generator for iterating over all choices of n elements from the input list vals."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uniq(iterable):\n\ttemp_dict = {}\n\tfor e in iterable:\n\t\tif e not in temp_dict:\n\t\t\tyield temp_dict.setdefault(e, e)", "response": "Yield the unique items of an iterable preserving order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding the non - unique items of an iterable preserving order.", "response": "def nonuniq(iterable):\n\t\"\"\"\n\tYield the non-unique items of an iterable, preserving order.  If an\n\titem occurs N > 0 times in the input sequence, it will occur N-1\n\ttimes in the output sequence.\n\n\tExample:\n\n\t>>> x = nonuniq([0, 0, 2, 6, 2, 0, 5])\n\t>>> list(x)\n\t[0, 2, 0]\n\t\"\"\"\n\ttemp_dict = {}\n\tfor e in iterable:\n\t\tif e in temp_dict:\n\t\t\tyield e\n\t\ttemp_dict.setdefault(e, e)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten(sequence, levels = 1):\n\tif levels == 0:\n\t\tfor x in sequence:\n\t\t\tyield x\n\telse:\n\t\tfor x in sequence:\n\t\t\tfor y in flatten(x, levels - 1):\n\t\t\t\tyield y", "response": "flatten a sequence of nested lists"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes Python's filter() builtin, but modifies the sequence in place. Example: >>> l = range(10) >>> inplace_filter(lambda x: x > 5, l) >>> l [6, 7, 8, 9] Performance considerations: the function iterates over the sequence, shuffling surviving members down and deleting whatever top part of the sequence is left empty at the end, so sequences whose surviving members are predominantly at the bottom will be processed faster.", "response": "def inplace_filter(func, sequence):\n\t\"\"\"\n\tLike Python's filter() builtin, but modifies the sequence in place.\n\n\tExample:\n\n\t>>> l = range(10)\n\t>>> inplace_filter(lambda x: x > 5, l)\n\t>>> l\n\t[6, 7, 8, 9]\n\n\tPerformance considerations:  the function iterates over the\n\tsequence, shuffling surviving members down and deleting whatever\n\ttop part of the sequence is left empty at the end, so sequences\n\twhose surviving members are predominantly at the bottom will be\n\tprocessed faster.\n\t\"\"\"\n\ttarget = 0\n\tfor source in xrange(len(sequence)):\n\t\tif func(sequence[source]):\n\t\t\tsequence[target] = sequence[source]\n\t\t\ttarget += 1\n\tdel sequence[target:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inorder(*iterables, **kwargs):\n\treverse = kwargs.pop(\"reverse\", False)\n\tkeyfunc = kwargs.pop(\"key\", lambda x: x) # default = identity\n\tif kwargs:\n\t\traise TypeError(\"invalid keyword argument '%s'\" % kwargs.keys()[0])\n\tnextvals = {}\n\tfor iterable in iterables:\n\t\tnext = iter(iterable).next\n\t\ttry:\n\t\t\tnextval = next()\n\t\t\tnextvals[next] = keyfunc(nextval), nextval, next\n\t\texcept StopIteration:\n\t\t\tpass\n\tif not nextvals:\n\t\t# all sequences are empty\n\t\treturn\n\tif reverse:\n\t\tselect = max\n\telse:\n\t\tselect = min\n\tvalues = nextvals.itervalues\n\tif len(nextvals) > 1:\n\t\twhile 1:\n\t\t\t_, val, next = select(values())\n\t\t\tyield val\n\t\t\ttry:\n\t\t\t\tnextval = next()\n\t\t\t\tnextvals[next] = keyfunc(nextval), nextval, next\n\t\t\texcept StopIteration:\n\t\t\t\tdel nextvals[next]\n\t\t\t\tif len(nextvals) < 2:\n\t\t\t\t\tbreak\n\t# exactly one sequence remains, short circuit and drain it\n\t(_, val, next), = values()\n\tyield val\n\twhile 1:\n\t\tyield next()", "response": "inorder is a generator that yields the values from several ordered iterables\n\tin order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef randindex(lo, hi, n = 1.):\n\tif not 0 <= lo < hi:\n\t\traise ValueError(\"require 0 <= lo < hi: lo = %d, hi = %d\" % (lo, hi))\n\tif n <= 0.:\n\t\traise ValueError(\"n <= 0: %g\" % n)\n\telif n == 1.:\n\t\t# special case for uniform distribution\n\t\ttry:\n\t\t\tlnP = math.log(1. / (hi - lo))\n\t\texcept ValueError:\n\t\t\traise ValueError(\"[lo, hi) domain error\")\n\t\thi -= 1\n\t\trnd = random.randint\n\t\twhile 1:\n\t\t\tyield rnd(lo, hi), lnP\n\n\t# CDF evaluated at index boundaries\n\tlnP = numpy.arange(lo, hi + 1, dtype = \"double\")**n\n\tlnP -= lnP[0]\n\tlnP /= lnP[-1]\n\t# differences give probabilities\n\tlnP = tuple(numpy.log(lnP[1:] - lnP[:-1]))\n\tif numpy.isinf(lnP).any():\n\t\traise ValueError(\"[lo, hi) domain error\")\n\n\tbeta = lo**n / (hi**n - lo**n)\n\tn = 1. / n\n\talpha = hi / (1. + beta)**n\n\tflr = math.floor\n\trnd = random.random\n\twhile 1:\n\t\tindex = int(flr(alpha * (rnd() + beta)**n))\n\t\t# the tuple look-up provides the second part of the\n\t\t# range safety check on index\n\t\tassert index >= lo\n\t\tyield index, lnP[index - lo]", "response": "Randomly generates random integers from the range [ lo hi ) where 0 < = lo < hi."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshifts this segment by x and returns a new tuple whose upper and lower bounds are given by adding x to the segment s upper and lower bounds.", "response": "def shift(self, x):\n\t\t\"\"\"\n\t\tReturn a new segment whose bounds are given by adding x to\n\t\tthe segment's upper and lower bounds.\n\t\t\"\"\"\n\t\treturn tuple.__new__(self.__class__, (self[0] + x, self[1] + x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the segment whose end - points denote the maximum and minimum extent of the segmentlist.", "response": "def extent(self):\n\t\t\"\"\"\n\t\tReturn the segment whose end-points denote the maximum and\n\t\tminimum extent of the segmentlist.  Does not require the\n\t\tsegmentlist to be coalesced.\n\t\t\"\"\"\n\t\tif not len(self):\n\t\t\traise ValueError(\"empty list\")\n\t\tmin, max = self[0]\n\t\tfor lo, hi in self:\n\t\t\tif min > lo:\n\t\t\t\tmin = lo\n\t\t\tif max < hi:\n\t\t\t\tmax = hi\n\t\treturn segment(min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(self, item):\n\t\tfor i, seg in enumerate(self):\n\t\t\tif item in seg:\n\t\t\t\treturn i\n\t\traise ValueError(item)", "response": "Returns the index of an item in the segmentlist that wholly contains item. Raises ValueError if no such item exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersects_segment(self, other):\n\t\ti = _bisect_left(self, other)\n\t\treturn ((i != 0) and (other[0] < self[i-1][1])) or ((i != len(self)) and (other[1] > self[i][0]))", "response": "Returns True if the intersection of self and other is not the null set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef intersects(self, other):\n\t\t# if either has zero length, the answer is False\n\t\tif not (self and other):\n\t\t\treturn False\n\t\t# walk through both lists in order, searching for a match\n\t\ti = j = 0\n\t\tseg = self[0]\n\t\totherseg = other[0]\n\t\twhile True:\n\t\t\tif seg[1] <= otherseg[0]:\n\t\t\t\ti += 1\n\t\t\t\tif i >= len(self):\n\t\t\t\t\treturn False\n\t\t\t\tseg = self[i]\n\t\t\telif otherseg[1] <= seg[0]:\n\t\t\t\tj += 1\n\t\t\t\tif j >= len(other):\n\t\t\t\t\treturn False\n\t\t\t\totherseg = other[j]\n\t\t\telse:\n\t\t\t\treturn True", "response": "Returns True if the intersection of self and other is not the null set otherwise returns\n\tFalse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncoalesce the segments of the segmentlist into a single segment.", "response": "def coalesce(self):\n\t\t\"\"\"\n\t\tSort the elements of the list into ascending order, and merge\n\t\tcontinuous segments into single segments.  Segmentlist is\n\t\tmodified in place.  This operation is O(n log n).\n\t\t\"\"\"\n\t\tself.sort()\n\t\ti = j = 0\n\t\tn = len(self)\n\t\twhile j < n:\n\t\t\tlo, hi = self[j]\n\t\t\tj += 1\n\t\t\twhile j < n and hi >= self[j][0]:\n\t\t\t\thi = max(hi, self[j][1])\n\t\t\t\tj += 1\n\t\t\tif lo != hi:\n\t\t\t\tself[i] = segment(lo, hi)\n\t\t\t\ti += 1\n\t\tdel self[i : ]\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the. protract method on each segment in the list and coalesce the result.", "response": "def protract(self, x):\n\t\t\"\"\"\n\t\tExecute the .protract() method on each segment in the list\n\t\tand coalesce the result.  Segmentlist is modified in place.\n\t\t\"\"\"\n\t\tfor i in xrange(len(self)):\n\t\t\tself[i] = self[i].protract(x)\n\t\treturn self.coalesce()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the. contract method on each segment in the list and coalesce the result.", "response": "def contract(self, x):\n\t\t\"\"\"\n\t\tExecute the .contract() method on each segment in the list\n\t\tand coalesce the result.  Segmentlist is modified in place.\n\t\t\"\"\"\n\t\tfor i in xrange(len(self)):\n\t\t\tself[i] = self[i].contract(x)\n\t\treturn self.coalesce()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the. shift method on each segment in the list.", "response": "def shift(self, x):\n\t\t\"\"\"\n\t\tExecute the .shift() method on each segment in the list.\n\t\tThe algorithm is O(n) and does not require the list to be\n\t\tcoalesced nor does it coalesce the list.  Segmentlist is\n\t\tmodified in place.\n\t\t\"\"\"\n\t\tfor i in xrange(len(self)):\n\t\t\tself[i] = self[i].shift(x)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, d):\n\t\tfor key, value in d.iteritems():\n\t\t\tif key in self:\n\t\t\t\tself[key] = value", "response": "Update the internal dictionary with the values from the given segmentlistdict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, keys = None):\n\t\tif keys is None:\n\t\t\tkeys = self\n\t\tnew = self.__class__()\n\t\tfor key in keys:\n\t\t\tnew[key] = _shallowcopy(self[key])\n\t\t\tdict.__setitem__(new.offsets, key, self.offsets[key])\n\t\treturn new", "response": "This method creates a shallow copy of the segmentlistdict object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map(self, func):\n\t\treturn dict((key, func(value)) for key, value in self.iteritems())", "response": "Returns a dictionary of the results of func applied to each segmentlist object in self."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the extent of all segments in the dictionary.", "response": "def extent_all(self):\n\t\t\"\"\"\n\t\tReturn the result of running .extent() on the union of all\n\t\tlists in the dictionary.\n\t\t\"\"\"\n\t\tsegs = tuple(seglist.extent() for seglist in self.values() if seglist)\n\t\tif not segs:\n\t\t\traise ValueError(\"empty list\")\n\t\treturn segment(min(seg[0] for seg in segs), max(seg[1] for seg in segs))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keys_at(self, x):\n\t\treturn [key for key, segs in self.items() if x in segs]", "response": "Return a list of the keys for the segment lists that contain x."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if any segmentlist in self intersects the segment otherwise returns False.", "response": "def intersects_segment(self, seg):\n\t\t\"\"\"\n\t\tReturns True if any segmentlist in self intersects the\n\t\tsegment, otherwise returns False.\n\t\t\"\"\"\n\t\treturn any(value.intersects_segment(seg) for value in self.itervalues())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if there exists a segmentlist in self that intersects the corresponding segmentlist in other ; returns False otherwise.", "response": "def intersects(self, other):\n\t\t\"\"\"\n\t\tReturns True if there exists a segmentlist in self that\n\t\tintersects the corresponding segmentlist in other;  returns\n\t\tFalse otherwise.\n\n\t\tSee also:\n\n\t\t.intersects_all(), .all_intersects(), .all_intersects_all()\n\t\t\"\"\"\n\t\treturn any(key in self and self[key].intersects(value) for key, value in other.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef intersects_all(self, other):\n\t\treturn all(key in self and self[key].intersects(value) for key, value in other.iteritems()) and bool(other)", "response": "Returns True if each segmentlist in other intersects the\n\tcorresponding segmentlist in self ; returns False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_intersects_all(self, other):\n\t\treturn set(self) == set(other) and all(other[key].intersects(value) for key, value in self.iteritems()) and bool(self)", "response": "Returns True if self and other have the same keys and all the segmentlists in the other are intersected."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extend(self, other):\n\t\tfor key, value in other.iteritems():\n\t\t\tif key not in self:\n\t\t\t\tself[key] = _shallowcopy(value)\n\t\t\telse:\n\t\t\t\tself[key].extend(value)", "response": "Appends the segmentlists from other to the corresponding\n\tsegmentlists in self."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncontract x on all segmentlists.", "response": "def contract(self, x):\n\t\t\"\"\"\n\t\tRun .contract(x) on all segmentlists.\n\t\t\"\"\"\n\t\tfor value in self.itervalues():\n\t\t\tvalue.contract(x)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprotract x on all segmentlists and returns self.", "response": "def protract(self, x):\n\t\t\"\"\"\n\t\tRun .protract(x) on all segmentlists.\n\t\t\"\"\"\n\t\tfor value in self.itervalues():\n\t\t\tvalue.protract(x)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_common(self, keys):\n\t\tkeys = set(keys)\n\t\tnew = self.__class__()\n\t\tintersection = self.intersection(keys)\n\t\tfor key in keys:\n\t\t\tdict.__setitem__(new, key, _shallowcopy(intersection))\n\t\t\tdict.__setitem__(new.offsets, key, self.offsets[key])\n\t\treturn new", "response": "Returns a new segmentlistdict containing only those\n\tsegmentlists associated with the keys in keys with each\n\tset to their mutual intersection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if any segment in any list in self intersects any segment in any list in other.", "response": "def is_coincident(self, other, keys = None):\n\t\t\"\"\"\n\t\tReturn True if any segment in any list in self intersects\n\t\tany segment in any list in other.  If the optional keys\n\t\targument is not None, then it should be an iterable of keys\n\t\tand only segment lists for those keys will be considered in\n\t\tthe test (instead of raising KeyError, keys not present in\n\t\tboth segment list dictionaries will be ignored).  If keys\n\t\tis None (the default) then all segment lists are\n\t\tconsidered.\n\n\t\tThis method is equivalent to the intersects() method, but\n\t\twithout requiring the keys of the intersecting segment\n\t\tlists to match.\n\t\t\"\"\"\n\t\tif keys is not None:\n\t\t\tkeys = set(keys)\n\t\t\tself = tuple(self[key] for key in set(self) & keys)\n\t\t\tother = tuple(other[key] for key in set(other) & keys)\n\t\telse:\n\t\t\tself = tuple(self.values())\n\t\t\tother = tuple(other.values())\n\t\t# make sure inner loop is smallest\n\t\tif len(self) < len(other):\n\t\t\tself, other = other, self\n\t\treturn any(a.intersects(b) for a in self for b in other)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intersection(self, keys):\n\t\tkeys = set(keys)\n\t\tif not keys:\n\t\t\treturn segmentlist()\n\t\tseglist = _shallowcopy(self[keys.pop()])\n\t\tfor key in keys:\n\t\t\tseglist &= self[key]\n\t\treturn seglist", "response": "Return the intersection of the segmentlists associated with the keys in keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_ids(connection, xmldoc, verbose = False):\n\t# NOTE:  it's critical that the xmldoc object be retrieved *before*\n\t# the rows whose IDs need to be updated are inserted.  The xml\n\t# retrieval resets the \"last max row ID\" values inside the table\n\t# objects, so if retrieval of the xmldoc is deferred until after\n\t# the rows are inserted, nothing will get updated.  therefore, the\n\t# connection and xmldoc need to be passed separately to this\n\t# function, even though it seems this function could reconstruct\n\t# the xmldoc itself from the connection.\n\ttable_elems = xmldoc.getElementsByTagName(ligolw.Table.tagName)\n\tfor i, tbl in enumerate(table_elems):\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"updating IDs: %d%%\\r\" % (100.0 * i / len(table_elems)),\n\t\ttbl.applyKeyMapping()\n\tif verbose:\n\t\tprint >>sys.stderr, \"updating IDs: 100%\"\n\n\t# reset ID mapping for next document\n\tdbtables.idmap_reset(connection)", "response": "Update the IDs of the rows in the specified xmldoc object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses and insert a LIGO Light Weight document at the given URL into the database.", "response": "def insert_from_url(url, preserve_ids = False, verbose = False, contenthandler = None):\n\t\"\"\"\n\tParse and insert the LIGO Light Weight document at the URL into the\n\tdatabase with which the content handler is associated.  If\n\tpreserve_ids is False (default), then row IDs are modified during\n\tthe insert process to prevent collisions with IDs already in the\n\tdatabase.  If preserve_ids is True then IDs are not modified;  this\n\twill result in database consistency violations if any of the IDs of\n\tnewly-inserted rows collide with row IDs already in the database,\n\tand is generally only sensible when inserting a document into an\n\tempty database.  If verbose is True then progress reports will be\n\tprinted to stderr.  See pycbc_glue.ligolw.dbtables.use_in() for more\n\tinformation about constructing a suitable content handler class.\n\t\"\"\"\n\t#\n\t# enable/disable ID remapping\n\t#\n\n\torig_DBTable_append = dbtables.DBTable.append\n\n\tif not preserve_ids:\n\t\ttry:\n\t\t\tdbtables.idmap_create(contenthandler.connection)\n\t\texcept sqlite3.OperationalError:\n\t\t\t# assume table already exists\n\t\t\tpass\n\t\tdbtables.idmap_sync(contenthandler.connection)\n\t\tdbtables.DBTable.append = dbtables.DBTable._remapping_append\n\telse:\n\t\tdbtables.DBTable.append = dbtables.DBTable._append\n\n\ttry:\n\t\t#\n\t\t# load document.  this process inserts the document's contents into\n\t\t# the database.  the XML tree constructed by this process contains\n\t\t# a table object for each table found in the newly-inserted\n\t\t# document and those table objects' last_max_rowid values have been\n\t\t# initialized prior to rows being inserted.  therefore, this is the\n\t\t# XML tree that must be passed to update_ids in order to ensure (a)\n\t\t# that all newly-inserted tables are processed and (b) all\n\t\t# newly-inserted rows are processed.  NOTE:  it is assumed the\n\t\t# content handler is creating DBTable instances in the XML tree,\n\t\t# not regular Table instances, but this is not checked.\n\t\t#\n\n\t\txmldoc = ligolw_utils.load_url(url, verbose = verbose, contenthandler = contenthandler)\n\n\t\t#\n\t\t# update references to row IDs and cleanup ID remapping\n\t\t#\n\n\t\tif not preserve_ids:\n\t\t\tupdate_ids(contenthandler.connection, xmldoc, verbose = verbose)\n\n\tfinally:\n\t\tdbtables.DBTable.append = orig_DBTable_append\n\n\t#\n\t# done.  unlink the document to delete database cursor objects it\n\t# retains\n\t#\n\n\tcontenthandler.connection.commit()\n\txmldoc.unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_from_xmldoc(connection, source_xmldoc, preserve_ids = False, verbose = False):\n\t#\n\t# enable/disable ID remapping\n\t#\n\n\torig_DBTable_append = dbtables.DBTable.append\n\n\tif not preserve_ids:\n\t\ttry:\n\t\t\tdbtables.idmap_create(connection)\n\t\texcept sqlite3.OperationalError:\n\t\t\t# assume table already exists\n\t\t\tpass\n\t\tdbtables.idmap_sync(connection)\n\t\tdbtables.DBTable.append = dbtables.DBTable._remapping_append\n\telse:\n\t\tdbtables.DBTable.append = dbtables.DBTable._append\n\n\ttry:\n\t\t#\n\t\t# create a place-holder XML representation of the target\n\t\t# document so we can pass the correct tree to update_ids().\n\t\t# note that only tables present in the source document need\n\t\t# ID ramapping, so xmldoc only contains representations of\n\t\t# the tables in the target document that are also in the\n\t\t# source document\n\t\t#\n\n\t\txmldoc = ligolw.Document()\n\t\txmldoc.appendChild(ligolw.LIGO_LW())\n\n\t\t#\n\t\t# iterate over tables in the source XML tree, inserting\n\t\t# each into the target database\n\t\t#\n\n\t\tfor tbl in source_xmldoc.getElementsByTagName(ligolw.Table.tagName):\n\t\t\t#\n\t\t\t# instantiate the correct table class, connected to the\n\t\t\t# target database, and save in XML tree\n\t\t\t#\n\n\t\t\tname = tbl.Name\n\t\t\ttry:\n\t\t\t\tcls = dbtables.TableByName[name]\n\t\t\texcept KeyError:\n\t\t\t\tcls = dbtables.DBTable\n\t\t\tdbtbl = xmldoc.childNodes[-1].appendChild(cls(tbl.attributes, connection = connection))\n\n\t\t\t#\n\t\t\t# copy table element child nodes from source XML tree\n\t\t\t#\n\n\t\t\tfor elem in tbl.childNodes:\n\t\t\t\tif elem.tagName == ligolw.Stream.tagName:\n\t\t\t\t\tdbtbl._end_of_columns()\n\t\t\t\tdbtbl.appendChild(type(elem)(elem.attributes))\n\n\t\t\t#\n\t\t\t# copy table rows from source XML tree\n\t\t\t#\n\n\t\t\tfor row in tbl:\n\t\t\t\tdbtbl.append(row)\n\t\t\tdbtbl._end_of_rows()\n\n\t\t#\n\t\t# update references to row IDs and clean up ID remapping\n\t\t#\n\n\t\tif not preserve_ids:\n\t\t\tupdate_ids(connection, xmldoc, verbose = verbose)\n\n\tfinally:\n\t\tdbtables.DBTable.append = orig_DBTable_append\n\n\t#\n\t# done.  unlink the document to delete database cursor objects it\n\t# retains\n\t#\n\n\tconnection.commit()\n\txmldoc.unlink()", "response": "Insert the tables from an in - ram XML document into the database at the given connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninserts documents from a sequence of URLs.", "response": "def insert_from_urls(urls, contenthandler, **kwargs):\n\t\"\"\"\n\tIterate over a sequence of URLs, calling insert_from_url() on each,\n\tthen build the indexes indicated by the metadata in lsctables.py.\n\tSee insert_from_url() for a description of the additional\n\targuments.\n\t\"\"\"\n\tverbose = kwargs.get(\"verbose\", False)\n\n\t#\n\t# load documents\n\t#\n\n\tfor n, url in enumerate(urls, 1):\n\t\tif verbose:\n\t\t\tprint >>sys.stderr, \"%d/%d:\" % (n, len(urls)),\n\t\tinsert_from_url(url, contenthandler = contenthandler, **kwargs)\n\n\t#\n\t# done.  build indexes\n\t#\n\n\tdbtables.build_indexes(contenthandler.connection, verbose)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the database at the given connection to a tabular LIGO Light - Weight XML document.", "response": "def extract(connection, filename, table_names = None, verbose = False, xsl_file = None):\n\t\"\"\"\n\tConvert the database at the given connection to a tabular LIGO\n\tLight-Weight XML document.  The XML document is written to the file\n\tnamed filename.  If table_names is not None, it should be a\n\tsequence of strings and only the tables in that sequence will be\n\tconverted.  If verbose is True then progress messages will be\n\tprinted to stderr.\n\t\"\"\"\n\txmldoc = ligolw.Document()\n\txmldoc.appendChild(dbtables.get_xml(connection, table_names))\n\tligolw_utils.write_filename(xmldoc, filename, gz = (filename or \"stdout\").endswith(\".gz\"), verbose = verbose, xsl_file = xsl_file)\n\n\t# delete cursors\n\txmldoc.unlink()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend search summary information associated with the given process to the search summary table in xmldoc. Returns the newly - created SearchSummary table row.", "response": "def append_search_summary(xmldoc, process, shared_object = \"standalone\", lalwrapper_cvs_tag = \"\", lal_cvs_tag = \"\", comment = None, ifos = None, inseg = None, outseg = None, nevents = 0, nnodes = 1):\n\t\"\"\"\n\tAppend search summary information associated with the given process\n\tto the search summary table in xmldoc.  Returns the newly-created\n\tsearch_summary table row.\n\t\"\"\"\n\trow = lsctables.SearchSummary()\n\trow.process_id = process.process_id\n\trow.shared_object = shared_object\n\trow.lalwrapper_cvs_tag = lalwrapper_cvs_tag\n\trow.lal_cvs_tag = lal_cvs_tag\n\trow.comment = comment or process.comment\n\trow.instruments = ifos if ifos is not None else process.instruments\n\trow.in_segment = inseg\n\trow.out_segment = outseg\n\trow.nevents = nevents\n\trow.nnodes = nnodes\n\n\ttry:\n\t\ttbl = lsctables.SearchSummaryTable.get_table(xmldoc)\n\texcept ValueError:\n\t\ttbl = xmldoc.childNodes[0].appendChild(lsctables.New(lsctables.SearchSummaryTable))\n\ttbl.append(row)\n\n\treturn row"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef segmentlistdict_fromsearchsummary_out(xmldoc, program = None):\n\tstbl = lsctables.SearchSummaryTable.get_table(xmldoc)\n\tptbl = lsctables.ProcessTable.get_table(xmldoc)\n\treturn stbl.get_out_segmentlistdict(program and ptbl.get_ids_by_program(program))", "response": "This function is used to create a segmentlistdict object from a search summary table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_private_and_public(username, password_verifier, private, preset):\n    session = SRPServerSession(\n        SRPContext(username, prime=preset[0], generator=preset[1]),\n        hex_from_b64(password_verifier), private=private)\n\n    click.secho('Server private: %s' % session.private_b64)\n    click.secho('Server public: %s' % session.public_b64)", "response": "Print out server private and public."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_session_data( username, password_verifier, salt, client_public, private, preset):\n    session = SRPServerSession(\n        SRPContext(username, prime=preset[0], generator=preset[1]),\n        hex_from_b64(password_verifier), private=private)\n\n    session.process(client_public, salt, base64=True)\n\n    click.secho('Server session key: %s' % session.key_b64)\n    click.secho('Server session key proof: %s' % session.key_proof_b64)\n    click.secho('Server session key hash: %s' % session.key_proof_hash_b64)", "response": "Print out server session data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint out server private and public.", "response": "def get_private_and_public(ctx, username, password, private, preset):\n    \"\"\"Print out server public and private.\"\"\"\n    session = SRPClientSession(\n        SRPContext(username, password, prime=preset[0], generator=preset[1]),\n        private=private)\n\n    click.secho('Client private: %s' % session.private_b64)\n    click.secho('Client public: %s' % session.public_b64)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint out client session data.", "response": "def get_session_data(ctx, username, password, salt, server_public, private, preset):\n    \"\"\"Print out client session data.\"\"\"\n    session = SRPClientSession(\n        SRPContext(username, password, prime=preset[0], generator=preset[1]),\n        private=private)\n\n    session.process(server_public, salt, base64=True)\n\n    click.secho('Client session key: %s' % session.key_b64)\n    click.secho('Client session key proof: %s' % session.key_proof_b64)\n    click.secho('Client session key hash: %s' % session.key_proof_hash_b64)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting out user data triplet.", "response": "def get_user_data_triplet(username, password):\n    \"\"\"Print out user data triplet: username, password verifier, salt.\"\"\"\n    context = SRPContext(username, password)\n    username, password_verifier, salt = context.get_user_data_triplet(base64=True)\n\n    click.secho('Username: %s' % username)\n    click.secho('Password verifier: %s' % password_verifier)\n    click.secho('Salt: %s' % salt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cwd_decorator(func):\n\n    def wrapper(*args, **kw):\n        cur_dir = os.getcwd()\n        found = False\n        for arg in sys.argv:\n            if arg.endswith(\".rst\"):\n                found = arg\n                break\n\n        if found:\n            directory = os.path.dirname(found)\n            if directory:\n                os.chdir(directory)\n        data = func(*args, **kw)\n        os.chdir(cur_dir)\n        return data\n\n    return wrapper", "response": "decorator to change cwd to directory containing rst for this function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_xml(node, pretty=False):\n    fout = Sio()\n    etree = et.ElementTree(node)\n\n    etree.write(fout)\n    xml = fout.getvalue()\n    if pretty:\n        xml = pretty_xml(xml, True)\n    return xml", "response": "convert an etree node to xml"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a cell to the current slide", "response": "def add_cell(preso, pos, width, height, padding=1, top_margin=4, left_margin=2):\n    \"\"\" Add a text frame to current slide \"\"\"\n    available_width = SLIDE_WIDTH\n    available_width -= left_margin * 2\n    available_width -= padding * (width - 1)\n    column_width = available_width / width\n    avail_height = SLIDE_HEIGHT\n    avail_height -= top_margin\n    avail_height -= padding * (height - 1)\n    column_height = avail_height / height\n\n    col_pos = int((pos - 1) % width)\n    row_pos = int((pos - 1) / width)\n\n    w = \"{}cm\".format(column_width)\n    h = \"{}cm\".format(column_height)\n    x = \"{}cm\".format(left_margin + (col_pos * column_width + (col_pos) * padding))\n    y = \"{}cm\".format(top_margin + (row_pos * column_height + (row_pos) * padding))\n    attr = {\n        \"presentation:class\": \"outline\",\n        \"presentation:style-name\": \"Default-outline1\",\n        \"svg:width\": w,\n        \"svg:height\": h,\n        \"svg:x\": x,\n        \"svg:y\": y,\n    }\n    preso.slides[-1].add_text_frame(attr)\n    preso.slides[-1].grid_w_h_x_y = (w, h, x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_line(preso, x1, y1, x2, y2, width=\"3pt\", color=\"red\"):\n    marker_end_ratio = .459 / 3  # .459cm/3pt\n    marker_start_ratio = .359 / 3  # .359cm/3pt\n    stroke_ratio = .106 / 3  # .106cm/3pt\n\n    w = float(width[0:width.index(\"pt\")])\n    sw = w * stroke_ratio\n    mew = w * marker_end_ratio\n    msw = w * marker_start_ratio\n    attribs = {\n        \"svg:stroke-width\": \"{}cm\".format(sw),\n        \"svg:stroke-color\": color,  # \"#ed1c24\",\n        \"draw:marker-start-width\": \"{}cm\".format(msw),\n        \"draw:marker-end\": \"Arrow\",\n        \"draw:marker-end-width\": \"{}cm\".format(mew),\n        \"draw:fill\": \"none\",\n        \"draw:textarea-vertical-align\": \"middle\",\n    }\n    style = LineStyle(**attribs)\n    # node = style.style_node()\n    preso.add_style(style)\n    line_attrib = {\n        \"draw:style-name\": style.name,\n        \"draw:layer\": \"layout\",\n        \"svg:x1\": x1,\n        \"svg:y1\": y1,\n        \"svg:x2\": x2,\n        \"svg:y2\": y2,\n    }\n    line_node = el(\"draw:line\", attrib=line_attrib)\n    preso.slides[-1]._page.append(line_node)", "response": "Adds a line to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_otp_style(self, zip_odp, style_file):\n        style = zipwrap.Zippier(style_file)\n        for picture_file in style.ls(\"Pictures\"):\n            zip_odp.write(picture_file, style.cat(picture_file, True))\n        xml_data = style.cat(\"styles.xml\", False)\n        # import pdb;pdb.set_trace()\n        xml_data = self.override_styles(xml_data)\n        zip_odp.write(\"styles.xml\", xml_data)", "response": "Adds the OTP style file to the OTP file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_file(self, filename=None, write_style=True):\n        out = zipwrap.Zippier(filename, \"w\")\n        out.write(\"mimetype\", self.mime_type)\n        for p in self._pictures:\n            out.write(\"Pictures/%s\" % p.internal_name, p.get_data())\n        out.write(\"content.xml\", self.to_xml())\n        if write_style:\n            out.write(\"styles.xml\", self.styles_xml())\n        out.write(\"meta.xml\", self.meta_xml())\n        out.write(\"settings.xml\", self.settings_xml())\n        out.write(\"META-INF/manifest.xml\", self.manifest_xml(out))\n        return out", "response": "Writes the current instance to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the next node in the hierarchy.", "response": "def get_node(self):\n        \"\"\"\n\t    <anim:par smil:begin=\"next\">\n\t      <anim:par smil:begin=\"0s\">\n\t\t<anim:par smil:begin=\"0s\" smil:fill=\"hold\" presentation:node-type=\"on-click\" presentation:preset-class=\"entrance\" presentation:preset-id=\"ooo-entrance-appear\">\n\t\t  <anim:set smil:begin=\"0s\" smil:dur=\"0.001s\" smil:fill=\"hold\" smil:targetElement=\"id1\" anim:sub-item=\"text\" smil:attributeName=\"visibility\" smil:to=\"visible\"/>\n\t\t</anim:par>\n\t      </anim:par>\n\t    </anim:par>\n        \"\"\"\n        par = el(\"anim:par\", attrib={\"smil:begin\": \"next\"})\n        par2 = sub_el(par, \"anim:par\", attrib={\"smil:begin\": \"0s\"})\n        par3 = sub_el(\n            par2,\n            \"anim:par\",\n            attrib={\n                \"smil:begin\": \"0s\",\n                \"smil:fill\": \"hold\",\n                \"presentation:node-type\": \"on-click\",\n                \"presentation:preset-class\": \"entrance\",\n                \"presentation:preset-id\": \"ooo-entrance-appear\",\n            },\n        )\n        if self.ids:\n            for id in self.ids:\n                sub_el(\n                    par3,\n                    \"anim:set\",\n                    attrib={\n                        \"smil:begin\": \"0s\",\n                        \"smil:dur\": \"0.001s\",\n                        \"smil:fill\": \"hold\",\n                        \"smil:targetElement\": id,\n                        \"anim:sub-item\": \"text\",\n                        \"smil:attributeName\": \"visibility\",\n                        \"smil:to\": \"visible\",\n                    },\n                )\n\n        else:\n            sub_el(\n                par3,\n                \"anim:set\",\n                attrib={\n                    \"smil:begin\": \"0s\",\n                    \"smil:dur\": \"0.001s\",\n                    \"smil:fill\": \"hold\",\n                    \"smil:targetElement\": self.id,\n                    \"anim:sub-item\": \"text\",\n                    \"smil:attributeName\": \"visibility\",\n                    \"smil:to\": \"visible\",\n                },\n            )\n\n        return par"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_frame_attributes(self, attrib):\n\n        if \"align\" in self.user_defined:\n            align = self.user_defined[\"align\"]\n            if \"top\" in align:\n                attrib[\"style:vertical-pos\"] = \"top\"\n            if \"right\" in align:\n                attrib[\"style:horizontal-pos\"] = \"right\"\n        return attrib", "response": "For positioning update the frame attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse to update fill - color", "response": "def update_style(self, mapping):\n        \"\"\"Use to update fill-color\"\"\"\n        default = {\n            \"presentation:background-visible\": \"true\",\n            \"presentation:background-objects-visible\": \"true\",\n            \"draw:fill\": \"solid\",\n            \"draw:fill-color\": \"#772953\",\n            \"draw:fill-image-width\": \"0cm\",\n            \"draw:fill-image-height\": \"0cm\",\n            \"presentation:display-footer\": \"true\",\n            \"presentation:display-page-number\": \"false\",\n            \"presentation:display-date-time\": \"true\",\n        }\n        default.update(mapping)\n        style = PageStyle(**default)\n        node = style.style_node()\n        # add style to automatic-style\n        self.preso._auto_styles.append(node)\n        # update page style-name\n        # found in ._page\n        self._page.set(ns(\"draw\", \"style-name\"), node.attrib[ns(\"style\", \"name\")])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npush a pending node to the current element.", "response": "def push_pending_node(self, name, attr):\n        \"\"\"\n        pending nodes are for affecting type, such as wrapping content\n        with text:a to make a hyperlink.  Anything in pending nodes\n        will be written before the actual text.\n        User needs to remember to pop out of it.\n        \"\"\"\n        if self.cur_element is None:\n            self.add_text_frame()\n        self.cur_element.pending_nodes.append((name, attr))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_picture(self, p):\n        # pictures should be added the the draw:frame element\n        self.pic_frame = PictureFrame(self, p)\n        self.pic_frame.add_node(\n            \"draw:image\",\n            attrib={\n                \"xlink:href\": \"Pictures/\" + p.internal_name,\n                \"xlink:type\": \"simple\",\n                \"xlink:show\": \"embed\",\n                \"xlink:actuate\": \"onLoad\",\n            },\n        )\n        self._preso._pictures.append(p)\n        node = self.pic_frame.get_node()\n        self._page.append(node)", "response": "Adds a picture to the page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _copy(self):\n        ins = copy.copy(self)\n        ins._fire_page_number(self.page_number + 1)\n        return ins", "response": "returns a copy of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_node(self):\n        # already added title, text frames\n        # add animation chunks\n        if self.animations:\n            anim_par = el(\"anim:par\", attrib={\"presentation:node-type\": \"timing-root\"})\n            self._page.append(anim_par)\n            anim_seq = sub_el(\n                anim_par, \"anim:seq\", attrib={\"presentation:node-type\": \"main-sequence\"}\n            )\n            for a in self.animations:\n                a_node = a.get_node()\n                anim_seq.append(a_node)\n\n        # add notes now (so they are last)\n        if self.notes_frame:\n            notes = self.notes_frame.get_node()\n            self._page.append(notes)\n        if self.footer:\n            self._page.attrib[ns(\"presentation\", \"use-footer-name\")] = self.footer.name\n        return self._page", "response": "return etree Element representing this slide"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_list(self, bl):\n        # text:list doesn't like being a child of text:p\n        if self.cur_element is None:\n            self.add_text_frame()\n        self.push_element()\n        self.cur_element._text_box.append(bl.node)\n        style = bl.style_name\n        if style not in self._preso._styles_added:\n            self._preso._styles_added[style] = 1\n            content = bl.default_styles_root()[0]\n            self._preso._auto_styles.append(content)\n        self.cur_element = bl", "response": "add_list adds a new entry to the list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_table(self, t):\n        self.push_element()\n        self._page.append(t.node)\n        self.cur_element = t", "response": "add a new table to the page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over nodes replace text with mapping", "response": "def update_text(self, mapping):\n        \"\"\"Iterate over nodes, replace text with mapping\"\"\"\n        found = False\n        for node in self._page.iter(\"*\"):\n            if node.text or node.tail:\n                for old, new in mapping.items():\n                    if node.text and old in node.text:\n                        node.text = node.text.replace(old, new)\n                        found = True\n                    if node.tail and old in node.tail:\n                        node.tail = node.tail.replace(old, new)\n                        found = True\n        if not found:\n            raise KeyError(\"Updating text failed with mapping:{}\".format(mapping))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing to parent of node with name and set as cur_node.", "response": "def parent_of(self, name):\n        \"\"\"\n        go to parent of node with name, and set as cur_node.  Useful\n        for creating new paragraphs\n       \"\"\"\n        if not self._in_tag(name):\n            return\n\n        node = self.cur_node\n        while node.tag != name:\n            node = node.getparent()\n        self.cur_node = node.getparent()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if cur_node is tagname with attributes", "response": "def _is_last_child(self, tagname, attributes=None):\n        \"\"\"\n        Check if last child of cur_node is tagname with attributes\n        \"\"\"\n        children = self.cur_node.getchildren()\n        if children:\n            result = self._is_node(tagname, attributes, node=children[-1])\n            return result\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _in_tag(self, tagname, attributes=None):\n        node = self.cur_node\n        while not node is None:\n            if node.tag == tagname:\n                if attributes and node.attrib == attributes:\n                    return True\n\n                elif attributes:\n                    return False\n\n                return True\n\n            node = node.getparent()\n        return False", "response": "Determine if we are already in a certain tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns False if bad to make name a child of parent.", "response": "def _check_add_node(self, parent, name):\n        \"\"\" Returns False if bad to make name a child of parent \"\"\"\n        if name == ns(\"text\", \"a\"):\n            if parent.tag == ns(\"draw\", \"text-box\"):\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_styles(self, add_paragraph=True, add_text=True):\n        p_styles = self.get_para_styles()\n        t_styles = self.get_span_styles()\n        for s in self.slide.pending_styles:\n            if isinstance(s, ParagraphStyle):\n                p_styles.update(s.styles)\n            elif isinstance(s, TextStyle):\n                t_styles.update(s.styles)\n\n        para = ParagraphStyle(**p_styles)\n\n        if add_paragraph or self.slide.paragraph_attribs:\n            p_attrib = {ns(\"text\", \"style-name\"): para.name}\n            p_attrib.update(self.slide.paragraph_attribs)\n            if not self._in_tag(ns(\"text\", \"p\"), p_attrib):\n                self.parent_of(ns(\"text\", \"p\"))\n                # Create paragraph style first\n                self.slide._preso.add_style(para)\n                self.add_node(\"text:p\", attrib=p_attrib)\n\n        # span is only necessary if style changes\n        if add_text and t_styles:\n            text = TextStyle(**t_styles)\n            children = self.cur_node.getchildren()\n            if children:\n                # if we already are using this text style, reuse the last one\n                last = children[-1]\n                if (\n                    last.tag == ns(\"text\", \"span\")\n                    and last.attrib[ns(\"text\", \"style-name\")] == text.name\n                    and last.tail is None\n                ):  # if we have a tail, we can't reuse\n                    self.cur_node = children[-1]\n                    return\n\n            if not self._is_node(\n                ns(\"text\", \"span\"), {ns(\"text\", \"style-name\"): text.name}\n            ):\n                # Create text style\n                self.slide._preso.add_style(text)\n                self.add_node(\"text:span\", attrib={\"text:style-name\": text.name})", "response": "Adds paragraph and span wrappers if necessary based on style\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line_break(self):\n        for i in range(self.slide.insert_line_break):\n            # needs to be inside text:p\n            if not self._in_tag(ns(\"text\", \"p\")):\n                # we can just add a text:p and no line-break\n                # Create paragraph style first\n                self.add_node(ns(\"text\", \"p\"))\n            self.add_node(ns(\"text\", \"line-break\"))\n            self.pop_node()\n            if self.cur_node.tag == ns(\"text\", \"p\"):\n                return\n\n            if self.cur_node.getparent().tag != ns(\"text\", \"p\"):\n                self.pop_node()\n        self.slide.insert_line_break = 0", "response": "insert as many line breaks as the insert_line_break variable says\n            is not set"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the text to the current node.", "response": "def write(self, text, add_p_style=True, add_t_style=True):\n        \"\"\"\n        see mixed content\n        http://effbot.org/zone/element-infoset.htm#mixed-content\n        Writing is complicated by requirements of odp to ignore\n        duplicate spaces together.  Deal with this by splitting on\n        white spaces then dealing with the '' (empty strings) which\n        would be the extra spaces\n        \"\"\"\n        self._add_styles(add_p_style, add_t_style)\n        self._add_pending_nodes()\n\n        spaces = []\n        for i, letter in enumerate(text):\n            if letter == \" \":\n                spaces.append(letter)\n                continue\n\n            elif len(spaces) == 1:\n                self._write(\" \")\n                self._write(letter)\n                spaces = []\n                continue\n\n            elif spaces:\n                num_spaces = len(spaces) - 1\n                # write just a plain space at the start\n                self._write(\" \")\n                if num_spaces > 1:\n                    # write the attrib only if more than one space\n                    self.add_node(\"text:s\", {\"text:c\": str(num_spaces)})\n                else:\n                    self.add_node(\"text:s\")\n                self.pop_node()\n                self._write(letter)\n                spaces = []\n                continue\n\n            self._write(letter)\n\n        if spaces:\n            num_spaces = len(spaces)\n            if num_spaces > 1:\n                self.add_node(\"text:s\", {\"text:c\": str(num_spaces)})\n            else:\n                self.add_node(\"text:s\")\n            self.pop_node()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef style_node(self, additional_style_attrib=None):\n        style_attrib = {\"style:name\": self.name, \"style:family\": self.FAMILY}\n        if additional_style_attrib:\n            style_attrib.update(additional_style_attrib)\n        if self.PARENT_STYLE_DICT:\n            style_attrib.update(self.PARENT_STYLE_DICT)\n\n        node = el(\"style:style\", attrib=style_attrib)\n        props = sub_el(node, self.STYLE_PROP, attrib=self.styles)\n        return node", "response": "generate a style node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_file_operation(outdoc, filenames, use_segment_table, operation, preserve = True):\n\n    proc_id = table.get_table(outdoc, lsctables.ProcessTable.tableName)[0].process_id\n\n    # load up the files into individual documents\n    xmldocs = [ligolw_add.ligolw_add(ligolw.Document(), [fname]) for fname in filenames]\n\n\n    # Get the list of dinstinct segment_definers across all docs\n    segment_definers = {}\n\n    def register_definer(seg_def):\n        key = (seg_def.ifos, seg_def.name, seg_def.version)\n        segment_definers[key] = True\n        return key\n\n    for xmldoc in xmldocs:\n        seg_def_table = table.get_table(xmldoc, lsctables.SegmentDefTable.tableName)\n        map (register_definer, seg_def_table)\n\n    # For each unique segment definer, find the intersection\n    for ifo, name, version in segment_definers:\n        if operation == INTERSECT:\n            # If I were feeling especially functional-ist I'd write this\n            # with reduce()\n            result = pycbc_glue.segments.segmentlist([pycbc_glue.segments.segment(-pycbc_glue.segments.infinity(), pycbc_glue.segments.infinity())])\n            for xmldoc in xmldocs:\n                result &= find_segments(xmldoc, '%s:%s:%d' % (ifo, name, version), use_segment_table)\n        elif operation == UNION:\n            result = pycbc_glue.segments.segmentlist([])\n\n            for xmldoc in xmldocs:\n                result |= find_segments(xmldoc, '%s:%s:%d' % (ifo, name, version), use_segment_table)\n        elif operation == DIFF:\n            result = find_segments(xmldocs[0], '%s:%s:%d' % (ifo, name, version), use_segment_table)\n            \n            for xmldoc in xmldocs[1:]:\n                result -= find_segments(xmldoc, '%s:%s:%d' % (ifo, name, version), use_segment_table)\n        else:\n            raise NameError (\"%s is not a known operation (intersect, union or diff)\" % operation)\n\n\n        # Add a segment definer for the result\n        seg_def_id = add_to_segment_definer(outdoc, proc_id, ifo, name, version)\n\n        # Add the segments\n        if use_segment_table:\n            add_to_segment(outdoc, proc_id, seg_def_id, result)\n        else:\n            add_to_segment_summary(outdoc, proc_id, seg_def_id, result)\n\n    # If we're preserving, also load up everything into the output document.\n    if preserve:\n        # Add them to the output document\n        map(lambda x: outdoc.appendChild(x.childNodes[0]), xmldocs)\n\n        # Merge the ligolw elements and tables\n        ligolw_add.merge_ligolws(outdoc)\n        ligolw_add.merge_compatible_tables(outdoc)\n\n    return outdoc, abs(result)", "response": "This function runs an operation across a set of files and returns a list of files that are found in the output document."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_segment_operation(outdoc, filenames, segments, use_segment_table, operation, result_name = 'RESULT', preserve = True):\n\n    proc_id = table.get_table(outdoc, lsctables.ProcessTable.tableName)[0].process_id\n\n    if preserve:\n        indoc = ligolw_add.ligolw_add(outdoc, filenames)\n    else:\n        indoc = ligolw_add.ligolw_add(ligolw.Document(), filenames)\n\n    # Start with a segment covering all of time, then\n    # intersect with each of the fields of interest\n    keys = segments.split(',')\n\n    if operation == INTERSECT:\n        sgmntlist = pycbc_glue.segments.segmentlist([pycbc_glue.segments.segment(-pycbc_glue.segments.infinity(), pycbc_glue.segments.infinity())])\n\n        for key in keys:\n            sgmntlist &= find_segments(indoc, key, use_segment_table)\n\n    elif operation == UNION:\n        sgmntlist = pycbc_glue.segments.segmentlist([])\n\n        for key in keys:\n            sgmntlist |= find_segments(indoc, key, use_segment_table)\n    elif operation == DIFF:\n        sgmntlist = find_segments(indoc, keys[0], use_segment_table)\n\n        for key in keys[1:]:\n            sgmntlist -= find_segments(indoc, key, use_segment_table)\n    else:\n        raise NameError(\"%s is not a known operation (intersect, union or diff)\" % operation)\n\n\n    # Add a segment definer and segments\n    seg_def_id = add_to_segment_definer(outdoc, proc_id, '', result_name, 1)\n\n    if use_segment_table:\n        add_to_segment(outdoc, proc_id, seg_def_id, sgmntlist)\n    else:\n        add_to_segment_summary(outdoc, proc_id, seg_def_id, sgmntlist)\n\n    return outdoc, abs(sgmntlist)", "response": "This function runs an operation across a set of segments. This function returns a set of segments that are not in use_segment_table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __setParentSymbol(self, value):\n\n\t\terrors = []\n\t\tif not value is str and not value.split():\n\t\t\terrors.append('parentSymbol_ERROR : Symbol : must be char or string!')\n\t\telse:\n\t\t\tself.__parentSymbol = value\n\n\t\tif errors:\n\t\t\tview.Tli.showErrors('SymbolError', errors)", "response": "setter for parentSymbol variable setter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __tableStringParser(self, tableString):\n\n\t\terror = []\n\t\theader = []\n\t\tdecisions = []\n\n\t\tif tableString.split() == []:\n\t\t\terror.append('Table variable is empty!')\n\t\telse:\n\t\t\ttableString = tableString.split('\\n')\n\t\t\tnewData = []\n\t\t\tfor element in tableString:\n\t\t\t\tif element.strip():\n\t\t\t\t\tnewData.append(element)\n\n\t\t\tfor element in newData[0].split():\n\t\t\t\tif not element in header:\n\t\t\t\t\theader.append(element)\n\t\t\t\telse:\n\t\t\t\t\terror.append('Header element: ' + element + ' is not unique!')\n\n\t\t\tfor i, tableString in enumerate(newData[2:]):\n\t\t\t\tsplit = tableString.split()\n\t\t\t\tif len(split) == len(header):\n\t\t\t\t\tdecisions.append(split)\n\t\t\t\telse:\n\t\t\t\t\terror.append('Row: {}==> missing: {} data'.format(\n\t\t\t\t\t\tstr(i).ljust(4),\n\t\t\t\t\t\tstr(len(header) - len(split)).ljust(2))\n\t\t\t\t\t)\n\n\t\tif error:\n\t\t\tview.Tli.showErrors('TableStringError', error)\n\t\telse:\n\t\t\treturn [header, decisions]", "response": "This function will parse and check the tableString parameter for any invalid strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __replaceSpecialValues(self, decisions):\n\t\terror = []\n\t\tfor row, line in enumerate(decisions):\n\t\t\tif '.' in line:\n\t\t\t\tfor i, element in enumerate(line):\n\t\t\t\t\tif row == 0:\n\t\t\t\t\t\terror.append(\n\t\t\t\t\t\t\t\"Row: {}colume: {}==> don't have parent value\".format(str(row).ljust(4), str(i).ljust(4)))\n\t\t\t\t\tif element == self.__parentSymbol:\n\t\t\t\t\t\tif decisions[row - 1][i] == '.':\n\t\t\t\t\t\t\terror.append(\"Row: {}Colume: {}==> don't have parent value\".format(str(row).ljust(4),\n\t\t\t\t\t\t\t                                                                   str(i).ljust(4)))\n\n\t\t\t\t\t\tdecisions[row][i] = decisions[row - 1][i]\n\n\t\tif error:\n\t\t\tview.Tli.showErrors('ReplaceSpecialValuesError', error)\n\t\telse:\n\t\t\treturn decisions", "response": "This method replaces special values in decisions array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __valueKeyWithHeaderIndex(self, values):\n\n\t\tmachingIndexes = {}\n\t\tfor index, name in enumerate(self.header):\n\t\t\tif name in values:\n\t\t\t\tmachingIndexes[index] = values[name]\n\t\treturn machingIndexes", "response": "This is hellper function so that we can mach decision values with row index with header index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncheckering of decision parameters, it will raise ValueError if finds something wrong. Args: result (array of str): See public decision methods **values (array of str): See public decision methods Raise: ValueError: Result array none. ValueError: Values dict none. ValueError: Not find result key in header. ValueError: Result value is empty. Returns: Error array values", "response": "def __checkDecisionParameters(self, result, **values):\n\t\t\"\"\"\n\t\tChecker of decision parameters, it will raise ValueError if finds something wrong.\n\n\t\tArgs:\n\t\t\tresult (array of str): See public decision methods\n\t\t\t**values (array of str): See public decision methods\n\n\t\tRaise:\n\t\t\tValueError: Result array none.\n\t\t\tValueError: Values dict none.\n\t\t\tValueError: Not find result key in header.\n\t\t\tValueError: Result value is empty.\n\n\t\tReturns:\n\t\t\tError array values\n\n\t\t\"\"\"\n\t\terror = []\n\n\t\tif not result:\n\t\t\terror.append('Function parameter (result array) should contain one or more header string!')\n\n\t\tif not values:\n\t\t\terror.append('Function parameter (values variables) should contain one or more variable')\n\n\t\tfor header in result:\n\t\t\tif not header in self.header:\n\t\t\t\terror.append('String (' + header + ') in result is not in header!')\n\n\t\tfor header in values:\n\t\t\tif not header in self.header:\n\t\t\t\terror.append('Variable (' + header + ') in values is not in header!')\n\t\t\telif not values[header].split():\n\t\t\t\terror.append('Variable (' + header + ') in values is empty string')\n\n\t\tif error:\n\t\t\treturn error"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __getDecision(self, result, multiple=False, **values):\n\n\t\tvalues = self.__toString(values)\n\t\t__valueKeyWithHeaderIndex = self.__valueKeyWithHeaderIndex(values)\n\n\t\terrors = self.__checkDecisionParameters(result, **values)\n\t\tif errors:\n\t\t\tview.Tli.showErrors('ParametersError', errors)\n\n\t\tmachingData = {}\n\t\tfor line in self.decisions:\n\n\t\t\tmatch = True\n\n\t\t\tfor index in __valueKeyWithHeaderIndex:\n\t\t\t\tif line[index] != __valueKeyWithHeaderIndex[index]:\n\t\t\t\t\tif line[index] != self.__wildcardSymbol:\n\t\t\t\t\t\tmatch = False\n\t\t\t\t\t\tbreak\n\n\t\t\tif match:\n\t\t\t\tif multiple:\n\t\t\t\t\tfor header in result:\n\t\t\t\t\t\tif header not in machingData:\n\t\t\t\t\t\t\tmachingData[header] = [line[self.header.index(header)]]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tmachingData[header].append(line[self.header.index(header)])\n\t\t\t\telse:\n\t\t\t\t\tfor header in result:\n\t\t\t\t\t\tmachingData[header] = line[self.header.index(header)]\n\t\t\t\t\treturn machingData\n\n\t\tif multiple:\n\t\t\tif machingData:\n\t\t\t\treturn machingData\n\n\t\t# Return none if not found (not string so\n\t\t# not found value can be recognized\n\t\treturn dict((key, None) for key in result)", "response": "This method is used to get decision values from the result array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decisionCall(self, callback, result, **values):\n\t\tcallback(**self.__getDecision(result, **values))", "response": "This method will call the decision method with callback option. This method will find matching row construct decision table and call callback with dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decision(self, result, **values):\n\t\tdata = self.__getDecision(result, **values)\n\t\tdata = [data[value] for value in result]\n\t\tif len(data) == 1:\n\t\t\treturn data[0]\n\t\telse:\n\t\t\treturn data", "response": "This method will find matching row in result and call callback with dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\njoust like self.decision but for multiple finded values. Returns: Arrays of arrays of finded elements or if finds only one mach, array of strings.", "response": "def allDecisions(self, result, **values):\n\t\t\"\"\"\n\t\tJoust like self.decision but for multiple finded values.\n\n\t\tReturns:\n\t\t\tArrays of arrays of finded elements or if finds only one mach, array of strings.\n\t\t\"\"\"\n\t\tdata = self.__getDecision(result, multiple=True, **values)\n\t\tdata = [data[value] for value in result]\n\t\tif len(data) == 1:\n\t\t\treturn data[0]\n\t\telse:\n\t\t\treturn data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_server(region='EU-London', mode=None):\n    if mode:\n        region = '%s:%s' % (region, mode)\n    opener = urllib.request.build_opener()\n    opener.addheaders = default_headers\n    data = '%s\\n%s' % (region, handshake_version)\n    return opener.open('http://m.agar.io/', data=data.encode()) \\\n        .read().decode().split('\\n')[0:2]", "response": "Find a server in a given region."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_party_address(party_token):\n    opener = urllib.request.build_opener()\n    opener.addheaders = default_headers\n    try:\n        data = party_token.encode()\n        return opener.open('http://m.agar.io/getToken', data=data) \\\n            .read().decode().split('\\n')[0]\n    except urllib.error.HTTPError:\n        raise ValueError('Invalid token \"%s\" (maybe timed out,'\n                         ' try creating a new one)' % party_token)", "response": "Returns the address of the party server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate random lines of the image.", "response": "def create_lines(self, draw, n_line, width, height):\n        '''\u7ed8\u5236\u5e72\u6270\u7ebf'''\n        line_num = randint(n_line[0], n_line[1])  # \u5e72\u6270\u7ebf\u6761\u6570\n        for i in range(line_num):\n            # \u8d77\u59cb\u70b9\n            begin = (randint(0, width), randint(0, height))\n            # \u7ed3\u675f\u70b9\n            end = (randint(0, width), randint(0, height))\n            draw.line([begin, end], fill=(0, 0, 0))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the points in the screen.", "response": "def create_points(self, draw, point_chance, width, height):\n        '''\u7ed8\u5236\u5e72\u6270\u70b9'''\n        chance = min(100, max(0, int(point_chance)))  # \u5927\u5c0f\u9650\u5236\u5728[0, 100]\n\n        for w in range(width):\n            for h in range(height):\n                tmp = randint(0, 100)\n                if tmp > 100 - chance:\n                    draw.point((w, h), fill=(0, 0, 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_out(command):\n  # start external command process\n  p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n  # get outputs\n  out, _ = p.communicate()\n\n  return p.returncode, out.strip()", "response": "Run the given command and return a tuple of the return code and output of enclosing whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_call_out(command):\n  # start external command process\n  p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n  # get outputs\n  out, _ = p.communicate()\n\n  # throw exception if process failed\n  if p.returncode != 0:\n    raise GitInvocationError, 'failed to run \"%s\"' % \" \".join(command)\n\n  return out.strip()", "response": "Run the given command and return the output as a\n string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dict_to_map_str_str(self, d):\n        return dict(map(\n            lambda (k, v): (k, str(v).lower() if isinstance(v, bool) else str(v)),\n            d.iteritems()\n        ))", "response": "Convert dict values to map str values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_domain_name(url):\n    if not validate_url(url):\n        raise WrongUrlError(f'Couldn\\'t match domain name of this url: {url}')\n\n    ext = tldextract.extract(url)\n    return f'{ext.domain}.{ext.suffix}'", "response": "Extract a domain name from the url."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_video_data(_data):\n\n    data = _data.copy()\n\n    # TODO: fix this ugliness\n    title = data.get('title')\n    if title:\n        data['title'] = clean_title(title)\n\n    return data", "response": "Clean video data:\n        -> cleans title\n        -> ...\n\n    Args:\n        _data (dict): Information about the video.\n\n    Returns:\n        dict: Refined video data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean title -> remove dates, remove duplicated spaces and strip title. Args: title (str): Title. Returns: str: Clean title without dates, duplicated, trailing and leading spaces.", "response": "def clean_title(title):\n    \"\"\"\n    Clean title -> remove dates, remove duplicated spaces and strip title.\n\n    Args:\n        title (str): Title.\n\n    Returns:\n        str: Clean title without dates, duplicated, trailing and leading spaces.\n\n    \"\"\"\n    date_pattern = re.compile(r'\\W*'\n                              r'\\d{1,2}'\n                              r'[/\\-.]'\n                              r'\\d{1,2}'\n                              r'[/\\-.]'\n                              r'(?=\\d*)(?:.{4}|.{2})'\n                              r'\\W*')\n    title = date_pattern.sub(' ', title)\n    title = re.sub(r'\\s{2,}', ' ', title)\n    title = title.strip()\n    return title"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ext(url):\n\n    parsed = urllib.parse.urlparse(url)\n    root, ext = os.path.splitext(parsed.path)\n    return ext.lstrip('.')", "response": "Extract an extension from a url."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_duplicates(seq):\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if not (x in seen or seen_add(x))]", "response": "Removes duplicates from an iterable preserving the order."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming an eqJoin on with the given model and returns the result.", "response": "def joinOn(self, model, onIndex):\n        \"\"\"\n        Performs an eqJoin on with the given model. The resulting join will be\n        accessible through the models name.\n        \"\"\"\n        return self._joinOnAsPriv(model, onIndex, model.__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike joinOn but allows setting the joined results name to access it from.", "response": "def joinOnAs(self, model, onIndex, whatAs):\n        \"\"\"\n        Like `joinOn` but allows setting the joined results name to access it\n        from.\n\n        Performs an eqJoin on with the given model. The resulting join will be\n        accessible through the given name.\n        \"\"\"\n        return self._joinOnAsPriv(model, onIndex, whatAs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _joinOnAsPriv(self, model, onIndex, whatAs):\n        if self._join:\n            raise Exception(\"Already joined with a table!\")\n\n        self._join = model\n        self._joinedField = whatAs\n        table = model.table\n        self._query = self._query.eq_join(onIndex, r.table(table))\n        return self", "response": "Private method for handling joins."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nallow for the results to be ordered by a specific field.", "response": "def orderBy(self, field, direct=\"desc\"):\n        \"\"\"\n        Allows for the results to be ordered by a specific field. If given,\n        direction can be set with passing an additional argument in the form\n        of \"asc\" or \"desc\"\n        \"\"\"\n        if direct == \"desc\":\n            self._query = self._query.order_by(r.desc(field))\n        else:\n            self._query = self._query.order_by(r.asc(field))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nallows for skipping a specified number of results in the query. Useful for pagination.", "response": "def offset(self, value):\n        \"\"\"\n        Allows for skipping a specified number of results in query. Useful\n        for pagination.\n        \"\"\"\n\n        self._query = self._query.skip(value)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallowing for limiting number of results returned for query. Useful for pagination.", "response": "def limit(self, value):\n        \"\"\"\n        Allows for limiting number of results returned for query. Useful\n        for pagination.\n        \"\"\"\n        self._query = self._query.limit(value)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch(self):\n        returnResults = []\n\n        results = self._query.run()\n        for result in results:\n            if self._join:\n                # Because we can tell the models to ignore certian fields,\n                # through the protectedItems blacklist, we can nest models by\n                # name and have each one act normal and not accidentally store\n                # extra data from other models\n                item = self._model.fromRawEntry(**result[\"left\"])\n                joined = self._join.fromRawEntry(**result[\"right\"])\n                item.protectedItems = self._joinedField\n                item[self._joinedField] = joined\n\n            else:\n                item = self._model.fromRawEntry(**result)\n\n            returnResults.append(item)\n\n        self._documents = returnResults\n        return self._documents", "response": "Fetches the data from the database and stores it in the self. _documents attribute."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncoercing PUT requests to POST requests.", "response": "def coerce_put_post(request):\n    \"\"\"\n    Django doesn't particularly understand REST.\n    In case we send data over PUT, Django won't\n    actually look at the data and load it. We need\n    to twist its arm here.\n\n    The try/except abominiation here is due to a bug\n    in mod_python. This should fix it.\n    \"\"\"\n    if request.method == \"PUT\":\n        # Bug fix: if _load_post_and_files has already been called, for\n        # example by middleware accessing request.POST, the below code to\n        # pretend the request is a POST instead of a PUT will be too late\n        # to make a difference. Also calling _load_post_and_files will result\n        # in the following exception:\n        #   AttributeError: You cannot set the upload handlers after the upload has been processed.\n        # The fix is to check for the presence of the _post field which is set\n        # the first time _load_post_and_files is called (both by wsgi.py and\n        # modpython.py). If it's set, the request has to be 'reset' to redo\n        # the query value parsing in POST mode.\n        if hasattr(request, '_post'):\n            del request._post\n            del request._files\n\n        try:\n            request.method = \"POST\"\n            request._load_post_and_files()\n            request.method = \"PUT\"\n        except AttributeError:\n            request.META['REQUEST_METHOD'] = 'POST'\n            request._load_post_and_files()\n            request.META['REQUEST_METHOD'] = 'PUT'\n\n        request.PUT = request.POST"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a function ref to deserialize content for a certain mimetype.", "response": "def loader_for_type(self, ctype):\n        \"\"\"\n        Gets a function ref to deserialize content\n        for a certain mimetype.\n        \"\"\"\n        for loadee, mimes in Mimer.TYPES.iteritems():\n            for mime in mimes:\n                if ctype.startswith(mime):\n                    return loadee"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the content type of the request in all cases where the request is not a submitted form - application - x - www - form - urlencoded", "response": "def content_type(self):\n        \"\"\"\n        Returns the content type of the request in all cases where it is\n        different than a submitted form - application/x-www-form-urlencoded\n        \"\"\"\n        type_formencoded = \"application/x-www-form-urlencoded\"\n\n        ctype = self.request.META.get('CONTENT_TYPE', type_formencoded)\n\n        if type_formencoded in ctype:\n            return None\n\n        return ctype"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef translate(self):\n        ctype = self.content_type()\n        self.request.content_type = ctype\n\n        if not self.is_multipart() and ctype:\n            loadee = self.loader_for_type(ctype)\n\n            if loadee:\n                try:\n                    self.request.data = loadee(self.request.raw_post_data)\n\n                    # Reset both POST and PUT from request, as its\n                    # misleading having their presence around.\n                    self.request.POST = self.request.PUT = dict()\n                except (TypeError, ValueError):\n                    # This also catches if loadee is None.\n                    raise MimerDataException\n            else:\n                self.request.data = None\n\n        return self.request", "response": "Translate the content - type of the request into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getsteps(levels, tagmax):\n\tntw = levels\n\tif ntw < 2:\n\t\tntw = 2\n\n\tsteps = [(stp, 1 + (stp * int(math.ceil(tagmax * 1.0 / ntw - 1))))\n\t\t\t\tfor stp in range(ntw)]\n\t# just to be sure~\n\tsteps[-1] = (steps[-1][0], tagmax+1)\n\treturn steps", "response": "Returns a list with the max number of posts per tagcloud level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build(site, tagdata):\n\n\ttagdata.sort()\n\n\t# we get the most popular tag to calculate the tags' weigth\n\ttagmax = 0\n\tfor tagname, tagcount in tagdata:\n\t\tif tagcount > tagmax:\n\t\t\ttagmax = tagcount\n\tsteps = getsteps(site.tagcloud_levels, tagmax)\n\n\ttags = []\n\tfor tagname, tagcount in tagdata:\n\t\tweight = [twt[0] \\\n\t\t\tfor twt in steps if twt[1] >= tagcount and twt[1] > 0][0]+1\n\t\ttags.append({'tagname':tagname, 'count':tagcount, 'weight':weight})\n\treturn tags", "response": "Returns the tag cloud for a list of tags."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a query and get the results.", "response": "def getquery(query):\n\t'Performs a query and get the results.'\n\ttry:\n\t\tconn = connection.cursor()\n\t\tconn.execute(query)\n\t\tdata = conn.fetchall()\n\t\tconn.close()\n\texcept: data = list()\n\treturn data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cloudata(site):\n\n\t# XXX: this looks like it can be done via ORM\n\ttagdata = getquery(\"\"\"\n\t\tSELECT feedjack_post.feed_id, feedjack_tag.name, COUNT(*)\n\t\tFROM feedjack_post, feedjack_subscriber, feedjack_tag,\n\t\tfeedjack_post_tags\n\t\tWHERE feedjack_post.feed_id=feedjack_subscriber.feed_id AND\n\t\tfeedjack_post_tags.tag_id=feedjack_tag.id AND\n\t\tfeedjack_post_tags.post_id=feedjack_post.id AND\n\t\tfeedjack_subscriber.site_id=%d\n\t\tGROUP BY feedjack_post.feed_id, feedjack_tag.name\n\t\tORDER BY feedjack_post.feed_id, feedjack_tag.name\"\"\" % site.id)\n\ttagdict = {}\n\tglobaldict = {}\n\tcloudict = {}\n\tfor feed_id, tagname, tagcount in tagdata:\n\t\tif feed_id not in tagdict:\n\t\t\ttagdict[feed_id] = []\n\t\ttagdict[feed_id].append((tagname, tagcount))\n\t\ttry:\n\t\t\tglobaldict[tagname] += tagcount\n\t\texcept KeyError:\n\t\t\tglobaldict[tagname] = tagcount\n\ttagdict[0] = globaldict.items()\n\tfor key, val in tagdict.items():\n\t\tcloudict[key] = build(site, val)\n\treturn cloudict", "response": "Returns a dictionary with all the tag clouds related to a site."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the tag cloud for a site or a site s subscriber.", "response": "def getcloud(site, feed_id=None):\n\t\"\"\" Returns the tag cloud for a site or a site's subscriber.\n\t\"\"\"\n\n\tcloudict = fjcache.cache_get(site.id, 'tagclouds')\n\tif not cloudict:\n\t\tcloudict = cloudata(site)\n\t\tfjcache.cache_set(site, 'tagclouds', cloudict)\n\n\t# A subscriber's tag cloud has been requested.\n\tif feed_id:\n\t\tfeed_id = int(feed_id)\n\t\tif feed_id in cloudict:\n\t\t\treturn cloudict[feed_id]\n\t\treturn []\n\t# The site tagcloud has been requested.\n\treturn cloudict[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_stations(page):\n    '''Extract bus stations from routine page.\n\n    :param page: crawled page.\n    '''\n    stations = [_(station.value) for station in page('.stateName')]\n    return {\n        'terminal': {\n            stations[0]: list(reversed(stations)),\n            stations[-1]: stations\n        },\n        'stations': stations\n    }", "response": "Extract bus stations from routine page.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract current routine information from page.", "response": "def extract_current_routine(page, stations):\n    '''Extract current routine information from page.\n\n    :param page: crawled page.\n    :param stations: bus stations list. See `~extract_stations`.\n    '''\n    current_routines = CURRENT_ROUTINE_PATTERN.findall(page.text())\n    if not current_routines:\n        return\n\n    terminal_station = stations['stations'][-1]\n    for routine in current_routines:\n        if _(routine[0]) == terminal_station:\n            distance = int(routine[1])\n    stations_to_this_dir = stations['terminal'][terminal_station]\n\n    waiting_station = _(page('.now .stateName').val())\n    idx = stations_to_this_dir.index(waiting_station)\n    bus_station = stations_to_this_dir[idx - distance + 1]\n\n    return {\n        'destinate_station': terminal_station,\n        'bus_station': bus_station,\n        'waiting_station': waiting_station,\n        'distance': distance\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_bus_routine(page):\n    '''Extract bus routine information from page.\n\n    :param page: crawled page.\n    '''\n    if not isinstance(page, pq):\n        page = pq(page)\n\n    stations = extract_stations(page)\n    return {\n        # Routine name.\n        'name': extract_routine_name(page),\n\n        # Bus stations.\n        'stations': stations,\n\n        # Current routine.\n        'current': extract_current_routine(page, stations)\n    }", "response": "Extract bus routine information from page.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the result of the function against the first function in the group.", "response": "def validate(self):\n        \"\"\"\n        Execute the code once to get it's results (to be used in function validation). Compare the result to the\n        first function in the group.\n        \"\"\"\n        validation_code = self.setup_src + '\\nvalidation_result = ' + self.stmt\n        validation_scope = {}\n        exec(validation_code, validation_scope)\n        # Store the result in the first function in the group.\n        if len(self.groups[self.group]) == 1:\n            self.result = validation_scope['validation_result']\n            logging.info('PyPerform: Validating group \"{b.group}\" against function \"{b.callable.__name__}\"'\n                         .format(b=self))\n        else:\n            compare_against_benchmark = self.groups[self.group][0]\n            test = [benchmark.result_validation for benchmark in self.groups[self.group]]\n            if not all(test):\n                raise ValueError('All functions within a group must have the same validation flag.')\n            compare_result = compare_against_benchmark.result\n            if self.validation_func:\n                results_are_valid = self.validation_func(compare_result, validation_scope['validation_result'])\n            else:\n                results_are_valid = compare_result == validation_scope['validation_result']\n            if results_are_valid:\n                logging.info('PyPerform: Validating {}......PASSED!'.format(self.callable.__name__))\n            else:\n                error = 'Results of functions {0} and {1} are not equivalent.\\n{0}:\\t {2}\\n{1}:\\t{3}'\n                raise ValidationError(error.format(compare_against_benchmark.callable.__name__, self.callable.__name__,\n                                          compare_result, validation_scope['validation_result']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntabulating and write the results of ComparisonBenchmarks in a file or standard out.", "response": "def summarize(group, fs=None, include_source=True):\n        \"\"\"\n        Tabulate and write the results of ComparisonBenchmarks to a file or standard out.\n        :param str group: name of the comparison group.\n        :param fs: file-like object (Optional)\n        \"\"\"\n        _line_break = '{0:-<120}\\n'.format('')\n        tests = sorted(ComparisonBenchmark.groups[group], key=lambda t: getattr(t, 'time_average_seconds'))\n        log = StringIO.StringIO()\n        log.write('Call statement:\\n\\n')\n        log.write('\\t' + tests[0].stmt)\n        log.write('\\n\\n\\n')\n        fmt = \"{0: <8} {1: <35} {2: <12} {3: <15} {4: <15} {5: <14}\\n\"\n        log.write(fmt.format('Rank', 'Function Name', 'Time', '% of Slowest', 'timeit_repeat', 'timeit_number'))\n        log.write(_line_break)\n        log.write('\\n')\n\n        for i, t in enumerate(tests):\n            func_name = \"{}.{}\".format(t.classname, t.callable.__name__) if t.classname else t.callable.__name__\n            if i == len(tests)-1:\n                time_percent = 'Slowest'\n            else:\n                time_percent = \"{:.1f}\".format(t.time_average_seconds / tests[-1].time_average_seconds * 100)\n            log.write(fmt.format(i+1,\n                                 func_name,\n                                 convert_time_units(t.time_average_seconds),\n                                 time_percent,\n                                 t.timeit_repeat,\n                                 t.timeit_number))\n        log.write(_line_break)\n\n        if include_source:\n            log.write('\\n\\n\\nSource Code:\\n')\n            log.write(_line_break)\n            for test in tests:\n                log.write(test.log.getvalue())\n                log.write(_line_break)\n\n        if isinstance(fs, str):\n            with open(fs, 'w') as f:\n                f.write(log.getvalue())\n\n        elif fs is None:\n            print(log.getvalue())\n        else:\n            try:\n                fs.write(log.getvalue())\n            except AttributeError as e:\n                print(e)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _grabData(self, key):\n        rawCursor = r.table(self.table).get(key).run(self._conn)\n        if rawCursor:\n            self._data = rawCursor\n            self._new = False\n            return True\n        else:\n            return False", "response": "Tries to find the existing document in the database and sets the _data attribute to that document. If it is found returns True otherwise returns False."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value of the given item in the object.", "response": "def _set(self, item, value):\n        \"\"\"\n        Helper function to keep the __setattr__ and __setitem__ calls\n        KISSish\n\n        Will only set the objects _data if the given items name is not prefixed\n        with _ or if the item exists in the protected items List.\n        \"\"\"\n        if item not in object.__getattribute__(self, \"_protectedItems\") \\\n                and item[0] != \"_\":\n            keys = object.__getattribute__(self, \"_data\")\n            if not hasattr(value, '__call__'):\n                keys[item] = value\n                return value\n            if hasattr(value, '__call__') and item in keys:\n                raise Exception(\"\"\"Cannot set model data to a function, same \\\nname exists in data\"\"\")\n        return object.__setattr__(self, item, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new object from a raw entry.", "response": "def fromRawEntry(cls, **kwargs):\n        \"\"\"\n        Helper function to allow wrapping existing data/entries, such as\n        those returned by collections.\n        \"\"\"\n        id = kwargs[\"id\"]\n\n        kwargs.pop(\"id\")\n\n        what = cls(**kwargs)\n        what._new = False\n        what.id = id\n\n        return what"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(cls, id=None, **kwargs):\n        what = cls(**kwargs)\n        if id:\n            setattr(what, cls.primaryKey, id)\n        what.save()\n        return what", "response": "Create a new object of the specified class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self):\n        if not self._new:\n            data = self._data.copy()\n            ID = data.pop(self.primaryKey)\n            reply = r.table(self.table).get(ID) \\\n                .update(data,\n                        durability=self.durability,\n                        non_atomic=self.non_atomic) \\\n                .run(self._conn)\n\n        else:\n            reply = r.table(self.table) \\\n                .insert(self._data,\n                        durability=self.durability,\n                        upsert=self.upsert) \\\n                .run(self._conn)\n            self._new = False\n\n        if \"generated_keys\" in reply and reply[\"generated_keys\"]:\n            self._data[self.primaryKey] = reply[\"generated_keys\"][0]\n\n        if \"errors\" in reply and reply[\"errors\"] > 0:\n            raise Exception(\"Could not insert entry: %s\"\n                            % reply[\"first_error\"])\n\n        return True", "response": "Save the current object to the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self):\n        if self._new:\n            raise Exception(\"This is a new object, %s not in data, \\\nindicating this entry isn't stored.\" % self.primaryKey)\n\n        r.table(self.table).get(self._data[self.primaryKey]) \\\n            .delete(durability=self.durability).run(self._conn)\n        return True", "response": "Deletes the current instance of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_edge(self, node1_name, node2_name, edge_length=DEFAULT_EDGE_LENGTH):\n        if not self.__has_node(name=node1_name):\n            raise ValueError(\"Can not add an edge to a non-existing node {name}\".format(name=node1_name))\n        if self.__has_node(name=node2_name):\n            raise ValueError(\"Can not add an edge to already existing node {name}\".format(name=node2_name))\n        self.multicolors_are_up_to_date = False\n        self.__get_node_by_name(name=node1_name).add_child(name=node2_name, dist=edge_length)", "response": "Adds an edge between two nodes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_node_by_name(self, name):\n        try:\n            for entry in filter(lambda x: x.name == name, self.nodes()):\n                return entry\n        except StopIteration:\n            raise ValueError(\"Attempted to retrieve a non-existing tree node with name: {name}\"\n                             \"\".format(name=name))", "response": "Returns a TreeNode object which name matches the specified argument name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boolean flag telling if a tree has an edge with two nodes specified by their names as arguments", "response": "def __has_edge(self, node1_name, node2_name, account_for_direction=True):\n        \"\"\" Returns a boolean flag, telling if a tree has an edge with two nodes, specified by their names as arguments\n\n        If a account_for_direction is specified as True, the order of specified node names has to relate to parent - child relation,\n        otherwise both possibilities are checked\n        \"\"\"\n        try:\n            p1 = self.__get_node_by_name(name=node1_name)\n            wdir = node2_name in (node.name for node in p1.children)\n            if account_for_direction:\n                return wdir\n            else:\n                p2 = self.__get_node_by_name(name=node2_name)\n                return wdir or node1_name in (node.name for node in p2.children)\n        except ValueError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_edge(self, node1_name, node2_name, account_for_direction=True):\n        return self.__has_edge(node1_name=node1_name, node2_name=node2_name, account_for_direction=account_for_direction)", "response": "Returns True if the node1_name is in the tree and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the length of an edge or path that is specified by a pair of vertices node1_name and node2_name.", "response": "def get_distance(self, node1_name, node2_name):\n        \"\"\" Returns a length of an edge / path, if exists, from the current tree\n\n        :param node1_name: a first node name in current tree\n        :param node2_name: a second node name in current tree\n        :return: a length of specified by a pair of vertices edge / path\n        :rtype: `Number`\n        :raises: ValueError, if requested a length of an edge, that is not present in current tree\n        \"\"\"\n        return self.__root.get_distance(target=node1_name, target2=node2_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __get_v_tree_consistent_leaf_based_hashable_multicolors(self):\n        result = []\n        nodes = deque([self.__root])\n        while len(nodes) > 0:\n            current_node = nodes.popleft()\n            children = current_node.children\n            nodes.extend(children)\n            if not current_node.is_leaf():\n                leaves = filter(lambda node: node.is_leaf(), current_node.get_descendants())\n                result.append(Multicolor(*[self.__leaf_wrapper(leaf.name) for leaf in leaves]))\n            else:\n                result.append(Multicolor(self.__leaf_wrapper(current_node.name)))\n        result.append(Multicolor())\n        return result", "response": "Internally used method that recalculates VTree - consistent sets of leaves in the current tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __update_consistent_multicolors(self):\n        v_t_consistent_multicolors = self.__get_v_tree_consistent_leaf_based_hashable_multicolors()\n\n        hashed_vtree_consistent_leaves_multicolors = {mc.hashable_representation for mc in v_t_consistent_multicolors}\n        self.vtree_consistent_multicolors_set = hashed_vtree_consistent_leaves_multicolors\n        self.vtree_consistent_multicolors = [Multicolor(*hashed_multicolor) for hashed_multicolor in\n                                             hashed_vtree_consistent_leaves_multicolors]\n        result = []\n        # T-consistent multicolors can be viewed as VT-consistent multicolors united with all of their complements\n        full_multicolor = v_t_consistent_multicolors[0]\n        for multicolor in v_t_consistent_multicolors:\n            result.append(multicolor)\n            result.append(full_multicolor - multicolor)\n\n        hashed_tree_consistent_leaves_multicolors = {mc.hashable_representation for mc in result}\n        self.tree_consistent_multicolors_set = hashed_tree_consistent_leaves_multicolors\n        self.tree_consistent_multicolors = [Multicolor(*hashed_multicolor) for hashed_multicolor in\n                                            hashed_tree_consistent_leaves_multicolors]\n        self.multicolors_are_up_to_date = True", "response": "Internally used method that recalculates T - consistent multicolors for current tree topology and VT - consistent multicolors for current tree topology."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a specified tree to the node with the specified name.", "response": "def append(self, node_name, tree, copy=False):\n        \"\"\" Append a specified tree (represented by a root TreeNode element) to the node, specified by its name\n\n        :param copy: a flag denoting if the appended tree has to be added as is, or is the deepcopy of it has to be used\n        :type copy: Boolean\n        :raises: ValueError (if no node with a specified name, to which the specified tree has to be appended, is present in the current tree)\n        \"\"\"\n        self.multicolors_are_up_to_date = False\n        tree_to_append = tree.__root if not copy else deepcopy(tree.__root)\n        self.__get_node_by_name(node_name).add_child(tree_to_append)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a square thumbnail", "response": "def generateThumbnail(self):\n        \"\"\"Generates a square thumbnail\"\"\"\n        image = pilImage.open(ROOT / self.source.name)\n        box, width, height = cropBox(self.width, self.height)\n\n        # Resize\n        image.thumbnail((width, height), pilImage.ANTIALIAS)\n        # Crop from center\n        box = cropBox(*image.size)[0]\n        image = image.crop(box)\n        # save\n        self.thumbnail = self.source.name.replace(self.hash, '__{}'.format(self.hash))\n        image.save(ROOT / self.thumbnail.name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, hashVal, hashPath, tags=None, galleries=None):\n\n        self.source = hashPath.replace('\\\\', '/').replace(ROOT, '')\n        galleries = galleries or []\n        tags = tags or []\n\n        # -- Get info\n        videodata = self.info()\n        self.width = videodata['width']\n        self.height = videodata['height']\n        self.framerate = videodata['framerate']\n        self.duration = videodata['duration']\n\n        self.generateThumbnail()\n\n        for gal in galleries:\n            g = Gallery.objects.get(pk=int(gal))\n            g.videos.add(self)\n\n        self.tagArtist()\n\n        for tagName in tags:\n            tag = Tag.objects.get_or_create(name=tagName)[0]\n            self.tags.add(tag)\n\n        if not self.guid:\n            self.guid = self.getGuid().guid\n\n        # -- Set the temp video while processing\n        queuedvideo = VideoQueue.objects.get_or_create(video=self)[0]\n        queuedvideo.save()\n\n        self.save()\n\n        try:\n            item = VideoQueue()\n            item.video = self\n            item.save()\n        except IntegrityError:\n            # -- Already queued\n            pass", "response": "This function will export the image to the asset folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generateThumbnail(self):\n        source = ROOT / self.source.name\n        thumbnail = source.parent / '_{}.jpg'.format(source.namebase)\n\n        # -- Save thumbnail and put into queue\n        poster = source.parent / '__{}.jpg'.format(source.namebase)\n        cmd = [FROG_FFMPEG, '-i', str(source), '-ss', '1', '-vframes', '1', str(thumbnail), '-y']\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        proc.communicate()\n        image = pilImage.open(thumbnail)\n        image.save(poster)\n        self.poster = poster.replace(ROOT, '')\n\n        box, width, height = cropBox(self.width, self.height)\n\n        # Resize\n        image.thumbnail((width, height), pilImage.ANTIALIAS)\n        # Crop from center\n        box = cropBox(*image.size)[0]\n        image = image.crop(box)\n        # save\n        self.thumbnail = thumbnail.replace(ROOT, '')\n        image.save(thumbnail)", "response": "Generates a square thumbnail"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reload_me(*args, ignore_patterns=[]):\n    \n    command = [sys.executable, sys.argv[0]]\n    command.extend(args)\n\n    reload(*command, ignore_patterns=ignore_patterns)", "response": "Reload currently running command with given args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_data_string(data_string):\n        data_string = data_string.strip()\n        linear_terminator_index = data_string.index(\"$\") if \"$\" in data_string else -1\n        circular_terminator_index = data_string.index(\"@\") if \"@\" in data_string else -1\n        if linear_terminator_index < 0 and circular_terminator_index < 0:\n            raise ValueError(\"Invalid data string. No chromosome termination sign ($|@) found.\")\n        if linear_terminator_index == 0 or circular_terminator_index == 0:\n            raise ValueError(\"Invalid data string. No data found before chromosome was terminated.\")\n        if linear_terminator_index < 0 or 0 < circular_terminator_index < linear_terminator_index:\n            ###############################################################################################\n            #\n            # we either encountered only a circular chromosome termination sign\n            # or we have encountered it before we've encountered the circular chromosome termination sign first\n            #\n            ###############################################################################################\n            chr_type = \"@\"\n            terminator_index = circular_terminator_index\n        else:\n            chr_type = \"$\"\n            terminator_index = linear_terminator_index\n        ###############################################################################################\n        #\n        # everything after first fragment termination sign is omitted\n        #\n        ###############################################################################################\n        data = data_string[:terminator_index].strip()\n        ###############################################################################################\n        #\n        # genomic blocks are separated between each other by the space character\n        #\n        ###############################################################################################\n        split_data = data.split()\n        blocks = []\n        for block in split_data:\n            ###############################################################################################\n            #\n            # since positively oriented blocks can be denoted both as \"+block\" as well as \"block\"\n            # we need to figure out where \"block\" name starts\n            #\n            ###############################################################################################\n            cut_index = 1 if block.startswith(\"-\") or block.startswith(\"+\") else 0\n            if cut_index == 1 and len(block) == 1:\n                ###############################################################################################\n                #\n                # block can not be empty\n                # from this one can derive the fact, that names \"+\" and \"-\" for blocks are forbidden\n                #\n                ###############################################################################################\n                raise ValueError(\"Empty block name definition\")\n            blocks.append((\"-\" if block.startswith(\"-\") else \"+\", block[cut_index:]))\n        return chr_type, blocks", "response": "Parses a string containing gene order data into a tuple of the appropriate type and size."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign usual BreakpointGraph type vertices to supplied block.", "response": "def __assign_vertex_pair(block):\n        \"\"\" Assigns usual BreakpointGraph type vertices to supplied block.\n\n        Vertices are labeled as \"block_name\" + \"h\" and \"block_name\" + \"t\" according to blocks orientation.\n\n        :param block: information about a genomic block to create a pair of vertices for in a format of ( ``+`` | ``-``, block_name)\n        :type block: ``(str, str)``\n        :return: a pair of vertices labeled according to supplied blocks name (respecting blocks orientation)\n        :rtype: ``(str, str)``\n        \"\"\"\n        sign, name = block\n        data = name.split(BlockVertex.NAME_SEPARATOR)\n        root_name, data = data[0], data[1:]\n        tags = [entry.split(TaggedVertex.TAG_SEPARATOR) for entry in data]\n        for tag_entry in tags:\n            if len(tag_entry) == 1:\n                tag_entry.append(None)\n            elif len(tag_entry) > 2:\n                tag_entry[1:] = [TaggedVertex.TAG_SEPARATOR.join(tag_entry[1:])]\n        tail, head = root_name + \"t\", root_name + \"h\"\n        tail, head = TaggedBlockVertex(tail), TaggedBlockVertex(head)\n        tail.mate_vertex = head\n        head.mate_vertex = tail\n        for tag, value in tags:\n            head.add_tag(tag, value)\n            tail.add_tag(tag, value)\n        return (tail, head) if sign == \"+\" else (head, tail)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_edges_from_parsed_data(parsed_data):\n        chr_type, blocks = parsed_data\n        vertices = []\n        for block in blocks:\n            ###############################################################################################\n            #\n            # each block is represented as a pair of vertices (that correspond to block extremities)\n            #\n            ###############################################################################################\n            v1, v2 = GRIMMReader.__assign_vertex_pair(block)\n            vertices.append(v1)\n            vertices.append(v2)\n        if chr_type == \"@\":\n            ###############################################################################################\n            #\n            # if we parse a circular genomic fragment we must introduce an additional pair of vertices (edge)\n            # that would connect two outer most vertices in the vertex list, thus connecting fragment extremities\n            #\n            ###############################################################################################\n            vertex = vertices.pop()\n            vertices.insert(0, vertex)\n        elif chr_type == \"$\":\n            ###############################################################################################\n            #\n            # if we parse linear genomic fragment, we introduce two artificial (infinity) vertices\n            # that correspond to fragments ends, and introduce edges between them and respective outermost block vertices\n            #\n            # if outermost vertices at this moment are repeat vertices, the outermost pair shall be discarded and the innermost\n            # vertex info shall be utilized in the infinity vertex, that is introduced for the fragment extremity\n            #\n            ###############################################################################################\n            if vertices[0].is_repeat_vertex:\n                left_iv_tags = sorted([(tag, value) if tag != \"repeat\" else (tag, BGVertex.get_vertex_name_root(vertices[1].name))\n                                       for tag, value in vertices[1].tags])\n                left_iv_root_name = BGVertex.get_vertex_name_root(vertices[2].name)\n                vertices = vertices[2:]\n            else:\n                left_iv_tags = []\n                left_iv_root_name = vertices[0].name\n            if vertices[-1].is_repeat_vertex:\n                right_iv_tags = sorted(\n                        [(tag, value) if tag != \"repeat\" else (tag, BGVertex.get_vertex_name_root(vertices[-2].name))\n                         for tag, value in vertices[-2].tags])\n                right_iv_root_name = BGVertex.get_vertex_name_root(vertices[-3].name)\n                vertices = vertices[:-2]\n            else:\n                right_iv_tags = []\n                right_iv_root_name = BGVertex.get_vertex_name_root(vertices[-1].name)\n            left_iv, right_iv = TaggedInfinityVertex(left_iv_root_name), TaggedInfinityVertex(right_iv_root_name)\n            left_iv.tags = left_iv_tags\n            right_iv.tags = right_iv_tags\n            vertices.insert(0, left_iv)\n            vertices.append(right_iv)\n        return [(v1, v2) for v1, v2 in zip(vertices[::2], vertices[1::2])]", "response": "Given a list of parsed data returns a list of edges that correspond to the edges in the current GRIMM file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_breakpoint_graph(stream, merge_edges=True):\n        result = BreakpointGraph()\n        current_genome = None\n        fragment_data = {}\n        for line in stream:\n            line = line.strip()\n            if len(line) == 0:\n                ###############################################################################################\n                #\n                # empty lines are omitted\n                #\n                ###############################################################################################\n                continue\n            if GRIMMReader.is_genome_declaration_string(data_string=line):\n                ###############################################################################################\n                #\n                # is we have a genome declaration, we must update current genome\n                # all following gene order data (before EOF or next genome declaration) will be attributed to current genome\n                #\n                ###############################################################################################\n                current_genome = GRIMMReader.parse_genome_declaration_string(data_string=line)\n                fragment_data = {}\n            elif GRIMMReader.is_comment_string(data_string=line):\n                if GRIMMReader.is_comment_data_string(string=line):\n                    path, (key, value) = GRIMMReader.parse_comment_data_string(comment_data_string=line)\n                    if len(path) > 0 and path[0] == \"fragment\":\n                        add_to_dict_with_path(destination_dict=fragment_data, key=key, value=value, path=path)\n                else:\n                    continue\n            elif current_genome is not None:\n                ###############################################################################################\n                #\n                # gene order information that is specified before the first genome is specified can not be attributed to anything\n                # and thus omitted\n                #\n                ###############################################################################################\n                parsed_data = GRIMMReader.parse_data_string(data_string=line)\n                edges = GRIMMReader.get_edges_from_parsed_data(parsed_data=parsed_data)\n                for v1, v2 in edges:\n                    edge_specific_data = {\n                        \"fragment\": {\n                            \"forward_orientation\": (v1, v2)\n                        }\n                    }\n                    edge = BGEdge(vertex1=v1, vertex2=v2, multicolor=Multicolor(current_genome), data=deepcopy(fragment_data))\n                    edge.update_data(source=edge_specific_data)\n                    result.add_bgedge(bgedge=edge,\n                                      merge=merge_edges)\n        return result", "response": "Returns a BreakpointGraph object that contains information about adjacencies in genome specified in GRIMM formatted input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of strings which represent genomes present in breakpoint graph as orders of blocks and is compatible with GRIMM format", "response": "def get_blocks_in_grimm_from_breakpoint_graph(bg):\n        \"\"\"\n        :param bg: a breakpoint graph, that contians all the information\n        :type bg: ``bg.breakpoint_graph.BreakpointGraph``\n        :return: list of strings, which represent genomes present in breakpoint graph as orders of blocks and is compatible with GRIMM format\n        \"\"\"\n        result = []\n        genomes = bg.get_overall_set_of_colors()\n        for genome in genomes:\n            genome_graph = bg.get_genome_graph(color=genome)\n            genome_blocks_orders = genome_graph.get_blocks_order()\n            blocks_orders = genome_blocks_orders[genome]\n            if len(blocks_orders) > 0:\n                result.append(\">{genome_name}\".format(genome_name=genome.name))\n            for chr_type, blocks_order in blocks_orders:\n                string = \" \".join(value if sign == \"+\" else sign + value for sign, value in blocks_order)\n                string += \" {chr_type}\".format(chr_type=chr_type)\n                result.append(string)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_file(request, filename, content_type='image/jpeg'):\n    wrapper = FixedFileWrapper(file(filename, 'rb'))\n    response = HttpResponse(wrapper, content_type=content_type)\n    response['Content-Length'] = os.path.getsize(filename)\n    return response", "response": "Send a file through Django without loading it into an arbitrary memory at once."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_zipfile(request, fileList):\n    temp = tempfile.TemporaryFile()\n    archive = zipfile.ZipFile(temp, 'w', zipfile.ZIP_DEFLATED)\n    for artist,files in fileList.iteritems():\n        for f in files:\n            archive.write(f[0], '%s/%s' % (artist, f[1]))\n    archive.close()\n    wrapper = FixedFileWrapper(temp)\n    response = HttpResponse(wrapper, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=FrogSources.zip'\n    response['Content-Length'] = temp.tell()\n    temp.seek(0)\n    return response", "response": "Send a ZIP file to the client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef name(self):\n        return self.NAME_SEPARATOR.join([super(InfinityVertex, self).name, self.NAME_SUFFIX])", "response": "get the name of the class attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the name of the class", "response": "def name(self):\n        \"\"\" access to classic name attribute is hidden by this property \"\"\"\n        return self.NAME_SEPARATOR.join([super(TaggedVertex, self).name] + self.get_tags_as_list_of_strings())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_tag(self, tag, value):\n        index = bisect_left(self.tags, (tag, value))\n        contains = False\n        if index < len(self.tags):\n            contains = self.tags[index] == (tag, value)\n        if not contains:\n            self.tags.insert(index, (tag, value))", "response": "add a tag to the list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a tag from the list of tags", "response": "def remove_tag(self, tag, value, silent_fail=False):\n        \"\"\" we try to remove supplied pair tag -- value, and if does not exist outcome depends on the silent_fail flag \"\"\"\n        try:\n            self.tags.remove((tag, value))\n        except ValueError as err:\n            if not silent_fail:\n                raise err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new undirected edge with the same multicolor and edge1 and edge2.", "response": "def merge(cls, edge1, edge2):\n        \"\"\" Merges multi-color information from two supplied :class:`BGEdge` instances into a new :class:`BGEdge`\n\n        Since :class:`BGEdge` represents an undirected edge, created edge's vertices are assign according to the order in first supplied edge.\n\n        Accounts for subclassing.\n\n        :param edge1: first out of two edge information from which is to be merged into a new one\n        :param edge2: second out of two edge information from which is to be merged into a new one\n        :return: a new undirected with multi-color information merged from two supplied :class:`BGEdge` objects\n        :raises: ``ValueError``\n        \"\"\"\n        if edge1.vertex1 != edge2.vertex1 and edge1.vertex1 != edge2.vertex2:\n            raise ValueError(\"Edges to be merged do not connect same vertices\")\n        forward = edge1.vertex1 == edge2.vertex1\n        if forward and edge1.vertex2 != edge2.vertex2:\n            raise ValueError(\"Edges to be merged do not connect same vertices\")\n        elif not forward and edge1.vertex2 != edge2.vertex1:\n            raise ValueError(\"Edges to be merged do not connect same vertices\")\n        return cls(vertex1=edge1.vertex1, vertex2=edge1.vertex2, multicolor=edge1.multicolor + edge2.multicolor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef colors_json_ids(self):\n        return [color.json_id if hasattr(color, \"json_id\") else hash(color) for color in self.multicolor.multicolors.elements()]", "response": "A proxy property based access to vertices in current edge\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a connection object given a string url", "response": "def _get_connection_from_url(self, url, timeout, **kwargs):\n        \"\"\"Returns a connection object given a string url\"\"\"\n\n        url = self._decode_url(url, \"\")\n\n        if url.scheme == 'http' or url.scheme == 'https':\n            return HttpConnection(url.geturl(), timeout=timeout, **kwargs)\n        else:\n            if sys.version_info[0] > 2:\n                raise ValueError(\"Thrift transport is not available \"\n                                 \"for Python 3\")\n\n            try:\n                from thrift_connection import ThriftConnection\n            except ImportError:\n                raise ImportError(\"The 'thrift' python package \"\n                                    \"does not seem to be installed.\")\n            return ThriftConnection(url.hostname, url.port,\n                                    timeout=timeout, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_log_level(level, name, logger_name=None):\n    @CustomLogLevel(level, name, logger_name)\n    def _default_template(logger, msg, *args, **kwargs):\n        return msg, args, kwargs", "response": "Create a new log level that behaves like the default levels in the logging module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef userToJson(user):\n    obj = {\n        'id': user.id,\n        'username': user.username,\n        'name': user.get_full_name(),\n        'email': user.email,\n    }\n\n    return obj", "response": "Returns a serializable User dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a serializable Comment dict for the given Comment", "response": "def commentToJson(comment):\n    \"\"\"Returns a serializable Comment dict\n\n    :param comment: Comment to get info for\n    :type comment: Comment\n    :returns: dict\n    \"\"\"\n    obj = {\n        'id': comment.id,\n        'comment': comment.comment,\n        'user': userToJson(comment.user),\n        'date': comment.submit_date.isoformat(),\n    }\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPutData(request):\n    dataDict = {}\n    data = request.body\n\n    for n in urlparse.parse_qsl(data):\n        dataDict[n[0]] = n[1]\n\n    setattr(request, 'PUT', dataDict)\n    setattr(request, 'DELETE', dataDict)", "response": "Adds raw post to PUT and DELETE querydicts on the request so they behave like post\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getHashForFile(f):\n    hashVal = hashlib.sha1()\n    while True:\n        r = f.read(1024)\n        if not r:\n            break\n        hashVal.update(r)\n    f.seek(0)\n\n    return hashVal.hexdigest()", "response": "Returns a hash value for a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of objects based on a guid list", "response": "def getObjectsFromGuids(guids):\n    \"\"\"Gets the model objects based on a guid list\n\n    :param guids: Guids to get objects for\n    :type guids: list\n    :returns: list\n    \"\"\"\n    guids = guids[:]\n    img = list(Image.objects.filter(guid__in=guids))\n    vid = list(Video.objects.filter(guid__in=guids))\n    objects = img + vid\n    sortedobjects = []\n\n    if objects:\n        while guids:\n            for obj in iter(objects):\n                if obj.guid == guids[0]:\n                    sortedobjects.append(obj)\n                    guids.pop(0)\n                    break\n\n    return sortedobjects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getClientIP(request):\n    forwardedfor = request.META.get('HTTP_X_FORWARDED_FOR')\n    if forwardedfor:\n        ip = forwardedfor.split(',')[0]\n    else:\n        ip = request.META.get('REMOTE_ADDR')\n\n    return ip", "response": "Returns the best IP address found from the request"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndiscovers the plugin classes contained in Python files given a list of directory names to scan. Return a list of plugin classes.", "response": "def __discoverPlugins():\n    \"\"\" Discover the plugin classes contained in Python files, given a\n        list of directory names to scan. Return a list of plugin classes.\n    \"\"\"\n    for app in settings.INSTALLED_APPS:\n        if not app.startswith('django'):\n            module = __import__(app)\n            moduledir = path.Path(module.__file__).parent\n            plugin = moduledir / 'frog_plugin.py'\n            if plugin.exists():\n                file_, fpath, desc = imp.find_module('frog_plugin', [moduledir])\n                if file_:\n                    imp.load_module('frog_plugin', file_, fpath, desc)\n\n    return FrogPluginRegistry.plugins"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append(self, val):\n        self.values.append(val)\n        self.value = self.values[0]", "response": "Appends the object to the end of the list. Will also set the value to the first item in the values list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a serializable object", "response": "def asDict(self):\n        \"\"\"Returns a serializable object\"\"\"\n        return {\n            'isError': self.isError,\n            'message': self.message,\n            'values': self.values,\n            'value': self.value,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth(self, password):\n        future = concurrent.TracebackFuture()\n\n        def on_response(response):\n            \"\"\"Process the redis response\n\n            :param response: The future with the response\n            :type response: tornado.concurrent.Future\n\n            \"\"\"\n            exc = response.exception()\n            if exc:\n                if exc.args[0] == b'invalid password':\n                    future.set_exception(exceptions.AuthError(exc))\n                else:\n                    future.set_exception(exc)\n            else:\n                future.set_result(response.result())\n\n        execute_future = self._execute([b'AUTH', password], b'OK')\n        self.io_loop.add_future(execute_future, on_response)\n        return future", "response": "Request for authentication in a password - protected Redis server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect the DB with having the specified zero - based index.", "response": "def select(self, index=0):\n        \"\"\"Select the DB with having the specified zero-based numeric index.\n        New connections always use DB ``0``.\n\n        :param int index: The database to select\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        :raises: :exc:`~tredis.exceptions.InvalidClusterCommand`\n\n        \"\"\"\n        if self._clustering:\n            raise exceptions.InvalidClusterCommand\n        future = self._execute(\n            [b'SELECT', ascii(index).encode('ascii')], b'OK')\n\n        def on_selected(f):\n            self._connection.database = index\n\n        self.io_loop.add_future(future, on_selected)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the current time from the redis server.", "response": "def time(self):\n        \"\"\"Retrieve the current time from the redis server.\n\n        :rtype: float\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n\n        def format_response(value):\n            \"\"\"Format a TIME response into a datetime.datetime\n\n            :param list value: TIME response is a list of the number\n                of seconds since the epoch and the number of micros\n                as two byte strings\n            :rtype: float\n\n            \"\"\"\n            seconds, micros = value\n            return float(seconds) + (float(micros) / 1000000.0)\n\n        return self._execute([b'TIME'], format_callback=format_response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting video show name from the website.", "response": "def get_show_name(self):\n        \"\"\"\n        Get video show name from the website. It's located in the div with 'data-hover'\n        attribute under the 'title' key.\n\n        Returns:\n            str: Video show name.\n\n        \"\"\"\n        div = self.soup.find('div', attrs={'data-hover': True})\n        data = json.loads(div['data-hover'])\n        show_name = data.get('title')\n\n        return show_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes base url for a set of items.", "response": "def mk_url(self, *args, **kwargs):\n        \"\"\"\n        Args get parameterized into base url:\n            *(foo, bar, baz) -> /foo/bar/baz\n        Kwargs get encoded and appended to base url:\n            **{'hello':'world'} -> /foo/bar/baz?hello=world\n        \"\"\"\n        params = urlencode(kwargs)\n        url = '/' + '/'.join([x for x in args if x])\n        if params:\n            url += '?' + params\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ping(self, callback=None, **kwargs):\n        self.client.fetch(\n            self.mk_req('', method='HEAD', **kwargs),\n            callback = callback\n        )", "response": "Ping the elastic elastic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting basic info from the current cluster.", "response": "def info(self, callback=None, **kwargs):\n        \"\"\"\n        Get the basic info from the current cluster.\n        \"\"\"\n        self.client.fetch(\n            self.mk_req('', method='GET', **kwargs),\n            callback = callback\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a typed JSON document in a specific index.", "response": "def create_doc(self,\n            index,\n            doc_type,\n            body,\n            doc_id   = None,\n            params   = {},\n            callback = None,\n            **kwargs\n        ):\n        \"\"\"\n        Adds a typed JSON document in a specific index, making it searchable.\n        Behind the scenes this method calls index(..., op_type='create')\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html>`_\n        :arg index: The name of the index\n        :arg doc_type: The type of the document\n        :arg doc_id: Document ID\n        :arg body: The document\n        :arg consistency: Explicit write consistency setting for the operation\n        :arg id: Specific document ID (when the POST method is used)\n        :arg parent: ID of the parent document\n        :arg percolate: Percolator queries to execute while indexing the document\n        :arg refresh: Refresh the index after performing the operation\n        :arg replication: Specific replication type (default: sync)\n        :arg routing: Specific routing value\n        :arg timeout: Explicit operation timeout\n        :arg timestamp: Explicit timestamp for the document\n        :arg ttl: Expiration time for the document\n        :arg version: Explicit version number for concurrency control\n        :arg version_type: Specific version type\n        \"\"\"\n\n        method = 'PUT' if doc_id else 'POST'\n\n        query_params = ('consistency', 'op_type', 'parent', 'refresh',\n            'replication', 'routing', 'timeout', 'timestamp', 'ttl', 'version',\n            'version_type',\n        )\n\n        params = self._filter_params(query_params, params)\n\n        url = self.mk_url(*[index, doc_type, doc_id], **params)\n\n        self.client.fetch(\n            self.mk_req(url, method=method, body=body, **kwargs),\n            callback = callback\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scroll(self, scroll_id, params={}, callback=None, **kwargs):\n\n        query_params = ('scroll',)\n\n        params = self._filter_params(query_params, params)\n\n        url = self.mk_url(*['/_search/scroll'], **params)\n\n        self.client.fetch(\n            self.mk_req(url, method='GET', body=scroll_id, **kwargs),\n            callback = callback\n        )", "response": "Scroll a search request created by specifying the scroll parameter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_scroll(self,\n            scroll_id = None,\n            body      = '',\n            params    = {},\n            callback  = None,\n            **kwargs\n        ):\n        \"\"\"\n        Clear the scroll request created by specifying the scroll parameter to\n        search.\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html>`_\n        :arg scroll_id: The scroll ID or a list of scroll IDs\n        :arg body: A comma-separated list of scroll IDs to clear if none was\n            specified via the scroll_id parameter\n        \"\"\"\n\n        url = self.mk_url(*['_search', 'scroll', scroll_id])\n\n        self.client.fetch(\n            self.mk_req(url, method='DELETE', body=body, **kwargs),\n            callback = callback\n        )", "response": "Clear the scroll request for the specified item in the specified page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef abort_benchmark(self, name=None, params={}, body='', callback=None, **kwargs):\n\n        url = self.mk_url(*['_bench', 'abort', name])\n\n        self.client.fetch(\n            self.mk_req(url, method='POST', body=body, **kwargs),\n            callback = callback\n        )", "response": "Abort a running benchmark."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_benchmarks(self,\n            index    = None,\n            doc_type = None,\n            params   = {},\n            cb       = None,\n            **kwargs\n        ):\n        \"\"\"\n        View the progress of long-running benchmarks.\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-benchmark.html>`_\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg doc_type: The name of the document type\n        \"\"\"\n\n        url = self.mk_url(*[index, doc_type, '_bench'], **params)\n\n        self.client.fetch(\n            self.mk_req(url, method='GET', **kwargs),\n            callback = callback\n        )", "response": "List the long - running benchmarks for the specified language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a script in given language with specified ID.", "response": "def put_script(self, lang, script_id, body, params={}, callback=None, **kwargs):\n        \"\"\"\n        Create a script in given language with specified ID.\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html>`_\n        :arg lang: Script language\n        :arg script_id: Script ID\n        :arg body: The document\n        :arg op_type: Explicit operation type, default u'index'\n        :arg version: Explicit version number for concurrency control\n        :arg version_type: Specific version type\n        \"\"\"\n        query_params = ('op_type', 'version', 'version_type',)\n\n        params = self._filter_params(query_params, params)\n\n        url = self.mk_url(*['_scripts', lang, script_id], **params)\n\n        self.client.fetch(\n            self.mk_req(url, method='PUT', body=body, **kwargs),\n            callback = callback\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_template(self, temp_id, body, params={}, callback=None, **kwargs):\n\n        url = self.mk_url(*['_search', 'template', temp_id])\n\n        self.client.fetch(\n            self.mk_req(url, method='PUT', body=body, **kwargs),\n            callback = callback\n        )", "response": "Create a search template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a search template.", "response": "def delete_template(self, temp_id=None, params={}, callback=None, **kwargs):\n        \"\"\"\n        Delete a search template.\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_\n        :arg temp_id: Template ID\n        \"\"\"\n\n        url = self.mk_url(*['_search', 'template', temp_id])\n\n        self.client.fetch(\n            self.mk_req(url, method='DELETE', **kwargs),\n            callback = callback\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report(title='Unhandled Exception', exec_info=(), **kwargs):\n\n    exc_type, exc_value, tb = exec_info or sys.exc_info()\n    reporter = ExceptionReporter(exc_type, exc_value, tb)\n    html = reporter.get_traceback_html(**kwargs)\n\n    mail_admins(title, 'html only', html_message=html)", "response": "Create a technical server error response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_traceback_data(self):\n        default_template_engine = None\n\n        if default_template_engine is None:\n            template_loaders = []\n\n        frames = self.get_traceback_frames()\n        for i, frame in enumerate(frames):\n            if 'vars' in frame:\n                frame_vars = []\n                for k, v in frame['vars']:\n                    v = pformat(v)\n                    # The escape filter assume unicode, make sure that works\n                    if isinstance(v, six.binary_type):\n                        v = v.decode('utf-8', 'replace')  # don't choke on non-utf-8 input\n                    # Trim large blobs of data\n                    if v and len(v) > 4096:\n                        v = '%s... <trimmed %d bytes string>' % (v[0:4096], len(v))\n                    frame_vars.append((k, v))\n                frame['vars'] = frame_vars\n            frames[i] = frame\n\n        unicode_hint = ''\n        if self.exc_type and issubclass(self.exc_type, UnicodeError):\n            start = getattr(self.exc_value, 'start', None)\n            end = getattr(self.exc_value, 'end', None)\n            if start is not None and end is not None:\n                unicode_str = self.exc_value.args[1]\n\n        c = {\n            'is_email': False,\n            'frames': frames,\n            'sys_executable': sys.executable,\n            'sys_version_info': '%d.%d.%d' % sys.version_info[0:3],\n            'sys_path': sys.path,\n        }\n\n        # Check whether exception info is available\n        if self.exc_type:\n            c['exception_type'] = self.exc_type.__name__\n        if self.exc_value:\n            c['exception_value'] = self.exc_value\n        if frames:\n            c['lastframe'] = frames[-1]\n        return c", "response": "Return a dictionary containing traceback information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn HTML version of debug 500 HTTP error page.", "response": "def get_traceback_html(self, **kwargs):\n        \"Return HTML version of debug 500 HTTP error page.\"\n        t = Template(TECHNICAL_500_TEMPLATE)\n        c = self.get_traceback_data()\n        c['kwargs'] = kwargs\n        return t.render(Context(c))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n        source = None\n        if loader is not None and hasattr(loader, \"get_source\"):\n            try:\n                source = loader.get_source(module_name)\n            except ImportError:\n                pass\n            if source is not None:\n                source = source.splitlines()\n        if source is None:\n            try:\n                with open(filename, 'rb') as fp:\n                    source = fp.read().splitlines()\n            except (OSError, IOError):\n                pass\n        if source is None:\n            return None, [], None, []\n\n        # If we just read the source from a file, or if the loader did not\n        # apply tokenize.detect_encoding to decode the source into a Unicode\n        # string, then we should do that ourselves.\n        if isinstance(source[0], six.binary_type):\n            encoding = 'ascii'\n            for line in source[:2]:\n                # File coding may be specified. Match pattern from PEP-263\n                # (http://www.python.org/dev/peps/pep-0263/)\n                match = re.search(br'coding[:=]\\s*([-\\w.]+)', line)\n                if match:\n                    encoding = match.group(1).decode('ascii')\n                    break\n            source = [six.text_type(sline, encoding, 'replace') for sline in source]\n\n        lower_bound = max(0, lineno - context_lines)\n        upper_bound = lineno + context_lines\n\n        pre_context = source[lower_bound:lineno]\n        context_line = source[lineno]\n        post_context = source[lineno + 1:upper_bound]\n\n        return lower_bound, pre_context, context_line, post_context", "response": "Get lines from a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the traceback frames as a list", "response": "def get_traceback_frames(self):\n        \"\"\"Returns the traceback frames as a list\"\"\"\n        frames = []\n        tb = self.tb\n        while tb is not None:\n            # Support for __traceback_hide__ which is used by a few libraries\n            # to hide internal frames.\n            if tb.tb_frame.f_locals.get('__traceback_hide__'):\n                tb = tb.tb_next\n                continue\n            filename = tb.tb_frame.f_code.co_filename\n            function = tb.tb_frame.f_code.co_name\n            lineno = tb.tb_lineno - 1\n            loader = tb.tb_frame.f_globals.get('__loader__')\n            module_name = tb.tb_frame.f_globals.get('__name__') or ''\n            pre_context_lineno, pre_context, context_line, post_context = self._get_lines_from_file(\n                filename, lineno, 7, loader, module_name,\n            )\n            if pre_context_lineno is not None:\n                frames.append({\n                    'tb': tb,\n                    'type': 'django' if module_name.startswith('django.') else 'user',\n                    'filename': filename,\n                    'function': function,\n                    'lineno': lineno + 1,\n                    'vars': list(six.iteritems(tb.tb_frame.f_locals)),\n                    'id': id(tb),\n                    'pre_context': pre_context,\n                    'context_line': context_line,\n                    'post_context': post_context,\n                    'pre_context_lineno': pre_context_lineno + 1,\n                })\n            tb = tb.tb_next\n\n        return frames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_exception(self):\n        import traceback\n        frames = self.get_traceback_frames()\n        tb = [(f['filename'], f['lineno'], f['function'], f['context_line']) for f in frames]\n        list = ['Traceback (most recent call last):\\n']\n        list += traceback.format_list(tb)\n        list += traceback.format_exception_only(self.exc_type, self.exc_value)\n        return list", "response": "Return the same data as from traceback. format_exception."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, *keys):\n        return self._execute([b'DEL'] + list(keys), len(keys))", "response": "Removes the specified keys from the set or set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expire(self, key, timeout):\n        return self._execute(\n            [b'EXPIRE', key, ascii(timeout).encode('ascii')], 1)", "response": "Set a timeout on a key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the expiration time for the object with the given key.", "response": "def expireat(self, key, timestamp):\n        \"\"\":meth:`~tredis.RedisClient.expireat` has the same effect and\n        semantic as :meth:`~tredis.RedisClient.expire`, but instead of\n        specifying the number of seconds representing the TTL (time to live),\n        it takes an absolute Unix timestamp (seconds since January 1, 1970).\n\n        Please for the specific semantics of the command refer to the\n        documentation of :meth:`~tredis.RedisClient.expire`.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``\n\n        :param key: The key to set an expiration for\n        :type key: :class:`str`, :class:`bytes`\n        :param int timestamp: The UNIX epoch value for the expiration\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        return self._execute(\n            [b'EXPIREAT', key,\n             ascii(timestamp).encode('ascii')], 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef migrate(self,\n                host,\n                port,\n                key,\n                destination_db,\n                timeout,\n                copy=False,\n                replace=False):\n        \"\"\"Atomically transfer a key from a source Redis instance to a\n        destination Redis instance. On success the key is deleted from the\n        original instance and is guaranteed to exist in the target instance.\n\n        The command is atomic and blocks the two instances for the time\n        required to transfer the key, at any given time the key will appear to\n        exist in a given instance or in the other instance, unless a timeout\n        error occurs.\n\n        .. note::\n\n           **Time complexity**: This command actually executes a DUMP+DEL in\n           the source instance, and a RESTORE in the target instance. See the\n           pages of these commands for time complexity. Also an ``O(N)`` data\n           transfer between the two instances is performed.\n\n        :param host: The host to migrate the key to\n        :type host: bytes, str\n        :param int port: The port to connect on\n        :param key: The key to migrate\n        :type key: bytes, str\n        :param int destination_db: The database number to select\n        :param int timeout: The maximum idle time in milliseconds\n        :param bool copy: Do not remove the key from the local instance\n        :param bool replace: Replace existing key on the remote instance\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        command = [\n            b'MIGRATE', host,\n            ascii(port).encode('ascii'), key,\n            ascii(destination_db).encode('ascii'),\n            ascii(timeout).encode('ascii')\n        ]\n        if copy is True:\n            command.append(b'COPY')\n        if replace is True:\n            command.append(b'REPLACE')\n        return self._execute(command, b'OK')", "response": "This method will transfer a key from one Redis instance to another."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, key, db):\n        return self._execute([b'MOVE', key, ascii(db).encode('ascii')], 1)", "response": "Move the specified key from the currently selected database to the specified destination\n        database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pexpireat(self, key, timestamp):\n        return self._execute(\n            [b'PEXPIREAT', key,\n             ascii(timestamp).encode('ascii')], 1)", "response": "Set the expiration time for the key at the specified UNIX epoch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming the key to new_key.", "response": "def rename(self, key, new_key):\n        \"\"\"Renames ``key`` to ``new_key``. It returns an error when the source\n        and destination names are the same, or when ``key`` does not exist.\n        If ``new_key`` already exists it is overwritten, when this happens\n        :meth:`~tredis.RedisClient.rename` executes an implicit\n        :meth:`~tredis.RedisClient.delete` operation, so if the deleted key\n        contains a very big value it may cause high latency even if\n        :meth:`~tredis.RedisClient.rename` itself is usually a constant-time\n        operation.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``\n\n        :param key: The key to rename\n        :type key: :class:`str`, :class:`bytes`\n        :param new_key: The key to rename it to\n        :type new_key: :class:`str`, :class:`bytes`\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        return self._execute([b'RENAME', key, new_key], b'OK')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore(self, key, ttl, value, replace=False):\n        command = [b'RESTORE', key, ascii(ttl).encode('ascii'), value]\n        if replace:\n            command.append(b'REPLACE')\n        return self._execute(command, b'OK')", "response": "Restores the value associated with a key associated with a value obtained by dump."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan(self, cursor=0, pattern=None, count=None):\n\n        def format_response(value):\n            \"\"\"Format the response from redis\n\n            :param tuple value: The return response from redis\n            :rtype: tuple(int, list)\n\n            \"\"\"\n            return int(value[0]), value[1]\n\n        command = [b'SCAN', ascii(cursor).encode('ascii')]\n        if pattern:\n            command += [b'MATCH', pattern]\n        if count:\n            command += [b'COUNT', ascii(count).encode('ascii')]\n        return self._execute(command, format_callback=format_response)", "response": "This method is used to iterate over the redis database for a set of keys in the currently selected Redis database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort(self,\n             key,\n             by=None,\n             external=None,\n             offset=0,\n             limit=None,\n             order=None,\n             alpha=False,\n             store_as=None):\n        \"\"\"Returns or stores the elements contained in the list, set or sorted\n        set at key. By default, sorting is numeric and elements are compared by\n        their value interpreted as double precision floating point number.\n\n        The ``external`` parameter is used to specify the\n        `GET <http://redis.io/commands/sort#retrieving-external-keys>_`\n        parameter for retrieving external keys. It can be a single string\n        or a list of strings.\n\n        .. note::\n\n           **Time complexity**: ``O(N+M*log(M))`` where ``N`` is the number of\n           elements in the list or set to sort, and ``M`` the number of\n           returned elements. When the elements are not sorted, complexity is\n           currently ``O(N)`` as there is a copy step that will be avoided in\n           next releases.\n\n        :param key: The key to get the refcount for\n        :type key: :class:`str`, :class:`bytes`\n\n        :param by: The optional pattern for external sorting keys\n        :type by: :class:`str`, :class:`bytes`\n        :param external: Pattern or list of patterns to return external keys\n        :type external: :class:`str`, :class:`bytes`, list\n        :param int offset: The starting offset when using limit\n        :param int limit: The number of elements to return\n        :param order: The sort order - one of ``ASC`` or ``DESC``\n        :type order: :class:`str`, :class:`bytes`\n        :param bool alpha: Sort the results lexicographically\n        :param store_as: When specified, the key to store the results as\n        :type store_as: :class:`str`, :class:`bytes`, None\n        :rtype: list|int\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        :raises: :exc:`ValueError`\n\n        \"\"\"\n        if order and order not in [b'ASC', b'DESC', 'ASC', 'DESC']:\n            raise ValueError('invalid sort order \"{}\"'.format(order))\n\n        command = [b'SORT', key]\n        if by:\n            command += [b'BY', by]\n        if external and isinstance(external, list):\n            for entry in external:\n                command += [b'GET', entry]\n        elif external:\n            command += [b'GET', external]\n        if limit:\n            command += [\n                b'LIMIT',\n                ascii(offset).encode('utf-8'),\n                ascii(limit).encode('utf-8')\n            ]\n        if order:\n            command.append(order)\n        if alpha is True:\n            command.append(b'ALPHA')\n        if store_as:\n            command += [b'STORE', store_as]\n\n        return self._execute(command)", "response": "Returns or stores the elements contained in the list set or sorted\n        set at key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_thread_profiling(profile_dir, exception_callback=None):\n    global profiled_thread_enabled, Thread, Process\n    if os.path.isdir(profile_dir):\n        _Profiler.profile_dir = profile_dir\n    else:\n        raise OSError('%s does not exist' % profile_dir)\n    _Profiler.exception_callback = exception_callback\n    Thread = threading.Thread = ProfiledThread\n    Process = multiprocessing.Process = ProfiledProcess\n    profiled_thread_enabled = True", "response": "Enable profiling for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_thread_logging(exception_callback=None):\n    global logged_thread_enabled, Thread\n    LoggedThread.exception_callback = exception_callback\n    Thread = threading.Thread = LoggedThread\n    logged_thread_enabled = True", "response": "Monkey - patch threading. Thread with our own LoggedThread. exception_callback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a request based on method and calls the appropriate function", "response": "def image(request, obj_id):\n    \"\"\"Handles a request based on method and calls the appropriate function\"\"\"\n    obj = Image.objects.get(pk=obj_id)\n    if request.method == 'POST':\n        return post(request, obj)\n    elif request.method == 'PUT':\n        getPutData(request)\n        return put(request, obj)\n    elif request.method == 'DELETE':\n        getPutData(request)\n        return delete(request, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef video(request, obj_id):\n    obj = Video.objects.get(pk=obj_id)\n    if request.method == 'POST':\n        return post(request, obj)\n    elif request.method == 'PUT':\n        getPutData(request)\n        return put(request, obj)\n    elif request.method == 'DELETE':\n        getPutData(request)\n        return delete(request, obj)", "response": "Handles a request based on method and calls the appropriate function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_from_calc(self):\n        from aiida.common.exceptions import InvalidOperation\n        from aiida.common import aiidalogger\n        from aiida.backends.djsite.utils import get_dblogger_extra\n\n        import ase, ase.io\n\n        parserlogger = aiidalogger.getChild('aseparser')\n        logger_extra = get_dblogger_extra(self._calc)\n\n        # suppose at the start that the job is successful\n        successful = True\n\n        # check that calculation is in the right state\n        state = self._calc.get_state()\n        if state != calc_states.PARSING:\n            raise InvalidOperation(\"Calculation not in {} state\"\n                                   .format(calc_states.PARSING) )\n\n        # select the folder object\n        out_folder = self._calc.get_retrieved_node()\n\n        # check what is inside the folder\n        list_of_files = out_folder.get_folder_list()\n\n        # at least the stdout should exist\n        if not self._calc._OUTPUT_FILE_NAME in list_of_files:\n            successful = False\n            parserlogger.error(\"Standard output not found\",extra=logger_extra)\n            return successful,()\n\n        # output structure\n        has_out_atoms = True if self._calc._output_aseatoms in list_of_files else False\n        if has_out_atoms:\n            out_atoms = ase.io.read( out_folder.get_abs_path( self._calc._output_aseatoms ) )\n            out_structure = StructureData().set_ase(out_atoms)\n\n        # load the results dictionary\n        json_outfile = out_folder.get_abs_path( self._calc._OUTPUT_FILE_NAME )\n        with open(json_outfile,'r') as f:\n            json_params = json.load(f)\n\n        # extract arrays from json_params\n        dictionary_array = {}\n        for k,v in list(json_params.iteritems()):\n            if isinstance(v, (list,tuple)):\n                dictionary_array[k] = json_params.pop(k)\n\n        # look at warnings\n        warnings = []\n        with open(out_folder.get_abs_path( self._calc._SCHED_ERROR_FILE )) as f:\n            errors = f.read()\n        if errors:\n            warnings = [errors]\n        json_params['warnings'] = warnings\n\n        # save the outputs\n        new_nodes_list= []\n\n        # save the arrays\n        if dictionary_array:\n            array_data = ArrayData()\n            for k,v in dictionary_array.iteritems():\n                array_data.set_array(k,numpy.array(v))\n            new_nodes_list.append( (self._outarray_name, array_data) )\n\n        # save the parameters\n        if json_params:\n            parameter_data = ParameterData( dict=json_params )\n            new_nodes_list.append( (self._outdict_name, parameter_data) )\n\n        if has_out_atoms:\n            structure_data = StructureData()\n            new_nodes_list.append( (self._outstruc_name, structure_data) )\n\n        return successful,new_nodes_list", "response": "Parses the datafolder and stores the results in the DB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hgetall(self, key):\n\n        def format_response(value):\n            return dict(zip(value[::2], value[1::2]))\n\n        return self._execute(\n            [b'HGETALL', key], format_callback=format_response)", "response": "Returns all fields and values of the has stored at `key`.\n\n        The underlying redis `HGETALL`_ command returns an array of\n        pairs.  This method converts that to a Python :class:`dict`.\n        It will return an empty :class:`dict` when the key is not\n        found.\n\n        .. note::\n\n           **Time complexity**: ``O(N)`` where ``N`` is the size\n           of the hash.\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :returns: a :class:`dict` of key to value mappings for all\n            fields in the hash\n\n        .. _HGETALL: http://redis.io/commands/hgetall"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hmset(self, key, value_dict):\n        if not value_dict:\n            future = concurrent.TracebackFuture()\n            future.set_result(False)\n        else:\n            command = [b'HMSET', key]\n            command.extend(sum(value_dict.items(), ()))\n            future = self._execute(command)\n        return future", "response": "Sets fields to values as in `value_dict` in the hash stored at `key`.\n\n        Sets the specified fields to their respective values in the hash\n        stored at `key`.  This command overwrites any specified fields\n        already existing in the hash.  If `key` does not exist, a new  key\n        holding a hash is created.\n\n        .. note::\n\n           **Time complexity**: ``O(N)`` where ``N`` is the number of\n           fields being set.\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :param value_dict: field to value mapping\n        :type value_dict: :class:`dict`\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the values associated with the specified fields in a hash.", "response": "def hmget(self, key, *fields):\n        \"\"\"\n        Returns the values associated with the specified `fields` in a hash.\n\n        For every ``field`` that does not exist in the hash, :data:`None`\n        is returned.  Because a non-existing keys are treated as empty\n        hashes, calling :meth:`hmget` against a non-existing key will\n        return a list of :data:`None` values.\n\n        .. note::\n\n           *Time complexity*: ``O(N)`` where ``N`` is the number of fields\n           being requested.\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :param fields: iterable of field names to retrieve\n        :returns: a :class:`dict` of field name to value mappings for\n            each of the requested fields\n        :rtype: dict\n\n        \"\"\"\n\n        def format_response(val_array):\n            return dict(zip(fields, val_array))\n\n        command = [b'HMGET', key]\n        command.extend(fields)\n        return self._execute(command, format_callback=format_response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the specified fields from the hash stored at key.", "response": "def hdel(self, key, *fields):\n        \"\"\"\n        Remove the specified fields from the hash stored at `key`.\n\n        Specified fields that do not exist within this hash are ignored.\n        If `key` does not exist, it is treated as an empty hash and this\n        command returns zero.\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :param fields: iterable of field names to retrieve\n        :returns: the number of fields that were removed from the hash,\n            not including specified by non-existing fields.\n        :rtype: int\n\n        \"\"\"\n        if not fields:\n            future = concurrent.TracebackFuture()\n            future.set_result(0)\n        else:\n            future = self._execute([b'HDEL', key] + list(fields))\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the number stored at field in the hash stored at key by the given amount.", "response": "def hincrby(self, key, field, increment):\n        \"\"\"\n        Increments the number stored at `field` in the hash stored at `key`.\n\n        If `key` does not exist, a new key holding a hash is created.  If\n        `field` does not exist the value is set to ``0`` before the operation\n        is performed.  The range of values supported is limited to 64-bit\n        signed integers.\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :param field: name of the field to increment\n        :type key: :class:`str`, :class:`bytes`\n        :param increment: amount to increment by\n        :type increment: int\n\n        :returns: the value at `field` after the increment occurs\n        :rtype: int\n\n        \"\"\"\n        return self._execute(\n            [b'HINCRBY', key, field, increment], format_callback=int)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hincrbyfloat(self, key, field, increment):\n        return self._execute(\n            [b'HINCRBYFLOAT', key, field, increment], format_callback=float)", "response": "Increment the value of a field in the hash stored at key by the specified amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets `field` in the hash stored at `key` only if it does not exist. Sets `field` in the hash stored at `key` only if `field` does not yet exist. If `key` does not exist, a new key holding a hash is created. If `field` already exists, this operation has no effect. .. note:: *Time complexity*: ``O(1)`` :param key: The key of the hash :type key: :class:`str`, :class:`bytes` :param field: The field in the hash to set :type key: :class:`str`, :class:`bytes` :param value: The value to set the field to :returns: ``1`` if `field` is a new field in the hash and `value` was set. ``0`` if `field` already exists in the hash and no operation was performed :rtype: int", "response": "def hsetnx(self, key, field, value):\n        \"\"\"\n        Sets `field` in the hash stored at `key` only if it does not exist.\n\n        Sets `field` in the hash stored at `key` only if `field` does not\n        yet exist.  If `key` does not exist, a new key holding a hash is\n        created.  If `field` already exists, this operation has no effect.\n\n        .. note::\n\n           *Time complexity*: ``O(1)``\n\n        :param key: The key of the hash\n        :type key: :class:`str`, :class:`bytes`\n        :param field: The field in the hash to set\n        :type key: :class:`str`, :class:`bytes`\n        :param value: The value to set the field to\n        :returns: ``1`` if `field` is a new field in the hash and `value`\n            was set.  ``0`` if `field` already exists in the hash and\n            no operation was performed\n        :rtype: int\n\n        \"\"\"\n        return self._execute([b'HSETNX', key, field, value])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_snapshot(self, repository, snapshot, body, params={}, callback=None, **kwargs):\n\n        query_params = ('master_timeout', 'wait_for_completion',)\n\n        params = self._filter_params(query_params, params)\n\n        url = self.mk_url(*['_snapshot', repository, snapshot], **params)\n\n        self.client.fetch(\n            self.mk_req(url, body=body, method='PUT', **kwargs),\n            callback = callback\n        )", "response": "Create a snapshot in repository"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies a master node in a repository.", "response": "def verify_repository(self,\n            repository,\n            master_timeout = 10,\n            timeout        = 10,\n            body           = '',\n            params         = {},\n            callback       = None,\n            **kwargs\n        ):\n        \"\"\"\n        Returns a list of nodes where repository was successfully verified or\n        an error message if verification process failed.\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        \"\"\"\n\n        query_params = ('master_timeout', 'timeout',)\n\n        params = self._filter_params(query_params, params)\n\n        url = self.mk_url(*['_snapshot', repository, '_verify'], **params)\n\n        self.client.fetch(\n            self.mk_req(url, body=body, method='POST', **kwargs),\n            callback = callback\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():  # pylint: disable-msg=R0912,R0915\n    parser = optparse.OptionParser()\n    parser.usage = textwrap.dedent(\"\"\"\\\n    %prog {--run|--install_key|--dump_config} [options]\n\n    SSH command authenticator.\n\n    Used to restrict which commands can be run via trusted SSH keys.\n    \"\"\")\n\n    group = optparse.OptionGroup(\n        parser, 'Run Mode Options',\n        'These options determine in which mode the authprogs '\n        'program runs.')\n    group.add_option(\n        '-r', '--run', dest='run', action='store_true',\n        help='Act as ssh command authenticator. Use this '\n        'when calling from authorized_keys.')\n    group.add_option(\n        '--dump_config', dest='dump_config',\n        action='store_true',\n        help='Dump configuration (python format) '\n        'to standard out and exit.')\n    group.add_option(\n        '--install_key', dest='install_key',\n        help='Install the named ssh public key file to '\n        'authorized_keys.', metavar='FILE')\n    parser.add_option_group(group)\n\n    group = optparse.OptionGroup(parser, 'Other Options')\n    group.add_option(\n        '--keyname', dest='keyname',\n        help='Name for this key, used when matching '\n        'config blocks.')\n    group.add_option(\n        '--configfile', dest='configfile',\n        help='Path to authprogs configuration file. '\n        'Defaults to ~/.ssh/authprogs.yaml',\n        metavar='FILE')\n    group.add_option(\n        '--configdir', dest='configdir',\n        help='Path to authprogs configuration directory. '\n        'Defaults to ~/.ssh/authprogs.d',\n        metavar='DIR')\n    group.add_option('--logfile', dest='logfile',\n                     help='Write logging info to this file. '\n                     'Defaults to no logging.',\n                     metavar='FILE')\n    group.add_option('--debug', dest='debug', action='store_true',\n                     help='Write additional debugging information '\n                     'to --logfile')\n    group.add_option('--authorized_keys', dest='authorized_keys',\n                     default=os.path.expanduser('~/.ssh/authorized_keys'),\n                     help='Location of authorized_keys file for '\n                     '--install_key. Defaults to ~/.ssh/authorized_keys',\n                     metavar='FILE')\n    parser.add_option_group(group)\n\n    opts, args = parser.parse_args()\n    if args:\n        sys.exit('authprogs does not accept commandline arguments.')\n\n    if not opts.configfile:\n        cfg = os.path.expanduser('~/.ssh/authprogs.yaml')\n        if os.path.isfile(cfg):\n            opts.configfile = cfg\n    if not opts.configdir:\n        cfg = os.path.expanduser('~/.ssh/authprogs.d')\n        if os.path.isdir(cfg):\n            opts.configdir = cfg\n\n    if opts.debug and not opts.logfile:\n        parser.error('--debug requires use of --logfile')\n\n    ap = None\n    try:\n        ap = AuthProgs(logfile=opts.logfile,  # pylint: disable-msg=C0103\n                       configfile=opts.configfile,\n                       configdir=opts.configdir,\n                       debug=opts.debug,\n                       keyname=opts.keyname)\n\n        if opts.dump_config:\n            ap.dump_config()\n            sys.exit(0)\n\n        elif opts.install_key:\n            try:\n                ap.install_key(opts.install_key, opts.authorized_keys)\n                sys.stderr.write('Key installed successfully.\\n')\n                sys.exit(0)\n            except InstallError as err:\n                sys.stderr.write('Key install failed: %s' % err)\n                sys.exit(1)\n\n        elif opts.run:\n            ap.exec_command()\n            sys.exit('authprogs command returned - should '\n                     'never happen.')\n        else:\n            parser.error('Not sure what to do. Consider --help')\n\n    except SSHEnvironmentError as err:\n        ap.log('SSHEnvironmentError \"%s\"\\n%s\\n' % (\n               err, traceback.format_exc()))\n        sys.exit('authprogs: %s' % err)\n    except ConfigError as err:\n        ap.log('ConfigError \"%s\"\\n%s\\n' % (\n               err, traceback.format_exc()))\n        sys.exit('authprogs: %s' % err)\n    except CommandRejected as err:\n        sys.exit('authprogs: %s' % err)\n    except Exception as err:\n        if ap:\n            ap.log('Unexpected exception: %s\\n%s\\n' % (\n                   err, traceback.format_exc()))\n        else:\n            sys.stderr.write('Unexpected exception: %s\\n%s\\n' % (\n                             err, traceback.format_exc()))\n        sys.exit('authprogs experienced an unexpected exception.')", "response": "Main function for the authprogs command authenticator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_and_log_error(self, error, message):\n        self.log('raising %s, traceback %s\\n' %\n                 (error, traceback.format_exc()))\n        raise error(message)", "response": "Raise error including message and original traceback."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the client IP from the environment.", "response": "def get_client_ip(self):\n        \"\"\"Return the client IP from the environment.\"\"\"\n\n        if self.client_ip:\n            return self.client_ip\n\n        try:\n            client = os.environ.get('SSH_CONNECTION',\n                                    os.environ.get('SSH_CLIENT'))\n            self.client_ip = client.split()[0]\n            self.logdebug('client_ip: %s\\n' % self.client_ip)\n            return self.client_ip\n        except:\n            raise SSHEnvironmentError('cannot identify the ssh client '\n                                      'IP address')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_keyname(self, rule):\n\n        keynames = rule.get('keynames')\n        if not keynames:\n            self.logdebug('no keynames requirement.\\n')\n            return True\n        if not isinstance(keynames, list):\n            keynames = [keynames]\n\n        if self.keyname in keynames:\n            self.logdebug('keyname \"%s\" matches rule.\\n' % self.keyname)\n            return True\n        else:\n            self.logdebug('keyname \"%s\" does not match rule.\\n' % self.keyname)\n            return False", "response": "Check if a key name is specified and if it is permitted."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_client_ip(self, rule):\n\n        if not rule.get('from'):\n            self.logdebug('no \"from\" requirement.\\n')\n            return True\n\n        allow_from = rule.get('from')\n        if not isinstance(allow_from, list):\n            allow_from = [allow_from]\n        client_ip = self.get_client_ip()\n\n        if client_ip in allow_from:\n            self.logdebug('client_ip %s in %s\\n' % (client_ip, allow_from))\n            return True\n        else:\n            self.logdebug('client_ip %s not in %s' % (client_ip, allow_from))\n            return False", "response": "Check if a client IP is permitted."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_merged_config(self):\n        if self.yamldocs:\n            return\n\n        loadfiles = []\n        if self.configfile:\n            loadfiles.append(self.configfile)\n\n        if self.configdir:\n            # Gets list of all non-dotfile files from configdir.\n            loadfiles.extend(\n                [f for f in\n                 [os.path.join(self.configdir, x) for x in\n                  os.listdir(self.configdir)]\n                 if os.path.isfile(f) and\n                 not os.path.basename(f).startswith('.')])\n\n        merged_configfile = io.StringIO()\n        merged_configfile.write('-\\n')\n        for thefile in loadfiles:\n            self.logdebug('reading in config file %s\\n' % thefile)\n            merged_configfile.write(open(thefile).read())\n            merged_configfile.write('\\n-\\n')\n        merged_configfile.seek(0)\n        self.logdebug('merged log file: \"\"\"\\n%s\\n\"\"\"\\n' %\n                      merged_configfile.read())\n        merged_configfile.seek(0)\n        return merged_configfile", "response": "Get merged config file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading our config log and raise on error.", "response": "def load(self):\n        \"\"\"Load our config, log and raise on error.\"\"\"\n        try:\n            merged_configfile = self.get_merged_config()\n            self.yamldocs = yaml.load(merged_configfile, Loader=Loader)\n\n            # Strip out the top level 'None's we get from concatenation.\n            # Functionally not required, but makes dumps cleaner.\n            self.yamldocs = [x for x in self.yamldocs if x]\n            self.logdebug('parsed_rules:\\n%s\\n' % pretty(self.yamldocs))\n\n        except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n            self.raise_and_log_error(ConfigError, 'error parsing config.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninstall the key data into the open file.", "response": "def install_key_data(self, keydata, target):\n        \"\"\"Install the key data into the open file.\"\"\"\n\n        target.seek(0)\n        contents = target.read()\n        ssh_opts = 'no-port-forwarding'\n        if keydata in contents:\n            raise InstallError('key data already in file - refusing '\n                               'to double-install.\\n')\n        command = '%s --run' % self.authprogs_binary\n        if self.logfile:\n            command += ' --logfile=%s' % self.logfile\n        if self.keyname:\n            command += ' --keyname=%s' % self.keyname\n\n        target.write('command=\"%(command)s\",%(ssh_opts)s %(keydata)s\\n' %\n                     {'command': command,\n                      'keydata': keydata,\n                      'ssh_opts': ssh_opts})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_key(self, keyfile, authorized_keys):\n\n        # Make the directory containing the authorized_keys\n        # file, if it doesn't exist. (Typically ~/.ssh).\n        # Ignore errors; we'll fail shortly if we can't\n        # create the authkeys file.\n        try:\n            os.makedirs(os.path.dirname(authorized_keys), 0o700)\n        except OSError:\n            pass\n\n        keydata = open(keyfile).read()\n        target_fd = os.open(authorized_keys, os.O_RDWR | os.O_CREAT, 0o600)\n        self.install_key_data(keydata, os.fdopen(target_fd, 'w+'))", "response": "Install a key into the authorized_keys file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_match_command(self, rule):\n\n        command_string = rule['command']\n        command_list = command_string.split()\n\n        self.logdebug('comparing \"%s\" to \"%s\"\\n' %\n                      (command_list, self.original_command_list))\n        if rule.get('allow_trailing_args'):\n            self.logdebug('allow_trailing_args is true - comparing initial '\n                          'list.\\n')\n            # Verify the initial arguments are all the same\n            if (self.original_command_list[:len(command_list)] ==\n                    command_list):\n                self.logdebug('initial list is same\\n')\n                return {'command': self.original_command_list}\n            else:\n                self.logdebug('initial list is not same\\n')\n\n        elif rule.get('pcre_match'):\n            if re.search(command_string, self.original_command_string):\n                return {'command': self.original_command_list}\n\n        elif command_list == self.original_command_list:\n            return {'command': command_list}", "response": "Return a matching ( possibly munged ) command if found in rule."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_match(self):\n\n        self.load()\n        for yamldoc in self.yamldocs:\n            self.logdebug('\\nchecking rule \"\"\"%s\"\"\"\\n' % yamldoc)\n\n            if not yamldoc:\n                continue\n\n            if not self.check_client_ip(yamldoc):\n                # Rejected - Client IP does not match\n                continue\n\n            if not self.check_keyname(yamldoc):\n                # Rejected - keyname does not match\n                continue\n\n            rules = yamldoc.get('allow')\n            if not isinstance(rules, list):\n                rules = [rules]\n\n            for rule in rules:\n                rule_type = rule.get('rule_type', 'command')\n                if rule_type == 'command':\n                    sub = self.find_match_command\n                elif rule_type == 'scp':\n                    sub = self.find_match_scp\n                else:\n                    self.log('fatal: no such rule_type \"%s\"\\n' % rule_type)\n                    self.raise_and_log_error(ConfigError,\n                                             'error parsing config.')\n\n                match = sub(rule)\n                if match:\n                    return match\n\n        # No matches, time to give up.\n        raise CommandRejected('command \"%s\" denied.' %\n                              self.original_command_string)", "response": "Load the config and find a matching rule."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exec_command(self):\n        if not self.original_command_string:\n            raise SSHEnvironmentError('no SSH command found; '\n                                      'interactive shell disallowed.')\n\n        command_info = {'from': self.get_client_ip(),\n                        'keyname': self.keyname,\n                        'ssh_original_comand': self.original_command_string,\n                        'time': time.time()}\n\n        os.environ['AUTHPROGS_KEYNAME'] = self.keyname\n\n        retcode = 126\n        try:\n            match = self.find_match()\n            command_info['command'] = match.get('command')\n            self.logdebug('find_match returned \"%s\"\\n' % match)\n\n            command = match['command']\n            retcode = subprocess.call(command)\n            command_info['code'] = retcode\n            self.log('result: %s\\n' % command_info)\n            sys.exit(retcode)\n        except (CommandRejected, OSError) as err:\n            command_info['exception'] = '%s' % err\n            self.log('result: %s\\n' % command_info)\n            sys.exit(retcode)", "response": "Execute the command in the interactive shell and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _py2_crc16(value):\n    crc = 0\n    for byte in value:\n        crc = ((crc << 8) & 0xffff) ^ \\\n              _CRC16_LOOKUP[((crc >> 8) ^ ord(byte)) & 0xff]\n    return crc", "response": "Calculate the CRC for the value in Python 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _py3_crc16(value):\n    crc = 0\n    for byte in value:\n        crc = ((crc << 8) & 0xffff) ^ _CRC16_LOOKUP[((crc >> 8) ^ byte) & 0xff]\n    return crc", "response": "Calculate the CRC for the value in Python 3"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_url(cls, url: str) -> Optional[Match[str]]:\n        match = re.match(cls._VALID_URL, url)\n        return match", "response": "Check if the Extractor can handle the given url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget information about the videos from YoutubeDL package.", "response": "def get_info(self) -> dict:\n        \"\"\"Get information about the videos from YoutubeDL package.\"\"\"\n        with suppress_stdout():\n            with youtube_dl.YoutubeDL() as ydl:\n                info_dict = ydl.extract_info(self.url, download=False)\n                return info_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_entries(entries: Entries, data: dict) -> None:\n        # TODO: Is mutating the list okay, making copies is such a pain in the ass\n        for entry in entries:\n            entry.update(data)", "response": "Update each entry in the list with some data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_extra_context(site, ctx):\n\t'Returns extra data useful to the templates.'\n\t# XXX: clean this up from obsolete stuff\n\tctx['site'] = site\n\tctx['feeds'] = feeds = site.active_feeds.order_by('name')\n\n\tdef get_mod_chk(k):\n\t\tmod, chk = (\n\t\t\t(max(vals) if vals else None) for vals in (\n\t\t\t\tfilter(None, it.imap(op.attrgetter(k), feeds))\n\t\t\t\tfor k in ['last_modified', 'last_checked'] ) )\n\t\tchk = chk or datetime(1970, 1, 1, 0, 0, 0, 0, timezone.utc)\n\t\tctx['last_modified'], ctx['last_checked'] = mod or chk, chk\n\t\treturn ctx[k]\n\tfor k in 'last_modified', 'last_checked':\n\t\tctx[k] = lambda: get_mod_chk(k)\n\n\t# media_url is set here for historical reasons,\n\t#  use static_url or STATIC_URL (from django context) in any new templates.\n\tctx['media_url'] = ctx['static_url'] =\\\n\t\t'{}feedjack/{}'.format(settings.STATIC_URL, site.template)", "response": "Returns extra data useful to the templates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a qtags property in every post object in a page. Use \"qtags\" instead of \"tags\" in templates to avoid unnecesary DB hits.", "response": "def get_posts_tags(subscribers, object_list, feed, tag_name):\n\t'''Adds a qtags property in every post object in a page.\n\t\tUse \"qtags\" instead of \"tags\" in templates to avoid unnecesary DB hits.'''\n\n\ttagd = dict()\n\tuser_obj = None\n\ttag_obj = None\n\ttags = models.Tag.objects.extra(\n\t\tselect=dict(post_id='{0}.{1}'.format(\n\t\t\t*it.imap( connection.ops.quote_name,\n\t\t\t\t('feedjack_post_tags', 'post_id') ) )),\n\t\ttables=['feedjack_post_tags'],\n\t\twhere=[\n\t\t'{0}.{1}={2}.{3}'.format(*it.imap( connection.ops.quote_name,\n\t\t\t('feedjack_tag', 'id', 'feedjack_post_tags', 'tag_id') )),\n\t\t'{0}.{1} IN ({2})'.format(\n\t\t\tconnection.ops.quote_name('feedjack_post_tags'),\n\t\t\tconnection.ops.quote_name('post_id'),\n\t\t\t', '.join([str(post.id) for post in object_list]) ) ] )\n\n\tfor tag in tags:\n\t\tif tag.post_id not in tagd: tagd[tag.post_id] = list()\n\t\ttagd[tag.post_id].append(tag)\n\t\tif tag_name and tag.name == tag_name: tag_obj = tag\n\n\tsubd = dict()\n\tfor sub in subscribers: subd[sub.feed.id] = sub\n\tfor post in object_list:\n\t\tif post.id in tagd: post.qtags = tagd[post.id]\n\t\telse: post.qtags = list()\n\t\tpost.subscriber = subd[post.feed.id]\n\t\tif feed == post.feed: user_obj = post.subscriber\n\n\treturn user_obj, tag_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a paginator object and a requested page from it.", "response": "def get_page(site, page=1, **criterias):\n\t'Returns a paginator object and a requested page from it.'\n\tglobal _since_formats_vary\n\n\tif 'since' in criterias:\n\t\tsince = criterias['since']\n\t\tif since in _since_offsets:\n\t\t\tsince = datetime.today() - timedelta(_since_offsets[since])\n\t\telse:\n\t\t\tif _since_formats_vary:\n\t\t\t\tfor fmt, substs in it.product( list(_since_formats),\n\t\t\t\t\t\tit.chain.from_iterable(\n\t\t\t\t\t\t\tit.combinations(_since_formats_vary, n)\n\t\t\t\t\t\t\tfor n in xrange(1, len(_since_formats_vary)) ) ):\n\t\t\t\t\tfor src, dst in substs: fmt = fmt.replace(src, dst)\n\t\t\t\t\t_since_formats.add(fmt)\n\t\t\t\t_since_formats_vary = None # to avoid doing it again\n\t\t\tfor fmt in _since_formats:\n\t\t\t\ttry: since = datetime.strptime(since, fmt)\n\t\t\t\texcept ValueError: pass\n\t\t\t\telse: break\n\t\t\telse: raise Http404 # invalid format\n\t\ttry:\n\t\t\tcriterias['since'] = timezone.make_aware(\n\t\t\t\tsince, timezone.get_current_timezone() )\n\t\texcept (\n\t\t\t\ttimezone.pytz.exceptions.AmbiguousTimeError\n\t\t\t\tif timezone.pytz else RuntimeError ):\n\t\t\t# Since there's no \"right\" way here anyway...\n\t\t\tcriterias['since'] = since.replace(tzinfo=timezone)\n\torder_force = criterias.pop('asc', None)\n\n\tposts = models.Post.objects.filtered(site, **criterias)\\\n\t\t.sorted(site.order_posts_by, force=order_force)\\\n\t\t.select_related('feed')\n\n\tpaginator = Paginator(posts, site.posts_per_page)\n\ttry: return paginator.page(page)\n\texcept InvalidPage: raise Http404"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef page_context(request, site, **criterias):\n\t'Returns the context dictionary for a page view.'\n\ttry: page = int(request.GET.get('page', 1))\n\texcept ValueError: page = 1\n\n\tfeed, tag = criterias.get('feed'), criterias.get('tag')\n\tif feed:\n\t\ttry: feed = models.Feed.objects.get(pk=feed)\n\t\texcept ObjectDoesNotExist: raise Http404\n\n\tpage = get_page(site, page=page, **criterias)\n\tsubscribers = site.active_subscribers\n\n\tif site.show_tagcloud and page.object_list:\n\t\tfrom feedjack import fjcloud\n\t\t# This will hit the DB once per page instead of once for every post in\n\t\t#  a page. To take advantage of this the template designer must call\n\t\t#  the qtags property in every item, instead of the default tags property.\n\t\tuser_obj, tag_obj = get_posts_tags(\n\t\t\tsubscribers, page.object_list, feed, tag )\n\t\ttag_cloud = fjcloud.getcloud(site, feed and feed.id)\n\telse:\n\t\ttag_obj, tag_cloud = None, tuple()\n\t\ttry:\n\t\t\tuser_obj = models.Subscriber.objects\\\n\t\t\t\t.get(site=site, feed=feed) if feed else None\n\t\texcept ObjectDoesNotExist: raise Http404\n\n\tsite_proc_tags = site.processing_tags.strip()\n\tif site_proc_tags != 'none':\n\t\tsite_proc_tags = filter( None,\n\t\t\tmap(op.methodcaller('strip'), site.processing_tags.split(',')) )\n\t\t# XXX: database hit that can be cached\n\t\tfor site_feed, posts in it.groupby(page.object_list, key=op.attrgetter('feed')):\n\t\t\tproc = site_feed.processor_for_tags(site_proc_tags)\n\t\t\tif proc: proc.apply_overlay_to_posts(posts)\n\n\tctx = dict(\n\t\tlast_modified = max(it.imap(\n\t\t\t\top.attrgetter('date_updated'), page.object_list ))\\\n\t\t\tif len(page.object_list) else datetime(1970, 1, 1, 0, 0, 0, 0, timezone.utc),\n\n\t\tobject_list = page.object_list,\n\t\tsubscribers = subscribers.select_related('feed'),\n\t\ttag = tag_obj,\n\t\ttagcloud = tag_cloud,\n\n\t\tfeed = feed,\n\t\turl_suffix = ''.join((\n\t\t\t'/feed/{0}'.format(feed.id) if feed else '',\n\t\t\t'/tag/{0}'.format(escape(tag)) if tag else '' )),\n\n\t\tp = page, # \"page\" is taken by legacy number\n\t\tp_10neighbors = OrderedDict(\n\t\t\t# OrderedDict of \"num: exists\" values\n\t\t\t# Use as \"{% for p_num, p_exists in p_10neighbors.items|slice:\"7:-7\" %}\"\n\t\t\t(p, p >= 1 and p <= page.paginator.num_pages)\n\t\t\tfor p in ((page.number + n) for n in xrange(-10, 11)) ),\n\n\t\t## DEPRECATED:\n\n\t\t# Totally misnamed and inconsistent b/w user/user_obj,\n\t\t#  use \"feed\" and \"subscribers\" instead.\n\t\tuser_id = feed and feed.id,\n\t\tuser = user_obj,\n\n\t\t# Legacy flat pagination context, use \"p\" instead.\n\t\tis_paginated = page.paginator.num_pages > 1,\n\t\tresults_per_page = site.posts_per_page,\n\t\thas_next = page.has_next(),\n\t\thas_previous = page.has_previous(),\n\t\tpage = page.number,\n\t\tnext = page.number + 1,\n\t\tprevious = page.number - 1,\n\t\tpages = page.paginator.num_pages,\n\t\thits = page.paginator.count )\n\n\tget_extra_context(site, ctx)\n\n\treturn ctx", "response": "Returns the context dictionary for a page view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bitcount(self, key, start=None, end=None):\n        command = [b'BITCOUNT', key]\n        if start is not None and end is None:\n            raise ValueError('Can not specify start without an end')\n        elif start is None and end is not None:\n            raise ValueError('Can not specify start without an end')\n        elif start is not None and end is not None:\n            command += [ascii(start), ascii(end)]\n        return self._execute(command)", "response": "Return the number of bits set in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a bitwise operation between multiple keys and store the result in dest_key.", "response": "def bitop(self, operation, dest_key, *keys):\n        \"\"\"Perform a bitwise operation between multiple keys (containing\n        string values) and store the result in the destination key.\n\n        The values for operation can be one of:\n\n            - ``b'AND'``\n            - ``b'OR'``\n            - ``b'XOR'``\n            - ``b'NOT'``\n            - :data:`tredis.BITOP_AND` or ``b'&'``\n            - :data:`tredis.BITOP_OR` or ``b'|'``\n            - :data:`tredis.BITOP_XOR` or ``b'^'``\n            - :data:`tredis.BITOP_NOT` or ``b'~'``\n\n        ``b'NOT'`` is special as it only takes an input key, because it\n        performs inversion of bits so it only makes sense as an unary operator.\n\n        The result of the operation is always stored at ``dest_key``.\n\n        **Handling of strings with different lengths**\n\n        When an operation is performed between strings having different\n        lengths, all the strings shorter than the longest string in the set are\n        treated as if they were zero-padded up to the length of the longest\n        string.\n\n        The same holds true for non-existent keys, that are considered as a\n        stream of zero bytes up to the length of the longest string.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(N)``\n\n        :param bytes operation: The operation to perform\n        :param dest_key: The key to store the bitwise operation results to\n        :type dest_key: :class:`str`, :class:`bytes`\n        :param keys: One or more keys as keyword parameters for the bitwise op\n        :type keys: :class:`str`, :class:`bytes`\n        :return: The size of the string stored in the destination key, that is\n                 equal to the size of the longest input string.\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.RedisError`, :exc:`ValueError`\n\n        \"\"\"\n        if (operation not in _BITOPTS.keys()\n                and operation not in _BITOPTS.values()):\n            raise ValueError('Invalid operation value: {}'.format(operation))\n        elif operation in [b'~', b'NOT'] and len(keys) > 1:\n            raise ValueError('NOT can only be used with 1 key')\n\n        if operation in _BITOPTS.keys():\n            operation = _BITOPTS[operation]\n\n        return self._execute([b'BITOP', operation, dest_key] + list(keys))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the position of the first bit set to 1 or 0 in a string.", "response": "def bitpos(self, key, bit, start=None, end=None):\n        \"\"\"Return the position of the first bit set to ``1`` or ``0`` in a\n        string.\n\n        The position is returned, thinking of the string as an array of bits\n        from left to right, where the first byte's most significant bit is at\n        position 0, the second byte's most significant bit is at position\n        ``8``, and so forth.\n\n        The same bit position convention is followed by\n        :meth:`~tredis.RedisClient.getbit` and\n        :meth:`~tredis.RedisClient.setbit`.\n\n        By default, all the bytes contained in the string are examined. It is\n        possible to look for bits only in a specified interval passing the\n        additional arguments start and end (it is possible to just pass start,\n        the operation will assume that the end is the last byte of the string.\n        However there are semantic differences as explained later). The range\n        is interpreted as a range of bytes and not a range of bits, so\n        ``start=0`` and ``end=2`` means to look at the first three bytes.\n\n        Note that bit positions are returned always as absolute values starting\n        from bit zero even when start and end are used to specify a range.\n\n        Like for the :meth:`~tredis.RedisClient.getrange` command start and\n        end can contain negative values in order to index bytes starting from\n        the end of the string, where ``-1`` is the last byte, ``-2`` is the\n        penultimate, and so forth.\n\n        Non-existent keys are treated as empty strings.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(N)``\n\n        :param key: The key to get\n        :type key: :class:`str`, :class:`bytes`\n        :param int bit: The bit value to search for (``1`` or ``0``)\n        :param int start: The start position to evaluate in the string\n        :param int end: The end position to evaluate in the string\n        :returns: The position of the first bit set to ``1`` or ``0``\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.RedisError`, :exc:`ValueError`\n\n        \"\"\"\n        if 0 < bit > 1:\n            raise ValueError('bit must be 1 or 0, not {}'.format(bit))\n        command = [b'BITPOS', key, ascii(bit)]\n        if start is not None and end is None:\n            raise ValueError('Can not specify start without an end')\n        elif start is None and end is not None:\n            raise ValueError('Can not specify start without an end')\n        elif start is not None and end is not None:\n            command += [ascii(start), ascii(end)]\n        return self._execute(command)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the bit at offset in the string stored at key.", "response": "def getbit(self, key, offset):\n        \"\"\"Returns the bit value at offset in the string value stored at key.\n\n        When offset is beyond the string length, the string is assumed to be a\n        contiguous space with 0 bits. When key does not exist it is assumed to\n        be an empty string, so offset is always out of range and the value is\n        also assumed to be a contiguous space with 0 bits.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(1)``\n\n        :param key: The key to get the bit from\n        :type key: :class:`str`, :class:`bytes`\n        :param int offset: The bit offset to fetch the bit from\n        :rtype: bytes|None\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        return self._execute([b'GETBIT', key, ascii(offset)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the bit value stored at key between start and end.", "response": "def getrange(self, key, start, end):\n        \"\"\"Returns the bit value at offset in the string value stored at key.\n\n        When offset is beyond the string length, the string is assumed to be a\n        contiguous space with 0 bits. When key does not exist it is assumed to\n        be an empty string, so offset is always out of range and the value is\n        also assumed to be a contiguous space with 0 bits.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(N)`` where ``N`` is the length of\n           the returned string. The complexity is ultimately determined by the\n           returned length, but because creating a substring from an existing\n           string is very cheap, it can be considered ``O(1)`` for small\n           strings.\n\n        :param key: The key to get the bit from\n        :type key: :class:`str`, :class:`bytes`\n        :param int start: The start position to evaluate in the string\n        :param int end: The end position to evaluate in the string\n        :rtype: bytes|None\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        return self._execute([b'GETRANGE', key, ascii(start), ascii(end)])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef incrby(self, key, increment):\n        return self._execute([b'INCRBY', key, ascii(increment)])", "response": "Increments the number stored at key by increment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincrements the string representing a floating point number stored at key by the specified increment. If the key does not exist, it is set to 0 before performing the operation. An error is returned if one of the following conditions occur: - The key contains a value of the wrong type (not a string). - The current key content or the specified increment are not parsable as a double precision floating point number. If the command is successful the new incremented value is stored as the new value of the key (replacing the old one), and returned to the caller as a string. Both the value already contained in the string key and the increment argument can be optionally provided in exponential notation, however the value computed after the increment is stored consistently in the same format, that is, an integer number followed (if needed) by a dot, and a variable number of digits representing the decimal part of the number. Trailing zeroes are always removed. The precision of the output is fixed at 17 digits after the decimal point regardless of the actual internal precision of the computation. .. versionadded:: 0.2.0 .. note:: **Time complexity**: ``O(1)`` :param key: The key to increment :type key: :class:`str`, :class:`bytes` :param float increment: The amount to increment by :returns: The value of key after the increment :rtype: bytes :raises: :exc:`~tredis.exceptions.RedisError`", "response": "def incrbyfloat(self, key, increment):\n        \"\"\"Increment the string representing a floating point number stored at\n        key by the specified increment. If the key does not exist, it is set to\n        0 before performing the operation. An error is returned if one of the\n        following conditions occur:\n\n          - The key contains a value of the wrong type (not a string).\n          - The current key content or the specified increment are not\n            parsable as a double precision floating point number.\n\n        If the command is successful the new incremented value is stored as the\n        new value of the key (replacing the old one), and returned to the\n        caller as a string.\n\n        Both the value already contained in the string key and the increment\n        argument can be optionally provided in exponential notation, however\n        the value computed after the increment is stored consistently in the\n        same format, that is, an integer number followed (if needed) by a dot,\n        and a variable number of digits representing the decimal part of the\n        number. Trailing zeroes are always removed.\n\n        The precision of the output is fixed at 17 digits after the decimal\n        point regardless of the actual internal precision of the computation.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(1)``\n\n        :param key: The key to increment\n        :type key: :class:`str`, :class:`bytes`\n        :param float increment: The amount to increment by\n        :returns: The value of key after the increment\n        :rtype: bytes\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        return self._execute([b'INCRBYFLOAT', key, ascii(increment)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the given keys to their respective values.", "response": "def mset(self, mapping):\n        \"\"\"Sets the given keys to their respective values.\n        :meth:`~tredis.RedisClient.mset` replaces existing values with new\n        values, just as regular :meth:`~tredis.RedisClient.set`. See\n        :meth:`~tredis.RedisClient.msetnx` if you don't want to overwrite\n        existing values.\n\n        :meth:`~tredis.RedisClient.mset` is atomic, so all given keys are set\n        at once. It is not possible for clients to see that some of the keys\n        were updated while others are unchanged.\n\n        .. versionadded:: 0.2.0\n\n        .. note:: **Time complexity**: ``O(N)`` where ``N`` is the number of\n           keys to set.\n\n        :param dict mapping: A mapping of key/value pairs to set\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        command = [b'MSET']\n        for key, value in mapping.items():\n            command += [key, value]\n        return self._execute(command, b'OK')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef msetnx(self, mapping):\n        command = [b'MSETNX']\n        for key, value in mapping.items():\n            command += [key, value]\n        return self._execute(command, 1)", "response": "Set the given keys to their respective values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the string value of the key to hold the string value.", "response": "def set(self, key, value, ex=None, px=None, nx=False, xx=False):\n        \"\"\"Set key to hold the string value. If key already holds a value, it\n        is overwritten, regardless of its type. Any previous time to live\n        associated with the key is discarded on successful\n        :meth:`~tredis.RedisClient.set` operation.\n\n        If the value is not one of :class:`str`, :class:`bytes`, or\n        :class:`int`, a :exc:`ValueError` will be raised.\n\n        .. note:: **Time complexity**: ``O(1)``\n\n        :param key: The key to remove\n        :type key: :class:`str`, :class:`bytes`\n        :param value: The value to set\n        :type value: :class:`str`, :class:`bytes`, :class:`int`\n        :param int ex: Set the specified expire time, in seconds\n        :param int px: Set the specified expire time, in milliseconds\n        :param bool nx: Only set the key if it does not already exist\n        :param bool xx: Only set the key if it already exist\n        :rtype: bool\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        :raises: :exc:`ValueError`\n\n        \"\"\"\n        command = [b'SET', key, value]\n        if ex:\n            command += [b'EX', ascii(ex).encode('ascii')]\n        if px:\n            command += [b'PX', ascii(px).encode('ascii')]\n        if nx:\n            command.append(b'NX')\n        if xx:\n            command.append(b'XX')\n        return self._execute(command, b'OK')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setbit(self, key, offset, bit):\n        if 0 < bit > 1:\n            raise ValueError('bit must be 1 or 0, not {}'.format(bit))\n        return self._execute([b'SETBIT', key, ascii(offset), ascii(bit)])", "response": "Set or clears the bit at offset in the string value stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setex(self, key, seconds, value):\n        return self._execute([b'SETEX', key, ascii(seconds), value], b'OK')", "response": "Set the string value of the key to expire after the number of seconds given."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setrange(self, key, offset, value):\n        return self._execute([b'SETRANGE', key, ascii(offset), value])", "response": "Set the value of the bit in the string stored at key starting at the specified offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the extension conflicts with an already accepted extension.", "response": "def conflicts(self, ext):\n        \"\"\"\n        Check if the extension conflicts with an already accepted extension.\n        This may be the case when the two extensions use the same reserved\n        bits, or have the same name (when the same extension is negotiated\n        multiple times with different parameters).\n        \"\"\"\n        return ext.rsv1 and self.rsv1 \\\n            or ext.rsv2 and self.rsv2 \\\n            or ext.rsv3 and self.rsv3 \\\n            or set(ext.names) & set(self.names) \\\n            or set(ext.opcodes) & set(self.opcodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnegotiates a set of parameters for a specific extension name.", "response": "def negotiate_safe(self, name, params):\n        \"\"\"\n        `name` and `params` are sent in the HTTP request by the client. Check\n        if the extension name is supported by this extension, and validate the\n        parameters. Returns a dict with accepted parameters, or None if not\n        accepted.\n        \"\"\"\n        for param in params.iterkeys():\n            if param not in self.defaults:\n                return\n\n        try:\n            return dict(self.negotiate(name, params))\n        except (KeyError, ValueError, AssertionError):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a request based on method and calls the appropriate function", "response": "def index(request):\n    \"\"\"Handles a request based on method and calls the appropriate function\"\"\"\n    if request.method == 'GET':\n        return get(request)\n    elif request.method == 'POST':\n        return post(request)\n    return HttpResponse('')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the currently logged in users preferences", "response": "def get(request):\n    \"\"\"Gets the currently logged in users preferences\n\n    :returns: json\n    \"\"\"\n    res = Result()\n    obj, created = UserPref.objects.get_or_create(user=request.user, defaults={'data': json.dumps(DefaultPrefs.copy())})\n\n    data = obj.json()\n    data['subscriptions'] = [_.json() for _ in GallerySubscription.objects.filter(user=request.user)]\n\n    res.append(data)\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a key to a value on the currently logged in users preferences", "response": "def post(request):\n    \"\"\"Sets a key to a value on the currently logged in users preferences\n\n    :param key: Key to set\n    :type key: str\n    :param val: Value to set\n    :type val: primitive\n    :returns: json\n    \"\"\"\n    data = request.POST or json.loads(request.body)['body']\n    key = data.get('key', None)\n    val = data.get('val', None)\n    res = Result()\n    if key is not None and val is not None:\n        obj, created = UserPref.objects.get_or_create(user=request.user)\n        if created:\n            obj.data = json.dumps(DefaultPrefs.copy())\n            obj.save()\n        try:\n            val = json.loads(val)\n        except (TypeError, ValueError):\n            pass\n        obj.setKey(key, val)\n        obj.save()\n        res.append(obj.json())\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns best possible guess to post modification timestamp.", "response": "def get_modified_date(parsed, raw):\n\t'Return best possible guess to post modification timestamp.'\n\tif parsed: return feedparser_ts(parsed)\n\tif not raw: return None\n\n\t# Parse weird timestamps that feedparser can't handle, e.g.: July 30, 2013\n\tts, val = None, raw.replace('_', ' ')\n\tif not ts:\n\t\t# coreutils' \"date\" parses virtually everything, but is more expensive to use\n\t\tfrom subprocess import Popen, PIPE\n\t\twith open(os.devnull, 'w') as devnull:\n\t\t\tproc = Popen(['date', '+%s', '-d', val], stdout=PIPE, stderr=devnull)\n\t\t\tval = proc.stdout.read()\n\t\t\tif not proc.wait():\n\t\t\t\tts = datetime.fromtimestamp(int(val.strip()), tz=timezone.utc)\n\tif ts: return ts\n\traise ValueError('Unrecognized raw value format: {0!r}'.format(val))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_entry(self, entry):\n\t\t'Construct a Post from a feedparser entry and save/update it in db'\n\n\t\tfrom feedjack.models import Post, Tag\n\n\t\t## Construct a Post object from feedparser entry (FeedParserDict)\n\t\tpost = Post(feed=self.feed)\n\t\tpost.link = entry.get('link', self.feed.link)\n\t\tpost.title = entry.get('title', post.link)\n\t\tpost.guid = self._get_guid(entry)\n\n\t\tif 'author_detail' in entry:\n\t\t\tpost.author = entry.author_detail.get('name', '')\n\t\t\tpost.author_email = entry.author_detail.get('email', '')\n\t\tif not post.author: post.author = entry.get('author', entry.get('creator', ''))\n\t\tif not post.author_email: post.author_email = 'nospam@nospam.com'\n\n\t\ttry: post.content = entry.content[0].value\n\t\texcept: post.content = entry.get('summary', entry.get('description', ''))\n\n\t\t# Try to get the post date from \"updated\" then \"published\" then \"created\"\n\t\tts_parsed = ts_raw = None\n\t\tfor k in self.post_timestamp_keys:\n\t\t\ttry:\n\t\t\t\tpost.date_modified = get_modified_date(\n\t\t\t\t\tentry.get('{0}_parsed'.format(k)), entry.get(k) )\n\t\t\texcept ValueError as err:\n\t\t\t\tlog.warn( 'Failed to process post timestamp:'\n\t\t\t\t\t' {0} (feed_id: {1}, post_guid: {2})'.format(err, self.feed.id, post.guid) )\n\t\t\tif post.date_modified: break\n\n\t\tpost.comments = entry.get('comments', '')\n\n\t\tenclosures = entry.get('enclosures', list())\n\t\tif 'media_content' in entry:\n\t\t\tfor mc in entry.media_content:\n\t\t\t\tif 'url' in mc: e = dict(href=mc['url'], medium=mc.get('medium', 'image'))\n\t\t\t\telse: e = entry.media_content\n\t\t\t\te['type'] = 'application/x-media-content' # special ct for these things\n\t\t\t\tenclosures.append(e)\n\t\t\tassert enclosures, enclosures\n\t\tpost.enclosures = enclosures\n\n\t\t## Get a list of tag objects from an entry\n\t\t# Note that these objects can't go into m2m field until properly saved\n\t\tfcat = list()\n\t\tif entry.has_key('tags'):\n\t\t\tfor tcat in entry.tags:\n\t\t\t\tqcat = tcat.label if tcat.label is not None else tcat.term\n\t\t\t\tif not qcat: continue\n\n\t\t\t\tqcat = qcat.strip()\n\t\t\t\tif ',' in qcat or '/' in qcat: qcat = qcat.replace(',', '/').split('/')\n\t\t\t\telse: qcat = [qcat]\n\n\t\t\t\tfor zcat in qcat:\n\t\t\t\t\ttagname = ' '.join(zcat.lower().split()).strip()[:255]\n\t\t\t\t\tif not tagname: continue\n\t\t\t\t\tif not Tag.objects.filter(name=tagname):\n\t\t\t\t\t\tcobj = Tag(name=tagname)\n\t\t\t\t\t\tcobj.save()\n\t\t\t\t\tfcat.append(Tag.objects.get(name=tagname))\n\n\t\t## Some feedback\n\t\tpost_base_fields = 'title link guid author author_email'.split()\n\n\t\tlog.debug('[{0}] Entry\\n{1}'.format(self.feed.id, '\\n'.join(\n\t\t\t['  {0}: {1}'.format(key, getattr(post, key)) for key in post_base_fields]\n\t\t\t+ ['tags: {0}'.format(' '.join(it.imap(op.attrgetter('name'), fcat)))] )))\n\n\t\t## Store / update a post\n\t\tif post.guid in self.postdict: # post exists, update if it was modified (and feed is mutable)\n\t\t\tpost_old = self.postdict[post.guid]\n\t\t\tchanged = post_old.content != post.content or (\n\t\t\t\tpost.date_modified and post_old.date_modified != post.date_modified )\n\n\t\t\tif not self.feed.immutable and changed:\n\t\t\t\tretval = ENTRY_UPDATED\n\t\t\t\tlog.extra('[{0}] Updating existing post: {1}'.format(self.feed.id, post.link))\n\t\t\t\t# Update fields\n\t\t\t\tfor field in post_base_fields + ['content', 'comments']:\n\t\t\t\t\tsetattr(post_old, field, getattr(post, field))\n\t\t\t\tpost_old.date_modified = post.date_modified or post_old.date_modified\n\t\t\t\t# Update tags\n\t\t\t\tpost_old.tags.clear()\n\t\t\t\tfor tcat in fcat: post_old.tags.add(tcat)\n\t\t\t\tpost_old.save()\n\t\t\telse:\n\t\t\t\tretval = ENTRY_SAME\n\t\t\t\tlog.extra( ( '[{0}] Post has not changed: {1}' if not changed else\n\t\t\t\t\t'[{0}] Post changed, but feed is marked as immutable: {1}' )\\\n\t\t\t\t\t\t.format(self.feed.id, post.link) )\n\n\t\telse: # new post, store it into database\n\t\t\tretval = ENTRY_NEW\n\t\t\tlog.extra( '[{0}] Saving new post: {1} (timestamp: {2})'\\\n\t\t\t\t.format(self.feed.id, post.guid, post.date_modified) )\n\n\t\t\t# Try hard to set date_modified: feed.modified, http.modified and now() as a last resort\n\t\t\tif not post.date_modified and self.fpf:\n\t\t\t\ttry:\n\t\t\t\t\tpost.date_modified = get_modified_date(\n\t\t\t\t\t\tself.fpf.feed.get('modified_parsed') or self.fpf.get('modified_parsed'),\n\t\t\t\t\t\tself.fpf.feed.get('modified') or self.fpf.get('modified') )\n\t\t\t\texcept ValueError as err:\n\t\t\t\t\tlog.warn(( 'Failed to process feed/http timestamp: {0} (feed_id: {1},'\n\t\t\t\t\t\t' post_guid: {2}), falling back to \"now\"' ).format(err, self.feed.id, post.guid))\n\t\t\t\tif not post.date_modified:\n\t\t\t\t\tpost.date_modified = timezone.now()\n\t\t\t\t\tlog.debug(( '[{0}] Using current time for post'\n\t\t\t\t\t\t' ({1}) timestamp' ).format(self.feed.id, post.guid))\n\t\t\t\telse:\n\t\t\t\t\tlog.debug(\n\t\t\t\t\t\t'[{0}] Using timestamp from feed/http for post ({1}): {2}'\\\n\t\t\t\t\t\t.format(self.feed.id, post.guid, post.date_modified) )\n\n\t\t\tif self.options.hidden: post.hidden = True\n\t\t\ttry: post.save()\n\t\t\texcept IntegrityError:\n\t\t\t\tlog.error( 'IntegrityError while saving (supposedly) new'\\\n\t\t\t\t\t' post with guid: {0.guid}, link: {0.link}, title: {0.title}'.format(post) )\n\t\t\t\traise\n\t\t\tfor tcat in fcat: post.tags.add(tcat)\n\t\t\tself.postdict[post.guid] = post\n\n\t\treturn retval", "response": "Construct a Post from a feedparser entry and save / update it in db"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process(self):\n\t\t'Downloads and parses a feed.'\n\n\t\tret_values = {\n\t\t\tENTRY_NEW: 0,\n\t\t\tENTRY_UPDATED: 0,\n\t\t\tENTRY_SAME: 0,\n\t\t\tENTRY_ERR: 0 }\n\t\treport_errors = not self.options.report_after\\\n\t\t\tor not self.feed.last_checked\\\n\t\t\tor (self.feed.last_checked + self.options.report_after < timezone.now())\n\n\t\tfeedparser_kws = dict()\n\t\tif sys.hexversion >= 0x2070900 and not self.feed.verify_tls_certs:\n\t\t\timport urllib2, ssl\n\t\t\tctx = ssl.create_default_context()\n\t\t\tctx.check_hostname, ctx.verify_mode = False, ssl.CERT_NONE\n\t\t\tfeedparser_kws['handlers'] = [urllib2.HTTPSHandler(context=ctx)]\n\n\t\ttry:\n\t\t\tself.fpf = feedparser.parse( self.feed.feed_url, agent=USER_AGENT,\n\t\t\t\tetag=self.feed.etag if not self.options.force else '', **feedparser_kws )\n\t\texcept KeyboardInterrupt: raise\n\t\texcept:\n\t\t\tif report_errors:\n\t\t\t\tlog.error( 'Feed cannot be parsed: {0} (#{1})'\\\n\t\t\t\t\t.format(self.feed.feed_url, self.feed.id) )\n\t\t\treturn FEED_ERRPARSE, ret_values\n\n\t\tif hasattr(self.fpf, 'status'):\n\t\t\tlog.extra('[{0}] HTTP status {1}: {2}'.format(\n\t\t\t\tself.feed.id, self.fpf.status, self.feed.feed_url ))\n\t\t\tif self.fpf.status == 304:\n\t\t\t\tlog.extra(( '[{0}] Feed has not changed since '\n\t\t\t\t\t'last check: {1}' ).format(self.feed.id, self.feed.feed_url))\n\t\t\t\t# Fast-path: just update last_checked timestamp\n\t\t\t\tself.feed.last_checked = timezone.now()\n\t\t\t\tself.feed.save()\n\t\t\t\treturn FEED_SAME, ret_values\n\n\t\t\tif self.fpf.status >= 400:\n\t\t\t\tif report_errors:\n\t\t\t\t\tlog.warn('[{0}] HTTP error {1}: {2}'.format(\n\t\t\t\t\t\tself.feed.id, self.fpf.status, self.feed.feed_url ))\n\t\t\t\treturn FEED_ERRFETCH, ret_values\n\n\t\tif self.fpf.bozo:\n\t\t\tbozo = getattr(self.fpf, 'bozo_exception', 'unknown error')\n\t\t\tif not self.feed.skip_errors:\n\t\t\t\tif report_errors:\n\t\t\t\t\tlog.warn( '[{0}] Failed to fetch feed: {1} ({2})'\\\n\t\t\t\t\t\t.format(self.feed.id, self.feed.feed_url, bozo) )\n\t\t\t\treturn FEED_ERRFETCH, ret_values\n\t\t\telif report_errors:\n\t\t\t\tlog.info( '[{0}] Skipped feed error: {1} ({2})'\\\n\t\t\t\t\t.format(self.feed.id, self.feed.feed_url, bozo) )\n\n\t\tself.feed.title = self.fpf.feed.get('title', '')[:200]\n\t\tself.feed.tagline = self.fpf.feed.get('tagline', '')\n\t\tself.feed.link = self.fpf.feed.get('link', '')\n\t\tself.feed.last_checked = timezone.now()\n\n\t\tlog.debug('[{0}] Feed info for: {1}\\n{2}'.format(\n\t\t\tself.feed.id, self.feed.feed_url, '\\n'.join(\n\t\t\t'  {0}: {1}'.format(key, getattr(self.feed, key))\n\t\t\tfor key in ['title', 'tagline', 'link', 'last_checked'] )))\n\n\t\tguids = filter(None, it.imap(self._get_guid, self.fpf.entries))\n\t\tif guids:\n\t\t\tfrom feedjack.models import Post\n\t\t\tself.postdict = dict( (post.guid, post)\n\t\t\t\tfor post in Post.objects.filter(\n\t\t\t\t\tfeed=self.feed.id, guid__in=guids ) )\n\t\t\tif self.options.max_diff:\n\t\t\t\t# Do not calculate diff for empty (probably just-added) feeds\n\t\t\t\tif not self.postdict and Post.objects.filter(feed=self.feed.id).count() == 0: diff = 0\n\t\t\t\telse: diff = op.truediv(len(guids) - len(self.postdict), len(guids)) * 100\n\t\t\t\tif diff > self.options.max_diff:\n\t\t\t\t\tlog.warn( '[{0}] Feed validation failed: {1} (diff: {2}% > {3}%)'\\\n\t\t\t\t\t\t.format(self.feed.id, self.feed.feed_url, round(diff, 1), self.options.max_diff) )\n\t\t\t\t\treturn FEED_INVALID, ret_values\n\t\telse: self.postdict = dict()\n\n\t\tself.feed.save() # etag/mtime aren't updated yet\n\n\t\tfor entry in self.fpf.entries:\n\t\t\ttry:\n\t\t\t\twith transaction.atomic(): ret_entry = self.process_entry(entry)\n\t\t\texcept:\n\t\t\t\tprint_exc(self.feed.id)\n\t\t\t\tret_entry = ENTRY_ERR\n\t\t\tret_values[ret_entry] += 1\n\n\t\tif not ret_values[ENTRY_ERR]: # etag/mtime updated only if there's no errors\n\t\t\tself.feed.etag = self.fpf.get('etag') or ''\n\t\t\ttry: self.feed.last_modified = feedparser_ts(self.fpf.modified_parsed)\n\t\t\texcept AttributeError: pass\n\t\t\tself.feed.save()\n\n\t\treturn FEED_OK if ret_values[ENTRY_NEW]\\\n\t\t\tor ret_values[ENTRY_UPDATED] else FEED_SAME, ret_values", "response": "Downloads and parses a feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_realtime_routine(bus_name, cur_station=None):\n    '''Get real time routine.\n\n    TODO support fuzzy matching.\n\n    :param bus_name: the routine name of the bus.\n    :param cur_station: current station, deaults to starting station\n                        of the routine.\n    '''\n    routines = query_routines(bus_name)\n    if not routines:\n        return\n\n    rv = []\n    for routine in routines:\n        bid = routine['bid']\n        _cur_station = cur_station or routine['starting_station']\n        page = _get_realtime_page(bus_name, bid, _cur_station)\n        rv.append(extract_bus_routine(page))\n\n    return rv", "response": "Get real time routine."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getRoot():\n    root = settings.MEDIA_ROOT.replace('\\\\', '/')\n    if not root.endswith('/'):\n        root += '/'\n\n    return path.Path(root)", "response": "Convenience to return the media root with forward slashes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emailUser(video, error=None):\n    html = render_to_string('frog/video_email.html', {\n        'user': video.author,\n        'error': error,\n        'video': video,\n        'SITE_URL': FROG_SITE_URL,\n    })\n    subject, from_email, to = 'Video Processing Finished{}'.format(error or ''), 'noreply@frogmediaserver.com', video.author.email\n    text_content = 'This is an important message.'\n    html_content = html\n\n    send_mail(subject, text_content, from_email, [to], html_message=html_content)", "response": "Emails the author of the video that it has finished processing"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding all the specified members with the specified scores to the sorted set stored at key. It is possible to specify multiple score / member pairs. If a specified member is already a member of the sorted set, the score is updated and the element reinserted at the right position to ensure the correct ordering. If key does not exist, a new sorted set with the specified members as sole members is created, like if the sorted set was empty. If the key exists but does not hold a sorted set, an error is returned. The score values should be the string representation of a double precision floating point number. +inf and -inf values are valid values as well. **Members parameters** ``members`` could be either: - a single dict where keys correspond to scores and values to elements - multiple strings paired as score then element .. code:: python yield client.zadd('myzset', {'1': 'one', '2': 'two'}) yield client.zadd('myzset', '1', 'one', '2', 'two') **ZADD options (Redis 3.0.2 or greater)** ZADD supports a list of options. Options are: - ``xx``: Only update elements that already exist. Never add elements. - ``nx``: Don't update already existing elements. Always add new elements. - ``ch``: Modify the return value from the number of new elements added, to the total number of elements changed (CH is an abbreviation of changed). Changed elements are new elements added and elements already existing for which the score was updated. So elements specified in the command having the same score as they had in the past are not counted. Note: normally the return value of ``ZADD`` only counts the number of new elements added. - ``incr``: When this option is specified ``ZADD`` acts like :meth:`~tredis.RedisClient.zincrby`. Only one score-element pair can be specified in this mode. .. note:: **Time complexity**: ``O(log(N))`` for each item added, where ``N`` is the number of elements in the sorted set. :param key: The key of the sorted set :type key: :class:`str`, :class:`bytes` :param members: Elements to add :type members: :class:`dict`, :class:`str`, :class:`bytes` :keyword bool xx: Only update elements that already exist :keyword bool nx: Don't update already existing elements :keyword bool ch: Return the number of changed elements :keyword bool incr: Increment the score of an element :rtype: int, :class:`str`, :class:`bytes` :returns: Number of elements changed, or the new score if incr is set :raises: :exc:`~tredis.exceptions.RedisError`", "response": "def zadd(self, key, *members, **kwargs):\n        \"\"\"Adds all the specified members with the specified scores to the\n        sorted set stored at key. It is possible to specify multiple score /\n        member pairs. If a specified member is already a member of the sorted\n        set, the score is updated and the element reinserted at the right\n        position to ensure the correct ordering.\n\n        If key does not exist, a new sorted set with the specified members as\n        sole members is created, like if the sorted set was empty. If the key\n        exists but does not hold a sorted set, an error is returned.\n\n        The score values should be the string representation of a double\n        precision floating point number. +inf and -inf values are valid values\n        as well.\n\n        **Members parameters**\n\n        ``members`` could be either:\n        - a single dict where keys correspond to scores and values to elements\n        - multiple strings paired as score then element\n\n        .. code:: python\n\n            yield client.zadd('myzset', {'1': 'one', '2': 'two'})\n            yield client.zadd('myzset', '1', 'one', '2', 'two')\n\n        **ZADD options (Redis 3.0.2 or greater)**\n\n        ZADD supports a list of options. Options are:\n\n        - ``xx``: Only update elements that already exist. Never add elements.\n        - ``nx``: Don't update already existing elements. Always add new\n            elements.\n        - ``ch``: Modify the return value from the number of new elements\n            added, to the total number of elements changed (CH is an\n            abbreviation of changed). Changed elements are new elements added\n            and elements already existing for which the score was updated. So\n            elements specified in the command having the same score as they had\n            in the past are not counted. Note: normally the return value of\n            ``ZADD`` only counts the number of new elements added.\n        - ``incr``: When this option is specified ``ZADD`` acts like\n            :meth:`~tredis.RedisClient.zincrby`. Only one score-element pair\n            can be specified in this mode.\n\n        .. note::\n\n           **Time complexity**: ``O(log(N))`` for each item added, where ``N``\n           is the number of elements in the sorted set.\n\n        :param key: The key of the sorted set\n        :type key: :class:`str`, :class:`bytes`\n        :param members: Elements to add\n        :type members: :class:`dict`, :class:`str`, :class:`bytes`\n        :keyword bool xx: Only update elements that already exist\n        :keyword bool nx: Don't update already existing elements\n        :keyword bool ch: Return the number of changed elements\n        :keyword bool incr: Increment the score of an element\n        :rtype: int, :class:`str`, :class:`bytes`\n        :returns: Number of elements changed, or the new score if incr is set\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        xx = kwargs.pop('xx', False)\n        nx = kwargs.pop('nx', False)\n        ch = kwargs.pop('ch', False)\n        incr = kwargs.pop('incr', False)\n        command = [b'ZADD', key]\n        if xx:\n            command += ['XX']\n        if nx:\n            command += ['NX']\n        if ch:\n            command += ['CH']\n        if incr:\n            command += ['INCR']\n\n        if len(members) == 1:\n            for k in members[0]:\n                command += [k, members[0][k]]\n        else:\n            command += list(members)\n        return self._execute(command)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the specified range of elements in the sorted set stored at key.", "response": "def zrange(self, key, start=0, stop=-1, with_scores=False):\n        \"\"\"Returns the specified range of elements in the sorted set stored at\n        key. The elements are considered to be ordered from the lowest to the\n        highest score. Lexicographical order is used for elements with equal\n        score.\n\n        See :meth:`tredis.Client.zrevrange` when you need the elements ordered\n        from highest to lowest score (and descending lexicographical order for\n        elements with equal score).\n\n        Both start and stop are zero-based indexes, where ``0`` is the first\n        element, ``1`` is the next element and so on. They can also be negative\n        numbers indicating offsets from the end of the sorted set, with ``-1``\n        being the last element of the sorted set, ``-2`` the penultimate\n        element and so on.\n\n        ``start`` and ``stop`` are inclusive ranges, so for example\n        ``ZRANGE myzset 0 1`` will return both the first and the second element\n        of the sorted set.\n\n        Out of range indexes will not produce an error. If start is larger than\n        the largest index in the sorted set, or ``start > stop``, an empty list\n        is returned. If stop is larger than the end of the sorted set Redis\n        will treat it like it is the last element of the sorted set.\n\n        It is possible to pass the ``WITHSCORES`` option in order to return the\n        scores of the elements together with the elements. The returned list\n        will contain ``value1,score1,...,valueN,scoreN`` instead of\n        ``value1,...,valueN``. Client libraries are free to return a more\n        appropriate data type (suggestion: an array with (value, score)\n        arrays/tuples).\n\n        .. note::\n\n           **Time complexity**: ``O(log(N)+M)`` with ``N`` being the number of\n           elements in the sorted set and ``M`` the number of elements\n           returned.\n\n        :param key: The key of the sorted set\n        :type key: :class:`str`, :class:`bytes`\n        :param int start: The starting index of the sorted set\n        :param int stop: The ending index of the sorted set\n        :param bool with_scores: Return the scores with the elements\n\n        :rtype: list\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        \"\"\"\n        command = [b'ZRANGE', key, start, stop]\n        if with_scores:\n            command += ['WITHSCORES']\n        return self._execute(command)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zrangebyscore(self,\n                      key,\n                      min_score,\n                      max_score,\n                      with_scores=False,\n                      offset=0,\n                      count=0):\n        \"\"\"Returns all the elements in the sorted set at key with a score\n        between min and max (including elements with score equal to min or\n        max). The elements are considered to be ordered from low to high\n        scores.\n\n        The elements having the same score are returned in lexicographical\n        order (this follows from a property of the sorted set implementation in\n        Redis and does not involve further computation).\n\n        The optional ``offset`` and ``count`` arguments can be used to only get\n        a range of the matching elements (similar to SELECT LIMIT offset, count\n        in SQL). Keep in mind that if offset is large, the sorted set needs to\n        be traversed for offset elements before getting to the elements to\n        return, which can add up to ``O(N)`` time complexity.\n\n        The optional ``with_scores`` argument makes the command return both the\n        element and its score, instead of the element alone. This option is\n        available since Redis 2.0.\n\n        **Exclusive intervals and infinity**\n\n        ``min_score`` and ``max_score`` can be ``-inf`` and ``+inf``, so that\n        you are not required to know the highest or lowest score in the sorted\n        set to get all elements from or up to a certain score.\n\n        By default, the interval specified by ``min_score`` and ``max_score``\n        is closed (inclusive). It is possible to specify an open interval\n        (exclusive) by prefixing the score with the character ``(``. For\n        example:\n\n        .. code::\n\n            ZRANGEBYSCORE zset (1 5\n\n        Will return all elements with ``1 < score <= 5`` while:\n\n        .. code::\n\n            ZRANGEBYSCORE zset (5 (10\n\n        Will return all the elements with ``5 < score < 10`` (5 and 10\n        excluded).\n\n        .. note::\n\n           **Time complexity**: ``O(log(N)+M)`` with ``N`` being the number of\n           elements in the sorted set and ``M`` the number of elements being\n           returned. If ``M`` is constant (e.g. always asking for the first\n           10 elements with ``count``), you can consider it ``O(log(N))``.\n\n        :param key: The key of the sorted set\n        :type key: :class:`str`, :class:`bytes`\n        :param min_score: Lowest score definition\n        :type min_score: :class:`str`, :class:`bytes`\n        :param max_score: Highest score definition\n        :type max_score: :class:`str`, :class:`bytes`\n        :param bool with_scores: Return elements and scores\n        :param offset: The number of elements to skip\n        :type min_score: :class:`str`, :class:`bytes`\n        :param count: The number of elements to return\n        :type min_score: :class:`str`, :class:`bytes`\n        :rtype: list\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        \"\"\"\n        command = [b'ZRANGEBYSCORE', key, min_score, max_score]\n        if with_scores:\n            command += ['WITHSCORES']\n        if offset or count:\n            command += ['LIMIT', offset, count]\n        return self._execute(command)", "response": "Returns all the elements in the sorted set at key with a score between min and max."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the specified members from the sorted set stored at key. Returns an error if the key does not exist.", "response": "def zrem(self, key, *members):\n        \"\"\"Removes the specified members from the sorted set stored at key.\n         Non existing members are ignored.\n\n        An error is returned when key exists and does not hold a sorted set.\n\n        .. note::\n\n           **Time complexity**: ``O(M*log(N))`` with ``N`` being the number of\n           elements in the sorted set and ``M`` the number of elements to be\n           removed.\n\n        :param key: The key of the sorted set\n        :type key: :class:`str`, :class:`bytes`\n        :param members: One or more member values to remove\n        :type members: :class:`str`, :class:`bytes`\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        \"\"\"\n        return self._execute([b'ZREM', key] + list(members))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all elements in the sorted set stored at key with a score between min and max.", "response": "def zremrangebyscore(self, key, min_score, max_score):\n        \"\"\"Removes all elements in the sorted set stored at key with a score\n        between min and max.\n\n        Intervals are described in :meth:`~tredis.RedisClient.zrangebyscore`.\n\n        Returns the number of elements removed.\n\n        .. note::\n\n           **Time complexity**: ``O(log(N)+M)`` with ``N`` being the number of\n           elements in the sorted set and M the number of elements removed by\n           the operation.\n\n        :param key: The key of the sorted set\n        :type key: :class:`str`, :class:`bytes`\n        :param min_score: Lowest score definition\n        :type min_score: :class:`str`, :class:`bytes`\n        :param max_score: Highest score definition\n        :type max_score: :class:`str`, :class:`bytes`\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.RedisError`\n        \"\"\"\n        return self._execute([b'ZREMRANGEBYSCORE', key, min_score, max_score])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zrevrange(self, key, start=0, stop=-1, with_scores=False):\n        command = [b'ZREVRANGE', key, start, stop]\n        if with_scores:\n            command += ['WITHSCORES']\n        return self._execute(command)", "response": "Returns the specified range of elements in the sorted set stored at\n        key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the cache key depending on its type.", "response": "def getkey(stype, site_id=None, key=None):\n\t'Returns the cache key depending on its type.'\n\tbase = '{0}.feedjack'.format(settings.CACHE_MIDDLEWARE_KEY_PREFIX)\n\tif stype == T_HOST: return '{0}.hostcache'.format(base)\n\telif stype == T_ITEM: return '{0}.{1}.item.{2}'.format(base, site_id, str2md5(key))\n\telif stype == T_META: return '{0}.{1}.meta'.format(base, site_id)\n\telif stype == T_INTERVAL: return '{0}.interval.{1}'.format(base, str2md5(key))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget adaptive interval between checks for a feed.", "response": "def feed_interval_get(feed_id, parameters):\n\t'Get adaptive interval between checks for a feed.'\n\tval = cache.get(getkey( T_INTERVAL,\n\t\tkey=feed_interval_key(feed_id, parameters) ))\n\treturn val if isinstance(val, tuple) else (val, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef feed_interval_set(feed_id, parameters, interval, interval_ts):\n\t'Set adaptive interval between checks for a feed.'\n\tcache.set(getkey( T_INTERVAL,\n\t\tkey=feed_interval_key(feed_id, parameters) ), (interval, interval_ts))", "response": "Set adaptive interval between checks for a feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef feed_interval_delete(feed_id, parameters):\n\t'Invalidate cached adaptive interval value.'\n\tcache.delete(getkey( T_INTERVAL,\n\t\tkey=feed_interval_key(feed_id, parameters) ))", "response": "Invalidate cached adaptive interval value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cache_set(site, key, data):\n\t'''Sets cache data for a site.\n\t\tAll keys related to a site are stored in a meta key. This key is per-site.'''\n\ttkey = getkey(T_ITEM, site.id, key)\n\tmkey = getkey(T_META, site.id)\n\ttmp = cache.get(mkey)\n\tlongdur = 365*24*60*60\n\tif not tmp:\n\t\ttmp = [tkey]\n\t\tcache.set(mkey, [tkey], longdur)\n\telif tkey not in tmp:\n\t\ttmp.append(tkey)\n\t\tcache.set(mkey, tmp, longdur)\n\tcache.set(tkey, data, site.cache_duration)", "response": "Sets the cache data for a site."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache_delsite(site_id):\n\t'Removes all cache data from a site.'\n\tmkey = getkey(T_META, site_id)\n\ttmp = cache.get(mkey)\n\tif not tmp:\n\t\treturn\n\tfor tkey in tmp:\n\t\tcache.delete(tkey)\n\tcache.delete(mkey)", "response": "Removes all cache data from a site."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a message with the NTM GSSwrap method.", "response": "def wrap(self, message):\n        \"\"\"\n        NTM GSSwrap()\n        :param message: The message to be encrypted\n        :return: The signed and encrypted message\n        \"\"\"\n        cipher_text = _Ntlm1Session.encrypt(self, message)\n        signature = _Ntlm1Session.sign(self, message)\n        return cipher_text, signature"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _weaken_key(flags, key):\n        if flags & NegotiateFlag.NTLMSSP_KEY_128:\n            return key\n        if flags & NegotiateFlag.NTLMSSP_NEGOTIATE_56:\n            return key[:7]\n        else:\n            return key[:5]", "response": "This function is used to generate a 16 - byte key for the NTLM2 negotiated NTLM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a signature for the supplied message using NTLM2 Session Security", "response": "def sign(self, message):\n        \"\"\"\n        Generates a signature for the supplied message using NTLM2 Session Security\n        Note: [MS-NLMP] Section 3.4.4\n        The message signature for NTLM with extended session security is a 16-byte value that contains the following\n        components, as described by the NTLMSSP_MESSAGE_SIGNATURE structure:\n         - A 4-byte version-number value that is set to 1\n         - The first eight bytes of the message's HMAC_MD5\n         - The 4-byte sequence number (SeqNum)\n        :param message: The message to be signed\n        :return: The signature for supplied message\n        \"\"\"\n        hmac_context = hmac.new(self.outgoing_signing_key)\n        hmac_context.update(struct.pack('<i', self.outgoing_sequence) + message)\n\n        # If a key exchange key is negotiated the first 8 bytes of the HMAC MD5 are encrypted with RC4\n        if self.key_exchange:\n            checksum = self.outgoing_seal.update(hmac_context.digest()[:8])\n        else:\n            checksum = hmac_context.digest()[:8]\n\n        mac = _Ntlm2MessageSignature()\n        mac['checksum'] = struct.unpack('<q', checksum)[0]\n        mac['sequence'] = self.outgoing_sequence\n        #logger.debug(\"Signing Sequence Number: %s\", str(self.outgoing_sequence))\n\n        # Increment the sequence number after signing each message\n        self.outgoing_sequence += 1\n        return str(mac)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying the signature attached to the message using NTLM2 Session Security", "response": "def verify(self, message, signature):\n        \"\"\"\n        Verified the signature attached to the supplied message using NTLM2 Session Security\n        :param message: The message whose signature will verified\n        :return: True if the signature is valid, otherwise False\n        \"\"\"\n        # Parse the signature header\n        mac = _Ntlm2MessageSignature()\n        mac.from_string(signature)\n\n        # validate the sequence\n        if mac['sequence'] != self.incoming_sequence:\n            raise Exception(\"The message was not received in the correct sequence.\")\n\n        # extract the supplied checksum\n        checksum = struct.pack('<q', mac['checksum'])\n        if self.key_exchange:\n            checksum = self.incoming_seal.update(checksum)\n\n        # calculate the expected checksum for the message\n        hmac_context = hmac.new(self.incoming_signing_key)\n        hmac_context.update(struct.pack('<i', self.incoming_sequence) + message)\n        expected_checksum = hmac_context.digest()[:8]\n\n        # validate the supplied checksum is correct\n        if checksum != expected_checksum:\n            raise Exception(\"The message has been altered\")\n\n        #logger.debug(\"Verify Sequence Number: %s\", AsHex(self.outgoing_sequence))\n        self.incoming_sequence += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps a message with the NTM GSSwrap method.", "response": "def wrap(self, message):\n        \"\"\"\n        NTM GSSwrap()\n        :param message: The message to be encrypted\n        :return: A Tuple containing the signature and the encrypted messaging\n        \"\"\"\n        cipher_text = _Ntlm2Session.encrypt(self, message)\n        signature = _Ntlm2Session.sign(self, message)\n        return cipher_text, signature"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mark_dead(self, connection, now=None):\n        # allow inject for testing purposes\n        now = now if now else time.time()\n        try:\n            self.connections.remove(connection)\n        except ValueError:\n            # connection not alive or another thread marked it already, ignore\n            return\n        else:\n            dead_count = self.dead_count.get(connection, 0) + 1\n            self.dead_count[connection] = dead_count\n            timeout = self.dead_timeout * 2 ** min(dead_count - 1,\n                                                   self.timeout_cutoff)\n            self.dead.put((now + timeout, connection))\n            logger.warning(\n                'Connection %r has failed for %i times in a row,'\n                ' putting on %i second timeout.',\n                connection, dead_count, timeout\n            )", "response": "Mark the connection as dead."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresurrect a connection from the dead pool.", "response": "def resurrect(self, force=False):\n        \"\"\"\n        Attempt to resurrect a connection from the dead pool. It will try to\n        locate one (not all) eligible (it's timeout is over) connection to\n        return to th live pool.\n\n        :arg force: resurrect a connection even if there is none eligible (used\n            when we have no live connections)\n\n        \"\"\"\n        # no dead connections\n        if self.dead.empty():\n            return\n\n        try:\n            # retrieve a connection to check\n            timeout, connection = self.dead.get(block=False)\n        except Empty:\n            # other thread has been faster and the queue is now empty\n            return\n\n        if not force and timeout > time.time():\n            # return it back if not eligible and not forced\n            self.dead.put((timeout, connection))\n            return\n\n        # either we were forced or the connection is elligible to be retried\n        self.connections.append(connection)\n        logger.info('Resurrecting connection %r (force=%s).', connection, force)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a connection instance from the pool using the ConnectionSelector instance.", "response": "def get_connection(self):\n        \"\"\"\n        Return a connection from the pool using the `ConnectionSelector`\n        instance.\n\n        It tries to resurrect eligible connections, forces a resurrection when\n        no connections are availible and passes the list of live connections to\n        the selector instance to choose from.\n\n        Returns a connection instance and it's current fail count.\n        \"\"\"\n        self.resurrect()\n\n        # no live nodes, resurrect one by force\n        if not self.connections:\n            self.resurrect(True)\n        connection = self.selector.select(self.connections)\n\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _canvas_route(self, *args, **kwargs):\n    def outer(view_fn):\n        @self.route(*args, **kwargs)\n        def inner(*args, **kwargs):\n            fn_args = getargspec(view_fn)\n            try:\n                idx = fn_args.args.index(_ARG_KEY)\n            except ValueError:\n                idx = -1\n\n            if idx > -1:\n                if 'error' in flask_request.args:\n                    return redirect('%s?error=%s' % (\n                        self.config.get('CANVAS_ERROR_URI', '/'),\n                        flask_request.args.get('error')))\n\n                if 'signed_request' not in flask_request.form:\n                    self.logger.error('signed_request not in request.form')\n                    abort(403)\n\n                try:\n                    _, decoded_data = _decode_signed_user(\n                        *flask_request.form['signed_request'].split('.'))\n                except ValueError as e:\n                    self.logger.error(e.message)\n                    abort(403)\n\n                if 'oauth_token' not in decoded_data:\n                    app.logger.info('unauthorized user, redirecting')\n                    return _authorize()\n\n                user = User(**decoded_data)\n\n                if not app.config.get('CANVAS_SKIP_AUTH_CHECK', False) \\\n                    and not user.has_permissions():\n                    self.logger.info(\n                        'user does not have the required permission set.')\n                    return _authorize()\n\n                self.logger.info('all required permissions have been granted')\n                args = args[:idx - 1] + (user,) + args[idx:]\n\n            return view_fn(*args, **kwargs)\n        return inner\n    return outer", "response": "Decorator for canvas route \n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecodes the POSTed signed data and verifies that the signature matches the hash of the data.", "response": "def _decode_signed_user(encoded_sig, encoded_data):\n    \"\"\" Decodes the ``POST``ed signed data\n    \"\"\"\n    decoded_sig = _decode(encoded_sig)\n    decoded_data = loads(_decode(encoded_data))\n\n    if decoded_sig != hmac.new(app.config['CANVAS_CLIENT_SECRET'], \n        encoded_data, sha256).digest():\n        raise ValueError(\"sig doesn't match hash\")\n\n    return decoded_sig, decoded_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request(self, path, data=None, method='GET'):\n        url = '%s%s?access_token=%s' % (\n            'https://graph.facebook.com',\n            path,\n            self['oauth_token'])\n\n        req = Request(url, data=data)\n        req.get_method = lambda: method\n\n        return loads(urlopen(req).read())", "response": "Utility function to request resources via Facebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_permissions(self):\n        perms = self.request('/me/permissions')['data'][0].keys()\n        return all(k in perms for k in app.config[\n            'CANVAS_SCOPE'].split(','))", "response": "Check current user permission set against the one being requested\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef same_guid(post, parameter=DEFAULT_SIMILARITY_TIMESPAN):\n\t'''Skip posts with exactly same GUID.\n\t\tParameter: comparison timespan, seconds (int, 0 = inf, default: {0}).'''\n\tfrom feedjack.models import Post\n\tif isinstance(parameter, types.StringTypes): parameter = int(parameter.strip())\n\tsimilar = Post.objects.filtered(for_display=False)\\\n\t\t.exclude(id=post.id).filter(guid=post.guid)\n\tif parameter:\n\t\tsimilar = similar.filter(date_updated__gt=timezone.now() - timedelta(seconds=parameter))\n\treturn not bool(similar.exists())", "response": "Skip posts with exactly same GUID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nskip posts with fuzzy - matched title.", "response": "def similar_title(post, parameter=None):\n\t'''Skip posts with fuzzy-matched (threshold = levenshtein distance / length) title.\n\t\tParameters (comma-delimited):\n\t\t\tminimal threshold, at which values are considired similar (float, 0 < x < 1, default: {0});\n\t\t\tcomparison timespan, seconds (int, 0 = inf, default: {1}).'''\n\tfrom feedjack.models import Post\n\tthreshold, timespan = DEFAULT_SIMILARITY_THRESHOLD, DEFAULT_SIMILARITY_TIMESPAN\n\tif parameter:\n\t\tparameter = map(op.methodcaller('strip'), parameter.split(',', 1))\n\t\tthreshold = parameter.pop()\n\t\ttry: threshold, timespan = parameter.pop(), threshold\n\t\texcept IndexError: pass\n\t\tthreshold, timespan = float(threshold), int(timespan)\n\tsimilar = Post.objects.filtered(for_display=False)\\\n\t\t.exclude(id=post.id).similar(threshold, title=post.title)\n\tif timespan:\n\t\tsimilar = similar.filter(date_updated__gt=timezone.now() - timedelta(seconds=timespan))\n\treturn not bool(similar.exists())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides URL of the Post to point to url of the first enclosure with href attribute non - empty and type matching specified regexp parameter.", "response": "def pick_enclosure_link(post, parameter=''):\n\t'''Override URL of the Post to point to url of the first enclosure with\n\t\t\thref attribute non-empty and type matching specified regexp parameter (empty=any).\n\t\tMissing \"type\" attribute for enclosure will be matched as an empty string.\n\t\tIf none of the enclosures match, link won't be updated.'''\n\tfor e in (post.enclosures or list()):\n\t\thref = e.get('href')\n\t\tif not href: continue\n\t\tif parameter and not re.search(parameter, e.get('type', '')): continue\n\t\treturn dict(link=href)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new log level to the logging module.", "response": "def add_log_level(value, name):\n    \"\"\"\n    Add a new log level to the :mod:`logging` module.\n\n    :param value: The log level's number (an integer).\n    :param name: The name for the log level (a string).\n    \"\"\"\n    logging.addLevelName(value, name)\n    setattr(logging, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notice(self, msg, *args, **kw):\n        if self.isEnabledFor(NOTICE):\n            self._log(NOTICE, msg, args, **kw)", "response": "Log a message with level NOTICE."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spam(self, msg, *args, **kw):\n        if self.isEnabledFor(SPAM):\n            self._log(SPAM, msg, args, **kw)", "response": "Log a message with level SPAM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging a message with level : data : SUCCESS.", "response": "def success(self, msg, *args, **kw):\n        \"\"\"Log a message with level :data:`SUCCESS`. The arguments are interpreted as for :func:`logging.debug()`.\"\"\"\n        if self.isEnabledFor(SUCCESS):\n            self._log(SUCCESS, msg, args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog a message with level verbose.", "response": "def verbose(self, msg, *args, **kw):\n        \"\"\"Log a message with level :data:`VERBOSE`. The arguments are interpreted as for :func:`logging.debug()`.\"\"\"\n        if self.isEnabledFor(VERBOSE):\n            self._log(VERBOSE, msg, args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the import string for the calculator_name.", "response": "def get_calculator_impstr(calculator_name):\n    \"\"\"\n    Returns the import string for the calculator\n    \"\"\"\n    if calculator_name.lower() == \"gpaw\" or calculator_name is None:\n        return \"from gpaw import GPAW as custom_calculator\"\n    elif calculator_name.lower() == \"espresso\":\n        return \"from espresso import espresso as custom_calculator\"\n    else:\n        possibilities = {\"abinit\":\"abinit.Abinit\",\n                         \"aims\":\"aims.Aims\",\n                         \"ase_qmmm_manyqm\":\"AseQmmmManyqm\",\n                         \"castep\":\"Castep\",\n                         \"dacapo\":\"Dacapo\",\n                         \"dftb\":\"Dftb\",\n                         \"eam\":\"EAM\",\n                         \"elk\":\"ELK\",\n                         \"emt\":\"EMT\",\n                         \"exciting\":\"Exciting\",\n                         \"fleur\":\"FLEUR\",\n                         \"gaussian\":\"Gaussian\",\n                         \"gromacs\":\"Gromacs\",\n                         \"mopac\":\"Mopac\",\n                         \"morse\":\"MorsePotential\",\n                         \"nwchem\":\"NWChem\",\n                         'siesta':\"Siesta\",\n                         \"tip3p\":\"TIP3P\",\n                         \"turbomole\":\"Turbomole\",\n                         \"vasp\":\"Vasp\",\n                         }\n        \n        current_val = possibilities.get(calculator_name.lower())\n        \n        package, class_name = (calculator_name,current_val) if current_val else calculator_name.rsplit('.',1)\n        \n        return \"from ase.calculators.{} import {} as custom_calculator\".format(package, class_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_optimizer_impstr(optimizer_name):\n    possibilities = {\"bfgs\":\"BFGS\",\n                     \"bfgslinesearch\":\"BFGSLineSearch\",\n                     \"fire\":\"FIRE\",\n                     \"goodoldquasinewton\":\"GoodOldQuasiNewton\",\n                     \"hesslbfgs\":\"HessLBFGS\",\n                     \"lbfgs\":\"LBFGS\",\n                     \"lbfgslinesearch\":\"LBFGSLineSearch\",\n                     \"linelbfgs\":\"LineLBFGS\",\n                     \"mdmin\":\"MDMin\",\n                     \"ndpoly\":\"NDPoly\",\n                     \"quasinewton\":\"QuasiNewton\",\n                     \"scipyfmin\":\"SciPyFmin\",\n                     \"scipyfminbfgs\":\"SciPyFminBFGS\",\n                     \"scipyfmincg\":\"SciPyFminCG\",\n                     \"scipyfminpowell\":\"SciPyFminPowell\",\n                     \"scipygradientlessoptimizer\":\"SciPyGradientlessOptimizer\",\n                     }\n    \n    current_val = possibilities.get(optimizer_name.lower())\n    \n    if current_val:\n        return \"from ase.optimize import {} as custom_optimizer\".format(current_val)\n    else:\n        package,current_val = optimizer_name.rsplit('.',1)\n        return \"from ase.optimize.{} import {} as custom_optimizer\".format(package,current_val)", "response": "Returns the import string for the optimizer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_the_getters(getters):\n    return_list = []\n    for getter in getters:\n        \n        if isinstance(getter,basestring):\n            out_args = \"\"\n            method_name = getter\n            \n        else:\n            method_name, a = getter\n            \n            out_args = convert_the_args(a)\n            \n        return_list.append( (method_name, out_args) )\n    return return_list", "response": "A function used to prepare the arguments of the getters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_the_args(raw_args):\n    if not raw_args:\n        return \"\"\n    if isinstance(raw_args,dict):\n        out_args = \", \".join([ \"{}={}\".format(k,v) for k,v in raw_args.iteritems() ])\n        \n    elif isinstance(raw_args,(list,tuple)):\n        new_list = []\n        for x in raw_args:\n            if isinstance(x,basestring):\n                new_list.append(x)\n            elif isinstance(x,dict):\n                new_list.append( \", \".join([ \"{}={}\".format(k,v) for k,v in x.iteritems() ]) )\n            else:\n                raise ValueError(\"Error preparing the getters\")\n        out_args = \", \".join(new_list)\n    else:\n        raise ValueError(\"Couldn't recognize list of getters\")\n    return out_args", "response": "Function used to convert the arguments of methods\n    to the format used by the methods\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runcmd(command, command_input=None, cwd=None):\n    proc = subprocess.Popen(command, stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE,\n                            cwd=cwd)\n    (stdout, stderr) = proc.communicate(command_input)\n    if proc.returncode != 0:\n        sys.stderr.write('ABORTING: command \"%s\" failed w/ code %s:\\n'\n                         '%s\\n%s' % (command, proc.returncode,\n                                     stdout, stderr))\n        sys.exit(proc.returncode)\n    return proc.returncode, stdout, stderr", "response": "Run a command and return the return code stdout and stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies and convert various documentation files.", "response": "def dd_docs(self):\n        \"\"\"Copy and convert various documentation files.\"\"\"\n        top = os.path.join(os.path.dirname(__file__))\n        doc = os.path.join(top, 'doc')\n\n        # Markdown to ronn to man page\n        man_md = os.path.join(doc, 'authprogs.md')\n        man_ronn = os.path.join(doc, 'authprogs.1.ronn')\n        man_1 = os.path.join(doc, 'authprogs.1')\n\n        # Create manpage\n        try:\n            if not os.path.exists(man_1):\n                shutil.copy(man_md, man_ronn)\n                self.created.append(man_ronn)\n                retval = subprocess.call(['ronn', '-r', man_ronn])\n                if retval != 0:\n                    raise Exception('ronn man page conversion failed, '\n                                    'returned %s' % retval)\n                self.created.append(man_1)\n        except:\n            raise Exception('ronn required for manpage conversion - do you '\n                            'have it installed?')\n\n        # Markdown files in docs dir get converted to .html\n        for name in MARKDOWN2HTML:\n            htmlfile = os.path.join(doc, '%s.html' % name)\n            if os.path.exists(htmlfile):\n                continue\n\n            target = open(htmlfile, 'w')\n            self.created.append(htmlfile)\n            stdout = runcmd(['python', '-m', 'markdown',\n                             os.path.join(doc, '%s.md' % name)])[1]\n            if not stdout:\n                raise Exception('markdown conversion failed, no output.')\n            target.write(stdout)\n            target.close()\n\n        # Markdown files in top level just get renamed sans .md\n        for name in MARKDOWN2TEXT:\n            target = os.path.join(top, name)\n            if os.path.exists(target):\n                continue\n            source = os.path.join(top, '%s.md' % target)\n            shutil.copy(source, target)\n            self.created.append(target)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform scraped_info dictionary into an entry containing title formats thumbnail etc.", "response": "def extract_entry(scraped_info):\n        \"\"\"\n        Transform scraped_info dictionary into an entry, under the assumption that there is only\n        one track in 'track' list, since each video/audio is instantiated individually\n        on the RMF website and each of them is scraped independently, so there shouldn't be cases\n        when there are 2 unrelated tracks in one info_dict.\n\n        Args:\n            scraped_info (dict): Video info dict, scraped straight from the website.\n\n        Returns:\n            dict: Entry containing title, formats (url, quality), thumbnail, etc.\n\n        \"\"\"\n        quality_mapping = {  # ascending in terms of quality\n            'lo': 0,\n            'hi': 1\n        }\n\n        entry = scraped_info['tracks'][0]\n        '''\n        The structure of entry is as follows:\n\n        'src': {\n            'hi': [\n                {\n                'src': 'http://v.iplsc.com/30-11-gosc-marek-jakubiak/0007124B3CGCAE6P-A1.mp4',\n                'type': 'video/mp4'\n                }\n            ],\n            'lo': [\n                {\n                'src': 'http://v.iplsc.com/30-11-gosc-marek-jakubiak/0007124B3CGCAE6P-A1.mp4',\n                'type': 'video/mp4'\n                }\n            ]\n        }\n        '''\n\n        sources = entry.pop('src')\n\n        # TODO: #LOW_PRIOR Remove date from title of audio files e.g. '10.06 Go\u015b\u0107: Jaros\u0142aw Gowin'\n\n        formats = []\n        for src_name, src in sources.items():\n            url = src[0]['src']\n            formats.append({\n                'url': url,\n                'quality': quality_mapping[src_name],\n                'ext': get_ext(url),\n                'width': int(scraped_info.get('width', 0)),\n                'height': int(scraped_info.get('height', 0)),\n            })\n\n        # outer level url and ext come from the video of the lowest quality\n        # you can access rest of the urls under 'formats' key\n        worst_format = min(formats, key=lambda f: f['quality'])\n        entry.update({\n            **entry.pop('data'),\n            'formats': formats,\n            'url': worst_format['url'],\n            'ext': worst_format['ext']\n        })\n\n        return entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index(request, obj_id):\n    if request.method == 'GET':\n        return get(request, obj_id)\n    elif request.method == 'PUT':\n        getPutData(request)\n        return put(request, obj_id)", "response": "Handles a request based on the method and calls the appropriate function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a serialized object", "response": "def get(request, obj_id):\n    \"\"\"Returns a serialized object\n    :param obj_id: ID of comment object\n    :type obj_id: int\n    :returns: json\n    \"\"\"\n    res = Result()\n    c = Comment.objects.get(pk=obj_id)\n    res.append(commentToJson(c))\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post(request):\n    data = request.POST or json.loads(request.body)['body']\n    guid = data.get('guid', None)\n    res = Result()\n\n    if guid:\n        obj = getObjectsFromGuids([guid,])[0]\n        comment = Comment()\n        comment.comment = data.get('comment', 'No comment')\n        comment.user = request.user\n        comment.user_name = request.user.get_full_name()\n        comment.user_email = request.user.email\n        comment.content_object = obj\n        # For our purposes, we never have more than one site\n        comment.site_id = 1\n        comment.save()\n\n        obj.comment_count += 1\n        obj.save()\n\n        emailComment(comment, obj, request)\n\n        res.append(commentToJson(comment))\n\n    return JsonResponse(res.asDict())", "response": "Returns a serialized object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the content of a comment object", "response": "def put(request, obj_id):\n    \"\"\"Updates the content of a comment\n    :param obj_id: ID of comment object\n    :type obj_id: int\n    :returns: json\n    \"\"\"\n    res = Result()\n    c = Comment.objects.get(pk=obj_id)\n    data = request.PUT or json.loads(request.body)['body']\n    content = data.get('comment', None)\n    if content:\n        c.comment = content\n        c.save()\n\n        res.append(commentToJson(c))\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef commentList(request):\n    if request.method == 'POST':\n        return post(request)\n\n    comments = []\n    guid = request.GET.get('guid', None)\n\n    if guid:\n        obj = getObjectsFromGuids([guid])[0]\n        if obj.AssetType == 1:\n            model = 'image'\n        else:\n            model = 'video'\n        contenttype = ContentType.objects.get(app_label=\"frog\", model=model)\n        comments = Comment.objects.filter(object_pk=obj.id, content_type=contenttype)\n\n    res = Result()\n    for comment in comments:\n        res.append(commentToJson(comment))\n    return JsonResponse(res.asDict())", "response": "Returns a rendered list of comments for the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending an email to the author about a new comment", "response": "def emailComment(comment, obj, request):\n    \"\"\"Send an email to the author about a new comment\"\"\"\n    if not obj.author.frog_prefs.get().json()['emailComments']:\n        return\n\n    if obj.author == request.user:\n        return\n\n    html = render_to_string('frog/comment_email.html', {\n        'user': comment.user,\n        'comment': comment.comment,\n        'object': obj,\n        'action_type': 'commented on',\n        'image': isinstance(obj, Image),\n        'SITE_URL': FROG_SITE_URL,\n    })\n\n    subject = '{}: Comment from {}'.format(getSiteConfig()['name'], comment.user_name)\n    fromemail = comment.user_email\n    to = obj.author.email\n    text_content = 'This is an important message.'\n    html_content = html\n\n    send_mail(subject, text_content, fromemail, [to], html_message=html_content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the specified members to the set stored at key.", "response": "def sadd(self, key, *members):\n        \"\"\"Add the specified members to the set stored at key. Specified\n        members that are already a member of this set are ignored. If key does\n        not exist, a new set is created before adding the specified members.\n\n        An error is returned when the value stored at key is not a set.\n\n        Returns :data:`True` if all requested members are added. If more\n        than one member is passed in and not all members are added, the\n        number of added members is returned.\n\n        .. note::\n\n           **Time complexity**: ``O(N)`` where ``N`` is the number of members\n           to be added.\n\n        :param key: The key of the set\n        :type key: :class:`str`, :class:`bytes`\n        :param members: One or more positional arguments to add to the set\n        :type key: :class:`str`, :class:`bytes`\n        :returns: Number of items added to the set\n        :rtype: bool, int\n\n        \"\"\"\n        return self._execute([b'SADD', key] + list(members), len(members))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sinterstore(self, destination, *keys):\n        return self._execute([b'SINTERSTORE', destination] + list(keys))", "response": "This command is equal to : meth : ~tredis. RedisClient. sinter but\n        instead of returning the resulting set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smove(self, source, destination, member):\n        return self._execute([b'SMOVE', source, destination, member], 1)", "response": "Move a member from one set to another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove and returns one or more random elements from the set value store at key.", "response": "def spop(self, key, count=None):\n        \"\"\"Removes and returns one or more random elements from the set value\n        store at key.\n\n        This operation is similar to :meth:`~tredis.RedisClient.srandmember`,\n        that returns one or more random elements from a set but does not remove\n        it.\n\n        The count argument will be available in a later version and is not\n        available in 2.6, 2.8, 3.0\n\n        Redis 3.2 will be the first version where an optional count argument\n        can be passed to :meth:`~tredis.RedisClient.spop` in order to retrieve\n        multiple elements in a single call. The implementation is already\n        available in the unstable branch.\n\n        .. note::\n\n           **Time complexity**: Without the count argument ``O(1)``, otherwise\n           ``O(N)`` where ``N`` is the absolute value of the passed count.\n\n        :param key: The key to get one or more random members from\n        :type key: :class:`str`, :class:`bytes`\n        :param int count: The number of members to return\n        :rtype: bytes, list\n        :raises: :exc:`~tredis.exceptions.RedisError`\n\n        \"\"\"\n        command = [b'SPOP', key]\n        if count:  # pragma: nocover\n            command.append(ascii(count).encode('ascii'))\n        return self._execute(command)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef srandmember(self, key, count=None):\n        command = [b'SRANDMEMBER', key]\n        if count:\n            command.append(ascii(count).encode('ascii'))\n        return self._execute(command)", "response": "Returns a random element from the set stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef srem(self, key, *members):\n        return self._execute([b'SREM', key] + list(members), len(members))", "response": "Remove the specified members from the set stored at key. Specified\n        members that are not a member of this set are ignored. If key does not\n        exist, it is treated as an empty set and this command returns ``0``.\n\n        An error is returned when the value stored at key is not a set.\n\n        Returns :data:`True` if all requested members are removed. If more\n        than one member is passed in and not all members are removed, the\n        number of removed members is returned.\n\n        .. note::\n\n           **Time complexity**: ``O(N)`` where ``N`` is the number of members\n           to be removed.\n\n        :param key: The key to remove the member from\n        :type key: :class:`str`, :class:`bytes`\n        :param mixed members: One or more member values to remove\n        :rtype: bool, int\n        :raises: :exc:`~tredis.exceptions.RedisError`"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sscan(self, key, cursor=0, pattern=None, count=None):\n\n        def format_response(value):\n            \"\"\"Format the response from redis\n\n            :param tuple value: The return response from redis\n            :rtype: tuple(int, list)\n\n            \"\"\"\n            return int(value[0]), value[1]\n\n        command = [b'SSCAN', key, ascii(cursor).encode('ascii')]\n        if pattern:\n            command += [b'MATCH', pattern]\n        if count:\n            command += [b'COUNT', ascii(count).encode('ascii')]\n        return self._execute(command, format_callback=format_response)", "response": "This method is used to iterate over the set of keys in the currently selected Redis database and return the values in the sorted set that match the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_id(self) -> str:\n        match = re.match(self._VALID_URL, self.url)\n\n        if match:\n            return match.group('video_id')\n        else:\n            raise VideoIdNotMatchedError", "response": "Extracts the video_id needed to obtain the real_url of the video."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_info(raw_info: VideoInfo) -> VideoInfo:\n        raw_date = raw_info.date\n        date = datetime.strptime(raw_date, '%Y-%m-%d %H:%M')  # 2018-04-05 17:00\n        video_info = raw_info._replace(date=date)\n        return video_info", "response": "Process raw information about the video."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts time in seconds into reasonable time units.", "response": "def convert_time_units(t):\n    \"\"\" Convert time in seconds into reasonable time units. \"\"\"\n    if t == 0:\n        return '0 s'\n    order = log10(t)\n    if -9 < order < -6:\n        time_units = 'ns'\n        factor = 1000000000\n    elif -6 <= order < -3:\n        time_units = 'us'\n        factor = 1000000\n    elif -3 <= order < -1:\n        time_units = 'ms'\n        factor = 1000.\n    elif -1 <= order:\n        time_units = 's'\n        factor = 1\n    return \"{:.3f} {}\".format(factor * t, time_units)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstrips the indentation level so the code runs in the global scope.", "response": "def globalize_indentation(src):\n    \"\"\" Strip the indentation level so the code runs in the global scope. \"\"\"\n    lines = src.splitlines()\n    indent = len(lines[0]) - len(lines[0].strip(' '))\n    func_src = ''\n    for ii, l in enumerate(src.splitlines()):\n        line = l[indent:]\n        func_src += line + '\\n'\n    return func_src"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_decorators(src):\n    src = src.strip()\n    src_lines = src.splitlines()\n    multi_line = False\n    n_deleted = 0\n    for n in range(len(src_lines)):\n        line = src_lines[n - n_deleted].strip()\n        if (line.startswith('@') and 'Benchmark' in line) or multi_line:\n            del src_lines[n - n_deleted]\n            n_deleted += 1\n            if line.endswith(')'):\n                multi_line = False\n            else:\n                multi_line = True\n    setup_src = '\\n'.join(src_lines)\n    return setup_src", "response": "Removes decorators from the source code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk_tree(start, attr):\n    path = [start]\n    for child in path:\n        yield child\n        idx = path.index(child)\n        for grandchild in reversed(getattr(child, attr)):\n            path.insert(idx + 1, grandchild)", "response": "Walk through a tree and yield all the nodes that have the given attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the code once to get it s results. Compare the result to the first function in the group.", "response": "def validate(self, benchmarks):\n        \"\"\"\n        Execute the code once to get it's results (to be used in function validation). Compare the result to the\n        first function in the group.\n        :param benchmarks: list of benchmarks to validate.\n        \"\"\"\n        class_code = self.setup_src\n        instance_creation = '\\ninstance = {}'.format(self.stmt)\n        for i, benchmark in enumerate(benchmarks):\n            if not benchmark.result_validation:\n                break\n\n            validation_code = class_code + instance_creation + '\\nvalidation_result = ' + benchmark.stmt\n            validation_scope = {}\n            exec(validation_code, validation_scope)\n            # Store the result in the first function in the group.\n            if i == 0:\n                compare_against_function = benchmarks[0].callable.__name__\n                compare_against_result = validation_scope['validation_result']\n                logging.info('PyPerform: Validating group \"{b.group}\" against method '\n                             '\"{b.classname}.{b.callable.__name__}\"'.format(b=benchmarks[0]))\n            else:\n                if compare_against_result == validation_scope['validation_result']:\n                    logging.info('PyPerform: Validating {b.classname}.{b.callable.__name__}......PASSED!'\n                                 .format(b=benchmark))\n                else:\n                    error = 'Results of functions {0} and {1} are not equivalent.\\n{0}:\\t {2}\\n{1}:\\t{3}'\n                    raise ValidationError(error.format(compare_against_function, benchmark.callable.__name__,\n                                              compare_against_result, validation_scope['validation_result']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if the cache files have expired or if it is still valid.", "response": "def is_cache_valid(self):\n        ''' Determines if the cache files have expired, or if it is still\n        valid '''\n\n        if os.path.isfile(self.cache_path_cache):\n            mod_time = os.path.getmtime(self.cache_path_cache)\n            current_time = time()\n            if (mod_time + self.cache_max_age) > current_time:\n                if os.path.isfile(self.cache_path_index):\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the settings from the ec2. ini file and populates the instance variables.", "response": "def read_settings(self, configfile):\n        ''' Reads the settings from the ec2.ini file '''\n        if six.PY3:\n            config = configparser.ConfigParser()\n        else:\n            config = configparser.SafeConfigParser()\n        config.read(configfile)\n\n        # is eucalyptus?\n        self.eucalyptus_host = None\n        self.eucalyptus = False\n        if config.has_option('ec2', 'eucalyptus'):\n            self.eucalyptus = config.getboolean('ec2', 'eucalyptus')\n        if self.eucalyptus and config.has_option('ec2', 'eucalyptus_host'):\n            self.eucalyptus_host = config.get('ec2', 'eucalyptus_host')\n\n        # Regions\n        self.regions = []\n        configRegions = config.get('ec2', 'regions')\n        configRegions_exclude = config.get('ec2', 'regions_exclude')\n        if (configRegions == 'all'):\n            if self.eucalyptus_host:\n                self.regions.append(boto.connect_euca(host=self.eucalyptus_host).region.name)\n            else:\n                for regionInfo in ec2.regions():\n                    if regionInfo.name not in configRegions_exclude:\n                        self.regions.append(regionInfo.name)\n        else:\n            self.regions = configRegions.split(\",\")\n\n        # Destination addresses\n        self.destination_variable = config.get('ec2', 'destination_variable')\n        self.vpc_destination_variable = config.get('ec2',\n                                                   'vpc_destination_variable')\n\n        # Route53\n        self.route53_enabled = config.getboolean('ec2', 'route53')\n        self.route53_excluded_zones = []\n        if config.has_option('ec2', 'route53_excluded_zones'):\n            self.route53_excluded_zones.extend(\n                config.get('ec2', 'route53_excluded_zones', '').split(','))\n\n        # Include RDS instances?\n        self.rds_enabled = True\n        if config.has_option('ec2', 'rds'):\n            self.rds_enabled = config.getboolean('ec2', 'rds')\n\n        # Include ElastiCache instances?\n        self.elasticache_enabled = True\n        if config.has_option('ec2', 'elasticache'):\n            self.elasticache_enabled = config.getboolean('ec2', 'elasticache')\n\n        # Return all EC2 instances?\n        if config.has_option('ec2', 'all_instances'):\n            self.all_instances = config.getboolean('ec2', 'all_instances')\n        else:\n            self.all_instances = False\n\n        # Instance states to be gathered in inventory. Default is 'running'.\n        # Setting 'all_instances' to 'yes' overrides this option.\n        ec2_valid_instance_states = [\n            'pending',\n            'running',\n            'shutting-down',\n            'terminated',\n            'stopping',\n            'stopped'\n        ]\n        self.ec2_instance_states = []\n        if self.all_instances:\n            self.ec2_instance_states = ec2_valid_instance_states\n        elif config.has_option('ec2', 'instance_states'):\n            for instance_state in config.get('ec2',\n                                             'instance_states').split(','):\n                instance_state = instance_state.strip()\n                if instance_state not in ec2_valid_instance_states:\n                    continue\n                self.ec2_instance_states.append(instance_state)\n        else:\n            self.ec2_instance_states = ['running']\n\n        # Return all RDS instances? (if RDS is enabled)\n        if config.has_option('ec2', 'all_rds_instances') and self.rds_enabled:\n            self.all_rds_instances = config.getboolean('ec2',\n                                                       'all_rds_instances')\n        else:\n            self.all_rds_instances = False\n\n        # Return all ElastiCache replication groups?\n        # (if ElastiCache is enabled)\n        if (config.has_option('ec2', 'all_elasticache_replication_groups')\n                and self.elasticache_enabled):\n            self.all_elasticache_replication_groups = config.getboolean(\n                'ec2', 'all_elasticache_replication_groups')\n        else:\n            self.all_elasticache_replication_groups = False\n\n        # Return all ElastiCache clusters? (if ElastiCache is enabled)\n        if (config.has_option('ec2', 'all_elasticache_clusters')\n                and self.elasticache_enabled):\n            self.all_elasticache_clusters = config.getboolean(\n                'ec2', 'all_elasticache_clusters')\n        else:\n            self.all_elasticache_clusters = False\n\n        # Return all ElastiCache nodes? (if ElastiCache is enabled)\n        if config.has_option('ec2', 'all_elasticache_nodes') and self.elasticache_enabled:\n            self.all_elasticache_nodes = config.getboolean('ec2', 'all_elasticache_nodes')\n        else:\n            self.all_elasticache_nodes = False\n\n        # boto configuration profile (prefer CLI argument)\n        if config.has_option('ec2', 'boto_profile') and not self.boto_profile:\n            self.boto_profile = config.get('ec2', 'boto_profile')\n\n        # Cache related\n        cache_dir = os.path.expanduser(config.get('ec2', 'cache_path'))\n        if self.boto_profile:\n            cache_dir = os.path.join(cache_dir, 'profile_' + self.boto_profile)\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        self.cache_path_cache = cache_dir + \"/ansible-ec2.cache\"\n        self.cache_path_index = cache_dir + \"/ansible-ec2.index\"\n        self.cache_max_age = config.getint('ec2', 'cache_max_age')\n\n        if config.has_option('ec2', 'expand_csv_tags'):\n            self.expand_csv_tags = config.getboolean('ec2', 'expand_csv_tags')\n        else:\n            self.expand_csv_tags = False\n\n        # Configure nested groups instead of flat namespace.\n        if config.has_option('ec2', 'nested_groups'):\n            self.nested_groups = config.getboolean('ec2', 'nested_groups')\n        else:\n            self.nested_groups = False\n\n        # Replace dash or not in group names\n        if config.has_option('ec2', 'replace_dash_in_groups'):\n            self.replace_dash_in_groups = config.getboolean('ec2', 'replace_dash_in_groups')\n        else:\n            self.replace_dash_in_groups = True\n\n        # Configure which groups should be created.\n        group_by_options = [\n            'group_by_instance_id',\n            'group_by_region',\n            'group_by_availability_zone',\n            'group_by_ami_id',\n            'group_by_instance_type',\n            'group_by_key_pair',\n            'group_by_vpc_id',\n            'group_by_security_group',\n            'group_by_tag_keys',\n            'group_by_tag_none',\n            'group_by_route53_names',\n            'group_by_rds_engine',\n            'group_by_rds_parameter_group',\n            'group_by_elasticache_engine',\n            'group_by_elasticache_cluster',\n            'group_by_elasticache_parameter_group',\n            'group_by_elasticache_replication_group',\n        ]\n        for option in group_by_options:\n            if config.has_option('ec2', option):\n                setattr(self, option, config.getboolean('ec2', option))\n            else:\n                setattr(self, option, True)\n\n        # Do we need to just include hosts that match a pattern?\n        try:\n            pattern_include = config.get('ec2', 'pattern_include')\n            if pattern_include and len(pattern_include) > 0:\n                self.pattern_include = re.compile(pattern_include)\n            else:\n                self.pattern_include = None\n        except configparser.NoOptionError:\n            self.pattern_include = None\n\n        # Do we need to exclude hosts that match a pattern?\n        try:\n            pattern_exclude = config.get('ec2', 'pattern_exclude');\n            if pattern_exclude and len(pattern_exclude) > 0:\n                self.pattern_exclude = re.compile(pattern_exclude)\n            else:\n                self.pattern_exclude = None\n        except configparser.NoOptionError:\n            self.pattern_exclude = None\n\n        # Instance filters (see boto and EC2 API docs). Ignore invalid filters.\n        self.ec2_instance_filters = defaultdict(list)\n        if config.has_option('ec2', 'instance_filters'):\n            for instance_filter in config.get('ec2', 'instance_filters', '').split(','):\n                instance_filter = instance_filter.strip()\n                if not instance_filter or '=' not in instance_filter:\n                    continue\n                filter_key, filter_value = [x.strip() for x in instance_filter.split('=', 1)]\n                if not filter_key:\n                    continue\n                self.ec2_instance_filters[filter_key].append(filter_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_api_calls_update_cache(self):\n        ''' Do API calls to each region, and save data in cache files '''\n\n        if self.route53_enabled:\n            self.get_route53_records()\n\n        for region in self.regions:\n            self.get_instances_by_region(region)\n            if self.rds_enabled:\n                self.get_rds_instances_by_region(region)\n            if self.elasticache_enabled:\n                self.get_elasticache_clusters_by_region(region)\n                self.get_elasticache_replication_groups_by_region(region)\n\n        self.write_to_cache(self.inventory, self.cache_path_cache)\n        self.write_to_cache(self.index, self.cache_path_index)", "response": "Do API calls to each region and save data in cache files"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate connection to the API server", "response": "def connect(self, region):\n        ''' create connection to api server'''\n        if self.eucalyptus:\n            conn = boto.connect_euca(host=self.eucalyptus_host)\n            conn.APIVersion = '2010-08-31'\n        else:\n            conn = self.connect_to_aws(ec2, region)\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmonkey patch for boto issue boto / boto#2100", "response": "def boto_fix_security_token_in_profile(self, connect_args):\n        ''' monkey patch for boto issue boto/boto#2100 '''\n        profile = 'profile ' + self.boto_profile\n        if boto.config.has_option(profile, 'aws_security_token'):\n            connect_args['security_token'] = boto.config.get(profile, 'aws_security_token')\n        return connect_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking an AWS EC2 API call to the list of instances in a particular region", "response": "def get_instances_by_region(self, region):\n        ''' Makes an AWS EC2 API call to the list of instances in a particular\n        region '''\n\n        try:\n            conn = self.connect(region)\n            reservations = []\n            if self.ec2_instance_filters:\n                for filter_key, filter_values in self.ec2_instance_filters.items():\n                    reservations.extend(conn.get_all_instances(filters = { filter_key : filter_values }))\n            else:\n                reservations = conn.get_all_instances()\n\n            for reservation in reservations:\n                for instance in reservation.instances:\n                    self.add_instance(instance, region)\n\n        except boto.exception.BotoServerError as e:\n            if e.error_code == 'AuthFailure':\n                error = self.get_auth_error_message()\n            else:\n                backend = 'Eucalyptus' if self.eucalyptus else 'AWS' \n                error = \"Error connecting to %s backend.\\n%s\" % (backend, e.message)\n            self.fail_with_error(error, 'getting EC2 instances')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake an AWS API call to the list of RDS instances in a particular region", "response": "def get_rds_instances_by_region(self, region):\n        ''' Makes an AWS API call to the list of RDS instances in a particular\n        region '''\n\n        try:\n            conn = self.connect_to_aws(rds, region)\n            if conn:\n                instances = conn.get_all_dbinstances()\n                for instance in instances:\n                    self.add_rds_instance(instance, region)\n        except boto.exception.BotoServerError as e:\n            error = e.reason\n\n            if e.error_code == 'AuthFailure':\n                error = self.get_auth_error_message()\n            if not e.reason == \"Forbidden\":\n                error = \"Looks like AWS RDS is down:\\n%s\" % e.message\n            self.fail_with_error(error, 'getting RDS instances')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_elasticache_clusters_by_region(self, region):\n        ''' Makes an AWS API call to the list of ElastiCache clusters (with\n        nodes' info) in a particular region.'''\n\n        # ElastiCache boto module doesn't provide a get_all_intances method,\n        # that's why we need to call describe directly (it would be called by\n        # the shorthand method anyway...)\n        try:\n            conn = elasticache.connect_to_region(region)\n            if conn:\n                # show_cache_node_info = True\n                # because we also want nodes' information\n                response = conn.describe_cache_clusters(None, None, None, True)\n\n        except boto.exception.BotoServerError as e:\n            error = e.reason\n\n            if e.error_code == 'AuthFailure':\n                error = self.get_auth_error_message()\n            if not e.reason == \"Forbidden\":\n                error = \"Looks like AWS ElastiCache is down:\\n%s\" % e.message\n            self.fail_with_error(error, 'getting ElastiCache clusters')\n\n        try:\n            # Boto also doesn't provide wrapper classes to CacheClusters or\n            # CacheNodes. Because of that wo can't make use of the get_list\n            # method in the AWSQueryConnection. Let's do the work manually\n            clusters = response['DescribeCacheClustersResponse']['DescribeCacheClustersResult']['CacheClusters']\n\n        except KeyError as e:\n            error = \"ElastiCache query to AWS failed (unexpected format).\"\n            self.fail_with_error(error, 'getting ElastiCache clusters')\n\n        for cluster in clusters:\n            self.add_elasticache_cluster(cluster, region)", "response": "Makes an AWS API call to the list of ElastiCache clusters in a particular region."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_elasticache_replication_groups_by_region(self, region):\n        ''' Makes an AWS API call to the list of ElastiCache replication groups\n        in a particular region.'''\n\n        # ElastiCache boto module doesn't provide a get_all_intances method,\n        # that's why we need to call describe directly (it would be called by\n        # the shorthand method anyway...)\n        try:\n            conn = elasticache.connect_to_region(region)\n            if conn:\n                response = conn.describe_replication_groups()\n\n        except boto.exception.BotoServerError as e:\n            error = e.reason\n\n            if e.error_code == 'AuthFailure':\n                error = self.get_auth_error_message()\n            if not e.reason == \"Forbidden\":\n                error = \"Looks like AWS ElastiCache [Replication Groups] is down:\\n%s\" % e.message\n            self.fail_with_error(error, 'getting ElastiCache clusters')\n\n        try:\n            # Boto also doesn't provide wrapper classes to ReplicationGroups\n            # Because of that wo can't make use of the get_list method in the\n            # AWSQueryConnection. Let's do the work manually\n            replication_groups = response['DescribeReplicationGroupsResponse']['DescribeReplicationGroupsResult']['ReplicationGroups']\n\n        except KeyError as e:\n            error = \"ElastiCache [Replication Groups] query to AWS failed (unexpected format).\"\n            self.fail_with_error(error, 'getting ElastiCache clusters')\n\n        for replication_group in replication_groups:\n            self.add_elasticache_replication_group(replication_group, region)", "response": "Makes an AWS API call to the list of ElastiCache replication groups in a particular region."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_auth_error_message(self):\n        ''' create an informative error message if there is an issue authenticating'''\n        errors = [\"Authentication error retrieving ec2 inventory.\"]\n        if None in [os.environ.get('AWS_ACCESS_KEY_ID'), os.environ.get('AWS_SECRET_ACCESS_KEY')]:\n            errors.append(' - No AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY environment vars found')\n        else:\n            errors.append(' - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment vars found but may not be correct')\n\n        boto_paths = ['/etc/boto.cfg', '~/.boto', '~/.aws/credentials']\n        boto_config_found = list(p for p in boto_paths if os.path.isfile(os.path.expanduser(p)))\n        if len(boto_config_found) > 0:\n            errors.append(\" - Boto configs found at '%s', but the credentials contained may not be correct\" % ', '.join(boto_config_found))\n        else:\n            errors.append(\" - No Boto config found at any expected location '%s'\" % ', '.join(boto_paths))\n\n        return '\\n'.join(errors)", "response": "create an informative error message if there is an issue authenticating"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs an error to std err for ansible - playbook to consume and exit", "response": "def fail_with_error(self, err_msg, err_operation=None):\n        '''log an error to std err for ansible-playbook to consume and exit'''\n        if err_operation:\n            err_msg = 'ERROR: \"{err_msg}\", while: {err_operation}'.format(\n                err_msg=err_msg, err_operation=err_operation)\n        sys.stderr.write(err_msg)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_instance(self, instance, region):\n        ''' Adds an instance to the inventory and index, as long as it is\n        addressable '''\n\n        # Only return instances with desired instance states\n        if instance.state not in self.ec2_instance_states:\n            return\n\n        # Select the best destination address\n        if instance.subnet_id:\n            dest = getattr(instance, self.vpc_destination_variable, None)\n            if dest is None:\n                dest = getattr(instance, 'tags').get(self.vpc_destination_variable, None)\n        else:\n            dest = getattr(instance, self.destination_variable, None)\n            if dest is None:\n                dest = getattr(instance, 'tags').get(self.destination_variable, None)\n\n        if not dest:\n            # Skip instances we cannot address (e.g. private VPC subnet)\n            return\n\n        # if we only want to include hosts that match a pattern, skip those that don't\n        if self.pattern_include and not self.pattern_include.match(dest):\n            return\n\n        # if we need to exclude hosts that match a pattern, skip those\n        if self.pattern_exclude and self.pattern_exclude.match(dest):\n            return\n\n        # Add to index\n        self.index[dest] = [region, instance.id]\n\n        # Inventory: Group by instance ID (always a group of 1)\n        if self.group_by_instance_id:\n            self.inventory[instance.id] = [dest]\n            if self.nested_groups:\n                self.push_group(self.inventory, 'instances', instance.id)\n\n        # Inventory: Group by region\n        if self.group_by_region:\n            self.push(self.inventory, region, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'regions', region)\n\n        # Inventory: Group by availability zone\n        if self.group_by_availability_zone:\n            self.push(self.inventory, instance.placement, dest)\n            if self.nested_groups:\n                if self.group_by_region:\n                    self.push_group(self.inventory, region, instance.placement)\n                self.push_group(self.inventory, 'zones', instance.placement)\n\n        # Inventory: Group by Amazon Machine Image (AMI) ID\n        if self.group_by_ami_id:\n            ami_id = self.to_safe(instance.image_id)\n            self.push(self.inventory, ami_id, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'images', ami_id)\n\n        # Inventory: Group by instance type\n        if self.group_by_instance_type:\n            type_name = self.to_safe('type_' + instance.instance_type)\n            self.push(self.inventory, type_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'types', type_name)\n\n        # Inventory: Group by key pair\n        if self.group_by_key_pair and instance.key_name:\n            key_name = self.to_safe('key_' + instance.key_name)\n            self.push(self.inventory, key_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'keys', key_name)\n\n        # Inventory: Group by VPC\n        if self.group_by_vpc_id and instance.vpc_id:\n            vpc_id_name = self.to_safe('vpc_id_' + instance.vpc_id)\n            self.push(self.inventory, vpc_id_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'vpcs', vpc_id_name)\n\n        # Inventory: Group by security group\n        if self.group_by_security_group:\n            try:\n                for group in instance.groups:\n                    key = self.to_safe(\"security_group_\" + group.name)\n                    self.push(self.inventory, key, dest)\n                    if self.nested_groups:\n                        self.push_group(self.inventory, 'security_groups', key)\n            except AttributeError:\n                self.fail_with_error('\\n'.join(['Package boto seems a bit older.', \n                                            'Please upgrade boto >= 2.3.0.']))\n\n        # Inventory: Group by tag keys\n        if self.group_by_tag_keys:\n            for k, v in instance.tags.items():\n                if self.expand_csv_tags and v and ',' in v:\n                    values = map(lambda x: x.strip(), v.split(','))\n                else:\n                    values = [v]\n\n                for v in values:\n                    if v:\n                        key = self.to_safe(\"tag_\" + k + \"=\" + v)\n                    else:\n                        key = self.to_safe(\"tag_\" + k)\n                    self.push(self.inventory, key, dest)\n                    if self.nested_groups:\n                        self.push_group(self.inventory, 'tags', self.to_safe(\"tag_\" + k))\n                        if v:\n                            self.push_group(self.inventory, self.to_safe(\"tag_\" + k), key)\n\n        # Inventory: Group by Route53 domain names if enabled\n        if self.route53_enabled and self.group_by_route53_names:\n            route53_names = self.get_instance_route53_names(instance)\n            for name in route53_names:\n                self.push(self.inventory, name, dest)\n                if self.nested_groups:\n                    self.push_group(self.inventory, 'route53', name)\n\n        # Global Tag: instances without tags\n        if self.group_by_tag_none and len(instance.tags) == 0:\n            self.push(self.inventory, 'tag_none', dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'tags', 'tag_none')\n\n        # Global Tag: tag all EC2 instances\n        self.push(self.inventory, 'ec2', dest)\n\n        self.inventory[\"_meta\"][\"hostvars\"][dest] = self.get_host_info_dict_from_instance(instance)", "response": "Adds an instance to the inventory and index as long as it is addressable"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an RDS instance to the inventory and index as long as it is addressable", "response": "def add_rds_instance(self, instance, region):\n        ''' Adds an RDS instance to the inventory and index, as long as it is\n        addressable '''\n\n        # Only want available instances unless all_rds_instances is True\n        if not self.all_rds_instances and instance.status != 'available':\n            return\n\n        # Select the best destination address\n        dest = instance.endpoint[0]\n\n        if not dest:\n            # Skip instances we cannot address (e.g. private VPC subnet)\n            return\n\n        # Add to index\n        self.index[dest] = [region, instance.id]\n\n        # Inventory: Group by instance ID (always a group of 1)\n        if self.group_by_instance_id:\n            self.inventory[instance.id] = [dest]\n            if self.nested_groups:\n                self.push_group(self.inventory, 'instances', instance.id)\n\n        # Inventory: Group by region\n        if self.group_by_region:\n            self.push(self.inventory, region, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'regions', region)\n\n        # Inventory: Group by availability zone\n        if self.group_by_availability_zone:\n            self.push(self.inventory, instance.availability_zone, dest)\n            if self.nested_groups:\n                if self.group_by_region:\n                    self.push_group(self.inventory, region, instance.availability_zone)\n                self.push_group(self.inventory, 'zones', instance.availability_zone)\n\n        # Inventory: Group by instance type\n        if self.group_by_instance_type:\n            type_name = self.to_safe('type_' + instance.instance_class)\n            self.push(self.inventory, type_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'types', type_name)\n\n        # Inventory: Group by VPC\n        if self.group_by_vpc_id and instance.subnet_group and instance.subnet_group.vpc_id:\n            vpc_id_name = self.to_safe('vpc_id_' + instance.subnet_group.vpc_id)\n            self.push(self.inventory, vpc_id_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'vpcs', vpc_id_name)\n\n        # Inventory: Group by security group\n        if self.group_by_security_group:\n            try:\n                if instance.security_group:\n                    key = self.to_safe(\"security_group_\" + instance.security_group.name)\n                    self.push(self.inventory, key, dest)\n                    if self.nested_groups:\n                        self.push_group(self.inventory, 'security_groups', key)\n\n            except AttributeError:\n                self.fail_with_error('\\n'.join(['Package boto seems a bit older.', \n                                            'Please upgrade boto >= 2.3.0.']))\n\n\n        # Inventory: Group by engine\n        if self.group_by_rds_engine:\n            self.push(self.inventory, self.to_safe(\"rds_\" + instance.engine), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'rds_engines', self.to_safe(\"rds_\" + instance.engine))\n\n        # Inventory: Group by parameter group\n        if self.group_by_rds_parameter_group:\n            self.push(self.inventory, self.to_safe(\"rds_parameter_group_\" + instance.parameter_group.name), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'rds_parameter_groups', self.to_safe(\"rds_parameter_group_\" + instance.parameter_group.name))\n\n        # Global Tag: all RDS instances\n        self.push(self.inventory, 'rds', dest)\n\n        self.inventory[\"_meta\"][\"hostvars\"][dest] = self.get_host_info_dict_from_instance(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_elasticache_cluster(self, cluster, region):\n        ''' Adds an ElastiCache cluster to the inventory and index, as long as\n        it's nodes are addressable '''\n\n        # Only want available clusters unless all_elasticache_clusters is True\n        if not self.all_elasticache_clusters and cluster['CacheClusterStatus'] != 'available':\n            return\n\n        # Select the best destination address\n        if 'ConfigurationEndpoint' in cluster and cluster['ConfigurationEndpoint']:\n            # Memcached cluster\n            dest = cluster['ConfigurationEndpoint']['Address']\n            is_redis = False\n        else:\n            # Redis sigle node cluster\n            # Because all Redis clusters are single nodes, we'll merge the\n            # info from the cluster with info about the node\n            dest = cluster['CacheNodes'][0]['Endpoint']['Address']\n            is_redis = True\n\n        if not dest:\n            # Skip clusters we cannot address (e.g. private VPC subnet)\n            return\n\n        # Add to index\n        self.index[dest] = [region, cluster['CacheClusterId']]\n\n        # Inventory: Group by instance ID (always a group of 1)\n        if self.group_by_instance_id:\n            self.inventory[cluster['CacheClusterId']] = [dest]\n            if self.nested_groups:\n                self.push_group(self.inventory, 'instances', cluster['CacheClusterId'])\n\n        # Inventory: Group by region\n        if self.group_by_region and not is_redis:\n            self.push(self.inventory, region, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'regions', region)\n\n        # Inventory: Group by availability zone\n        if self.group_by_availability_zone and not is_redis:\n            self.push(self.inventory, cluster['PreferredAvailabilityZone'], dest)\n            if self.nested_groups:\n                if self.group_by_region:\n                    self.push_group(self.inventory, region, cluster['PreferredAvailabilityZone'])\n                self.push_group(self.inventory, 'zones', cluster['PreferredAvailabilityZone'])\n\n        # Inventory: Group by node type\n        if self.group_by_instance_type and not is_redis:\n            type_name = self.to_safe('type_' + cluster['CacheNodeType'])\n            self.push(self.inventory, type_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'types', type_name)\n\n        # Inventory: Group by VPC (information not available in the current\n        # AWS API version for ElastiCache)\n\n        # Inventory: Group by security group\n        if self.group_by_security_group and not is_redis:\n\n            # Check for the existence of the 'SecurityGroups' key and also if\n            # this key has some value. When the cluster is not placed in a SG\n            # the query can return None here and cause an error.\n            if 'SecurityGroups' in cluster and cluster['SecurityGroups'] is not None:\n                for security_group in cluster['SecurityGroups']:\n                    key = self.to_safe(\"security_group_\" + security_group['SecurityGroupId'])\n                    self.push(self.inventory, key, dest)\n                    if self.nested_groups:\n                        self.push_group(self.inventory, 'security_groups', key)\n\n        # Inventory: Group by engine\n        if self.group_by_elasticache_engine and not is_redis:\n            self.push(self.inventory, self.to_safe(\"elasticache_\" + cluster['Engine']), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'elasticache_engines', self.to_safe(cluster['Engine']))\n\n        # Inventory: Group by parameter group\n        if self.group_by_elasticache_parameter_group:\n            self.push(self.inventory, self.to_safe(\"elasticache_parameter_group_\" + cluster['CacheParameterGroup']['CacheParameterGroupName']), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'elasticache_parameter_groups', self.to_safe(cluster['CacheParameterGroup']['CacheParameterGroupName']))\n\n        # Inventory: Group by replication group\n        if self.group_by_elasticache_replication_group and 'ReplicationGroupId' in cluster and cluster['ReplicationGroupId']:\n            self.push(self.inventory, self.to_safe(\"elasticache_replication_group_\" + cluster['ReplicationGroupId']), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'elasticache_replication_groups', self.to_safe(cluster['ReplicationGroupId']))\n\n        # Global Tag: all ElastiCache clusters\n        self.push(self.inventory, 'elasticache_clusters', cluster['CacheClusterId'])\n\n        host_info = self.get_host_info_dict_from_describe_dict(cluster)\n\n        self.inventory[\"_meta\"][\"hostvars\"][dest] = host_info\n\n        # Add the nodes\n        for node in cluster['CacheNodes']:\n            self.add_elasticache_node(node, cluster, region)", "response": "Adds an ElastiCache cluster to the inventory and index as long as the cluster s nodes are addressable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_elasticache_node(self, node, cluster, region):\n        ''' Adds an ElastiCache node to the inventory and index, as long as\n        it is addressable '''\n\n        # Only want available nodes unless all_elasticache_nodes is True\n        if not self.all_elasticache_nodes and node['CacheNodeStatus'] != 'available':\n            return\n\n        # Select the best destination address\n        dest = node['Endpoint']['Address']\n\n        if not dest:\n            # Skip nodes we cannot address (e.g. private VPC subnet)\n            return\n\n        node_id = self.to_safe(cluster['CacheClusterId'] + '_' + node['CacheNodeId'])\n\n        # Add to index\n        self.index[dest] = [region, node_id]\n\n        # Inventory: Group by node ID (always a group of 1)\n        if self.group_by_instance_id:\n            self.inventory[node_id] = [dest]\n            if self.nested_groups:\n                self.push_group(self.inventory, 'instances', node_id)\n\n        # Inventory: Group by region\n        if self.group_by_region:\n            self.push(self.inventory, region, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'regions', region)\n\n        # Inventory: Group by availability zone\n        if self.group_by_availability_zone:\n            self.push(self.inventory, cluster['PreferredAvailabilityZone'], dest)\n            if self.nested_groups:\n                if self.group_by_region:\n                    self.push_group(self.inventory, region, cluster['PreferredAvailabilityZone'])\n                self.push_group(self.inventory, 'zones', cluster['PreferredAvailabilityZone'])\n\n        # Inventory: Group by node type\n        if self.group_by_instance_type:\n            type_name = self.to_safe('type_' + cluster['CacheNodeType'])\n            self.push(self.inventory, type_name, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'types', type_name)\n\n        # Inventory: Group by VPC (information not available in the current\n        # AWS API version for ElastiCache)\n\n        # Inventory: Group by security group\n        if self.group_by_security_group:\n\n            # Check for the existence of the 'SecurityGroups' key and also if\n            # this key has some value. When the cluster is not placed in a SG\n            # the query can return None here and cause an error.\n            if 'SecurityGroups' in cluster and cluster['SecurityGroups'] is not None:\n                for security_group in cluster['SecurityGroups']:\n                    key = self.to_safe(\"security_group_\" + security_group['SecurityGroupId'])\n                    self.push(self.inventory, key, dest)\n                    if self.nested_groups:\n                        self.push_group(self.inventory, 'security_groups', key)\n\n        # Inventory: Group by engine\n        if self.group_by_elasticache_engine:\n            self.push(self.inventory, self.to_safe(\"elasticache_\" + cluster['Engine']), dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'elasticache_engines', self.to_safe(\"elasticache_\" + cluster['Engine']))\n\n        # Inventory: Group by parameter group (done at cluster level)\n\n        # Inventory: Group by replication group (done at cluster level)\n\n        # Inventory: Group by ElastiCache Cluster\n        if self.group_by_elasticache_cluster:\n            self.push(self.inventory, self.to_safe(\"elasticache_cluster_\" + cluster['CacheClusterId']), dest)\n\n        # Global Tag: all ElastiCache nodes\n        self.push(self.inventory, 'elasticache_nodes', dest)\n\n        host_info = self.get_host_info_dict_from_describe_dict(node)\n\n        if dest in self.inventory[\"_meta\"][\"hostvars\"]:\n            self.inventory[\"_meta\"][\"hostvars\"][dest].update(host_info)\n        else:\n            self.inventory[\"_meta\"][\"hostvars\"][dest] = host_info", "response": "Adds an ElastiCache node to the inventory and index as long as\n        it is addressable"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an ElastiCache replication group to the inventory and index.", "response": "def add_elasticache_replication_group(self, replication_group, region):\n        ''' Adds an ElastiCache replication group to the inventory and index '''\n\n        # Only want available clusters unless all_elasticache_replication_groups is True\n        if not self.all_elasticache_replication_groups and replication_group['Status'] != 'available':\n            return\n\n        # Select the best destination address (PrimaryEndpoint)\n        dest = replication_group['NodeGroups'][0]['PrimaryEndpoint']['Address']\n\n        if not dest:\n            # Skip clusters we cannot address (e.g. private VPC subnet)\n            return\n\n        # Add to index\n        self.index[dest] = [region, replication_group['ReplicationGroupId']]\n\n        # Inventory: Group by ID (always a group of 1)\n        if self.group_by_instance_id:\n            self.inventory[replication_group['ReplicationGroupId']] = [dest]\n            if self.nested_groups:\n                self.push_group(self.inventory, 'instances', replication_group['ReplicationGroupId'])\n\n        # Inventory: Group by region\n        if self.group_by_region:\n            self.push(self.inventory, region, dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'regions', region)\n\n        # Inventory: Group by availability zone (doesn't apply to replication groups)\n\n        # Inventory: Group by node type (doesn't apply to replication groups)\n\n        # Inventory: Group by VPC (information not available in the current\n        # AWS API version for replication groups\n\n        # Inventory: Group by security group (doesn't apply to replication groups)\n        # Check this value in cluster level\n\n        # Inventory: Group by engine (replication groups are always Redis)\n        if self.group_by_elasticache_engine:\n            self.push(self.inventory, 'elasticache_redis', dest)\n            if self.nested_groups:\n                self.push_group(self.inventory, 'elasticache_engines', 'redis')\n\n        # Global Tag: all ElastiCache clusters\n        self.push(self.inventory, 'elasticache_replication_groups', replication_group['ReplicationGroupId'])\n\n        host_info = self.get_host_info_dict_from_describe_dict(replication_group)\n\n        self.inventory[\"_meta\"][\"hostvars\"][dest] = host_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets and store the map of resource records to domain names that they point to.", "response": "def get_route53_records(self):\n        ''' Get and store the map of resource records to domain names that\n        point to them. '''\n\n        r53_conn = route53.Route53Connection()\n        all_zones = r53_conn.get_zones()\n\n        route53_zones = [ zone for zone in all_zones if zone.name[:-1]\n                          not in self.route53_excluded_zones ]\n\n        self.route53_records = {}\n\n        for zone in route53_zones:\n            rrsets = r53_conn.get_all_rrsets(zone.id)\n\n            for record_set in rrsets:\n                record_name = record_set.name\n\n                if record_name.endswith('.'):\n                    record_name = record_name[:-1]\n\n                for resource in record_set.resource_records:\n                    self.route53_records.setdefault(resource, set())\n                    self.route53_records[resource].add(record_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_instance_route53_names(self, instance):\n        ''' Check if an instance is referenced in the records we have from\n        Route53. If it is, return the list of domain names pointing to said\n        instance. If nothing points to it, return an empty list. '''\n\n        instance_attributes = [ 'public_dns_name', 'private_dns_name',\n                                'ip_address', 'private_ip_address' ]\n\n        name_list = set()\n\n        for attrib in instance_attributes:\n            try:\n                value = getattr(instance, attrib)\n            except AttributeError:\n                continue\n\n            if value in self.route53_records:\n                name_list.update(self.route53_records[value])\n\n        return list(name_list)", "response": "Check if an instance is referenced in the records we have from\n        Route53. If it is return the list of domain names pointing to said\n        instance. If it is not return an empty list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_host_info_dict_from_describe_dict(self, describe_dict):\n        ''' Parses the dictionary returned by the API call into a flat list\n            of parameters. This method should be used only when 'describe' is\n            used directly because Boto doesn't provide specific classes. '''\n\n        # I really don't agree with prefixing everything with 'ec2'\n        # because EC2, RDS and ElastiCache are different services.\n        # I'm just following the pattern used until now to not break any\n        # compatibility.\n\n        host_info = {}\n        for key in describe_dict:\n            value = describe_dict[key]\n            key = self.to_safe('ec2_' + self.uncammelize(key))\n\n            # Handle complex types\n\n            # Target: Memcached Cache Clusters\n            if key == 'ec2_configuration_endpoint' and value:\n                host_info['ec2_configuration_endpoint_address'] = value['Address']\n                host_info['ec2_configuration_endpoint_port'] = value['Port']\n\n            # Target: Cache Nodes and Redis Cache Clusters (single node)\n            if key == 'ec2_endpoint' and value:\n                host_info['ec2_endpoint_address'] = value['Address']\n                host_info['ec2_endpoint_port'] = value['Port']\n\n            # Target: Redis Replication Groups\n            if key == 'ec2_node_groups' and value:\n                host_info['ec2_endpoint_address'] = value[0]['PrimaryEndpoint']['Address']\n                host_info['ec2_endpoint_port'] = value[0]['PrimaryEndpoint']['Port']\n                replica_count = 0\n                for node in value[0]['NodeGroupMembers']:\n                    if node['CurrentRole'] == 'primary':\n                        host_info['ec2_primary_cluster_address'] = node['ReadEndpoint']['Address']\n                        host_info['ec2_primary_cluster_port'] = node['ReadEndpoint']['Port']\n                        host_info['ec2_primary_cluster_id'] = node['CacheClusterId']\n                    elif node['CurrentRole'] == 'replica':\n                        host_info['ec2_replica_cluster_address_'+ str(replica_count)] = node['ReadEndpoint']['Address']\n                        host_info['ec2_replica_cluster_port_'+ str(replica_count)] = node['ReadEndpoint']['Port']\n                        host_info['ec2_replica_cluster_id_'+ str(replica_count)] = node['CacheClusterId']\n                        replica_count += 1\n\n            # Target: Redis Replication Groups\n            if key == 'ec2_member_clusters' and value:\n                host_info['ec2_member_clusters'] = ','.join([str(i) for i in value])\n\n            # Target: All Cache Clusters\n            elif key == 'ec2_cache_parameter_group':\n                host_info[\"ec2_cache_node_ids_to_reboot\"] = ','.join([str(i) for i in value['CacheNodeIdsToReboot']])\n                host_info['ec2_cache_parameter_group_name'] = value['CacheParameterGroupName']\n                host_info['ec2_cache_parameter_apply_status'] = value['ParameterApplyStatus']\n\n            # Target: Almost everything\n            elif key == 'ec2_security_groups':\n\n                # Skip if SecurityGroups is None\n                # (it is possible to have the key defined but no value in it).\n                if value is not None:\n                    sg_ids = []\n                    for sg in value:\n                        sg_ids.append(sg['SecurityGroupId'])\n                    host_info[\"ec2_security_group_ids\"] = ','.join([str(i) for i in sg_ids])\n\n            # Target: Everything\n            # Preserve booleans and integers\n            elif type(value) in [int, bool]:\n                host_info[key] = value\n\n            # Target: Everything\n            # Sanitize string values\n            elif isinstance(value, six.string_types):\n                host_info[key] = value.strip()\n\n            # Target: Everything\n            # Replace None by an empty string\n            elif type(value) == type(None):\n                host_info[key] = ''\n\n            else:\n                # Remove non-processed complex types\n                pass\n\n        return host_info", "response": "Parses the dictionary returned by the API call into a flat list of hosts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_host(self, host):\n        ''' Get variables about a specific host '''\n\n        if len(self.index) == 0:\n            # Need to load index from cache\n            self.load_index_from_cache()\n\n        if not host in self.index:\n            # try updating the cache\n            self.do_api_calls_update_cache()\n            if not host in self.index:\n                # host might not exist anymore\n                return {}\n\n        (region, instance_id) = self.index[host]\n\n        instance = self.get_instance(region, instance_id)\n        return self.get_host_info_dict_from_instance(instance)", "response": "Get variables about a specific host"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push(self, my_dict, key, element):\n        ''' Push an element onto an array that may not have been defined in\n        the dict '''\n        group_info = my_dict.setdefault(key, [])\n        if isinstance(group_info, dict):\n            host_list = group_info.setdefault('hosts', [])\n            host_list.append(element)\n        else:\n            group_info.append(element)", "response": "Push an element onto an array that may not have been defined in\n        the dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes a group as a child of another group.", "response": "def push_group(self, my_dict, key, element):\n        ''' Push a group as a child of another group. '''\n        parent_group = my_dict.setdefault(key, {})\n        if not isinstance(parent_group, dict):\n            parent_group = my_dict[key] = {'hosts': parent_group}\n        child_groups = parent_group.setdefault('children', [])\n        if element not in child_groups:\n            child_groups.append(element)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the inventory from the cache file and returns it as a JSON object", "response": "def load_inventory_from_cache(self):\n        ''' Reads the inventory from the cache file and returns it as a JSON\n        object '''\n\n        cache = open(self.cache_path_cache, 'r')\n        json_inventory = cache.read()\n        self.inventory = json.loads(json_inventory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the index from the cache file sets self. index", "response": "def load_index_from_cache(self):\n        ''' Reads the index from the cache file sets self.index '''\n\n        cache = open(self.cache_path_index, 'r')\n        json_index = cache.read()\n        self.index = json.loads(json_index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_to_cache(self, data, filename):\n        ''' Writes data in JSON format to a file '''\n\n        json_data = json.dumps(data, sort_keys=True, indent=2)\n        cache = open(filename, 'w')\n        cache.write(json_data)\n        cache.close()", "response": "Writes data in JSON format to a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_safe(self, word):\n        ''' Converts 'bad' characters in a string to underscores so they can be used as Ansible groups '''\n        regex = \"[^A-Za-z0-9\\_\"\n        if not self.replace_dash_in_groups:\n            regex += \"\\-\"\n        return re.sub(regex + \"]\", \"_\", word)", "response": "Converts bad characters in a string to underscores so they can be used as Ansible groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_keyword_list(self, keyword_list, strictness=2, timeout=3):\n        keyword_tags = nltk.pos_tag(keyword_list)\n\n        start = time()\n        while time() - start < timeout:\n            index = 0\n            output_list = []\n            tagged_sent = self.random_sentences.get_tagged_sent()\n            for word, tag in tagged_sent:\n                if index >= len(keyword_tags):\n                    return self.get_overlap(keyword_list, output_list, is_word_list=True)\n\n                if self.match_pos(tag, keyword_tags[index][1], strictness=strictness):\n                    output_list.append(keyword_tags[index][0])\n                    index += 1\n                else:\n                    output_list.append(word)\n\n        return []", "response": "Convert a list of keywords to sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eval(self, script, keys=None, args=None):\n        if not keys:\n            keys = []\n        if not args:\n            args = []\n        return self._execute([b'EVAL', script, str(len(keys))] + keys + args)", "response": "Execute a Lua script and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evalsha(self, sha1, keys=None, args=None):\n        if not keys:\n            keys = []\n        if not args:\n            args = []\n        return self._execute([b'EVALSHA', sha1, str(len(keys))] + keys + args)", "response": "Evaluates a script cached on the server side by its SHA1 digest."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders path by filling the path template with video information.", "response": "def render_path(self) -> str:\n        \"\"\"Render path by filling the path template with video information.\"\"\"\n        # TODO: Fix defaults when date is not found (empty string or None)\n        # https://stackoverflow.com/questions/23407295/default-kwarg-values-for-pythons-str-format-method\n\n        from string import Formatter\n\n        class UnseenFormatter(Formatter):\n            def get_value(self, key, args, kwds):\n                if isinstance(key, str):\n                    try:\n                        return kwds[key]\n                    except KeyError:\n                        return key\n                else:\n                    return super().get_value(key, args, kwds)\n\n        data = self.video.data\n        site_name = data['site']\n\n        try:\n            template = self.templates[site_name]\n        except KeyError:\n            raise NoTemplateFoundError\n\n        fmt = UnseenFormatter()\n        filename_raw = fmt.format(template, **data)\n        filename = clean_filename(filename_raw)\n        path = os.path.join(self.download_dir, filename)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_lm_response(self, flags, challenge):\n        # If lm compatibility level lower than 3, but the server negotiated NTLM2, generate an\n        # NTLM2 response in preference to the weaker LMv1\n        if flags & NegotiateFlag.NTLMSSP_NTLM2_KEY and self._lm_compatibility < 3:\n            response, key = self._client_challenge + b'\\0' * 16, None\n\n        elif 0 <= self._lm_compatibility <= 1:\n            response, key = PasswordAuthentication._get_lm_response(self._password, challenge)\n        elif self._lm_compatibility == 2:\n            response, key = PasswordAuthentication.get_ntlmv1_response(self._password, challenge)\n        else:\n            response, key = PasswordAuthentication.get_lmv2_response(self._domain, self._username, self._password,\n                                                                     challenge, self._client_challenge)\n        return response, key", "response": "Computes the LMHash password hash given the 8 byte server challenge."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ntlm_response(self, flags, challenge, target_info=None, channel_binding=None):\n        # TODO: IMPLEMENT THE FOLLOWING FEATURES\n        # If NTLM v2 authentication is used and the CHALLENGE_MESSAGE does not contain both MsvAvNbComputerName and\n        # MsvAvNbDomainName AVPairs and either Integrity is TRUE or Confidentiality is TRUE, then return STATUS_LOGON_FAILURE.\n\n        # If NTLM v2 authentication is used and the CHALLENGE_MESSAGE contains a TargetInfo field, the client SHOULD NOT send\n        # the LmChallengeResponse and SHOULD set the LmChallengeResponseLen and LmChallengeResponseMaxLen fields in the\n        # AUTHENTICATE_MESSAGE to zero.\n\n        # If lm compatibility level is 3 or lower, but the server negotiated NTLM2, generate an\n        # NTLM2 response in preference to the weaker NTLMv1.\n        if flags & NegotiateFlag.NTLMSSP_NTLM2_KEY and self._lm_compatibility < 3:\n            response, key = PasswordAuthentication.get_ntlm2_response(self._password, challenge, self._client_challenge)\n\n        elif 0 <= self._lm_compatibility < 3:\n            response, key = PasswordAuthentication.get_ntlmv1_response(self._password, challenge)\n        else:\n            # We should use the timestamp included in TargetInfo, if no timestamp is set we generate one and add it to\n            # the outgoing TargetInfo. If the timestamp is set, we should also set the MIC flag\n            if target_info is None:\n                target_info = TargetInfo()\n            if target_info[TargetInfo.NTLMSSP_AV_TIME] is None:\n                timestamp = PasswordAuthentication._get_ntlm_timestamp()\n            else:\n                # TODO: If the CHALLENGE_MESSAGE TargetInfo field (section 2.2.1.2) has an MsvAvTimestamp present,\n                # TODO: the client SHOULD provide a MIC.\n                timestamp = target_info[TargetInfo.NTLMSSP_AV_TIME][1]\n\n            #target_info[TargetInfo.NTLMSSP_AV_FLAGS] = struct.pack('<I', 2)\n            # Calculating channel bindings is poorly documented. It is implemented in winrmlib, and needs to be\n            # moved here\n            # if self._av_channel_bindings is True and channel_binding is not None:\n            #     target_info[TargetInfo.NTLMSSP_AV_CHANNEL_BINDINGS] = channel_binding\n\n            response, key, target_info = PasswordAuthentication.get_ntlmv2_response(\n                self._domain, self._username, self._password.encode('utf-16le'), challenge,\n                self._client_challenge, timestamp, target_info)\n\n        return response, key, target_info", "response": "Computes the NTLM challenge response given the 8 byte server challenge and the NTLM session key and the NTLM server key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand_des_key(key):\n        key = key[:7] + b'\\0' * (7 - len(key))\n        byte = struct.unpack_from('BBBBBBB', key)\n        s  = struct.pack('B', ((byte[0] >> 1) & 0x7f) << 1)\n        s += struct.pack(\"B\", ((byte[0] & 0x01) << 6 | ((byte[1] >> 2) & 0x3f)) << 1)\n        s += struct.pack(\"B\", ((byte[1] & 0x03) << 5 | ((byte[2] >> 3) & 0x1f)) << 1)\n        s += struct.pack(\"B\", ((byte[2] & 0x07) << 4 | ((byte[3] >> 4) & 0x0f)) << 1)\n        s += struct.pack(\"B\", ((byte[3] & 0x0f) << 3 | ((byte[4] >> 5) & 0x07)) << 1)\n        s += struct.pack(\"B\", ((byte[4] & 0x1f) << 2 | ((byte[5] >> 6) & 0x03)) << 1)\n        s += struct.pack(\"B\", ((byte[5] & 0x3f) << 1 | ((byte[6] >> 7) & 0x01)) << 1)\n        s += struct.pack(\"B\", (byte[6] & 0x7f) << 1)\n        return s", "response": "Expand the key from a 7 - byte password key into a 8 - byte DES key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the Unicode MD4 hash for the password associated with these credentials and return the response and the session key.", "response": "def get_ntlmv1_response(password, challenge):\n        \"\"\"\n        Generate the Unicode MD4 hash for the password associated with these credentials.\n        \"\"\"\n        ntlm_hash = PasswordAuthentication.ntowfv1(password.encode('utf-16le'))\n        response  = PasswordAuthentication._encrypt_des_block(ntlm_hash[:7], challenge)\n        response += PasswordAuthentication._encrypt_des_block(ntlm_hash[7:14], challenge)\n        response += PasswordAuthentication._encrypt_des_block(ntlm_hash[14:], challenge)\n\n        # The NTLMv1 session key is simply the MD4 hash of the ntlm hash\n        session_hash = hashlib.new('md4')\n        session_hash.update(ntlm_hash)\n        return response, session_hash.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ntlm2_response(password, server_challenge, client_challenge):\n        md5 = hashlib.new('md5')\n        md5.update(server_challenge + client_challenge)\n        ntlm2_session_hash = md5.digest()[:8]\n        ntlm_hash = PasswordAuthentication.ntowfv1(password.encode('utf-16le'))\n        response  = PasswordAuthentication._encrypt_des_block(ntlm_hash[:7], ntlm2_session_hash)\n        response += PasswordAuthentication._encrypt_des_block(ntlm_hash[7:14], ntlm2_session_hash)\n        response += PasswordAuthentication._encrypt_des_block(ntlm_hash[14:], ntlm2_session_hash)\n\n        session_hash = hashlib.new('md4')\n        session_hash.update(ntlm_hash)\n        hmac_context = hmac.HMAC(session_hash.digest(), hashes.MD5(), backend=default_backend())\n        hmac_context.update(server_challenge + client_challenge)\n        return response, hmac_context.finalize()", "response": "Generate the NTLM 2 response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ntowfv2(domain, user, password):\n        md4 = hashlib.new('md4')\n        md4.update(password)\n        hmac_context = hmac.HMAC(md4.digest(), hashes.MD5(), backend=default_backend())\n        hmac_context.update(user.upper().encode('utf-16le'))\n        hmac_context.update(domain.encode('utf-16le'))\n        return hmac_context.finalize()", "response": "Implementation of NTOWFv2 Authentication Protocols NTOWFv2 NTLM v2 Authentication"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the response key for the NTLMv2 authentication.", "response": "def _compute_response(response_key, server_challenge, client_challenge):\n        \"\"\"\n        ComputeResponse() has been refactored slightly to reduce its complexity and improve\n        readability, the 'if' clause which switches between LMv2 and NTLMv2 computation has been\n        removed. Users should not call this method directly, they should rely on get_lmv2_response\n        and get_ntlmv2_response depending on the negotiated flags.\n\n        [MS-NLMP] v20140502 NT LAN Manager (NTLM) Authentication Protocol\n        3.3.2 NTLM v2 Authentication\n        \"\"\"\n        hmac_context = hmac.HMAC(response_key, hashes.MD5(), backend=default_backend())\n        hmac_context.update(server_challenge)\n        hmac_context.update(client_challenge)\n        return hmac_context.finalize()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lmv2_response(domain, username, password, server_challenge, client_challenge):\n        ntlmv2_hash = PasswordAuthentication.ntowfv2(domain, username, password.encode('utf-16le'))\n        hmac_context = hmac.HMAC(ntlmv2_hash, hashes.MD5(), backend=default_backend())\n        hmac_context.update(server_challenge)\n        hmac_context.update(client_challenge)\n        lmv2_hash = hmac_context.finalize()\n\n        # The LMv2 master user session key is a HMAC MD5 of the NTLMv2 and LMv2 hash\n        session_key = hmac.HMAC(ntlmv2_hash, hashes.MD5(), backend=default_backend())\n        session_key.update(lmv2_hash)\n\n        return lmv2_hash + client_challenge, session_key.finalize()", "response": "Computes an appropriate LMv2 response based on the supplied arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ntlmv2_response(domain, user, password, server_challenge, client_challenge, timestamp, target_info):\n        lo_response_version = b'\\x01'\n        hi_response_version = b'\\x01'\n        reserved_dword = b'\\x00' * 4\n        reserved_bytes = b'\\x00' * 6\n\n        response_key = PasswordAuthentication.ntowfv2(domain, user, password)\n        proof_material = lo_response_version\n        proof_material += hi_response_version\n        proof_material += reserved_bytes\n        proof_material += timestamp\n        proof_material += client_challenge\n        proof_material += reserved_dword\n        proof_material += target_info.get_data()\n        proof_material += reserved_dword\n        proof = PasswordAuthentication._compute_response(response_key, server_challenge, proof_material)\n\n        # The master session key derivation\n        session_key = hmac.HMAC(response_key, hashes.MD5(), backend=default_backend())\n        session_key.update(proof)\n        session_master_key = session_key.finalize()\n        return proof + proof_material, session_master_key, target_info", "response": "This function is used to compute the NTLMv2 response from the NTLMv2 protocol."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xml(self, value):\n\n        self._xml = value\n        self._root = s2t(value)", "response": "Set new XML string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef root(self, value):\n\n        self._xml = t2s(value)\n        self._root = value", "response": "Set new XML tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind all XSL styled runs normalize related paragraph and returns list of XslElements", "response": "def xsl_elements(self):\n        \"\"\"Find all \"XSL\" styled runs, normalize related paragraph and returns list of XslElements\"\"\"\n\n        def append_xsl_elements(xsl_elements, r, xsl):\n            if r is not None:\n                r.xpath('.//w:t',  namespaces=self.namespaces)[0].text = xsl\n                xe = XslElement(r, logger=self.logger)\n                xsl_elements.append(xe)\n            return None, ''\n\n        if not getattr(self, '_xsl_elements', None):\n            xsl_elements = []\n            for p in self.root.xpath('.//w:p', namespaces=self.namespaces):\n                xsl_r, xsl = None, ''\n                for r in p:\n                    # find first XSL run and add all XSL meta text\n                    text = ''.join(t.text for t in r.xpath('.//w:t', namespaces=self.namespaces))\n                    if r.xpath('.//w:rPr/w:rStyle[@w:val=\"%s\"]' % self.style, namespaces=self.namespaces):\n                        xsl += text\n                        if xsl_r is None and text:\n                            xsl_r = r\n                        else:\n                            r.getparent().remove(r)\n                    elif text:\n                        xsl_r, xsl = append_xsl_elements(xsl_elements, xsl_r, xsl)\n                xsl_r, xsl = append_xsl_elements(xsl_elements, xsl_r, xsl)\n            self._xsl_elements = xsl_elements\n\n        return self._xsl_elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_xsl(self, node, context):\n\n        for e in self.xsl_elements:\n            e.render(e.run)", "response": "Render all XSL elements in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_style(self):\n\n        for n in self.root.xpath('.//w:rStyle[@w:val=\"%s\"]' % self.style, namespaces=self.namespaces):\n            n.getparent().remove(n)", "response": "Remove all XSL run rStyle elements"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering xml string and apply XSLT transfomation with context", "response": "def render(self, xml, context, raise_on_errors=True):\n        \"\"\"Render xml string and apply XSLT transfomation with context\"\"\"\n\n        if xml:\n            self.xml = xml\n\n            # render XSL\n            self.render_xsl(self.root, context)\n\n            # create root XSL sheet\n            xsl_ns = self.namespaces['xsl']\n            rootName = etree.QName(xsl_ns, 'stylesheet')\n            root = etree.Element(rootName, nsmap={'xsl': xsl_ns})\n            sheet = etree.ElementTree(root)\n            template = etree.SubElement(root, etree.QName(xsl_ns, \"template\"), match='/')\n\n            # put OpenOffice tree into XSLT sheet\n            template.append(self.root)\n            self.root = root\n\n            # drop XSL styles\n            self.remove_style()\n\n            #self.debug(self.xml)\n\n            try:\n                # transform XSL\n                xsl = etree.XSLT(self.root)\n                self.root = xsl(context)\n\n            except etree.Error as e:\n                # log errors\n                for l in e.error_log:\n                    self.error(\"XSLT error at line %s col %s:\" % (l.line, l.column))\n                    self.error(\"    message: %s\" % l.message)\n                    self.error(\"    domain: %s (%d)\" % (l.domain_name, l.domain))\n                    self.error('    type: %s (%d)' % (l.type_name, l.type))\n                    self.error('    level: %s (%d)' % (l.level_name, l.level))\n                    self.error('    filename: %s' % l.filename)\n\n                if raise_on_errors:\n                    raise\n\n            return self.xml\n\n        else:\n            return xml"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_image_posts(function=None):\n    @wraps(function, assigned=available_attrs(function))\n    def _wrapped_view(request, *args, **kwargs):\n        if 'image' in request.META['CONTENT_TYPE']:\n            name = default_storage.save(os.path.join('images', 'aloha-uploads', request.META['HTTP_X_FILE_NAME']),\n                                        ContentFile(base64.b64decode(request.body.split(\",\", 1)[1])))\n            return HttpResponse(posixpath.join(settings.MEDIA_URL, name), content_type=\"text/plain\")\n        else:\n            return function(request, *args, **kwargs)\n    return _wrapped_view", "response": "Decorator for views that handles ajax image posts in base64 encoding saving the image and returning the url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verboselogs_class_transform(cls):\n    if cls.name == 'RootLogger':\n        for meth in ['notice', 'spam', 'success', 'verbose']:\n            cls.locals[meth] = [scoped_nodes.Function(meth, None)]", "response": "Make Pylint aware of our custom logger methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake Pylint aware of our custom log levels.", "response": "def verboselogs_module_transform(mod):\n    \"\"\"Make Pylint aware of our custom log levels.\"\"\"\n    if mod.name == 'logging':\n        for const in ['NOTICE', 'SPAM', 'SUCCESS', 'VERBOSE']:\n            mod.locals[const] = [nodes.Const(const)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cache_etag(request, *argz, **kwz):\n\t'''Produce etag value for a cached page.\n\t\tIntended for usage in conditional views (@condition decorator).'''\n\tresponse, site, cachekey = kwz.get('_view_data') or initview(request)\n\tif not response: return None\n\treturn fjcache.str2md5(\n\t\t'{0}--{1}--{2}'.format( site.id if site else 'x', cachekey,\n\t\t\tresponse[1].strftime('%Y-%m-%d %H:%M:%S%z') ) )", "response": "Produce etag value for a cached page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlast modification date for a cached page.", "response": "def cache_last_modified(request, *argz, **kwz):\n\t'''Last modification date for a cached page.\n\t\tIntended for usage in conditional views (@condition decorator).'''\n\tresponse, site, cachekey = kwz.get('_view_data') or initview(request)\n\tif not response: return None\n\treturn response[1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initview(request, response_cache=True):\n\t'''Retrieves the basic data needed by all feeds (host, feeds, etc)\n\t\tReturns a tuple of:\n\t\t\t1. A valid cached response or None\n\t\t\t2. The current site object\n\t\t\t3. The cache key\n\t\t\t4. The subscribers for the site (objects)\n\t\t\t5. The feeds for the site (ids)'''\n\n\thttp_host, path_info = ( smart_unicode(part.strip('/')) for part in\n\t\t[ request.META['HTTP_HOST'],\n\t\t\trequest.META.get('REQUEST_URI', request.META.get('PATH_INFO', '/')) ] )\n\tquery_string = request.META['QUERY_STRING']\n\n\turl = '{0}/{1}'.format(http_host, path_info)\n\tcachekey = u'{0}?{1}'.format(*it.imap(smart_unicode, (path_info, query_string)))\n\thostdict = fjcache.hostcache_get() or dict()\n\n\tsite_id = hostdict[url] if url in hostdict else None\n\tif site_id and response_cache:\n\t\tresponse = fjcache.cache_get(site_id, cachekey)\n\t\tif response: return response, None, cachekey\n\n\tif site_id: site = models.Site.objects.get(pk=site_id)\n\telse: # match site from all of them\n\t\tsites = list(models.Site.objects.all())\n\n\t\tif not sites:\n\t\t\t# Somebody is requesting something, but the user\n\t\t\t#  didn't create a site yet. Creating a default one...\n\t\t\tsite = models.Site(\n\t\t\t\tname='Default Feedjack Site/Planet',\n\t\t\t\turl=request.build_absolute_uri(request.path),\n\t\t\t\ttitle='Feedjack Site Title',\n\t\t\t\tdescription='Feedjack Site Description.'\n\t\t\t\t\t' Please change this in the admin interface.',\n\t\t\t\ttemplate='bootstrap' )\n\t\t\tsite.save()\n\n\t\telse:\n\t\t\t# Select the most matching site possible,\n\t\t\t#  preferring \"default\" when everything else is equal\n\t\t\tresults = defaultdict(list)\n\t\t\tfor site in sites:\n\t\t\t\trelevance, site_url = 0, urlparse(site.url)\n\t\t\t\tif site_url.netloc == http_host: relevance += 10 # host matches\n\t\t\t\tif path_info.startswith(site_url.path.strip('/')): relevance += 10 # path matches\n\t\t\t\tif site.default_site: relevance += 5 # marked as \"default\"\n\t\t\t\tresults[relevance].append((site_url, site))\n\t\t\tfor relevance in sorted(results, reverse=True):\n\t\t\t\ttry: site_url, site = results[relevance][0]\n\t\t\t\texcept IndexError: pass\n\t\t\t\telse: break\n\t\t\tif site_url.netloc != http_host: # redirect to proper site hostname\n\t\t\t\tresponse = HttpResponsePermanentRedirect(\n\t\t\t\t\t'http://{0}/{1}{2}'.format( site_url.netloc, path_info,\n\t\t\t\t\t\t'?{0}'.format(query_string) if query_string.strip() else '') )\n\t\t\t\treturn (response, timezone.now()), None, cachekey\n\n\t\thostdict[url] = site_id = site.id\n\t\tfjcache.hostcache_set(hostdict)\n\n\tif response_cache:\n\t\tresponse = fjcache.cache_get(site_id, cachekey)\n\t\tif response: return response, None, cachekey\n\n\treturn None, site, cachekey", "response": "This function is used to initialize the site object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blogroll(request, btype):\n\t'View that handles the generation of blogrolls.'\n\tresponse, site, cachekey = initview(request)\n\tif response: return response[0]\n\n\ttemplate = loader.get_template('feedjack/{0}.xml'.format(btype))\n\tctx = dict()\n\tfjlib.get_extra_context(site, ctx)\n\tctx = Context(ctx)\n\tresponse = HttpResponse(\n\t\ttemplate.render(ctx), content_type='text/xml; charset=utf-8' )\n\n\tpatch_vary_headers(response, ['Host'])\n\tfjcache.cache_set(site, cachekey, (response, ctx_get(ctx, 'last_modified')))\n\treturn response", "response": "View that handles the generation of blogrolls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nviewing that handles the feeds.", "response": "def buildfeed(request, feedclass, **criterias):\n\t'View that handles the feeds.'\n\tview_data = initview(request)\n\twrap = lambda func: ft.partial(func, _view_data=view_data, **criterias)\n\treturn condition(\n\t\t\tetag_func=wrap(cache_etag),\n\t\t\tlast_modified_func=wrap(cache_last_modified) )\\\n\t\t(_buildfeed)(request, feedclass, view_data, **criterias)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mainview(request, **criterias):\n\t'View that handles all page requests.'\n\tview_data = initview(request)\n\twrap = lambda func: ft.partial(func, _view_data=view_data, **criterias)\n\treturn condition(\n\t\t\tetag_func=wrap(cache_etag),\n\t\t\tlast_modified_func=wrap(cache_last_modified) )\\\n\t\t(_mainview)(request, view_data, **criterias)", "response": "View that handles all page requests."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(request, obj_id=None):\n    if request.method == 'GET':\n        return get(request, obj_id)\n    elif request.method == 'POST':\n        return post(request)\n    elif request.method == 'PUT':\n        getPutData(request)\n        return put(request, obj_id)\n    elif request.method == 'DELETE':\n        getPutData(request)\n        return delete(request, obj_id)", "response": "Handles a request based on method and calls the appropriate function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding Image and Video objects to Gallery based on GUIDs.", "response": "def put(request, obj_id=None):\n    \"\"\" Adds Image and Video objects to Gallery based on GUIDs \"\"\"\n    data = request.PUT or json.loads(request.body)['body']\n    guids = data.get('guids', '').split(',')\n    move = data.get('from')\n    security = request.PUT.get('security')\n    gallery = Gallery.objects.get(pk=obj_id)\n    \n    if guids:\n        objects = getObjectsFromGuids(guids)\n\n        images = filter(lambda x: isinstance(x, Image), objects)\n        videos = filter(lambda x: isinstance(x, Video), objects)\n\n        gallery.images.add(*images)\n        gallery.videos.add(*videos)\n\n        if move:\n            fromgallery = Gallery.objects.get(pk=move)\n            fromgallery.images.remove(*images)\n            fromgallery.videos.remove(*videos)\n    \n    if security is not None:\n        gallery.security = json.loads(security)\n        gallery.save()\n        for child in gallery.gallery_set.all():\n            child.security = gallery.security\n            child.save()\n\n    res = Result()\n    res.append(gallery.json())\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove ImageVideo objects from Gallery", "response": "def delete(request, obj_id=None):\n    \"\"\" Removes ImageVideo objects from Gallery \"\"\"\n    data = request.DELETE or json.loads(request.body)\n    guids = data.get('guids').split(',')\n    objects = getObjectsFromGuids(guids)\n    gallery = Gallery.objects.get(pk=obj_id)\n\n    LOGGER.info('{} removed {} from {}'.format(request.user.email, guids, gallery))\n\n    for o in objects:\n        if isinstance(o, Image):\n            gallery.images.remove(o)\n        elif isinstance(o, Video):\n            gallery.videos.remove(o)\n\n    res = Result()\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filterObjects(request, obj_id):\n    if int(obj_id) == 0:\n        obj = None\n    else:\n        obj = Gallery.objects.get(pk=obj_id)\n\n    isanonymous = request.user.is_anonymous()\n\n    if isanonymous and obj is None:\n        LOGGER.warn('There was an anonymous access attempt from {} to {}'.format(getClientIP(request), obj))\n        raise PermissionDenied()\n\n    if isanonymous and obj and obj.security != Gallery.PUBLIC:\n        LOGGER.warn('There was an anonymous access attempt from {} to {}'.format(getClientIP(request), obj))\n        raise PermissionDenied()\n\n    tags = json.loads(request.GET.get('filters', '[[]]'))\n    more = json.loads(request.GET.get('more', 'false'))\n    orderby = request.GET.get('orderby', request.user.frog_prefs.get().json()['orderby'])\n\n    tags = [t for t in tags if t]\n\n    return _filter(request, obj, tags=tags, more=more, orderby=orderby)", "response": "Filters Gallery for the requested ImageVideo objects. Returns a Result object with serialized objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter Piece objects based on filters search and range", "response": "def _filter(request, object_, tags=None, more=False, orderby='created'):\n    \"\"\"Filters Piece objects from self based on filters, search, and range\n\n    :param tags: List of tag IDs to filter\n    :type tags: list\n    :param more -- bool, Returns more of the same filtered set of images based on session range\n\n    return list, Objects filtered\n    \"\"\"\n    res = Result()\n\n    models = QUERY_MODELS\n\n    idDict = {}\n    objDict = {}\n    data = {}\n    modelmap = {}\n    length = 75\n\n    # -- Get all IDs for each model\n    for m in models:\n        modelmap[m.model_class()] = m.model\n\n        if object_:\n            idDict[m.model] = m.model_class().objects.filter(gallery=object_)\n        else:\n            idDict[m.model] = m.model_class().objects.all()\n\n        if idDict[m.model] is None:\n            continue\n\n        if tags:\n            for bucket in tags:\n                searchQuery = \"\"\n                o = None\n                for item in bucket:\n                    if item == 0:\n                        # -- filter by tagless\n                        idDict[m.model].annotate(num_tags=Count('tags'))\n                        if not o:\n                            o = Q()\n                        o |= Q(num_tags__lte=1)\n                        break\n                    elif isinstance(item, six.integer_types):\n                        # -- filter by tag\n                        if not o:\n                            o = Q()\n                        o |= Q(tags__id=item)\n                    else:\n                        # -- add to search string\n                        searchQuery += item + ' '\n                        if not HAYSTACK:\n                            if not o:\n                                o = Q()\n                            # -- use a basic search\n                            o |= Q(title__icontains=item)\n\n                if HAYSTACK and searchQuery != \"\":\n                    # -- once all tags have been filtered, filter by search\n                    searchIDs = search(searchQuery, m.model_class())\n                    if searchIDs:\n                        if not o:\n                            o = Q()\n                        o |= Q(id__in=searchIDs)\n\n                if o:\n                    # -- apply the filters\n                    idDict[m.model] = idDict[m.model].annotate(num_tags=Count('tags')).filter(o)\n                else:\n                    idDict[m.model] = idDict[m.model].none()\n\n        # -- Get all ids of filtered objects, this will be a very fast query\n        idDict[m.model] = list(idDict[m.model].order_by('-{}'.format(orderby)).values_list('id', flat=True))\n        lastid = request.session.get('last_{}'.format(m.model), 0)\n        if not idDict[m.model]:\n            continue\n\n        if not more:\n            lastid = idDict[m.model][0]\n\n        index = idDict[m.model].index(lastid)\n        if more and lastid != 0:\n            index += 1\n        idDict[m.model] = idDict[m.model][index:index + length]\n\n        # -- perform the main query to retrieve the objects we want\n        objDict[m.model] = m.model_class().objects.filter(id__in=idDict[m.model])\n        objDict[m.model] = objDict[m.model].select_related('author').prefetch_related('tags').order_by('-{}'.format(orderby))\n        objDict[m.model] = list(objDict[m.model])\n\n        # -- combine and sort all objects by date\n    objects = _sortObjects(orderby, **objDict) if len(models) > 1 else objDict.values()[0]\n    objects = objects[:length]\n\n    # -- Find out last ids\n    lastids = {}\n    for obj in objects:\n        lastids['last_{}'.format(modelmap[obj.__class__])] = obj.id\n\n    for key, value in lastids.items():\n        request.session[key] = value\n\n    # -- serialize objects\n    for i in objects:\n        res.append(i.json())\n\n    data['count'] = len(objects)\n    if settings.DEBUG:\n        data['queries'] = connection.queries\n\n    res.value = data\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sortObjects(orderby='created', **kwargs):\n    o = []\n    \n    for m in kwargs.values():\n        for l in iter(m):\n            o.append(l)\n    o = list(set(o))\n    sortfunc = _sortByCreated if orderby == 'created' else _sortByModified\n    if six.PY2:\n        o.sort(sortfunc)\n    else:\n        o.sort(key=functools.cmp_to_key(sortfunc))\n\n    return o", "response": "Sorts lists of objects and combines them into a single list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sortByCreated(a, b):\n    if a.created < b.created:\n        return 1\n    elif a.created > b.created:\n        return -1\n    else:\n        return 0", "response": "Sort function for object by created date"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sortByModified(a, b):\n    if a.modified < b.modified:\n        return 1\n    elif a.modified > b.modified:\n        return -1\n    else:\n        return 0", "response": "Sort function for object by modified date"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(query, model):\n    query = query.strip()\n    LOGGER.debug(query)\n    sqs = SearchQuerySet()\n    results = sqs.raw_search('{}*'.format(query)).models(model)\n    if not results:\n        results = sqs.raw_search('*{}'.format(query)).models(model)\n    if not results:\n        results = sqs.raw_search('*{}*'.format(query)).models(model)\n\n    return [o.pk for o in results]", "response": "Perform a search query and returns the object ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(whatever=None, language=None, iso639_1=None,\n         iso639_2=None, native=None):\n    \"\"\"Find data row with the language.\n\n    :param whatever: key to search in any of the following fields\n    :param language: key to search in English language name\n    :param iso639_1: key to search in ISO 639-1 code (2 digits)\n    :param iso639_2: key to search in ISO 639-2 code (3 digits,\n                     bibliographic & terminological)\n    :param native: key to search in native language name\n    :return: a dict with keys (u'name', u'iso639_1', u'iso639_2_b',\n                     u'iso639_2_t', u'native')\n\n    All arguments can be both string or unicode (Python 2).\n    If there are multiple names defined, any of these can be looked for.\n    \"\"\"\n    if whatever:\n        keys = [u'name', u'iso639_1', u'iso639_2_b', u'iso639_2_t', u'native']\n        val = whatever\n    elif language:\n        keys = [u'name']\n        val = language\n    elif iso639_1:\n        keys = [u'iso639_1']\n        val = iso639_1\n    elif iso639_2:\n        keys = [u'iso639_2_b', u'iso639_2_t']\n        val = iso639_2\n    elif native:\n        keys = [u'native']\n        val = native\n    else:\n        raise ValueError('Invalid search criteria.')\n    val = unicode(val).lower()\n    return next((item for item in data if any(\n        val in item[key].lower().split(\"; \") for key in keys)), None)", "response": "Find data row with the language."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds ISO 639 - 1 code for language specified by key.", "response": "def to_iso639_1(key):\n    \"\"\"Find ISO 639-1 code for language specified by key.\n\n    >>> to_iso639_1(\"swe\")\n    u'sv'\n    >>> to_iso639_1(\"English\")\n    u'en'\n    \"\"\"\n    item = find(whatever=key)\n    if not item:\n        raise NonExistentLanguageError('Language does not exist.')\n    return item[u'iso639_1']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_iso639_2(key, type='B'):\n    if type not in ('B', 'T'):\n        raise ValueError('Type must be either \"B\" or \"T\".')\n    item = find(whatever=key)\n    if not item:\n        raise NonExistentLanguageError('Language does not exist.')\n    if type == 'T' and item[u'iso639_2_t']:\n        return item[u'iso639_2_t']\n    return item[u'iso639_2_b']", "response": "Find ISO 639 - 2 code for language specified by key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_name(key):\n    item = find(whatever=key)\n    if not item:\n        raise NonExistentLanguageError('Language does not exist.')\n    return item[u'name']", "response": "Find the English name for the language specified by key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the native name for the language specified by key.", "response": "def to_native(key):\n    \"\"\"Find the native name for the language specified by key.\n\n    >>> to_native('br')\n    u'brezhoneg'\n    >>> to_native('sw')\n    u'Kiswahili'\n    \"\"\"\n    item = find(whatever=key)\n    if not item:\n        raise NonExistentLanguageError('Language does not exist.')\n    return item[u'native']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef address_inline(request, prefix=\"\", country_code=None, template_name=\"postal/form.html\"):\n    \n    country_prefix = \"country\"\n    prefix = request.POST.get('prefix', prefix)\n    \n    if prefix:\n        country_prefix = prefix + '-country'\n    \n    country_code = request.POST.get(country_prefix, country_code)\n    form_class = form_factory(country_code=country_code)\n    \n    if request.method == \"POST\":\n        data = {}\n        for (key, val) in request.POST.items():\n            if val is not None and len(val) > 0:\n                data[key] = val\n        data.update({country_prefix: country_code})\n        \n        form = form_class(prefix=prefix, initial=data)\n    else:\n        form = form_class(prefix=prefix)\n        \n    return render_to_string(template_name, RequestContext(request, {\n        \"form\": form,\n        \"prefix\": prefix,\n    }))", "response": "Displays the postal address with localized fields"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the results of the benchmark to a log file.", "response": "def write_log(self, fs=None):\n        \"\"\"\n        Write the results of the benchmark to a log file.\n        :param fs: file-like object.\n        \"\"\"\n        log = StringIO.StringIO()\n        log.write(self.setup_src)\n\n        # If the function is not bound, write the test score to the log\n        if not self.is_class_method:\n            time_avg = convert_time_units(self.time_average_seconds)\n            log.write(\"\\nAverage time: {0} \\n\".format(time_avg))\n\n        if fs:\n            with open(fs, 'w') as _f:\n                _f.write(log.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_timeit(self, stmt, setup):\n        _timer = timeit.Timer(stmt=stmt, setup=setup)\n        trials = _timer.repeat(self.timeit_repeat, self.timeit_number)\n        self.time_average_seconds = sum(trials) / len(trials) / self.timeit_number\n        # Convert into reasonable time units\n        time_avg = convert_time_units(self.time_average_seconds)\n        return time_avg", "response": "Run timeit and return the time average seconds."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_mx_exchanges(domain):\n    try:\n        answer = resolver.query(domain, 'MX')\n        return [str(record.exchange).lower()[:-1] for record in answer]\n    except (resolver.NoAnswer, resolver.NoNameservers, resolver.NotAbsolute,\n            resolver.NoRootSOA, resolver.NXDOMAIN, resolver.Timeout) as error:\n        LOGGER.error('Error querying MX for %s: %r', domain, error)\n        return []", "response": "Fetch the MX records for the specified domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _domain_check(domain, domain_list, resolve):\n    if domain in domain_list:\n        return True\n    if resolve:\n        for exchange in _get_mx_exchanges(domain):\n            for value in domain_list:\n                if exchange.endswith(value):\n                    return True\n    return False", "response": "Checks if the domain is serviced by the domain_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize(email_address, resolve=True):\n    address = utils.parseaddr(email_address)\n    local_part, domain_part = address[1].lower().split('@')\n\n    # Plus addressing is supported by Microsoft domains and FastMail\n    if domain_part in MICROSOFT_DOMAINS:\n        if '+' in local_part:\n            local_part = local_part.split('+')[0]\n\n    # GMail supports plus addressing and throw-away period delimiters\n    elif _is_gmail(domain_part, resolve):\n        local_part = local_part.replace('.', '').split('+')[0]\n\n    # Yahoo domain handling of - is like plus addressing\n    elif _is_yahoo(domain_part, resolve):\n        if '-' in local_part:\n            local_part = local_part.split('-')[0]\n\n    # FastMail has domain part username aliasing and plus addressing\n    elif _is_fastmail(domain_part, resolve):\n        domain_segments = domain_part.split('.')\n        if len(domain_segments) > 2:\n            local_part = domain_segments[0]\n            domain_part = '.'.join(domain_segments[1:])\n        elif '+' in local_part:\n            local_part = local_part.split('+')[0]\n\n    return '@'.join([local_part, domain_part])", "response": "Normalizes an email address removing the domain name aliasing and plus addressing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(filename, verbose=False):\n    # is_css = filename.endswith(\".css\")\n\n    is_remote = filename.startswith(\"http://\") or filename.startswith(\n        \"https://\")\n    with tempfile.TemporaryFile() if is_remote else open(\n            filename, \"rb\") as f:\n\n        if is_remote:\n            r = requests.get(filename, verify=False)\n            f.write(r.content)\n            f.seek(0)\n\n        # if is_css:\n        #     cmd = (\n        #         \"curl -sF \\\"file=@%s;type=text/css\\\" -F output=json -F warning=0 %s\"\n        #         % (quoted_filename, CSS_VALIDATOR_URL))\n        #     _ = cmd\n        # else:\n        r = requests.post(\n            HTML_VALIDATOR_URL,\n            files={\"file\": (filename, f, \"text/html\")},\n            data={\n                \"out\": \"json\",\n                \"showsource\": \"yes\",\n            },\n            verify=False)\n\n    return r.json()", "response": "Validate file and return JSON result as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the command line and run the validator.", "response": "def main():\n    \"\"\"Parser the command line and run the validator.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"[v\" + __version__ + \"] \" + __doc__,\n        prog=\"w3c_validator\",\n    )\n    parser.add_argument(\n        \"--log\",\n        default=\"INFO\",\n        help=(\"log level: DEBUG, INFO or INFO \"\n              \"(default: INFO)\"))\n    parser.add_argument(\n        \"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n    parser.add_argument(\n        \"--verbose\", help=\"increase output verbosity\", action=\"store_true\")\n    parser.add_argument(\n        \"source\", metavar=\"F\", type=str, nargs=\"+\", help=\"file or URL\")\n    args = parser.parse_args()\n\n    logging.basicConfig(level=getattr(logging, args.log))\n\n    LOGGER.info(\"Files to validate: \\n  {0}\".format(\"\\n  \".join(args.source)))\n    LOGGER.info(\"Number of files: {0}\".format(len(args.source)))\n\n    errors = 0\n    warnings = 0\n    for f in args.source:\n        LOGGER.info(\"validating: %s ...\" % f)\n        retrys = 0\n        while retrys < 2:\n            result = validate(f, verbose=args.verbose)\n            if result:\n                break\n\n            time.sleep(2)\n            retrys += 1\n            LOGGER.info(\"retrying: %s ...\" % f)\n        else:\n            LOGGER.info(\"failed: %s\" % f)\n            errors += 1\n            continue\n\n        # import pdb; pdb.set_trace()\n        if f.endswith(\".css\"):\n            errorcount = result[\"cssvalidation\"][\"result\"][\"errorcount\"]\n            warningcount = result[\"cssvalidation\"][\"result\"][\"warningcount\"]\n            errors += errorcount\n            warnings += warningcount\n            if errorcount > 0:\n                LOGGER.info(\"errors: %d\" % errorcount)\n            if warningcount > 0:\n                LOGGER.info(\"warnings: %d\" % warningcount)\n        else:\n            for msg in result[\"messages\"]:\n                print_msg(msg)\n                if msg[\"type\"] == \"error\":\n                    errors += 1\n                else:\n                    warnings += 1\n    sys.exit(min(errors, 255))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the response from redis WorkItem into a dictionary.", "response": "def format_info_response(value):\n    \"\"\"Format the response from redis\n\n    :param str value: The return response from redis\n    :rtype: dict\n\n    \"\"\"\n    info = {}\n    for line in value.decode('utf-8').splitlines():\n        if not line or line[0] == '#':\n            continue\n        if ':' in line:\n            key, value = line.split(':', 1)\n            info[key] = parse_info_value(value)\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, *args):\n        self.multicolors = self.multicolors + Counter(arg for arg in args)", "response": "Updates the multicolor. multicolors attribute with the number of colors added to the current multicolor instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the specified color from the given multicolor.", "response": "def __delete(self, multicolor):\n        \"\"\" Reduces information :class:`Multicolor` attribute by iterating over supplied colors data.\n\n        In case supplied argument is a :class:`Multicolor` instance, multi-color specific information to de deleted is set to its :attr:`Multicolor.multicolors`.\n        In other cases multi-color specific information to de deleted is obtained from iterating over the argument.\n\n        Colors and their multiplicity is reduces with a help of ``-`` method of python Counter object.\n\n        :param multicolor: information about colors to be deleted from :class:`Multicolor` object\n        :type multicolor: any iterable with colors object as entries or :class:`Multicolor`\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        if isinstance(multicolor, Multicolor):\n            to_delete = multicolor.multicolors\n        else:\n            to_delete = Counter(color for color in multicolor)\n        self.multicolors = self.multicolors - to_delete"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nproduces a new : class:`Multicolor` object resulting from gathering information from all supplied arguments.", "response": "def __merge(cls, *multicolors):\n        \"\"\" Produces a new :class:`Multicolor` object resulting from gathering information from all supplied :class:`Multicolor` instances.\n\n        New :class:`Multicolor` is created and its :attr:`Multicolor.multicolors` attribute is updated with similar attributes of supplied :class:`Multicolor` objects.\n\n        Accounts for subclassing.\n\n        :param multicolors: variable number of :class:`Multicolor` objects\n        :type multicolors: :class:`Multicolor`\n        :return: object containing gathered information from all supplied arguments\n        :rtype: :class:`Multicolor`\n        \"\"\"\n        result = cls()\n        for multicolor in multicolors:\n            result.multicolors = result.multicolors + multicolor.multicolors\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates first supplied multicolor instance with information from second supplied multicolor instance.", "response": "def __left_merge(multicolor1, multicolor2):\n        \"\"\" Updates first supplied :class:`Multicolor` instance with information from second supplied :class:`Multicolor` instance.\n\n        First supplied instances attribute :attr:`Multicolor.multicolors` is updated with a help of ``+`` method of python Counter object.\n\n        :param multicolor1: instance to update information in\n        :type multicolor1: :class:`Multicolor`\n        :param multicolor2: instance to use information for update from\n        :type multicolor2: :class:`Multicolor`\n        :return: updated first supplied :class:`Multicolor` instance\n        :rtype: :class:`Multicolor`\n        \"\"\"\n        multicolor1.multicolors = multicolor1.multicolors + multicolor2.multicolors\n        return multicolor1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef similarity_score(multicolor1, multicolor2):\n        result = 0\n        for key, value in multicolor1.multicolors.items():\n            if key in multicolor2.multicolors:\n                result += min(value, multicolor2.multicolors[key])\n        return result", "response": "Compute the similarity score between two supplied multicolor objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_colors(cls, multicolor, guidance=None, sorted_guidance=False,\n                     account_for_color_multiplicity_in_guidance=True):\n        \"\"\" Produces several new instances of :class:`Multicolor` object by splitting information about colors by using provided guidance iterable set-like object.\n\n        Guidance is an iterable type of object where each entry has information about groups of colors that has to be separated for current :attr:`Multicolor.multicolors` chunk.\n        If no Guidance is provided, single-color guidance of :attr:`Multicolor.multicolors` is created.\n        Guidance object is first reversed sorted to iterate over it from larges color set to the smallest one, as small color sets might be subsets of bigger ones, and shall be utilized only if bigger sets didn't help in separating.\n\n        During the first iteration over the guidance information all subsets of :attr:`Multicolor.multicolors` that equal to entries of guidance are recorded.\n        During second iteration over remaining of the guidance information, if colors in :attr:`Multicolor.multicolors` form subsets of guidance entries, such instances are recorded.\n        After this two iterations, the rest of :attr:`Multicolor.multicolors` is recorded as non-tackled and is recorded on its own.\n\n        Multiplicity of all separated colors in respective chunks is preserved.\n\n        Accounts for subclassing.\n\n        :param multicolor: an instance information about colors in which is to be split\n        :type multicolor: :class:`Multicolor`\n        :param guidance: information how colors have to be split in current :class:`Multicolor` object\n        :type guidance: iterable where each entry is iterable with colors entries\n        :param sorted_guidance: a flag, that indicates is sorting of provided guidance is in order\n        :return: a list of new :class:`Multicolor` object colors information in which complies with guidance information\n        :rtype: ``list`` of :class:`Multicolor` objects\n        \"\"\"\n        if guidance is None:\n            ###############################################################################################\n            #\n            # if guidance is not specified, it will be derived from colors in the targeted multicolor\n            # initially the multiplicity of colors remains as is\n            #\n            ###############################################################################################\n            guidance = [Multicolor(*(color for _ in range(multicolor.multicolors[color]))) for color in multicolor.colors]\n            ###############################################################################################\n            #\n            # since at this we have a single-colored (maybe with multiplicity greater than 1)\n            # we don't need to sort anything, as there will be no overlapping multicolor in guidance\n            #\n            ###############################################################################################\n            sorted_guidance = True\n        ###############################################################################################\n        #\n        # a reference to the targeted multicolor.\n        # such reference is created only for the future requirement to access information about original multicolor\n        # Is done for the sake of code clarity and consistency.\n        #\n        ###############################################################################################\n        splitting_multicolor = deepcopy(multicolor)\n        if not account_for_color_multiplicity_in_guidance:\n            ###############################################################################################\n            #\n            # we need to create a new guidance (even if original is perfect)\n            # a new one shall preserve the order of the original, but all multicolors in it\n            #   while keeping information about the actual colors itself, shall have multiplicity equal to 1\n            #\n            ###############################################################################################\n            splitting_multicolor = Multicolor(*multicolor.colors)\n            colors_guidance = [Multicolor(*tmp_multicolor.colors) for tmp_multicolor in guidance]\n            ###############################################################################################\n            #\n            # since there might be different multicolors, with the same colors content\n            # and they will be changed to same multicolors object, after colors multiplicity adjustment\n            # we need, while preserving the order, leave only unique ones in (the first appearance)\n            #\n            ###############################################################################################\n            unique = set()\n            guidance = []\n            for c_multicolor in colors_guidance:\n                if c_multicolor.hashable_representation not in unique:\n                    unique.add(c_multicolor.hashable_representation)\n                    guidance.append(c_multicolor)\n        if not sorted_guidance:\n            ###############################################################################################\n            #\n            # if arguments in function call do not specify explicitly, that the guidance shall be used \"as is\"\n            # it is sorted to put \"bigger\" multicolors in front, and smaller at the back\n            # as bigger multicolor might contain several smaller multicolors from the guidance, but the correct splitting\n            # always assumes that the smaller is the splitted result, the better it is\n            # and such minimization can be obtained only if the biggest chunk of targeted multicolor are ripped off of it first\n            #\n            ###############################################################################################\n            guidance = sorted({g_multicolor.hashable_representation for g_multicolor in guidance},\n                              key=lambda g_multicolor: len(g_multicolor),\n                              reverse=True)\n            guidance = [Multicolor(*hashed) for hashed in guidance]\n        first_run_result = []\n        second_run_result = []\n        for g_multicolor in guidance:\n            ###############################################################################################\n            #\n            # first we determine which multicolors in guidance are fully present in the multicolor to split\n            #   \"<=\" operator can be read as \"is_multi_subset_of\"\n            # and retrieve as many copies of it from the multicolor, as we can\n            # Example:\n            #   multicolor has only one color \"blue\" with multiplicity \"4\"\n            #   in guidance we have multicolor with color \"blue\" with multiplicity \"2\"\n            #   we must retrieve it fully twice\n            #\n            ###############################################################################################\n            ###############################################################################################\n            #\n            # empty guidance multicolors shall be ignored, as they have no impact on the splitting algorithm\n            #\n            ###############################################################################################\n            if len(g_multicolor.colors) == 0:\n                continue\n            while g_multicolor <= splitting_multicolor:\n                first_run_result.append(g_multicolor)\n                splitting_multicolor -= g_multicolor\n        for g_multicolor in guidance:\n            ###############################################################################################\n            #\n            # secondly we determine which multicolors in guidance are partially present in the multicolor\n            # NOTE that this is not possible for the case of tree consistent multicolor\n            #   as every partially present\n            #\n            ###############################################################################################\n            while len(g_multicolor.intersect(splitting_multicolor).multicolors) > 0:\n                second_run_result.append(g_multicolor.intersect(splitting_multicolor))\n                splitting_multicolor -= g_multicolor.intersect(splitting_multicolor)\n        appendix = splitting_multicolor\n        result = deepcopy(first_run_result) + deepcopy(second_run_result) + deepcopy([appendix] if len(appendix.multicolors) > 0 else [])\n        if not account_for_color_multiplicity_in_guidance:\n            ###############################################################################################\n            #\n            # if we didn't care for guidance multicolors colors multiplicity, we we splitting a specially created Multicolor\n            # based only on the colors content.\n            # After this is done, we need to restore the original multiplicity of each color in result multicolors to the\n            # count they had in the targeted for splitting multicolor.\n            # This is possible since in the case when we do not account for colors multiplicity in guidance, we have\n            # splitting_color variable referencing not the supplied multicolor, and thus internal changes are not made to\n            # supplied multicolor.\n            #\n            ###############################################################################################\n            for r_multicolor in result:\n                for color in r_multicolor.colors:\n                    r_multicolor.multicolors[color] = multicolor.multicolors[color]\n        return result", "response": "Generates several new instances of multicolor objects by splitting information about colors in the given multicolor and guidance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intersect(self, other):\n        if not isinstance(other, Multicolor):\n            raise TypeError(\"Multicolor can be intersected only with another Multicolor object\")\n        intersection_colors_core = self.colors.intersection(other.colors)\n        colors_count = {color: min(self.multicolors[color], other.multicolors[color]) for color in intersection_colors_core}\n        return Multicolor(*(color for color in colors_count for _ in range(colors_count[color])))", "response": "Computes the multiset intersection between the current Multicolor and the supplied Multicolor object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef text2wfreq(text, output_file, hashtablesize=1000000, verbosity=2):\n    cmd = ['text2wfreq', '-hash', hashtablesize,\n                         '-verbosity', verbosity]\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with tempfile.SpooledTemporaryFile() as input_f:\n        input_f.write(text.encode('utf-8') if sys.version_info >= (3,) and type(text) is str else text)\n        input_f.seek(0)\n        with open(output_file,'w+') as output_f:\n            with  output_to_debuglogger() as err_f:\n                exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))", "response": "Convert a text file into a word - frequency file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a word unigram file as produced by text2wfreq and converts it to a vocabulary file.", "response": "def wfreq2vocab(wfreq_file, output_file, top=None, gt=None, records=1000000, verbosity=2):\n    \"\"\"\n        Takes a a word unigram file, as produced by text2wfreq and converts it to a vocabulary file.\n        The top parameter allows the user to specify the size of the vocabulary; if the function is called with the parameter top=20000, then the vocabulary will consist of the most common 20,000 words.\n        The gt parameter allows the user to specify the number of times that a word must occur to be included in the vocabulary; if the function is called with the parameter gt=10, then the vocabulary will consist of all the words which occurred more than 10 times.\n        If neither the gt, nor the top parameters are specified, then the function runs with the default setting of taking the top 20,000 words.\n        The records parameter (default: 1000000) allows the user to specify how many of the word and count records to allocate memory for. If the number of words in the input exceeds this number, then the function will fail and raise a ConversionError, but a high number will obviously result in a higher memory requirement.\n    \"\"\"\n    cmd = ['wfreq2vocab', '-verbosity', verbosity,\n                           '-records', records]\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    if top:\n        cmd.extend(['-top',top])\n    elif gt:\n        cmd.extend(['-gt',gt])\n\n    with open(wfreq_file,'r') as input_f:\n        with open(output_file,'w+') as output_f:\n            with  output_to_debuglogger() as err_f:\n                exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text2wngram(text, output_file, n=3, chars=63636363, words=9090909, compress=False, verbosity=2):\n    cmd = ['text2wngram']\n    \n    if n:\n        cmd.extend(['-n', n])\n    \n    if chars:\n        cmd.extend(['-chars', chars])\n\n    if words:\n        cmd.extend(['-words', words])\n\n    if compress:\n        cmd.append('-compress')\n    \n    if verbosity:\n        cmd.extend(['-verbosity', verbosity])\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with tempfile.SpooledTemporaryFile() as input_f:\n        input_f.write(text.encode('utf-8') if sys.version_info >= (3,) and type(text) is str else text)\n        input_f.seek(0)\n        with open(output_file,'w+') as output_f:\n            with  output_to_debuglogger() as err_f:\n                with do_in_tempdir():\n                    exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))", "response": "This function will take a text file and write it to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ngram2mgram(input_file, output_file, n, m, words=False, ascii_idngram=False):\n    cmd = ['ngram2mgram', '-n', n,\n                          '-m', m]\n\n    if words and ascii_idngram:\n        raise ConversionError(\"Parameters 'words' and 'ascii_idngram' cannot both be True\")\n\n    if words:\n        cmd.append('-words')\n    elif ascii_idngram:\n        cmd.append('-ascii')\n    else:\n        cmd.append('-binary')\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with open(input_file,'r') as input_f:\n        with open(output_file,'w+') as output_f:\n            with  output_to_debuglogger() as err_f:\n                exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))", "response": "Takes either a n - gram file or an id n - gram file and outputs a file of the same type where n < m."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wngram2idngram(input_file, vocab_file, output_file, buffersize=100, hashtablesize=2000000, files=20, compress=False, verbosity=2, n=3, write_ascii=False, fof_size=10):\n    cmd = ['wngram2idngram', '-vocab', os.path.abspath(vocab_file),\n                             '-idngram', os.path.abspath(output_file)]\n    if buffersize:\n        cmd.extend(['-buffer', buffersize])\n\n    if hashtablesize:\n        cmd.extend(['-hash', hashtablesize])\n\n    if files:\n        cmd.extend(['-files', files])\n\n    if verbosity:\n        cmd.extend(['-verbosity', verbosity])\n\n    if n:\n        cmd.extend(['-n', n])\n\n    if fof_size:\n        cmd.extend(['-fof_size', fof_size])\n\n    if compress:\n        cmd.append('-compress')\n    if write_ascii:\n        cmd.append('-write_ascii')\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n    \n    with tempfile.SpooledTemporaryFile() as output_f:\n        with tempfile.SpooledTemporaryFile() as input_f:\n            input_f.write(text.encode('utf-8') if sys.version_info >= (3,) and type(text) is str else text)\n            input_f.seek(0)\n            with  output_to_debuglogger() as err_f:\n                with do_in_tempdir():\n                    exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n        output = output_f.read()\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%r' returned with non-zero exit status '%s'\" % (cmd, exitcode))\n\n    if sys.version_info >= (3,) and type(output) is bytes:\n        output = output.decode('utf-8')\n\n    return output.strip()", "response": "Takes a word N - gram file and a vocabulary file and lists every id n - gram which occurred in the text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an idngram file to a stats file.", "response": "def idngram2stats(input_file, output_file, n=3, fof_size=50, verbosity=2, ascii_input=False):\n    \"\"\"\n        Lists the frequency-of-frequencies for each of the 2-grams, ... , n-grams, which can enable the user to choose appropriate cut-offs, and to specify appropriate memory requirements with the spec_num parameter in idngram2lm.\n    \"\"\"\n    cmd = ['idngram2stats']\n    if n:\n        cmd.extend(['-n', n])\n    \n    if fof_size:\n        cmd.extend(['-fof_size'], fof_size)\n\n    if verbosity:\n        cmd.extend(['-verbosity'], verbosity)\n\n    if ascii_input:\n        cmd.append(['-ascii_input'])\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with open(input_file,'r') as input_f:\n        with open(output_file,'w+') as output_f:\n            with  output_to_debuglogger() as err_f:\n                exitcode = subprocess.call(cmd, stdin=input_f, stdout=output_f, stderr=err_f)\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a set of id n-gram files (in either binary (by default) or ASCII (if specified) format - note that they should all be in the same format, however) and outputs a merged id N-gram. Notes : This function can also be used to convert id n-gram files between ascii and binary formats.", "response": "def mergeidngram(output_file, input_files, n=3, ascii_input=False, ascii_output=False):\n    \"\"\"\n        Takes a set of id n-gram files (in either binary (by default) or ASCII (if specified) format - note that they should all be in the same format, however) and outputs a merged id N-gram.\n\n        Notes : This function can also be used to convert id n-gram files between ascii and binary formats.\n    \"\"\"\n    cmd = ['mergeidngram']\n    if n:\n        cmd.extend(['-n', n])\n    \n    if ascii_input:\n        cmd.append('-ascii_input')\n    \n    if ascii_output:\n        cmd.append('-ascii_output')\n\n    if len(input_file) > 1:\n        raise MergeError(\"mergeidngram needs at least 1 input file\")\n\n    cmd.extend(input_files)\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with open(output_file,'w+') as output_f:\n        with  output_to_debuglogger() as err_f:\n            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking an idngram-file (in either binary (by default) or ASCII (if specified) format), a vocabulary file, and (optionally) a context cues file. Additional command line parameters will specify the cutoffs, the discounting strategy and parameters, etc. It outputs a language model, in either binary format (to be read by evallm), or in ARPA format.", "response": "def idngram2lm(idngram_file, vocab_file, output_file, context_file=None, vocab_type=1, oov_fraction=0.5, four_byte_counts=False, min_unicount=0, zeroton_fraction=False, n=3, verbosity=2, arpa_output=True, ascii_input=False):\n    \"\"\"\n        Takes an idngram-file (in either binary (by default) or ASCII (if specified) format), a vocabulary file, and (optionally) a context cues file. Additional command line parameters will specify the cutoffs, the discounting strategy and parameters, etc. It outputs a language model, in either binary format (to be read by evallm), or in ARPA format.\n    \"\"\"\n     # TODO: Args still missing\n     # [ -calc_mem | -buffer 100 | -spec_num y ... z ]\n     # [ -two_byte_bo_weights   \n     #     [ -min_bo_weight nnnnn] [ -max_bo_weight nnnnn] [ -out_of_range_bo_weights] ]\n     # [ -linear | -absolute | -good_turing | -witten_bell ]\n     # [ -disc_ranges 1 7 7 ]\n     # [ -cutoffs 0 ... 0 ]\n\n    cmd = ['idngram2lm', '-idngram', os.path.abspath(idngram_file),\n                         '-vocab', os.path.abspath(vocab_file),\n                         '-vocab_type', vocab_type,\n                         '-oov_fraction', oov_fraction,\n                         '-min_unicount',min_unicount,\n                         '-verbosity',verbosity,\n                         '-n',n]\n    if arpa_output:\n        cmd.extend(['-arpa',output_file])\n    else:\n        cmd.extend(['-binary',output_file])\n\n    if four_byte_counts:\n        cmd.append('-four_byte_counts')\n\n    if zeroton_fraction:\n        cmd.append('-zeroton_fraction')\n\n    if ascii_input:\n        cmd.append('-ascii_input')\n    else:\n        cmd.append('-bin_input')\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with tempfile.SpooledTemporaryFile() as output_f:\n        with  output_to_debuglogger() as err_f:\n            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)\n        output = output_f.read()\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))\n\n    if sys.version_info >= (3,) and type(output) is bytes:\n        output = output.decode('utf-8')\n\n    return output.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef binlm2arpa(input_file, output_file, verbosity=2):\n    cmd = ['binlm2arpa', '-binary', input_file,\n                         '-arpa'. output_file]\n    \n    if verbosity:\n        cmd.extend(['-verbosity', verbosity])\n    \n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with tempfile.SpooledTemporaryFile() as output_f:\n        with  output_to_debuglogger() as err_f:\n            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)\n        output = output_f.read()\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))        \n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))\n\n    if sys.version_info >= (3,) and type(output) is bytes:\n        output = output.decode('utf-8')\n\n    return output.strip()", "response": "This function converts a binary format language model into an ARPA format language model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef text2vocab(text, output_file, text2wfreq_kwargs={}, wfreq2vocab_kwargs={}):\n    with tempfile.NamedTemporaryFile(suffix='.wfreq', delete=False) as f:\n        wfreq_file = f.name\n\n    try:\n        text2wfreq(text, wfreq_file, **text2wfreq_kwargs)\n        wfreq2vocab(wfreq_file, output_file, **wfreq2vocab_kwargs)\n    except ConversionError:\n        raise\n    finally:\n        os.remove(wfreq_file)", "response": "Convienience function that uses text2wfreq and wfreq2vocab to create a vocabulary file from text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef text2lm(text, output_file, vocab_file=None, text2idngram_kwargs={}, idngram2lm_kwargs={}):\n    if vocab_file:\n        used_vocab_file = vocab_file\n    else:\n        # Create temporary vocab file\n        with tempfile.NamedTemporaryFile(suffix='.vocab', delete=False) as f:\n            used_vocab_file = f.name\n        text2vocab(text, used_vocab_file)\n\n    # Create temporary idngram file\n    with tempfile.NamedTemporaryFile(suffix='.idngram', delete=False) as f:\n        idngram_file = f.name\n\n    try:\n        output1 = text2idngram(text, vocab_file=used_vocab_file, output_file=idngram_file, **text2idngram_kwargs)\n        output2 = idngram2lm(idngram_file, vocab_file=used_vocab_file, output_file=output_file, **idngram2lm_kwargs)\n    except ConversionError:\n        output = (None, None)\n        raise\n    else:\n        output = (output1, output2)\n    finally:\n        # Remove temporary files\n        if not vocab_file:\n            os.remove(used_vocab_file)\n        os.remove(idngram_file)\n    return output", "response": "Convienience function to directly convert text into a language model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pfadd(self, key, *elements):\n        return self._execute([b'PFADD', key] + list(elements), 1)", "response": "Adds all the elements to the HyperLogLog data structure at the specified variable name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pfmerge(self, dest_key, *keys):\n        return self._execute([b'PFMERGE', dest_key] + list(keys), b'OK')", "response": "Merge multiple HyperLogLog values into a single value that approximate the cardinality of the union of the observed Sets of the source HyperLogLogs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef construct(self):\n        def _any(thing, fields=None):\n            \"\"\"\n            Dispatch, all types are routed through here.\n            \"\"\"\n            ret = None\n\n            if isinstance(thing, QuerySet):\n                ret = _qs(thing, fields)\n            elif isinstance(thing, (tuple, list, set)):\n                ret = _list(thing, fields)\n            elif isinstance(thing, dict):\n                ret = _dict(thing, fields)\n            elif isinstance(thing, decimal.Decimal):\n                ret = str(thing)\n            elif isinstance(thing, Model):\n                ret = _model(thing, fields)\n            elif isinstance(thing, HttpResponse):\n                raise HttpStatusCode(thing)\n            elif inspect.isfunction(thing):\n                if not inspect.getargspec(thing)[0]:\n                    ret = _any(thing())\n            elif hasattr(thing, '__emittable__'):\n                f = thing.__emittable__\n                if inspect.ismethod(f) and len(inspect.getargspec(f)[0]) == 1:\n                    ret = _any(f())\n            elif repr(thing).startswith(\"<django.db.models.fields.related.RelatedManager\"):\n                ret = _any(thing.all())\n            else:\n                ret = smart_unicode(thing, strings_only=True)\n\n            return ret\n\n        def _fk(data, field):\n            \"\"\"\n            Foreign keys.\n            \"\"\"\n            return _any(getattr(data, field.name))\n\n        def _related(data, fields=None):\n            \"\"\"\n            Foreign keys.\n            \"\"\"\n            return [ _model(m, fields) for m in data.iterator() ]\n\n        def _m2m(data, field, fields=None):\n            \"\"\"\n            Many to many (re-route to `_model`.)\n            \"\"\"\n            return [ _model(m, fields) for m in getattr(data, field.name).iterator() ]\n\n        def _model(data, fields=None):\n            \"\"\"\n            Models. Will respect the `fields` and/or\n            `exclude` on the handler (see `typemapper`.)\n            \"\"\"\n            ret = { }\n            handler = self.in_typemapper(type(data), self.anonymous)\n            get_absolute_uri = False\n\n            if handler or fields:\n                v = lambda f: getattr(data, f.attname)\n\n                if handler:\n                    fields = getattr(handler, 'fields')\n\n                if not fields or hasattr(handler, 'fields'):\n                    \"\"\"\n                    Fields was not specified, try to find teh correct\n                    version in the typemapper we were sent.\n                    \"\"\"\n                    mapped = self.in_typemapper(type(data), self.anonymous)\n                    get_fields = set(mapped.fields)\n                    exclude_fields = set(mapped.exclude).difference(get_fields)\n\n                    if 'absolute_uri' in get_fields:\n                        get_absolute_uri = True\n\n                    if not get_fields:\n                        get_fields = set([ f.attname.replace(\"_id\", \"\", 1)\n                            for f in data._meta.fields + data._meta.virtual_fields])\n\n                    if hasattr(mapped, 'extra_fields'):\n                        get_fields.update(mapped.extra_fields)\n\n                    # sets can be negated.\n                    for exclude in exclude_fields:\n                        if isinstance(exclude, basestring):\n                            get_fields.discard(exclude)\n\n                        elif isinstance(exclude, re._pattern_type):\n                            for field in get_fields.copy():\n                                if exclude.match(field):\n                                    get_fields.discard(field)\n\n                else:\n                    get_fields = set(fields)\n\n                met_fields = self.method_fields(handler, get_fields)\n\n                for f in data._meta.local_fields + data._meta.virtual_fields:\n                    if f.serialize and not any([ p in met_fields for p in [ f.attname, f.name ]]):\n                        if not f.rel:\n                            if f.attname in get_fields:\n                                ret[f.attname] = _any(v(f))\n                                get_fields.remove(f.attname)\n                        else:\n                            if f.attname[:-3] in get_fields:\n                                ret[f.name] = _fk(data, f)\n                                get_fields.remove(f.name)\n\n                for mf in data._meta.many_to_many:\n                    if mf.serialize and mf.attname not in met_fields:\n                        if mf.attname in get_fields:\n                            ret[mf.name] = _m2m(data, mf)\n                            get_fields.remove(mf.name)\n\n                # try to get the remainder of fields\n                for maybe_field in get_fields:\n                    if isinstance(maybe_field, (list, tuple)):\n                        model, fields = maybe_field\n                        inst = getattr(data, model, None)\n\n                        if inst:\n                            if hasattr(inst, 'all'):\n                                ret[model] = _related(inst, fields)\n                            elif callable(inst):\n                                if len(inspect.getargspec(inst)[0]) == 1:\n                                    ret[model] = _any(inst(), fields)\n                            else:\n                                ret[model] = _model(inst, fields)\n\n                    elif maybe_field in met_fields:\n                        # Overriding normal field which has a \"resource method\"\n                        # so you can alter the contents of certain fields without\n                        # using different names.\n                        ret[maybe_field] = _any(met_fields[maybe_field](data))\n\n                    else:\n                        maybe = getattr(data, maybe_field, None)\n                        if maybe is not None:\n                            if callable(maybe):\n                                if len(inspect.getargspec(maybe)[0]) <= 1:\n                                    ret[maybe_field] = _any(maybe())\n                            else:\n                                ret[maybe_field] = _any(maybe)\n                        else:\n                            handler_f = getattr(handler or self.handler, maybe_field, None)\n\n                            if handler_f:\n                                ret[maybe_field] = _any(handler_f(data))\n\n            else:\n                for f in data._meta.fields:\n                    ret[f.attname] = _any(getattr(data, f.attname))\n\n                fields = dir(data.__class__) + ret.keys()\n                add_ons = [k for k in dir(data) if k not in fields]\n\n                for k in add_ons:\n                    ret[k] = _any(getattr(data, k))\n\n            # resouce uri\n            if self.in_typemapper(type(data), self.anonymous):\n                handler = self.in_typemapper(type(data), self.anonymous)\n                if hasattr(handler, 'resource_uri'):\n                    url_id, fields = handler.resource_uri(data)\n\n                    try:\n                        ret['resource_uri'] = permalink(lambda: (url_id, fields))()\n                    except NoReverseMatch, e:\n                        pass\n\n            if hasattr(data, 'get_api_url') and 'resource_uri' not in ret:\n                try:\n                    ret['resource_uri'] = data.get_api_url()\n                except:\n                    pass\n\n            # absolute uri\n            if hasattr(data, 'get_absolute_url') and get_absolute_uri:\n                try:\n                    ret['absolute_uri'] = data.get_absolute_url()\n                except:\n                    pass\n\n            return ret\n\n        def _qs(data, fields=None):\n            \"\"\"\n            Querysets.\n            \"\"\"\n            return [_any(v, fields) for v in data ]\n\n        def _list(data, fields=None):\n            \"\"\"\n            Lists.\n            \"\"\"\n            return [_any(v, fields) for v in data ]\n\n        def _dict(data, fields=None):\n            \"\"\"\n            Dictionaries.\n            \"\"\"\n            return dict([(k, _any(v, fields)) for k, v in data.iteritems()])\n\n        # Kickstart the seralizin'.\n        return _any(self.data, self.fields)", "response": "Construct a new instance of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting an emitter returns the class and content - type.", "response": "def get(cls, format):\n        \"\"\"\n        Gets an emitter, returns the class and a content-type.\n        \"\"\"\n        if cls.EMITTERS.has_key(format):\n            return cls.EMITTERS.get(format)\n\n        raise ValueError(\"No emitters found for type %s\" % format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters an emitter. Parameters:: - `name`: The name of the emitter ('json', 'xml', 'yaml', ...) - `klass`: The emitter class. - `content_type`: The content type to serve response as.", "response": "def register(cls, name, klass, content_type='text/plain'):\n        \"\"\"\n        Register an emitter.\n\n        Parameters::\n         - `name`: The name of the emitter ('json', 'xml', 'yaml', ...)\n         - `klass`: The emitter class.\n         - `content_type`: The content type to serve response as.\n        \"\"\"\n        cls.EMITTERS[name] = (klass, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions for determening which emitter to use for output.", "response": "def determine_emitter(self, request, *args, **kwargs):\n        \"\"\"\n        Function for determening which emitter to use\n        for output. It lives here so you can easily subclass\n        `Resource` in order to change how emission is detected.\n\n        You could also check for the `Accept` HTTP header here,\n        since that pretty much makes sense. Refer to `Mimer` for\n        that as well.\n        \"\"\"\n        em = kwargs.pop('emitter_format', None)\n\n        if not em:\n            em = request.GET.get('format', 'json')\n\n        return em"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef form_validation_response(self, e):\n        resp = rc.BAD_REQUEST\n        resp.write(' '+str(e.form.errors))\n        return resp", "response": "Method to return form validation error information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef anonymous(self):\n        if hasattr(self.handler, 'anonymous'):\n            anon = self.handler.anonymous\n\n            if callable(anon):\n                return anon\n\n            for klass in typemapper.keys():\n                if anon == klass.__name__:\n                    return klass\n\n        return None", "response": "Gets the anonymous handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cleanup_request(request):\n        for method_type in ('GET', 'PUT', 'POST', 'DELETE'):\n            block = getattr(request, method_type, { })\n\n            if True in [ k.startswith(\"oauth_\") for k in block.keys() ]:\n                sanitized = block.copy()\n\n                for k in sanitized.keys():\n                    if k.startswith(\"oauth_\"):\n                        sanitized.pop(k)\n\n                setattr(request, method_type, sanitized)\n\n        return request", "response": "Removes oauth_ keys from various dicts on the request object and returns the sanitized version of the request object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride this method to add handling of errors customized for your needs", "response": "def error_handler(self, e, request, meth, em_format):\n        \"\"\"\n        Override this method to add handling of errors customized for your \n        needs\n        \"\"\"\n        if isinstance(e, FormValidationError):\n            return self.form_validation_response(e)\n\n        elif isinstance(e, TypeError):\n            result = rc.BAD_REQUEST\n            hm = HandlerMethod(meth)\n            sig = hm.signature\n\n            msg = 'Method signature does not match.\\n\\n'\n\n            if sig:\n                msg += 'Signature should be: %s' % sig\n            else:\n                msg += 'Resource does not expect any parameters.'\n\n            if self.display_errors:\n                msg += '\\n\\nException was: %s' % str(e)\n\n            result.content = format_error(msg)\n            return result\n        elif isinstance(e, Http404):\n            return rc.NOT_FOUND\n\n        elif isinstance(e, HttpStatusCode):\n            return e.response\n \n        else: \n            \"\"\"\n            On errors (like code errors), we'd like to be able to\n            give crash reports to both admins and also the calling\n            user. There's two setting parameters for this:\n\n            Parameters::\n             - `PISTON_EMAIL_ERRORS`: Will send a Django formatted\n               error email to people in `settings.ADMINS`.\n             - `PISTON_DISPLAY_ERRORS`: Will return a simple traceback\n               to the caller, so he can tell you what error they got.\n\n            If `PISTON_DISPLAY_ERRORS` is not enabled, the caller will\n            receive a basic \"500 Internal Server Error\" message.\n            \"\"\"\n            exc_type, exc_value, tb = sys.exc_info()\n            rep = ExceptionReporter(request, exc_type, exc_value, tb.tb_next)\n            if self.email_errors:\n                self.email_exception(rep)\n            if self.display_errors:\n                return HttpResponseServerError(\n                    format_error('\\n'.join(rep.format_exception())))\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(path):\n\n    doc = ET.parse(path).getroot()\n\n    channel = doc.find(\"./channel\")\n\n    blog = _parse_blog(channel)\n    authors = _parse_authors(channel)\n    categories = _parse_categories(channel)\n    tags = _parse_tags(channel)\n    posts = _parse_posts(channel)\n\n    return {\n        \"blog\": blog,\n        \"authors\": authors,\n        \"categories\": categories,\n        \"tags\": tags,\n        \"posts\": posts,\n    }", "response": "Parses xml and returns a dict of the content of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse and return genral blog data.", "response": "def _parse_blog(element):\n    \"\"\"\n    Parse and return genral blog data (title, tagline etc).\n    \"\"\"\n\n    title = element.find(\"./title\").text\n    tagline = element.find(\"./description\").text\n    language = element.find(\"./language\").text\n    site_url = element.find(\"./{%s}base_site_url\" % WP_NAMESPACE).text\n    blog_url = element.find(\"./{%s}base_blog_url\" % WP_NAMESPACE).text\n\n    return {\n        \"title\": title,\n        \"tagline\": tagline,\n        \"language\": language,\n        \"site_url\": site_url,\n        \"blog_url\": blog_url,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_authors(element):\n\n    authors = []\n    items = element.findall(\"./{%s}author\" % WP_NAMESPACE)\n\n    for item in items:\n        login = item.find(\"./{%s}author_login\" % WP_NAMESPACE).text\n        email = item.find(\"./{%s}author_email\" % WP_NAMESPACE).text\n        first_name = item.find(\"./{%s}author_first_name\" % WP_NAMESPACE).text\n        last_name = item.find(\"./{%s}author_last_name\" % WP_NAMESPACE).text\n        display_name = item.find(\n            \"./{%s}author_display_name\" % WP_NAMESPACE).text\n\n        authors.append({\n            \"login\": login,\n            \"email\": email,\n            \"display_name\": display_name,\n            \"first_name\": first_name,\n            \"last_name\": last_name\n        })\n\n    return authors", "response": "Parses the authors of the article."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list with categories with relations.", "response": "def _parse_categories(element):\n    \"\"\"\n    Returns a list with categories with relations.\n    \"\"\"\n    reference = {}\n    items = element.findall(\"./{%s}category\" % WP_NAMESPACE)\n\n    for item in items:\n        term_id = item.find(\"./{%s}term_id\" % WP_NAMESPACE).text\n        nicename = item.find(\"./{%s}category_nicename\" % WP_NAMESPACE).text\n        name = item.find(\"./{%s}cat_name\" % WP_NAMESPACE).text\n        parent = item.find(\"./{%s}category_parent\" % WP_NAMESPACE).text\n\n        category = {\n            \"term_id\": term_id,\n            \"nicename\": nicename,\n            \"name\": name,\n            \"parent\": parent\n        }\n\n        reference[nicename] = category\n\n    return _build_category_tree(None, reference=reference)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_category_tree(slug, reference=None, items=None):\n\n    if items is None:\n        items = []\n\n    for key in reference:\n        category = reference[key]\n\n        if category[\"parent\"] == slug:\n            children = _build_category_tree(category[\"nicename\"],\n                                            reference=reference)\n            category[\"children\"] = children\n            items.append(category)\n\n    return items", "response": "Builds a recursive tree with category relations as children."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_tags(element):\n\n    tags = []\n    items = element.findall(\"./{%s}tag\" % WP_NAMESPACE)\n\n    for item in items:\n        term_id = item.find(\"./{%s}term_id\" % WP_NAMESPACE).text\n        slug = item.find(\"./{%s}tag_slug\" % WP_NAMESPACE).text\n        name = item.find(\"./{%s}tag_name\" % WP_NAMESPACE).text\n\n        tag = {\n            \"term_id\": term_id,\n            \"slug\": slug,\n            \"name\": name,\n        }\n\n        tags.append(tag)\n\n    return tags", "response": "Retrieves and parses tags into a array / dict.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_posts(element):\n\n    posts = []\n    items = element.findall(\"item\")\n\n    for item in items:\n        title = item.find(\"./title\").text\n        link = item.find(\"./link\").text\n        pub_date = item.find(\"./pubDate\").text\n        creator = item.find(\"./{%s}creator\" % DC_NAMESPACE).text\n        guid = item.find(\"./guid\").text\n        description = item.find(\"./description\").text\n        content = item.find(\"./{%s}encoded\" % CONTENT_NAMESPACE).text\n        excerpt = item.find(\"./{%s}encoded\" % EXCERPT_NAMESPACE).text\n        post_id = item.find(\"./{%s}post_id\" % WP_NAMESPACE).text\n        post_date = item.find(\"./{%s}post_date\" % WP_NAMESPACE).text\n        post_date_gmt = item.find(\"./{%s}post_date_gmt\" % WP_NAMESPACE).text\n        status = item.find(\"./{%s}status\" % WP_NAMESPACE).text\n        post_parent = item.find(\"./{%s}post_parent\" % WP_NAMESPACE).text\n        menu_order = item.find(\"./{%s}menu_order\" % WP_NAMESPACE).text\n        post_type = item.find(\"./{%s}post_type\" % WP_NAMESPACE).text\n        post_name = item.find(\"./{%s}post_name\" % WP_NAMESPACE).text\n        is_sticky = item.find(\"./{%s}is_sticky\" % WP_NAMESPACE).text\n        ping_status = item.find(\"./{%s}ping_status\" % WP_NAMESPACE).text\n        post_password = item.find(\"./{%s}post_password\" % WP_NAMESPACE).text\n        category_items = item.findall(\"./category\")\n\n        categories = []\n        tags = []\n\n        for category_item in category_items:\n            if category_item.attrib[\"domain\"] == \"category\":\n                item_list = categories\n            else:\n                item_list = tags\n\n            item_list.append(category_item.attrib[\"nicename\"])\n\n        post = {\n            \"title\": title,\n            \"link\": link,\n            \"pub_date\": pub_date,\n            \"creator\": creator,\n            \"guid\": guid,\n            \"description\": description,\n            \"content\": content,\n            \"excerpt\": excerpt,\n            \"post_id\": post_id,\n            \"post_date\": post_date,\n            \"post_date_gmt\": post_date_gmt,\n            \"status\": status,\n            \"post_parent\": post_parent,\n            \"menu_order\": menu_order,\n            \"post_type\": post_type,\n            \"post_name\": post_name,\n            \"categories\": categories,\n            \"is_sticky\": is_sticky,\n            \"ping_status\": ping_status,\n            \"post_password\": post_password,\n            \"tags\": tags,\n        }\n\n        post[\"postmeta\"] = _parse_postmeta(item)\n        post[\"comments\"] = _parse_comments(item)\n        posts.append(post)\n\n    return posts", "response": "Parses the posts from the XML element."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_postmeta(element):\n    import phpserialize\n\n    \"\"\"\n    Retrive post metadata as a dictionary\n    \"\"\"\n\n    metadata = {}\n    fields = element.findall(\"./{%s}postmeta\" % WP_NAMESPACE)\n\n    for field in fields:\n        key = field.find(\"./{%s}meta_key\" % WP_NAMESPACE).text\n        value = field.find(\"./{%s}meta_value\" % WP_NAMESPACE).text\n\n        if key == \"_wp_attachment_metadata\":\n            stream = StringIO(value.encode())\n            try:\n                data = phpserialize.load(stream)\n                metadata[\"attachment_metadata\"] = data\n            except ValueError as e:\n                pass\n            except Exception as e:\n                raise(e)\n\n        if key == \"_wp_attached_file\":\n            metadata[\"attached_file\"] = value\n\n    return metadata", "response": "Parses the post meta element and returns a dictionary of the post metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_comments(element):\n\n    comments = []\n    items = element.findall(\"./{%s}comment\" % WP_NAMESPACE)\n\n    for item in items:\n        comment_id = item.find(\"./{%s}comment_id\" % WP_NAMESPACE).text\n        author = item.find(\"./{%s}comment_author\" % WP_NAMESPACE).text\n        email = item.find(\"./{%s}comment_author_email\" % WP_NAMESPACE).text\n        author_url = item.find(\"./{%s}comment_author_url\" % WP_NAMESPACE).text\n        author_ip = item.find(\"./{%s}comment_author_IP\" % WP_NAMESPACE).text\n        date = item.find(\"./{%s}comment_date\" % WP_NAMESPACE).text\n        date_gmt = item.find(\"./{%s}comment_date_gmt\" % WP_NAMESPACE).text\n        content = item.find(\"./{%s}comment_content\" % WP_NAMESPACE).text\n        approved = item.find(\"./{%s}comment_approved\" % WP_NAMESPACE).text\n        comment_type = item.find(\"./{%s}comment_type\" % WP_NAMESPACE).text\n        parent = item.find(\"./{%s}comment_parent\" % WP_NAMESPACE).text\n        user_id = item.find(\"./{%s}comment_user_id\" % WP_NAMESPACE).text\n\n        comment = {\n            \"id\": comment_id,\n            \"author\": author,\n            \"author_email\": email,\n            \"author_url\": author_url,\n            \"author_ip\": author_ip,\n            \"date\": date,\n            \"date_gmt\": date_gmt,\n            \"content\": content,\n            \"approved\": approved,\n            \"type\": comment_type,\n            \"parent\": parent,\n            \"user_id\": user_id,\n        }\n\n        comments.append(comment)\n\n    return comments", "response": "Returns a list with comments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens an uManager session.", "response": "def open_umanager(self):\n        \"\"\"Used to open an uManager session.\n        \n        \"\"\"\n\n        if self.umanager_opened:\n            return\n        self.ser.write(self.cmd_umanager_invocation)\n        # optimistic approach first: assume umanager is not invoked\n        if self.read_loop(lambda x: x.endswith(self.umanager_prompt),self.timeout*self.umanager_waitcoeff):\n            self.umanager_opened = True\n        else:\n            #if we are already in umanager, this will give us a fresh prompt\n            self.ser.write(self.cr)\n            if self.read_loop(lambda x: x.endswith(self.umanager_prompt),self.timeout):\n                self.umanager_opened = True\n        \n        if self.umanager_opened:\n            log.debug(\"uManager opened\")\n        else:\n            raise Dam1021Error(1,\"Failed to open uManager\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close_umanager(self, force=False):\n      \n        if not (force or self.umanager_opened):\n            return\n        # make sure we've got a fresh prompt\n        self.ser.write(self.cr)\n        if self.read_loop(lambda x: x.endswith(self.umanager_prompt),self.timeout):\n            self.ser.write(''.join((self.cmd_umanager_termination,self.cr)))\n            if self.read_loop(lambda x: x.endswith(self.buf_on_exit),self.timeout):\n                log.debug(\"uManager closed\")\n            else:\n                raise Dam1021Error(2,\"Failed to close uManager\")\n        else:\n            log.debug(\"uManager already closed\")\n            \n        self.umanager_opened = False", "response": "Used to close an uManager session."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download(self,data,um_update=False):\n        \n        self.open_umanager()\n        self.ser.write(''.join((self.cmd_download,self.cr)))\n        if self.read_loop(lambda x: x.endswith(self.xmodem_crc),self.timeout):\n            if self.xmodem.send(StringIO.StringIO(data)):\n                log.info(\"Data sent\")\n            else:\n                raise Dam1021Error(4,\"Error during file download\")\n        else:\n            raise Dam1021Error(3,\"uManager is not ready to accept a data\")\n      \n        if self.read_loop(lambda x: x.lower().find(self.reprogram_ack) != -1,self.timeout):\n            skr_sum = hashlib.sha1(data).hexdigest()\n            log.info(\"File downloaded. Data SHA-1 checksum: {}\".format(skr_sum))\n        else:\n            raise Dam1021Error(5,\"uManager accepted data and not reprogrammed\")\n\n        if um_update:\n            self.ser.write(''.join((self.cmd_update,self.cr)))\n            if self.read_loop(lambda x: x.lower().find(self.update_confirmation) != -1,self.timeout*self.umanager_waitcoeff):\n                self.ser.write(self.update_ack)\n            else:\n                raise Dam1021Error(13,\"Error during update command invocation\")\n\n            if self.read_loop(lambda x: x.lower().find(self.update_reset) != -1,self.timeout*self.umanager_waitcoeff):\n                log.info(\"uManager updated\")\n            else:\n                raise Dam1021Error(14,\"Update failed\")\n        else:\n            self.close_umanager()            \n\n\n        return skr_sum", "response": "This function downloads the firmware or filter set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_current_filter_set(self,raw=False):\n        \n        buf = []\n\n        self.open_umanager()\n        self.ser.write(''.join((self.cmd_current_filter_list,self.cr)))\n        if self.read_loop(lambda x: x.endswith(self.umanager_prompt),self.timeout,lambda x,y,z: buf.append(y.rstrip()[:-1])):\n            if raw:\n                rv = buf = buf[0]\n            else:\n                rv, buf = self.filter_organizer(buf[0])\n        else:\n            raise Dam1021Error(16,\"Failed to list currently selected filter set\")\n        self.close_umanager()\n\n        log.info(buf)\n\n        return rv", "response": "User to list a currently selected filter set"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lrange(self, key, start, end):\n        return self._execute([b'LRANGE', key, start, end])", "response": "Returns the specified elements of the list stored at key.\n\n        :param key: The list's key\n        :type key: :class:`str`, :class:`bytes`\n        :param int start: zero-based index to start retrieving elements from\n        :param int end: zero-based index at which to stop retrieving elements\n\n        :rtype: list\n        :raises: :exc:`~tredis.exceptions.TRedisException`\n\n        The offsets start and stop are zero-based indexes, with 0 being the\n        first element of the list (the head of the list), 1 being the next\n        element and so on.\n\n        These offsets can also be negative numbers indicating offsets\n        starting at the end of the list. For example, -1 is the last element\n        of the list, -2 the penultimate, and so on.\n\n        Note that if you have a list of numbers from 0 to 100,\n        ``lrange(key, 0, 10)`` will return 11 elements, that is, the\n        rightmost item is included. This may or may not be consistent with\n        behavior of range-related functions in your programming language of\n        choice (think Ruby's ``Range.new``, ``Array#slice`` or Python's\n        :func:`range` function).\n\n        Out of range indexes will not produce an error. If start is larger\n        than the end of the list, an empty list is returned. If stop is\n        larger than the actual end of the list, Redis will treat it like the\n        last element of the list.\n\n        .. note::\n\n           **Time complexity** ``O(S+N)`` where ``S`` is the distance of\n           start offset from ``HEAD`` for small lists, from nearest end\n           (``HEAD`` or ``TAIL``) for large lists; and ``N`` is the number\n           of elements in the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ltrim(self, key, start, stop):\n        return self._execute([b'LTRIM', key, start, stop], b'OK')", "response": "Removes the specified range of elements from the list stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lpush(self, key, *values):\n        return self._execute([b'LPUSH', key] + list(values))", "response": "Insert all the specified values at the head of the list stored at key.\n\n        :param key: The list's key\n        :type key: :class:`str`, :class:`bytes`\n        :param values: One or more positional arguments to insert at the\n            beginning of the list.  Each value is inserted at the beginning\n            of the list individually (see discussion below).\n        :returns: the length of the list after push operations\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.TRedisException`\n\n        If `key` does not exist, it is created as empty list before\n        performing the push operations. When key holds a value that is not a\n        list, an error is returned.\n\n        It is possible to push multiple elements using a single command call\n        just specifying multiple arguments at the end of the command.\n        Elements are inserted one after the other to the head of the list,\n        from the leftmost element to the rightmost element. So for instance\n        ``client.lpush('mylist', 'a', 'b', 'c')`` will result into a list\n        containing ``c`` as first element, ``b`` as second element and ``a``\n        as third element.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert values at the head of an existing list. :param key: The list's key :type key: :class:`str`, :class:`bytes` :param values: One or more positional arguments to insert at the beginning of the list. Each value is inserted at the beginning of the list individually (see discussion below). :returns: the length of the list after push operations, zero if `key` does not refer to a list :rtype: int :raises: :exc:`~tredis.exceptions.TRedisException` This method inserts `values` at the head of the list stored at `key`, only if `key` already exists and holds a list. In contrary to :meth:`.lpush`, no operation will be performed when key does not yet exist. .. note:: **Time complexity**: ``O(1)``", "response": "def lpushx(self, key, *values):\n        \"\"\"\n        Insert values at the head of an existing list.\n\n        :param key: The list's key\n        :type key: :class:`str`, :class:`bytes`\n        :param values: One or more positional arguments to insert at the\n            beginning of the list.  Each value is inserted at the beginning\n            of the list individually (see discussion below).\n        :returns: the length of the list after push operations, zero if\n            `key` does not refer to a list\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.TRedisException`\n\n        This method inserts `values` at the head of the list stored at `key`,\n        only if `key` already exists and holds a list. In contrary to\n        :meth:`.lpush`, no operation will be performed when key does not yet\n        exist.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``\n\n        \"\"\"\n        return self._execute([b'LPUSHX', key] + list(values))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts all the specified values at the tail of the list stored at key. :param key: The list's key :type key: :class:`str`, :class:`bytes` :param values: One or more positional arguments to insert at the tail of the list. :returns: the length of the list after push operations :rtype: int :raises: :exc:`~tredis.exceptions.TRedisException` If `key` does not exist, it is created as empty list before performing the push operation. When `key` holds a value that is not a list, an error is returned. It is possible to push multiple elements using a single command call just specifying multiple arguments at the end of the command. Elements are inserted one after the other to the tail of the list, from the leftmost element to the rightmost element. So for instance the command ``client.rpush('mylist', 'a', 'b', 'c')`` will result in a list containing ``a`` as first element, ``b`` as second element and ``c`` as third element. .. note:: **Time complexity**: ``O(1)``", "response": "def rpush(self, key, *values):\n        \"\"\"\n        Insert all the specified values at the tail of the list stored at key.\n\n        :param key: The list's key\n        :type key: :class:`str`, :class:`bytes`\n        :param values: One or more positional arguments to insert at the\n            tail of the list.\n        :returns: the length of the list after push operations\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.TRedisException`\n\n        If `key` does not exist, it is created as empty list before performing\n        the push operation. When `key` holds a value that is not a list, an\n        error is returned.\n\n        It is possible to push multiple elements using a single command call\n        just specifying multiple arguments at the end of the command.\n        Elements are inserted one after the other to the tail of the list,\n        from the leftmost element to the rightmost element. So for instance\n        the command  ``client.rpush('mylist', 'a', 'b', 'c')`` will result\n        in a list containing ``a`` as first element, ``b`` as second element\n        and ``c`` as third element.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``\n\n        \"\"\"\n        return self._execute([b'RPUSH', key] + list(values))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninserts values at the tail of an existing list. :param key: The list's key :type key: :class:`str`, :class:`bytes` :param values: One or more positional arguments to insert at the tail of the list. :returns: the length of the list after push operations or zero if `key` does not refer to a list :rtype: int :raises: :exc:`~tredis.exceptions.TRedisException` This method inserts value at the tail of the list stored at `key`, only if `key` already exists and holds a list. In contrary to method:`.rpush`, no operation will be performed when `key` does not yet exist. .. note:: **Time complexity**: ``O(1)``", "response": "def rpushx(self, key, *values):\n        \"\"\"\n        Insert values at the tail of an existing list.\n\n        :param key: The list's key\n        :type key: :class:`str`, :class:`bytes`\n        :param values: One or more positional arguments to insert at the\n            tail of the list.\n        :returns: the length of the list after push operations or\n            zero if `key` does not refer to a list\n        :rtype: int\n        :raises: :exc:`~tredis.exceptions.TRedisException`\n\n        This method inserts value at the tail of the list stored at `key`,\n        only if `key` already exists and holds a list. In contrary to\n        method:`.rpush`, no operation will be performed when `key` does not\n        yet exist.\n\n        .. note::\n\n           **Time complexity**: ``O(1)``\n\n        \"\"\"\n        return self._execute([b'RPUSHX', key] + list(values))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_date_optional_time(obj):\n    if isinstance(obj, datetime.datetime):\n        return timezone(\"UTC\").normalize(obj.astimezone(timezone(\"UTC\"))).strftime('%Y-%m-%dT%H:%M:%SZ')\n    raise TypeError(\"{0} is not JSON serializable\".format(repr(obj)))", "response": "Encode a date or time object into a ISO - encoded string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a dict with all kwargs of the `copy_file` or `link_file` method of the super class and add it to the queue for later processing.", "response": "def file_handler(self, handler_type, path, prefixed_path, source_storage):\n        \"\"\"\n        Create a dict with all kwargs of the `copy_file` or `link_file` method of the super class and add it to\n        the queue for later processing.\n        \"\"\"\n        if self.faster:\n            if prefixed_path not in self.found_files:\n                self.found_files[prefixed_path] = (source_storage, path)\n\n            self.task_queue.put({\n                'handler_type': handler_type,\n                'path': path,\n                'prefixed_path': prefixed_path,\n                'source_storage': source_storage\n            })\n            self.counter += 1\n        else:\n            if handler_type == 'link':\n                super(Command, self).link_file(path, prefixed_path, source_storage)\n            else:\n                super(Command, self).copy_file(path, prefixed_path, source_storage)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_file(self, path, prefixed_path, source_storage):\n        if self.faster:\n            return True\n        else:\n            return super(Command, self).delete_file(path, prefixed_path, source_storage)", "response": "Delete a file from the local cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates some concurrent workers that process the tasks simultaneously.", "response": "def collect(self):\n        \"\"\"\n        Create some concurrent workers that process the tasks simultaneously.\n        \"\"\"\n        collected = super(Command, self).collect()\n        if self.faster:\n            self.worker_spawn_method()\n            self.post_processor()\n        return collected"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gevent_spawn(self):\n        monkey.patch_all(thread=False)\n        joinall([spawn(self.gevent_worker) for x in range(self.queue_worker_amount)])", "response": "Spawn worker threads using gevent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gevent_worker(self):\n        while not self.task_queue.empty():\n            task_kwargs = self.task_queue.get()\n            handler_type = task_kwargs.pop('handler_type')\n\n            if handler_type == 'link':\n                super(Command, self).link_file(**task_kwargs)\n            else:\n                super(Command, self).copy_file(**task_kwargs)", "response": "Process one task after another by calling the handler method of the super class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nspawning worker processes using multiprocessing", "response": "def mp_spawn(self):\n        \"\"\" Spawn worker processes (using multiprocessing) \"\"\"\n        processes = []\n        for x in range(self.queue_worker_amount):\n            process = multiprocessing.Process(target=self.mp_worker)\n            process.start()\n            processes.append(process)\n        for process in processes:\n            process.join()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mp_worker(self):\n        while not self.task_queue.empty():\n            task_kwargs = self.task_queue.get()\n            handler_type = task_kwargs.pop('handler_type')\n\n            if handler_type == 'link':\n                super(Command, self).link_file(**task_kwargs)\n            else:\n                super(Command, self).copy_file(**task_kwargs)\n\n            self.task_queue.task_done()", "response": "Process one task after another by calling the handler method of the super class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_github_hooks(github_url='https://api.github.com'):\n    try:\n        resp = requests.get(github_url + '/meta')\n        if resp.status_code == 200:\n            return resp.json()['hooks']\n        else:\n            if resp.headers.get('X-RateLimit-Remaining') == '0':\n                reset_ts = int(resp.headers['X-RateLimit-Reset'])\n                reset_string = time.strftime('%a, %d %b %Y %H:%M:%S GMT',\n                                             time.gmtime(reset_ts))\n                raise ServiceUnavailable('Rate limited from GitHub until ' +\n                                         reset_string)\n            else:\n                raise ServiceUnavailable('Error reaching GitHub')\n    except (KeyError, ValueError, requests.exceptions.ConnectionError):\n        raise ServiceUnavailable('Error reaching GitHub')", "response": "Request GitHub s IP block from their API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_github_ip(ip_str):\n    if isinstance(ip_str, bytes):\n        ip_str = ip_str.decode()\n\n    ip = ipaddress.ip_address(ip_str)\n    if ip.version == 6 and ip.ipv4_mapped:\n        ip = ip.ipv4_mapped\n\n    for block in load_github_hooks():\n        if ip in ipaddress.ip_network(block):\n            return True\n    return False", "response": "Verify that an IP address is owned by GitHub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_signature(signature, key, data):\n    if isinstance(key, type(u'')):\n        key = key.encode()\n\n    digest = 'sha1=' + hmac.new(key, data, hashlib.sha1).hexdigest()\n\n    # Covert everything to byte sequences\n    if isinstance(digest, type(u'')):\n        digest = digest.encode()\n    if isinstance(signature, type(u'')):\n        signature = signature.encode()\n\n    return werkzeug.security.safe_str_cmp(digest, signature)", "response": "Compute the HMAC signature and test against a given hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister the URL route to the application.", "response": "def init_app(self, app, url='/hooks'):\n        \"\"\"Register the URL route to the application.\n\n        :param app: the optional :class:`~flask.Flask` instance to\n                register the extension\n        :param url: the url that events will be posted to\n        \"\"\"\n        app.config.setdefault('VALIDATE_IP', True)\n        app.config.setdefault('VALIDATE_SIGNATURE', True)\n\n        @app.route(url, methods=['POST'])\n        def hook():\n            if app.config['VALIDATE_IP']:\n                if not is_github_ip(request.remote_addr):\n                    raise Forbidden('Requests must originate from GitHub')\n\n            if app.config['VALIDATE_SIGNATURE']:\n                key = app.config.get('GITHUB_WEBHOOKS_KEY', app.secret_key)\n                signature = request.headers.get('X-Hub-Signature')\n\n                if hasattr(request, 'get_data'):\n                    # Werkzeug >= 0.9\n                    payload = request.get_data()\n                else:\n                    payload = request.data\n\n                if not signature:\n                    raise BadRequest('Missing signature')\n\n                if not check_signature(signature, key, payload):\n                    raise BadRequest('Wrong signature')\n\n            event = request.headers.get('X-GitHub-Event')\n            guid = request.headers.get('X-GitHub-Delivery')\n            if not event:\n                raise BadRequest('Missing header: X-GitHub-Event')\n            elif not guid:\n                raise BadRequest('Missing header: X-GitHub-Delivery')\n\n            if hasattr(request, 'get_json'):\n                # Flask >= 0.10\n                data = request.get_json()\n            else:\n                data = request.json\n\n            if event in self._hooks:\n                return self._hooks[event](data, guid)\n            else:\n                return 'Hook not used\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_hook(self, hook_name, fn):\n        if hook_name not in self._hooks:\n            self._hooks[hook_name] = fn\n        else:\n            raise Exception('%s hook already registered' % hook_name)", "response": "Register a function to be called on a GitHub event."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hook(self, hook_name):\n        def wrapper(fn):\n            self.register_hook(hook_name, fn)\n            return fn\n        return wrapper", "response": "A decorator that can be used to register a new hook handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept(self):\n        sock, address = self.sock.accept()\n        wsock = websocket(sock)\n        wsock.secure = self.secure\n        ServerHandshake(wsock).perform(self)\n        wsock.handshake_sent = True\n        return wsock, address", "response": "Returns a websocket instance and address of the socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, address):\n        self.sock.connect(address)\n        ClientHandshake(self).perform()\n        self.handshake_sent = True", "response": "Equivalent to socket. connect but sends a client handshake request\n        after connecting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send(self, *args):\n        for frame in args:\n            self.sock.sendall(self.apply_send_hooks(frame, False).pack())", "response": "Send a number of frames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_async_send(self):\n        assert len(self.sendbuf)\n\n        nwritten = self.sock.send(self.sendbuf)\n        nframes = 0\n\n        for entry in self.sendbuf_frames:\n            frame, offset, callback = entry\n\n            if offset <= nwritten:\n                nframes += 1\n\n                if callback:\n                    callback()\n            else:\n                entry[1] -= nwritten\n\n        self.sendbuf = self.sendbuf[nwritten:]\n        self.sendbuf_frames = self.sendbuf_frames[nframes:]", "response": "Send any queued data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreceives any completed frames from the socket. This function should only be called after a read event on a file descriptor.", "response": "def do_async_recv(self, bufsize):\n        \"\"\"\n        Receive any completed frames from the socket. This function should only\n        be called after a read event on a file descriptor.\n        \"\"\"\n        data = self.sock.recv(bufsize)\n\n        if len(data) == 0:\n            raise socket.error('no data to receive')\n\n        self.recvbuf += data\n\n        while contains_frame(self.recvbuf):\n            frame, self.recvbuf = pop_frame(self.recvbuf)\n            frame = self.apply_recv_hooks(frame, False)\n\n            if not self.recv_callback:\n                raise ValueError('no callback installed for %s' % frame)\n\n            self.recv_callback(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenabling SSL for this socket.", "response": "def enable_ssl(self, *args, **kwargs):\n        \"\"\"\n        Transforms the regular socket.socket to an ssl.SSLSocket for secure\n        connections. Any arguments are passed to ssl.wrap_socket:\n        http://docs.python.org/dev/library/ssl.html#ssl.wrap_socket\n        \"\"\"\n        if self.handshake_sent:\n            raise SSLError('can only enable SSL before handshake')\n\n        self.secure = True\n        self.sock = ssl.wrap_socket(self.sock, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nis the nonce one that was generated by this library using the provided secret?", "response": "def validate_nonce(nonce, secret):\n    '''\n    Is the nonce one that was generated by this library using the provided secret?\n    '''\n    nonce_components = nonce.split(':', 2)\n    if not len(nonce_components) == 3:\n        return False\n    timestamp = nonce_components[0]\n    salt = nonce_components[1]\n    nonce_signature = nonce_components[2]\n\n    calculated_nonce = calculate_nonce(timestamp, secret, salt)\n\n    if not nonce == calculated_nonce:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_partial_digest(username, realm, password):\n    '''\n    Calculate a partial digest that may be stored and used to authenticate future\n    HTTP Digest sessions.\n    '''\n    return md5.md5(\"%s:%s:%s\" % (username.encode('utf-8'), realm, password.encode('utf-8'))).hexdigest()", "response": "Calculate a partial digest that may be stored and used to authenticate future\n    HTTP Digest sessions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a Digest challenge that can be sent to the client.", "response": "def build_digest_challenge(timestamp, secret, realm, opaque, stale):\n    '''\n    Builds a Digest challenge that may be sent as the value of the 'WWW-Authenticate' header\n    in a 401 or 403 response.\n\n    'opaque' may be any value - it will be returned by the client.\n\n    'timestamp' will be incorporated and signed in the nonce - it may be retrieved from the\n    client's authentication request using get_nonce_timestamp()\n    '''\n    nonce = calculate_nonce(timestamp, secret)\n\n    return 'Digest %s' % format_parts(realm=realm, qop='auth', nonce=nonce,\n                                      opaque=opaque, algorithm='MD5',\n                                      stale=stale and 'true' or 'false')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_request_digest(method, partial_digest, digest_response=None,\n                             uri=None, nonce=None, nonce_count=None, client_nonce=None):\n    '''\n    Calculates a value for the 'response' value of the client authentication request.\n    Requires the 'partial_digest' calculated from the realm, username, and password.\n\n    Either call it with a digest_response to use the values from an authentication request,\n    or pass the individual parameters (i.e. to generate an authentication request).\n    '''\n    if digest_response:\n        if uri or nonce or nonce_count or client_nonce:\n            raise Exception(\"Both digest_response and one or more \"\n                            \"individual parameters were sent.\")\n        uri = digest_response.uri\n        nonce = digest_response.nonce\n        nonce_count = digest_response.nc\n        client_nonce=digest_response.cnonce\n    elif not (uri and nonce and (nonce_count != None) and client_nonce):\n        raise Exception(\"Neither digest_response nor all individual parameters were sent.\")\n\n    ha2 = md5.md5(\"%s:%s\" % (method, uri)).hexdigest()\n    data = \"%s:%s:%s:%s:%s\" % (nonce, \"%08x\" % nonce_count, client_nonce, 'auth', ha2)\n    kd = md5.md5(\"%s:%s\" % (partial_digest, data)).hexdigest()\n    return kd", "response": "Calculates a request digest for the client authentication request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the timestamp from a Nonce.", "response": "def get_nonce_timestamp(nonce):\n    '''\n    Extract the timestamp from a Nonce. To be sure the timestamp was generated by this site,\n    make sure you validate the nonce using validate_nonce().\n    '''\n    components = nonce.split(':',2)\n    if not len(components) == 3:\n        return None\n\n    try:\n        return float(components[0])\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a nonce using the provided timestamp secret and salt.", "response": "def calculate_nonce(timestamp, secret, salt=None):\n    '''\n    Generate a nonce using the provided timestamp, secret, and salt. If the salt is not provided,\n    (and one should only be provided when validating a nonce) one will be generated randomly\n    in order to ensure that two simultaneous requests do not generate identical nonces.\n    '''\n    if not salt:\n        salt = ''.join([random.choice('0123456789ABCDEF') for x in range(4)])\n    return \"%s:%s:%s\" % (timestamp, salt,\n                         md5.md5(\"%s:%s:%s\" % (timestamp, salt, secret)).hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds an authorization request.", "response": "def build_authorization_request(username, method, uri, nonce_count, digest_challenge=None,\n                                realm=None, nonce=None, opaque=None, password=None,\n                                request_digest=None, client_nonce=None):\n    '''\n    Builds an authorization request that may be sent as the value of the 'Authorization'\n    header in an HTTP request.\n\n    Either a digest_challenge object (as returned from parse_digest_challenge) or its required\n    component parameters (nonce, realm, opaque) must be provided.\n\n    The nonce_count should be the last used nonce_count plus one.\n\n    Either the password or the request_digest should be provided - if provided, the password\n    will be used to generate a request digest. The client_nonce is optional - if not provided,\n    a random value will be generated.\n    '''\n    if not client_nonce:\n        client_nonce =  ''.join([random.choice('0123456789ABCDEF') for x in range(32)])\n\n    if digest_challenge and (realm or nonce or opaque):\n        raise Exception(\"Both digest_challenge and one or more of realm, nonce, and opaque\"\n                        \"were sent.\")\n\n    if digest_challenge:\n        if isinstance(digest_challenge, types.StringType):\n            digest_challenge_header = digest_challenge\n            digest_challenge = parse_digest_challenge(digest_challenge_header)\n            if not digest_challenge:\n                raise Exception(\"The provided digest challenge header could not be parsed: %s\" %\n                                digest_challenge_header)\n        realm = digest_challenge.realm\n        nonce = digest_challenge.nonce\n        opaque = digest_challenge.opaque\n    elif not (realm and nonce and opaque):\n        raise Exception(\"Either digest_challenge or realm, nonce, and opaque must be sent.\")\n\n    if password and request_digest:\n        raise Exception(\"Both password and calculated request_digest were sent.\")\n    elif not request_digest:\n        if not password:\n            raise Exception(\"Either password or calculated request_digest must be provided.\")\n\n        partial_digest = calculate_partial_digest(username, realm, password)\n        request_digest = calculate_request_digest(method, partial_digest, uri=uri, nonce=nonce,\n                                                  nonce_count=nonce_count,\n                                                  client_nonce=client_nonce)\n\n    return 'Digest %s' % format_parts(username=username, realm=realm, nonce=nonce, uri=uri,\n                                      response=request_digest, algorithm='MD5', opaque=opaque,\n                                      qop='auth', nc='%08x' % nonce_count, cnonce=client_nonce)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the parameters of a Digest response string.", "response": "def parse_digest_response(digest_response_string):\n    '''\n    Parse the parameters of a Digest response. The input is a comma separated list of\n    token=(token|quoted-string). See RFCs 2616 and 2617 for details.\n\n    Known issue: this implementation will fail if there are commas embedded in quoted-strings.\n    '''\n\n    parts = parse_parts(digest_response_string, defaults={'algorithm': 'MD5'})\n    if not _check_required_parts(parts, _REQUIRED_DIGEST_RESPONSE_PARTS):\n        return None\n\n    if not parts['nc'] or [c for c in parts['nc'] if not c in '0123456789abcdefABCDEF']:\n        return None\n    parts['nc'] = int(parts['nc'], 16)\n\n    digest_response = _build_object_from_parts(parts, _REQUIRED_DIGEST_RESPONSE_PARTS)\n    if ('MD5', 'auth') != (digest_response.algorithm, digest_response.qop):\n        return None\n\n    return digest_response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_digest_challenge(authentication_header):\n    '''\n    Parses the value of a 'WWW-Authenticate' header. Returns an object with properties\n    corresponding to each of the recognized parameters in the header.\n    '''\n    if not is_digest_challenge(authentication_header):\n        return None\n\n    parts = parse_parts(authentication_header[7:], defaults={'algorithm': 'MD5',\n                                                             'stale': 'false'})\n    if not _check_required_parts(parts, _REQUIRED_DIGEST_CHALLENGE_PARTS):\n        return None\n\n    parts['stale'] = parts['stale'].lower() == 'true'\n\n    digest_challenge = _build_object_from_parts(parts, _REQUIRED_DIGEST_CHALLENGE_PARTS)\n    if ('MD5', 'auth') != (digest_challenge.algorithm, digest_challenge.qop):\n        return None\n\n    return digest_challenge", "response": "Parses the value of a WWW - Authenticate header. Returns an object with properties\n    corresponding to each of the recognized parameters in the header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over edges in current : class:`BreakpointGraph` instance.", "response": "def __edges(self, nbunch=None, keys=False):\n        \"\"\" Iterates over edges in current :class:`BreakpointGraph` instance.\n\n        Returns a generator over the edges in current :class:`BreakpointGraph` instance producing instances of :class:`bg.edge.BGEdge` instances wrapping around information in underlying MultiGraph object.\n\n        :param nbunch: a vertex to iterate over edges outgoing from, if not provided,iteration over all edges is performed.\n        :type nbuch: any hashable python object\n        :param keys: a flag to indicate if information about unique edge's ids has to be returned alongside with edge\n        :type keys: ``Boolean``\n        :return: generator over edges in current :class:`BreakpointGraph`\n        :rtype: ``generator``\n        \"\"\"\n        for v1, v2, key, data in self.bg.edges(nbunch=nbunch, data=True, keys=True):\n            bgedge = BGEdge(vertex1=v1, vertex2=v2, multicolor=data[\"attr_dict\"][\"multicolor\"],\n                            data=data[\"attr_dict\"][\"data\"])\n            if not keys:\n                yield bgedge\n            else:\n                yield bgedge, key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef edges(self, nbunch=None, keys=False):\n        for entry in self.__edges(nbunch=nbunch, keys=keys):\n            yield entry", "response": "Iterate over edges in current BreakpointGraph instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_edge(self, vertex1, vertex2, multicolor, merge=True, data=None):\n        self.__add_bgedge(BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=multicolor, data=data), merge=merge)", "response": "Adds a new edge between two vertices and a multi - color perspective."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __add_bgedge(self, bgedge, merge=True):\n        if bgedge.vertex1 in self.bg and bgedge.vertex2 in self.bg[bgedge.vertex1] and merge:\n            key = min(self.bg[bgedge.vertex1][bgedge.vertex2].keys())\n            self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"multicolor\"] += bgedge.multicolor\n            self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"data\"] = {}\n        else:\n            self.bg.add_edge(bgedge.vertex1, bgedge.vertex2, attr_dict={\"multicolor\": deepcopy(bgedge.multicolor),\n                                                                        \"data\": bgedge.data})\n        self.cache_valid[\"overall_set_of_colors\"] = False", "response": "Adds supplied bgedge to current instance of BreakpointGraph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds supplied bgedge object to current instance of BreakpointGraph.", "response": "def add_bgedge(self, bgedge, merge=True):\n        \"\"\" Adds supplied :class:`bg.edge.BGEdge` object to current instance of :class:`BreakpointGraph`.\n\n        Proxies a call to :meth:`BreakpointGraph._BreakpointGraph__add_bgedge` method.\n\n        :param bgedge: instance of :class:`bg.edge.BGEdge` infromation form which is to be added to current :class:`BreakpointGraph`\n        :type bgedge: :class:`bg.edge.BGEdge`\n        :param merge: a flag to merge supplied information from multi-color perspective into a first existing edge between two supplied vertices\n        :type merge: ``Boolean``\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        self.__add_bgedge(bgedge=bgedge, merge=merge)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_vertex_by_name(self, vertex_name):\n        vertex_class = BGVertex.get_vertex_class_from_vertex_name(vertex_name)\n        data = vertex_name.split(BlockVertex.NAME_SEPARATOR)\n        root_name, data = data[0], data[1:]\n        if issubclass(vertex_class, TaggedVertex):\n            tags = [entry.split(TaggedVertex.TAG_SEPARATOR) for entry in data]\n            for tag_entry in tags:\n                if len(tag_entry) == 1:\n                    tag_entry.append(None)\n                elif len(tag_entry) > 2:\n                    tag_entry[1:] = [TaggedVertex.TAG_SEPARATOR.join(tag_entry[1:])]\n            result = vertex_class(root_name)\n            for tag, value in tags:\n                if tag == InfinityVertex.NAME_SUFFIX and issubclass(vertex_class, InfinityVertex):\n                    continue\n                result.add_tag(tag, value)\n        else:\n            result = vertex_class(root_name)\n\n        if result in self.bg:\n            adjacencies = self.bg[result]\n            for key, _ in adjacencies.items():\n                for ref_key, values in self.bg[key].items():\n                    if ref_key == result:\n                        return ref_key\n            return list(self.bg[result].keys())[0]\n        return None", "response": "Gets a vertex object by its name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an edge between two vertices.", "response": "def __get_edge_by_two_vertices(self, vertex1, vertex2, key=None):\n        \"\"\" Returns an instance of :class:`bg.edge.BBGEdge` edge between to supplied vertices (if ``key`` is supplied, returns a :class:`bg.edge.BBGEdge` instance about specified edge).\n\n        Checks that both specified vertices are in current :class:`BreakpointGraph` and then depending on ``key`` argument, creates a new :class:`bg.edge.BBGEdge` instance and incorporates respective multi-color information into it.\n\n        :param vertex1: first vertex instance out of two in current :class:`BreakpointGraph`\n        :type vertex1: any hashable object\n        :param vertex2: second vertex instance out of two in current :class:`BreakpointGraph`\n        :type vertex2: any hashable object\n        :param key: unique identifier of edge of interested to be retrieved from current :class:`BreakpointGraph`\n        :type key: any python object. ``None`` or ``int`` is expected\n        :return: edge between two specified edges respecting a ``key`` argument.\n        :rtype: :class:`bg.edge.BGEdge`\n        \"\"\"\n        if vertex1 in self.bg and vertex2 in self.bg[vertex1]:\n            if key is None:\n                key = min(self.bg[vertex1][vertex2])\n            return BGEdge(vertex1=vertex1, vertex2=vertex2,\n                          multicolor=self.bg[vertex1][vertex2][key][\"attr_dict\"][\"multicolor\"],\n                          data=self.bg[vertex1][vertex2][key][\"attr_dict\"][\"data\"])\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_edge_by_two_vertices(self, vertex1, vertex2, key=None):\n        return self.__get_edge_by_two_vertices(vertex1=vertex1, vertex2=vertex2, key=key)", "response": "Returns an edge between two vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_edges_by_vertex(self, vertex, keys=False):\n        if vertex in self.bg:\n            for vertex2, edges in self.bg[vertex].items():\n                for key, data in self.bg[vertex][vertex2].items():\n                    bg_edge = BGEdge(vertex1=vertex, vertex2=vertex2, multicolor=data[\"attr_dict\"][\"multicolor\"],\n                                     data=data[\"attr_dict\"][\"data\"])\n                    if keys:\n                        yield bg_edge, key\n                    else:\n                        yield bg_edge", "response": "Returns an iterator over all edges that are incident to the supplied vertex."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_edges_by_vertex(self, vertex, keys=False):\n        for entry in self.__get_edges_by_vertex(vertex=vertex, keys=keys):\n            yield entry", "response": "Returns an iterator over edges that are incident to supplied vertex."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __edges_between_two_vertices(self, vertex1, vertex2, keys=False):\n        for vertex in vertex1, vertex2:\n            if vertex not in self.bg:\n                raise ValueError(\"Supplied vertex ({vertex_name}) is not present in current BreakpointGraph\"\n                                 \"\".format(vertex_name=str(vertex.name)))\n        for bgedge, key in self.__get_edges_by_vertex(vertex=vertex1, keys=True):\n            if bgedge.vertex2 == vertex2:\n                if keys:\n                    yield bgedge, key\n                else:\n                    yield bgedge", "response": "Yields all edges that are located between two supplied vertices in current BreakpointGraph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator over edges between two supplied vertices in current BreakpointGraph.", "response": "def edges_between_two_vertices(self, vertex1, vertex2, keys=False):\n        \"\"\" Iterates over edges between two supplied vertices in current :class:`BreakpointGraph`\n\n        Proxies a call to :meth:`Breakpoint._Breakpoint__edges_between_two_vertices` method.\n\n        :param vertex1: a first vertex out of two, edges of interest are incident to\n        :type vertex1: any hashable object, :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second vertex out of two, edges of interest are incident to\n        :type vertex2: any hashable object, :class:`bg.vertex.BGVertex` is expected\n        :param keys: a flag to indicate if information about unique edge's ids has to be returned alongside with edge\n        :type keys: ``Boolean``\n        :return: generator over edges (tuples ``edge, edge_id`` if keys specified) between two supplied vertices in current :class:`BreakpointGraph` wrapped in :class:`bg.vertex.BGVertex`\n        :rtype: ``generator``\n        \"\"\"\n        for entry in self.__edges_between_two_vertices(vertex1=vertex1, vertex2=vertex2, keys=keys):\n            yield entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connected_components_subgraphs(self, copy=True):\n        for component in nx.connected_components(self.bg):\n            component = self.bg.subgraph(component)\n            if copy:\n                component.copy()\n            yield BreakpointGraph(component)", "response": "Returns a generator over all connected components in current BreakpointGraph object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a supplied bgedge from a perspective of multi - color substitution.", "response": "def __delete_bgedge(self, bgedge, key=None, keep_vertices=False):\n        \"\"\" Deletes a supplied :class:`bg.edge.BGEdge` from a perspective of multi-color substitution. If unique identifier ``key`` is not provided, most similar (from perspective of :meth:`bg.multicolor.Multicolor.similarity_score` result) edge between respective vertices is chosen for change.\n\n        If no unique identifier for edge to be changed is specified, edge to be updated is determined by iterating over all edges between vertices in supplied :class:`bg.edge.BGEdge` instance and the edge with most similarity score to supplied one is chosen.\n        Once the edge to be substituted from is determined, substitution if performed form a perspective of :class:`bg.multicolor.Multicolor` substitution.\n        If after substitution the remaining multicolor of respective edge is empty, such edge is deleted form a perspective of MultiGraph edge deletion.\n\n        :param bgedge: an edge to be deleted from a perspective of multi-color substitution\n        :type bgedge: :class:`bg.edge.BGEdge`\n        :param key: unique identifier of existing edges in current :class:`BreakpointGraph` instance to be changed\n        :type: any python object. ``int`` is expected.\n        :return: ``None``, performed inplace changes.\n        \"\"\"\n        ############################################################################################################\n        #\n        # determines which edge to delete\n        # candidate edges setup\n        #\n        ############################################################################################################\n\n        if key is not None:\n            ############################################################################################################\n            #\n            # is an edge specific key is provided, only edge with that key can undergo multicolor deletion\n            # even if that edge is not the most suited to the edge to be deleted\n            #\n            ############################################################################################################\n            self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"multicolor\"] -= bgedge.multicolor\n            if len(self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"multicolor\"].multicolors) == 0:\n                ############################################################################################################\n                #\n                # since edge deletion correspond to multicolor substitution one must make sure\n                # that no edges with empty multicolor are left in the graph\n                #\n                ############################################################################################################\n                self.bg.remove_edge(v=bgedge.vertex1, u=bgedge.vertex2, key=key)\n                if keep_vertices:\n                    self.bg.add_node(bgedge.vertex1)\n                    self.bg.add_node(bgedge.vertex2)\n        else:\n            candidate_data, candidate_id, candidate_score = self.__determine_most_suitable_edge_for_deletion(bgedge)\n            if candidate_data is not None:\n                candidate_data[\"attr_dict\"][\"multicolor\"] -= bgedge.multicolor\n                if len(self.bg[bgedge.vertex1][bgedge.vertex2][candidate_id][\"attr_dict\"][\n                           \"multicolor\"].multicolors) == 0:\n                    self.bg.remove_edge(v=bgedge.vertex1, u=bgedge.vertex2, key=candidate_id)\n                    if keep_vertices:\n                        self.bg.add_node(bgedge.vertex1)\n                        self.bg.add_node(bgedge.vertex2)\n        self.cache_valid[\"overall_set_of_colors\"] = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_edge(self, vertex1, vertex2, multicolor, key=None):\n        self.__delete_bgedge(bgedge=BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=multicolor), key=key)", "response": "Deletes an edge between two vertices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_bgedge(self, bgedge, key=None):\n        self.__delete_bgedge(bgedge=bgedge, key=key)", "response": "Deletes a supplied edge from a perspective of multi - color substitution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __split_bgedge(self, bgedge, guidance=None, sorted_guidance=False,\n                       account_for_colors_multiplicity_in_guidance=True, key=None):\n        \"\"\" Splits a :class:`bg.edge.BGEdge` in current :class:`BreakpointGraph` most similar to supplied one (if no unique identifier ``key`` is provided) with respect to supplied guidance.\n\n        If no unique identifier for edge to be changed is specified, edge to be split is determined by iterating over all edges between vertices in supplied :class:`bg.edge.BGEdge` instance and the edge with most similarity score to supplied one is chosen.\n        Once the edge to be split is determined, split if performed form a perspective of :class:`bg.multicolor.Multicolor` split.\n        The originally detected edge is deleted, and new edges containing information about multi-colors after splitting, are added to the current :class:`BreakpointGraph`.\n\n\n        :param bgedge: an edge to find most \"similar to\" among existing edges for a split\n        :type bgedge: :class:`bg.edge.BGEdge`\n        :param guidance: a guidance for underlying :class:`bg.multicolor.Multicolor` object to be split\n        :type guidance: iterable where each entry is iterable with colors entries\n        :param duplication_splitting: flag (**not** currently implemented) for a splitting of color-based splitting to take into account multiplicity of respective colors\n        :type duplication_splitting: ``Boolean``\n        :param key: unique identifier of edge to be split\n        :type key: any python object. ``int`` is expected\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        candidate_id = None\n        candidate_score = 0\n        candidate_data = None\n        if key is not None:\n            new_multicolors = Multicolor.split_colors(\n                multicolor=self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"multicolor\"],\n                guidance=guidance, sorted_guidance=sorted_guidance,\n                account_for_color_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance)\n            self.__delete_bgedge(bgedge=BGEdge(vertex1=bgedge.vertex1, vertex2=bgedge.vertex2,\n                                               multicolor=self.bg[bgedge.vertex1][bgedge.vertex2][key][\"attr_dict\"][\"multicolor\"]),\n                                 key=key)\n            for multicolor in new_multicolors:\n                self.__add_bgedge(BGEdge(vertex1=bgedge.vertex1, vertex2=bgedge.vertex2, multicolor=multicolor),\n                                  merge=False)\n        else:\n            for v1, v2, key, data in self.bg.edges(nbunch=bgedge.vertex1, data=True, keys=True):\n                if v2 == bgedge.vertex2:\n                    score = Multicolor.similarity_score(bgedge.multicolor, data[\"attr_dict\"][\"multicolor\"])\n                    if score > candidate_score:\n                        candidate_id = key\n                        candidate_data = data\n                        candidate_score = score\n            if candidate_data is not None:\n                new_multicolors = Multicolor.split_colors(multicolor=candidate_data[\"attr_dict\"][\"multicolor\"],\n                                                          guidance=guidance, sorted_guidance=sorted_guidance,\n                                                          account_for_color_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance)\n                self.__delete_bgedge(bgedge=BGEdge(vertex1=bgedge.vertex1, vertex2=bgedge.vertex2,\n                                                   multicolor=candidate_data[\"attr_dict\"][\"multicolor\"]),\n                                     key=candidate_id)\n                for multicolor in new_multicolors:\n                    self.__add_bgedge(BGEdge(vertex1=bgedge.vertex1, vertex2=bgedge.vertex2,\n                                             multicolor=multicolor), merge=False)", "response": "Splits a bgedge into two edges with respect to supplied guidance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit an edge in current breakpoint graph with respect to supplied data.", "response": "def split_edge(self, vertex1, vertex2, multicolor, guidance=None, sorted_guidance=False,\n                   account_for_colors_multiplicity_in_guidance=True, key=None):\n        \"\"\" Splits an edge in current :class:`BreakpointGraph` most similar to supplied data (if no unique identifier ``key`` is provided) with respect to supplied guidance.\n\n        Proxies a call to :meth:`BreakpointGraph._BreakpointGraph__split_bgedge` method.\n\n        :param vertex1: a first vertex out of two the edge to be split is incident to\n        :type vertex1: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second vertex out of two the edge to be split is incident to\n        :type vertex2: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param multicolor: a multi-color to find most suitable edge to be split\n        :type multicolor: :class:`bg.multicolor.Multicolor`\n        :param duplication_splitting: flag (**not** currently implemented) for a splitting of color-based splitting to take into account multiplicity of respective colors\n        :type duplication_splitting: ``Boolean``\n        :param key: unique identifier of edge to be split\n        :type key: any python object. ``int`` is expected\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        self.__split_bgedge(bgedge=BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=multicolor), guidance=guidance,\n                            sorted_guidance=sorted_guidance,\n                            account_for_colors_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance,\n                            key=key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_bgedge(self, bgedge, guidance=None, sorted_guidance=False,\n                     account_for_colors_multiplicity_in_guidance=True,\n                     key=None):\n        \"\"\" Splits a :class:`bg.edge.BGEdge` in current :class:`BreakpointGraph` most similar to supplied one (if no unique identifier ``key`` is provided) with respect to supplied guidance.\n\n        Proxies a call to :meth:`BreakpointGraph._BreakpointGraph__split_bgedge` method.\n\n        :param bgedge: an edge to find most \"similar to\" among existing edges for a split\n        :type bgedge: :class:`bg.edge.BGEdge`\n        :param guidance: a guidance for underlying :class:`bg.multicolor.Multicolor` object to be split\n        :type guidance: iterable where each entry is iterable with colors entries\n        :param duplication_splitting: flag (**not** currently implemented) for a splitting of color-based splitting to take into account multiplicity of respective colors\n        :type duplication_splitting: ``Boolean``\n        :param key: unique identifier of edge to be split\n        :type key: any python object. ``int`` is expected\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        self.__split_bgedge(bgedge=bgedge, guidance=guidance, sorted_guidance=sorted_guidance,\n                            account_for_colors_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance,\n                            key=key)", "response": "Splits a BGEdge into two edges with respect to supplied guidance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __split_all_edges_between_two_vertices(self, vertex1, vertex2, guidance=None, sorted_guidance=False,\n                                               account_for_colors_multiplicity_in_guidance=True):\n        \"\"\" Splits all edges between two supplied vertices in current :class:`BreakpointGraph` instance with respect to the provided guidance.\n\n        Iterates over all edges between two supplied vertices and splits each one of them with respect to the guidance.\n\n        :param vertex1: a first out of two vertices edges between which are to be split\n        :type vertex1: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second out of two vertices edges between which are to be split\n        :type vertex2: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param guidance: a guidance for underlying :class:`bg.multicolor.Multicolor` objects to be split\n        :type guidance: iterable where each entry is iterable with colors entries\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        edges_to_be_split_keys = [key for v1, v2, key in self.bg.edges(nbunch=vertex1, keys=True) if v2 == vertex2]\n        for key in edges_to_be_split_keys:\n            self.__split_bgedge(BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=None), guidance=guidance,\n                                sorted_guidance=sorted_guidance,\n                                account_for_colors_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance,\n                                key=key)", "response": "Splits all edges between two supplied vertices and splits them into two sets of BreakpointGraphs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_all_edges_between_two_vertices(self, vertex1, vertex2, guidance=None, sorted_guidance=False,\n                                             account_for_colors_multiplicity_in_guidance=True):\n        \"\"\" Splits all edges between two supplied vertices in current :class:`BreakpointGraph` instance with respect to the provided guidance.\n\n        Proxies a call to :meth:`BreakpointGraph._BreakpointGraph__split_all_edges_between_two_vertices` method.\n\n        :param vertex1: a first out of two vertices edges between which are to be split\n        :type vertex1: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second out of two vertices edges between which are to be split\n        :type vertex2: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param guidance: a guidance for underlying :class:`bg.multicolor.Multicolor` objects to be split\n        :type guidance: iterable where each entry is iterable with colors entries\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        self.__split_all_edges_between_two_vertices(vertex1=vertex1, vertex2=vertex2, guidance=guidance,\n                                                    sorted_guidance=sorted_guidance,\n                                                    account_for_colors_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance)", "response": "Splits all edges between two vertices in current breakpoint graph with respect to the provided guidance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_all_edges(self, guidance=None, sorted_guidance=False, account_for_colors_multiplicity_in_guidance=True):\n        vertex_pairs = [(edge.vertex1, edge.vertex2) for edge in self.edges()]\n        for v1, v2 in vertex_pairs:\n            self.__split_all_edges_between_two_vertices(vertex1=v1, vertex2=v2, guidance=guidance,\n                                                        sorted_guidance=sorted_guidance,\n                                                        account_for_colors_multiplicity_in_guidance=account_for_colors_multiplicity_in_guidance)", "response": "Splits all edges between two vertices in current breakpoint graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete all edges between two vertices in the cluster.", "response": "def __delete_all_bgedges_between_two_vertices(self, vertex1, vertex2):\n        \"\"\" Deletes all edges between two supplied vertices\n\n        :param vertex1: a first out of two vertices edges between which are to be deleted\n        :type vertex1: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second out of two vertices edges between which are to be deleted\n        :type vertex2: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        edges_to_be_deleted_with_keys = [(key, data) for v1, v2, key, data in self.bg.edges(nbunch=vertex1, keys=True,\n                                                                                            data=True) if v2 == vertex2]\n        for key, data in edges_to_be_deleted_with_keys:\n            self.__delete_bgedge(BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=data[\"attr_dict\"][\"multicolor\"]),\n                                 key=key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_all_edges_between_two_vertices(self, vertex1, vertex2):\n        self.__delete_all_bgedges_between_two_vertices(vertex1=vertex1, vertex2=vertex2)", "response": "Deletes all edges between two vertices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge all bgedges between two supplied vertices into a single edge.", "response": "def __merge_all_bgedges_between_two_vertices(self, vertex1, vertex2):\n        \"\"\" Merges all edge between two supplied vertices into a single edge from a perspective of multi-color merging.\n\n        :param vertex1: a first out of two vertices edges between which are to be merged together\n        :type vertex1: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :param vertex2: a second out of two vertices edges between which are to be merged together\n        :type vertex2: any python hashable object. :class:`bg.vertex.BGVertex` is expected\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        ############################################################################################################\n        #\n        # no actual merging is performed, but rather all edges between two vertices are deleted\n        # and then added with a merge argument set to true\n        #\n        ############################################################################################################\n        edges_multicolors = [deepcopy(data[\"attr_dict\"][\"multicolor\"]) for v1, v2, data in\n                             self.bg.edges(nbunch=vertex1, data=True) if v2 == vertex2]\n        self.__delete_all_bgedges_between_two_vertices(vertex1=vertex1, vertex2=vertex2)\n        for multicolor in edges_multicolors:\n            self.__add_bgedge(BGEdge(vertex1=vertex1, vertex2=vertex2, multicolor=multicolor), merge=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_all_edges_between_two_vertices(self, vertex1, vertex2):\n        self.__merge_all_bgedges_between_two_vertices(vertex1=vertex1, vertex2=vertex2)", "response": "This method merges all edges between two supplied vertices into a single edge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_all_edges(self):\n        pairs_of_vetices = [(edge.vertex1, edge.vertex2) for edge in self.edges()]\n        for v1, v2 in pairs_of_vetices:\n            ############################################################################################################\n            #\n            # we iterate over all pairs of vertices in the given graph and merge edges between them\n            #\n            ############################################################################################################\n            self.__merge_all_bgedges_between_two_vertices(vertex1=v1, vertex2=v2)", "response": "Merges all edges between same pairs of vertices into a single edge."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new breakpoint graph object that contains all available information from two given breakpoint graphs.", "response": "def merge(cls, breakpoint_graph1, breakpoint_graph2, merge_edges=False):\n        \"\"\" Merges two given instances of :class`BreakpointGraph` into a new one, that gather all available information from both supplied objects.\n\n        Depending of a ``merge_edges`` flag, while merging of two dat structures is occurring, edges between similar vertices can be merged during the creation of a result :class`BreakpointGraph` obejct.\n\n        Accounts for subclassing.\n\n        :param breakpoint_graph1: a first out of two :class`BreakpointGraph` instances to gather information from\n        :type breakpoint_graph1: :class`BreakpointGraph`\n        :param breakpoint_graph2: a second out of two :class`BreakpointGraph` instances to gather information from\n        :type breakpoint_graph2: :class`BreakpointGraph`\n        :param merge_edges: flag to indicate if edges in a new merged :class`BreakpointGraph` object has to be merged between same vertices, or if splitting from supplied graphs shall be preserved.\n        :type merge_edges: ``Boolean``\n        :return: a new breakpoint graph object that contains all information gathered from both supplied breakpoint graphs\n        :rtype: :class`BreakpointGraph`\n        \"\"\"\n        result = cls()\n        for bgedge in breakpoint_graph1.edges():\n            result.__add_bgedge(bgedge=bgedge, merge=merge_edges)\n        for bgedge in breakpoint_graph2.edges():\n            result.__add_bgedge(bgedge=bgedge, merge=merge_edges)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate a current breakpoint graph with information from a supplied breakpoint graph.", "response": "def __update(self, breakpoint_graph, merge_edges=False):\n        \"\"\" Updates a current :class`BreakpointGraph` object with information from a supplied :class`BreakpointGraph` instance.\n\n        Depending of a ``merge_edges`` flag, while updating of a current :class`BreakpointGraph` object is occuring, edges between similar vertices can be merged to already existing ones.\n\n        :param breakpoint_graph: a breakpoint graph to extract information from, which will be then added to the current\n        :type breakpoint_graph: :class`BreakpointGraph`\n        :param merge_edges: flag to indicate if edges to be added to current :class`BreakpointGraph` object are to be merged to already existing ones\n        :type merge_edges: ``Boolean``\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        for bgedge in breakpoint_graph.edges():\n            self.__add_bgedge(bgedge=deepcopy(bgedge), merge=merge_edges)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the current breakpoint graph with information from a supplied breakpoint graph.", "response": "def update(self, breakpoint_graph, merge_edges=False):\n        \"\"\" Updates a current :class`BreakpointGraph` object with information from a supplied :class`BreakpointGraph` instance.\n\n        Proxoes a call to :meth:`BreakpointGraph._BreakpointGraph__update` method.\n\n        :param breakpoint_graph: a breakpoint graph to extract information from, which will be then added to the current\n        :type breakpoint_graph: :class:`BreakpointGraph`\n        :param merge_edges: flag to indicate if edges to be added to current :class`BreakpointGraph` object are to be merged to already existing ones\n        :type merge_edges: ``Boolean``\n        :return: ``None``, performs inplace changes\n        \"\"\"\n        self.__update(breakpoint_graph=breakpoint_graph,\n                      merge_edges=merge_edges)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_kbreak(self, kbreak, merge=True):\n        ############################################################################################################\n        #\n        # k-break must ba valid to be applied\n        #\n        ############################################################################################################\n        vertices = {}\n        edge_data = {}\n        if not isinstance(kbreak, KBreak):\n            raise TypeError(\"Only KBreak and derivatives are allowed as kbreak argument\")\n        if not KBreak.valid_kbreak_matchings(kbreak.start_edges, kbreak.result_edges):\n            raise ValueError(\"Supplied KBreak is not valid form perspective of starting/resulting sets of vertices\")\n        for vertex1, vertex2 in kbreak.start_edges:\n\n            if vertex1.is_infinity_vertex and vertex2.is_infinity_vertex:\n                ############################################################################################################\n                #\n                # when we encounter a fully infinity edge (both vertices are infinity vertices)\n                # we shall not check if they are present in the current graph, because hat portion of a kbreak is artificial\n                #\n                ############################################################################################################\n                continue\n            if vertex1 not in self.bg or vertex2 not in self.bg:\n                raise ValueError(\"Supplied KBreak targets vertices (`{v1}` and `{v2}`) at least one of which \"\n                                 \"does not exist in current BreakpointGraph\"\n                                 \"\".format(v1=vertex1.name, v2=vertex2.name))\n        for vertex1, vertex2 in kbreak.start_edges:\n            if vertex1.is_infinity_vertex and vertex2.is_infinity_vertex:\n                continue\n            for bgedge in self.__edges_between_two_vertices(vertex1=vertex1, vertex2=vertex2):\n                ############################################################################################################\n                #\n                # at least one edge between supplied pair of vertices must contain a multicolor that is specified for the kbreak\n                #\n                ############################################################################################################\n                if kbreak.multicolor <= bgedge.multicolor:\n                    break\n            else:\n                raise ValueError(\"Some targeted by kbreak edge with specified multicolor does not exists\")\n        for vertex1, vertex2 in kbreak.start_edges:\n            if vertex1.is_infinity_vertex and vertex2.is_infinity_vertex:\n                continue\n            v1 = self.__get_vertex_by_name(vertex_name=vertex1.name)\n            vertices[v1] = v1\n            v2 = self.__get_vertex_by_name(vertex_name=vertex2.name)\n            vertices[v2] = v2\n            bgedge = BGEdge(vertex1=v1, vertex2=v2, multicolor=kbreak.multicolor)\n            candidate_data, candidate_id, candidate_score = self.__determine_most_suitable_edge_for_deletion(\n                bgedge=bgedge)\n            data = candidate_data[\"attr_dict\"][\"data\"]\n            edge_data[v1] = data\n            edge_data[v2] = data\n            self.__delete_bgedge(bgedge=bgedge, keep_vertices=True)\n        for vertex_set in kbreak.start_edges:\n            for vertex in vertex_set:\n                if vertex.is_infinity_vertex and vertex in self.bg:\n                    ############################################################################################################\n                    #\n                    # after the first portion of a kbreak is performed one must make sure we don't leave any infinity vertices\n                    # that have edges going to them, as infinity vertex is a special artificial vertex\n                    #  and it has meaning only if there are edges going to / from it\n                    #\n                    ############################################################################################################\n                    if len(list(self.get_edges_by_vertex(vertex=vertex))) == 0:\n                        self.bg.remove_node(vertex)\n        for vertex1, vertex2 in kbreak.result_edges:\n            if vertex1.is_infinity_vertex and vertex2.is_infinity_vertex:\n                ############################################################################################################\n                #\n                # if we encounter a pair of infinity vertices in result edges set, we shall not add them\n                # as at least a part of kbreak corresponded to fusion\n                # and those infinity edges on their own won't have any meaning\n                #\n                ############################################################################################################\n                continue\n            origin = kbreak.data.get(\"origin\", None)\n            v1 = vertices.get(vertex1, vertex1)\n            v2 = vertices.get(vertex2, vertex2)\n            bg_edge = BGEdge(vertex1=v1, vertex2=v2, multicolor=kbreak.multicolor)\n            if \"origin\" in bg_edge.data:\n                bg_edge.data[\"origin\"] = origin\n            if kbreak.is_a_fusion:\n                edge1_data = edge_data[v1]\n                edge2_data = edge_data[v2]\n                merged_edge_fragment_data = merge_fragment_edge_data(edge1_data[\"fragment\"], edge2_data[\"fragment\"])\n                result_edge_data = {}\n                recursive_dict_update(result_edge_data, edge1_data)\n                recursive_dict_update(result_edge_data, edge2_data)\n                recursive_dict_update(result_edge_data, {\"fragment\": merged_edge_fragment_data})\n                recursive_dict_update(bg_edge.data, result_edge_data)\n            self.__add_bgedge(bg_edge, merge=merge)", "response": "Applies a k - break to the current breakpoint graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(cls, data, genomes_data=None, genomes_deserialization_required=True, merge=False):\n        result = cls()\n        merge = merge\n        vertices_dict = {}\n        genomes_dict = genomes_data if genomes_data is not None and not genomes_deserialization_required else None\n        if genomes_dict is None:\n            ############################################################################################################\n            #\n            # if we need to recover genomes information from breakpoint graph json object\n            # we are happy to do that\n            #\n            ############################################################################################################\n            genomes_dict = {}\n            try:\n                source = genomes_data if genomes_data is not None and genomes_deserialization_required else data[\n                    \"genomes\"]\n            except KeyError as exc:\n                raise ValueError(\"Error during breakpoint graph deserialization. No \\\"genomes\\\" information found\")\n            for g_dict in source:\n                ############################################################################################################\n                #\n                # if explicitly specified in genome json object, it can be decoded using provided schema name,\n                # of course a decoding breakpoint graph object shall be aware of such scheme\n                # (it has to be specified in the `genomes_json_schemas` class wide dict)\n                #\n                ############################################################################################################\n                schema_name = g_dict.get(BGGenome_JSON_SCHEMA_JSON_KEY, None)\n                schema_class = None if schema_name is None else cls.genomes_json_schemas.get(schema_name, None)\n                genomes_dict[g_dict[\"g_id\"]] = BGGenome.from_json(data=g_dict, json_schema_class=schema_class)\n        if \"vertices\" not in data:\n            ############################################################################################################\n            #\n            # breakpoint graph can not be decoded without having information about vertices explicitly\n            # as vertices are referenced in edges object, rather than explicitly provided\n            #\n            ############################################################################################################\n            raise ValueError(\n                \"Error during breakpoint graph deserialization. \\\"vertices\\\" key is not present in json object\")\n        for vertex_dict in data[\"vertices\"]:\n            ############################################################################################################\n            #\n            # if explicitly specified in vertex json object, it can be decoded using provided schema name,\n            # of course a decoding breakpoint graph object shall be aware of such scheme\n            # (it has to be specified in the `vertices_json_schemas` class wide dict)\n            #\n            ############################################################################################################\n            schema_name = vertex_dict.get(BGVertex_JSON_SCHEMA_JSON_KEY, None)\n            schema_class = None if schema_name is None else cls.vertices_json_schemas.get(schema_name, None)\n            try:\n                ############################################################################################################\n                #\n                # we try to recover a specific vertex class based on its name.\n                # it does not overwrite the schema based behaviour\n                # but provides a correct default schema for a specific vertex type\n                #\n                ############################################################################################################\n                vertex_class = BGVertex.get_vertex_class_from_vertex_name(vertex_dict[\"name\"])\n            except KeyError:\n                vertex_class = BGVertex\n            vertices_dict[vertex_dict[\"v_id\"]] = vertex_class.from_json(data=vertex_dict,\n                                                                        json_schema_class=schema_class)\n        for edge_dict in data[\"edges\"]:\n            ############################################################################################################\n            #\n            # if explicitly specified in edge json object, it can be decoded using provided schema name,\n            # of course a decoding breakpoint graph object shall be aware of such scheme\n            # (it has to be specified in the `edges_json_schemas` class wide dict)\n            #\n            ############################################################################################################\n            schema_name = edge_dict.get(BGEdge_JSON_SCHEMA_JSON_KEY, None)\n            schema = None if schema_name is None else cls.edges_json_schemas.get(schema_name, None)\n            edge = BGEdge.from_json(data=edge_dict, json_schema_class=schema)\n            try:\n                edge.vertex1 = vertices_dict[edge.vertex1]\n                edge.vertex2 = vertices_dict[edge.vertex2]\n            except KeyError:\n                ############################################################################################################\n                #\n                # as edge references a pair of vertices, we must be sure respective vertices were decoded\n                #\n                ############################################################################################################\n                raise ValueError(\n                    \"Error during breakpoint graph deserialization. Deserialized edge references non-present vertex\")\n            if len(edge.multicolor) == 0:\n                ############################################################################################################\n                #\n                # edges with empty multicolor are not permitted in breakpoint graphs\n                #\n                ############################################################################################################\n                raise ValueError(\n                    \"Error during breakpoint graph deserialization. Empty multicolor for deserialized edge\")\n            try:\n                edge.multicolor = Multicolor(*[genomes_dict[g_id] for g_id in edge.multicolor])\n            except KeyError:\n                raise ValueError(\n                    \"Error during breakpoint graph deserialization. Deserialized edge reference non-present \"\n                    \"genome in its multicolor\")\n            result.__add_bgedge(edge, merge=merge)\n        return result", "response": "A JSON deserialization operation that recovers a breakpoint graph from its JSON representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(request, obj_id=None):\n    res = Result()\n    if obj_id:\n        if obj_id == '0':\n            obj = {\n                'id': 0,\n                'name': 'TAGLESS',\n                'artist': False,\n            }\n        else:\n            obj = get_object_or_404(Tag, pk=obj_id).json()\n\n        res.append(obj)\n        return JsonResponse(res.asDict())\n    else:\n        if request.GET.get('count'):\n            itags = Tag.objects.all().annotate(icount=Count('image'))\n            vtags = Tag.objects.all().annotate(vcount=Count('video'))\n\n            for i, tag in enumerate(itags):\n                tag.count = itags[i].icount + vtags[i].vcount\n                res.append(tag.json())\n        else:\n            for tag in Tag.objects.all():\n                res.append(tag.json())\n\n        return JsonResponse(res.asDict())", "response": "Lists all tags\n\n    :returns: json"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(request):\n    res = Result()\n    data = request.POST or json.loads(request.body)['body']\n    name = data.get('name', None)\n\n    if not name:\n        res.isError = True\n        res.message = \"No name given\"\n\n        return JsonResponse(res.asDict())\n    \n    tag = Tag.objects.get_or_create(name=name.lower())[0]\n\n    res.append(tag.json())\n\n    return JsonResponse(res.asDict())", "response": "Creates a tag object for a given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(request, obj_id=None):\n    res = Result()\n    data = request.PUT or json.loads(request.body)['body']\n    if obj_id:\n        # -- Edit the tag\n        tag = Tag.objects.get(pk=obj_id)\n        tag.name = data.get('name', tag.name)\n        tag.artist = data.get('artist', tag.artist)\n        tag.save()\n    else:\n        tags = [_ for _ in data.get('tags', '').split(',') if _]\n        guids = [_ for _ in data.get('guids', '').split(',') if _]\n\n        _manageTags(tags, guids)\n\n    return JsonResponse(res.asDict())", "response": "Adds tags from objects resolved from guids\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(request, obj_id=None):\n    res = Result()\n\n    if obj_id:\n        # -- Delete the tag itself\n        tag = Tag.objects.get(pk=obj_id)\n        guids = []\n        images = Image.objects.filter(tags__id=obj_id)\n        guids += [_.guid for _ in images]\n        videos = Video.objects.filter(tags__id=obj_id)\n        guids += [_.guid for _ in videos]\n        # -- Remove all tags from objects\n        _manageTags([tag.id], guids, add=False)\n        # -- Delete old tags\n        tag.delete()\n    else:\n        tags = [_ for _ in request.DELETE.get('tags', '').split(',') if _]\n        guids = [_ for _ in request.DELETE.get('guids', '').split(',') if _]\n\n        _manageTags(tags, guids, add=False)\n\n    return JsonResponse(res.asDict())", "response": "Removes tags from objects resolved from guids\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for Tag objects and returns a list of searialize Tag objects.", "response": "def search(request):\n    \"\"\"\n    Search for Tag objects and returns a Result object with a list of searialize Tag\n    objects.\n\n    :param search: Append a \"Search for\" tag\n    :type search: bool\n    :param zero: Exclude Tags with no items\n    :type zero: bool\n    :param artist: Exclude artist tags\n    :type artist: bool\n    :returns: json\n    \"\"\"\n    q = request.GET.get('q', '')\n    includeSearch = request.GET.get('search', False)\n    nonZero = request.GET.get('zero', False)\n    excludeArtist = request.GET.get('artist', False)\n\n    if includeSearch:\n        l = [{'id': 0, 'name': 'Search for: %s' % q}]\n    else:\n        l = []\n\n    query = Tag.objects.filter(name__icontains=q)\n\n    if excludeArtist:\n        query = query.exclude(artist=True)\n\n    if nonZero:\n        l += [t.json() for t in query if t.count() > 0]\n    else:\n        l += [t.json() for t in query]\n\n    return JsonResponse(l, safe=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges multiple tags into a single tag and all related objects are reassigned", "response": "def merge(request, obj_id):\n    \"\"\"Merges multiple tags into a single tag and all related objects are reassigned\"\"\"\n    res = Result()\n    if request.POST:\n        tags = json.loads(request.POST['tags'])\n    else:\n        tags = json.loads(request.body)['body']['tags']\n\n    guids = []\n    images = Image.objects.filter(tags__id__in=tags)\n    guids += [_.guid for _ in images]\n    videos = Video.objects.filter(tags__id__in=tags)\n    guids += [_.guid for _ in videos]\n    # -- Remove all tags from objects\n    _manageTags(tags, guids, add=False)\n    # -- Add merged tag to all objects\n    _manageTags([obj_id], guids, add=True)\n    # -- Delete old tags\n    Tag.objects.filter(pk__in=tags).delete()\n\n    return JsonResponse(res.asDict())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _manageTags(tagList, guids, add=True):\n    objects = getObjectsFromGuids(guids)\n    tags = []\n    for tag in tagList:\n        try:\n            t = Tag.objects.get(pk=int(tag))\n        except ValueError:\n            t = Tag.objects.get_or_create(name=tag.lower())[0]\n        tags.append(t)\n\n    if add:\n        return _addTags(tags, objects)\n    else:\n        return _removeTags(tags, objects)", "response": "Adds or Removes Guids from Tags"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds tags to objects", "response": "def _addTags(tags, objects):\n    \"\"\" Adds tags to objects \"\"\"\n    for t in tags:\n        for o in objects:\n            o.tags.add(t)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _removeTags(tags, objects):\n    for t in tags:\n        for o in objects:\n            o.tags.remove(t)\n\n    return True", "response": "Removes tags from objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, xsl='text'):\n\n        try:\n            cmd_text, option_text = xsl.split(None, 1)\n        except ValueError:\n            cmd_text = xsl\n            option_text = ''\n\n        try:\n            context, cmd = cmd_text.strip().lower().split(':', 1)\n        except ValueError:\n            cmd = cmd_text.lower()\n            context = None\n\n        if not cmd in DEFAULT_CMD_TO_CONTEXT_MAPPING:\n            raise ParseError(\"unknown command %s\" % cmd)\n\n        if context and not context in CONTEXTS:\n            raise ParseError(\"unknown context %s\" % context)\n\n        self.context = context\n        self.cmd = cmd\n        self.text = None\n        self.meta_commands = []\n        self.options = {}\n\n        try:\n            if cmd in ('choose', 'text', 'meta'):\n                raise ValueError()\n            option_name, expr = option_text.split('=', 1)\n            option_name = option_name.strip().lower()\n            expr = unescape(expr).strip(\"'\").strip('\"').strip()\n            self.options = {option_name: expr}\n\n        except ValueError:\n            text = unescape(option_text)\n\n            if cmd == 'meta':\n                for mc in filter(lambda c: c, map(lambda c: c.strip(), text.lower().split(';'))):\n                    if mc in META_COMMANDS:\n                        # store in stack order\n                        self.meta_commands = [mc] + self.meta_commands\n                    else:\n                        raise ParseError(\"unknown meta command %s\" % self.text)\n\n            else:\n                self.text = text", "response": "Parse the command line and return the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the top level w : p node", "response": "def p0(self, e):\n        \"\"\"Returns top level w:p node\"\"\"\n        body = self.body(e)\n        p = self.p(e)\n        if body == p.getparent():\n            return p\n        else:\n            raise ElementNotFound(\"could not find top level w:p element in %s\" % e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating regexp for parsing of shortened relative timestamps as shown in the table.", "response": "def _short_ts_regexp():\n\t'''Generates regexp for parsing of\n\t\tshortened relative timestamps, as shown in the table.'''\n\tts_re = ['^']\n\tfor k in it.chain(_short_ts_days, _short_ts_s):\n\t\tts_re.append(r'(?P<{0}>\\d+{0}\\s*)?'.format(k))\n\treturn re.compile(''.join(ts_re), re.I | re.U)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a timestamp string into a datetime object.", "response": "def parse_timestamp(ts_str):\n\t'''Match time either in human-readable format (as accepted by dateutil),\n\t\tor same time-offset format, as used in the table (e.g. \"NdMh ago\", or just \"NdMh\").'''\n\tassert isinstance(ts_str, bytes), [type(ts_str), repr(ts_str)]\n\tts_str = ts_str.replace('_', ' ')\n\n\t# Try to parse time offset in short format\n\tmatch = _short_ts_regexp.search(ts_str)\n\tif match and any(match.groups()):\n\t\tdelta = list()\n\t\tparse_int = lambda v: int(''.join(c for c in v if c.isdigit()))\n\t\tfor units in [_short_ts_days, _short_ts_s]:\n\t\t\tval = 0\n\t\t\tfor k, v in units.iteritems():\n\t\t\t\ttry:\n\t\t\t\t\tif not match.group(k): continue\n\t\t\t\t\tn = parse_int(match.group(k))\n\t\t\t\texcept IndexError: continue\n\t\t\t\tval += n * v\n\t\t\tdelta.append(val)\n\t\treturn timezone.localtime(timezone.now()) - timedelta(*delta)\n\n\t# Fallback to other generic formats\n\tts = None\n\tif not ts:\n\t\tmatch = re.search( # common BE format\n\t\t\tr'^(?P<date>(?:\\d{2}|(?P<Y>\\d{4}))-\\d{2}-\\d{2})'\n\t\t\tr'(?:[ T](?P<time>\\d{2}(?::\\d{2}(?::\\d{2})?)?)?)?$', ts_str )\n\t\tif match:\n\t\t\ttpl = 'y' if not match.group('Y') else 'Y'\n\t\t\ttpl, ts_str = '%{}-%m-%d'.format(tpl), match.group('date')\n\t\t\tif match.group('time'):\n\t\t\t\ttpl_time = ['%H', '%M', '%S']\n\t\t\t\tts_str_time = match.group('time').split(':')\n\t\t\t\tts_str += ' ' + ':'.join(ts_str_time)\n\t\t\t\ttpl += ' ' + ':'.join(tpl_time[:len(ts_str_time)])\n\t\t\ttry: ts = timezone.make_aware(datetime.strptime(ts_str, tpl))\n\t\t\texcept ValueError: pass\n\tif not ts:\n\t\t# coreutils' \"date\" parses virtually everything, but is more expensive to use\n\t\twith open(os.devnull, 'w') as devnull:\n\t\t\tproc = subprocess.Popen(\n\t\t\t\t['date', '+%s', '-d', ts_str],\n\t\t\t\tstdout=subprocess.PIPE, stderr=devnull, close_fds=True )\n\t\t\tval = proc.stdout.read()\n\t\t\tif not proc.wait():\n\t\t\t\tts = timezone.make_aware(datetime.fromtimestamp(int(val.strip())))\n\n\tif ts: return ts\n\traise ValueError('Unable to parse date/time string: {0}'.format(ts_str))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get(self, timeout):\n        with self._lock:\n            if timeout is None:\n                while self.empty():\n                    self._not_empty.wait()\n            else:\n                time_end = time.time() + timeout\n                while self.empty():\n                    time_left = time_end - time.time()\n                    if time_left < 0:\n                        raise PoolEmptyError\n                    self._not_empty.wait(time_left)\n\n            rtracker = self._reference_queue[self._resource_start]\n            self._resource_start = (self._resource_start + 1) % self.maxsize\n            self._available -= 1\n\n        return rtracker", "response": "Get a resource from the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_tracker(self, resource):\n        with self._lock:\n            for rt in self._reference_queue:\n                if rt is not None and resource is rt.resource:\n                    return rt\n\n        raise UnknownResourceError('Resource not created by pool')", "response": "Return the resource tracker that is tracking resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _harvest_lost_resources(self):\n        with self._lock:\n            for i in self._unavailable_range():\n                rtracker = self._reference_queue[i]\n                if rtracker is not None and rtracker.available():\n                    self.put_resource(rtracker.resource)", "response": "Return lost resources to pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a resource instance for the specified resource class.", "response": "def _make_resource(self):\n        \"\"\"\n        Returns a resource instance.\n        \"\"\"\n        with self._lock:\n            for i in self._unavailable_range():\n                if self._reference_queue[i] is None:\n                    rtracker = _ResourceTracker(\n                        self._factory(**self._factory_arguments))\n\n                    self._reference_queue[i] = rtracker\n                    self._size += 1\n\n                    return rtracker\n\n            raise PoolFullError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nputting a resource back in the queue.", "response": "def _put(self, rtracker):\n        \"\"\"\n        Put a resource back in the queue.\n\n        :param rtracker: A resource.\n        :type rtracker: :class:`_ResourceTracker`\n\n        :raises PoolFullError: If pool is full.\n        :raises UnknownResourceError: If resource can't be found.\n        \"\"\"\n        with self._lock:\n            if self._available < self.capacity:\n                for i in self._unavailable_range():\n                    if self._reference_queue[i] is rtracker:\n                        # i retains its value and will be used to swap with\n                        # first \"empty\" space in queue.\n                        break\n                else:\n                    raise UnknownResourceError\n\n                j = self._resource_end\n                rq = self._reference_queue\n                rq[i], rq[j] = rq[j], rq[i]\n\n                self._resource_end = (self._resource_end + 1) % self.maxsize\n                self._available += 1\n\n                self._not_empty.notify()\n            else:\n                raise PoolFullError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a resource from the pool.", "response": "def _remove(self, rtracker):\n        \"\"\"\n        Remove a resource from the pool.\n\n        :param rtracker: A resource.\n        :type rtracker: :class:`_ResourceTracker`\n        \"\"\"\n        with self._lock:\n            i = self._reference_queue.index(rtracker)\n            self._reference_queue[i] = None\n            self._size -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator for the indices of the unavailable region of the resource queue.", "response": "def _unavailable_range(self):\n        \"\"\"\n        Return a generator for the indices of the unavailable region of\n        ``_reference_queue``.\n        \"\"\"\n        with self._lock:\n            i = self._resource_end\n            j = self._resource_start\n            if j < i or self.empty():\n                j += self.maxsize\n\n            for k in range(i, j):\n                yield k % self.maxsize"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_resource(self, resource_wrapper=None):\n        rtracker = None\n\n        if resource_wrapper is None:\n            resource_wrapper = self._resource_wrapper\n\n        if self.empty():\n            self._harvest_lost_resources()\n\n        try:\n            rtracker = self._get(0)\n        except PoolEmptyError:\n            pass\n\n        if rtracker is None:\n            # Could not find resource, try to make one.\n            try:\n                rtracker = self._make_resource()\n            except PoolFullError:\n                pass\n\n        if rtracker is None:\n            # Could not find or make resource, so must wait for a resource\n            # to be returned to the pool.\n            try:\n                rtracker = self._get(timeout=self._timeout)\n            except PoolEmptyError:\n                pass\n\n        if rtracker is None:\n            raise PoolEmptyError\n\n        # Ensure resource is active.\n        if not self.ping(rtracker.resource):\n            # Lock here to prevent another thread creating a resource in the\n            # index that will have this resource removed. This ensures there\n            # will be space for _make_resource() to place a newly created\n            # resource.\n            with self._lock:\n                self._remove(rtracker)\n                rtracker = self._make_resource()\n\n        # Ensure all resources leave pool with same attributes.\n        # normalize_connection() is used since it calls\n        # normalize_resource(), so if a user implements either one, the\n        # resource will still be normalized. This will be changed in 1.0 to\n        # call normalize_resource() when normalize_connection() is\n        # removed.\n        self.normalize_connection(rtracker.resource)\n\n        return rtracker.wrap_resource(self, resource_wrapper)", "response": "Returns a resource instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a resource to the pool or discards it if the pool is full.", "response": "def put_resource(self, resource):\n        \"\"\"\n        Adds a resource back to the pool or discards it if the pool is full.\n\n        :param resource: A resource object.\n\n        :raises UnknownResourceError: If resource was not made by the\n                                        pool.\n        \"\"\"\n        rtracker = self._get_tracker(resource)\n\n        try:\n            self._put(rtracker)\n        except PoolFullError:\n            self._remove(rtracker)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap the resource in a resource_wrapper.", "response": "def wrap_resource(self, pool, resource_wrapper):\n        \"\"\"\n        Return a resource wrapped in ``resource_wrapper``.\n\n        :param pool: A pool instance.\n        :type pool: :class:`CuttlePool`\n        :param resource_wrapper: A wrapper class for the resource.\n        :type resource_wrapper: :class:`Resource`\n        :return: A wrapped resource.\n        :rtype: :class:`Resource`\n        \"\"\"\n        resource = resource_wrapper(self.resource, pool)\n        self._weakref = weakref.ref(resource)\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the resource and returns the resource to the resource pool.", "response": "def close(self):\n        \"\"\"\n        Returns the resource to the resource pool.\n        \"\"\"\n        if self._resource is not None:\n            self._pool.put_resource(self._resource)\n            self._resource = None\n            self._pool = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, message, fragment_size=None, mask=False):\n        for frame in self.message_to_frames(message, fragment_size, mask):\n            self.send_frame(frame)", "response": "Send a message to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreceives a message from the socket.", "response": "def recv(self):\n        \"\"\"\n        Receive a message. A message may consist of multiple (ordered) data\n        frames. A control frame may be delivered at any time, also when\n        expecting the next continuation frame of a fragmented message. These\n        control frames are handled immediately by handle_control_frame().\n        \"\"\"\n        fragments = []\n\n        while not len(fragments) or not fragments[-1].final:\n            frame = self.sock.recv()\n\n            if isinstance(frame, ControlFrame):\n                self.handle_control_frame(frame)\n            elif len(fragments) > 0 and frame.opcode != OPCODE_CONTINUATION:\n                raise ValueError('expected continuation/control frame, got %s '\n                                 'instead' % frame)\n            else:\n                fragments.append(frame)\n\n        return self.concat_fragments(fragments)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_control_frame(self, frame):\n        if frame.opcode == OPCODE_CLOSE:\n            self.close_frame_received = True\n            code, reason = frame.unpack_close()\n\n            if self.close_frame_sent:\n                self.onclose(code, reason)\n                self.sock.close()\n                raise SocketClosed(True)\n            else:\n                self.close_params = (code, reason)\n                self.send_close_frame(code, reason)\n\n        elif frame.opcode == OPCODE_PING:\n            # Respond with a pong message with identical payload\n            self.send_frame(ControlFrame(OPCODE_PONG, frame.payload))\n\n        elif frame.opcode == OPCODE_PONG:\n            # Assert that the PONG payload is identical to that of the PING\n            if not self.ping_sent:\n                raise PingError('received PONG while no PING was sent')\n\n            self.ping_sent = False\n\n            if frame.payload != self.ping_payload:\n                raise PingError('received PONG with invalid payload')\n\n            self.ping_payload = None\n            self.onpong(frame.payload)", "response": "Handle a control frame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive_forever(self):\n        while True:\n            try:\n                self.onmessage(self.recv())\n            except (KeyboardInterrupt, SystemExit, SocketClosed):\n                break\n            except Exception as e:\n                self.onerror(e)\n                self.onclose(None, 'error: %s' % e)\n\n                try:\n                    self.sock.close()\n                except socket.error:\n                    pass\n\n                raise e", "response": "Receive and handle messages in an endless loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a PING control frame with an optional payload.", "response": "def send_ping(self, payload=''):\n        \"\"\"\n        Send a PING control frame with an optional payload.\n        \"\"\"\n        self.send_frame(ControlFrame(OPCODE_PING, payload),\n                        lambda: self.onping(payload))\n        self.ping_payload = payload\n        self.ping_sent = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self, code=None, reason=''):\n        self.send_close_frame(code, reason)\n\n        frame = self.sock.recv()\n\n        if frame.opcode != OPCODE_CLOSE:\n            raise ValueError('expected CLOSE frame, got %s' % frame)\n\n        self.handle_control_frame(frame)", "response": "Close the socket by sending a CLOSE frame and waiting for a response\n        close message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_by_string(cls, fields, query):\n\t'''Get object by numeric id or exact\n\t\tand unique part of specified attrs (name, title, etc).'''\n\ttry: pk = int(query)\n\texcept ValueError: pass\n\telse: return cls.objects.get(pk=pk)\n\tobj = list(cls.objects.filter(reduce( op.or_,\n\t\tlist(Q(**{'{}__icontains'.format(f): query}) for f in fields) )))\n\tif len(obj) > 1:\n\t\traise cls.MultipleObjectsReturned((\n\t\t\tu'Unable to uniquely identify {}'\n\t\t\t\t' by provided criteria: {!r} (candidates: {})' )\\\n\t\t\t.format(cls.__name__, query, ', '.join(it.imap(unicode, obj))) )\n\telif not len(obj):\n\t\traise cls.DoesNotExist(\n\t\t\tu'Unable to find site by provided criteria: {!r}'.format(query) )\n\treturn obj[0]", "response": "Get object by string query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate interval for checks as average between updates for specified period.", "response": "def calculate_check_interval( self,\n\t\t\tmax_interval, ewma_factor, max_days=None,\n\t\t\tmax_updates=None, ewma=0, ewma_ts=None,\n\t\t\tadd_partial=None ):\n\t\t'''Calculate interval for checks as average\n\t\t\ttime (ewma) between updates for specified period.'''\n\t\tif not add_partial:\n\t\t\tposts_base = self.posts.only('date_modified').order_by('date_modified')\n\t\t\tif ewma_ts: posts_base = posts_base.filter(date_modified__gt=ewma_ts)\n\t\t\tposts = posts_base\n\t\t\tif max_days:\n\t\t\t\tposts = posts.filter(date_modified__gt=timezone.now() - timedelta(max_days))\n\t\t\tif max_updates and max_updates > 0:\n\t\t\t\tposts = posts[:max_updates]\n\t\t\t\tif len(posts) < max_updates: posts = posts_base[:max_updates]\n\t\t\ttimestamps = posts.values_list('date_modified', flat=True)\n\t\telse: timestamps = list()\n\t\tif add_partial:\n\t\t\tif not ewma_ts:\n\t\t\t\ttry:\n\t\t\t\t\tewma_ts = self.posts.only('date_modified')\\\n\t\t\t\t\t\t.order_by('-date_modified')[0].date_modified\n\t\t\t\texcept (ObjectDoesNotExist, IndexError): return 0 # no previous timestamp available\n\t\t\ttimestamps.append(add_partial)\n\t\t\tif (add_partial - ewma_ts).total_seconds() < ewma:\n\t\t\t\t# It doesn't make sense to lower interval due to frequent check attempts.\n\t\t\t\treturn ewma\n\t\tfor ts in timestamps:\n\t\t\tif ewma_ts is None: # first post\n\t\t\t\tewma_ts = ts\n\t\t\t\tcontinue\n\t\t\tewma_ts, interval = ts, (ts - ewma_ts).total_seconds()\n\t\t\tewma = ewma_factor * interval + (1 - ewma_factor) * ewma\n\t\treturn min(timedelta(max_interval).total_seconds(), ewma)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_handler(feeds):\n\t\t'''Update all cross-referencing filters results for feeds and others, related to them.\n\t\t\tIntended to be called from non-Feed update hooks (like new Post saving).'''\n\t\t# Check if this call is a result of actions initiated from\n\t\t#  one of the hooks in a higher frame (resulting in recursion).\n\t\tif Feed._filters_update_handler_lock: return\n\t\treturn Feed._filters_update_handler(Feed, feeds, force=True)", "response": "Update all cross - referencing filters results for feeds and others."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds text - based fields with similarity with specified threshold.", "response": "def similar(self, threshold, **criterias):\n\t\t'''Find text-based field matches with similarity (1-levenshtein/length)\n\t\t\thigher than specified threshold (0 to 1, 1 being an exact match)'''\n\t\t# XXX: use F from https://docs.djangoproject.com/en/1.8/ref/models/expressions/\n\t\tmeta = self.model._meta\n\t\tfuncs, params = list(), list()\n\t\tfor name,val in criterias.iteritems():\n\t\t\tname = meta.get_field(name, many_to_many=False).column\n\t\t\tname = '.'.join(it.imap(connection.ops.quote_name, (meta.db_table, name)))\n\t\t\t# Alas, pg_trgm is for containment tests, not fuzzy matches,\n\t\t\t#  but it can potentially be used to find closest results as well\n\t\t\t# funcs.append( 'similarity(CAST({0}.{1} as text), CAST(%s as text))'\\\n\t\t\t# Ok, these two are just to make sure levenshtein() won't crash\n\t\t\t#  w/ \"argument exceeds the maximum length of N bytes error\"\n\t\t\tfuncs.append('octet_length({0}) <= {1}'.format(name, self.levenshtein_limit))\n\t\t\tfuncs.append('octet_length(%s) <= {0}'.format(self.levenshtein_limit))\n\t\t\t# Then there's a possibility of division by zero...\n\t\t\tfuncs.append('length({0}) > 0'.format(name))\n\t\t\t# And if everything else fits, the comparison itself\n\t\t\tfuncs.append('levenshtein({0}, %s) / CAST(length({0}) AS numeric) < %s'.format(name))\n\t\t\tparams.extend((val, val, float(1 - threshold)))\n\t\treturn self.extra(where=funcs, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filtering_result_checked(self, by_or):\n\t\t'''Check if post passes all / at_least_one (by_or parameter) filter(s).\n\t\t\tFilters are evaluated on only-if-necessary (\"lazy\") basis.'''\n\t\tfilters, results = it.imap(set, ( self.feed.filters.all(),\n\t\t\tself.filtering_results.values_list('filter', flat=True) ))\n\n\t\t# Check if conclusion can already be made, based on cached results.\n\t\tif results.issubset(filters):\n\t\t\t# If at least one failed/passed test is already there, and/or outcome is defined.\n\t\t\ttry: return self._filtering_result(by_or)\n\t\t\texcept IndexError: # inconclusive until results are consistent\n\t\t\t\tif filters == results: return not by_or\n\n\t\t# Consistency check / update.\n\t\tif filters != results:\n\t\t\t# Drop obsolete (removed, unbound from feed)\n\t\t\t#  filters' results (they WILL corrupt outcome).\n\t\t\tself.filtering_results.filter(filter__in=results.difference(filters)).delete()\n\t\t\t# One more try, now that results are only from feed filters' subset.\n\t\t\ttry: return self._filtering_result(by_or)\n\t\t\texcept IndexError: pass\n\t\t\t# Check if any filter-results are not cached yet, create them (perform actual filtering).\n\t\t\t# Note that independent filters applied first, since\n\t\t\t#  crossrefs should be more resource-hungry in general.\n\t\t\tfor filter_obj in sorted(filters.difference(results), key=op.attrgetter('base.crossref')):\n\t\t\t\tfilter_op = FilterResult(filter=filter_obj, post=self, result=filter_obj.handler(self))\n\t\t\t\tfilter_op.save()\n\t\t\t\tif filter_op.result == by_or: return by_or # return as soon as first passed / failed\n\n\t\t# Final result\n\t\ttry: return self._filtering_result(by_or)\n\t\texcept IndexError: return not by_or", "response": "Check if post passes all at least one filter and return True if it passes all."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self):\n        future = concurrent.Future()\n\n        if self.connected:\n            raise exceptions.ConnectError('already connected')\n\n        LOGGER.debug('%s connecting', self.name)\n        self.io_loop.add_future(\n            self._client.connect(self.host, self.port),\n            lambda f: self._on_connected(f, future))\n        return future", "response": "Connect to the Redis server if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, command, future):\n        LOGGER.debug('execute(%r, %r)', command, future)\n        if self.connected:\n            self._write(command, future)\n        else:\n\n            def on_connected(cfuture):\n                if cfuture.exception():\n                    return future.set_exception(cfuture.exception())\n                self._write(command, future)\n\n            self.io_loop.add_future(self.connect(), on_connected)", "response": "Execute a command after connecting if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _on_closed(self):\n        LOGGER.error('Redis connection closed')\n        self.connected = False\n        self._on_close()\n        self._stream = None", "response": "Invoked when the connection is closed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _on_connected(self, stream_future, connect_future):\n        if stream_future.exception():\n            connect_future.set_exception(\n                exceptions.ConnectError(stream_future.exception()))\n        else:\n            self._stream = stream_future.result()\n            self._stream.set_close_callback(self._on_closed)\n            self.connected = True\n            connect_future.set_result(self)", "response": "Invoked when the socket stream has connected"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a command to the socket.", "response": "def _write(self, command, future):\n        \"\"\"Write a command to the socket\n\n        :param Command command: the Command data structure\n\n        \"\"\"\n\n        def on_written():\n            self._on_written(command, future)\n\n        try:\n            self._stream.write(command.command, callback=on_written)\n        except iostream.StreamClosedError as error:\n            future.set_exception(exceptions.ConnectionError(error))\n        except Exception as error:\n            LOGGER.exception('unhandled write failure - %r', error)\n            future.set_exception(exceptions.ConnectionError(error))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self):\n        LOGGER.debug('Creating a%s connection to %s:%s (db %s)',\n                     ' cluster node'\n                     if self._clustering else '', self._hosts[0]['host'],\n                     self._hosts[0]['port'], self._hosts[0].get(\n                         'db', DEFAULT_DB))\n        self._connect_future = concurrent.Future()\n        conn = _Connection(\n            self._hosts[0]['host'],\n            self._hosts[0]['port'],\n            self._hosts[0].get('db', DEFAULT_DB),\n            self._read,\n            self._on_closed,\n            self.io_loop,\n            cluster_node=self._clustering)\n        self.io_loop.add_future(conn.connect(), self._on_connected)\n        return self._connect_future", "response": "Connect to the Redis server or Cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose any open connections to Redis.", "response": "def close(self):\n        \"\"\"Close any open connections to Redis.\n\n        :raises: :exc:`tredis.exceptions.ConnectionError`\n\n        \"\"\"\n        if not self._connected.is_set():\n            raise exceptions.ConnectionError('not connected')\n        self._closing = True\n        if self._clustering:\n            for host in self._cluster.keys():\n                self._cluster[host].close()\n        elif self._connection:\n            self._connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ready(self):\n        if self._clustering:\n            return (all([c.connected for c in self._cluster.values()])\n                    and len(self._cluster))\n        return (self._connection and self._connection.connected)", "response": "Indicates that the client is connected to the Redis server and is ready for use."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_cluster_connection(self, node):\n        LOGGER.debug('Creating a cluster connection to %s:%s', node.ip,\n                     node.port)\n        conn = _Connection(\n            node.ip,\n            node.port,\n            0,\n            self._read,\n            self._on_closed,\n            self.io_loop,\n            cluster_node=True,\n            read_only='slave' in node.flags,\n            slots=node.slots)\n        self.io_loop.add_future(conn.connect(), self._on_connected)", "response": "Create a connection to a Redis server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _eval_expectation(command, response, future):\n        if isinstance(command.expectation, int) and command.expectation > 1:\n            future.set_result(response == command.expectation or response)\n        else:\n            future.set_result(response == command.expectation)", "response": "Evaluate the response from Redis to see if it matches the expected\n        response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninvoking when the Redis server has responded to the CLUSTER_NODES command.", "response": "def _on_cluster_discovery(self, future):\n        \"\"\"Invoked when the Redis server has responded to the ``CLUSTER_NODES``\n        command.\n\n        :param future: The future containing the response from Redis\n        :type future: tornado.concurrent.Future\n\n        \"\"\"\n        LOGGER.debug('_on_cluster_discovery(%r)', future)\n        common.maybe_raise_exception(future)\n        nodes = future.result()\n        for node in nodes:\n            name = '{}:{}'.format(node.ip, node.port)\n            if name in self._cluster:\n                LOGGER.debug('Updating cluster connection info for %s:%s',\n                             node.ip, node.port)\n                self._cluster[name].set_slots(node.slots)\n                self._cluster[name].set_read_only('slave' in node.flags)\n            else:\n                self._create_cluster_connection(node)\n        self._discovery = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_closed(self):\n        self._connected.clear()\n        if not self._closing:\n            if self._on_close_callback:\n                self._on_close_callback()\n            else:\n                raise exceptions.ConnectionError('closed')", "response": "Invoked by the connection when they are closed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _on_cluster_data_moved(self, response, command, future):\n        LOGGER.debug('on_cluster_data_moved(%r, %r, %r)', response, command,\n                     future)\n        parts = response.split(' ')\n        name = '{}:{}'.format(*common.split_connection_host_port(parts[2]))\n        LOGGER.debug('Moved to %r', name)\n        if name not in self._cluster:\n            raise exceptions.ConnectionError(\n                '{} is not connected'.format(name))\n        self._cluster[name].execute(\n            command._replace(connection=self._cluster[name]), future)", "response": "Process the MOVED command from a Redis cluster node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninvoking when the connection is established.", "response": "def _on_connected(self, future):\n        \"\"\"Invoked when connections have been established. If the client is\n        in clustering mode, it will kick of the discovery step if needed. If\n        not, it will select the configured database.\n\n        :param future: The connection future\n        :type future: tornado.concurrent.Future\n\n        \"\"\"\n        if future.exception():\n            self._connect_future.set_exception(future.exception())\n            return\n\n        conn = future.result()\n        LOGGER.debug('Connected to %s (%r, %r, %r)', conn.name,\n                     self._clustering, self._discovery, self._connected)\n        if self._clustering:\n            self._cluster[conn.name] = conn\n            if not self._discovery:\n                self.io_loop.add_future(self.cluster_nodes(),\n                                        self._on_cluster_discovery)\n            elif self.ready:\n                LOGGER.debug('Cluster nodes all connected')\n                if not self._connect_future.done():\n                    self._connect_future.set_result(True)\n                self._connected.set()\n        else:\n\n            def on_selected(sfuture):\n                LOGGER.debug('Initial setup and selection processed')\n                if sfuture.exception():\n                    self._connect_future.set_exception(sfuture.exception())\n                else:\n                    self._connect_future.set_result(True)\n                self._connected.set()\n\n            select_future = concurrent.Future()\n            self.io_loop.add_future(select_future, on_selected)\n            self._connection = conn\n            cmd = Command(\n                self._build_command(['SELECT', str(conn.database)]),\n                self._connection, None, None)\n            cmd.connection.execute(cmd, select_future)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_read_only_error(self, command, future):\n        failover_future = concurrent.TracebackFuture()\n\n        def on_replication_info(_):\n            common.maybe_raise_exception(failover_future)\n            LOGGER.debug('Failover closing current read-only connection')\n            self._closing = True\n            database = self._connection.database\n            self._connection.close()\n            self._connected.clear()\n            self._connect_future = concurrent.Future()\n\n            info = failover_future.result()\n            LOGGER.debug('Failover connecting to %s:%s', info['master_host'],\n                         info['master_port'])\n            self._connection = _Connection(\n                info['master_host'], info['master_port'], database, self._read,\n                self._on_closed, self.io_loop, self._clustering)\n\n            # When the connection is re-established, re-run the command\n            self.io_loop.add_future(\n                self._connect_future,\n                lambda f: self._connection.execute(\n                    command._replace(connection=self._connection), future))\n\n            # Use the normal connection processing flow when connecting\n            self.io_loop.add_future(self._connection.connect(),\n                                    self._on_connected)\n\n        if self._clustering:\n            command.connection.set_readonly(True)\n\n        LOGGER.debug('%s is read-only, need to failover to new master',\n                     command.connection.name)\n\n        cmd = Command(\n            self._build_command(['INFO', 'REPLICATION']), self._connection,\n            None, common.format_info_response)\n\n        self.io_loop.add_future(failover_future, on_replication_info)\n        cmd.connection.execute(cmd, failover_future)", "response": "Invoked when a Redis node returns an error indicating it s in read - only mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read(self, command, future):\n        response = self._reader.gets()\n        if response is not False:\n            if isinstance(response, hiredis.ReplyError):\n                if response.args[0].startswith('MOVED '):\n                    self._on_cluster_data_moved(response.args[0], command,\n                                                future)\n                elif response.args[0].startswith('READONLY '):\n                    self._on_read_only_error(command, future)\n                else:\n                    future.set_exception(exceptions.RedisError(response))\n            elif command.callback is not None:\n                future.set_result(command.callback(response))\n            elif command.expectation is not None:\n                self._eval_expectation(command, response, future)\n            else:\n                future.set_result(response)\n        else:\n\n            def on_data(data):\n                # LOGGER.debug('Read %r', data)\n                self._reader.feed(data)\n                self._read(command, future)\n\n            command.connection.read(on_data)", "response": "Invoked when a command is executed to read and parse its results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the Redis cluster host for the specified value.", "response": "def _pick_cluster_host(self, value):\n        \"\"\"Selects the Redis cluster host for the specified value.\n\n        :param mixed value: The value to use when looking for the host\n        :rtype: tredis.client._Connection\n\n        \"\"\"\n        crc = crc16.crc16(self._encode_resp(value[1])) % HASH_SLOTS\n        for host in self._cluster.keys():\n            for slot in self._cluster[host].slots:\n                if slot[0] <= crc <= slot[1]:\n                    return self._cluster[host]\n        LOGGER.debug('Host not found for %r, returning first connection',\n                     value)\n        host_keys = sorted(list(self._cluster.keys()))\n        return self._cluster[host_keys[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_lines(stream, separator=None):\n    separator = None if separator is None else unicode(separator)\n    for line in stream:\n        line = line.rstrip(u'\\r\\n')\n        row = [interpret_segment(i) for i in line.split(separator)]\n        yield line, row", "response": "Takes each line of a stream creates a generator that yields tuples of line row - where row is the line split by separator\n    or by whitespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_buffer(stream, separator=None):\n    rows = []\n    lines = []\n    for line, row in parse_lines(stream, separator):\n        lines.append(line)\n        rows.append(row)\n    cols = zip(*rows)\n    return {\n        'rows': rows,\n        'lines': lines,\n        'cols': cols,\n        }", "response": "Parses a buffer into a dictionary of the lines of the n - term tables and the columns of the n - term tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to execute the given command importing objects which cause a NameError in the command Taxonomy", "response": "def safe_evaluate(command, glob, local):\n    \"\"\"\n    Continue to attempt to execute the given command, importing objects which\n    cause a NameError in the command\n\n    :param command: command for eval\n    :param glob: globals dict for eval\n    :param local: locals dict for eval\n    :return: command result\n    \"\"\"\n    while True:\n        try:\n            return eval(command, glob, local)\n        except NameError as e:\n            match = re.match(\"name '(.*)' is not defined\", e.message)\n            if not match:\n                raise e\n            try:\n                exec ('import %s' % (match.group(1), )) in glob\n            except ImportError:\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef int_to_gematria(num, gershayim=True):\n    # 1. Lookup in specials\n    if num in specialnumbers['specials']:\n        retval = specialnumbers['specials'][num]\n        return _add_gershayim(retval) if gershayim else retval\n\n    # 2. Generate numeral normally\n    parts = []\n    rest = str(num)\n    while rest:\n        digit = int(rest[0])\n        rest = rest[1:]\n        if digit == 0:\n            continue\n        power = 10 ** len(rest)\n        parts.append(specialnumbers['numerals'][power * digit])\n    retval = ''.join(parts)\n    # 3. Add gershayim\n    return _add_gershayim(retval) if gershayim else retval", "response": "convert integers between 1 and 999 to Hebrew numerals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting video urls from a link to the onetab shared page.", "response": "def get_urls_from_onetab(onetab):\n    \"\"\"\n    Get video urls from a link to the onetab shared page.\n\n    Args:\n        onetab (str): Link to a onetab shared page.\n\n    Returns:\n        list: List of links to the videos.\n\n    \"\"\"\n    html = requests.get(onetab).text\n    soup = BeautifulSoup(html, 'lxml')\n\n    divs = soup.findAll('div', {'style': 'padding-left: 24px; '\n                                         'padding-top: 8px; '\n                                         'position: relative; '\n                                         'font-size: 13px;'})\n\n    return [div.find('a').attrs['href'] for div in divs]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse to create an instance of this class from a pstats dict item", "response": "def from_dict(cls, d):\n        \"\"\"Used to create an instance of this class from a pstats dict item\"\"\"\n        stats = []\n        for (filename, lineno, name), stat_values in d.iteritems():\n            if len(stat_values) == 5:\n                ncalls, ncall_nr, total_time, cum_time, subcall_stats = stat_values\n            else:\n                ncalls, ncall_nr, total_time, cum_time = stat_values\n                subcall_stats = None\n            stat = cProfileFuncStat(filename, lineno, name, ncalls, ncall_nr, total_time, cum_time, subcall_stats)\n            stats.append(stat)\n\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert back to the pstats dictionary representation", "response": "def to_dict(self):\n        \"\"\"Convert back to the pstats dictionary representation (used for saving back as pstats binary file)\"\"\"\n        if self.subcall is not None:\n            if isinstance(self.subcall, dict):\n                subcalls = self.subcall\n            else:\n                subcalls = {}\n                for s in self.subcall:\n                    subcalls.update(s.to_dict())\n            return {(self.filename, self.line_number, self.name): \\\n                        (self.ncalls, self.nonrecursive_calls, self.own_time_s, self.cummulative_time_s, subcalls)}\n        else:\n            return {(self.filename, self.line_number, self.name): \\\n                        (self.ncalls, self.nonrecursive_calls, self.own_time_s, self.cummulative_time_s)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexcluding the contributions from the following functions.", "response": "def exclude_functions(self, *funcs):\n        \"\"\"\n        Excludes the contributions from the following functions.\n        \"\"\"\n        for f in funcs:\n            f.exclude = True\n        run_time_s = sum(0 if s.exclude else s.own_time_s for s in self.stats)\n        cProfileFuncStat.run_time_s = run_time_s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the top n values when sorting by stat", "response": "def get_top(self, stat, n):\n        \"\"\"Return the top n values when sorting by 'stat'\"\"\"\n        return sorted(self.stats, key=lambda x: getattr(x, stat), reverse=True)[:n]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_pstat(self, path):\n        stats = {}\n        for s in self.stats:\n            if not s.exclude:\n                stats.update(s.to_dict())\n\n        with open(path, 'wb') as f:\n            marshal.dump(stats, f)", "response": "Save the modified pstats file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattempts to convert a value to int ; returns 0 if conversion failed", "response": "def safe_int(value):\n    \"\"\"\n    Tries to convert a value to int; returns 0 if conversion failed\n    \"\"\"\n    try:\n        result = int(value)\n        if result < 0:\n            raise NegativeDurationError(\n                'Negative values in duration strings are not allowed!'\n            )\n    except NegativeDurationError as exc:\n        raise exc\n    except (TypeError, ValueError):\n        result = 0\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse(value, strict=True):\n    pattern = r'(?:(?P<hours>\\d+):)?(?P<minutes>\\d+):(?P<seconds>\\d+)'\n    match = re.match(pattern, value)\n    if not match:\n        raise ValueError('Invalid duration value: %s' % value)\n    hours = safe_int(match.group('hours'))\n    minutes = safe_int(match.group('minutes'))\n    seconds = safe_int(match.group('seconds'))\n\n    check_tuple((hours, minutes, seconds,), strict)\n\n    return (hours, minutes, seconds,)", "response": "Parses a duration value into hours minutes seconds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_iso8601(value, strict=True, force_int=True):\n    # split seconds to larger units\n    # seconds = value.total_seconds()\n    seconds = to_seconds(value, strict, force_int)\n\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    days, hours = divmod(hours, 24)\n    days, hours, minutes = map(int, (days, hours, minutes))\n    seconds = round(seconds, 6)\n\n    # build date\n    date = ''\n    if days:\n        date = '%sD' % days\n\n    # build time\n    time = 'T'\n\n    # hours\n    bigger_exists = date or hours\n    if bigger_exists:\n        time += '{:02}H'.format(hours)\n\n    # minutes\n    bigger_exists = bigger_exists or minutes\n    if bigger_exists:\n        time += '{:02}M'.format(minutes)\n\n    # seconds\n    if isinstance(seconds, int) or force_int:\n        seconds = '{:02}'.format(int(seconds))\n    else:\n        # 9 chars long w/leading 0, 6 digits after decimal\n        seconds = '%09.6f' % seconds\n\n    time += '{}S'.format(seconds)\n    return 'P' + date + time", "response": "converts a duration value to ISO8601 string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a duration value to seconds", "response": "def to_seconds(value, strict=True, force_int=True):\n    \"\"\"\n    converts duration value to integer seconds\n\n    strict=True (by default) raises StrictnessError if either hours,\n    minutes or seconds in duration value exceed allowed values\n    \"\"\"\n    if isinstance(value, int):\n        return value  # assuming it's seconds\n    elif isinstance(value, timedelta):\n        seconds = value.total_seconds()\n        if force_int:\n            seconds = int(round(seconds))\n        return seconds\n    elif isinstance(value, str):\n        hours, minutes, seconds = _parse(value, strict)\n    elif isinstance(value, tuple):\n        check_tuple(value, strict)\n        hours, minutes, seconds = value\n    else:\n        raise TypeError(\n            'Value %s (type %s) not supported' % (\n                value, type(value).__name__\n            )\n        )\n\n    if not (hours or minutes or seconds):\n        raise ValueError('No hours, minutes or seconds found')\n\n    result = hours*3600 + minutes*60 + seconds\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts duration string to timedelta", "response": "def to_timedelta(value, strict=True):\n    \"\"\"\n    converts duration string to timedelta\n\n    strict=True (by default) raises StrictnessError if either hours,\n    minutes or seconds in duration string exceed allowed values\n    \"\"\"\n    if isinstance(value, int):\n        return timedelta(seconds=value)  # assuming it's seconds\n    elif isinstance(value, timedelta):\n        return value\n    elif isinstance(value, str):\n        hours, minutes, seconds = _parse(value, strict)\n    elif isinstance(value, tuple):\n        check_tuple(value, strict)\n        hours, minutes, seconds = value\n    else:\n        raise TypeError(\n            'Value %s (type %s) not supported' % (\n                value, type(value).__name__\n            )\n        )\n    return timedelta(hours=hours, minutes=minutes, seconds=seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_tuple(value, strict=True, force_int=True):\n    if isinstance(value, int):\n        seconds = value\n        minutes, seconds = divmod(seconds, 60)\n        hours, minutes = divmod(minutes, 60)\n    elif isinstance(value, str):\n        hours, minutes, seconds = _fix_tuple(\n            _parse(value, strict)\n        )\n    elif isinstance(value, tuple):\n        check_tuple(value, strict)\n        hours, minutes, seconds = _fix_tuple(value)\n    elif isinstance(value, timedelta):\n        seconds = value.total_seconds()\n        if force_int:\n            seconds = int(round(seconds))\n        minutes, seconds = divmod(seconds, 60)\n        hours, minutes = divmod(minutes, 60)\n\n    return (hours, minutes, seconds,)", "response": "Converts duration value to tuple of integers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a URL for a method in a driver and cloud.", "response": "def name_url(provider, cloud, method_name):\n    \"\"\"\n    Get a URL for a method in a driver\n    \"\"\"\n    snake_parts = method_name.split('_')\n    if len(snake_parts) <= 1:\n        return False\n\n    # Convention for libcloud is ex_ are extended methods\n    if snake_parts[0] == 'ex':\n        extra = True\n        method_name = method_name.replace('ex_', '', 1)\n    else:\n        extra = False\n    snake_parts = method_name.split('_')\n    # Try to semantically match the method name to a REST action\n    if snake_parts[0] in get_sem_verbs:\n        method = 'GET'\n        for verb in get_sem_verbs:\n            method_name = method_name.replace('%s_' % verb, '', 1)\n    elif snake_parts[0] in delete_sem_verbs:\n        method = 'DELETE'\n    elif snake_parts[0] in put_sem_verbs:\n        method = 'PUT'\n    else:\n        method = 'POST'\n\n    uri = '/%s/%s/%s%s' % (provider,\n                           cloud,\n                           'extensions/' if extra else '',\n                           method_name)\n    return (method, uri)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contains_frame(data):\n    if len(data) < 2:\n        return False\n\n    b2 = struct.unpack('!B', data[1])[0]\n    payload_len = b2 & 0x7F\n    payload_start = 2\n\n    if payload_len == 126:\n        if len(data) > 4:\n            payload_len = struct.unpack('!H', data[2:4])[0]\n\n        payload_start = 4\n    elif payload_len == 127:\n        if len(data) > 12:\n            payload_len = struct.unpack('!Q', data[4:12])[0]\n\n        payload_start = 12\n\n    return len(data) >= payload_len + payload_start", "response": "Check if the data contains a single frame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmask an octet string using the given masking key.", "response": "def mask(key, original):\n    \"\"\"\n    Mask an octet string using the given masking key.\n    The following masking algorithm is used, as defined in RFC 6455:\n\n    for each octet:\n        j = i MOD 4\n        transformed-octet-i = original-octet-i XOR masking-key-octet-j\n    \"\"\"\n    if len(key) != 4:\n        raise ValueError('invalid masking key \"%s\"' % key)\n\n    key = map(ord, key)\n    masked = bytearray(original)\n\n    for i in xrange(len(masked)):\n        masked[i] ^= key[i % 4]\n\n    return masked"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pack(self):\n        header = struct.pack('!B', (self.final << 7) | (self.rsv1 << 6)\n                                   | (self.rsv2 << 5) | (self.rsv3 << 4)\n                                   | (self.opcode & 0xf))\n        mask = bool(self.masking_key) << 7\n        payload_len = len(self.payload)\n\n        if payload_len <= 125:\n            header += struct.pack('!B', mask | payload_len)\n        elif payload_len < (1 << 16):\n            header += struct.pack('!BH', mask | 126, payload_len)\n        elif payload_len < (1 << 63):\n            header += struct.pack('!BQ', mask | 127, payload_len)\n        else:\n            # FIXME: RFC 6455 defines an action for this...\n            raise Exception('the payload length is too damn high!')\n\n        if mask:\n            return header + self.masking_key + self.mask_payload()\n\n        return header + self.payload", "response": "Pack the current state of the message into a string according to the RFC 2965 section 3. 1. 3. 1."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fragment(self, fragment_size, mask=False):\n        frames = []\n\n        for start in xrange(0, len(self.payload), fragment_size):\n            payload = self.payload[start:start + fragment_size]\n            frames.append(Frame(OPCODE_CONTINUATION, payload, mask=mask,\n                                final=False))\n\n        frames[0].opcode = self.opcode\n        frames[-1].final = True\n\n        return frames", "response": "This function returns a list of frames containing at least one fragment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_close(self):\n        if self.payload:\n            code = struct.unpack('!H', str(self.payload[:2]))[0]\n            reason = str(self.payload[2:])\n        else:\n            code = None\n            reason = ''\n\n        return code, reason", "response": "Unpack a close message into a status code and a reason."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading n bytes from the socket.", "response": "def readn(self, n):\n        \"\"\"\n        Keep receiving data until exactly `n` bytes have been read.\n        \"\"\"\n        data = ''\n\n        while len(data) < n:\n            received = self.sock.recv(n - len(data))\n\n            if not len(received):\n                raise socket.error('no data read from socket')\n\n            data += received\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mic_required(target_info):\n    if target_info is not None and target_info[TargetInfo.NTLMSSP_AV_FLAGS] is not None:\n        flags = struct.unpack('<I', target_info[TargetInfo.NTLMSSP_AV_FLAGS][1])[0]\n        return bool(flags & 0x00000002)", "response": "Checks the MsvAvFlags field of the supplied TargetInfo structure to determine if the MIC flag is set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize_security_context(self):\n        # Generate the NTLM Negotiate Request\n        negotiate_token = self._negotiate(self.flags)\n        challenge_token = yield negotiate_token\n\n        # Generate the Authenticate Response\n        authenticate_token = self._challenge_response(negotiate_token, challenge_token)\n        yield authenticate_token", "response": "Generator function that returns the NTLM Negotiate Request and Authenticate Response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping a message with the appropriate signature and optionally encrypts it.", "response": "def wrap_message(self, message):\n        \"\"\"\n        Cryptographically signs and optionally encrypts the supplied message. The message is only encrypted if\n        'confidentiality' was negotiated, otherwise the message is left untouched.\n        :return: A tuple containing the message signature and the optionally encrypted message\n        \"\"\"\n        if not self.is_established:\n            raise Exception(\"Context has not been established\")\n        if self._wrapper is None:\n            raise Exception(\"Neither sealing or signing have been negotiated\")\n        else:\n            return self._wrapper.wrap(message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unwrap_message(self, message, signature):\n        if not self.is_established:\n            raise Exception(\"Context has not been established\")\n        if self._wrapper is None:\n            raise Exception(\"Neither sealing or signing have been negotiated\")\n        else:\n            return self._wrapper.unwrap(message, signature)", "response": "Unwraps the supplied message and returns the decrypted message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_options():\n    parser = argparse.ArgumentParser(description='Video downloader by radzak.',\n                                     prog='RTVdownloader')\n    urls_group = parser.add_mutually_exclusive_group(required=True)\n    urls_group.add_argument('urls',\n                            type=str,\n                            metavar='URL',\n                            default=[],\n                            nargs='*',\n                            help='urls of sites containing videos you wish to download'\n                            )\n\n    urls_group.add_argument('-f',\n                            type=argparse.FileType('r'),\n                            dest='files',\n                            metavar='FILE',\n                            default=[],\n                            nargs='*',\n                            help='text file with urls of sites containing videos you '\n                                 'wish to download '\n                            )\n\n    urls_group.add_argument('-o',\n                            type=str,\n                            dest='onetabs',\n                            metavar='ONETAB',\n                            default=[],\n                            nargs='*',\n                            help='onetab links containing urls of the videos you wish to download'\n                            )\n\n    options = DEFAULT_OPTIONS\n\n    # TODO: add dir option that defaults to the DEFAULT_OPTIONS['dl_path']\n\n    args = parser.parse_args()\n    return options, args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the url of the TVP Info article.", "response": "def get_article_url(self):\n        \"\"\"\n        Get the url of the TVP Info article itself, not the url of the preview with\n        the 'Przejd\u017a do artyku\u0142u' hyperlink.\n\n        Returns:\n            (str): Url of the article with the video.\n\n        \"\"\"\n        html = requests.get(self.url).text\n        soup = BeautifulSoup(html, 'lxml')\n        div = soup.find('div', class_='more-back')\n\n        if div:\n            parsed_uri = urlparse(self.url)\n            domain = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)\n            suffix = div.find('a', href=True)['href'].strip()\n            article_url = domain + suffix\n            return article_url\n        else:\n            return self.url"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the CAIM to the set of missing values.", "response": "def fit(self, X, y):\n        \"\"\"\n        Fit CAIM\n        Parameters\n        ----------\n        X : array-like, pandas dataframe, shape [n_samples, n_feature]\n            Input array can contain missing values\n        y:  array-like, pandas dataframe, shape [n_samples]\n            Target variable. Must be categorical.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        self.split_scheme = dict()\n        if isinstance(X, pd.DataFrame):\n            # self.indx = X.index\n            # self.columns = X.columns\n            if isinstance(self._features, list):\n                self.categorical = [X.columns.get_loc(label) for label in self._features]\n            X = X.values\n            y = y.values\n        if self._features == 'auto':\n            self.categorical = self.check_categorical(X, y)\n        categorical = self.categorical\n        print('Categorical', categorical)\n\n        min_splits = np.unique(y).shape[0]\n\n        for j in range(X.shape[1]):\n            if j in categorical:\n                continue\n            xj = X[:, j]\n            xj = xj[np.invert(np.isnan(xj))]\n            new_index = xj.argsort()\n            xj = xj[new_index]\n            yj = y[new_index]\n            allsplits = np.unique(xj)[1:-1].tolist()  # potential split points\n            global_caim = -1\n            mainscheme = [xj[0], xj[-1]]\n            best_caim = 0\n            k = 1\n            while (k <= min_splits) or ((global_caim < best_caim) and (allsplits)):\n                split_points = np.random.permutation(allsplits).tolist()\n                best_scheme = None\n                best_point = None\n                best_caim = 0\n                k = k + 1\n                while split_points:\n                    scheme = mainscheme[:]\n                    sp = split_points.pop()\n                    scheme.append(sp)\n                    scheme.sort()\n                    c = self.get_caim(scheme, xj, yj)\n                    if c > best_caim:\n                        best_caim = c\n                        best_scheme = scheme\n                        best_point = sp\n                if (k <= min_splits) or (best_caim > global_caim):\n                    mainscheme = best_scheme\n                    global_caim = best_caim\n                    try:\n                        allsplits.remove(best_point)\n                    except ValueError:\n                        raise NotEnoughPoints('The feature #' + str(j) + ' does not have' +\n                                              ' enough unique values for discretization!' +\n                                              ' Add it to categorical list!')\n\n            self.split_scheme[j] = mainscheme\n            print('#', j, ' GLOBAL CAIM ', global_caim)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(self, X):\n\n        if isinstance(X, pd.DataFrame):\n            self.indx = X.index\n            self.columns = X.columns\n            X = X.values\n        X_di = X.copy()\n        categorical = self.categorical\n\n        scheme = self.split_scheme\n        for j in range(X.shape[1]):\n            if j in categorical:\n                continue\n            sh = scheme[j]\n            sh[-1] = sh[-1] + 1\n            xj = X[:, j]\n            # xi = xi[np.invert(np.isnan(xi))]\n            for i in range(len(sh) - 1):\n                ind = np.where((xj >= sh[i]) & (xj < sh[i + 1]))[0]\n                X_di[ind, j] = i\n        if hasattr(self, 'indx'):\n            return pd.DataFrame(X_di, index=self.indx, columns=self.columns)\n        return X_di", "response": "Discretizes X using a split scheme obtained with CAIM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prettyhtml(value, autoescape=None):\n\t'Clean (and optionally escape) passed html of unsafe tags and attributes.'\n\tvalue = html_cleaner(value)\n\treturn escape(value) if autoescape\\\n\t\tand not isinstance(value, SafeData) else mark_safe(value)", "response": "Clean and optionally escape passed html of unsafe tags and attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget N chars of secure hash hexdigest of value.", "response": "def hash(value, chars=None):\n\t'Get N chars (default: all) of secure hash hexdigest of value.'\n\tvalue = hash_func(value).hexdigest()\n\tif chars: value = value[:chars]\n\treturn mark_safe(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_with_wd_noise(f_n, amp_n, f_n_wd, amp_n_wd):\n\n    # interpolate wd noise\n    amp_n_wd_interp = interpolate.interp1d(f_n_wd, amp_n_wd, bounds_error=False, fill_value=1e-30)\n\n    # find points of wd noise amplitude at noise curve frequencies\n    amp_n_wd = amp_n_wd_interp(f_n)\n\n    # keep the greater value at each frequency\n    amp_n = amp_n*(amp_n >= amp_n_wd) + amp_n_wd*(amp_n < amp_n_wd)\n    return f_n, amp_n", "response": "Combine noise with wd noise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist available noise curves in noise_curves folder.", "response": "def show_available_noise_curves(return_curves=True, print_curves=False):\n    \"\"\"List available sensitivity curves\n\n    This function lists the available sensitivity curve strings in noise_curves folder.\n\n    Args:\n        return_curves (bool, optional): If True, return a list of curve options.\n        print_curves (bool, optional): If True, print each curve option.\n\n    Returns:\n        (optional list of str): List of curve options.\n\n    Raises:\n        ValueError: Both args are False.\n\n    \"\"\"\n    if return_curves is False and print_curves is False:\n        raise ValueError(\"Both return curves and print_curves are False.\"\n                         + \" You will not see the options\")\n    cfd = os.path.dirname(os.path.abspath(__file__))\n    curves = [curve.split('.')[0] for curve in os.listdir(cfd + '/noise_curves/')]\n    if print_curves:\n        for f in curves:\n            print(f)\n    if return_curves:\n        return curves\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake string and returns list of options that can be used to expand a single curly phrase.", "response": "def expand_curlys(s):\n    \"\"\"Takes string and returns list of options:\n\n    Example\n    -------\n    >>> expand_curlys(\"py{26, 27}\")\n    [\"py26\", \"py27\"]\n\n    \"\"\"\n    from functools import reduce\n    curleys = list(re.finditer(r\"{[^{}]*}\", s))\n    return reduce(_replace_curly, reversed(curleys), [s])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_out_of_braces(s):\n    prev = 0\n    for m in re.finditer(r\"{[^{}]*}|\\s*,\\s*\", s):\n        if not m.group().startswith(\"{\"):\n            part = s[prev:m.start()]\n            if part:\n                yield s[prev:m.start()]\n            prev = m.end()\n    part = s[prev:]\n    if part:\n        yield part", "response": "Generator to split comma seperated string but not split commas inside\n    curly braces."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expand_factor_conditions(s, env):\n    try:\n        factor, value = re.split(r'\\s*\\:\\s*', s)\n    except ValueError:\n        return s\n\n    if matches_factor_conditions(factor, env):\n        return value\n    else:\n        return ''", "response": "Expand the factor conditions in a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if py33 34 expanded is contained in env. name.", "response": "def matches_factor_conditions(s, env):\n    \"\"\"\"Returns True if py{33, 34} expanded is contained in env.name.\"\"\"\n    env_labels = set(env.name.split('-'))\n    labels = set(bash_expand(s))\n    return bool(labels & env_labels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_on(s, sep=\" \"):\n    pattern = '''((?:[^%s\"']|\"[^\"]*\"|'[^']*')+)''' % sep\n\n    return [_strip_speechmarks(t) for t in re.split(pattern, s)[1::2]]", "response": "Split s by sep unless it s inside a quote."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes tox substitutions to s with respect to environment env.", "response": "def replace_braces(s, env):\n    \"\"\"Makes tox substitutions to s, with respect to environment env.\n\n    Example\n    -------\n    >>> replace_braces(\"echo {posargs:{env:USER:} passed no posargs}\")\n    \"echo andy passed no posargs\"\n\n    Note: first \"{env:USER:}\" is replaced with os.environ.get(\"USER\", \"\"),\n    the \"{posargs:andy}\" is replaced with \"andy\" (since no posargs were\n    passed).\n\n    \"\"\"\n    def replace(m):\n        return _replace_match(m, env)\n    for _ in range(DEPTH):\n        s = re.sub(r\"{[^{}]*}\", replace, s)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _replace_match(m, env):\n    # ditch the curly braces\n    s = m.group()[1:-1].strip()\n\n    try:\n        # get the env attributes e.g. envpython or toxinidir.\n        # Note: if you ask for a env methodname this will raise\n        # later on... so don't do that.\n        return getattr(env, s)\n    except AttributeError:\n        pass\n\n    for r in [_replace_envvar, _replace_config, _replace_posargs]:\n        try:\n            return r(s, env)\n        except ValueError:\n            pass\n\n    raise NotImplementedError(\"{%s} not understood in tox.ini file.\" % s)", "response": "Given a match object having matched something inside curly braces replace the contents if matches one of the supported tox - substitutions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace environment variables in a string.", "response": "def _replace_envvar(s, _):\n    \"\"\"env:KEY or env:KEY:DEFAULT\"\"\"\n    e = s.split(\":\")\n    if len(e) > 3 or len(e) == 1 or e[0] != \"env\":\n        raise ValueError()\n    elif len(e) == 2:\n        # Note: this can/should raise a KeyError (according to spec).\n        return os.environ[e[1]]\n    else:  # len(e) == 3\n        return os.environ.get(e[1], e[2])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace factor conditions in a config string.", "response": "def _replace_config(s, env):\n    \"\"\"[sectionname]optionname\"\"\"\n    m = re.match(r\"\\[(.*?)\\](.*)\", s)\n    if m:\n        section, option = m.groups()\n        expanded = env.config.get(section, option)\n        return '\\n'.join([expand_factor_conditions(e, env)\n                          for e in expanded.split(\"\\n\")])\n    else:\n        raise ValueError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace posargs in a string.", "response": "def _replace_posargs(s, env):\n    \"posargs:DEFAULT\"\n    e = re.split(r'\\s*\\:\\s*', s)\n    if e and e[0] == \"posargs\":\n        from ctox.main import positional_args\n        return (\" \".join(positional_args(env.options)) or\n                (e[1] if len(e) > 1 else \"\"))\n    else:\n        raise ValueError()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef csnr(freqs, hc, hn, fmrg, fpeak, prefactor=1.0):\n    cfd = os.path.dirname(os.path.abspath(__file__))\n    if 'phenomd.cpython-35m-darwin.so' in os.listdir(cfd):\n        exec_call = cfd + '/phenomd.cpython-35m-darwin.so'\n\n    else:\n        exec_call = cfd + '/phenomd/phenomd.so'\n\n    c_obj = ctypes.CDLL(exec_call)\n\n    # check dimensionality\n    remove_axis = False\n    try:\n        len(fmrg)\n    except TypeError:\n        remove_axis = True\n        freqs, hc = np.array([freqs]), np.array([hc])\n        hn, fmrg, fpeak = np.array([hn]), np.array([fmrg]), np.array([fpeak])\n\n    # this implimentation in ctypes works with 1D arrays\n    freqs_in = freqs.flatten()\n    hc_in = hc.flatten()\n    hn_in = hn.flatten()\n\n    num_binaries, length_of_signal = hc.shape\n\n    # prepare outout arrays\n    snr_cast = ctypes.c_double*num_binaries\n    snr_all = snr_cast()\n    snr_ins = snr_cast()\n    snr_mrg = snr_cast()\n    snr_rd = snr_cast()\n\n    # find SNR values\n    c_obj.SNR_function(ctypes.byref(snr_all), ctypes.byref(snr_ins),\n                       ctypes.byref(snr_mrg), ctypes.byref(snr_rd),\n                       freqs_in.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                       hc_in.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                       hn_in.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                       fmrg.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                       fpeak.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                       ctypes.c_int(length_of_signal), ctypes.c_int(num_binaries))\n\n    # make into numpy arrays\n    snr_all, snr_ins, = np.ctypeslib.as_array(snr_all), np.ctypeslib.as_array(snr_ins)\n    snr_mrg, snr_rd = np.ctypeslib.as_array(snr_mrg), np.ctypeslib.as_array(snr_rd)\n\n    # remove axis if one binary\n    if remove_axis:\n        snr_all, snr_ins, snr_mrg, snr_rd = snr_all[0], snr_ins[0], snr_mrg[0], snr_rd[0]\n\n    # prepare output by multiplying by prefactor\n    return ({'all': snr_all*prefactor, 'ins': snr_ins*prefactor,\n            'mrg': snr_mrg*prefactor, 'rd': snr_rd*prefactor})", "response": "Calculate the SNR of a frequency domain waveform."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsolves the T problem for a single class", "response": "def solve_t(self, Mt):\n        \"\"\"\n        Mt is dim_r x dim_c x d tensor\n        \"\"\"\n        if len(Mt.shape)==2:    _Mt = Mt[:, :, sp.newaxis]\n        else:                   _Mt = Mt\n        LMt = vei_CoR_veX(_Mt, R=self.Lr(), C=self.Lc())\n        DMt = self.D()[:, :, sp.newaxis] * LMt\n        WrDMtWc = vei_CoR_veX(DMt, R=self.Wr().T, C=self.Wc().T)\n        ve_WrDMtWc = sp.reshape(WrDMtWc, (WrDMtWc.shape[0] * WrDMtWc.shape[1], _Mt.shape[2]), order='F')\n        Hi_ve_WrDMtWc = la.cho_solve((self.H_chol(), True), ve_WrDMtWc)\n        vei_HiveWrDMtWc = Hi_ve_WrDMtWc.reshape(WrDMtWc.shape, order = 'F')\n        Wr_HiveWrDMtWc_Wc = vei_CoR_veX(vei_HiveWrDMtWc, R=self.Wr(), C=self.Wc())\n        DWrHiveWrDMtWcWc = self.D()[:,:,sp.newaxis] * Wr_HiveWrDMtWc_Wc\n        RV = DMt - DWrHiveWrDMtWcWc\n        RV = vei_CoR_veX(RV, R=self.Lr().T, C=self.Lc().T)\n        if len(Mt.shape)==2:    RV = RV[:, :, 0]\n        return RV"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the dot product of the two sets of discrete elements", "response": "def _O_dot(self, Mt):\n        \"\"\"\n        Mt is dim_r x dim_c x d tensor\n        \"\"\"\n        DMt = self.D()[:, :, sp.newaxis] * Mt\n        WrDMtWc = vei_CoR_veX(DMt, R=self.Wr().T, C=self.Wc().T)\n        ve_WrDMtWc = sp.reshape(WrDMtWc, (WrDMtWc.shape[0] * WrDMtWc.shape[1], Mt.shape[2]), order='F')\n        Hi_ve_WrDMtWc = la.cho_solve((self.H_chol(), True), ve_WrDMtWc)\n        vei_HiveWrDMtWc = Hi_ve_WrDMtWc.reshape(WrDMtWc.shape, order = 'F')\n        Wr_HiveWrDMtWc_Wc = vei_CoR_veX(vei_HiveWrDMtWc, R=self.Wr(), C=self.Wc())\n        DWrHiveWrDMtWcWc = self.D()[:,:,sp.newaxis] * Wr_HiveWrDMtWc_Wc\n        RV = DMt - DWrHiveWrDMtWcWc\n        return RV"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clearFixedEffect(self):\n        self._A = []\n        self._F = []\n        self._B = []\n        self._A_identity = []\n        self._REML_term = []\n        self._n_terms = 0\n        self._n_fixed_effs = 0\n        self._n_fixed_effs_REML = 0\n        self.indicator = {'term':np.array([]),\n                            'row':np.array([]),\n                            'col':np.array([])}\n        self.clear_cache('Fstar','Astar','Xstar','Xhat',\n                         'Areml','Areml_eigh','Areml_chol','Areml_inv','beta_hat','B_hat',\n                         'LRLdiag_Xhat_tens','Areml_grad',\n                         'beta_grad','Xstar_beta_grad','Zstar','DLZ')", "response": "erase all fixed effects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addFixedEffect(self,F=None,A=None, REML=True, index=None):\n        if F is None:   F = np.ones((self.N,1))\n        if A is None:\n            A = np.eye(self.P)\n            A_identity = True\n        elif (A.shape == (self.P,self.P)) & (A==np.eye(self.P)).all():\n            A_identity = True\n        else:\n            A_identity = False\n\n        assert F.shape[0]==self.N, \"F dimension mismatch\"\n        assert A.shape[1]==self.P, \"A dimension mismatch\"\n        if index is None or index==self.n_terms:\n            self.F.append(F)\n            self.A.append(A)\n            self.A_identity.append(A_identity)\n            self.REML_term.append(REML)\n            # build B matrix and indicator\n            self.B.append(np.zeros((F.shape[1],A.shape[0])))\n            self._n_terms+=1\n            self._update_indicator(F.shape[1],A.shape[0])\n        elif index >self.n_terms:\n            raise Exception(\"index exceeds max index of terms\")\n        else:\n            self._n_fixed_effs-=self.F[index].shape[1]*self.A[index].shape[0]\n            if self.REML_term[index]:\n                self._n_fixed_effs_REML-=self.F[index].shape[1]*self.A[index].shape[0]\n            self.F[index] = F\n            self.A[index] = A\n            self.A_identity[index] = A_identity\n            self.REML_term[index]=REML\n            self.B[index] = np.zeros((F.shape[1],A.shape[0]))\n            self._rebuild_indicator()\n\n        self._n_fixed_effs+=F.shape[1]*A.shape[0]\n        if REML:\n            self._n_fixed_effs_REML+=F.shape[1]*A.shape[0]\n        self.clear_cache('Fstar','Astar','Xstar','Xhat',\n                         'Areml','Areml_eigh','Areml_chol','Areml_inv','beta_hat','B_hat',\n                         'LRLdiag_Xhat_tens','Areml_grad',\n                         'beta_grad','Xstar_beta_grad','Zstar','DLZ')", "response": "Add fixed effect to the set of terms."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef removeFixedEffect(self, index=None):\n        if self._n_terms==0:\n            pass\n        if index is None or index==(self._n_terms-1):\n\n            self._n_terms-=1\n            F = self._F.pop() #= self.F[:-1]\n            A = self._A.pop() #= self.A[:-1]\n            self._A_identity.pop() #= self.A_identity[:-1]\n            REML_term = self._REML_term.pop()# = self.REML_term[:-1]\n            self._B.pop()# = self.B[:-1]\n            self._n_fixed_effs-=F.shape[1]*A.shape[0]\n            if REML_term:\n                self._n_fixed_effs_REML-=F.shape[1]*A.shape[0]\n\n            pass\n        elif index >= self.n_terms:\n            raise Exception(\"index exceeds max index of terms\")\n        else:\n            raise NotImplementedError(\"currently only last term can be removed\")\n            pass\n        self._rebuild_indicator()\n        self.clear_cache('Fstar','Astar','Xstar','Xhat',\n                         'Areml','Areml_eigh','Areml_chol','Areml_inv','beta_hat','B_hat',\n                         'LRLdiag_Xhat_tens','Areml_grad',\n                         'beta_grad','Xstar_beta_grad','Zstar','DLZ')", "response": "Removes the fixed effect for the current term."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Zstar(self):\n        RV = self.Ystar().copy()\n        for term_i in range(self.n_terms):\n            if self.identity_trick and self.A_identity[term_i]:\n                RV-=np.dot(self.Fstar()[term_i],self.B_hat()[term_i])\n            else:\n                RV-=np.dot(self.Fstar()[term_i],np.dot(self.B_hat()[term_i],self.Astar()[term_i]))\n        self.clear_cache('DLZ')\n        return RV", "response": "predict the value of the fixed effect"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the eigenvalue decomposition of Astar", "response": "def Areml_eigh(self):\n        \"\"\"compute the eigenvalue decomposition of Astar\"\"\"\n        s,U = LA.eigh(self.Areml(),lower=True)\n        i_pos = (s>1e-10)\n        s = s[i_pos]\n        U = U[:,i_pos]\n        return s,U"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npredicts the value of the fixed effect", "response": "def predict(self):\n        \"\"\" predict the value of the fixed effect \"\"\"\n        RV = np.zeros((self.N,self.P))\n        for term_i in range(self.n_terms):\n            RV+=np.dot(self.Fstar()[term_i],np.dot(self.B()[term_i],self.Astar()[term_i]))\n        return RV"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getGradient(self,j):\n        i = int(self.indicator['term'][j])\n        r = int(self.indicator['row'][j])\n        c = int(self.indicator['col'][j])\n        rv = -np.kron(self.Fstar()[i][:,[r]],self.Astar()[i][[c],:])\n        return rv", "response": "get rotated gradient for fixed effect i"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef XstarT_dot(self,M):\n        if 0:\n            #TODO: implement this properly\n            pass\n        else:\n            RV = np.dot(self.Xstar().T,M)\n        return RV", "response": "get dot product of Xhat and M"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregress out fixed effects and results residuals", "response": "def getResiduals(self):\n        \"\"\" regress out fixed effects and results residuals \"\"\"\n        X = np.zeros((self.N*self.P,self.n_fixed_effs))\n        ip = 0\n        for i in range(self.n_terms):\n            Ki = self.A[i].shape[0]*self.F[i].shape[1]\n            X[:,ip:ip+Ki] = np.kron(self.A[i].T,self.F[i])\n            ip += Ki\n        y = np.reshape(self.Y,(self.Y.size,1),order='F')\n        RV = regressOut(y,X)\n        RV = np.reshape(RV,self.Y.shape,order='F')\n        return RV"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget params from the term table", "response": "def getParams(self):\n        \"\"\" get params \"\"\"\n        rv = np.array([])\n        if self.n_terms>0:\n            rv = np.concatenate([np.reshape(self.B[term_i],self.B[term_i].size, order='F') for term_i in range(self.n_terms)])\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setParams(self,params):\n        start = 0\n        for i in range(self.n_terms):\n            n_effects = self.B[i].size\n            self.B[i] = np.reshape(params[start:start+n_effects],self.B[i].shape, order='F')\n            start += n_effects", "response": "set params of the log likelihood matrix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_toChange(x):\n        for key in list(x.keys()):\n            self.toChange[key] = True", "response": "set variables in list x toChange"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vei_CoR_veX(X, C=None, R=None):\n    _X = X.transpose((0,2,1))\n    if R is not None:   RV = sp.tensordot(R, _X, (1,0))\n    else:               RV = _X\n    if C is not None:   RV = sp.dot(RV, C.T)\n    return RV.transpose((0,2,1))", "response": "Function to calculate the covariance matrix of a single resource in the VEI system."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize_shade(self, shade_name, shade_color, alpha):\n\n        # Create the pygame surface\n        self.shades[shade_name] = [0, pygame.Surface(self.image.get_size())]\n\n        # Fill the surface with a solid color or an image\n        if type(shade_color) == str:\n            background = pygame.image.load(shade_color).convert()\n            background = pygame.transform.scale(background,\n                                                (self.image.get_width(),\n                                                 self.image.get_height()))\n            self.shades[shade_name][1].blit(background, (0, 0))\n        # Otherwise the background should contain an rgb value\n        else:\n            self.shades[shade_name][1].fill(shade_color)\n\n        # Set the alpha value for the shade\n        self.shades[shade_name][1].set_alpha(alpha)", "response": "This method will create semi - transparent surfaces with a specified color. The surface can be toggled on and off. The surface can be toggled on and off. The surface can be toggled on and off. The surface can be toggled on and off. The surface can be toggled off and the surface can be toggled on and off."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adjacent_tiles(self, tile, pattern):\n\n            # Initialize the list of tiles to return\n            adj_tiles = []\n\n            # Find the row and column of the input tile\n            for i in self:\n                for j in i:\n                    if j == tile:\n                        row = self.index(i)\n                        column = self[row].index(j)\n\n            # Define functions for the 2 distinct patterns\n            def plus_sign(self, row, column):\n                nonlocal adj_tiles\n                if row - 1 >= 0:\n                    adj_tiles += [self[row - 1][column]]\n                if row + 1 != len(self):\n                    adj_tiles += [self[row + 1][column]]\n                if column - 1 >= 0:\n                    adj_tiles += [self[row][column - 1]]\n                if column + 1 != len(self[row]):\n                    adj_tiles += [self[row][column + 1]]\n\n            def diagonal(self, row, column):\n                nonlocal adj_tiles\n                if column - 1 >= 0:\n                    if row - 1 >= 0:\n                        adj_tiles += [self[row - 1][column - 1]]\n                    if row + 1 != len(self):\n                        adj_tiles += [self[row + 1][column - 1]]\n                if column + 1 != len(self[row]):\n                    if row - 1 >= 0:\n                        adj_tiles += [self[row - 1][column + 1]]\n                    if row + 1 != len(self):\n                        adj_tiles += [self[row + 1][column + 1]]\n\n            # Return the tiles that form a plus sign with the given input tile\n            if pattern == 'p':\n                plus_sign(self, row, column)\n\n            # Return the tiles touching the four corners of the input tile\n            elif pattern == 'x':\n                diagonal(self, row, column)\n\n            # Return all of the tiles surrounding the input tile\n            elif pattern == 'b':\n                plus_sign(self, row, column)\n                diagonal(self, row, column)\n\n            return adj_tiles", "response": "This method returns a list of the tiles adjacent to a given tile."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode a file or stream to an object.", "response": "def bread(stream):\n    \"\"\" Decode a file or stream to an object.\n    \"\"\"\n    if hasattr(stream, \"read\"):\n        return bdecode(stream.read())\n    else:\n        handle = open(stream, \"rb\")\n        try:\n            return bdecode(handle.read())\n        finally:\n            handle.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bwrite(stream, obj):\n    handle = None\n    if not hasattr(stream, \"write\"):\n        stream = handle = open(stream, \"wb\")\n    try:\n        stream.write(bencode(obj))\n    finally:\n        if handle:\n            handle.close()", "response": "Encode a given object to a file or stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(self, check_trailer=False): # pylint: disable=I0011,R0912\n        try:\n            kind = self.data[self.offset]\n        except IndexError:\n            raise BencodeError(\"Unexpected end of data at offset %d/%d\" % (\n                self.offset, len(self.data),\n            ))\n\n        if kind.isdigit():\n            # String\n            try:\n                end = self.data.find(':', self.offset)\n                length = int(self.data[self.offset:end], 10)\n            except (ValueError, TypeError):\n                raise BencodeError(\"Bad string length at offset %d (%r...)\" % (\n                    self.offset, self.data[self.offset:self.offset+32]\n                ))\n\n            self.offset = end+length+1\n            obj = self.data[end+1:self.offset]\n\n            if self.char_encoding:\n                try:\n                    obj = obj.decode(self.char_encoding)\n                except (UnicodeError, AttributeError):\n                    # deliver non-decodable string (byte arrays) as-is\n                    pass\n        elif kind == 'i':\n            # Integer\n            try:\n                end = self.data.find('e', self.offset+1)\n                obj = int(self.data[self.offset+1:end], 10)\n            except (ValueError, TypeError):\n                raise BencodeError(\"Bad integer at offset %d (%r...)\" % (\n                    self.offset, self.data[self.offset:self.offset+32]\n                ))\n            self.offset = end+1\n        elif kind == 'l':\n            # List\n            self.offset += 1\n            obj = []\n            while self.data[self.offset:self.offset+1] != 'e':\n                obj.append(self.decode())\n            self.offset += 1\n        elif kind == 'd':\n            # Dict\n            self.offset += 1\n            obj = {}\n            while self.data[self.offset:self.offset+1] != 'e':\n                key = self.decode()\n                obj[key] = self.decode()\n            self.offset += 1\n        else:\n            raise BencodeError(\"Format error at offset %d (%r...)\" % (\n                self.offset, self.data[self.offset:self.offset+32]\n            ))\n\n        if check_trailer and self.offset != len(self.data):\n            raise BencodeError(\"Trailing data at offset %d (%r...)\" % (\n                self.offset, self.data[self.offset:self.offset+32]\n            ))\n\n        return obj", "response": "Decode data in C { self. data and return deserialized object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding the given object into a list of bytes.", "response": "def encode(self, obj):\n        \"\"\" Add the given object to the result.\n        \"\"\"\n        if isinstance(obj, int_like_types):\n            self.result.append(\"i%de\" % obj)\n        elif isinstance(obj, string_types):\n            self.result.extend([str(len(obj)), ':', str(obj)])\n        elif hasattr(obj, \"__bencode__\"):\n            self.encode(obj.__bencode__())\n        elif hasattr(obj, \"items\"):\n            # Dictionary\n            self.result.append('d')\n            for key, val in sorted(obj.items()):\n                key = str(key)\n                self.result.extend([str(len(key)), ':', key])\n                self.encode(val)\n            self.result.append('e')\n        else:\n            # Treat as iterable\n            try:\n                items = iter(obj)\n            except TypeError as exc:\n                raise BencodeError(\"Unsupported non-iterable object %r of type %s (%s)\" % (\n                    obj, type(obj), exc\n                ))\n            else:\n                self.result.append('l')\n                for item in items:\n                    self.encode(item)\n                self.result.append('e')\n\n        return self.result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_delta_c(c200):\n    top = (200. / 3.) * c200**3.\n    bottom = np.log(1. + c200) - (c200 / (1. + c200))\n    return (top / bottom)", "response": "Calculate the delta of the cluster characteristic overdensity from concentration."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(self, notebook=notebook_display):\n        print(\"\\nCluster Ensemble:\")\n        if notebook is True:\n            display(self._df)\n        elif notebook is False:\n            print(self._df)\n        self.massrich_parameters()", "response": "Display cluster properties and scaling relation parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_nfw(self, rbins, offsets=None, numTh=200, numRoff=200,\n                 numRinner=20, factorRouter=3):\n        \"\"\"Calculates Sigma and DeltaSigma profiles.\n\n        Generates the surface mass density (sigma_nfw attribute of parent\n        object) and differential surface mass density (deltasigma_nfw\n        attribute of parent object) profiles of each cluster, assuming a\n        spherical NFW model. Optionally includes the effect of cluster\n        miscentering offsets.\n\n        Parameters\n        ----------\n        rbins : array_like\n            Radial bins (in Mpc) for calculating cluster profiles. Should\n            be 1D, optionally with astropy.units of Mpc.\n        offsets : array_like, optional\n            Parameter describing the width (in Mpc) of the Gaussian\n            distribution of miscentering offsets. Should be 1D, optionally\n            with astropy.units of Mpc.\n\n        Other Parameters\n        -------------------\n        numTh : int, optional\n            Parameter to pass to SurfaceMassDensity(). Number of bins to\n            use for integration over theta, for calculating offset profiles\n            (no effect for offsets=None). Default 200.\n        numRoff : int, optional\n            Parameter to pass to SurfaceMassDensity(). Number of bins to\n            use for integration over R_off, for calculating offset profiles\n            (no effect for offsets=None). Default 200.\n        numRinner : int, optional\n            Parameter to pass to SurfaceMassDensity(). Number of bins at\n            r < min(rbins) to use for integration over Sigma(<r), for\n            calculating DeltaSigma (no effect for Sigma ever, and no effect\n            for DeltaSigma if offsets=None). Default 20.\n        factorRouter : int, optional\n            Parameter to pass to SurfaceMassDensity(). Factor increase over\n            number of rbins, at min(r) < r < max(r), of bins that will be\n            used at for integration over Sigma(<r), for calculating\n            DeltaSigma (no effect for Sigma, and no effect for DeltaSigma\n            if offsets=None). Default 3.\n        \"\"\"\n        if offsets is None:\n            self._sigoffset = np.zeros(self.number) * units.Mpc\n        else:\n            self._sigoffset = utils.check_units_and_type(offsets, units.Mpc,\n                                                         num=self.number)\n\n        self.rbins = utils.check_units_and_type(rbins, units.Mpc)\n\n        rhoc = self._rho_crit.to(units.Msun / units.pc**2 / units.Mpc)\n        smd = SurfaceMassDensity(self.rs, self.delta_c, rhoc,\n                                 offsets=self._sigoffset,\n                                 rbins=self.rbins,\n                                 numTh=numTh,\n                                 numRoff=numRoff,\n                                 numRinner=numRinner,\n                                 factorRouter=factorRouter)\n\n        self.sigma_nfw = smd.sigma_nfw()\n        self.deltasigma_nfw = smd.deltasigma_nfw()", "response": "Calculates the sigma and deltaSigma of each cluster in the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset frequency of the logarithmic class", "response": "def F(self,value):\n        \"\"\" set phenotype \"\"\"\n        assert value.shape[0]==self._N, 'Dimension mismatch'\n        self._K = value.shape[1]\n        self._F = value\n        self.clear_cache('predict','Yres')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw(molecule, TraversalType=SmilesTraversal):\n    result = []\n    atoms = allAtoms = molecule.atoms\n\n    visitedAtoms = {}\n    #\n    # Traverse all components of the graph to form\n    # the output string\n    while atoms:\n        atom = _get_lowest_symorder(atoms)\n        visitedAtoms[atom] = 1\n\n        visitedBonds = {}\n        nextTraverse = TraversalType()\n        atomsUsed, bondsUsed = [], []\n        _traverse(atom, nextTraverse, None,\n                  visitedAtoms, visitedBonds,\n                  atomsUsed, bondsUsed, TraversalType)\n        atoms = []\n        for atom in allAtoms:\n            if not visitedAtoms.has_key(atom):\n                atoms.append(atom)\n        assert nextTraverse.atoms == atomsUsed\n        assert nextTraverse.bonds == bondsUsed, \"%s %s\"%(\n            nextTraverse.bonds, bondsUsed)\n        \n\n        result.append((str(nextTraverse),\n                       atomsUsed, bondsUsed))\n\n    result.sort()\n    fragments = []\n    for r in result:\n        fragments.append(r[0])\n\n    return \".\".join(fragments), result", "response": "Returns a string representation of a molecule in the canonical form."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Delaunay triangulation graph.", "response": "def delaunay_graph(X, weighted=False):\r\n  '''Delaunay triangulation graph.\r\n  '''\r\n  e1, e2 = _delaunay_edges(X)\r\n  pairs = np.column_stack((e1, e2))\r\n  w = paired_distances(X[e1], X[e2]) if weighted else None\r\n  return Graph.from_edge_pairs(pairs, num_vertices=X.shape[0], symmetric=True,\r\n                               weights=w)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solve(self,b,overwrite_b=False,check_finite=True):\n        if self._s is not None:\n            res = self._U.T.dot(b)\n            res /= self._s[:,np.newaxis]\n            res = self._U.dot(res)\n        elif self._chol is not None:\n            res = la.cho_solve((self._chol,self._lower),b=b,overwrite_b=overwrite_b,check_finite=check_finite)\n        else:\n            res = np.zeros(b.shape)\n        return res", "response": "solve A \\ b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving A \\ b", "response": "def solve(self,b,overwrite_b=False,check_finite=True, p=None):\n        \"\"\"\n        solve A \\ b\n        \"\"\"\n        if p is None:\n            assert b.shape[:2]==(len(self.solver),self.dof_any)\n            solution = np.empty(b.shape)\n            #This is trivially parallelizable:\n            for p in range(self.P):\n                solution[p] = self.solver[p].solve(b=b[p])\n            return solution\n        else:\n            return self.solver[p].solve(b=b)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solve(self, b_any, b, check_finite=True, p=None):\n        #assert b.shape[:2]==(len(self.solver),self.dof_any)\n        \n\n        if self.schur_solver is None and self.A_any_solver is None:\n            assert ( (b is None) or (b.shape[0]==0) ) and ( (b_any is None) or (b_any.shape[0]==0) ), \"shape missmatch\"\n            return b, b_any\n        elif self.schur_solver is None:\n            assert (b is None) or (b.shape[0]==0), \"shape missmatch\"\n            solution_any = self.A_any_solver.solve(b=b_any,p=p)\n            return b,solution_any\n        elif self.A_any_solver is None:\n            assert (b_any is None) or (b_any.shape[0]==0), \"shape missmatch\"\n            solution = self.schur_solver.solve(b=b, check_finite=check_finite)\n            return solution, b_any\n        else:\n            assert p is None, \"p is not None\"\n            cross_term = np.tensordot(self.DinvC,b_any,axes=([0,1],[0,1]))\n            solution = self.schur_solver.solve(b=(b - cross_term), check_finite=check_finite)\n            solution_any = self.A_any_solver.solve(b=b_any, check_finite=check_finite, p=p)\n            solution_any -= self.DinvC.dot(solution)\n            return solution, solution_any", "response": "solve A \\ b_any \\ b_any \\ b_any \\ b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo any special processing of a template, including recognizing the templating language and resolving file: references, then return an appropriate wrapper object. Currently Tempita and Python string interpolation are supported. `lookup` is an optional callable that resolves any ambiguous template path.", "response": "def preparse(template_text, lookup=None):\n    \"\"\" Do any special processing of a template, including recognizing the templating language\n        and resolving file: references, then return an appropriate wrapper object.\n\n        Currently Tempita and Python string interpolation are supported.\n        `lookup` is an optional callable that resolves any ambiguous template path.\n    \"\"\"\n    # First, try to resolve file: references to their contents\n    template_path = None\n    try:\n        is_file = template_text.startswith(\"file:\")\n    except (AttributeError, TypeError):\n        pass # not a string\n    else:\n        if is_file:\n            template_path = template_text[5:]\n            if template_path.startswith('/'):\n                template_path = '/' + template_path.lstrip('/')\n            elif template_path.startswith('~'):\n                template_path = os.path.expanduser(template_path)\n            elif lookup:\n                template_path = lookup(template_path)\n\n            with closing(open(template_path, \"r\")) as handle:\n                template_text = handle.read().rstrip()\n\n    if hasattr(template_text, \"__engine__\"):\n        # Already preparsed\n        template = template_text\n    else:\n        if template_text.startswith(\"{{\"):\n            import tempita  # only on demand\n\n            template = tempita.Template(template_text, name=template_path)\n            template.__engine__ = \"tempita\"\n        else:\n            template = InterpolationTemplate(template_text)\n\n        template.__file__ = template_path\n\n    template.__text__ = template_text\n    return template"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads out an hdf5 file.", "response": "def hdf5_read_out(self):\n        \"\"\"Read out an hdf5 file.\n\n        Takes the output of :class:`gwsnrcalc.genconutils.genprocess.GenProcess`\n        and reads it out to an HDF5 file.\n\n        \"\"\"\n        with h5py.File(self.WORKING_DIRECTORY + '/' + self.output_file_name, 'w') as f:\n\n            header = f.create_group('header')\n            header.attrs['Title'] = 'Generated SNR Out'\n            header.attrs['Author'] = 'Generator by: Michael Katz'\n            header.attrs['Date/Time'] = str(datetime.datetime.now())\n\n            for which in ['x', 'y']:\n                header.attrs[which + 'val_name'] = getattr(self, which + 'val_name')\n                header.attrs['num_' + which + '_pts'] = getattr(self, 'num_' + which)\n\n            ecc = 'eccentricity' in self.__dict__\n            if ecc:\n                name_list = ['observation_time', 'start_frequency', 'start_separation'\n                             'eccentricity']\n            else:\n                name_list = ['spin_1', 'spin_2', 'spin', 'end_time']\n\n            name_list += ['total_mass', 'mass_ratio', 'start_time', 'luminosity_distance',\n                          'comoving_distance', 'redshift']\n\n            for name in name_list:\n                if name != self.xval_name and name != self.yval_name:\n                    try:\n                        getattr(self, name)\n                        header.attrs[name] = getattr(self, name)\n                    except AttributeError:\n                        pass\n\n            if self.added_note != '':\n                header.attrs['Added note'] = self.added_note\n\n            data = f.create_group('data')\n\n            # read out x,y values in compressed data set\n            dset = data.create_dataset(self.x_col_name, data=self.xvals,\n                                       dtype='float64', chunks=True,\n                                       compression='gzip', compression_opts=9)\n\n            dset = data.create_dataset(self.y_col_name, data=self.yvals,\n                                       dtype='float64', chunks=True,\n                                       compression='gzip', compression_opts=9)\n\n            # read out all datasets\n            for key in self.output_dict.keys():\n                dset = data.create_dataset(key, data=self.output_dict[key],\n                                           dtype='float64', chunks=True,\n                                           compression='gzip', compression_opts=9)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef txt_read_out(self):\n\n        header = '#Generated SNR Out\\n'\n        header += '#Generator by: Michael Katz\\n'\n        header += '#Date/Time: {}\\n'.format(datetime.datetime.now())\n\n        for which in ['x', 'y']:\n            header += '#' + which + 'val_name: {}\\n'.format(getattr(self, which + 'val_name'))\n            header += '#num_' + which + '_pts: {}\\n'.format(getattr(self, 'num_' + which))\n\n        ecc = 'eccentricity' in self.__dict__\n        if ecc:\n            name_list = ['observation_time', 'start_frequency', 'start_separation'\n                         'eccentricity']\n        else:\n            name_list = ['spin_1', 'spin_2', 'spin', 'end_time']\n\n        name_list += ['total_mass', 'mass_ratio', 'start_time', 'luminosity_distance',\n                      'comoving_distance', 'redshift']\n\n        for name in name_list:\n            if name != self.xval_name and name != self.yval_name:\n                try:\n                    getattr(self, name)\n                    header += '#{}: {}\\n'.format(name, getattr(self, name))\n                except AttributeError:\n                    pass\n\n        if self.added_note != '':\n            header += '#Added note: ' + self.added_note + '\\n'\n        else:\n            header += '#Added note: None\\n'\n\n        header += '#--------------------\\n'\n\n        header += self.x_col_name + '\\t'\n\n        header += self.y_col_name + '\\t'\n\n        for key in self.output_dict.keys():\n            header += key + '\\t'\n\n        # read out x,y and the data\n        x_and_y = np.asarray([self.xvals, self.yvals])\n        snr_out = np.asarray([self.output_dict[key] for key in self.output_dict.keys()]).T\n\n        data_out = np.concatenate([x_and_y.T, snr_out], axis=1)\n\n        np.savetxt(self.WORKING_DIRECTORY + '/' + self.output_file_name,\n                   data_out, delimiter='\\t', header=header, comments='')\n        return", "response": "Reads out the output of genprocess. GenProcess and writes it out to a txt file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trigger_function_installed(connection: connection):\n    installed = False\n\n    log('Checking if trigger function installed...', logger_name=_LOGGER_NAME)\n\n    try:\n        execute(connection, \"SELECT pg_get_functiondef('public.psycopg2_pgevents_create_event'::regproc);\")\n        installed = True\n    except ProgrammingError as e:\n        if e.args:\n            error_stdout = e.args[0].splitlines()\n            error = error_stdout.pop(0)\n            if error.endswith('does not exist'):\n                # Trigger function not installed\n                pass\n            else:\n                # Some other exception; re-raise\n                raise e\n        else:\n            # Some other exception; re-raise\n            raise e\n\n    log('...{}installed'.format('' if installed else 'NOT '), logger_name=_LOGGER_NAME)\n\n    return installed", "response": "Test whether or not the psycopg2 - pgevents trigger function is installed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests whether or not a psycopg2 - pgevents trigger is installed for a table.", "response": "def trigger_installed(connection: connection, table: str, schema: str='public'):\n    \"\"\"Test whether or not a psycopg2-pgevents trigger is installed for a table.\n\n    Parameters\n    ----------\n    connection: psycopg2.extensions.connection\n        Active connection to a PostGreSQL database.\n    table: str\n        Table whose trigger-existence will be checked.\n    schema: str\n        Schema to which the table belongs.\n\n    Returns\n    -------\n    bool\n        True if the trigger is installed, otherwise False.\n\n    \"\"\"\n    installed = False\n\n    log('Checking if {}.{} trigger installed...'.format(schema, table), logger_name=_LOGGER_NAME)\n\n    statement = SELECT_TRIGGER_STATEMENT.format(\n        table=table,\n        schema=schema\n    )\n\n    result = execute(connection, statement)\n    if result:\n        installed = True\n\n    log('...{}installed'.format('' if installed else 'NOT '), logger_name=_LOGGER_NAME)\n\n    return installed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_trigger_function(connection: connection, overwrite: bool=False) -> None:\n    prior_install = False\n\n    if not overwrite:\n        prior_install = trigger_function_installed(connection)\n\n    if not prior_install:\n        log('Installing trigger function...', logger_name=_LOGGER_NAME)\n\n        execute(connection, INSTALL_TRIGGER_FUNCTION_STATEMENT)\n    else:\n        log('Trigger function already installed; skipping...', logger_name=_LOGGER_NAME)", "response": "Installs the psycopg2 - pgevents trigger function against the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_trigger(connection: connection, table: str, schema: str='public', overwrite: bool=False) -> None:\n    prior_install = False\n\n    if not overwrite:\n        prior_install = trigger_installed(connection, table, schema)\n\n    if not prior_install:\n        log('Installing {}.{} trigger...'.format(schema, table), logger_name=_LOGGER_NAME)\n\n        statement = INSTALL_TRIGGER_STATEMENT.format(\n            schema=schema,\n            table=table\n        )\n        execute(connection, statement)\n    else:\n        log('{}.{} trigger already installed; skipping...'.format(schema, table), logger_name=_LOGGER_NAME)", "response": "Install a psycopg2 - pgevents trigger against a table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uninstall_trigger(connection: connection, table: str, schema: str='public') -> None:\n    log('Uninstalling {}.{} trigger...'.format(schema, table), logger_name=_LOGGER_NAME)\n\n    statement = UNINSTALL_TRIGGER_STATEMENT.format(\n        schema=schema,\n        table=table\n    )\n    execute(connection, statement)", "response": "Uninstall a psycopg2 - pgevents trigger from a table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef absolutify(url):\n    site_url = getattr(settings, 'SITE_URL', False)\n\n    # If we don't define it explicitly\n    if not site_url:\n        protocol = settings.PROTOCOL\n        hostname = settings.DOMAIN\n        port = settings.PORT\n        if (protocol, port) in (('https://', 443), ('http://', 80)):\n            site_url = ''.join(map(str, (protocol, hostname)))\n        else:\n            site_url = ''.join(map(str, (protocol, hostname, ':', port)))\n\n    return site_url + url", "response": "Takes a URL and prepends the SITE_URL"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a default configuration loader.", "response": "def create_config_loader(config=None, env_prefix='APP'):\n    \"\"\"Create a default configuration loader.\n\n    A configuration loader takes a Flask application and keyword arguments and\n    updates the Flask application's configuration as it sees fit.\n\n    This default configuration loader will load configuration in the following\n    order:\n\n        1. Load configuration from ``invenio_config.module`` entry points\n           group, following the alphabetical ascending order in case of\n           multiple entry points defined.\n           For example, the config of an app with entry point name ``10_app``\n           will be loaded after the config of an app with entry point name\n           ``00_app``.\n        2. Load configuration from ``config`` module if provided as argument.\n        3. Load configuration from the instance folder:\n           ``<app.instance_path>/<app.name>.cfg``.\n        4. Load configuration keyword arguments provided.\n        5. Load configuration from environment variables with the prefix\n           ``env_prefix``.\n\n    If no secret key has been set a warning will be issued.\n\n    :param config: Either an import string to a module with configuration or\n        alternatively the module itself.\n    :param env_prefix: Environment variable prefix to import configuration\n        from.\n    :return: A callable with the method signature\n        ``config_loader(app, **kwargs)``.\n\n    .. versionadded:: 1.0.0\n    \"\"\"\n    def _config_loader(app, **kwargs_config):\n        InvenioConfigEntryPointModule(app=app)\n        if config:\n            InvenioConfigModule(app=app, module=config)\n        InvenioConfigInstanceFolder(app=app)\n        app.config.update(**kwargs_config)\n        InvenioConfigEnvironment(app=app, prefix='{0}_'.format(env_prefix))\n        InvenioConfigDefault(app=app)\n\n    return _config_loader"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_conf_loader(*args, **kwargs):  # pragma: no cover\n    import warnings\n    warnings.warn(\n        '\"create_conf_loader\" has been renamed to \"create_config_loader\".',\n        DeprecationWarning\n    )\n    return create_config_loader(*args, **kwargs)", "response": "Create a default configuration loader."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the HTTP headers for the request.", "response": "def _build_headers(self, method, auth_session):\n        \"\"\"Create headers for the request.\n        Parameters\n            method (str)\n                HTTP method (e.g. 'POST').\n            auth_session (Session)\n                The Session object containing OAuth 2.0 credentials.\n        Returns\n            headers (dict)\n                Dictionary of access headers to attach to request.\n        Raises\n            LyftIllegalState (ApiError)\n                Raised if headers are invalid.\n        \"\"\"\n        token_type = auth_session.token_type\n\n        token = auth_session.oauth2credential.access_token\n\n        if not self._authorization_headers_valid(token_type, token):\n            message = 'Invalid token_type or token.'\n            raise LyftIllegalState(message)\n\n        headers = {\n            'Authorization': ' '.join([token_type, token]),\n        }\n\n        if method in http.BODY_METHODS:\n            headers.update(http.DEFAULT_CONTENT_HEADERS)\n\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify authorization headers for a request.", "response": "def _authorization_headers_valid(self, token_type, token):\n        \"\"\"Verify authorization headers for a request.\n        Parameters\n            token_type (str)\n                Type of token to access resources.\n            token (str)\n                Server Token or OAuth 2.0 Access Token.\n        Returns\n            (bool)\n                True iff token_type and token are valid.\n        \"\"\"\n        if token_type not in http.VALID_TOKEN_TYPES:\n            return False\n\n        allowed_chars = ascii_letters + digits + '_' + '-' + '=' + '/' + '+'\n\n        # True if token only contains allowed_chars\n        return all(characters in allowed_chars for characters in token)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_units_and_type(input, expected_units, num=None, is_scalar=False):\n    if hasattr(input, 'unit'):    # check units\n        if expected_units is None:\n            raise ValueError('Expecting dimensionless input')\n        elif input.unit != expected_units:\n            raise ValueError('Expecting input units of ' + str(expected_units))\n        else:\n            dimensionless = input.value\n\n    else:\n        dimensionless = input\n\n    # check its a 1D array and/or convert list to array\n    if is_scalar is False:\n        dimensionfull = check_array_or_list(dimensionless)\n    else:\n        dimensionfull = dimensionless\n\n    # include units if appropriate\n    if expected_units is not None:\n        dimensionfull = dimensionfull * expected_units\n\n    # check array length if appropriate\n    if num is not None:\n        check_input_size(dimensionfull, num)\n\n    return dimensionfull", "response": "Checks whether the variable has expected units and type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning 1D ndarray if input can be converted and elements are non - negative.", "response": "def check_array_or_list(input):\n    \"\"\"Return 1D ndarray, if input can be converted and elements are\n    non-negative.\"\"\"\n    if type(input) != np.ndarray:\n        if type(input) == list:\n            output = np.array(input)\n        else:\n            raise TypeError('Expecting input type as ndarray or list.')\n    else:\n        output = input\n\n    if output.ndim != 1:\n        raise ValueError('Input array must have 1 dimension.')\n\n    if np.sum(output < 0.) > 0:\n            raise ValueError(\"Input array values cannot be negative.\")\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef object_attributes( thing, all=False ) :\n    '''\n    Return a sorted list of names defined by thing that are not also names in\n    a standard object, except include __doc__.\n    '''\n    standard_names = set( dir( object() ) )\n    things_names = set( dir( thing ) )\n    if not all :\n        things_names -= standard_names\n        things_names |= set( ['__doc__'] )\n    return sorted( things_names )", "response": "Return a sorted list of names defined by thing that are not also names in\n    a standard object except include __doc__."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints the attribute names in thing vertically", "response": "def print_object_attributes( thing, heading=None, file=None ):\n    '''\n    Print the attribute names in thing vertically\n    '''\n    if heading : print( '==', heading, '==', file=file )\n    print( '\\n'.join( object_attributes( thing ) ), file=file )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a thing that might be a property class method or function reduce it to code object.", "response": "def _get_a_code_object_from( thing ) :\n    '''\n    Given a thing that might be a property, a class method,\n    a function or a code object, reduce it to code object.\n    If we cannot, return the thing itself.\n    '''\n    # If we were passed a Method wrapper, get its function\n    if isinstance( thing, types.MethodType ) :\n        thing = thing.__func__\n    # If we were passed a property object, get its getter function\n    # (no direct support for the fdel or fset functions)\n    if hasattr( thing, 'fget' ) :\n        thing = thing.fget\n    # If we were passed, or now have, a function, get its code object.\n    if isinstance( thing, types.FunctionType ) :\n        thing = thing.__code__\n    # We should now have a code object, or will never have it.\n    return thing"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the codelist string list to the given file.", "response": "def printcodelist(thing, to=sys.stdout, heading=None):\n    '''\n    Write the lines of the codelist string list to the given file, or to\n    the default output.\n\n    A little Python 3 problem: if the to-file is in binary mode, we need to\n    encode the strings, else a TypeError will be raised. Obvious answer, test\n    for 'b' in to.mode? Nope, only \"real\" file objects have a mode attribute.\n    StringIO objects, and the variant StringIO used as default sys.stdout, do\n    not have .mode.\n\n    However, all file-like objects that support string output DO have an\n    encoding attribute. (StringIO has one that is an empty string, but it\n    exists.) So, if hasattr(to,'encoding'), just shove the whole string into\n    it. Otherwise, encode the string utf-8 and shove that bytestring into it.\n    (See? Python 3 not so hard...)\n\n    '''\n    # If we were passed a list, assume that it is a CodeList or\n    # a manually-assembled list of code tuples.\n    if not isinstance( thing, list ) :\n        # Passed something else. Reduce it to a CodeList.\n        if isinstance( thing, Code ):\n            thing = thing.code\n        else :\n            # Convert various sources to a code object.\n            thing = _get_a_code_object_from( thing )\n            try :\n                thing = Code.from_code( thing ).code\n            except Exception as e:\n                raise ValueError('Invalid input to printcodelist')\n    # We have a CodeList or equivalent,\n    # get the whole disassembly as a string.\n    whole_thang = str( thing )\n    # if destination not a text file, encode it to bytes\n    if not hasattr( to, 'encoding' ) :\n        whole_thang = whole_thang.encode( 'UTF-8' )\n        if heading : # is not None or empty\n            heading = heading.encode( 'UTF-8' )\n    # send it on its way\n    if heading :\n        to.write( '===' + heading + '===\\n' )\n    to.write( whole_thang )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the offsets in a byte code which are the start of source lines of the code.", "response": "def _findlinestarts(code_object):\n        \"\"\"\n        Find the offsets in a byte code which are the start of source lines.\n\n        Generate pairs (offset, lineno) as described in Python/compile.c.\n\n        This is a modified version of dis.findlinestarts. This version allows\n        multiple \"line starts\" with the same line number. (The dis version\n        conditions its yield on a test \"if lineno != lastlineno\".)\n\n        FYI: code.co_lnotab is a byte array with one pair of bytes for each\n        effective source line number in the bytecode. An effective line is\n        one that generates code: not blank or comment lines. The first actual\n        line number, typically the number of the \"def\" statement, is in\n        code.co_firstlineno.\n\n        An even byte of co_lnotab is the offset to the bytecode generated\n        from the next effective line number. The following odd byte is an\n        increment on the previous line's number to the next line's number.\n        Thus co_firstlineno+co_lnotab[1] is the first effective line's\n        number, and co_lnotab[0] is the number of bytes it generated.\n\n        Note that an effective line number generates code by definition,\n        hence the even byte cannot be zero; and as line numbers are\n        monotonically increasing, the odd byte cannot be zero either.\n\n        But what, the curious reader might ask, does Python do if a source\n        line generates more than 255 bytes of code? In that *highly* unlikely\n        case compile.c generates multiple pairs of (255,0) until it has\n        accounted for all the generated code, then a final pair of\n        (offset%256, lineincr).\n\n        Oh, but what, the curious reader asks, do they do if there is a gap\n        of more than 255 between effective line numbers? It is not unheard of\n        to find blocks of comments larger than 255 lines (like this one?).\n        Then compile.c generates pairs of (0, 255) until it has accounted for\n        the line number difference and a final pair of (offset,lineincr%256).\n\n        Uh, but...? Yes, what now, annoying reader? Well, does the following\n        code handle these special cases of (255,0) and (0,255) properly?\n        It handles the (0,255) case correctly, because of the \"if byte_incr\"\n        test which skips the yield() but increments lineno. It does not handle\n        the case of (255,0) correctly; it will yield false pairs (255,0).\n        Fortunately that will only arise e.g. when disassembling some\n        \"obfuscated\" code where most newlines are replaced with semicolons.\n\n        Oh, and yes, the to_code() method does properly handle generation\n        of the (255,0) and (0,255) entries correctly.\n\n        \"\"\"\n        # grab the even bytes as integer byte_increments:\n        byte_increments = [c for c in code_object.co_lnotab[0::2]]\n        # grab the odd bytes as integer line_increments:\n        line_increments = [c for c in code_object.co_lnotab[1::2]]\n\n        lineno = code_object.co_firstlineno\n        addr = 0\n        for byte_incr, line_incr in zip(byte_increments, line_increments):\n            if byte_incr:\n                yield (addr, lineno)\n                addr += byte_incr\n            lineno += line_incr\n        yield (addr, lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_code(cls, code_object):\n        # It's an annoyance to keep having to add \".__code__\" to a function\n        # name, so let's automate that when needed.\n        if isinstance( code_object, types.FunctionType ) :\n            code_object = code_object.__code__\n\n        # get the actual bytecode string out of the code object\n        co_code = code_object.co_code\n\n        # Use dis.findlabels to locate the labeled bytecodes, that is, the\n        # ones that are jump targets. (They are \"labeled\" in a disassembly\n        # printout.) Store the list as a dict{ addr: Label object} for easy\n        # lookup.\n\n        labels = dict((addr, Label()) for addr in findlabels(co_code))\n\n        # Make a dict{ source_line : offset } for the source lines in the code.\n\n        linestarts = dict(cls._findlinestarts(code_object))\n\n        cellfree = code_object.co_cellvars + code_object.co_freevars\n\n        # Create a CodeList object to represent the bytecode string.\n\n        code = CodeList()   # receives (op,arg) tuples\n        n = len(co_code)    # number bytes in the bytecode string\n        i = 0               # index over the bytecode string\n        extended_arg = 0    # upper 16 bits of an extended arg\n\n        # Iterate over the bytecode string expanding it into (Opcode,arg) tuples.\n\n        while i < n:\n            # First byte is the opcode\n            op = Opcode( co_code[i] )\n\n            # If this op is a jump-target, insert (Label,) ahead of it.\n            if i in labels:\n                code.append((labels[i], None))\n\n            # If this op is the first from a source line, insert\n            # (SetLineno, line#) ahead of it.\n            if i in linestarts:\n                code.append((SetLineno, linestarts[i]))\n\n            i += 1 # step index to the argument if any\n\n            if op not in hasargx :\n                # No argument, push the minimal tuple, done.\n                code.append((op, None))\n            else:\n                # op takes an argument. Look for MAKE_FUNCTION or MAKE_CLOSURE.\n                if op in hascode :\n                    # special case: with these opcodes, at runtime, TOS1 should\n                    # be a code object. We require the normal opcode sequence:\n                    #    LOAD_CONST the code object\n                    #    LOAD_CONST the name of the function\n                    #    MAKE_FUNCTION/CLOSURE\n                    # When this exists, go back and convert the argument of the\n                    # first LOAD_CONST from a code object to a Code object.\n                    if len(code) >= 2 \\\n                       and code[-2][0] == LOAD_CONST \\\n                       and code[-1][0] == LOAD_CONST \\\n                       and isinstance( code[-2][1], types.CodeType ) :\n                        code[-2] = ( Opcode(LOAD_CONST), Code.from_code( code[-2][1] ) )\n                    else :\n                        raise ValueError(\n                            'Invalid opcode sequence for MAKE_FUNCTION/MAKE_CLOSURE'\n                        )\n                    # now continue and handle the argument of MAKE_F/C normally.\n\n                # Assemble the argument value from two bytes plus an extended\n                # arg when present.\n                arg = co_code[i] + co_code[i+1]*256 + extended_arg\n                extended_arg = 0 # clear extended arg bits if any\n                i += 2 # Step over the argument\n\n                if op == opcode.EXTENDED_ARG:\n                    # The EXTENDED_ARG op is just a way of storing the upper\n                    # 16 bits of a 32-bit arg in the bytestream. Collect\n                    # those bits, but generate no code tuple.\n                    extended_arg = arg << 16\n\n                elif op in hasconst:\n                    # When the argument is a constant, put the constant\n                    # itself in the opcode tuple. If that constant is a code\n                    # object, the test above (if op in hascode) will later\n                    # convert it into a Code object.\n                    code.append((op, code_object.co_consts[arg]))\n\n                elif op in hasname:\n                    # When the argument is a name, put the name string itself\n                    # in the opcode tuple.\n                    code.append((op, code_object.co_names[arg]))\n\n                elif op in hasjabs:\n                    # When the argument is an absolute jump, put the label\n                    # in the tuple (in place of the label list index)\n                    code.append((op, labels[arg]))\n\n                elif op in hasjrel:\n                    # When the argument is a relative jump, put the label\n                    # in the tuple in place of the forward offset.\n                    code.append((op, labels[i + arg]))\n\n                elif op in haslocal:\n                    # When the argument is a local var, put the name string\n                    # in the tuple.\n                    code.append((op, code_object.co_varnames[arg]))\n\n                elif op in hascompare:\n                    # When the argument is a relation (like \">=\") put that\n                    # string in the tuple instead.\n                    code.append((op, cmp_op[arg]))\n\n                elif op in hasfree:\n                    code.append((op, cellfree[arg]))\n\n                else:\n                    # whatever, just put the arg in the tuple\n                    code.append((op, arg))\n\n        # Store certain flags from the code object as booleans for convenient\n        # reference as Code members.\n\n        varargs = bool(code_object.co_flags & CO_VARARGS)\n        varkwargs = bool(code_object.co_flags & CO_VARKEYWORDS)\n        newlocals = bool(code_object.co_flags & CO_NEWLOCALS)\n\n        # Get the names of arguments as strings, from the varnames tuple. The\n        # order of name strings in co_varnames is:\n        #   co_argcount names of regular (positional-or-keyword) arguments\n        #   names of co_kwonlyargcount keyword-only arguments if any\n        #   name of a *vararg argument\n        #   name of a **kwarg argument if any (not present if kwonlyargs > 0)\n        #   names of other local variables\n        # Hence the count of argument names is\n        #   co_argcount + co_kwonlyargcount + varargs + varkwargs\n        nargs = code_object.co_argcount + code_object.co_kwonlyargcount + varargs + varkwargs\n        args = code_object.co_varnames[ : nargs ]\n\n        # Preserve a docstring if any. If there are constants and the first\n        # constant is a string, Python assumes that's a docstring.\n        docstring = None\n        if code_object.co_consts and isinstance(code_object.co_consts[0], str):\n            docstring = code_object.co_consts[0]\n\n        # Funnel all the collected bits through the Code.__init__() method.\n        return cls( code = code,\n                    freevars = code_object.co_freevars,\n                    args = args,\n                    varargs = varargs,\n                    varkwargs = varkwargs,\n                    kwonlyargcount = code_object.co_kwonlyargcount,\n                    newlocals = newlocals,\n                    coflags = code_object.co_flags,\n                    name = code_object.co_name,\n                    filename = code_object.co_filename,\n                    firstlineno = code_object.co_firstlineno,\n                    docstring = docstring\n                    )", "response": "Disassemble a Python code object and make a Code object from the bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the maximum stack usage for the current object s code list.", "response": "def _compute_stacksize(self):\n        '''\n        Given this object's code list, compute its maximal stack usage.\n        This is done by scanning the code, and computing for each opcode\n        the stack state at the opcode.\n\n        '''\n\n        # get local access to code, save some attribute lookups later\n        code = self.code\n\n        # A mapping from labels to their positions in the code list\n        label_pos = { op : pos\n                        for pos, (op, arg) in enumerate(code)\n                        if isinstance(op, Label)\n                    }\n\n        # sf_targets are the targets of SETUP_FINALLY opcodes. They are\n        # recorded because they have special stack behaviour. If an exception\n        # was raised in the block pushed by a SETUP_FINALLY opcode, the block\n        # is popped and 3 objects are pushed. On return or continue, the\n        # block is popped and 2 objects are pushed. If nothing happened, the\n        # block is popped by a POP_BLOCK opcode and 1 object is pushed by a\n        # (LOAD_CONST, None) operation.\n        #\n        # In Python 3, the targets of SETUP_WITH have similar behavior,\n        # complicated by the fact that they also have an __exit__ method\n        # stacked and what it returns determines what they pop. So their\n        # stack depth is one greater, a fact we are going to ignore for the\n        # time being :-/\n        #\n        # Our solution is to record the stack state of SETUP_FINALLY targets\n        # as having 3 objects pushed, which is the maximum. However, to make\n        # stack recording consistent, the get_next_stacks function will always\n        # yield the stack state of the target as if 1 object was pushed, but\n        # this will be corrected in the actual stack recording.\n\n        sf_targets = set( label_pos[arg]\n                          for op, arg in code\n                          if op == SETUP_FINALLY or op == SETUP_WITH\n                        )\n\n        # What we compute - for each opcode, its stack state, as an n-tuple.\n        # n is the number of blocks pushed. For each block, we record the number\n        # of objects pushed.\n        stacks = [None] * len(code)\n\n        def get_next_stacks(pos, curstack):\n            \"\"\"\n            Get a code position and the stack state before the operation\n            was done, and yield pairs (pos, curstack) for the next positions\n            to be explored - those are the positions to which you can get\n            from the given (pos, curstack).\n\n            If the given position was already explored, nothing will be yielded.\n            \"\"\"\n            op, arg = code[pos]\n\n            if isinstance(op, Label):\n                # We should check if we already reached a node only if it is\n                # a label.\n\n                if pos in sf_targets:\n                    # Adjust a SETUP_FINALLY from 1 to 3 stack entries.\n                    curstack = curstack[:-1] + (curstack[-1] + 2,)\n\n                if stacks[pos] is None:\n                    stacks[pos] = curstack\n                else:\n                    if stacks[pos] != curstack:\n                        raise ValueError(\"Inconsistent code\")\n                    return\n\n            def newstack(n):\n                # Return a new stack, modified by adding n elements to the last\n                # block\n                if curstack[-1] + n < 0:\n                    raise ValueError(\"Popped a non-existing element\")\n                return curstack[:-1] + (curstack[-1]+n,)\n\n            if not isopcode(op):\n                # label or SetLineno - just continue to next line\n                yield pos+1, curstack\n\n            elif op in ( RETURN_VALUE, RAISE_VARARGS ):\n                # No place in particular to continue to\n                pass\n\n            elif op in (JUMP_FORWARD, JUMP_ABSOLUTE):\n                # One possibility for a jump\n                yield label_pos[arg], curstack\n\n            elif op in (POP_JUMP_IF_FALSE, POP_JUMP_IF_TRUE):\n                # Two possibilities for a jump\n                yield label_pos[arg], newstack(-1)\n                yield pos+1, newstack(-1)\n\n            elif op in (JUMP_IF_TRUE_OR_POP, JUMP_IF_FALSE_OR_POP):\n                # Two possibilities for a jump\n                yield label_pos[arg], curstack\n                yield pos+1, newstack(-1)\n\n            elif op == FOR_ITER:\n                # FOR_ITER pushes next(TOS) on success, and pops TOS and jumps\n                # on failure\n                yield label_pos[arg], newstack(-1)\n                yield pos+1, newstack(1)\n\n            elif op == BREAK_LOOP:\n                # BREAK_LOOP goes to the end of a loop and pops a block\n                # but like RETURN_VALUE we have no instruction position\n                # to give. For now treat like RETURN_VALUE\n                pass\n\n            elif op == CONTINUE_LOOP:\n                # CONTINUE_LOOP jumps to the beginning of a loop which should\n                # already have been discovered. It does not change the stack\n                # state nor does it create or pop a block.\n                #yield label_pos[arg], curstack\n                #yield label_pos[arg], curstack[:-1]\n                pass\n\n            elif op == SETUP_LOOP:\n                # We continue with a new block.\n                # On break, we jump to the label and return to current stack\n                # state.\n                yield label_pos[arg], curstack\n                yield pos+1, curstack + (0,)\n\n            elif op == SETUP_EXCEPT:\n                # We continue with a new block.\n                # On exception, we jump to the label with 3 extra objects on\n                # stack\n                yield label_pos[arg], newstack(3)\n                yield pos+1, curstack + (0,)\n\n            elif op == SETUP_FINALLY or op == SETUP_WITH :\n                # We continue with a new block.\n                # On exception, we jump to the label with 3 extra objects on\n                # stack, but to keep stack recording consistent, we behave as\n                # if we add only 1 object. Extra 2 will be added to the actual\n                # recording.\n                yield label_pos[arg], newstack(1)\n                yield pos+1, curstack + ( int(op == SETUP_WITH) ,)\n\n            elif op == POP_BLOCK:\n                # Just pop the block\n                yield pos+1, curstack[:-1]\n\n            elif op == END_FINALLY :\n                # Since stack recording of SETUP_FINALLY targets is of 3 pushed\n                # objects (as when an exception is raised), we pop 3 objects.\n                yield pos+1, newstack(-3)\n\n            elif op == _WITH_CLEANUP_OPCODE:\n                # Since WITH_CLEANUP[_START] is always found after SETUP_FINALLY\n                # targets, and the stack recording is that of a raised\n                # exception, we can simply pop 1 object and let END_FINALLY\n                # pop the remaining 3.\n                yield pos+1, newstack(-1)\n\n            else:\n                # nothing special, use the CPython value\n                yield pos+1, newstack( stack_effect( op, arg ) )\n\n\n        # Now comes the calculation: open_positions holds positions which are\n        # yet to be explored. In each step we take one open position, and\n        # explore it by appending the positions to which it can go, to\n        # open_positions. On the way, we update maxsize.\n        #\n        # open_positions is a list of tuples: (pos, stack state)\n        #\n        # Sneaky Python coding trick here. get_next_stacks() is a generator,\n        # it contains yield statements. So when we call get_next_stacks()\n        # what is returned is an iterator. However, the yield statements in\n        # get_next_stacks() are not in a loop as usual; rather it is\n        # straight-line code that will execute 0, 1 or 2 yields depending on\n        # the Opcode at pos.\n        #\n        # the list.extend() method takes an iterator and exhausts it, adding\n        # all yielded values to the list. Hence the statement\n        #\n        #   open_positions.extend(get_next_stacks(pos,curstack))\n        #\n        # appends 0, 1 or 2 tuples (pos, stack_state) to open_positions.\n\n        maxsize = 0\n        open_positions = [(0, (0,))]\n        while open_positions:\n            pos, curstack = open_positions.pop()\n            maxsize = max(maxsize, sum(curstack))\n            open_positions.extend(get_next_stacks(pos, curstack))\n\n        return maxsize"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isomap(self, num_dims=None, directed=None):\n    '''Isomap embedding.\n\n    num_dims : dimension of embedded coordinates, defaults to input dimension\n    directed : used for .shortest_path() calculation\n    '''\n    W = -0.5 * self.shortest_path(directed=directed) ** 2\n    kpca = KernelPCA(n_components=num_dims, kernel='precomputed')\n    return kpca.fit_transform(W)", "response": "Isomap embedding.\n\n    num_dims : dimension of embedded coordinates, defaults to input dimension\n    directed : used for .shortest_path() calculation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef neighborhood_preserving_embedding(self, X, num_dims=None, reweight=True):\n    '''Neighborhood Preserving Embedding (NPE, linearized LLE).'''\n    if reweight:\n      W = self.barycenter_edge_weights(X).matrix()\n    else:\n      W = self.matrix()\n    # compute M = (I-W)'(I-W) as in LLE\n    M = W.T.dot(W) - W.T - W\n    if issparse(M):\n      M = M.toarray()\n    M.flat[::M.shape[0] + 1] += 1\n    # solve generalized eig problem: X'MXa = \\lambda X'Xa\n    vals, vecs = eig(X.T.dot(M).dot(X), X.T.dot(X), overwrite_a=True,\n                     overwrite_b=True)\n    if num_dims is None:\n      return vecs\n    return vecs[:,:num_dims]", "response": "Neighborhood Preserving Embedding ( NPE linearized LLE."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef laplacian_pca(self, coordinates, num_dims=None, beta=0.5):\n    '''Graph-Laplacian PCA (CVPR 2013).\n    coordinates : (n,d) array-like, assumed to be mean-centered.\n    beta : float in [0,1], scales how much PCA/LapEig contributes.\n    Returns an approximation of input coordinates, ala PCA.'''\n    X = np.atleast_2d(coordinates)\n    L = self.laplacian(normed=True)\n    kernel = X.dot(X.T)\n    kernel /= eigsh(kernel, k=1, which='LM', return_eigenvectors=False)\n    L /= eigsh(L, k=1, which='LM', return_eigenvectors=False)\n    W = (1-beta)*(np.identity(kernel.shape[0]) - kernel) + beta*L\n    if num_dims is None:\n      vals, vecs = np.linalg.eigh(W)\n    else:\n      vals, vecs = eigh(W, eigvals=(0, num_dims-1), overwrite_a=True)\n    return X.T.dot(vecs).dot(vecs.T).T", "response": "Graph - Laplacian PCA."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef layout_circle(self):\n    '''Position vertices evenly around a circle.'''\n    n = self.num_vertices()\n    t = np.linspace(0, 2*np.pi, n+1)[:n]\n    return np.column_stack((np.cos(t), np.sin(t)))", "response": "Position vertices evenly around a circle."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npositions vertices using the Fruchterman-Reingold (spring) algorithm. num_dims : int (default=2) Number of dimensions to embed vertices in. spring_constant : float (default=None) Optimal distance between nodes. If None the distance is set to 1/sqrt(n) where n is the number of nodes. Increase this value to move nodes farther apart. iterations : int (default=50) Number of iterations of spring-force relaxation initial_temp : float (default=0.1) Largest step-size allowed in the dynamics, decays linearly. Must be positive, should probably be less than 1. initial_layout : array-like of shape (n, num_dims) If provided, serves as the initial placement of vertex coordinates.", "response": "def layout_spring(self, num_dims=2, spring_constant=None, iterations=50,\n                    initial_temp=0.1, initial_layout=None):\n    '''Position vertices using the Fruchterman-Reingold (spring) algorithm.\n\n    num_dims : int (default=2)\n       Number of dimensions to embed vertices in.\n\n    spring_constant : float (default=None)\n       Optimal distance between nodes.  If None the distance is set to\n       1/sqrt(n) where n is the number of nodes.  Increase this value\n       to move nodes farther apart.\n\n    iterations : int (default=50)\n       Number of iterations of spring-force relaxation\n\n    initial_temp : float (default=0.1)\n       Largest step-size allowed in the dynamics, decays linearly.\n       Must be positive, should probably be less than 1.\n\n    initial_layout : array-like of shape (n, num_dims)\n       If provided, serves as the initial placement of vertex coordinates.\n    '''\n    if initial_layout is None:\n      X = np.random.random((self.num_vertices(), num_dims))\n    else:\n      X = np.array(initial_layout, dtype=float, copy=True)\n      assert X.shape == (self.num_vertices(), num_dims)\n    if spring_constant is None:\n      # default to sqrt(area_of_viewport / num_vertices)\n      spring_constant = X.shape[0] ** -0.5\n    S = self.matrix('csr', 'csc', 'coo', copy=True)\n    S.data[:] = 1. / S.data  # Convert to similarity\n    ii,jj = S.nonzero()  # cache nonzero indices\n    # simple cooling scheme, linearly steps down\n    cooling_scheme = np.linspace(initial_temp, 0, iterations+2)[:-2]\n    # this is still O(V^2)\n    # could use multilevel methods to speed this up significantly\n    for t in cooling_scheme:\n      delta = X[:,None] - X[None]\n      distance = _bounded_norm(delta, 1e-8)\n      # repulsion from all vertices\n      force = spring_constant**2 / distance\n      # attraction from connected vertices\n      force[ii,jj] -= S.data * distance[ii,jj]**2 / spring_constant\n      displacement = np.einsum('ijk,ij->ik', delta, force)\n      # update positions\n      length = _bounded_norm(displacement, 1e-2)\n      X += displacement * t / length[:,None]\n    return X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nproduce an optimization version of handler for the dispatcher to limit reference lookups.", "response": "def optimize_structure_handler(rule, handler):\n    \"\"\"\n    Produce an \"optimized\" version of handler for the dispatcher to\n    limit reference lookups.\n    \"\"\"\n\n    def runner(walk, dispatcher, node):\n        handler(dispatcher, node)\n        return\n        yield  # pragma: no cover\n\n    return runner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef optimize_layout_handler(rule, handler):\n\n    def runner(walk, dispatcher, node):\n        yield LayoutChunk(rule, handler, node)\n\n    return runner", "response": "Produce an optimization handler for a layout rule."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(dispatcher, node, definition=None):\n\n    # The inner walk function - this is actually exposed to the token\n    # rule objects so they can also make use of it to process the node\n    # with the dispatcher.\n\n    nodes = []\n    sourcepath_stack = [NotImplemented]\n\n    def _walk(dispatcher, node, definition=None, token=None):\n        if not isinstance(node, Node):\n            for fragment in dispatcher.token(\n                    token, nodes[-1], node, sourcepath_stack):\n                yield fragment\n            return\n\n        push = bool(node.sourcepath)\n        if push:\n            sourcepath_stack.append(node.sourcepath)\n        nodes.append(node)\n\n        if definition is None:\n            definition = dispatcher.get_optimized_definition(node)\n\n        for rule in definition:\n            for chunk in rule(_walk, dispatcher, node):\n                yield chunk\n\n        nodes.pop(-1)\n        if push:\n            sourcepath_stack.pop(-1)\n\n    # Format layout markers are not handled immediately in the walk -\n    # they will simply be buffered so that a collection of them can be\n    # handled at once.\n    def process_layouts(layout_rule_chunks, last_chunk, chunk):\n        before_text = last_chunk.text if last_chunk else None\n        after_text = chunk.text if chunk else None\n        # the text that was yielded by the previous layout handler\n        prev_text = None\n\n        # While Layout rules in a typical definition are typically\n        # interspersed with Tokens, certain assumptions with how the\n        # Layouts are specified within there will fail when Tokens fail\n        # to generate anything for any reason.  However, the dispatcher\n        # instance will be able to accept and resolve a tuple of Layouts\n        # to some handler function, so that a form of normalization can\n        # be done.  For instance, an (Indent, Newline, Dedent) can\n        # simply be resolved to no operations.  To achieve this, iterate\n        # through the layout_rule_chunks and generate a normalized form\n        # for the final handling to happen.\n\n        # the preliminary stack that will be cleared whenever a\n        # normalized layout rule chunk is generated.\n        lrcs_stack = []\n\n        # first pass: generate both the normalized/finalized lrcs.\n        for lrc in layout_rule_chunks:\n            lrcs_stack.append(lrc)\n\n            # check every single chunk from left to right...\n            for idx in range(len(lrcs_stack)):\n                rule = tuple(lrc.rule for lrc in lrcs_stack[idx:])\n                handler = dispatcher.layout(rule)\n                if handler is not NotImplemented:\n                    # not manipulating lrsc_stack from within the same\n                    # for loop that it is being iterated upon\n                    break\n            else:\n                # which continues back to the top of the outer for loop\n                continue\n\n            # So a handler is found from inside the rules; extend the\n            # chunks from the stack that didn't get normalized, and\n            # generate a new layout rule chunk.\n            lrcs_stack[:] = lrcs_stack[:idx]\n            lrcs_stack.append(LayoutChunk(\n                rule, handler,\n                layout_rule_chunks[idx].node,\n            ))\n\n        # second pass: now the processing can be done.\n        for lr_chunk in lrcs_stack:\n            gen = lr_chunk.handler(\n                dispatcher, lr_chunk.node, before_text, after_text, prev_text)\n            if not gen:\n                continue\n            for chunk_from_layout in gen:\n                yield chunk_from_layout\n                prev_text = chunk_from_layout.text\n\n    # The top level walker implementation\n    def walk():\n        last_chunk = None\n        layout_rule_chunks = []\n\n        for chunk in _walk(dispatcher, node, definition):\n            if isinstance(chunk, LayoutChunk):\n                layout_rule_chunks.append(chunk)\n            else:\n                # process layout rule chunks that had been cached.\n                for chunk_from_layout in process_layouts(\n                        layout_rule_chunks, last_chunk, chunk):\n                    yield chunk_from_layout\n                layout_rule_chunks[:] = []\n                yield chunk\n                last_chunk = chunk\n\n        # process the remaining layout rule chunks.\n        for chunk_from_layout in process_layouts(\n                layout_rule_chunks, last_chunk, None):\n            yield chunk_from_layout\n\n    for chunk in walk():\n        yield chunk", "response": "A recursive function that returns the nodes in the tree that are found in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef t_QUOTED_STRING(t):\n    r'\"([^\"\\\\]|\\\\[\"\\\\])*\"'\n    # TODO: Add support for:\n    # - An undefined escape sequence (such as \"\\a\" in a context where \"a\"\n    # has no special meaning) is interpreted as if there were no backslash\n    # (in this case, \"\\a\" is just \"a\"), though that may be changed by\n    # extensions.\n    # - Non-printing characters such as tabs, CRLF, and control characters\n    # are permitted in quoted strings.  Quoted strings MAY span multiple\n    # lines.  An unencoded NUL (US-ASCII 0) is not allowed in strings.\n    t.value = t.value.strip('\"').replace(r'\\\"', '\"').replace(r'\\\\', '\\\\')\n    return t", "response": "Replace double quotes with single quotes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef t_NUMBER(t):\n    r'[0-9]+[KkMmGg]?'\n    exponents = {\n            'G' : 30, 'g' : 30,\n            'M' : 20, 'm' : 20,\n            'K' : 10, 'k' : 10,\n            }\n    if t.value[-1] in exponents:\n        t.value = math.ldexp(int(t.value[:-1]), exponents[t.value[-1]])\n    else:\n        t.value = int(t.value)\n    return t", "response": "t_NUMBER returns a new T object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nraise an exception describing how the given set of elements are not in the required set.", "response": "def _missingExtraCheck(given, required, extraException, missingException):\n    \"\"\"\n    If the L{sets<set>} C{required} and C{given} do not contain the same\n    elements raise an exception describing how they are different.\n\n    @param given: The L{set} of elements that was actually given.\n    @param required: The L{set} of elements that must be given.\n\n    @param extraException: An exception to raise if there are elements in\n        C{given} that are not in C{required}.\n    @param missingException: An exception to raise if there are elements in\n        C{required} that are not in C{given}.\n\n    @return: C{None}\n    \"\"\"\n    extra = given - required\n    if extra:\n        raise extraException(extra)\n\n    missing = required - given\n    if missing:\n        raise missingException(missing)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a finite state machine from a definition of its states.", "response": "def constructFiniteStateMachine(inputs, outputs, states, table, initial,\n                                richInputs, inputContext, world,\n                                logger=LOGGER):\n    \"\"\"\n    Construct a new finite state machine from a definition of its states.\n\n    @param inputs: Definitions of all input symbols the resulting machine will\n        need to handle, as a L{twisted.python.constants.Names} subclass.\n\n    @param outputs: Definitions of all output symbols the resulting machine is\n        allowed to emit, as a L{twisted.python.constants.Names} subclass.\n\n    @param states: Definitions of all possible states the resulting machine\n        will be capable of inhabiting, as a L{twisted.python.constants.Names}\n        subclass.\n\n    @param table: The state transition table, defining which output and next\n        state results from the receipt of any and all inputs in any and all\n        states.\n    @type table: L{TransitionTable}\n\n    @param initial: The state the machine will start in (one of the symbols\n        from C{states}).\n\n    @param richInputs: A L{list} of types which correspond to each of the input\n        symbols from C{inputs}.\n    @type richInputs: L{list} of L{IRichInput} I{providers}\n\n    @param inputContext: A L{dict} mapping output symbols to L{Interface}\n        subclasses describing the requirements of the inputs which lead to\n        them.\n\n    @param world: An object responsible for turning FSM outputs into observable\n        side-effects.\n    @type world: L{IOutputExecutor} provider\n\n    @param logger: The logger to which to write messages.\n    @type logger: L{eliot.ILogger} or L{NoneType} if there is no logger.\n\n    @return: An L{IFiniteStateMachine} provider\n    \"\"\"\n    table = table.table\n\n    _missingExtraCheck(\n        set(table.keys()), set(states.iterconstants()),\n        ExtraTransitionState, MissingTransitionState)\n\n    _missingExtraCheck(\n        set(i for s in table.values() for i in s), set(inputs.iterconstants()),\n        ExtraTransitionInput, MissingTransitionInput)\n\n    _missingExtraCheck(\n        set(output for s in table.values() for transition in s.values() for output in transition.output),\n        set(outputs.iterconstants()),\n        ExtraTransitionOutput, MissingTransitionOutput)\n\n    try:\n        _missingExtraCheck(\n            set(transition.nextState for s in table.values() for transition in s.values()),\n            set(states.iterconstants()),\n            ExtraTransitionNextState, MissingTransitionNextState)\n    except MissingTransitionNextState as e:\n        if e.args != ({initial},):\n            raise\n\n    if initial not in states.iterconstants():\n        raise InvalidInitialState(initial)\n\n    extraInputContext = set(inputContext) - set(outputs.iterconstants())\n    if extraInputContext:\n        raise ExtraInputContext(extraInputContext)\n\n    _checkConsistency(richInputs, table, inputContext)\n\n    fsm = _FiniteStateMachine(inputs, outputs, states, table, initial)\n    executor = IOutputExecutor(world)\n    interpreter = _FiniteStateInterpreter(\n        tuple(richInputs), inputContext, fsm, executor)\n    if logger is not None:\n        interpreter = FiniteStateLogger(\n            interpreter, logger, executor.identifier())\n    return interpreter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _checkConsistency(richInputs, fsm, inputContext):\n    for richInput in richInputs:\n        for state in fsm:\n            for input in fsm[state]:\n                if richInput.symbol() == input:\n                    # This rich input will be supplied to represent this input\n                    # symbol in this state.  Check to see if it satisfies the\n                    # output requirements.\n                    outputs = fsm[state][input].output\n                    for output in outputs:\n                        try:\n                            required = inputContext[output]\n                        except KeyError:\n                            continue\n                        # Consider supporting non-interface based checking in\n                        # the future: extend this to also allow\n                        # issubclass(richInput, required)\n                        if required.implementedBy(richInput):\n                            continue\n                        raise DoesNotImplement(\n                            \"%r not implemented by %r, \"\n                            \"required by %r in state %r\" % (\n                                required, richInput,\n                                input, state))", "response": "Checks that the outputs that can be generated by the given rich inputs are satisfied by the given rich inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trivialInput(symbol):\n    return implementer(IRichInput)(type(\n            symbol.name.title(), (FancyStrMixin, object), {\n                \"symbol\": _symbol(symbol),\n                }))", "response": "Create a new L { IRichInput } implementation for the given input symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _copy(self):\n        table = {}\n        for existingState, existingOutputs in self.table.items():\n            table[existingState] = {}\n            for (existingInput, existingTransition) in existingOutputs.items():\n                table[existingState][existingInput] = existingTransition\n        return TransitionTable(table)", "response": "Create a copy of the underlying transition table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new transition table with all the same transitions as this one.", "response": "def addTransition(self, state, input, output, nextState):\n        \"\"\"\n        Create a new L{TransitionTable} with all the same transitions as this\n        L{TransitionTable} plus a new transition.\n\n        @param state: The state for which the new transition is defined.\n        @param input: The input that triggers the new transition.\n        @param output: The output produced by the new transition.\n        @param nextState: The state that will follow the new transition.\n\n        @return: The newly created L{TransitionTable}.\n        \"\"\"\n        return self.addTransitions(state, {input: (output, nextState)})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new transition table with all the same transitions as this one plus a number of new transitions.", "response": "def addTransitions(self, state, transitions):\n        \"\"\"\n        Create a new L{TransitionTable} with all the same transitions as this\n        L{TransitionTable} plus a number of new transitions.\n\n        @param state: The state for which the new transitions are defined.\n        @param transitions: A L{dict} mapping inputs to output, nextState\n            pairs.  Each item from this L{dict} will define a new transition in\n            C{state}.\n\n        @return: The newly created L{TransitionTable}.\n        \"\"\"\n        table = self._copy()\n        state = table.table.setdefault(state, {})\n        for (input, (output, nextState)) in transitions.items():\n            state[input] = Transition(output, nextState)\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addTerminalState(self, state):\n        table = self._copy()\n        table.table[state] = {}\n        return table", "response": "Create a new L { TransitionTable } with all of the same transitions as\n        this L { TransitionTable } plus a new state with no transitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrules for minifying output. Arguments: drop_semi Drop semicolons whenever possible. Note that if Dedent and OptionalNewline has a handler defined, it will stop final break statements from being resolved due to reliance on normalized resolution.", "response": "def minify(drop_semi=True):\n    \"\"\"\n    Rules for minifying output.\n\n    Arguments:\n\n    drop_semi\n        Drop semicolons whenever possible.  Note that if Dedent and\n        OptionalNewline has a handler defined, it will stop final break\n        statements from being resolved due to reliance on normalized\n        resolution.\n\n    \"\"\"\n\n    layout_handlers = {\n        OpenBlock: layout_handler_openbrace,\n        CloseBlock: layout_handler_closebrace,\n        EndStatement: layout_handler_semicolon,\n        Space: layout_handler_space_minimum,\n        OptionalSpace: layout_handler_space_minimum,\n        RequiredSpace: layout_handler_space_imply,\n        (Space, OpenBlock): layout_handler_openbrace,\n        (Space, EndStatement): layout_handler_semicolon,\n        (OptionalSpace, EndStatement): layout_handler_semicolon,\n    }\n\n    if drop_semi:\n        # if these are defined, they should be dropped; should really\n        # provide these as a flag.\n        # layout_handlers.update({\n        #     OptionalNewline: None,\n        #     Dedent: None,\n        # })\n\n        layout_handlers.update({\n            EndStatement: layout_handler_semicolon_optional,\n\n            # these two rules rely on the normalized resolution\n            (OptionalSpace, EndStatement): layout_handler_semicolon_optional,\n            (EndStatement, CloseBlock): layout_handler_closebrace,\n\n            # this is a fallback rule for when Dedent is defined by\n            # some other rule, which won't neuter all optional\n            # semicolons.\n            (EndStatement, Dedent): rule_handler_noop,\n            ((OptionalSpace, EndStatement), CloseBlock):\n                layout_handler_closebrace,\n        })\n\n    def minify_rule():\n        return {\n            'layout_handlers': layout_handlers,\n            'deferrable_handlers': {\n                Literal: deferrable_handler_literal_continuation,\n            },\n        }\n\n    return minify_rule"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef indent(indent_str=None):\n\n    def indentation_rule():\n        inst = Indentator(indent_str)\n        return {'layout_handlers': {\n            OpenBlock: layout_handler_openbrace,\n            CloseBlock: layout_handler_closebrace,\n            EndStatement: layout_handler_semicolon,\n            Space: layout_handler_space_imply,\n            OptionalSpace: layout_handler_space_optional_pretty,\n            RequiredSpace: layout_handler_space_imply,\n            Indent: inst.layout_handler_indent,\n            Dedent: inst.layout_handler_dedent,\n            Newline: inst.layout_handler_newline,\n            OptionalNewline: inst.layout_handler_newline_optional,\n            (Space, OpenBlock): NotImplemented,\n            (Space, EndStatement): layout_handler_semicolon,\n            (OptionalSpace, EndStatement): layout_handler_semicolon,\n            (Indent, Newline, Dedent): rule_handler_noop,\n        }}\n    return indentation_rule", "response": "A standalone indent ruleset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a documentation bundle", "response": "def dist_docs():\n    \"create a documentation bundle\"\n    dist_dir = path(\"dist\")\n    docs_package = path(\"%s/%s-%s-docs.zip\" % (dist_dir.abspath(), options.setup.name, options.setup.version))\n\n    dist_dir.exists() or dist_dir.makedirs()\n    docs_package.exists() and docs_package.remove()\n\n    sh(r'cd build/apidocs && zip -qr9 %s .' % (docs_package,))\n\n    print('')\n    print(\"Upload @ http://pypi.python.org/pypi?:action=pkg_edit&name=%s\" % ( options.setup.name,))\n    print(docs_package)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef release():\n    \"check release before upload to PyPI\"\n    sh(\"paver bdist_wheel\")\n    wheels = path(\"dist\").files(\"*.whl\")\n    if not wheels:\n        error(\"\\n*** ERROR: No release wheel was built!\")\n        sys.exit(1)\n    if any(\".dev\" in i for i in wheels):\n        error(\"\\n*** ERROR: You're still using a 'dev' version!\")\n        sys.exit(1)\n\n    # Check that source distribution can be built and is complete\n    print('')\n    print(\"~\" * 78)\n    print(\"TESTING SOURCE BUILD\")\n    sh( \"{ command cd dist/ && unzip -q %s-%s.zip && command cd %s-%s/\"\n        \"  && /usr/bin/python setup.py sdist >/dev/null\"\n        \"  && if { unzip -ql ../%s-%s.zip; unzip -ql dist/%s-%s.zip; }\"\n        \"        | cut -b26- | sort | uniq -c| egrep -v '^ +2 +' ; then\"\n        \"       echo '^^^ Difference in file lists! ^^^'; false;\"\n        \"    else true; fi; } 2>&1\"\n        % tuple([project[\"name\"], version] * 4)\n    )\n    path(\"dist/%s-%s\" % (project[\"name\"], version)).rmtree()\n    print(\"~\" * 78)\n\n    print('')\n    print(\"Created\", \" \".join([str(i) for i in path(\"dist\").listdir()]))\n    print(\"Use 'paver sdist bdist_wheel' to build the release and\")\n    print(\"    'twine upload dist/*.{zip,whl}' to upload to PyPI\")\n    print(\"Use 'paver dist_docs' to prepare an API documentation upload\")", "response": "check release before upload to PyPI"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncycle - > return a dictionary of pyrole nitrogen - like atoms in a cycle or a molecule", "response": "def getPyroleLikeAtoms(cycle):\n    \"\"\"cycle->return a dictionary of pyrole nitrogen-like atoms in\n    a cycle or a molecule  The dictionary is keyed on the atom.handle\"\"\"    \n    result = {}\n    # the outgoing bonds might need to be single or aromatic\n    for atom in cycle.atoms:\n        lookup = (atom.symbol, atom.charge, atom.hcount, len(atom.bonds))\n        if PyroleTable.get(lookup, 0):\n            result[atom.handle] = atom\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning AROMATIC if the ring is conjugatable and the cycle is not already aromatic and if the ring is aromatic and the cycle is not already aromatic and the cycle is not already aromatic and the cycle is not aromatic and the cycle is not aromatic and the cycle is not aromatic and the cycle is not aromatic.", "response": "def canBeAromatic(cycle, pyroleLike):\n    \"\"\"(cycle)-> returns AROMATIC if a ring is conjugatable and\n                                  passes the simple tests for aromaticity\n                 returns MAYBE if the ring in its present form\n                                can be aromatic but is not currently\n                         NEVER if the ring can never be aromatic\"\"\"\n    cycleLength = len(cycle)\n    # *******************************************************\n    #  check for kekular five membered rings\n    if cycleLength == 5:\n        # check atom types\n        for atom in cycle.atoms:\n            if not AROMATIC_PYROLE_ATOMS.has_key(atom.symbol):\n                return NEVER\n\n        # do we have exactly one pyrole nitrogen like atom?\n        pyroleCount = 0\n        for atom in cycle.atoms:\n            if pyroleLike.has_key(atom.handle):\n                pyrole = atom\n                pyroleCount += 1\n\n        \n        if pyroleCount < 1 or pyroleCount > 2:\n            return NEVER\n\n        # rotate the ring so that we start on the pyrole like atom\n        cycle.rotate(pyrole)\n        bonds = cycle.bonds[:]\n        # check the bonds for a kekular structure\n        for index, bond in zip(range(len(bonds)), bonds):\n            if bond.bondtype not in AROMATIC_5_RING[index]:\n                return MAYBE\n            \n        return AROMATIC\n    # *****************************************************\n    #  check for kekular six membered rings\n    #  kekular rings must have atoms in the AROMATIC_ATOMS\n    #  groups and must belong in 6 membered rings.\n    #  bonds must be conjugated\n    elif cycleLength == 6:\n        # XXX FIX ME -> there is a lot of problems with this\n        # code I think, what about bonds that are already fixed?\n        for atom in cycle.atoms:\n            if not AROMATIC_ATOMS.has_key(atom.symbol):\n                return NEVER\n            \n        bonds = cycle.bonds[:]        \n                \n        last = None\n        switch = {1:2, 2:1}\n        while bonds:\n            bond = bonds.pop()\n            bondtype = bond.bondtype\n\n            if bond.bondorder == 3:\n                return NEVER\n            \n            if last is None:\n                if bond.bondtype in [1,2]:\n                    last = bond.bondtype\n            else:\n                if last == 1 and bond.bondtype not in [2,4]:\n                    return MAYBE\n                elif last == 2 and bond.bondtype not in [1, 4]:\n                    return MAYBE\n\n                last = switch[last]\n                if bondtype != last:\n                    bond.bondorder = last\n\n        return AROMATIC\n    \n    else:\n        # we can never be aromatic\n        return NEVER"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a cycle into aromatized cycle.", "response": "def convert(cycle, pyroleLike, usedPyroles):\n    \"\"\"cycle, pyroleLike, aromatic=0-> aromatize the cycle\n    pyroleLike is a lookup of the pyrole like atoms in the\n    cycle.\n    return 1 if the cycle was aromatized\n           2 if the cycle could not be aromatized\"\"\"\n\n    bonds = cycle.bonds\n    atoms = cycle.atoms\n    initialBondStates = []\n    initialAtomStates = []\n    _usedPyroles = {}\n    for bond in bonds:\n        # store the initial states but assume the\n        # bond is aromatic\n        initialBondStates.append((bond, bond.symbol,\n                                  bond.bondorder, bond.bondtype,\n                                  bond.aromatic, bond.stereo))\n        # XXX FIX ME\n        # until we get proper conjugation, aromatic bond orders\n        # are 1.5\n        bond.reset(':', bond.bondorder, 4, bond.fixed, bond.stereo)\n        \n    aromatized = 1\n    for atom in atoms:\n        initialAtomStates.append((atom, atom.aromatic))\n        atom.aromatic = 1\n\n        nonhydrogens = atom.sumBondOrders() + atom.charge\n\n        # look for the lowest valence where we don't\n        #  have to change the charge of the atom to\n        #  fill the valences\n        for valence in atom.valences:\n            neededHydrogens = int(valence - nonhydrogens)\n            if neededHydrogens >= 0:\n                break\n        else:\n            # we can't change the aromaticity and have correct\n            # valence.\n            #\n            # there is one special case of a five membered\n            #  ring and a pyrole nitrogen like atom we need\n            #  to look for.\n            if len(cycle) == 5 and pyroleLike.has_key(atom.handle):\n                _usedPyroles[atom.handle] = 1\n            else:\n                # nope, the valences don't work out so\n                # we can't aromatize\n                aromatized = 0\n                break\n\n    # sanity check, this should be true because of the\n    # canBeAromatic routine above\n    assert len(_usedPyroles) <=1, \"Too many used pyroles!\"\n    \n    cycle.aromatic = aromatized\n    if not aromatized:\n        for bond, symbol, order, bondtype, aromatic, stereo in initialBondStates:\n            bond.reset(symbol, order, bondtype, bond.fixed, stereo)\n\n        for atom, aromatic in initialAtomStates:\n            atom.aromatic = aromatic\n    else:\n        # we used some pyroles, we'll have to send these to\n        # the valence checker later\n        usedPyroles.update(_usedPyroles)\n\n    return aromatized"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds implicit hydrogens to a molecule", "response": "def addHydrogens(molecule, usedPyroles=None):\n    \"\"\"(molecule) -> add implicit hydrogens to a molecule.\n    If the atom has specified valences and the atom must be\n    charged then a Valence Error is raised\"\"\"\n    for atom in molecule.atoms:\n        # if the atom has an explicit hcount, we can't set the\n        # hcount\n        if atom.has_explicit_hcount:\n            atom.hcount = atom.explicit_hcount\n            continue\n        \n        if atom.valences:            \n            for valence in atom.valences:\n                hcount = max(0, int(valence - atom.sumBondOrders() + atom.charge))\n                if hcount >= 0:\n                    break\n            else:\n                if usedPyroles and not usedPyroles.has_key(atom.handle):\n                    #print atom.symbol, atom.valences, atom.hcount, atom.charge,\\\n                    #      atom.sumBondOrders()\n                    #print [x.bondtype for x in atom.bonds]\n                    #print molecule.cansmiles()\n                    raise PinkyError(\"Valence error in atom %s\"%molecule.atoms.index(atom))\n                pass\n\n            #hcount = int(hcount)\n            atom.hcount = hcount\n    return molecule"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aromatize(molecule, usedPyroles=None):    \n    \n    pyroleLike = getPyroleLikeAtoms(molecule)\n\n    if usedPyroles is None:\n        usedPyroles = {}\n        \n    cyclesToCheck = []\n    # determine which cycles came in marked as aromatic\n    # and which need to be checked form the kekular form\n    #\n    # if a cycle came in as aromatic, convert it\n    # before going on.\n    for cycle in molecule.cycles:\n        for atom in cycle.atoms:\n            if not atom.aromatic:\n                cyclesToCheck.append(cycle)\n                break\n        else:\n            if not convert(cycle, pyroleLike, usedPyroles):\n                # XXX FIX ME\n                # oops, an aromatic ring came in but\n                # we can't convert it.  This is an error\n                # daylight would conjugate the ring\n                raise PinkyError(\"Bad initial aromaticity\")\n\n    # keep checking rings until something happens\n    while 1:\n        # assume nothing happened\n        needToCheckAgain = 0\n\n        _cyclesToCheck = []\n        for cycle in cyclesToCheck:\n            canAromatic = canBeAromatic(cycle, pyroleLike)\n            if canAromatic == NEVER:\n                # the ring can NEVER EVER be aromatic, so remove it for good\n                pass\n            elif canAromatic and convert(cycle, pyroleLike, usedPyroles):\n                needToCheckAgain = 1\n            else:\n                _cyclesToCheck.append(cycle)\n\n        cyclesToCheck = _cyclesToCheck\n        if not needToCheckAgain:\n            break\n\n    # fix bonds that have no bondorder if necessary\n    molecule = fixBonds(molecule, pyroleLike)\n    # add implicit hydrogens\n    return addHydrogens(molecule, usedPyroles)", "response": "aromatize a molecular graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs interpolated sensitivity curves This will construct the interpolated sensitivity curves using scipy.interpolate.interp1d. It will add wd noise if that is requested. Raises: ValueError: ``len(noise_type_in) != len(sensitivity_curves)`` ValueError: Issue with sensitivity curve type provided.", "response": "def _prep_noise_interpolants(self):\n        \"\"\"Construct interpolated sensitivity curves\n\n        This will construct the interpolated sensitivity curves\n        using scipy.interpolate.interp1d. It will add wd noise\n        if that is requested.\n\n        Raises:\n            ValueError: ``len(noise_type_in) != len(sensitivity_curves)``\n            ValueError: Issue with sensitivity curve type provided.\n\n        \"\"\"\n        noise_lists = {}\n        self.noise_interpolants = {}\n\n        if isinstance(self.sensitivity_curves, str):\n            self.sensitivity_curves = [self.sensitivity_curves]\n\n        if isinstance(self.noise_type_in, list):\n            if len(self.noise_type_in) != len(self.sensitivity_curves):\n                raise ValueError('noise_type_in must have same shape as sensitivity_curves if it is'\n                                 + 'provided as a list.'\n                                 + 'If all curves are of the same type, provide a string.')\n\n        else:\n            assert isinstance(self.noise_type_in, str)\n            self.noise_type_in = [self.noise_type_in for _ in self.sensitivity_curves]\n\n        if isinstance(self.signal_type, str):\n            self.signal_type = [self.signal_type]\n\n        # read in all the noise curves\n        for num, sc in enumerate(self.sensitivity_curves):\n            if isinstance(sc, str):\n                f, h_n = read_noise_curve(sc, noise_type_in=self.noise_type_in[num],\n                                          noise_type_out='char_strain')\n                if sc[-4:] == '.txt':\n                    key = sc.split('.')[0].split('/')[-1]\n                else:\n                    key = sc\n            elif isinstance(sc, list):\n                # TODO: add to docs if inputing special noise curve, make sure its char_strain\n                f, h_n = sc\n                key = str(num)\n            else:\n                raise ValueError('Sensitivity curves must either be string'\n                                 + 'or list containing f_n and asd_n.')\n\n            noise_lists[key] = [f, h_n]\n\n        # add wd noise\n        if str(self.add_wd_noise).lower() in ['true', 'both', 'yes']:\n            if isinstance(self.wd_noise, str):\n                f_n_wd, h_n_wd = read_noise_curve(self.wd_noise,\n                                                  noise_type_in=self.wd_noise_type_in,\n                                                  noise_type_out='char_strain')\n            elif isinstance(self, wd_noise, list):\n                f_n_wd, h_n_wd = self.wd_noise\n\n            trans_dict = {}\n            for sc in noise_lists.keys():\n                f_n, h_n = noise_lists[sc]\n                if self.add_wd_noise.lower() == 'both':\n                    trans_dict[sc] = [f_n, h_n]\n\n                f_n, h_n = combine_with_wd_noise(f_n, h_n, f_n_wd, h_n_wd)\n                trans_dict[sc + '_wd'] = [f_n, h_n]\n\n            noise_lists = trans_dict\n\n        # interpolate\n        for sc in noise_lists:\n            f_n, h_n = noise_lists[sc]\n            self.noise_interpolants[sc] = (interpolate.interp1d(f_n, h_n,\n                                           bounds_error=False, fill_value=1e30))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates self - similarity matrix from attributes of a vmo object.", "response": "def create_selfsim(oracle, method='rsfx'):\n    \"\"\" Create self similarity matrix from attributes of a vmo object\n\n    :param oracle: a encoded vmo object\n    :param method:\n        \"comp\":use the compression codes\n        \"sfx\" - use suffix links\n        \"rsfx\" - use reverse suffix links\n        \"lrs\" - use LRS values\n        \"seg\" - use patterns found\n    :return: the created self-similarity matrix\n    \"\"\"\n\n    len_oracle = oracle.n_states - 1\n    mat = np.zeros((len_oracle, len_oracle))\n    if method == 'com':\n        if not oracle.code:\n            print(\"Codes not generated. Generating codes with encode().\")\n            oracle.encode()\n        ind = 0  # index\n        for l, p in oracle.code:  # l for length, p for position\n\n            if l == 0:\n                inc = 1\n            else:\n                inc = l\n            mat[range(ind, ind + inc), range(p - 1, p - 1 + inc)] = 1\n            mat[range(p - 1, p - 1 + inc), range(ind, ind + inc)] = 1\n            ind = ind + l\n    elif method == 'sfx':\n        for i, s in enumerate(oracle.sfx[1:]):\n            if s != 0:\n                mat[i][s - 1] = 1\n                mat[s - 1][i] = 1\n    elif method == 'rsfx':\n        for cluster in oracle.latent:\n            p = itertools.product(cluster, repeat=2)\n            for _p in p:\n                mat[_p[0] - 1][_p[1] - 1] = 1\n    elif method == 'lrs':\n        for i, l in enumerate(oracle.lrs[1:]):\n            if l != 0:\n                s = oracle.sfx[i + 1]\n                mat[range((s - l) + 1, s + 1), range(i - l + 1, i + 1)] = 1\n                mat[range(i - l + 1, i + 1), range((s - l) + 1, s + 1)] = 1\n    elif method == 'seg':\n        seg = oracle.segment\n        ind = 0\n        for l, p in seg:  # l for length, p for position\n\n            if l == 0:\n                inc = 1\n            else:\n                inc = l\n            mat[range(ind, ind + inc), range(p - 1, p - 1 + inc)] = 1\n            mat[range(p - 1, p - 1 + inc), range(ind, ind + inc)] = 1\n            ind = ind + l\n\n    return mat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_transition(oracle, method='trn'):\n    mat, hist, n = _create_trn_mat_symbolic(oracle, method)\n    return mat, hist, n", "response": "Create a transition matrix based on oracle links"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(oracle, context, ab=None, verbose=False):\n    if verbose:\n        print(\"original context: \", context)\n    if ab is None:\n        ab = oracle.get_alphabet()\n\n    _b, _s, context = _test_context(oracle, context)\n    _lrs = [oracle.lrs[k] for k in oracle.rsfx[_s]]\n    context_state = []\n    while not context_state:\n        for _i, _l in enumerate(_lrs):\n            if _l >= len(context):\n                context_state.append(oracle.rsfx[_s][_i])\n        if context_state:\n            break\n        else:\n            context = context[1:]\n            _b, _s = oracle.accept(context)\n            _lrs = [oracle.lrs[k] for k in oracle.rsfx[_s]]\n    if verbose:\n        print(\"final context: \", context)\n        print(\"context_state: \", context_state)\n    d_count = len(ab)\n    hist = [1.0] * len(ab)  # initialize all histograms with 1s.\n\n    trn_data = [oracle.data[n] for n in oracle.trn[_s]]\n    for k in trn_data:\n        hist[ab[k]] += 1.0\n        d_count += 1.0\n\n    for i in context_state:\n        d_count, hist = _rsfx_count(oracle, i, d_count, hist, ab)\n\n    return [hist[idx] / d_count for idx in range(len(hist))], context", "response": "Single symbolic prediction given a context an oracle and an alphabet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_loss(oracle, test_seq, ab=[], m_order=None, verbose=False):\n\n    if not ab:\n        ab = oracle.get_alphabet()\n    if verbose:\n        print(' ')\n\n    logP = 0.0\n    context = []\n    increment = np.floor((len(test_seq) - 1) / 100)\n    bar_count = -1\n    maxContextLength = 0\n    avgContext = 0\n    for i, t in enumerate(test_seq):\n\n        p, c = predict(oracle, context, ab, verbose=False)\n        if len(c) < len(context):\n            context = context[-len(c):]\n        logP -= np.log2(p[ab[t]])\n        context.append(t)\n\n        if m_order is not None:\n            if len(context) > m_order:\n                context = context[-m_order:]\n        avgContext += float(len(context)) / len(test_seq)\n\n        if verbose:\n            percentage = np.mod(i, increment)\n            if percentage == 0:\n                bar_count += 1\n            if len(context) > maxContextLength:\n                maxContextLength = len(context)\n            sys.stdout.write('\\r')\n            sys.stdout.write(\"\\r[\" + \"=\" * bar_count +\n                             \" \" * (100 - bar_count) + \"] \" +\n                             str(bar_count) + \"% \" +\n                             str(i) + \"/\" + str(len(test_seq) - 1) + \" Current max length: \" + str(\n                maxContextLength))\n            sys.stdout.flush()\n    return logP / len(test_seq), avgContext", "response": "Evaluate the average log - loss of a sequence given an oracle"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _rsfx_count(oracle, s, count, hist, ab):\n\n    trn_data = [oracle.data[n] for n in oracle.trn[s]]\n    for k in trn_data:\n        hist[ab[k]] += 1.0\n        count += 1.0\n\n    rsfx_candidate = oracle.rsfx[s][:]\n    while rsfx_candidate:\n        s = rsfx_candidate.pop(0)\n        trn_data = [oracle.data[n] for n in oracle.trn[s]]\n        for k in trn_data:\n            hist[ab[k]] += 1.0\n            count += 1.0\n        rsfx_candidate.extend(oracle.rsfx[s])\n\n    return count, hist", "response": "Accumulate counts for context s and rsfx"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dist_obs_oracle(oracle, query, trn_list):\n    a = np.subtract(query, [oracle.f_array[t] for t in trn_list])\n    return (a * a).sum(axis=1)", "response": "A helper function calculating distances between a feature and frames in oracle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _query_k(k, i, P, oracle, query, trn, state_cache, dist_cache, smooth=False, D=None, weight=0.5):\n\n    _trn = trn(oracle, P[i - 1][k])\n    t = list(itertools.chain.from_iterable([oracle.latent[oracle.data[j]] for j in _trn]))\n    _trn_unseen = [_t for _t in _trn if _t not in state_cache]\n    state_cache.extend(_trn_unseen)\n\n    if _trn_unseen:\n        t_unseen = list(itertools.chain.from_iterable([oracle.latent[oracle.data[j]] for j in _trn_unseen]))\n        dist_cache[t_unseen] = _dist_obs_oracle(oracle, query[i], t_unseen)\n    dvec = dist_cache[t]\n    if smooth and P[i - 1][k] < oracle.n_states - 1:\n        dvec = dvec * (1.0 - weight) + weight * np.array([D[P[i - 1][k]][_t - 1] for _t in t])\n    _m = np.argmin(dvec)\n    return t[_m], dvec[_m]", "response": "A helper function for query_k_or_jumps."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_shell_out(cmd, verbose=False, **kwargs):\n    # TODO rename this suppressed_shell_out ?\n    # TODO this should probably return 1 if there's an error (i.e. vice-versa).\n    # print(\"cmd %s\" % cmd)\n    try:\n        with open(os.devnull, \"w\") as fnull:\n            with captured_output():\n                check_output(cmd, stderr=fnull, **kwargs)\n        return True\n    except (CalledProcessError, OSError) as e:\n        if verbose:\n            cprint(\"    Error running command %s\" % ' '.join(cmd), 'err')\n            print(e.output)\n        return False\n    except Exception as e:\n        # TODO no idea\n        # Can this be if you try and unistall pip? (don't do that)\n        return False", "response": "run cmd and return True if it went ok False if something went wrong."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cprint(message, status=None):\n    # TODO use less obscure dict, probably \"error\", \"warn\", \"success\" as keys\n    status = {'warn': Fore.YELLOW, 'err': Fore.RED,\n              'ok': Fore.GREEN, None: Style.BRIGHT}[status]\n    print(status + message + Style.RESET_ALL)", "response": "color printing based on status"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_dist(toxinidir, toxdir, package):\n    dist = os.path.join(toxdir, \"dist\")\n    # Suppress warnings.\n    success = safe_shell_out([\"python\", \"setup.py\", \"sdist\", \"--quiet\",\n                              \"--formats=zip\", \"--dist-dir\", dist],\n                             cwd=toxinidir)\n    if success:\n        return os.path.join(dist, package + \".zip\")", "response": "zip up the package into the toxdir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_pretty_command(env, command):\n    cmd = abbr_cmd = command[0]\n    if cmd.startswith(env.envbindir):\n        abbr_cmd = os.path.relpath(cmd, env.envbindir)\n        if abbr_cmd == \".\":\n            # TODO are there more edge cases?\n            abbr_cmd = cmd\n    command[0] = abbr_cmd\n    print('(%s)$ %s' % (env.name, ' '.join(['\"%s\"' % c if \" \" in c\n                                            else c\n                                            for c in command])))\n    command[0] = cmd\n    return abbr_cmd, cmd, command", "response": "This function is a hack for prettier printing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maximizeLikelihood(self, optimize_brlen=False,\n            approx_grad=False, logliktol=1.0e-2, nparamsretry=1,\n            printfunc=None):\n        \"\"\"Maximize the log likelihood.\n\n        Maximizes log likelihood with respect to model parameters\n        and potentially branch lengths depending on `optimize_brlen`.\n        If optimizing the branch lengths, iterates between optimizing\n        the model parameters and branch lengths.\n\n        Uses the L-BFGS-B method implemented in `scipy.optimize`.\n\n        There is no return variable, but after call object attributes\n        will correspond to maximimum likelihood values.\n\n        Args:\n            `optimize_brlen` (bool)\n                Do we optimize branch lengths?\n            `approx_grad` (bool)\n                If `True`, then we numerically approximate the gradient\n                rather than using the analytical values.\n            `logliktol` (float)\n                When using `optimize_brlen`, keep iterating between\n                optimization of parameters and branch lengths until\n                change in log likelihood is less than `logliktol`.\n            `nparamsretry` (int >= 0)\n                Number of times to retry parameter optimization from\n                different initial values if it fails the first time.\n            `printfunc` (`None` or a function)\n                If not `None`, then we print using `printfunc` the\n                detailed results of the optimization at each step.\n                For instance, `printfunc` might be `sys.stderr.write`\n                or `logger.info`.\n\n        Returns:\n            A string giving a summary of the maximization.\n        \"\"\"\n        # Some useful notes on optimization:\n        # http://www.scipy-lectures.org/advanced/mathematical_optimization/\n\n        assert len(self.paramsarray) > 0, \"No parameters to optimize\"\n        assert nparamsretry >= 0\n        assert logliktol > 0\n\n        def paramsfunc(x):\n            \"\"\"Negative log likelihood when `x` is params.\"\"\"\n            self.paramsarray = x\n            return -self.loglik\n\n        def paramsdfunc(x):\n            \"\"\"Negative gradient log likelihood with respect to params.\"\"\"\n            self.paramsarray = x\n            return -self.dloglikarray\n\n        def tfunc(x):\n            \"\"\"Negative log likelihood when `x` is branch lengths.\"\"\"\n            self.t = x\n            return -self.loglik\n\n        def tdfunc(x):\n            \"\"\"Negative gradient loglik with respect to branch lengths.\"\"\"\n            self.t = x\n            return -self.dloglik_dt\n\n        if approx_grad:\n            paramsdfunc = False\n            tdfunc = False\n            self.dtcurrent = False\n            self.dparamscurrent = False\n\n        def _printResult(opttype, result, i, old, new):\n            \"\"\"Print summary of optimization result.\"\"\"\n            if printfunc is not None:\n                printfunc('Step {0}, optimized {1}.\\n'\n                          'Likelihood went from {2} to {3}.\\n'\n                          'Max magnitude in Jacobian is {4}.\\n'\n                          'Full optimization result:\\n{5}\\n'.format(\n                          i, opttype, old, new,\n                          scipy.absolute(result.jac).max(), result))\n\n        oldloglik = self.loglik\n        converged = False\n        firstbrlenpass = True\n        options = {'ftol':1.0e-7} # optimization options\n        summary = []\n        i = 1\n        while not converged:\n            if (not self.dparamscurrent) and (not approx_grad):\n                self.dtcurrent = False\n                self.dparamscurrent = True\n            nparamstry = 0\n            origparamsarray = self.paramsarray.copy()\n            paramsconverged = False\n            while not paramsconverged:\n                result = scipy.optimize.minimize(paramsfunc, self.paramsarray,\n                        method='L-BFGS-B', jac=paramsdfunc,\n                        bounds=self.paramsarraybounds, options=options)\n                _printResult('params', result, i, oldloglik, self.loglik)\n                msg = ('Step {0}: optimized parameters, loglik went from '\n                        '{1:.2f} to {2:.2f} ({3} iterations, {4} function '\n                        'evals)'.format(i, oldloglik, self.loglik, result.nit,\n                        result.nfev))\n                summary.append(msg)\n                if result.success and (not (oldloglik - self.loglik > logliktol)):\n                    paramsconverged = True\n                    jacmax = scipy.absolute(result.jac).max()\n                    if (jacmax > 1000) and not (firstbrlenpass and optimize_brlen):\n                        warnings.warn(\"Optimizer reports convergence, \"\n                                \"but max element in Jacobian is {0}\\n\"\n                                \"Summary of optimization:\\n{1}\".format(\n                                jacmax, summary))\n                else:\n                    if not result.success:\n                        resultmessage = result.message\n                    else:\n                        resultmessage = ('loglik increased in param optimization '\n                                'from {0} to {1}'.format(oldloglik, self.loglik))\n                    nparamstry += 1\n                    failmsg = (\"Optimization failure {0}\\n{1}\\n{2}\".format(\n                            nparamstry, resultmessage, '\\n'.join(summary)))\n                    if nparamstry > nparamsretry:\n                        raise RuntimeError(failmsg)\n                    else:\n                        warnings.warn(failmsg + '\\n\\n' +\n                                \"Re-trying with different initial params.\")\n                        scipy.random.seed(nparamstry)\n                        # seed at geometric mean of original value, max\n                        # bound, min bound, and random number between max and min\n                        minarray = scipy.array([self.paramsarraybounds[j][0] for\n                                j in range(len(self.paramsarray))])\n                        maxarray = scipy.array([self.paramsarraybounds[j][1] for\n                                j in range(len(self.paramsarray))])\n                        randarray = scipy.random.uniform(minarray, maxarray)\n                        newarray = (minarray * maxarray * randarray *\n                                origparamsarray)**(1 / 4.) # geometric mean\n                        assert newarray.shape == self.paramsarray.shape\n                        assert (newarray > minarray).all()\n                        assert (newarray < maxarray).all()\n                        self.paramsarray = newarray\n            i += 1\n            assert oldloglik - self.loglik <= logliktol\n            if (self.loglik - oldloglik >= logliktol) or firstbrlenpass:\n                firstbrlenpass = False\n                oldloglik = self.loglik\n                if optimize_brlen:\n                    if not approx_grad:\n                        self.dparamscurrent = False\n                        self.dtcurrent = True\n                    result = scipy.optimize.minimize(tfunc, self.t,\n                            method='L-BFGS-B', jac=tdfunc, options=options,\n                            bounds=[(ALMOST_ZERO, None)] * len(self.t))\n                    _printResult('branches', result, i, oldloglik, self.loglik)\n                    summary.append('Step {0}: optimized branches, loglik '\n                            'went from {1:.2f} to {2:.2f} ({3} iterations, '\n                            '{4} function evals)'.format(i, oldloglik,\n                            self.loglik, result.nit, result.nfev))\n                    i += 1\n                    assert result.success, (\"Optimization failed\\n{0}\"\n                            \"\\n{1}\\n{2}\".format(result.message, self.t,\n                            '\\n'.join(summary)))\n                    if oldloglik - self.loglik > logliktol:\n                        raise RuntimeError(\"loglik increased during t \"\n                                \"optimization: {0} to {1}\".format(\n                                oldloglik, self.loglik))\n                    elif self.loglik - oldloglik >= logliktol:\n                        oldloglik = self.loglik\n                    else:\n                        converged = True\n                else:\n                    converged = True\n            else:\n                converged = True\n\n        return '\\n'.join(summary)", "response": "Maximizes the log likelihood with respect to model parameters and potentially branch lengths depending on the optimization."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tree(self):\n        bs = self.model.branchScale\n        for node in self._tree.find_clades():\n            if node != self._tree.root:\n                node.branch_length = self.t[self.name_to_nodeindex[node]] * bs\n        return self._tree", "response": "A tree with branch lengths in codon substitutions per site."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbounds for parameters in paramsarray.", "response": "def paramsarraybounds(self):\n        \"\"\"Bounds for parameters in `paramsarray`.\"\"\"\n        bounds = []\n        for (i, param) in self._index_to_param.items():\n            if isinstance(param, str):\n                bounds.append(self.model.PARAMLIMITS[param])\n            elif isinstance(param, tuple):\n                bounds.append(self.model.PARAMLIMITS[param[0]])\n            else:\n                raise ValueError(\"Invalid param type\")\n        bounds = [(tup[0] + ALMOST_ZERO, tup[1] - ALMOST_ZERO) for tup in bounds]\n        assert len(bounds) == len(self._index_to_param)\n        return tuple(bounds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paramsarray(self, value):\n        nparams = len(self._index_to_param)\n        assert (isinstance(value, scipy.ndarray) and value.ndim == 1), (\n                \"paramsarray must be 1-dim ndarray\")\n        assert len(value) == nparams, (\"Assigning paramsarray to ndarray \"\n                \"of the wrong length.\")\n        if (self._paramsarray is not None) and all(value == self._paramsarray):\n            return # do not need to do anything if value has not changed\n        # build `newvalues` to pass to `updateParams`\n        newvalues = {}\n        vectorized_params = {}\n        for (i, param) in self._index_to_param.items():\n            if isinstance(param, str):\n                newvalues[param] = float(value[i])\n            elif isinstance(param, tuple):\n                (iparam, iparamindex) = param\n                if iparam in vectorized_params:\n                    assert iparamindex not in vectorized_params[iparam]\n                    vectorized_params[iparam][iparamindex] = float(value[i])\n                else:\n                    vectorized_params[iparam] = {iparamindex:float(value[i])}\n            else:\n                raise ValueError(\"Invalid param type\")\n        for (param, paramd) in vectorized_params.items():\n            assert set(paramd.keys()) == set(range(len(paramd)))\n            newvalues[param] = scipy.array([paramd[i] for i in range(len(paramd))],\n                    dtype='float')\n        self.updateParams(newvalues)\n        self._paramsarray = self.paramsarray", "response": "Set new paramsarray and update via updateParams."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets value of dtcurrent update derivatives if needed.", "response": "def dtcurrent(self, value):\n        \"\"\"Set value of `dtcurrent`, update derivatives if needed.\"\"\"\n        assert isinstance(value, bool)\n        if value and self.dparamscurrent:\n            raise RuntimeError(\"Can't set both dparamscurrent and dtcurrent True\")\n        if value != self.dtcurrent:\n            self._dtcurrent = value\n            self._updateInternals()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef t(self, value):\n        assert (isinstance(value, scipy.ndarray) and (value.dtype ==\n                'float') and (value.shape == self.t.shape))\n        if (self._t != value).any():\n            self._t = value.copy()\n            self._updateInternals()", "response": "Set new branch lengths update likelihood and derivatives."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate model parameters and re - compute likelihoods.", "response": "def updateParams(self, newvalues):\n        \"\"\"Update model parameters and re-compute likelihoods.\n\n        This method is the **only** acceptable way to update model\n        parameters. The likelihood is re-computed as needed\n        by this method.\n\n        Args:\n            `newvalues` (dict)\n                A dictionary keyed by param name and with value as new\n                value to set. Each parameter name must either be a\n                valid model parameter (in `model.freeparams`).\n        \"\"\"\n        for (param, value) in newvalues.items():\n            if param not in self.model.freeparams:\n                raise RuntimeError(\"Can't handle param: {0}\".format(\n                        param))\n        if newvalues:\n            self.model.updateParams(newvalues)\n            self._updateInternals()\n            self._paramsarray = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _updateInternals(self):\n        rootnode = self.nnodes - 1\n        if self._distributionmodel:\n            catweights = self.model.catweights\n        else:\n            catweights = scipy.ones(1, dtype='float')\n        # When there are multiple categories, it is acceptable\n        # for some (but not all) of them to have underflow at\n        # any given site. Note that we still include a check for\n        # Underflow by ensuring that none of the site likelihoods is\n        # zero.\n        undererrstate = 'ignore' if len(catweights) > 1 else 'raise'\n        with scipy.errstate(over='raise', under=undererrstate,\n                divide='raise', invalid='raise'):\n            self.underflowlogscale.fill(0.0)\n            self._computePartialLikelihoods()\n            sitelik = scipy.zeros(self.nsites, dtype='float')\n            assert (self.L[rootnode] >= 0).all(), str(self.L[rootnode])\n            for k in self._catindices:\n                sitelik += scipy.sum(self._stationarystate(k) *\n                        self.L[rootnode][k], axis=1) * catweights[k]\n            assert (sitelik > 0).all(), \"Underflow:\\n{0}\\n{1}\".format(\n                    sitelik, self.underflowlogscale)\n            self.siteloglik = scipy.log(sitelik) + self.underflowlogscale\n            self.loglik = scipy.sum(self.siteloglik) + self.model.logprior\n            if self.dparamscurrent:\n                self._dloglik = {}\n                for param in self.model.freeparams:\n                    if self._distributionmodel and (param in\n                            self.model.distributionparams):\n                        name = self.model.distributedparam\n                        weighted_dk = (self.model.d_distributionparams[param]\n                                * catweights)\n                    else:\n                        name = param\n                        weighted_dk = catweights\n                    dsiteloglik = 0\n                    for k in self._catindices:\n                        dsiteloglik += (scipy.sum(\n                                self._dstationarystate(k, name) *\n                                self.L[rootnode][k] + self.dL[name][rootnode][k] *\n                                self._stationarystate(k), axis=-1) *\n                                weighted_dk[k])\n                    dsiteloglik /= sitelik\n                    self._dloglik[param] = (scipy.sum(dsiteloglik, axis=-1)\n                            + self.model.dlogprior(param))\n            if self.dtcurrent:\n                self._dloglik_dt = 0\n                dLnroot_dt = scipy.array([self.dL_dt[n2][rootnode] for\n                        n2 in sorted(self.dL_dt.keys())])\n                for k in self._catindices:\n                    if isinstance(k, int):\n                        dLnrootk_dt = dLnroot_dt.swapaxes(0, 1)[k]\n                    else:\n                        assert k == slice(None)\n                        dLnrootk_dt = dLnroot_dt\n                    self._dloglik_dt += catweights[k] * scipy.sum(\n                            self._stationarystate(k) *\n                            dLnrootk_dt, axis=-1)\n                self._dloglik_dt /= sitelik\n                self._dloglik_dt = scipy.sum(self._dloglik_dt, axis=-1)\n                assert self._dloglik_dt.shape == self.t.shape", "response": "Update internal attributes related to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _M(self, k, t, tips=None, gaps=None):\n        if self._distributionmodel:\n            return self.model.M(k, t, tips, gaps)\n        else:\n            return self.model.M(t, tips, gaps)", "response": "Returns matrix exponential M."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dM(self, k, t, param, M, tips=None, gaps=None):\n        if self._distributionmodel:\n            return self.model.dM(k, t, param, M, tips, gaps)\n        else:\n            return self.model.dM(t, param, M, tips, gaps)", "response": "Returns derivative of matrix exponential."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _stationarystate(self, k):\n        if self._distributionmodel:\n            return self.model.stationarystate(k)\n        else:\n            return self.model.stationarystate", "response": "Returns the stationarystate ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the dstationarystate .", "response": "def _dstationarystate(self, k, param):\n        \"\"\"Returns the dstationarystate .\"\"\"\n        if self._distributionmodel:\n            return self.model.dstationarystate(k, param)\n        else:\n            return self.model.dstationarystate(param)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist of parameters looped over in _computePartialLikelihoods.", "response": "def _paramlist_PartialLikelihoods(self):\n        \"\"\"List of parameters looped over in `_computePartialLikelihoods`.\"\"\"\n        if self._distributionmodel:\n            return [param for param in self.model.freeparams +\n                    [self.model.distributedparam] if param not in\n                    self.model.distributionparams]\n        else:\n            return self.model.freeparams"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _computePartialLikelihoods(self):\n        for n in range(self.ntips, self.nnodes):\n            ni = n - self.ntips # internal node number\n            nright = self.rdescend[ni]\n            nleft = self.ldescend[ni]\n            if nright < self.ntips:\n                istipr = True\n            else:\n                istipr = False\n            if nleft < self.ntips:\n                istipl = True\n            else:\n                istipl = False\n            tright = self.t[nright]\n            tleft = self.t[nleft]\n            self.L[n] = scipy.ndarray(self._Lshape, dtype='float')\n            if self.dparamscurrent:\n                for param in self._paramlist_PartialLikelihoods:\n                    self.dL[param][n] = scipy.ndarray(self._dLshape[param],\n                            dtype='float')\n            if self.dtcurrent:\n                for n2 in self.dL_dt.keys():\n                    self.dL_dt[n2][n] = scipy.zeros(self._Lshape, dtype='float')\n            for k in self._catindices:\n                if istipr:\n                    Mright = MLright = self._M(k, tright,\n                            self.tips[nright], self.gaps[nright])\n                else:\n                    Mright = self._M(k, tright)\n                    MLright = broadcastMatrixVectorMultiply(Mright,\n                            self.L[nright][k])\n                if istipl:\n                    Mleft = MLleft = self._M(k, tleft,\n                            self.tips[nleft], self.gaps[nleft])\n                else:\n                    Mleft = self._M(k, tleft)\n                    MLleft = broadcastMatrixVectorMultiply(Mleft,\n                            self.L[nleft][k])\n                self.L[n][k] = MLright * MLleft\n\n                if self.dtcurrent:\n                    for (tx, Mx, nx, MLxother, istipx) in [\n                            (tright, Mright, nright, MLleft, istipr),\n                            (tleft, Mleft, nleft, MLright, istipl)]:\n                        if istipx:\n                            tipsx = self.tips[nx]\n                            gapsx = self.gaps[nx]\n                        else:\n                            tipsx = gapsx = None\n                        dM_dt = self._dM(k, tx, 't', Mx, tipsx, gapsx)\n                        if istipx:\n                            LdM_dt = dM_dt\n                        else:\n                            LdM_dt = broadcastMatrixVectorMultiply(\n                                    dM_dt, self.L[nx][k])\n                        self.dL_dt[nx][n][k] = LdM_dt * MLxother\n                        for ndx in self.descendants[nx]:\n                            self.dL_dt[ndx][n][k] = broadcastMatrixVectorMultiply(\n                                    Mx, self.dL_dt[ndx][nx][k]) * MLxother\n\n                if self.dparamscurrent:\n                    for param in self._paramlist_PartialLikelihoods:\n                        if istipr:\n                            dMright = self._dM(k, tright, param, Mright,\n                                    self.tips[nright], self.gaps[nright])\n                        else:\n                            dMright = self._dM(k, tright, param, Mright)\n                        if istipl:\n                            dMleft = self._dM(k, tleft, param, Mleft,\n                                    self.tips[nleft], self.gaps[nleft])\n                        else:\n                            dMleft = self._dM(k, tleft, param, Mleft)\n                        for j in self._sub_index_param(param):\n                            if istipr:\n                                dMLright = dMright[j]\n                                MdLright = 0\n                            else:\n                                dMLright = broadcastMatrixVectorMultiply(\n                                        dMright[j], self.L[nright][k])\n                                MdLright = broadcastMatrixVectorMultiply(\n                                        Mright, self.dL[param][nright][k][j])\n                            if istipl:\n                                dMLleft = dMleft[j]\n                                MdLleft = 0\n                            else:\n                                dMLleft = broadcastMatrixVectorMultiply(\n                                        dMleft[j], self.L[nleft][k])\n                                MdLleft = broadcastMatrixVectorMultiply(\n                                        Mleft, self.dL[param][nleft][k][j])\n                            self.dL[param][n][k][j] = ((dMLright + MdLright)\n                                    * MLleft + MLright * (dMLleft + MdLleft))\n\n            if ni > 0 and ni % self.underflowfreq == 0:\n                # rescale by same amount for each category k\n                scale = scipy.amax(scipy.array([scipy.amax(self.L[n][k],\n                        axis=1) for k in self._catindices]), axis=0)\n                assert scale.shape == (self.nsites,)\n                self.underflowlogscale += scipy.log(scale)\n                for k in self._catindices:\n                    self.L[n][k] /= scale[:, scipy.newaxis]\n                    if self.dtcurrent:\n                        for n2 in self.dL_dt.keys():\n                            self.dL_dt[n2][n][k] /= scale[:, scipy.newaxis]\n                    if self.dparamscurrent:\n                        for param in self._paramlist_PartialLikelihoods:\n                            for j in self._sub_index_param(param):\n                                self.dL[param][n][k][j] /= scale[:, scipy.newaxis]\n\n            # free unneeded memory by deleting already used values\n            for ntodel in [nright, nleft]:\n                if ntodel in self.L:\n                    del self.L[ntodel]\n                if self.dparamscurrent:\n                    for param in self._paramlist_PartialLikelihoods:\n                        if ntodel in self.dL[param]:\n                            del self.dL[param][ntodel]\n                if self.dtcurrent:\n                    for n2 in self.dL_dt.keys():\n                        if ntodel in self.dL_dt[n2]:\n                            del self.dL_dt[n2][ntodel]", "response": "Update L dL and dL_dt based on the partial likelihoods of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns list of sub - indexes for param. Used in computing partial likelihoods ; loop over these indices ; return them.", "response": "def _sub_index_param(self, param):\n        \"\"\"Returns list of sub-indexes for `param`.\n\n        Used in computing partial likelihoods; loop over these indices.\"\"\"\n        if self._distributionmodel and (param ==\n                self.model.distributedparam):\n            indices = [()]\n        else:\n            paramvalue = getattr(self.model, param)\n            if isinstance(paramvalue, float):\n                indices = [()]\n            elif (isinstance(paramvalue, scipy.ndarray) and\n                    paramvalue.ndim == 1 and paramvalue.shape[0] > 1):\n                indices = [(j,) for j in range(len(paramvalue))]\n            else:\n                raise RuntimeError(\"Invalid param: {0}, {1}\".format(\n                        param, paramvalue))\n        return indices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_request(self, *args, **kwargs):\n        try:\n            return super(JSHost, self).send_request(*args, **kwargs)\n        except RequestsConnectionError as e:\n            if (\n                self.manager and\n                self.has_connected and\n                self.logfile and\n                'unsafe' not in kwargs\n            ):\n                raise ProcessError(\n                    '{} appears to have crashed, you can inspect the log file at {}'.format(\n                        self.get_name(),\n                        self.logfile,\n                    )\n                )\n            raise six.reraise(RequestsConnectionError, RequestsConnectionError(*e.args), sys.exc_info()[2])", "response": "Intercept connection errors which suggest that a managed host has crashed and raise an exception indicating the location of the log file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the minimal weight perfect b - matching using min - sum loopy - BP.", "response": "def b_matching(D, k, max_iter=1000, damping=1, conv_thresh=1e-4,\r\n               weighted=False, verbose=False):\r\n  '''\r\n  \"Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs\r\n  and its Relation to Linear Programs with Integer Solutions\"\r\n  Bayati et al.\r\n\r\n  Finds the minimal weight perfect b-matching using min-sum loopy-BP.\r\n\r\n  @param D pairwise distance matrix\r\n  @param k number of neighbors per vertex (scalar or array-like)\r\n\r\n  Based on the code at http://www.cs.columbia.edu/~bert/code/bmatching/bdmatch\r\n  '''\r\n  INTERVAL = 2\r\n  oscillation = 10\r\n  cbuff = np.zeros(100, dtype=float)\r\n  cbuffpos = 0\r\n  N = D.shape[0]\r\n  assert D.shape[1] == N, 'Input distance matrix must be square'\r\n  mask = ~np.eye(N, dtype=bool)  # Assume all nonzero except for diagonal\r\n  W = -D[mask].reshape((N, -1)).astype(float)\r\n  degrees = np.clip(np.atleast_1d(k), 0, N-1)\r\n  if degrees.size == 1:  # broadcast scalar up to length-N array\r\n    degrees = np.repeat(degrees, N)\r\n  else:\r\n    assert degrees.shape == (N,), 'Input degrees must have length N'\r\n  # TODO: remove these later\r\n  inds = np.tile(np.arange(N), (N, 1))\r\n  backinds = inds.copy()\r\n  inds = inds[mask].reshape((N, -1))\r\n  backinds = backinds.T.ravel()[:(N*(N-1))].reshape((N, -1))\r\n\r\n  # Run Belief Revision\r\n  change = 1.0\r\n  B = W.copy()\r\n  for n_iter in range(1, max_iter+1):\r\n    oldB = B.copy()\r\n    update_belief(oldB, B, W, degrees, damping, inds, backinds)\r\n\r\n    # check for convergence\r\n    if n_iter % INTERVAL == 0:\r\n      # track changes\r\n      c = np.abs(B[:,0]).sum()\r\n      # c may be infinite here, and that's ok\r\n      with np.errstate(invalid='ignore'):\r\n        if np.any(np.abs(c - cbuff) < conv_thresh):\r\n          oscillation -= 1\r\n      cbuff[cbuffpos] = c\r\n      cbuffpos = (cbuffpos + 1) % len(cbuff)\r\n\r\n      change = diff_belief(B, oldB)\r\n      if np.isnan(change):\r\n        warnings.warn(\"change is NaN! \"\r\n                      \"BP will quit but solution could be invalid. \"\r\n                      \"Problem may be infeasible.\")\r\n        break\r\n      if change < conv_thresh or oscillation < 1:\r\n        break\r\n  else:\r\n    warnings.warn(\"Hit iteration limit (%d) before converging\" % max_iter)\r\n\r\n  if verbose:  # pragma: no cover\r\n    if change < conv_thresh:\r\n      print(\"Converged to stable beliefs in %d iterations\" % n_iter)\r\n    elif oscillation < 1:\r\n      print(\"Stopped after reaching oscillation in %d iterations\" % n_iter)\r\n      print(\"No feasible solution found or there are multiple maxima.\")\r\n      print(\"Outputting best approximate solution. Try damping.\")\r\n\r\n  # recover result from B\r\n  thresholds = np.zeros(N)\r\n  for i,d in enumerate(degrees):\r\n    Brow = B[i]\r\n    if d >= N - 1:\r\n      thresholds[i] = -np.inf\r\n    elif d < 1:\r\n      thresholds[i] = np.inf\r\n    else:\r\n      thresholds[i] = Brow[quickselect(-Brow, d-1)]\r\n\r\n  ii,jj = np.where(B >= thresholds[:,None])\r\n  pairs = np.column_stack((ii, inds[ii,jj]))\r\n  w = D[ii, pairs[:,1]] if weighted else None\r\n  return Graph.from_edge_pairs(pairs, num_vertices=N, weights=w)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate B based on oldB and belief update function.", "response": "def _updateB(oldB, B, W, degrees, damping, inds, backinds):  # pragma: no cover\r\n  '''belief update function.'''\r\n  for j,d in enumerate(degrees):\r\n    kk = inds[j]\r\n    bk = backinds[j]\r\n\r\n    if d == 0:\r\n      B[kk,bk] = -np.inf\r\n      continue\r\n\r\n    belief = W[kk,bk] + W[j]\r\n    oldBj = oldB[j]\r\n    if d == oldBj.shape[0]:\r\n      bth = quickselect(-oldBj, d-1)\r\n      bplus = -1\r\n    else:\r\n      bth,bplus = quickselect(-oldBj, d-1, d)\r\n\r\n    belief -= np.where(oldBj >= oldBj[bth], oldBj[bplus], oldBj[bth])\r\n    B[kk,bk] = damping*belief + (1-damping)*oldB[kk,bk]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a copy of function f with a different code", "response": "def _func_copy(f, newcode) :\r\n    '''\r\n    Return a copy of function f with a different __code__\r\n    Because I can't find proper documentation on the\r\n    correct signature of the types.FunctionType() constructor,\r\n    I pass the minimum arguments then set the important\r\n    dunder-values by direct assignment.\r\n\r\n    Note you cannot assign __closure__, it is a \"read-only attribute\".\r\n    Ergo, you should not apply _make_constants() to a function that\r\n    has a closure!\r\n    '''\r\n    newf = types.FunctionType( newcode, f.__globals__ )\r\n    newf.__annotations__ = f.__annotations__\r\n    # newf.__closure__ = f.__closure__\r\n    newf.__defaults__ = f.__defaults__\r\n    newf.__doc__ = f.__doc__\r\n    newf.__name__ = f.__name__\r\n    newf.__kwdefaults__ = f.__kwdefaults__\r\n    newf.__qualname__ = f.__qualname__\r\n    return newf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind_all(mc, builtin_only=False, stoplist=[],  verbose=False):\r\n    import types\r\n    try:\r\n        d = vars(mc)\r\n    except TypeError:\r\n        return\r\n    for k, v in d.items():\r\n        if isinstance( v, types.FunctionType ) :\r\n            if verbose :\r\n                print( 'make_constants(', v.__name__, ')' )\r\n            newv = _make_constants(v, builtin_only, stoplist,  verbose)\r\n            setattr(mc, k, newv)\r\n        elif type(v) in ( type, types.ModuleType ):\r\n            bind_all(v, builtin_only, stoplist, verbose)", "response": "Recursively apply constant binding to functions in a module or class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_constants(builtin_only=False, stoplist=[], verbose=False):\r\n    if type(builtin_only) == type(make_constants):\r\n        raise ValueError(\"The make_constants decorator must have arguments.\")\r\n    return lambda f: _make_constants(f, builtin_only, stoplist, verbose)", "response": "Returns a decorator for optimizing global references."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef state():\n    '''Get The playback state: 'playing', 'paused', or 'stopped'.\n\n    If PLAYING or PAUSED, show information on current track.\n\n    Calls PlaybackController.get_state(), and if state is PLAYING or PAUSED, get\n      PlaybackController.get_current_track() and\n      PlaybackController.get_time_position()'''\n\n    server = getServer()\n    state = server.core.playback.get_state()\n    logging.debug('Got playback state: %r', state)\n    if state.upper() == 'STOPPED':\n        print('Playback is currently stopped')\n    else:\n        track = server.core.playback.get_current_track()\n        logging.debug('Track is %r', track)\n        logging.debug('Track loaded is %r', jsonrpclib.jsonclass.load(track))\n        pos = server.core.playback.get_time_position()\n        logging.debug('Pos is %r', pos)\n        print('{} track: \"{}\", by {} (at {})'.format(state.title(),\n                                                     track['name'],\n                                                     ','.join([a['name'] for a in track['artists']]),\n                                                     formatTimeposition(pos))\n              )", "response": "Get The playback state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pause():\n    '''Pause playback.\n\n    Calls PlaybackController.pause()'''\n\n    server = getServer()\n    server.core.playback.pause()\n    pos = server.core.playback.get_time_position()\n    print('Paused at {}'.format(formatTimeposition(pos)))", "response": "Pause playback.\n    Calls PlaybackController. pause"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tracklist():\n    '''Get tracklist\n\n    Calls TracklistController.get_tl_tracks()\n    '''\n    _c = 0\n    server = getServer()\n    _current = server.core.tracklist.index()\n    for t in server.core.tracklist.get_tl_tracks():\n        logging.debug('Got tl trak: %r', t)\n        currently = ' -- CURRENT' if t['tlid'] == _current else ''\n        print('{}: {}{}'.format(t['tlid'], t['track']['name'], currently))\n        _c = _c+1\n    print('==='*6)\n    print('{} tracks in tracklist'.format(_c))", "response": "Get tracklist\n    Calls TracklistController. get_tl_tracks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef play_backend_uri(argv=None):\n    '''Get album or track from backend uri and play all tracks found.\n\n    uri is a string which represents some directory belonging to a backend.\n\n    Calls LibraryController.browse(uri) to get an album and LibraryController.lookup(uri)\n    to get track'''\n\n    if argv is None:\n        argv = sys.argv[1:]\n    parser = argparse.ArgumentParser(description='Browse directories and tracks at the given uri and play them/it.')\n    parser.add_argument('uri',\n                        help='The key that represents some directory belonging to a backend. E.g. plex:album:2323 or spotify:album:xxxx')\n    parser.add_argument('-l', '--loglevel', help='Logging level. Default: %(default)s.',\n        choices=('debug', 'info', 'warning', 'error'), default='warning')\n    args = parse_args_and_apply_logging_level(parser, argv)\n    server = getServer()\n    hits = server.core.library.browse(args.uri)\n    # browse(): Returns a list of mopidy.models.Ref objects for the directories and tracks at the given uri.\n    logging.info('Got hits from browse(): %r', hits)\n    if len(hits) == 0:\n        # try track lookup\n        hits = server.core.library.lookup(args.uri)\n        logging.info('Got hits from lookup() : %r', hits)\n\n    if len(hits) == 0:\n        print('No hits for \"{}\"'.format(args.uri))\n    else:\n        server.core.tracklist.clear()\n        logging.debug('got special uris: %r', [t['uri'] for t in hits])\n        server.core.tracklist.add(uris=[t['uri'] for t in hits])\n        server.core.playback.play()", "response": "Get album or track from backend uri and play all tracks found."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns default is there is no subsequent item", "response": "def peek(self, default=None):\n        '''Returns `default` is there is no subsequent item'''\n        try:\n            result = self.pointer.next()\n            # immediately push it back onto the front of the iterable\n            self.pointer = itertools.chain([result], self.pointer)\n            return result\n        except StopIteration:\n            # nothing to put back; iterating doesn't change anything past the end\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute a unique integer for an atom", "response": "def compute_equiv_class(atom):\n    \"\"\"(atom)->Computes a unique integer for an atom\"\"\"    \n    try:\n        equiv_class = atom.number + \\\n                      1000*(atom.charge+10) + \\\n                      100000*(atom.hcount) + \\\n                      1000000*(atom.weight)\n    except TypeError:\n        raise ValueError, \\\n              \"Can't compute number from atom.number %s atom.charge %s atom.hcount %s\"\\\n              \" atom.weight %s\"%(atom.number, atom.charge, atom.hcount, atom.weight)\n    return equiv_class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the token for this payment.", "response": "def token(self):\n        \"\"\"\n        Token given by Transbank for payment initialization url.\n\n        Will raise PaymentError when an error ocurred.\n        \"\"\"\n        if not self._token:\n            self._token = self.fetch_token()\n            logger.payment(self)\n        return self._token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a secure random int between 0 and 999999999.", "response": "def transaction_id(self):\n        \"\"\"\n        Transaction ID for Transbank, a secure random int between 0 and 999999999.\n        \"\"\"\n        if not self._transaction_id:\n            self._transaction_id = random.randint(0, 10000000000 - 1)\n        return self._transaction_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if parameters are okay.", "response": "def _sanity_check(self):\n        \"\"\"Check if parameters are okay.\n\n        Sanity check makes sure each parameter is within an allowable range.\n\n        Raises:\n            ValueError: Problem with a specific parameter.\n\n        \"\"\"\n        if any(self.m1 < 0.0):\n            raise ValueError(\"Mass 1 is negative.\")\n        if any(self.m2 < 0.0):\n            raise ValueError(\"Mass 2 is negative.\")\n\n        if any(self.chi_1 < -1.0) or any(self.chi_1 > 1.0):\n            raise ValueError(\"Chi 1 is outside [-1.0, 1.0].\")\n\n        if any(self.chi_2 < -1.0) or any(self.chi_2 > 1.0):\n            raise ValueError(\"Chi 2 is outside [-1.0, 1.0].\")\n\n        if any(self.z <= 0.0):\n            raise ValueError(\"Redshift is zero or negative.\")\n\n        if any(self.dist <= 0.0):\n            raise ValueError(\"Distance is zero or negative.\")\n\n        if any(self.st < 0.0):\n            raise ValueError(\"Start Time is negative.\")\n\n        if any(self.et < 0.0):\n            raise ValueError(\"End Time is negative.\")\n\n        if len(np.where(self.st < self.et)[0]) != 0:\n            raise ValueError(\"Start Time is less than End time.\")\n\n        if any(self.m1/self.m2 > 1.0000001e4) or any(self.m1/self.m2 < 9.999999e-5):\n            raise ValueError(\"Mass Ratio too far from unity.\")\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _broadcast_and_set_attrs(self, local_dict):\n        del local_dict['self']\n        self.remove_axis = False\n        max_length = 0\n        for key in local_dict:\n            try:\n                length = len(local_dict[key])\n                if length > max_length:\n                    max_length = length\n\n            except TypeError:\n                pass\n\n        if max_length == 0:\n            self.remove_axis = True\n            for key in local_dict:\n                setattr(self, key, np.array([local_dict[key]]))\n\n        # check for bad length arrays\n        else:\n            for key in local_dict:\n                try:\n                    if len(local_dict[key]) < max_length and len(local_dict[key]) > 1:\n                        raise ValueError(\"Casting parameters not correct.\"\n                                         + \" Need all at a maximum shape and the rest being\"\n                                         + \"len-1 arrays or scalars\")\n                except TypeError:\n                    pass\n\n            # broadcast arrays\n            for key in local_dict:\n                try:\n                    if len(local_dict[key]) == max_length:\n                        setattr(self, key, local_dict[key])\n                    elif len(local_dict[key]) == 1:\n                        setattr(self, key, np.full((max_length,), local_dict[key][0]))\n                except TypeError:\n                    setattr(self, key, np.full((max_length,), local_dict[key]))\n        return", "response": "This method broadcasts all inputs to correct dimensions and sets the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate frequency domain waveforms.", "response": "def _create_waveforms(self):\n        \"\"\"Create frequency domain waveforms.\n\n        Method to create waveforms for PhenomDWaveforms class.\n        It adds waveform information in the form of attributes.\n\n        \"\"\"\n\n        c_obj = ctypes.CDLL(self.exec_call)\n\n        # prepare ctypes arrays\n        freq_amp_cast = ctypes.c_double*self.num_points*self.length\n        freqs = freq_amp_cast()\n        hc = freq_amp_cast()\n\n        fmrg_fpeak_cast = ctypes.c_double*self.length\n        fmrg = fmrg_fpeak_cast()\n        fpeak = fmrg_fpeak_cast()\n\n        # Find hc\n        c_obj.Amplitude(ctypes.byref(freqs), ctypes.byref(hc), ctypes.byref(fmrg),\n                        ctypes.byref(fpeak),\n                        self.m1.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.m2.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.chi_1.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.chi_2.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.dist.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.z.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.st.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        self.et.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n                        ctypes.c_int(self.length), ctypes.c_int(self.num_points))\n\n        # turn output into numpy arrays\n        self.freqs, self.hc = np.ctypeslib.as_array(freqs), np.ctypeslib.as_array(hc)\n        self.fmrg, self.fpeak = np.ctypeslib.as_array(fmrg), np.ctypeslib.as_array(fpeak)\n\n        # remove an axis if inputs were scalar.\n        if self.remove_axis:\n            self.freqs, self.hc, = np.squeeze(self.freqs), np.squeeze(self.hc)\n            self.fmrg, self.fpeak = self.fmrg[0], self.fpeak[0]\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if parameters are okay.", "response": "def _sanity_check(self):\n        \"\"\"Check if parameters are okay.\n\n        Sanity check makes sure each parameter is within an allowable range.\n\n        Raises:\n            ValueError: Problem with a specific parameter.\n\n        \"\"\"\n        if any(self.m1 < 0.0):\n            raise ValueError(\"Mass 1 is negative.\")\n        if any(self.m2 < 0.0):\n            raise ValueError(\"Mass 2 is negative.\")\n\n        if any(self.z <= 0.0):\n            raise ValueError(\"Redshift is zero or negative.\")\n\n        if any(self.dist <= 0.0):\n            raise ValueError(\"Distance is zero or negative.\")\n\n        if any(self.initial_point < 0.0):\n            raise ValueError(\"initial_point is negative.\")\n\n        if any(self.t_obs < 0.0):\n            raise ValueError(\"t_obs is negative.\")\n\n        if any(self.e0 <= 0.0):\n            raise ValueError(\"e0 must be greater than zero when using EccentricBinaries class.\")\n\n        if any(self.e0 > 1.0):\n            raise ValueError(\"e0 greater than 1.\")\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts units to geometrized units.", "response": "def _convert_units(self):\n        \"\"\"Convert units to geometrized units.\n\n        Change to G=c=1 (geometrized) units for ease in calculations.\n\n        \"\"\"\n        self.m1 = self.m1*M_sun*ct.G/ct.c**2\n        self.m2 = self.m2*M_sun*ct.G/ct.c**2\n        initial_cond_type_conversion = {\n            'time': ct.c*ct.Julian_year,\n            'frequency': 1./ct.c,\n            'separation': ct.parsec,\n        }\n\n        self.initial_point = self.initial_point*initial_cond_type_conversion[self.initial_cond_type]\n\n        self.t_obs = self.t_obs*ct.c*ct.Julian_year\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _t_of_e(self, a0=None, t_start=None, f0=None, ef=None, t_obs=5.0):\n        if ef is None:\n            ef = np.ones_like(self.e0)*0.0000001\n\n        beta = 64.0/5.0*self.m1*self.m2*(self.m1+self.m2)\n\n        e_vals = np.asarray([np.linspace(ef[i], self.e0[i], self.num_points)\n                            for i in range(len(self.e0))])\n        integrand = self._find_integrand(e_vals)\n        integral = np.asarray([np.trapz(integrand[:, i:], x=e_vals[:, i:])\n                              for i in range(e_vals.shape[1])]).T\n\n        if a0 is None and f0 is None:\n\n            a0 = (19./12.*t_start*beta*1/integral[:, 0])**(1./4.) * self._f_e(e_vals[:, -1])\n\n        elif a0 is None:\n            a0 = ((self.m1 + self.m2)/self.f0**2)**(1./3.)\n\n        c0 = self._c0_func(a0, self.e0)\n\n        a_vals = c0[:, np.newaxis]*self._f_e(e_vals)\n\n        delta_t = 12./19*c0[:, np.newaxis]**4/beta[:, np.newaxis]*integral\n\n        return e_vals, a_vals, delta_t", "response": "This function calculates the semi - major axis and eccentricity over time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _chirp_mass(self):\n        return (self.m1*self.m2)**(3./5.)/(self.m1+self.m2)**(1./5.)", "response": "Calculates the Chirp mass of the current set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _g_func(self):\n        return (self.n**4./32.\n                * ((jv(self.n-2., self.n*self.e_vals)\n                   - 2. * self.e_vals*jv(self.n-1., self.n*self.e_vals)\n                   + 2./self.n * jv(self.n, self.n*self.e_vals)\n                   + 2.*self.e_vals*jv(self.n+1., self.n*self.e_vals)\n                   - jv(self.n+2., self.n*self.e_vals))**2.\n                   + (1.-self.e_vals**2.) * (jv(self.n-2., self.n*self.e_vals)\n                   - 2.*jv(self.n, self.n*self.e_vals)\n                   + jv(self.n+2., self.n*self.e_vals))**2.\n                   + 4./(3.*self.n**2.)*(jv(self.n, self.n*self.e_vals))**2.))", "response": "Compute the G function for the current version of the logarithmic distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_waveforms(self):\n\n        # find eccentricity and semi major axis over time until e=0.\n        e_vals, a_vals, t_vals = self._t_of_e(a0=self.a0, f0=self.f0,\n                                              t_start=self.t_start, ef=None,\n                                              t_obs=self.t_obs)\n\n        f_mrg = 0.02/(self.m1 + self.m2)\n        a_mrg = ((self.m1+self.m2)/f_mrg**2)**(1/3)\n\n        # limit highest frequency to ISCO even though this is not innermost orbit for eccentric\n        # binaries\n        # find where binary goes farther than observation time or merger frequency limit.\n        a_ind_start = np.asarray([np.where(a_vals[i] > a_mrg[i])[0][0] for i in range(len(a_vals))])\n        t_ind_start = np.asarray([np.where(t_vals[i] < self.t_obs[i])[0][0]\n                                 for i in range(len(t_vals))])\n\n        ind_start = (a_ind_start*(a_ind_start >= t_ind_start)\n                     + t_ind_start*(a_ind_start < t_ind_start))\n\n        self.ef = np.asarray([e_vals[i][ind] for i, ind in enumerate(ind_start)])\n\n        # higher resolution over the eccentricities seen during observation\n        self.e_vals, self.a_vals, self.t_vals = self._t_of_e(a0=a_vals[:, -1],\n                                                             ef=self.ef,\n                                                             t_obs=self.t_obs)\n\n        self.freqs_orb = np.sqrt((self.m1[:, np.newaxis]+self.m2[:, np.newaxis])/self.a_vals**3)\n\n        # tile for efficient calculation across modes.\n        for attr in ['e_vals', 'a_vals', 't_vals', 'freqs_orb']:\n            arr = getattr(self, attr)\n            new_arr = (np.flip(\n                       np.tile(arr, self.n_max).reshape(len(arr)*self.n_max, len(arr[0])), -1))\n            setattr(self, attr, new_arr)\n\n        for attr in ['m1', 'm2', 'z', 'dist']:\n            arr = getattr(self, attr)\n            new_arr = np.repeat(arr, self.n_max)[:, np.newaxis]\n            setattr(self, attr, new_arr)\n\n        # setup modes\n        self.n = np.tile(np.arange(1, self.n_max + 1), self.length)[:, np.newaxis]\n\n        self._hcn_func()\n\n        # reshape hc\n        self.hc = self.hc.reshape(self.length, self.n_max, self.hc.shape[-1])\n        self.freqs = np.reshape(self.n*self.freqs_orb/(1+self.z)\n                                * ct.c,\n                                (self.length, self.n_max, self.freqs_orb.shape[-1]))\n\n        self.hc, self.freqs = np.squeeze(self.hc), np.squeeze(self.freqs)\n        return", "response": "Create the eccentric waveforms for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pyvolvePartitions(model, divselection=None):\n    codons = pyvolve.genetics.Genetics().codons\n    codon_dict = pyvolve.genetics.Genetics().codon_dict\n    pyrims = pyvolve.genetics.Genetics().pyrims\n    purines = pyvolve.genetics.Genetics().purines\n\n    if divselection:\n        (divomega, divsites) = divselection\n    else:\n        divsites = []\n\n    assert all([1 <= r <= model.nsites for r in divsites])\n\n    partitions = []\n    for r in range(model.nsites):\n        matrix = scipy.zeros((len(codons), len(codons)), dtype='float')\n        for (xi, x) in enumerate(codons):\n            for (yi, y) in enumerate(codons):\n                ntdiffs = [(x[j], y[j]) for j in range(3) if x[j] != y[j]]\n                if len(ntdiffs) == 1:\n                    (xnt, ynt) = ntdiffs[0]\n                    qxy = 1.0\n                    if (xnt in purines) == (ynt in purines):\n                        qxy *= model.kappa\n                    (xaa, yaa) = (codon_dict[x], codon_dict[y])\n                    fxy = 1.0\n                    if xaa != yaa:\n                        if type(model) == phydmslib.models.ExpCM_empirical_phi_divpressure:\n                            fxy *= model.omega * (1 + model.omega2 * model.deltar[r])\n                        elif r + 1 in divsites:\n                            fxy *= divomega\n                        else:\n                            fxy *= model.omega\n                    if type(model) in [phydmslib.models.ExpCM,\n                            phydmslib.models.ExpCM_empirical_phi, phydmslib.models.ExpCM_empirical_phi_divpressure]:\n                        qxy *= model.phi[NT_TO_INDEX[ynt]]\n                        pix = model.pi[r][AA_TO_INDEX[xaa]]**model.beta\n                        piy = model.pi[r][AA_TO_INDEX[yaa]]**model.beta\n                        if abs(pix - piy) > ALMOST_ZERO:\n                            fxy *= math.log(piy / pix) / (1.0 - pix / piy)\n                    elif type(model) == phydmslib.models.YNGKP_M0:\n                        for p in range(3):\n                            qxy *= model.phi[p][NT_TO_INDEX[y[p]]]\n                    else:\n                        raise ValueError(\"Can't handle model type {0}\".format(\n                                type(model)))\n                    matrix[xi][yi] = model.mu * qxy * fxy\n            matrix[xi][xi] = -matrix[xi].sum()\n\n        # create model in way that captures annoying print statements in pyvolve\n        old_stdout = sys.stdout\n        sys.stdout = open(os.devnull, 'w')\n        try:\n            m = pyvolve.Model(\"custom\", {\"matrix\":matrix})\n        finally:\n            sys.stdout.close()\n            sys.stdout = old_stdout\n        partitions.append(pyvolve.Partition(models=m, size=1))\n\n    return partitions", "response": "Returns a list of pyvolve. Partition objects for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsimulates an alignment given a model and a tree file.", "response": "def simulateAlignment(model, treeFile, alignmentPrefix, randomSeed=False):\n    \"\"\"\n    Simulate an alignment given a model and tree (units = subs/site).\n\n    Simulations done using `pyvolve`.\n\n    Args:\n        `model` (`phydmslib.models.Models` object)\n            The model used for the simulations. Only\n            models that can be passed to `pyvolve.Partitions`\n            are supported.\n        `treeFile` (str)\n            Name of newick file used to simulate the sequences.\n            The branch lengths should be in substitutions per site,\n            which is the default units for all `phydms` outputs.\n        `alignmentPrefix`\n            Prefix for the files created by `pyvolve`.\n\n    The result of this function is a simulated FASTA alignment\n    file with the name having the prefix giving by `alignmentPrefix`\n    and the suffix `'_simulatedalignment.fasta'`.\n    \"\"\"\n    if randomSeed == False:\n        pass\n    else:\n        random.seed(randomSeed)\n\n    #Transform the branch lengths by dividing by the model `branchScale`\n    tree = Bio.Phylo.read(treeFile, 'newick')\n    for node in tree.get_terminals() + tree.get_nonterminals():\n        if (node.branch_length == None) and (node == tree.root):\n            node.branch_length = 1e-06\n        else:\n            node.branch_length /= model.branchScale\n    fd, temp_path = mkstemp()\n    Bio.Phylo.write(tree, temp_path, 'newick')\n    os.close(fd)\n    pyvolve_tree = pyvolve.read_tree(file=temp_path)\n    os.remove(temp_path)\n\n\n    #Make the `pyvolve` partition\n    partitions = pyvolvePartitions(model)\n\n    #Simulate the alignment\n    alignment = '{0}_simulatedalignment.fasta'.format(alignmentPrefix)\n    info = '_temp_{0}info.txt'.format(alignmentPrefix)\n    rates = '_temp_{0}_ratefile.txt'.format(alignmentPrefix)\n    evolver = pyvolve.Evolver(partitions=partitions, tree=pyvolve_tree)\n    evolver(seqfile=alignment, infofile=info, ratefile=rates)\n    for f in [rates,info, \"custom_matrix_frequencies.txt\"]:\n        if os.path.isfile(f):\n            os.remove(f)\n    assert os.path.isfile(alignment)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_oracle(oracle_type, **kwargs):\n    if oracle_type == 'f':\n        return FO(**kwargs)\n    elif oracle_type == 'a':\n        return MO(**kwargs)\n    else:\n        return MO(**kwargs)", "response": "A routine for creating a factor oracle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the cumulative ir for the current state.", "response": "def _ir_cum2(self, alpha=1.0):\n        code, _ = self.encode()\n\n        N = self.n_states\n        BL = np.zeros(N - 1)  # BL is the block length of compror codewords\n\n        h0 = np.log2(np.cumsum(\n            [1.0 if sfx == 0 else 0.0 for sfx in self.sfx[1:]])\n        )\n        \"\"\"\n        h1 = np.array([h if m == 0 else h+np.log2(m) \n                       for h,m in zip(h0,self.lrs[1:])])\n        h1 = np.array([h if m == 0 else h+np.log2(m) \n                       for h,m in zip(h0,self.max_lrs[1:])])\n        h1 = np.array([h if m == 0 else h+np.log2(m) \n                       for h,m in zip(h0,self.avg_lrs[1:])])\n        \"\"\"\n        h1 = np.array([np.log2(i + 1) if m == 0 else np.log2(i + 1) + np.log2(m)\n                       for i, m in enumerate(self.max_lrs[1:])])\n\n        j = 0\n        for i in range(len(code)):\n            if self.code[i][0] == 0:\n                BL[j] = 1\n                j += 1\n            else:\n                L = code[i][0]\n                BL[j:j + L] = L  # range(1,L+1)\n                j = j + L\n\n        h1 = h1 / BL\n        ir = alpha * h0 - h1\n        ir[ir < 0] = 0  # Really a HACK here!!!!!\n        return ir, h0, h1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef accept(self, context):\n        _next = 0\n        for _s in context:\n            _data = [self.data[j] for j in self.trn[_next]]\n            if _s in _data:\n                _next = self.trn[_next][_data.index(_s)]\n            else:\n                return 0, _next\n        return 1, _next", "response": "Check if the context could be accepted by the oracle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_state(self, new_data, method='inc'):\n        self.sfx.append(0)\n        self.rsfx.append([])\n        self.trn.append([])\n        self.lrs.append(0)\n\n        # Experiment with pointer-based\n        self.f_array.add(new_data)\n\n        self.n_states += 1\n        i = self.n_states - 1\n\n        # assign new transition from state i-1 to i\n        self.trn[i - 1].append(i)\n        k = self.sfx[i - 1]\n        pi_1 = i - 1\n\n        # iteratively backtrack suffixes from state i-1\n        if method == 'inc':\n            suffix_candidate = 0\n        elif method == 'complete':\n            suffix_candidate = []\n        else:\n            suffix_candidate = 0\n\n        while k is not None:\n\n            if self.params['dfunc'] == 'other':\n                # dvec = self.dfunc_handle([new_data],\n                #                          self.f_array[self.trn[k]])[0]\n                dvec = dist.cdist([new_data],\n                                  self.f_array[self.trn[k]],\n                                  metric=self.params['dfunc_handle'])[0]\n            else:\n                dvec = dist.cdist([new_data],\n                                  self.f_array[self.trn[k]],\n                                  metric=self.params['dfunc'])[0]\n\n            I = np.where(dvec < self.params['threshold'])[0]\n            if len(I) == 0:  # if no transition from suffix\n                self.trn[k].append(i)  # Add new forward link to unvisited state\n                pi_1 = k\n                if method != 'complete':\n                    k = self.sfx[k]\n            else:\n                if method == 'inc':\n                    if I.shape[0] == 1:\n                        suffix_candidate = self.trn[k][I[0]]\n                    else:\n                        suffix_candidate = self.trn[k][I[np.argmin(dvec[I])]]\n                    break\n                elif method == 'complete':\n                    suffix_candidate.append((self.trn[k][I[np.argmin(dvec[I])]],\n                                             np.min(dvec)))\n                else:\n                    suffix_candidate = self.trn[k][I[np.argmin(dvec[I])]]\n                    break\n\n            if method == 'complete':\n                k = self.sfx[k]\n\n        if method == 'complete':\n            if not suffix_candidate:\n                self.sfx[i] = 0\n                self.lrs[i] = 0\n                self.latent.append([i])\n                self.data.append(len(self.latent) - 1)\n            else:\n                sorted_suffix_candidates = sorted(suffix_candidate,\n                                                  key=lambda suffix: suffix[1])\n                self.sfx[i] = sorted_suffix_candidates[0][0]\n                self.lrs[i] = self._len_common_suffix(pi_1, self.sfx[i] - 1) + 1\n                self.latent[self.data[self.sfx[i]]].append(i)\n                self.data.append(self.data[self.sfx[i]])\n        else:\n            if k is None:\n                self.sfx[i] = 0\n                self.lrs[i] = 0\n                self.latent.append([i])\n                self.data.append(len(self.latent) - 1)\n            else:\n                self.sfx[i] = suffix_candidate\n                self.lrs[i] = self._len_common_suffix(pi_1, self.sfx[i] - 1) + 1\n                self.latent[self.data[self.sfx[i]]].append(i)\n                self.data.append(self.data[self.sfx[i]])\n\n        # Temporary adjustment\n        k = self._find_better(i, self.data[i - self.lrs[i]])\n        if k is not None:\n            self.lrs[i] += 1\n            self.sfx[i] = k\n\n        self.rsfx[self.sfx[i]].append(i)\n\n        if self.lrs[i] > self.max_lrs[i - 1]:\n            self.max_lrs.append(self.lrs[i])\n        else:\n            self.max_lrs.append(self.max_lrs[i - 1])\n\n        self.avg_lrs.append(self.avg_lrs[i - 1] * ((i - 1.0) / (self.n_states - 1.0)) +\n                            self.lrs[i] * (1.0 / (self.n_states - 1.0)))", "response": "Add new state and update related links and compressed state"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rotate(self, atom):\n        try:\n            index = self.atoms.index(atom)\n        except ValueError:\n            raise CycleError(\"atom %s not in cycle\"%(atom))\n\n        self.atoms = self.atoms[index:] + self.atoms[:index]\n        self.bonds = self.bonds[index:] + self.bonds[:index]", "response": "rotate the cycle at the given atom"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the cycle to be an aromatic ring", "response": "def set_aromatic(self):\n        \"\"\"set the cycle to be an aromatic ring\"\"\"\n        #XXX FIX ME\n        # this probably shouldn't be here\n        for atom in self.atoms:\n            atom.aromatic = 1\n            \n        for bond in self.bonds:\n            bond.aromatic = 1\n            bond.bondorder = 1.5\n            bond.bondtype = 4\n            bond.symbol = \":\"\n            bond.fixed = 1\n\n        self.aromatic = 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_debug(enabled: bool):\n    global _DEBUG_ENABLED\n\n    if not enabled:\n        log('Disabling debug output...', logger_name=_LOGGER_NAME)\n        _DEBUG_ENABLED = False\n    else:\n        _DEBUG_ENABLED = True\n        log('Enabling debug output...', logger_name=_LOGGER_NAME)", "response": "Enable or disable debug logs for the entire package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a context - based logger.", "response": "def _create_logger(name: str, level: int) -> Generator[logging.Logger, None, None]:\n    \"\"\"Create a context-based logger.\n    Parameters\n    ----------\n    name: str\n        Name of logger to use when logging.\n    level: int\n        Logging level, one of logging's levels (e.g. INFO, ERROR, etc.).\n\n    Returns\n    -------\n    logging.Logger\n        Named logger that may be used for logging.\n    \"\"\"\n    # Get logger\n    logger = logging.getLogger(name)\n\n    # Set logger level\n    old_level = logger.level\n    logger.setLevel(level)\n\n    # Setup handler and add to logger\n    handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter('%(asctime)s %(levelname)-5s [%(name)s]: %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    yield logger\n\n    # Reset logger level\n    logger.setLevel(old_level)\n\n    # Remove handler from logger\n    logger.removeHandler(handler)\n    handler.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log(message: str, *args: str, category: str='info', logger_name: str='pgevents'):\n    global _DEBUG_ENABLED\n\n    if _DEBUG_ENABLED:\n        level = logging.INFO\n    else:\n        level = logging.CRITICAL + 1\n\n    with _create_logger(logger_name, level) as logger:\n        log_fn = getattr(logger, category, None)\n        if log_fn is None:\n            raise ValueError('Invalid log category \"{}\"'.format(category))\n\n        log_fn(message, *args)", "response": "Log a message to the given logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the molecule. cycles that contain the smallest set of smallest rings", "response": "def sssr(molecule):\n    \"\"\"molecule -> generate the molecule.cycles that contain\n    the smallest set of smallest rings\"\"\"\n    results = {}\n    lookup = {}\n    fullSet = {}\n    oatoms = {}\n    # XXX FIX ME\n    # copy atom.oatoms to atom._oatoms\n    # atom._oatoms will be modified my the routine\n\n    for atom in molecule.atoms:\n        atom.rings = []\n\tfullSet[atom.handle] = 1\n\tlookup[atom.handle] = atom\n        oatoms[atom.handle] = atom.oatoms[:]\n\n    for bond in molecule.bonds:\n        bond.rings = []\n        \n    trimSet = []\n\n    while fullSet:\n\tnodesN2 = []\n        minimum, minimum_degree = None, 100000\n\n\n\t# find the N2 atoms and remove atoms with degree 0\n\tfor atomID in fullSet.keys():\n\t    atom = lookup[atomID]\n\t    degree = len(oatoms[atom.handle])\n\t    if degree == 0:\n\t\tdel fullSet[atomID]\n\t\t#fullSet.remove(atomID)\n\t    elif degree == 2:\n\t\tnodesN2.append(atom)\n\n\t    # keep track of the minimum degree\n\t    if (degree > 0) and ( (not minimum) or \n\t\t                (degree < minimum_degree)):\n\t\tminimum, minimum_degree = atom, degree\n\n\tif not minimum:\n\t    # nothing to do!  (i.e. can't have a ring)\n\t    break\n\n\tif minimum_degree == 1:\n\t    # these cannot be in rings so trim and remove\n\t    # my version of trimming\n            for oatom in oatoms[minimum.handle]:\n\t\toatoms[oatom.handle].remove(minimum)\n            oatoms[minimum.handle] = []\n\t    del fullSet[minimum.handle]\n\n\telif minimum_degree == 2:\n\t    # find the rings!\n\t    startNodes = []\n\t    for atom in nodesN2:\n\t\tring, bonds = getRing(atom, fullSet, lookup, oatoms)\n\n\t\tif ring:\n                    rlookup = ring[:]\n                    rlookup.sort()\n                    rlookup = tuple(rlookup)\n\t\t    if (not results.has_key(rlookup)):# not in results):\n\t\t\tresults[rlookup] = ring, bonds\n\t\t\tstartNodes.append(atom)\n\n\t    # in case we didn't get a ring remove the head of the nodesN2\n\t    startNodes = startNodes or [nodesN2[0]]\n\t    for atom in startNodes:\n\t\t# again, my version of trimming\n                if oatoms[atom.handle]:\n\t\t    oatom = oatoms[atom.handle].pop()\n\t\t    oatoms[oatom.handle].remove(atom)\n\n\telif minimum_degree > 2:\n\t    # no N2 nodes so remove the \"optimum\" edge to create\n\t    # N2 nodes in the next go-around.\n\t    ring, bonds = getRing(minimum, fullSet, lookup, oatoms)\n            if ring:\n                key = ring[:]\n                key.sort()\n                key = tuple(key)\n                if not results.has_key(key):\n                    results[key] = ring, bonds\n                    atoms = map(lookup.get, ring)\n                    atoms, bonds = toposort(atoms, bonds)\n                    checkEdges(atoms, lookup, oatoms)\n            else:\n                del fullSet[minimum.handle]\n \telse:\n\t    raise ShouldntGetHereError\n\n    # assign the ring index to the atom\n    rings = []\n    index = 0\n\n    # transform the handles back to atoms\n    for result, bonds in results.values():\n\tring = []\n        for atomID in result:\n\t    atom = lookup[atomID]\n            assert atom.handle == atomID\n\t    ring.append(atom)\n\trings.append((ring, bonds))\n        index = index + 1\n\n    molecule.rings = rings\n    potentialCycles = []\n    index = 0\n    for atoms, bonds in rings:\n        # due to the dictionaries used in getRing\n        # the atoms are not in the order found\n        # we need to topologically sort these\n        # for the cycle\n        atoms, bonds = toposort(atoms, bonds)\n        potentialCycles.append((atoms, bonds))\n\n    rings = potentialCycles#checkRings(potentialCycles)\n    molecule.rings = rings\n    molecule.cycles = [Cycle(atoms, bonds) for atoms, bonds in rings]\n    return molecule"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toposort(initialAtoms, initialBonds):\n    atoms = []\n    a_append = atoms.append\n    bonds = []\n    b_append = bonds.append\n\n    # for the atom and bond hashes\n    # we ignore the first atom since we\n    # would have deleted it from the hash anyway\n    ahash = {}\n    bhash = {}\n    for atom in initialAtoms[1:]:\n        ahash[atom.handle] = 1\n        \n    for bond in initialBonds:\n        bhash[bond.handle] = bond\n\n    next = initialAtoms[0]\n    a_append(next)\n\n    # do until all the atoms are gone\n    while ahash:\n        # traverse to all the connected atoms\n        for atom in next.oatoms:\n            # both the bond and the atom have to be\n            # in our list of atoms and bonds to use\n            # ugg, nested if's...  There has to be a\n            # better control structure\n            if ahash.has_key(atom.handle):\n                bond = next.findbond(atom)\n                assert bond\n                # but wait! the bond has to be in our\n                # list of bonds we can use!\n                if bhash.has_key(bond.handle):\n                    a_append(atom)\n                    b_append(bond)\n                    del ahash[atom.handle]\n                    next = atom\n                    break\n        else:\n            raise RingException(\"Atoms are not in ring\")\n\n    assert len(initialAtoms) == len(atoms)\n    assert len(bonds) == len(atoms) - 1\n    lastBond = atoms[0].findbond(atoms[-1])\n    assert lastBond\n    b_append(lastBond)\n    return atoms, bonds", "response": "Return the topologically sorted atoms and bonds in a tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getRing(startAtom, atomSet, lookup, oatoms):\n\n    path = {}\n    bpaths = {}\n    for atomID in atomSet.keys():\n\t# initially the paths are empty\n\tpath[atomID] = None\n        bpaths[atomID] = []\n    \n    q = []\n    handle = startAtom.handle\n    for atom in oatoms[handle]:\n\tq.append((atom, handle))\n        path[atom.handle] = {atom.handle:1, handle:1}\n        bpaths[atom.handle] = [startAtom.findbond(atom)]\n            \n    qIndex = 0\n    lenQ = len(q)\n\n    while qIndex < lenQ:\t\n\tcurrent, sourceHandle = q[qIndex]\n        handle = current.handle\n\tqIndex += 1\n\n        for next in oatoms[handle]:\n\t    m = next.handle\n\n\t    if m != sourceHandle:\n\t\tif not atomSet.has_key(m):\n\t\t    return (), ()\n                \n\t\tif path.get(m, None):\n\t\t    intersections = 0\n\t\t    for atom in path[handle].keys():\n\t\t\tif path[m].has_key(atom):\n\t\t\t    intersections = intersections + 1\n\t\t\t    sharedAtom = atom\n\n\t\t    if intersections == 1:\n\t\t\tdel path[handle][sharedAtom]\n\t\t\tpath[handle].update(path[m])\n\t\t\tresult = path[handle].keys()\n                        bond = next.findbond(current)\n                        # assert bond not in bpaths[handle] and bond not in bpaths[m]\n                        bonds = bpaths[handle] + bpaths[m] + [bond]\n\t\t\treturn result, bonds\n\t\telse:\n\t\t    path[m] = path[handle].copy()\n\t\t    path[m][m] = 1\n                    bond = next.findbond(current)\n                    # assert bond not in bpaths[m] and bond not in bpaths[handle]\n                    bpaths[m] = bpaths[handle] + [next.findbond(current)]\n\t\t    q.append((next, handle))\n\t\t    lenQ = lenQ + 1\n\n    return (), ()", "response": "getRing - returns the smallest ring found at startAtom"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checkEdges(ringSet, lookup, oatoms):\n    bondedAtoms = map( None, ringSet[:-1], ringSet[1:] )\n    bondedAtoms += [ (ringSet[-1], ringSet[0]) ]\n\n    # form a lookup for the ringSet list\n    atomSet = {}\n    for atomID in ringSet:\n\tatomSet[atomID] = 1\n    results = []\n    \n    # for each bond in the ring, break it and find the smallest\n    # rings starting on either side of the bond\n    # keep the largest but rememeber to add the bond back at the\n    # end\n    for atom1, atom2 in bondedAtoms:\n        # break a single edge in the ring\n        handle1 = atom1.handle\n        handle2 = atom2.handle\n        oatoms1 = oatoms[handle1]\n        oatoms2 = oatoms[handle2]\n        index1 = oatoms1.index(atom2)\n        index2 = oatoms2.index(atom1)\n\n        # break the bond\n        del oatoms1[index1]\n\tdel oatoms2[index2]\n\n\tring1 = getRing(atom1, atomSet, lookup, oatoms)\n\tring2 = getRing(atom2, atomSet, lookup, oatoms)\n\t\n\t# keep the larger of the two rings\n\tif len(ring1) > len(ring2):\n\t    results.append((len(ring1),\n                            handle1, handle2,\n                            ring1))\n\telse:\n    \t    results.append((len(ring2),\n                            handle2, handle1,\n                            ring2))\n\n        # retie the bond\n        oatoms1.insert(index1, atom2)\n        oatoms2.insert(index2, atom1)\n\n\n    if not results:\n\treturn None\n\n    #  find the smallest ring\n    size, incidentHandle, adjacentHandle, smallestRing = min(results)\n    # dereference the handles\n    incident, adjacent = lookup[incidentHandle], lookup[adjacentHandle]\n\n    # break the bond between the incident and adjacent atoms\n    oatomsI = oatoms[incidentHandle]\n    oatomsA = oatoms[adjacentHandle]\n    assert incident in oatomsA\n    assert adjacent in oatomsI\n    \n    oatomsI.remove(adjacent)\n    oatomsA.remove(incident)", "response": "check the edges of a ring and return the largest non - N2 node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns n if n is non - negative integer otherwise an error.", "response": "def NonNegativeInt(n):\n    \"\"\"If *n* is non-negative integer returns it, otherwise an error.\n\n    >>> print(\"%d\" % NonNegativeInt('8'))\n    8\n\n    >>> NonNegativeInt('8.1')\n    Traceback (most recent call last):\n       ...\n    ValueError: 8.1 is not an integer\n\n    >>> print(\"%d\" % NonNegativeInt('0'))\n    0\n\n    >>> NonNegativeInt('-1')\n    Traceback (most recent call last):\n       ...\n    ValueError: -1 is not non-negative\n\n    \"\"\"\n    if not isinstance(n, str):\n        raise ValueError('%r is not a string' % n)\n    try:\n       n = int(n)\n    except:\n        raise ValueError('%s is not an integer' % n)\n    if n < 0:\n        raise ValueError('%d is not non-negative' % n)\n    else:\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef IntGreaterThanZero(n):\n    try:\n        n = int(n)\n    except:\n        raise ValueError(\"%s is not an integer\" % n)\n    if n <= 0:\n        raise ValueError(\"%d is not > 0\" % n)\n    else:\n        return n", "response": "Returns n if n is an integer > 0 otherwise an error."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef IntGreaterThanOne(n):\n    try:\n        n = int(n)\n    except:\n        raise ValueError(\"%s is not an integer\" % n)\n    if n <= 1:\n        raise ValueError(\"%d is not > 1\" % n)\n    else:\n        return n", "response": "Returns n if n is an integer > 1 otherwise an error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef FloatGreaterThanEqualToZero(x):\n    try:\n        x = float(x)\n    except:\n        raise ValueError(\"%r not float greater than or equal to zero\" % x)\n    if x >= 0:\n        return x\n    else:\n        raise ValueError(\"%r not float greater than or equal to zero\" % x)", "response": "Returns a base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base - 2 base base"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef FloatBetweenZeroAndOne(x):\n    x = float(x)\n    if 0 <= x <= 1:\n        return x\n    else:\n        raise ValueError(\"{0} not a float between 0 and 1.\".format(x))", "response": "Returns x if x is a float between 0 and 1."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse priorstring and returns tuple of prior", "response": "def diffPrefsPrior(priorstring):\n    \"\"\"Parses `priorstring` and returns `prior` tuple.\"\"\"\n    assert isinstance(priorstring, str)\n    prior = priorstring.split(',')\n    if len(prior) == 3 and prior[0] == 'invquadratic':\n        [c1, c2] = [float(x) for x in prior[1 : ]]\n        assert c1 > 0 and c2 > 0, \"C1 and C2 must be > 1 for invquadratic prior\"\n        return ('invquadratic', c1, c2)\n    else:\n        raise ValueError(\"Invalid diffprefsprior: {0}\".format(priorstring))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ExistingFileOrNone(fname):\n    if os.path.isfile(fname):\n        return fname\n    elif fname.lower() == 'none':\n        return None\n    else:\n        raise ValueError(\"%s must specify a valid file name or 'None'\" % fname)", "response": "Like Existingfile but returns None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ModelOption(model):\n    yngkpmatch = re.compile('^YNGKP_M[{0}]$'.format(''.join([m[1 : ] for m in yngkp_modelvariants])))\n    if yngkpmatch.search(model):\n        return model\n    elif len(model) > 6 and model[ : 6] == 'ExpCM_':\n        fname = model[6 : ]\n        if os.path.isfile(fname):\n            return ('ExpCM', fname)\n        else:\n            raise ValueError(\"ExpCM_ must be followed by the name of an existing file. You specified the following, which is not an existing file: %s\" % fname)\n    else:\n        raise ValueError(\"Invalid model\")", "response": "Returns the string if it specifies a YNGKP_ model variant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a parser for the PhyDMSPrepAlignment command line.", "response": "def PhyDMSPrepAlignmentParser():\n    \"\"\"Returns *argparse.ArgumentParser* for ``phydms_prepalignment``.\"\"\"\n    parser = ArgumentParserNoArgHelp(formatter_class=ArgumentDefaultsRawDescriptionFormatter,\n            description='\\n'.join([\n            \"Prepare alignment of protein-coding DNA sequences.\\n\",\n            \"Steps:\",\n            \" * Any sequences specified by '--purgeseqs' are removed.\",\n            \" * Sequences not of length divisible by 3 are removed.\",\n            \" * Sequences with ambiguous nucleotides are removed.\",\n            \" * Sequences with non-terminal stop codons are removed;\",\n            \"   terminal stop codons are trimmed.\",\n            \" * Sequences that do not encode unique proteins are removed\",\n            \"   unless they are specified for retention by '--keepseqs'.\",\n            \" * A multiple sequence alignment is built using MAFFT.\",\n            \"   This step is skipped if you specify '--prealigned'.\",\n            \" * Sites gapped in reference sequence are stripped.\",\n            \" * Sequences with too little protein identity to reference\",\n            \"   sequence are removed, counting both mismatches and unstripped\",\n            \"   gaps as differences. Identity cutoff set by '--minidentity'.\",\n            \" * Sequences too similar to other sequences are removed. An\",\n            \"   effort is made to keep one representative of sequences found\",\n            \"   many times in input set. Uniqueness threshold set \",\n            \"   by '--minuniqueness'. You can specify sequences to not\",\n            \"   remove via '--keepseqs'.\",\n            \" * Problematic characters in header names are replaced by\",\n            \"   underscores. This is any space, comma, colon, semicolon\",\n            \"   parenthesis, bracket, single quote, or double quote.\",\n            \" * An alignment is written, as well as a plot with same root\",\n            \"   but extension '.pdf' that shows divergence from reference\",\n            \"   of all sequences retained and purged due to identity or\",\n            \"   uniqueness.\\n\",\n            phydmslib.__acknowledgments__,\n            'Version {0}'.format(phydmslib.__version__),\n            'Full documentation at {0}'.format(phydmslib.__url__),\n            ]))\n    parser.add_argument('inseqs', type=ExistingFile, help=\"FASTA file giving input coding sequences.\")\n    parser.add_argument('alignment', help=\"Name of created output FASTA alignment. PDF plot has same root, but extension '.pdf'.\")\n    parser.add_argument('refseq', help=\"Reference sequence in 'inseqs': specify substring found ONLY in header for that sequence.\")\n    parser.set_defaults(prealigned=False)\n    parser.add_argument('--prealigned', action='store_true', dest='prealigned', help=\"Sequences in 'inseqs' are already aligned, do NOT re-align.\")\n    parser.add_argument('--mafft', help=\"Path to MAFFT (http://mafft.cbrc.jp/alignment/software/).\", default='mafft')\n    parser.add_argument('--minidentity', type=FloatBetweenZeroAndOne, help=\"Purge sequences with <= this protein identity to 'refseq'.\", default=0.7)\n    parser.add_argument('--minuniqueness', type=IntGreaterThanZero, default=2, help=\"Require each sequence to have >= this many protein differences relative to other sequences.\")\n    parser.add_argument('--purgeseqs', nargs='*', help=\"Specify sequences to always purge. Any sequences with any of the substrings specified here are always removed. The substrings can either be passed as repeated arguments here, or as the name of an existing file which has one substring per line.\")\n    parser.add_argument('--keepseqs', nargs='*', help=\"Do not purge any of these sequences for lack of identity or uniqueness. Specified in the same fashion as for '--purgeseqs'.\")\n    parser.add_argument('-v', '--version', action='version', version='%(prog)s {version}'.format(version=phydmslib.__version__))\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an argument parser for the PhyDMS logo plot.", "response": "def PhyDMSLogoPlotParser():\n    \"\"\"Returns `argparse.ArgumentParser` for ``phydms_logoplot``.\"\"\"\n    parser = ArgumentParserNoArgHelp(description=\n            \"Make logo plot of preferences or differential preferences. \"\n            \"Uses weblogo (http://weblogo.threeplusone.com/). \"\n            \"{0} Version {1}. Full documentation at {2}\".format(\n            phydmslib.__acknowledgments__,\n            phydmslib.__version__, phydmslib.__url__),\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('--prefs', type=ExistingFile, help=\"File with \"\n            \"amino-acid preferences; same format as input to 'phydms'.\")\n    group.add_argument('--diffprefs', type=ExistingFile, help=\"File with \"\n            \"differential preferences; in format output by 'phydms'.\")\n    parser.add_argument('outfile', help='Name of created PDF logo plot.')\n    parser.add_argument('--stringency', type=FloatGreaterThanEqualToZero,\n            default=1, help=\"Stringency parameter to re-scale prefs.\")\n    parser.add_argument('--nperline', type=IntGreaterThanZero, default=70,\n            help=\"Number of sites per line.\")\n    parser.add_argument('--numberevery', type=IntGreaterThanZero, default=10,\n            help=\"Number sites at this interval.\")\n    parser.add_argument('--mapmetric', default='functionalgroup', choices=['kd',\n            'mw', 'charge', 'functionalgroup'], help='Metric used to color '\n            'amino-acid letters. kd = Kyte-Doolittle hydrophobicity; '\n            'mw = molecular weight; functionalgroup = divide in 7 '\n            'groups; charge = charge at neutral pH.')\n    parser.add_argument('--colormap', type=str, default='jet',\n            help=\"Name of `matplotlib` color map for amino acids \"\n            \"when `--mapmetric` is 'kd' or 'mw'.\")\n    parser.add_argument('--diffprefheight', type=FloatGreaterThanZero,\n            default=1.0, help=\"Height of diffpref logo in each direction.\")\n    parser.add_argument('--omegabysite', help=\"Overlay omega on \"\n            \"logo plot. Specify '*_omegabysite.txt' file from 'phydms'.\",\n            type=ExistingFileOrNone)\n    parser.add_argument('--minP', type=FloatGreaterThanZero, default=1e-4,\n            help=\"Min plotted P-value for '--omegabysite' overlay.\")\n    parser.add_argument('-v', '--version', action='version',\n            version='%(prog)s {version}'.format(version=phydmslib.__version__))\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef PhyDMSComprehensiveParser():\n    parser = ArgumentParserNoArgHelp(description=(\"Comprehensive phylogenetic \"\n            \"model comparison and detection of selection informed by deep \"\n            \"mutational scanning data. This program runs 'phydms' repeatedly \"\n            \"to compare substitution models and detect selection. \"\n            \"{0} Version {1}. Full documentation at {2}\").format(\n            phydmslib.__acknowledgments__, phydmslib.__version__,\n            phydmslib.__url__),\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('outprefix', help='Output file prefix.', type=str)\n    parser.add_argument('alignment', help='Existing FASTA file with aligned '\n            'codon sequences.', type=ExistingFile)\n    parser.add_argument('prefsfiles', help='Existing files with site-specific '\n            'amino-acid preferences.', type=ExistingFile, nargs='+')\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('--raxml', help=\"Path to RAxML (e.g., 'raxml')\")\n    group.add_argument('--tree', type=ExistingFile,\n             help=\"Existing Newick file giving input tree.\")\n    parser.add_argument('--ncpus', default=-1, help='Use this many CPUs; -1 '\n            'means all available.', type=int)\n    parser.add_argument('--brlen', choices=['scale', 'optimize'],\n            default='optimize', help=(\"How to handle branch lengths: \"\n            \"scale by single parameter or optimize each one\"))\n    parser.set_defaults(omegabysite=False)\n    parser.add_argument('--omegabysite', dest='omegabysite',\n            action='store_true', help=\"Fit omega (dN/dS) for each site.\")\n    parser.set_defaults(diffprefsbysite=False)\n    parser.add_argument('--diffprefsbysite', dest='diffprefsbysite',\n            action='store_true', help=\"Fit differential preferences for \"\n            \"each site.\")\n    parser.set_defaults(gammaomega=False)\n    parser.add_argument('--gammaomega', dest='gammaomega', action=\\\n            'store_true', help=\"Fit ExpCM with gamma distributed omega.\")\n    parser.set_defaults(gammabeta=False)\n    parser.add_argument('--gammabeta', dest='gammabeta', action=\\\n            'store_true', help=\"Fit ExpCM with gamma distributed beta.\")\n    parser.set_defaults(noavgprefs=False)\n    parser.add_argument('--no-avgprefs', dest='noavgprefs', action='store_true',\n            help=\"No fitting of models with preferences averaged across sites \"\n            \"for ExpCM.\")\n    parser.set_defaults(randprefs=False)\n    parser.add_argument('--randprefs', dest='randprefs', action='store_true',\n            help=\"Include ExpCM models with randomized preferences.\")\n    parser.add_argument('-v', '--version', action='version', version=\n            '%(prog)s {version}'.format(version=phydmslib.__version__))\n    return parser", "response": "Returns a parser for PhyDMS - Comprehensive Phylogenetic Sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a parser for the Phylogenetic analysis.", "response": "def PhyDMSParser():\n    \"\"\"Returns *argparse.ArgumentParser* for ``phydms`` script.\"\"\"\n    parser = ArgumentParserNoArgHelp(description=('Phylogenetic analysis '\n            'informed by deep mutational scanning data. {0} Version {1}. Full'\n            ' documentation at {2}').format(phydmslib.__acknowledgments__,\n            phydmslib.__version__, phydmslib.__url__),\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('alignment', type=ExistingFile,\n            help='Existing FASTA file with aligned codon sequences.')\n    parser.add_argument('tree', type=ExistingFile,\n            help=\"Existing Newick file giving input tree.\")\n    parser.add_argument('model', type=ModelOption,\n            help=(\"Substitution model: ExpCM_<prefsfile> or YNGKP_<m> (\"\n            \"where <m> is {0}). For ExpCM, <prefsfile> has first \"\n            \"column labeled 'site' and others labeled by 1-letter \"\n            \"amino-acid code.\").format(', '.join(yngkp_modelvariants)))\n    parser.add_argument('outprefix', help='Output file prefix.', type=str)\n    parser.add_argument('--brlen', choices=['scale', 'optimize'],\n            default='optimize', help=(\"How to handle branch lengths: \"\n            \"scale by single parameter or optimize each one\"))\n    parser.set_defaults(gammaomega=False)\n    parser.add_argument('--gammaomega', action='store_true',\n            dest='gammaomega', help=\"Omega for ExpCM from gamma \"\n            \"distribution rather than single value. To achieve \"\n            \"same for YNGKP, use 'model' of YNGKP_M5.\")\n    parser.set_defaults(gammabeta=False)\n    parser.add_argument('--gammabeta', action='store_true',\n            dest='gammabeta', help=\"Beta for ExpCM from gamma \"\n            \"distribution rather than single value.\")\n    parser.set_defaults(omegabysite=False)\n    parser.add_argument('--omegabysite', dest='omegabysite',\n            action='store_true', help=\"Fit omega (dN/dS) for each site.\")\n    parser.set_defaults(omegabysite_fixsyn=False)\n    parser.add_argument('--omegabysite_fixsyn', dest='omegabysite_fixsyn',\n            action='store_true', help=\"For '--omegabysite', assign all \"\n            \"sites same dS rather than fit for each site.\")\n    parser.set_defaults(diffprefsbysite=False)\n    parser.add_argument('--diffprefsbysite', dest='diffprefsbysite',\n            action='store_true', help=\"Fit differential preferences \"\n            \"for each site.\")\n    parser.add_argument('--diffprefsprior', default='invquadratic,150,0.5',\n            type=diffPrefsPrior, help=\"Regularizing prior for \"\n            \"'--diffprefsbysite': 'invquadratic,C1,C2' is prior in \"\n            \"Bloom, Biology Direct, 12:1.\")\n    parser.set_defaults(fitphi=False)\n    parser.add_argument('--fitphi', action='store_true', dest='fitphi',\n            help='Fit ExpCM phi rather than setting so stationary '\n            'state matches alignment frequencies.')\n    parser.set_defaults(randprefs=False)\n    parser.add_argument('--randprefs', dest='randprefs', action='store_true',\n            help=\"Randomize preferences among sites for ExpCM.\")\n    parser.set_defaults(avgprefs=False)\n    parser.add_argument('--avgprefs', dest='avgprefs', action='store_true',\n            help=\"Average preferences across sites for ExpCM.\")\n    parser.add_argument('--divpressure', type=ExistingFileOrNone,\n            help=(\"Known diversifying pressure at sites: file with column 1 \"\n            \"= position, column 2 = diversification pressure; columns space-, \"\n            \"tab-, or comma-delimited.\"))\n    parser.add_argument('--ncpus', default=1, type=int,\n            help='Use this many CPUs; -1 means all available.')\n    parser.add_argument('--fitprefsmethod', choices=[1, 2], default=2,\n            help='Implementation to we use when fitting prefs.', type=int)\n    parser.add_argument('--ncats', default=4, type=IntGreaterThanOne,\n            help='Number of categories for gamma-distribution.')\n    parser.add_argument('--minbrlen', type=FloatGreaterThanZero,\n            default=phydmslib.constants.ALMOST_ZERO,\n            help=\"Adjust all branch lengths in starting 'tree' to >= this.\")\n    parser.add_argument('--minpref', default=0.002, type=FloatGreaterThanZero,\n            help=\"Adjust all preferences in ExpCM 'prefsfile' to >= this.\")\n    parser.add_argument('--seed', type=int, default=1, help=\"Random number seed.\")\n    parser.add_argument('--initparams', type=ExistingFile, help=\"Initialize \"\n            \"model params from this file, which should be format of \"\n            \"'*_modelparams.txt' file created by 'phydms' with this model.\")\n    parser.set_defaults(profile=False)\n    parser.add_argument('--profile', dest='profile', action='store_true',\n            help=\"Profile likelihood maximization, write pstats files. \"\n            \"For code-development purposes.\")\n    parser.set_defaults(opt_details=False)\n    parser.add_argument('--opt_details', dest='opt_details',\n            action='store_true', help='Print details about optimization')\n    parser.set_defaults(nograd=False)\n    parser.add_argument('--nograd', dest='nograd', action='store_true',\n            help=\"Do not use gradients for likelihood maximization.\")\n    parser.add_argument('-v', '--version', action='version', version=(\n            ('%(prog)s {version}'.format(version=phydmslib.__version__))))\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint error message then help.", "response": "def error(self, message):\n        \"\"\"Prints error message, then help.\"\"\"\n        sys.stderr.write('error: %s\\n\\n' % message)\n        self.print_help()\n        sys.exit(2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef symmetrize(self, method=None, copy=False):\n    '''Symmetrizes (ignores method). Returns a copy if copy=True.'''\n    if copy:\n      return SymmEdgePairGraph(self._pairs.copy(),\n                               num_vertices=self._num_vertices)\n    shape = (self._num_vertices, self._num_vertices)\n    flat_inds = np.union1d(np.ravel_multi_index(self._pairs.T, shape),\n                           np.ravel_multi_index(self._pairs.T[::-1], shape))\n    self._pairs = np.transpose(np.unravel_index(flat_inds, shape))\n    return self", "response": "Returns a copy of the graph with a copy if copy = True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all from - > to edges.", "response": "def remove_edges(self, from_idx, to_idx, symmetric=False, copy=False):\n    '''Removes all from->to and to->from edges.\n    Note: the symmetric kwarg is unused.'''\n    flat_inds = self._pairs.dot((self._num_vertices, 1))\n    # convert to sorted order and flatten\n    to_remove = (np.minimum(from_idx, to_idx) * self._num_vertices\n                 + np.maximum(from_idx, to_idx))\n    mask = np.in1d(flat_inds, to_remove, invert=True)\n    res = self.copy() if copy else self\n    res._pairs = res._pairs[mask]\n    res._offdiag_mask = res._offdiag_mask[mask]\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_edges(self, from_idx, to_idx, weight=1, symmetric=False, copy=False):\n    '''Adds all from->to edges. weight may be a scalar or 1d array.\n    If symmetric=True, also adds to->from edges with the same weights.'''\n    raise NotImplementedError()", "response": "Adds all from - > to edges."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding all i - > i edges.", "response": "def add_self_edges(self, weight=None, copy=False):\n    '''Adds all i->i edges. weight may be a scalar or 1d array.'''\n    ii = np.arange(self.num_vertices())\n    return self.add_edges(ii, ii, weight=weight, symmetric=False, copy=copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reweight(self, weight, edges=None, copy=False):\n    '''Replaces existing edge weights. weight may be a scalar or 1d array.\n    edges is a mask or index array that specifies a subset of edges to modify'''\n    if not self.is_weighted():\n      warnings.warn('Cannot supply weights for unweighted graph; '\n                    'ignoring call to reweight')\n      return self\n    if edges is None:\n      return self._update_edges(weight, copy=copy)\n    ii, jj = self.pairs()[edges].T\n    return self.add_edges(ii, jj, weight=weight, symmetric=False, copy=copy)", "response": "Replaces existing edge weights."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reweight_by_distance(self, coords, metric='l2', copy=False):\n    '''Replaces existing edge weights by distances between connected vertices.\n    The new weight of edge (i,j) is given by: metric(coords[i], coords[j]).\n    coords : (num_vertices x d) array of coordinates, in vertex order\n    metric : str or callable, see sklearn.metrics.pairwise.paired_distances'''\n    if not self.is_weighted():\n      warnings.warn('Cannot supply weights for unweighted graph; '\n                    'ignoring call to reweight_by_distance')\n      return self\n    # TODO: take advantage of symmetry of metric function\n    ii, jj = self.pairs().T\n    if metric == 'precomputed':\n      assert coords.ndim == 2 and coords.shape[0] == coords.shape[1]\n      d = coords[ii,jj]\n    else:\n      d = paired_distances(coords[ii], coords[jj], metric=metric)\n    return self._update_edges(d, copy=copy)", "response": "Replaces existing edge weights by distances between connected vertices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an array of vertex degrees.", "response": "def degree(self, kind='out', weighted=True):\n    '''Returns an array of vertex degrees.\n    kind : either 'in' or 'out', useful for directed graphs\n    weighted : controls whether to count edges or sum their weights\n    '''\n    if kind == 'out':\n      axis = 1\n      adj = self.matrix('dense', 'csc')\n    else:\n      axis = 0\n      adj = self.matrix('dense', 'csr')\n\n    if not weighted and self.is_weighted():\n      # With recent numpy and a dense matrix, could do:\n      # d = np.count_nonzero(adj, axis=axis)\n      d = (adj!=0).sum(axis=axis)\n    else:\n      d = adj.sum(axis=axis)\n    return np.asarray(d).ravel()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting this Graph object to an igraph - compatible object. Requires the python - igraph library.", "response": "def to_igraph(self, weighted=None):\n    '''Converts this Graph object to an igraph-compatible object.\n    Requires the python-igraph library.'''\n    # Import here to avoid ImportErrors when igraph isn't available.\n    import igraph\n    ig = igraph.Graph(n=self.num_vertices(), edges=self.pairs().tolist(),\n                      directed=self.is_directed())\n    if weighted is not False and self.is_weighted():\n      ig.es['weight'] = self.edge_weights()\n    return ig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_graph_tool(self):\n    '''Converts this Graph object to a graph_tool-compatible object.\n    Requires the graph_tool library.\n    Note that the internal ordering of graph_tool seems to be column-major.'''\n    # Import here to avoid ImportErrors when graph_tool isn't available.\n    import graph_tool\n    gt = graph_tool.Graph(directed=self.is_directed())\n    gt.add_edge_list(self.pairs())\n    if self.is_weighted():\n      weights = gt.new_edge_property('double')\n      for e,w in zip(gt.edges(), self.edge_weights()):\n        weights[e] = w\n      gt.edge_properties['weight'] = weights\n    return gt", "response": "Converts this Graph object to a graph_tool - compatible object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts this Graph object to a networkx - compatible object.", "response": "def to_networkx(self, directed=None):\n    '''Converts this Graph object to a networkx-compatible object.\n    Requires the networkx library.'''\n    import networkx as nx\n    directed = directed if directed is not None else self.is_directed()\n    cls = nx.DiGraph if directed else nx.Graph\n    adj = self.matrix()\n    if ss.issparse(adj):\n      return nx.from_scipy_sparse_matrix(adj, create_using=cls())\n    return nx.from_numpy_matrix(adj, create_using=cls())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_inputs(z, m):\n    try:\n        nz = len(z)\n        z = np.array(z)\n    except TypeError:\n        z = np.array([z])\n        nz = len(z)\n    try:\n        nm = len(m)\n        m = np.array(m)\n    except TypeError:\n        m = np.array([m])\n        nm = len(m)\n\n    if (z < 0).any() or (m < 0).any():\n        raise ValueError('z and m must be positive')\n\n    if nz != nm and nz > 1 and nm > 1:\n        raise ValueError('z and m arrays must be either equal in length, \\\n                          OR of different length with one of length 1.')\n\n    else:\n        if type(z) != np.ndarray:\n            z = np.array(z)\n        if type(m) != np.ndarray:\n            m = np.array(m)\n\n        return z, m", "response": "Check inputs are arrays of same length or array and a scalar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the concentration of a given c ( M ) relation published in Prada et al. 2001.", "response": "def c_Prada(z, m, h=h, Om_M=Om_M, Om_L=Om_L):\n    \"\"\"Concentration from c(M) relation published in Prada et al. (2012).\n\n    Parameters\n    ----------\n    z : float or array_like\n        Redshift(s) of halos.\n    m : float or array_like\n        Mass(es) of halos (m200 definition), in units of solar masses.\n    h : float, optional\n        Hubble parameter. Default is from Planck13.\n    Om_M : float, optional\n        Matter density parameter. Default is from Planck13.\n    Om_L : float, optional\n        Cosmological constant density parameter. Default is from Planck13.\n\n    Returns\n    ----------\n    ndarray\n        Concentration values (c200) for halos.\n\n    Notes\n    ----------\n    This c(M) relation is somewhat controversial, due to its upturn in\n    concentration for high masses (normally we expect concentration to\n    decrease with increasing mass). See the reference below for discussion.\n\n    References\n    ----------\n    Calculation based on results of N-body simulations presented in:\n\n    F. Prada, A.A. Klypin, A.J. Cuesta, J.E. Betancort-Rijo, and J.\n    Primack, \"Halo concentrations in the standard Lambda cold dark matter\n    cosmology,\" Monthly Notices of the Royal Astronomical Society, Volume\n    423, Issue 4, pp. 3018-3030, 2012.\n    \"\"\"\n\n    z, m = _check_inputs(z, m)\n\n    # EQ 13\n    x = (1. / (1. + z)) * (Om_L / Om_M)**(1. / 3.)\n\n    # EQ 12\n    intEQ12 = np.zeros(len(x))  # integral\n    for i in range(len(x)):\n        # v is integration variable\n        temp = integrate.quad(lambda v: (v / (1 + v**3.))**(1.5), 0, x[i])\n        intEQ12[i] = temp[0]\n\n    Da = 2.5 * ((Om_M / Om_L)**(1. / 3.)) * (np.sqrt(1. + x**3.) /\n                                             (x**(1.5))) * intEQ12\n\n    # EQ 23\n    y = (1.e+12) / (h * m)\n    sigma = Da * (16.9 * y**0.41) / (1. + (1.102 * y**0.2) + (6.22 * y**0.333))\n\n    # EQ 21 & 22 (constants)\n    c0 = 3.681\n    c1 = 5.033\n    alpha = 6.948\n    x0 = 0.424\n    s0 = 1.047  # sigma_0^-1\n    s1 = 1.646  # sigma_1^-1\n    beta = 7.386\n    x1 = 0.526\n\n    # EQ 19 & 20\n    cmin = c0 + (c1 - c0) * ((1. / np.pi) * np.arctan(alpha * (x - x0)) + 0.5)\n    smin = s0 + (s1 - s0) * ((1. / np.pi) * np.arctan(beta * (x - x1)) + 0.5)\n\n    # EQ 18\n    cmin1393 = c0 + (c1 - c0) * ((1. / np.pi) * np.arctan(alpha *\n                                                          (1.393 - x0)) + 0.5)\n    smin1393 = s0 + (s1 - s0) * ((1. / np.pi) * np.arctan(beta *\n                                                          (1.393 - x1)) + 0.5)\n    B0 = cmin / cmin1393\n    B1 = smin / smin1393\n\n    # EQ 15\n    sigma_prime = B1 * sigma\n\n    # EQ 17\n    A = 2.881\n    b = 1.257\n    c = 1.022\n    d = 0.06\n\n    # EQ 16\n    Cs = A * ((sigma_prime / b)**c + 1.) * np.exp(d / (sigma_prime**2.))\n\n    # EQ 14\n    concentration = B0 * Cs\n\n    return concentration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef c_DuttonMaccio(z, m, h=h):\n\n    z, m = _check_inputs(z, m)\n\n    a = 0.52 + 0.385 * np.exp(-0.617 * (z**1.21))  # EQ 10\n    b = -0.101 + 0.026 * z                         # EQ 11\n\n    logc200 = a + b * np.log10(m * h / (10.**12))  # EQ 7\n\n    concentration = 10.**logc200\n\n    return concentration", "response": "Concentration from c ( M relation in Dutton & Maccio."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef c_Duffy(z, m, h=h):\n\n    z, m = _check_inputs(z, m)\n\n    M_pivot = 2.e12 / h  # [M_solar]\n\n    A = 5.71\n    B = -0.084\n    C = -0.47\n\n    concentration = A * ((m / M_pivot)**B) * (1 + z)**C\n\n    return concentration", "response": "Concentration from c ( M relation published in Duffy et al. 2008."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_axis_limits(self, which, lims, d, scale, reverse=False):\n        setattr(self.limits, which + 'lims', lims)\n        setattr(self.limits, 'd' + which, d)\n        setattr(self.limits, which + 'scale', scale)\n\n        if reverse:\n            setattr(self.limits, 'reverse_' + which + '_axis', True)\n        return", "response": "Private method to set the limits on each axis for an individual plot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_xlim(self, xlims, dx, xscale, reverse=False):\n        self._set_axis_limits('x', xlims, dx, xscale, reverse)\n        return", "response": "Set the limits for the x axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the limits for the y axis.", "response": "def set_ylim(self, xlims, dx, xscale, reverse=False):\n        \"\"\"Set y limits for plot.\n\n        This will set the limits for the y axis\n        for the specific plot.\n\n        Args:\n            ylims (len-2 list of floats): The limits for the axis.\n            dy (float): Amount to increment by between the limits.\n            yscale (str): Scale of the axis. Either `log` or `lin`.\n            reverse (bool, optional): If True, reverse the axis tick marks. Default is False.\n\n        \"\"\"\n        self._set_axis_limits('y', xlims, dx, xscale, reverse)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a legend to the current figure.", "response": "def add_legend(self, labels=None, **kwargs):\n        \"\"\"Specify legend for a plot.\n\n        Adds labels and basic legend specifications for specific plot.\n\n        For the optional Args, refer to\n        https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html\n        for more information.\n\n        # TODO: Add legend capabilities for Loss/Gain plots. This is possible\n            using the return_fig_ax kwarg in the main plotting function.\n\n        Args:\n            labels (list of str): String representing each item in plot that\n                will be added to the legend.\n\n        Keyword Arguments:\n            loc (str, int, len-2 list of floats, optional): Location of\n                legend. See matplotlib documentation for more detail.\n                Default is None.\n            bbox_to_anchor (2-tuple or 4-tuple of floats, optional): Specify\n                position and size of legend box. 2-tuple will specify (x,y)\n                coordinate of part of box specified with `loc` kwarg.\n                4-tuple will specify (x, y, width, height). See matplotlib\n                documentation for more detail.\n                Default is None.\n            size (float, optional): Set size of legend using call to `prop`\n                dict in legend call. See matplotlib documentaiton for more\n                detail. Default is None.\n            ncol (int, optional): Number of columns in the legend.\n            Note: Other kwargs are available. See:\n                https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html\n\n        \"\"\"\n        if 'size' in kwargs:\n            if 'prop' not in kwargs:\n                kwargs['prop'] = {'size': kwargs['size']}\n            else:\n                kwargs['prop']['size'] = kwargs['size']\n            del kwargs['size']\n        self.legend.add_legend = True\n        self.legend.legend_labels = labels\n        self.legend.legend_kwargs = kwargs\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a dataset to a specific plot. This method adds a dataset to a plot. Its functional use is imperative to the plot generation. It handles adding new files as well as indexing to files that are added to other plots. All Args default to None. However, these are note the defaults in the code. See DataImportContainer attributes for defaults in code. Args: name (str, optional): Name (path) for file. Required if reading from a file (at least one). Required if file_name is not in \"general\". Must be \".txt\" or \".hdf5\". Can include path from working directory. label (str, optional): Column label in the dataset corresponding to desired SNR value. Required if reading from a file (at least one). x_column_label/y_column_label (str, optional): Column label from input file identifying x/y values. This can override setting in \"general\". Default is `x`/`y`. index (int, optional): Index of plot with preloaded data. Required if not loading a file. control (bool, optional): If True, this dataset is set to the control. This is needed for Ratio plots. It sets the baseline. Default is False. Raises: ValueError: If no options are passes. This means no file indication nor index.", "response": "def add_dataset(self, name=None, label=None,\n                    x_column_label=None, y_column_label=None, index=None, control=False):\n        \"\"\"Add a dataset to a specific plot.\n\n        This method adds a dataset to a plot. Its functional use is imperative\n        to the plot generation. It handles adding new files as well\n        as indexing to files that are added to other plots.\n\n        All Args default to None. However, these are note the defaults\n        in the code. See DataImportContainer attributes for defaults in code.\n\n        Args:\n            name (str, optional): Name (path) for file.\n                Required if reading from a file (at least one).\n                Required if file_name is not in \"general\". Must be \".txt\" or \".hdf5\".\n                Can include path from working directory.\n            label (str, optional): Column label in the dataset corresponding to desired SNR value.\n                Required if reading from a file (at least one).\n            x_column_label/y_column_label (str, optional): Column label from input file identifying\n                x/y values. This can override setting in \"general\". Default\n                is `x`/`y`.\n            index (int, optional): Index of plot with preloaded data.\n                Required if not loading a file.\n            control (bool, optional): If True, this dataset is set to the control.\n                This is needed for Ratio plots. It sets\n                the baseline. Default is False.\n\n        Raises:\n            ValueError: If no options are passes. This means no file indication\n                nor index.\n\n        \"\"\"\n        if name is None and label is None and index is None:\n            raise ValueError(\"Attempting to add a dataset without\"\n                             + \"supplying index or file information.\")\n\n        if index is None:\n            trans_dict = DataImportContainer()\n            if name is not None:\n                trans_dict.file_name = name\n\n            if label is not None:\n                trans_dict.label = label\n\n            if x_column_label is not None:\n                trans_dict.x_column_label = x_column_label\n\n            if y_column_label is not None:\n                trans_dict.y_column_label = y_column_label\n\n            if control:\n                self.control = trans_dict\n            else:\n                # need to append file to file list.\n                if 'file' not in self.__dict__:\n                    self.file = []\n                self.file.append(trans_dict)\n        else:\n            if control:\n                self.control = DataImportContainer()\n                self.control.index = index\n\n            else:\n                # need to append index to index list.\n                if 'indices' not in self.__dict__:\n                    self.indices = []\n\n                self.indices.append(index)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a figure during generation.", "response": "def savefig(self, output_path, **kwargs):\n        \"\"\"Save figure during generation.\n\n        This method is used to save a completed figure during the main function run.\n        It represents a call to ``matplotlib.pyplot.fig.savefig``.\n\n        # TODO: Switch to kwargs for matplotlib.pyplot.savefig\n\n        Args:\n            output_path (str): Relative path to the WORKING_DIRECTORY to save the figure.\n\n        Keyword Arguments:\n            dpi (int, optional): Dots per inch of figure. Default is 200.\n            Note: Other kwargs are available. See:\n                https://matplotlib.org/api/_as_gen/matplotlib.pyplot.savefig.html\n\n        \"\"\"\n        self.figure.save_figure = True\n        self.figure.output_path = output_path\n        self.figure.savefig_kwargs = kwargs\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_fig_size(self, width, height=None):\n        self.figure.figure_width = width\n        self.figure.figure_height = height\n        return", "response": "Set the figure size in inches."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the figure spacing.", "response": "def set_spacing(self, space):\n        \"\"\"Set the figure spacing.\n\n        Sets whether in general there is space between subplots.\n        If all axes are shared, this can be `tight`. Default in code is `wide`.\n\n        The main difference is the tick labels extend to the ends if space==`wide`.\n        If space==`tight`, the edge tick labels are cut off for clearity.\n\n        Args:\n            space (str): Sets spacing for subplots. Either `wide` or `tight`.\n\n        \"\"\"\n        self.figure.spacing = space\n        if 'subplots_adjust_kwargs' not in self.figure.__dict__:\n            self.figure.subplots_adjust_kwargs = {}\n        if space == 'wide':\n            self.figure.subplots_adjust_kwargs['hspace'] = 0.3\n            self.figure.subplots_adjust_kwargs['wspace'] = 0.3\n        else:\n            self.figure.subplots_adjust_kwargs['hspace'] = 0.0\n            self.figure.subplots_adjust_kwargs['wspace'] = 0.0\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subplots_adjust(self, **kwargs):\n        prop_default = {\n            'bottom': 0.1,\n            'top': 0.85,\n            'right': 0.9,\n            'left': 0.12,\n            'hspace': 0.3,\n            'wspace': 0.3,\n        }\n\n        if 'subplots_adjust_kwargs' in self.figure.__dict__:\n            for key, value in self.figure.subplots_adjust_kwargs.items():\n                prop_default[key] = value\n\n        for prop, default in prop_default.items():\n            kwargs[prop] = kwargs.get(prop, default)\n\n        self.figure.subplots_adjust_kwargs = kwargs\n        return", "response": "Adjust subplot spacing and dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting label for x axis on overall figure.", "response": "def set_fig_x_label(self, xlabel, **kwargs):\n        \"\"\"Set overall figure x.\n\n        Set label for x axis on overall figure. This is not for a specific plot.\n        It will place the label on the figure at the left with a call to ``fig.text``.\n\n        Args:\n            xlabel (str): xlabel for entire figure.\n\n        Keyword Arguments:\n            x/y (float, optional): The x/y location of the text in figure coordinates.\n                Defaults are 0.01 for x and 0.51 for y.\n            horizontalalignment/ha (str, optional): The horizontal alignment of\n                the text relative to (x, y). Optionas are 'center', 'left', or 'right'.\n                Default is 'center'.\n            verticalalignment/va (str, optional): The vertical alignment of the text\n                relative to (x, y). Optionas are 'top', 'center', 'bottom',\n                or 'baseline'. Default is 'center'.\n            fontsize/size (int): The font size of the text. Default is 20.\n            rotation (float or str): Rotation of label. Options are angle in degrees,\n                `horizontal`, or `vertical`. Default is `vertical`.\n            Note: Other kwargs are available.\n                See https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.figtext\n\n        \"\"\"\n        prop_default = {\n            'x': 0.01,\n            'y': 0.51,\n            'fontsize': 20,\n            'rotation': 'vertical',\n            'va': 'center',\n        }\n\n        for prop, default in prop_default.items():\n            kwargs[prop] = kwargs.get(prop, default)\n\n        self._set_fig_label('x', xlabel, **kwargs)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets label for y axis on overall figure.", "response": "def set_fig_y_label(self, ylabel, **kwargs):\n        \"\"\"Set overall figure y.\n\n        Set label for y axis on overall figure. This is not for a specific plot.\n        It will place the label on the figure at the left with a call to ``fig.text``.\n\n        Args:\n            ylabel (str): ylabel for entire figure.\n\n        Keyword Arguments:\n            x/y (float, optional): The x/y location of the text in figure coordinates.\n                Defaults are 0.45 for x and 0.02 for y.\n            horizontalalignment/ha (str, optional): The horizontal alignment of\n                the text relative to (x, y). Optionas are 'center', 'left', or 'right'.\n                Default is 'center'.\n            verticalalignment/va (str, optional): The vertical alignment of the text\n                relative to (x, y). Optionas are 'top', 'center', 'bottom',\n                or 'baseline'. Default is 'top'.\n            fontsize/size (int): The font size of the text. Default is 20.\n            rotation (float or str): Rotation of label. Options are angle in degrees,\n                `horizontal`, or `vertical`. Default is `horizontal`.\n            Note: Other kwargs are available.\n                See https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.figtext\n\n        \"\"\"\n        prop_default = {\n            'x': 0.45,\n            'y': 0.02,\n            'fontsize': 20,\n            'rotation': 'horizontal',\n            'ha': 'center',\n        }\n\n        for prop, default in prop_default.items():\n            kwargs[prop] = kwargs.get(prop, default)\n\n        self._set_fig_label('y', ylabel, **kwargs)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_fig_title(self, title, **kwargs):\n        prop_default = {\n            'fontsize': 20,\n        }\n\n        for prop, default in prop_default.items():\n            kwargs[prop] = kwargs.get(prop, default)\n\n        self.figure.fig_title = title\n        self.figure.fig_title_kwargs = kwargs\n        return", "response": "Set overall figure title."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_colorbar(self, plot_type, **kwargs):\n        prop_default = {\n            'cbar_label': None,\n            'cbar_ticks_fontsize': 15,\n            'cbar_label_fontsize': 20,\n            'cbar_axes': [],\n            'cbar_ticks': [],\n            'cbar_tick_labels': [],\n            'cbar_pos': 'use_default',\n            'cbar_label_pad': None,\n        }\n\n        for prop, default in prop_default.items():\n            kwargs[prop] = kwargs.get(prop[5:], default)\n\n            if prop[5:] in kwargs:\n                del kwargs[prop[5:]]\n\n        if 'colorbars' not in self.figure.__dict__:\n            self.figure.colorbars = {}\n\n        self.figure.colorbars[plot_type] = kwargs\n        return", "response": "Setup colorbar for specific type of plot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_all_file_column_labels(self, xlabel=None, ylabel=None):\n        if xlabel is not None:\n            self.general.x_column_label = xlabel\n        if ylabel is not None:\n            self.general.y_column_label = ylabel\n        if xlabel is None and ylabel is None:\n            warnings.warn(\"is not specifying x or y lables even\"\n                          + \"though column labels function is called.\", UserWarning)\n        return", "response": "This function sets the x and y column labels into the data files for all plots."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset limits and ticks for all the items in the current object.", "response": "def _set_all_lims(self, which, lim, d, scale, fontsize=None):\n        \"\"\"Set limits and ticks for an axis for whole figure.\n\n        This will set axis limits and tick marks for the entire figure.\n        It can be overridden in the SinglePlot class.\n\n        Args:\n            which (str): The indicator of which part of the plots\n                to adjust. This currently handles `x` and `y`.\n            lim (len-2 list of floats): The limits for the axis.\n            d (float): Amount to increment by between the limits.\n            scale (str): Scale of the axis. Either `log` or `lin`.\n            fontsize (int, optional): Set fontsize for associated axis tick marks.\n                Default is None.\n\n        \"\"\"\n\n        setattr(self.general, which + 'lims', lim)\n        setattr(self.general, 'd' + which, d)\n        setattr(self.general, which + 'scale', scale)\n\n        if fontsize is not None:\n            setattr(self.general, which + '_tick_label_fontsize', fontsize)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets limits and ticks for all of the items in the current figure.", "response": "def set_all_xlims(self, xlim, dx, xscale, fontsize=None):\n        \"\"\"Set limits and ticks for x axis for whole figure.\n\n        This will set x axis limits and tick marks for the entire figure.\n        It can be overridden in the SinglePlot class.\n\n        Args:\n            xlim (len-2 list of floats): The limits for the axis.\n            dx (float): Amount to increment by between the limits.\n            xscale (str): Scale of the axis. Either `log` or `lin`.\n            fontsize (int, optional): Set fontsize for x axis tick marks.\n                Default is None.\n\n        \"\"\"\n        self._set_all_lims('x', xlim, dx, xscale, fontsize)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_all_ylims(self, ylim, dy, yscale, fontsize=None):\n        self._set_all_lims('y', ylim, dy, yscale, fontsize)\n        return", "response": "Set limits and ticks for all of the elements in the y axis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreverse an axis in all figure plots.", "response": "def reverse_axis(self, axis_to_reverse):\n        \"\"\"Reverse an axis in all figure plots.\n\n        This will reverse the tick marks on an axis for each plot in the figure.\n        It can be overridden in SinglePlot class.\n\n        Args:\n            axis_to_reverse (str): Axis to reverse. Supports `x` and `y`.\n\n        Raises:\n            ValueError: The string representing the axis to reverse is not `x` or `y`.\n\n        \"\"\"\n        if axis_to_reverse.lower() == 'x':\n            self.general.reverse_x_axis = True\n        if axis_to_reverse.lower() == 'y':\n            self.general.reverse_y_axis = True\n        if axis_to_reverse.lower() != 'x' or axis_to_reverse.lower() != 'y':\n            raise ValueError('Axis for reversing needs to be either x or y.')\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef return_dict(self):\n        output_dict = {}\n        output_dict['general'] = self._iterate_through_class(self.general.__dict__)\n        output_dict['figure'] = self._iterate_through_class(self.figure.__dict__)\n\n        if self.total_plots > 1:\n            trans_dict = ({\n                           str(i): self._iterate_through_class(axis.__dict__) for i, axis\n                          in enumerate(self.ax)})\n            output_dict['plot_info'] = trans_dict\n\n        else:\n            output_dict['plot_info'] = {'0': self._iterate_through_class(self.ax.__dict__)}\n\n        if self.print_input:\n            print(output_dict)\n        return output_dict", "response": "Returns a dictionary for the main container class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread in txt files.", "response": "def txt_read_in(self):\n        \"\"\"Read in txt files.\n\n        Method for reading in text or csv files. This uses ascii class from astropy.io\n        for flexible input. It is slower than numpy, but has greater flexibility with less input.\n\n        \"\"\"\n\n        # read in\n        data = ascii.read(self.WORKING_DIRECTORY + '/' + self.file_name)\n\n        # find number of distinct x and y points.\n        num_x_pts = len(np.unique(data[self.x_column_label]))\n        num_y_pts = len(np.unique(data[self.y_column_label]))\n\n        # create 2D arrays of x,y,z\n        self.xvals = np.reshape(np.asarray(data[self.x_column_label]), (num_y_pts, num_x_pts))\n        self.yvals = np.reshape(np.asarray(data[self.y_column_label]), (num_y_pts, num_x_pts))\n        self.zvals = np.reshape(np.asarray(data[self.z_column_label]), (num_y_pts, num_x_pts))\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_parameters(self):\n\n        # declare 1D arrays of both paramters\n        if self.xscale != 'lin':\n            self.xvals = np.logspace(np.log10(float(self.x_low)),\n                                     np.log10(float(self.x_high)),\n                                     self.num_x)\n\n        else:\n            self.xvals = np.linspace(float(self.x_low),\n                                     float(self.x_high),\n                                     self.num_x)\n\n        if self.yscale != 'lin':\n            self.yvals = np.logspace(np.log10(float(self.y_low)),\n                                     np.log10(float(self.y_high)),\n                                     self.num_y)\n\n        else:\n            self.yvals = np.linspace(float(self.y_low),\n                                     float(self.y_high),\n                                     self.num_y)\n\n        self.xvals, self.yvals = np.meshgrid(self.xvals, self.yvals)\n        self.xvals, self.yvals = self.xvals.ravel(), self.yvals.ravel()\n\n        for which in ['x', 'y']:\n            setattr(self, getattr(self, which + 'val_name'), getattr(self, which + 'vals'))\n\n        self.ecc = 'eccentricity' in self.__dict__\n        if self.ecc:\n            if 'observation_time' not in self.__dict__:\n                if 'start_time' not in self.__dict__:\n                    raise ValueError('If no observation time is provided, the time before'\n                                     + 'merger must be the inital starting condition.')\n                self.observation_time = self.start_time  # small number so it is not zero\n        else:\n            if 'spin' in self.__dict__:\n                self.spin_1 = self.spin\n                self.spin_2 = self.spin\n\n        for key in ['redshift', 'luminosity_distance', 'comoving_distance']:\n            if key in self.__dict__:\n                self.dist_type = key\n                self.z_or_dist = getattr(self, key)\n\n            if self.ecc:\n                for key in ['start_frequency', 'start_time', 'start_separation']:\n                    if key in self.__dict__:\n                        self.initial_cond_type = key.split('_')[-1]\n                        self.initial_point = getattr(self, key)\n\n        # add m1 and m2\n        self.m1 = (self.total_mass / (1. + self.mass_ratio))\n        self.m2 = (self.total_mass * self.mass_ratio / (1. + self.mass_ratio))\n        return", "response": "Setup all the parameters for the binaries that are used by the Airflow and the associated parameters for the associated SNR function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the snr calculation.", "response": "def run_snr(self):\n        \"\"\"Run the snr calculation.\n\n        Takes results from ``self.set_parameters`` and other inputs and inputs these\n        into the snr calculator.\n\n        \"\"\"\n\n        if self.ecc:\n            required_kwargs = {'dist_type': self.dist_type,\n                               'initial_cond_type': self.initial_cond_type,\n                               'ecc': True}\n            input_args = [self.m1, self.m2, self.z_or_dist, self.initial_point,\n                          self.eccentricity, self.observation_time]\n\n        else:\n            required_kwargs = {'dist_type': self.dist_type}\n            input_args = [self.m1, self.m2, self.spin_1, self.spin_2,\n                          self.z_or_dist, self.start_time, self.end_time]\n\n        input_kwargs = {**required_kwargs,\n                        **self.general,\n                        **self.sensitivity_input,\n                        **self.snr_input,\n                        **self.parallel_input}\n\n        self.final_dict = snr(*input_args, **input_kwargs)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef event_payment(self, date, time, pid, commerce_id, transaction_id, request_ip, token, webpay_server):\n        '''Record the payment event\n\n        Official handler writes this information to TBK_EVN%Y%m%d file.\n        '''\n        raise NotImplementedError(\"Logging Handler must implement event_payment\")", "response": "Record the payment event in the TBK_EVN%Y%m%d file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef event_confirmation(self, date, time, pid, commerce_id, transaction_id, request_ip, order_id):\n        '''Record the confirmation event.\n\n        Official handler writes this information to TBK_EVN%Y%m%d file.\n        '''\n        raise NotImplementedError(\"Logging Handler must implement event_confirmation\")", "response": "Record the confirmation event."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop(self):\n        res = self.send_request('manager/stop', post=True)\n\n        if res.status_code != 200:\n            raise UnexpectedResponse(\n                'Attempted to stop manager. {res_code}: {res_text}'.format(\n                    res_code=res.status_code,\n                    res_text=res.text,\n                )\n            )\n\n        if settings.VERBOSITY >= verbosity.PROCESS_STOP:\n            print('Stopped {}'.format(self.get_name()))\n\n        # The request will end just before the process stops, so there is a tiny\n        # possibility of a race condition. We delay as a precaution so that we\n        # can be reasonably confident of the system's state.\n        time.sleep(0.05)", "response": "If the manager is running, tell it to stop its process"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop a managed host specified by config_file.", "response": "def stop_host(self, config_file):\n        \"\"\"\n        Stops a managed host specified by `config_file`.\n        \"\"\"\n        res = self.send_json_request('host/stop', data={'config': config_file})\n\n        if res.status_code != 200:\n            raise UnexpectedResponse(\n                'Attempted to stop a JSHost. Response: {res_code}: {res_text}'.format(\n                    res_code=res.status_code,\n                    res_text=res.text,\n                )\n            )\n\n        return res.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes the request and return the deserialided JSON from the response", "response": "def get_data(self, params={}):\n        \"\"\"\n        Make the request and return the deserialided JSON from the response\n\n        :param params: Dictionary mapping (string) query parameters to values\n        :type params: dict\n        :return: JSON object with the data fetched from that URL as a\n                 JSON-format object.\n        :rtype: (dict or array)\n\n        \"\"\"\n        if self and hasattr(self, 'proxies') and self.proxies is not None:\n            response = requests.request('GET',\n                                        url=PVWatts.PVWATTS_QUERY_URL,\n                                        params=params,\n                                        headers={'User-Agent': ''.join(\n                                                 ['pypvwatts/', VERSION,\n                                                  ' (Python)'])},\n                                        proxies=self.proxies)\n        else:\n            response = requests.request('GET',\n                                        url=PVWatts.PVWATTS_QUERY_URL,\n                                        params=params,\n                                        headers={'User-Agent': ''.join(\n                                                 ['pypvwatts/', VERSION,\n                                                  ' (Python)'])})\n\n        if response.status_code == 403:\n            raise PVWattsError(\"Forbidden, 403\")\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive(self, input):\n        if IRichInput.providedBy(input):\n            richInput = unicode(input)\n            symbolInput = unicode(input.symbol())\n        else:\n            richInput = None\n            symbolInput = unicode(input)\n\n        action = LOG_FSM_TRANSITION(\n            self.logger,\n            fsm_identifier=self.identifier,\n            fsm_state=unicode(self.state),\n            fsm_rich_input=richInput,\n            fsm_input=symbolInput)\n\n        with action as theAction:\n            output = super(FiniteStateLogger, self).receive(input)\n            theAction.addSuccessFields(\n                fsm_next_state=unicode(self.state), fsm_output=[unicode(o) for o in output])\n\n        if self._action is not None and self._isTerminal(self.state):\n            self._action.addSuccessFields(\n                fsm_terminal_state=unicode(self.state))\n            self._action.finish()\n            self._action = None\n\n        return output", "response": "Add logging of state transitions to the wrapped state machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smce_graph(X, metric='l2', sparsity_param=10, kmax=None, keep_ratio=0.95):\r\n  '''Sparse graph construction from the SMCE paper.\r\n\r\n  X : 2-dimensional array-like\r\n  metric : str, optional\r\n  sparsity_param : float, optional\r\n  kmax : int, optional\r\n  keep_ratio : float, optional\r\n    When <1, keep edges up to (keep_ratio * total weight)\r\n\r\n  Returns a graph with asymmetric similarity weights.\r\n  Call .symmetrize() and .kernelize('rbf') to convert to symmetric distances.\r\n\r\n  SMCE: \"Sparse Manifold Clustering and Embedding\"\r\n    Elhamifar & Vidal, NIPS 2011\r\n  '''\r\n  n = X.shape[0]\r\n  if kmax is None:\r\n    kmax = min(n-1, max(5, n // 10))\r\n\r\n  nn_dists, nn_inds = nearest_neighbors(X, metric=metric, k=kmax+1,\r\n                                        return_dists=True)\r\n  W = np.zeros((n, n))\r\n\r\n  # optimize each point separately\r\n  for i, pt in enumerate(X):\r\n    nbr_inds = nn_inds[i]\r\n    mask = nbr_inds != i  # remove self-edge\r\n    nbr_inds = nbr_inds[mask]\r\n    nbr_dist = nn_dists[i,mask]\r\n    Y = (X[nbr_inds] - pt) / nbr_dist[:,None]\r\n    # solve sparse optimization with ADMM\r\n    c = _solve_admm(Y, nbr_dist/nbr_dist.sum(), sparsity_param)\r\n    c = np.abs(c / nbr_dist)\r\n    W[i,nbr_inds] = c / c.sum()\r\n\r\n  W = ss.csr_matrix(W)\r\n  if keep_ratio < 1:\r\n    for i in range(n):\r\n      row_data = W.data[W.indptr[i]:W.indptr[i+1]]\r\n      order = np.argsort(row_data)[::-1]\r\n      stop_idx = np.searchsorted(np.cumsum(row_data[order]), keep_ratio) + 1\r\n      bad_inds = order[stop_idx:]\r\n      row_data[bad_inds] = 0\r\n    W.eliminate_zeros()\r\n\r\n  return Graph.from_adj_matrix(W)", "response": "Construct a sparse graph from the SMCE paper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sparse_regularized_graph(X, positive=False, sparsity_param=None, kmax=None):\r\n  '''Sparse Regularized Graph Construction, commonly known as an l1-graph.\r\n\r\n  positive : bool, optional\r\n    When True, computes the Sparse Probability Graph (SPG).\r\n  sparsity_param : float, optional\r\n    Controls sparsity cost in the LASSO optimization.\r\n    When None, uses cross-validation to find sparsity parameters.\r\n    This is very slow, but it gets good results.\r\n  kmax : int, optional\r\n    When None, allow all points to be edges. Otherwise, restrict to kNN set.\r\n\r\n  l1-graph: \"Semi-supervised Learning by Sparse Representation\"\r\n    Yan & Wang, SDM 2009\r\n    http://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.68\r\n\r\n  SPG: \"Nonnegative Sparse Coding for Discriminative Semi-supervised Learning\"\r\n    He et al., CVPR 2001\r\n  '''\r\n  clf, X = _l1_graph_setup(X, positive, sparsity_param)\r\n  if kmax is None:\r\n    W = _l1_graph_solve_full(clf, X)\r\n  else:\r\n    W = _l1_graph_solve_k(clf, X, kmax)\r\n  return Graph.from_adj_matrix(W)", "response": "Commonly known as an l1 - graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prep_parallel(self, binary_args, other_args):\n        if self.length < 100:\n            raise Exception(\"Run this across 1 processor by setting num_processors kwarg to None.\")\n        if self.num_processors == -1:\n            self.num_processors = mp.cpu_count()\n\n        split_val = int(np.ceil(self.length/self.num_splits))\n        split_inds = [self.num_splits*i for i in np.arange(1, split_val)]\n\n        inds_split_all = np.split(np.arange(self.length), split_inds)\n\n        self.args = []\n        for i, ind_split in enumerate(inds_split_all):\n            trans_args = []\n            for arg in binary_args:\n                try:\n                    trans_args.append(arg[ind_split])\n                except TypeError:\n                    trans_args.append(arg)\n\n            self.args.append((i, tuple(trans_args)) + other_args)\n        return", "response": "Prepare the arguments to be run in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_parallel(self, para_func):\n        if self.timer:\n            start_timer = time.time()\n\n        # for testing\n        # check = parallel_snr_func(*self.args[10])\n        # import pdb\n        # pdb.set_trace()\n\n        with mp.Pool(self.num_processors) as pool:\n            print('start pool with {} processors: {} total processes.\\n'.format(\n                    self.num_processors, len(self.args)))\n\n            results = [pool.apply_async(para_func, arg) for arg in self.args]\n            out = [r.get() for r in results]\n            out = {key: np.concatenate([out_i[key] for out_i in out]) for key in out[0].keys()}\n        if self.timer:\n            print(\"SNR calculation time:\", time.time()-start_timer)\n        return out", "response": "Run the function para_func on the number of processors and return the result of the function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RawParserUnparserFactory(parser_name, parse_callable, *unparse_callables):\n\n    def build_unparse(f):\n        @wraps(f)\n        def unparse(self, source, *a, **kw):\n            node = parse_callable(source)\n            return f(node, *a, **kw)\n        # a dumb and lazy docstring replacement\n        unparse.__doc__ = f.__doc__.replace(\n            'ast\\n        The AST ',\n            'source\\n        The source ',\n        )\n        return unparse\n\n    def build_parse(f):\n        @wraps(f)\n        def parse(self, source):\n            return f(source)\n        parse.__name__ = parser_name\n        parse.__qualname__ = parser_name\n        return parse\n\n    callables = {f.__name__: build_unparse(f) for f in unparse_callables}\n    callables['__call__'] = build_parse(parse_callable)\n    callables['__module__'] = PKGNAME\n    return type(parser_name, (object,), callables)()", "response": "Generates a callable object that also has callable attributes that passes its first argument to the parent callable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce a new parser unparser object from the names provided.", "response": "def ParserUnparserFactory(module_name, *unparser_names):\n    \"\"\"\n    Produce a new parser/unparser object from the names provided.\n    \"\"\"\n\n    parse_callable = import_module(PKGNAME + '.parsers.' + module_name).parse\n    unparser_module = import_module(PKGNAME + '.unparsers.' + module_name)\n    return RawParserUnparserFactory(module_name, parse_callable, *[\n        getattr(unparser_module, name) for name in unparser_names])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting a subset of points to preserve graph structure while minimizing athresize athresize", "response": "def epsilon_net(points, close_distance):\n  '''Selects a subset of `points` to preserve graph structure while minimizing\n  the number of points used, by removing points within `close_distance`.\n  Returns the downsampled indices.'''\n  num_points = points.shape[0]\n  indices = set(range(num_points))\n  selected = []\n  while indices:\n    idx = indices.pop()\n    nn_inds, = nearest_neighbors(points[idx], points, epsilon=close_distance)\n    indices.difference_update(nn_inds)\n    selected.append(idx)\n  return selected"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fuzzy_c_means(points, num_centers, m=2., tol=1e-4, max_iter=100,\n                  verbose=False):\n  '''Uses Fuzzy C-Means to downsample `points`.\n  m : aggregation parameter >1, larger implies smoother clusters\n  Returns indices of downsampled points.\n  '''\n  num_points = points.shape[0]\n  if num_centers >= num_points:\n    return np.arange(num_points)\n  # randomly initialize cluster assignments matrix\n  assn = np.random.random((points.shape[0], num_centers))\n  # iterate assignments until they converge\n  for i in range(max_iter):\n    # compute centers\n    w = assn ** m\n    w /= w.sum(axis=0)\n    centers = w.T.dot(points)\n    # calculate new assignments\n    d = pairwise_distances(points, centers)\n    d **= 2. / (m - 1)\n    np.maximum(d, 1e-10, out=d)\n    new_assn = 1. / np.einsum('ik,ij->ik', d, 1./d)\n    # check for convergence\n    change = np.linalg.norm(new_assn - assn)\n    if verbose:\n      print('At iteration %d: change = %g' % (i+1, change))\n    if change < tol:\n      break\n    assn = new_assn\n  else:\n    warnings.warn(\"fuzzy_c_means didn't converge in %d iterations\" % max_iter)\n  # find points closest to the selected cluster centers\n  return d.argmin(axis=0)", "response": "Uses Fuzzy C - Means to downsample points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a transport object from a given URL.", "response": "def transport_from_url(url):\n    \"\"\" Create a transport for the given URL.\n    \"\"\"\n    if '/' not in url and ':' in url and url.rsplit(':')[-1].isdigit():\n        url = 'scgi://' + url\n    url = urlparse.urlsplit(url, scheme=\"scgi\", allow_fragments=False)  # pylint: disable=redundant-keyword-arg\n\n    try:\n        transport = TRANSPORTS[url.scheme.lower()]\n    except KeyError:\n        if not any((url.netloc, url.query)) and url.path.isdigit():\n            # Support simplified \"domain:port\" URLs\n            return transport_from_url(\"scgi://%s:%s\" % (url.scheme, url.path))\n        else:\n            raise URLError(\"Unsupported scheme in URL %r\" % url.geturl())\n    else:\n        return transport(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps data in an SCGI request.", "response": "def _encode_payload(data, headers=None):\n    \"Wrap data in an SCGI request.\"\n    prolog = \"CONTENT_LENGTH\\0%d\\0SCGI\\x001\\0\" % len(data)\n    if headers:\n        prolog += _encode_headers(headers)\n\n    return _encode_netstring(prolog) + data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_headers(headers):\n    \"Get headers dict from header string.\"\n    try:\n        return dict(line.rstrip().split(\": \", 1)\n            for line in headers.splitlines()\n            if line\n        )\n    except (TypeError, ValueError) as exc:\n        raise SCGIException(\"Error in SCGI headers %r (%s)\" % (headers, exc,))", "response": "Get headers dict from header string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_response(resp):\n    # Assume they care for standards and send us CRLF (not just LF)\n    try:\n        headers, payload = resp.split(\"\\r\\n\\r\\n\", 1)\n    except (TypeError, ValueError) as exc:\n        raise SCGIException(\"No header delimiter in SCGI response of length %d (%s)\" % (len(resp), exc,))\n    headers = _parse_headers(headers)\n\n    clen = headers.get(\"Content-Length\")\n    if clen is not None:\n        # Check length, just in case the transport is bogus\n        assert len(payload) == int(clen)\n\n    return payload, headers", "response": "Parse xmlrpc response from scgi response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scgi_request(url, methodname, *params, **kw):\n    xmlreq = xmlrpclib.dumps(params, methodname)\n    xmlresp = SCGIRequest(url).send(xmlreq)\n\n    if kw.get(\"deserialize\", True):\n        # This fixes a bug with the Python xmlrpclib module\n        # (has no handler for <i8> in some versions)\n        xmlresp = xmlresp.replace(\"<i8>\", \"<i4>\").replace(\"</i8>\", \"</i4>\")\n\n        # Return deserialized data\n        return xmlrpclib.loads(xmlresp)[0][0]\n    else:\n        # Return raw XML\n        return xmlresp", "response": "Send a SCGI request over the given URL and return the XMLRPC response or the equivalent Python data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends data to the device and yield response chunks.", "response": "def send(self, data):\n        \"\"\" Open transport, send data, and yield response chunks.\n        \"\"\"\n        sock = socket.socket(*self.sock_args)\n        try:\n            sock.connect(self.sock_addr)\n        except socket.error as exc:\n            raise socket.error(\"Can't connect to %r (%s)\" % (self.url.geturl(), exc))\n\n        try:\n            # Send request\n            sock.send(data)\n\n            # Read response\n            while True:\n                chunk = sock.recv(self.CHUNK_SIZE)\n                if chunk:\n                    yield chunk\n                else:\n                    break\n        finally:\n            # Clean up\n            sock.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, data):\n        try:\n            proc = subprocess.Popen(self.cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except OSError as exc:\n            raise URLError(\"Calling %r failed (%s)!\" % (' '.join(self.cmd), exc))\n        else:\n            stdout, stderr = proc.communicate(data)\n            if proc.returncode:\n                raise URLError(\"Calling %r failed with RC=%d!\\n%s\" % (\n                   ' '.join(self.cmd), proc.returncode, stderr,\n                ))\n            yield stdout", "response": "Send data to the device and yield response chunks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send(self, data):\n        start = time.time()\n        try:\n            scgi_resp = ''.join(self.transport.send(_encode_payload(data)))\n        finally:\n            self.latency = time.time() - start\n\n        resp, self.resp_headers = _parse_response(scgi_resp)\n        return resp", "response": "Send data over the scgi to the URL and get response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn generator that iterates over frames in a traceback", "response": "def _frames(traceback):\n    '''\n    Returns generator that iterates over frames in a traceback\n    '''\n    frame = traceback\n    while frame.tb_next:\n      frame = frame.tb_next\n      yield frame.tb_frame\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _methodInTraceback(self, name, traceback):\n    '''\n    Returns boolean whether traceback contains method from this instance\n    '''\n    foundMethod = False\n    for frame in self._frames(traceback):\n      this = frame.f_locals.get('self')\n      if this is self and frame.f_code.co_name == name:\n        foundMethod = True\n        break\n    return foundMethod", "response": "Returns True if traceback contains method from this instance\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a rollback step with optional arguments.", "response": "def addStep(self, callback, *args, **kwargs):\n    '''\n    Add rollback step with optional arguments. If a rollback is\n    triggered, each step is called in LIFO order.\n    '''\n    self.steps.append((callback, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef doRollback(self):\n    '''\n    Call each rollback step in LIFO order.\n    '''\n    while self.steps:\n      callback, args, kwargs = self.steps.pop()\n      callback(*args, **kwargs)", "response": "Call each rollback step in LIFO order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a neighbor graph from pairwise distance information.", "response": "def neighbor_graph(X, metric='euclidean', k=None, epsilon=None,\r\n                   weighting='none', precomputed=False):\r\n  '''Build a neighbor graph from pairwise distance information.\r\n\r\n  X : two-dimensional array-like\r\n      Shape must either be (num_pts, num_dims) or (num_pts, num_pts).\r\n  k : int, maximum number of nearest neighbors\r\n  epsilon : float, maximum distance to a neighbor\r\n  metric : str, type of distance metric (see sklearn.metrics)\r\n      When metric='precomputed', X is a symmetric distance matrix.\r\n  weighting : str, one of {'binary', 'none'}\r\n      When weighting='binary', all edge weights == 1.\r\n  '''\r\n  if k is None and epsilon is None:\r\n    raise ValueError('Must provide `k` or `epsilon`.')\r\n  if weighting not in ('binary', 'none'):\r\n    raise ValueError('Invalid weighting param: %r' % weighting)\r\n\r\n  # TODO: deprecate the precomputed kwarg\r\n  precomputed = precomputed or (metric == 'precomputed')\r\n  binary = weighting == 'binary'\r\n\r\n  # Try the fast path, if possible.\r\n  if not precomputed and epsilon is None:\r\n    return _sparse_neighbor_graph(X, k, binary, metric)\r\n\r\n  if precomputed:\r\n    D = X\r\n  else:\r\n    D = pairwise_distances(X, metric=metric)\r\n  return _slow_neighbor_graph(D, k, epsilon, binary)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds nearest neighbors of query points from a matrix of target points.", "response": "def nearest_neighbors(query_pts, target_pts=None, metric='euclidean',\r\n                      k=None, epsilon=None, return_dists=False,\r\n                      precomputed=False):\r\n  '''Find nearest neighbors of query points from a matrix of target points.\r\n\r\n  Returns a list of indices of neighboring points, one list per query.\r\n  If no target_pts are specified, distances are calculated within query_pts.\r\n  When return_dists is True, returns two lists: (distances, indices)\r\n  '''\r\n  if k is None and epsilon is None:\r\n    raise ValueError('Must provide `k` or `epsilon`.')\r\n\r\n  # TODO: deprecate the precomputed kwarg\r\n  precomputed = precomputed or (metric == 'precomputed')\r\n\r\n  if precomputed and target_pts is not None:\r\n    raise ValueError('`target_pts` cannot be used with precomputed distances')\r\n\r\n  query_pts = np.array(query_pts)\r\n  if len(query_pts.shape) == 1:\r\n    query_pts = query_pts.reshape((1,-1))  # ensure that the query is a 1xD row\r\n\r\n  if precomputed:\r\n    dists = query_pts.copy()\r\n  else:\r\n    dists = pairwise_distances(query_pts, Y=target_pts, metric=metric)\r\n\r\n  if epsilon is not None:\r\n    if k is not None:\r\n      # kNN filtering\r\n      _, not_nn = _min_k_indices(dists, k, inv_ind=True)\r\n      dists[np.arange(dists.shape[0]), not_nn.T] = np.inf\r\n    # epsilon-ball\r\n    is_close = dists <= epsilon\r\n    if return_dists:\r\n      nnis,nnds = [],[]\r\n      for i,row in enumerate(is_close):\r\n        nns = np.nonzero(row)[0]\r\n        nnis.append(nns)\r\n        nnds.append(dists[i,nns])\r\n      return nnds, nnis\r\n    return np.array([np.nonzero(row)[0] for row in is_close])\r\n\r\n  # knn\r\n  nns = _min_k_indices(dists,k)\r\n  if return_dists:\r\n    # index each row of dists by each row of nns\r\n    row_inds = np.arange(len(nns))[:,np.newaxis]\r\n    nn_dists = dists[row_inds, nns]\r\n    return nn_dists, nns\r\n  return nns"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sparse_neighbor_graph(X, k, binary=False, metric='l2'):\r\n  '''Construct a sparse adj matrix from a matrix of points (one per row).\r\n  Non-zeros are unweighted/binary distance values, depending on the binary arg.\r\n  Doesn't include self-edges.'''\r\n  knn = NearestNeighbors(n_neighbors=k, metric=metric).fit(X)\r\n  mode = 'connectivity' if binary else 'distance'\r\n  try:\r\n    adj = knn.kneighbors_graph(None, mode=mode)\r\n  except IndexError:\r\n    # XXX: we must be running an old (<0.16) version of sklearn\r\n    #  We have to hack around an old bug:\r\n    if binary:\r\n      adj = knn.kneighbors_graph(X, k+1, mode=mode)\r\n      adj.setdiag(0)\r\n    else:\r\n      adj = knn.kneighbors_graph(X, k, mode=mode)\r\n  return Graph.from_adj_matrix(adj)", "response": "Construct a sparse adj matrix from a matrix of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef saffron(X, q=32, k=4, tangent_dim=1, curv_thresh=0.95, decay_rate=0.9,\n            max_iter=15, verbose=False):\n  '''\n  SAFFRON graph construction method.\n\n    X : (n,d)-array of coordinates\n    q : int, median number of candidate friends per vertex\n    k : int, number of friends to select per vertex, k < q\n    tangent_dim : int, dimensionality of manifold tangent space\n    curv_thresh : float, tolerance to curvature, lambda in the paper\n    decay_rate : float, controls step size per iteration, between 0 and 1\n    max_iter : int, cap on number of iterations\n    verbose : bool, print goodness measure per iteration when True\n\n  From \"Tangent Space Guided Intelligent Neighbor Finding\",\n    by Gashler & Martinez, 2011.\n  See http://axon.cs.byu.edu/papers/gashler2011ijcnn1.pdf\n  '''\n  n = len(X)\n  dist = pairwise_distances(X)\n  idx = np.argpartition(dist, q)[:, q]\n  # radius for finding candidate friends: median distance to qth neighbor\n  r = np.median(dist[np.arange(n), idx])\n\n  # make candidate graph + weights\n  W = neighbor_graph(dist, precomputed=True, epsilon=r).matrix('csr')\n  # NOTE: this differs from the paper, where W.data[:] = 1 initially\n  W.data[:] = 1 / W.data\n  # row normalize\n  normalize(W, norm='l1', axis=1, copy=False)\n  # XXX: hacky densify\n  W = W.toarray()\n\n  # iterate to learn optimal weights\n  prev_goodness = 1e-12\n  for it in range(max_iter):\n    goodness = 0\n    S = _estimate_tangent_spaces(X, W, tangent_dim)\n    # find aligned candidates\n    for i, row in enumerate(W):\n      nbrs = row.nonzero()[-1]\n\n      # compute alignment scores\n      edges = X[nbrs] - X[i]\n      edge_norms = (edges**2).sum(axis=1)\n      a1 = (edges.dot(S[i])**2).sum(axis=1) / edge_norms\n      a2 = (np.einsum('ij,ijk->ik', edges, S[nbrs])**2).sum(axis=1) / edge_norms\n      a3 = _principal_angle(S[i], S[nbrs]) ** 2\n      x = (np.minimum(curv_thresh, a1) *\n           np.minimum(curv_thresh, a2) *\n           np.minimum(curv_thresh, a3))\n\n      # decay weight of least-aligned candidates\n      excess = x.shape[0] - k\n      if excess > 0:\n        bad_idx = np.argpartition(x, excess-1)[:excess]\n        W[i, nbrs[bad_idx]] *= decay_rate\n        W[i] /= W[i].sum()\n\n      # update goodness measure (weighted alignment)\n      goodness += x.dot(W[i,nbrs])\n\n    if verbose:  # pragma: no cover\n      goodness /= n\n      print(it, goodness, goodness / prev_goodness)\n    if goodness / prev_goodness <= 1.0001:\n      break\n    prev_goodness = goodness\n  else:\n    warnings.warn('Failed to converge after %d iterations.' % max_iter)\n\n  # use the largest k weights for each row of W, weighted by original distance\n  indptr, indices, data = [0], [], []\n  for i, row in enumerate(W):\n    nbrs = row.nonzero()[-1]\n    if len(nbrs) > k:\n      nbrs = nbrs[np.argpartition(row[nbrs], len(nbrs)-k)[-k:]]\n    indices.extend(nbrs)\n    indptr.append(len(nbrs))\n    data.extend(dist[i, nbrs])\n  indptr = np.cumsum(indptr)\n  data = np.array(data)\n  indices = np.array(indices)\n  W = ss.csr_matrix((data, indices, indptr), shape=W.shape)\n  return Graph.from_adj_matrix(W)", "response": "SAFFRON graph construction method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the principal angle between two vectors a and b.", "response": "def _principal_angle(a, B):\n  '''a is (d,t), B is (k,d,t)'''\n  # TODO: check case for t = d-1\n  if a.shape[1] == 1:\n    return a.T.dot(B)[0,:,0]\n\n  # find normals that maximize distance when projected\n  x1 = np.einsum('abc,adc->abd', B, B).dot(a) - a   # b.dot(b.T).dot(a) - a\n  x2 = np.einsum('ab,cad->cbd', a.dot(a.T), B) - B  # a.dot(a.T).dot(b) - b\n  xx = np.vstack((x1, x2))\n\n  # batch PCA (1st comp. only)\n  xx -= xx.mean(axis=1)[:,None]\n  c = np.einsum('abc,abd->acd', xx, xx)\n  _, vecs = np.linalg.eigh(c)\n  fpc = vecs[:,:,-1]\n  fpc1 = fpc[:len(x1)]\n  fpc2 = fpc[len(x1):]\n\n  # a.dot(fpc1).dot(b.dot(fpc2))\n  lhs = a.dot(fpc1.T).T\n  rhs = np.einsum('abc,ac->ab', B, fpc2)\n  return np.einsum('ij,ij->i', lhs, rhs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(db, encoding, no_limit, zip, case_insensitive):\n\n    limit = 15\n\n    if no_limit:\n        limit = float('inf')\n\n    with open_db(db, zip, encoding=encoding, case_sensitive=not case_insensitive) as dbf:\n        for idx, row in enumerate(dbf, 1):\n            click.secho('')\n\n            for key, val in row._asdict().items():\n                click.secho('  %s: %s' % (key, val))\n\n            if idx == limit:\n                click.secho(\n                    'Note: Output is limited to %s rows. Use --no-limit option to bypass.' % limit, fg='red')\n                break", "response": "Show. dbf file contents ( rows )."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef describe(db, zip, case_insensitive):\n\n    with open_db(db, zip, case_sensitive=not case_insensitive) as dbf:\n        click.secho('Rows count: %s' % (dbf.prolog.records_count))\n        click.secho('Fields:')\n        for field in dbf.fields:\n            click.secho('  %s: %s' % (field.type, field))", "response": "Show. dbf file statistics."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclone the Playdoh repo into a custom path.", "response": "def clone_repo(pkg, dest, repo, repo_dest, branch):\n    \"\"\"Clone the Playdoh repo into a custom path.\"\"\"\n    git(['clone', '--recursive', '-b', branch, repo, repo_dest])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_pkg(pkg, repo_dest):\n    vars = {'pkg': pkg}\n    with dir_path(repo_dest):\n        patch(\"\"\"\\\n        diff --git a/manage.py b/manage.py\n        index 40ebb0a..cdfe363 100755\n        --- a/manage.py\n        +++ b/manage.py\n        @@ -3,7 +3,7 @@ import os\n         import sys\n\n         # Edit this if necessary or override the variable in your environment.\n        -os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'project.settings')\n        +os.environ.setdefault('DJANGO_SETTINGS_MODULE', '%(pkg)s.settings')\n\n         try:\n             # For local development in a virtualenv:\n        diff --git a/project/settings/base.py b/project/settings/base.py\n        index 312f280..c75e673 100644\n        --- a/project/settings/base.py\n        +++ b/project/settings/base.py\n        @@ -7,7 +7,7 @@ from funfactory.settings_base import *\n         # If you did not install Playdoh with the funfactory installer script\n         # you may need to edit this value. See the docs about installing from a\n         # clone.\n        -PROJECT_MODULE = 'project'\n        +PROJECT_MODULE = '%(pkg)s'\n\n         # Bundles is a dictionary of two dictionaries, css and js, which list css files\n         # and js files that can be bundled together by the minify app.\n        diff --git a/setup.py b/setup.py\n        index 58dbd93..9a38628 100644\n        --- a/setup.py\n        +++ b/setup.py\n        @@ -3,7 +3,7 @@ import os\n         from setuptools import setup, find_packages\n\n\n        -setup(name='project',\n        +setup(name='%(pkg)s',\n               version='1.0',\n               description='Django application.',\n               long_description='',\n        \"\"\" % vars)\n\n        git(['mv', 'project', pkg])\n        git(['commit', '-a', '-m', 'Renamed project module to %s' % pkg])", "response": "Initializes a custom named package module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_settings(pkg, repo_dest, db_user, db_name, db_password, db_host,\n                    db_port):\n    \"\"\"\n    Creates a local settings file out of the distributed template.\n\n    This also fills in database settings and generates a secret key, etc.\n    \"\"\"\n    vars = {'pkg': pkg,\n            'db_user': db_user,\n            'db_name': db_name,\n            'db_password': db_password or '',\n            'db_host': db_host or '',\n            'db_port': db_port or '',\n            'hmac_date': datetime.now().strftime('%Y-%m-%d'),\n            'hmac_key': generate_key(32),\n            'secret_key': generate_key(32)}\n    with dir_path(repo_dest):\n        shutil.copyfile('%s/settings/local.py-dist' % pkg,\n                        '%s/settings/local.py' % pkg)\n        patch(\"\"\"\\\n            --- a/%(pkg)s/settings/local.py\n            +++ b/%(pkg)s/settings/local.py\n            @@ -9,11 +9,11 @@ from . import base\n             DATABASES = {\n                 'default': {\n                     'ENGINE': 'django.db.backends.mysql',\n            -        'NAME': 'playdoh_app',\n            -        'USER': 'root',\n            -        'PASSWORD': '',\n            -        'HOST': '',\n            -        'PORT': '',\n            +        'NAME': '%(db_name)s',\n            +        'USER': '%(db_user)s',\n            +        'PASSWORD': '%(db_password)s',\n            +        'HOST': '%(db_host)s',\n            +        'PORT': '%(db_port)s',\n                     'OPTIONS': {\n                         'init_command': 'SET storage_engine=InnoDB',\n                         'charset' : 'utf8',\n            @@ -51,14 +51,14 @@ DEV = True\n             # Playdoh ships with Bcrypt+HMAC by default because it's the most secure.\n             # To use bcrypt, fill in a secret HMAC key. It cannot be blank.\n             HMAC_KEYS = {\n            -    #'2012-06-06': 'some secret',\n            +    '%(hmac_date)s': '%(hmac_key)s',\n             }\n\n             from django_sha2 import get_password_hashers\n             PASSWORD_HASHERS = get_password_hashers(base.BASE_PASSWORD_HASHERS, HMAC_KEYS)\n\n             # Make this unique, and don't share it with anybody.  It cannot be blank.\n            -SECRET_KEY = ''\n            +SECRET_KEY = '%(secret_key)s'\n\n             # Uncomment these to activate and customize Celery:\n             # CELERY_ALWAYS_EAGER = False  # required to activate celeryd\n            \"\"\" % vars)", "response": "Creates a distributed settings file out of the distributed template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a virtualenv within which to install your new application.", "response": "def create_virtualenv(pkg, repo_dest, python):\n    \"\"\"Creates a virtualenv within which to install your new application.\"\"\"\n    workon_home = os.environ.get('WORKON_HOME')\n    venv_cmd = find_executable('virtualenv')\n    python_bin = find_executable(python)\n    if not python_bin:\n        raise EnvironmentError('%s is not installed or not '\n                               'available on your $PATH' % python)\n    if workon_home:\n        # Can't use mkvirtualenv directly here because relies too much on\n        # shell tricks. Simulate it:\n        venv = os.path.join(workon_home, pkg)\n    else:\n        venv = os.path.join(repo_dest, '.virtualenv')\n    if venv_cmd:\n        if not verbose:\n            log.info('Creating virtual environment in %r' % venv)\n        args = ['--python', python_bin, venv]\n        if not verbose:\n            args.insert(0, '-q')\n        subprocess.check_call([venv_cmd] + args)\n    else:\n        raise EnvironmentError('Could not locate the virtualenv. Install with '\n                               'pip install virtualenv.')\n    return venv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling all compiled requirements that can t be shipped in vendor.", "response": "def install_reqs(venv, repo_dest):\n    \"\"\"Installs all compiled requirements that can't be shipped in vendor.\"\"\"\n    with dir_path(repo_dest):\n        args = ['-r', 'requirements/compiled.txt']\n        if not verbose:\n            args.insert(0, '-q')\n        subprocess.check_call([os.path.join(venv, 'bin', 'pip'), 'install'] +\n                              args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_executable(name):\n    for pt in os.environ.get('PATH', '').split(':'):\n        candidate = os.path.join(pt, name)\n        if os.path.exists(candidate):\n            return candidate", "response": "Find the actual path to a named command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping amino - acid molecular weights to colors.", "response": "def MWColorMapping(maptype='jet', reverse=True):\n    \"\"\"Maps amino-acid molecular weights to colors. Otherwise, this\n    function is identical to *KyteDoolittleColorMapping*\n    \"\"\" \n    d = {'A':89,'R':174,'N':132,'D':133,'C':121,'Q':146,'E':147,\\\n         'G':75,'H':155,'I':131,'L':131,'K':146,'M':149,'F':165,\\\n         'P':115,'S':105,'T':119,'W':204,'Y':181,'V':117}\n    \n    aas = sorted(AA_TO_INDEX.keys())\n    mws  = [d[aa] for aa in aas]\n    if reverse:\n        mws = [-1 * x for x in mws]\n    mapper = pylab.cm.ScalarMappable(cmap=maptype)\n    mapper.set_clim(min(mws), max(mws))\n    mapping_d = {'*':'#000000'}\n    for (aa, h) in zip(aas, mws):\n        tup = mapper.to_rgba(h, bytes=True)\n        (red, green, blue, alpha) = tup\n        mapping_d[aa] = '#%02x%02x%02x' % (red, green, blue)\n        assert len(mapping_d[aa]) == 7\n    cmap = mapper.get_cmap()\n    return (cmap, mapping_d, mapper)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ChargeColorMapping(maptype='jet', reverse=False):\n\n    pos_color = '#FF0000'\n    neg_color = '#0000FF'\n    neut_color = '#000000'\n\n    mapping_d = {'A':neut_color,'R':pos_color,'N':neut_color,\\\n                 'D':neg_color,'C':neut_color,'Q':neut_color,\\\n                 'E':neg_color,'G':neut_color,'H':pos_color,\\\n                 'I':neut_color,'L':neut_color,'K':pos_color,\\\n                 'M':neut_color,'F':neut_color,'P':neut_color,\\\n                 'S':neut_color,'T':neut_color,'W':neut_color,\\\n                 'Y':neut_color,'V':neut_color}\n\n    return (None, mapping_d, None)", "response": "Maps amino - acid charge at neutral pH to colors. \n    Currently does not use the keyword arguments for KyteDoolittleColorMapping and MWColorMapping for now but accepts these arguments for KyteDoolittleColorMapping and MWColorMapping for now."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef FunctionalGroupColorMapping(maptype='jet', reverse=False):\n\n    small_color = '#f76ab4'\n    nucleophilic_color = '#ff7f00'\n    hydrophobic_color = '#12ab0d'\n    aromatic_color = '#84380b'\n    acidic_color = '#e41a1c'\n    amide_color = '#972aa8'\n    basic_color = '#3c58e5'\n\n    mapping_d = {'G':small_color, 'A':small_color,\n                 'S':nucleophilic_color, 'T':nucleophilic_color, 'C':nucleophilic_color,\n                 'V':hydrophobic_color, 'L':hydrophobic_color, 'I':hydrophobic_color, 'M':hydrophobic_color, 'P':hydrophobic_color,\n                 'F':aromatic_color, 'Y':aromatic_color, 'W':aromatic_color,\n                 'D':acidic_color, 'E':acidic_color,\n                 'H':basic_color, 'K':basic_color, 'R':basic_color,\n                 'N':amide_color, 'Q':amide_color,\n                 '*':'#000000'}\n    return (None, mapping_d, None)", "response": "Maps amino - acid functional groups to colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef LogoPlot(sites, datatype, data, plotfile, nperline,\n        numberevery=10, allowunsorted=False, ydatamax=1.01,\n        overlay=None, fix_limits={}, fixlongname=False,\n        overlay_cmap=None, ylimits=None, relativestackheight=1,\n        custom_cmap='jet', map_metric='kd', noseparator=False,\n        underlay=False, scalebar=False):\n    \"\"\"Create sequence logo showing amino-acid or nucleotide preferences.\n\n    The heights of each letter is equal to the preference of\n    that site for that amino acid or nucleotide.\n\n    Note that stop codons may or may not be included in the logo\n    depending on whether they are present in *pi_d*.  \n\n    CALLING VARIABLES:\n\n    * *sites* is a list of all of the sites that are being included\n      in the logo, as strings. They must be in natural sort or an error\n      will be raised **unless** *allowunsorted* is *True*. The sites\n      in the plot are ordered in the same arrangement\n      listed in *sites*. These should be **strings**, not integers.\n\n    * *datatype* should be one of the following strings:\n    \n        * 'prefs' for preferences\n        \n        * 'diffprefs' for differential preferences\n        \n        * 'diffsel' for differential selection\n\n    * *data* is a dictionary that has a key for every entry in\n      *sites*. For every site *r* in *sites*, *sites[r][x]*\n      is the value for character *x*. \n      Preferences must sum to one; differential preferences to zero.\n      All sites must have the same set of characters. The characters\n      must be the set of nucleotides or amino acids with or without\n      stop codons.\n\n    * *plotfile* is a string giving the name of the created PDF file \n      of the logo plot.\n      It must end in the extension ``.pdf``.\n\n    * *nperline* is the number of sites per line. Often 40 to 80 are good values.\n\n    * *numberevery* is specifies how frequently we put labels for the sites on\n      x-axis.\n\n    * *allowunsorted* : if *True* then we allow the entries in *sites* to \n      **not** be sorted. This means that the logo plot will **not** have\n      sites in sorted order.\n\n    * *ydatamax* : meaningful only if *datatype* is 'diffprefs'. In this case, it gives\n      the maximum that the logo stacks extend in the positive and negative directions.\n      Cannot be smaller than the maximum extent of the differential preferences.\n\n    * *ylimits*: is **mandatory** if *datatype* is 'diffsel', and meaningless \n      otherwise. It is *(ymin, ymax)* where *ymax > 0 > ymin*, and gives extent \n      of the data in the positive and negative directions. Must encompass the \n      actual maximum and minimum of the data.\n\n    * *overlay* : make overlay bars that indicate other properties for\n      the sites. If you set to something other than `None`, it should be\n      a list giving one to three properties. Each property is a tuple:\n      *(prop_d, shortname, longname)* where:\n\n        - *prop_d* is a dictionary keyed by site numbers that are in *sites*.\n          For each *r* in *sites*, *prop_d[r]* gives the value of the property,\n          or if there is no entry in *prop_d* for *r*, then the property\n          is undefined and is colored white. Properties can either be:\n\n            * continuous: in this case, all of the values should be numbers.\n\n            * discrete : in this case, all of the values should be strings.\n              While in practice, if you have more than a few discrete\n              categories (different strings), the plot will be a mess.\n\n        - *shortname* : short name for the property; will not format well\n          if more than 4 or 5 characters.\n\n        - *longname* : longer name for property used on axes label. Can be the\n          same as *shortname* if you don't need a different long name.\n\n        - In the special case where both *shortname* and *longname* are \n          the string `wildtype`, then rather than an overlay bar we\n          right the one-character wildtype identity in `prop_d` for each\n          site.\n\n    * *fix_limits* is only meaningful if *overlay* is being used. In this case, for any\n      *shortname* in *overlay* that also keys an entry in *fix_limits*, we use\n      *fix_limits[shortname]* to set the limits for *shortname*. Specifically,\n      *fix_limits[shortname]* should be the 2-tuple *(ticks, ticknames)*. *ticks*\n      should be a list of tick locations (numbers) and *ticknames* should be a list of\n      the corresponding tick label for that tick.\n\n    * If *fixlongname* is *True*, then we use the *longname* in *overlay* exactly as written;\n      otherwise we add a parenthesis indicating the *shortname* for which this *longname*\n      stands.\n\n    * *overlay_cmap* can be the name of a valid *matplotlib.colors.Colormap*, such as the\n      string *jet* or *bwr*. Otherwise, it can be *None* and a (hopefully) good choice will \n      be made for you.\n\n    * *custom_cmap* can be the name of a valid *matplotlib.colors.Colormap* which will be\n      used to color amino-acid one-letter codes in the logoplot by the *map_metric* when\n      either 'kd' or 'mw' is used as *map_metric*.\n\n    * *relativestackheight* indicates how high the letter stack is relative to\n      the default. The default is multiplied by this number, so make it > 1\n      for a higher letter stack.\n\n    * *map_metric* specifies the amino-acid property metric used to map colors to amino-acid\n      letters. Valid options are 'kd' (Kyte-Doolittle hydrophobicity scale, default), 'mw' \n      (molecular weight), 'functionalgroup' (functional groups: small, nucleophilic, hydrophobic,\n      aromatic, basic, acidic, and amide), and 'charge' (charge at neutral pH). If 'charge' is used, then the\n      argument for *custom_cmap* will no longer be meaningful, since 'charge' uses its own \n      blue/black/red colormapping. Similarly, 'functionalgroup' uses its own colormapping.\n\n    * *noseparator* is only meaningful if *datatype* is 'diffsel' or 'diffprefs'.\n      If it set to *True*, then we do **not** print a black horizontal line to\n      separate positive and negative values.\n\n    * *underlay* if `True` then make an underlay rather than an overlay.\n\n    * *scalebar*: show a scale bar. If `False`, no scale bar shown. Otherwise\n      should be a 2-tuple of `(scalebarlen, scalebarlabel)`. Currently only\n      works when data is `diffsel`.\n    \"\"\"\n    assert datatype in ['prefs', 'diffprefs', 'diffsel'], \"Invalid datatype {0}\".format(datatype)\n\n    # check data, and get characters\n    assert sites, \"No sites specified\"\n    assert set(sites) == set(data.keys()), \"Not a match between sites and the keys of data\"\n    characters = list(data[sites[0]].keys())\n    aas = sorted(AA_TO_INDEX.keys())\n    if set(characters) == set(NT_TO_INDEX.keys()):\n        alphabet_type = 'nt'\n    elif set(characters) == set(aas) or set(characters) == set(aas + ['*']):\n        alphabet_type = 'aa'\n    else:\n        raise ValueError(\"Invalid set of characters in data. Does not specify either nucleotides or amino acids:\\n%s\" % str(characters))\n    for r in sites:\n        if set(data[r].keys()) != set(characters):\n            raise ValueError(\"Not all sites in data have the same set of characters\")\n\n    firstblankchar = 'B' # character for first blank space for diffprefs / diffsel\n    assert firstblankchar not in characters, \"firstblankchar in characters\"\n    lastblankchar = 'b' # character for last blank space for diffprefs / diffsel\n    assert lastblankchar not in characters, \"lastblankchar in characters\"\n    separatorchar = '-' # separates positive and negative for diffprefs / diffsel\n    assert separatorchar not in characters, \"lastblankchar in characters\"\n    if noseparator:\n        separatorheight = 0\n    else:\n        separatorheight = 0.02 # height of separator as frac of total for diffprefs / diffsel\n\n    if os.path.splitext(plotfile)[1].lower() != '.pdf':\n        raise ValueError(\"plotfile must end in .pdf: %s\" % plotfile)\n    if os.path.isfile(plotfile):\n        os.remove(plotfile) # remove existing plot\n\n    if not allowunsorted:\n        sorted_sites = natsort.natsorted([r for r in sites])\n        if sorted_sites != sites:\n            raise ValueError(\"sites is not properly sorted\")\n\n    # Following are specifications of weblogo sizing taken from its documentation\n    stackwidth = 9.5 # stack width in points, not default size of 10.8, but set to this in weblogo call below\n    barheight = 5.5 # height of bars in points if using overlay\n    barspacing = 2.0 # spacing between bars in points if using overlay\n    stackaspectratio = 4.4 # ratio of stack height:width, doesn't count part going over maximum value of 1\n    assert relativestackheight > 0, \"relativestackheight must be > 0\"\n    stackaspectratio *= relativestackheight\n    if overlay:\n        if len(overlay) > 3:\n            raise ValueError(\"overlay cannot have more than 3 entries\")\n        ymax = (stackaspectratio * stackwidth + len(overlay) * (barspacing + barheight)) / float(stackaspectratio * stackwidth)\n        aspectratio = ymax * stackaspectratio # effective aspect ratio for full range\n    else:\n        ymax = 1.0 \n        aspectratio = stackaspectratio\n    rmargin = 11.5 # right margin in points, fixed by weblogo\n    stackheightmargin = 16 # margin between stacks in points, fixed by weblogo\n\n    showscalebar = False\n    try:\n        # write data into transfacfile (a temporary file)\n        (fd, transfacfile) = tempfile.mkstemp()\n        f = os.fdopen(fd, 'w')\n        ordered_alphabets = {} # keyed by site index (0, 1, ...) with values ordered lists for characters from bottom to top\n        if datatype == 'prefs':\n            chars_for_string = characters\n            f.write('ID ID\\nBF BF\\nP0 %s\\n' % ' '.join(chars_for_string))\n            for (isite, r) in enumerate(sites):\n                f.write('%d %s\\n' % (isite, ' '.join([str(data[r][x]) for x in characters])))\n                pi_r = [(data[r][x], x) for x in characters]\n                pi_r.sort()\n                ordered_alphabets[isite] = [tup[1] for tup in pi_r] # order from smallest to biggest\n        elif datatype == 'diffprefs':\n            chars_for_string = characters + [firstblankchar, lastblankchar, separatorchar]\n            ydatamax *= 2.0 # maximum possible range of data, multiply by two for range\n            f.write('ID ID\\nBF BF\\nP0 %s\\n' % ' '.join(chars_for_string))\n            for (isite, r) in enumerate(sites):\n                positivesum = sum([data[r][x] for x in characters if data[r][x] > 0]) + separatorheight / 2.0\n                negativesum = sum([data[r][x] for x in characters if data[r][x] < 0]) - separatorheight / 2.0\n                if abs(positivesum + negativesum) > 1.0e-3:\n                    raise ValueError(\"Differential preferences sum of %s is not close to zero for site %s\" % (positivesum + negativesum, r))\n                if 2.0 * positivesum > ydatamax:\n                    raise ValueError(\"You need to increase ydatamax: the total differential preferences sum to more than the y-axis limits. Right now, ydatamax is %.3f while the total differential preferences are %.3f\" % (ydatamax, 2.0 * positivesum))\n                f.write('%d' % isite)\n                deltapi_r = []\n                for x in characters:\n                    deltapi_r.append((data[r][x], x))\n                    f.write(' %s' % (abs(data[r][x]) / float(ydatamax)))\n                deltapi_r.sort()\n                firstpositiveindex = 0\n                while deltapi_r[firstpositiveindex][0] < 0:\n                    firstpositiveindex += 1\n                ordered_alphabets[isite] = [firstblankchar] + [tup[1] for tup in deltapi_r[ : firstpositiveindex]] + [separatorchar] + [tup[1] for tup in deltapi_r[firstpositiveindex : ]] + [lastblankchar] # order from most negative to most positive with blank characters and separators\n                f.write(' %g %g %g\\n' % (0.5 * (ydatamax + 2.0 * negativesum) / ydatamax, 0.5 * (ydatamax + 2.0 * negativesum) / ydatamax, separatorheight)) # heights for blank charactors and separators\n        elif datatype == 'diffsel':\n            assert ylimits, \"You must specify ylimits if using diffsel\"\n            (dataymin, dataymax) = ylimits\n            assert dataymax > 0 > dataymin, \"Invalid ylimits of {0}\".format(ylimits)\n            yextent = float(dataymax - dataymin)\n            separatorheight *= yextent\n            chars_for_string = characters + [firstblankchar, lastblankchar, separatorchar]\n            f.write('ID ID\\nBF BF\\nP0 {0}\\n'.format(' '.join(chars_for_string)))\n            for (isite, r) in enumerate(sites):\n                positivesum = sum([data[r][x] for x in characters if data[r][x] > 0]) + separatorheight / 2.0\n                negativesum = sum([data[r][x] for x in characters if data[r][x] < 0]) - separatorheight / 2.0\n                assert positivesum <= dataymax, \"Data exceeds ylimits in positive direction\"\n                assert negativesum >= dataymin, \"Data exceeds ylimits in negative direction\"\n                f.write('{0}'.format(isite))\n                diffsel_r = []\n                for x in characters:\n                    diffsel_r.append((data[r][x], x))\n                    f.write(' {0}'.format(abs(data[r][x]) / yextent))\n                diffsel_r.sort()\n                firstpositiveindex = 0\n                while diffsel_r[firstpositiveindex][0] < 0:\n                    firstpositiveindex += 1\n                ordered_alphabets[isite] = [firstblankchar] + [tup[1] for tup in diffsel_r[ : firstpositiveindex]] + [separatorchar] + [tup[1] for tup in diffsel_r[firstpositiveindex : ]] + [lastblankchar] # order from most negative to most positive with blank characters and separators\n                f.write(' %g %g %g\\n' % ((negativesum - dataymin) / yextent, (dataymax - positivesum) / yextent, separatorheight / yextent)) # heights for blank charactors and separators\n            # height of one unit on y-axis in points\n            heightofone = stackwidth * stackaspectratio / yextent\n            assert heightofone > 0\n            if scalebar:\n                showscalebar = (heightofone * scalebar[0], scalebar[1])\n        else:\n            raise ValueError(\"Invalid datatype of %s\" % datatype)\n        f.close()\n\n        # create web logo\n        charstring = ''.join(chars_for_string)\n        assert len(charstring) == len(chars_for_string), \"Length of charstring doesn't match length of chars_for_string. Do you have unallowable multi-letter characters?\\n%s\" % (str(chars_for_string))\n        logoprior = weblogolib.parse_prior('equiprobable', charstring, 0)\n        motif = _my_Motif.read_transfac(open(transfacfile), charstring)\n        logodata = weblogolib.LogoData.from_counts(motif.alphabet, motif, logoprior)\n        logo_options = weblogolib.LogoOptions()\n        logo_options.fineprint = None\n        logo_options.stacks_per_line = nperline\n        logo_options.stack_aspect_ratio = aspectratio\n        logo_options.stack_width = stackwidth\n        logo_options.unit_name = 'probability'\n        logo_options.show_yaxis = False\n        logo_options.yaxis_scale = ymax \n        if alphabet_type == 'aa':\n            map_functions = {'kd':KyteDoolittleColorMapping,\n                             'mw': MWColorMapping,\n                             'charge' : ChargeColorMapping,\n                             'functionalgroup':FunctionalGroupColorMapping}\n            map_fcn = map_functions[map_metric]\n            (cmap, colormapping, mapper) = map_fcn(maptype=custom_cmap)\n        elif alphabet_type == 'nt':\n            colormapping = {}\n            colormapping['A'] = '#008000'\n            colormapping['T'] = '#FF0000'\n            colormapping['C'] = '#0000FF'\n            colormapping['G'] = '#FFA500'\n        else:\n            raise ValueError(\"Invalid alphabet_type %s\" % alphabet_type)\n        colormapping[firstblankchar] = colormapping[lastblankchar] = '#000000' # black, but color doesn't matter as modified weblogo code replaces with empty space\n        colormapping[separatorchar] = '#000000' # black\n        color_scheme = weblogolib.colorscheme.ColorScheme()\n        for x in chars_for_string:\n            if hasattr(color_scheme, 'rules'):\n                color_scheme.rules.append(weblogolib.colorscheme.SymbolColor(x, colormapping[x], \"'%s'\" % x))\n            else:\n                # this part is needed for weblogo 3.4\n                color_scheme.groups.append(weblogolib.colorscheme.ColorGroup(x, colormapping[x], \"'%s'\" % x))\n        logo_options.color_scheme = color_scheme\n        logo_options.annotate = [{True:r, False:''}[0 == isite % numberevery] for (isite, r) in enumerate(sites)]\n        logoformat = weblogolib.LogoFormat(logodata, logo_options)\n        # _my_pdf_formatter is modified from weblogo version 3.4 source code\n        # to allow custom ordering of the symbols.\n        pdf = _my_pdf_formatter(logodata, logoformat, ordered_alphabets) \n        with open(plotfile, 'wb') as f:\n            f.write(pdf)\n        assert os.path.isfile(plotfile), \"Failed to find expected plotfile %s\" % plotfile\n    finally:\n        # close if still open\n        try:\n            f.close()\n        except:\n            pass\n        # remove temporary file\n        if os.path.isfile(transfacfile):\n            os.remove(transfacfile)\n\n    # now build the overlay\n    if overlay or showscalebar:\n        try:\n            (fdoverlay, overlayfile) = tempfile.mkstemp(suffix='.pdf')\n            (fdmerged, mergedfile) = tempfile.mkstemp(suffix='.pdf')\n            foverlay = os.fdopen(fdoverlay, 'wb')\n            foverlay.close() # close, but we still have the path overlayfile...\n            fmerged = os.fdopen(fdmerged, 'wb')\n            logoheight = stackwidth * stackaspectratio + stackheightmargin\n            LogoOverlay(sites, overlayfile, overlay, nperline, sitewidth=stackwidth, rmargin=rmargin, logoheight=logoheight, barheight=barheight, barspacing=barspacing, fix_limits=fix_limits, fixlongname=fixlongname, overlay_cmap=overlay_cmap, underlay=underlay, scalebar=showscalebar)\n            plotfile_f = open(plotfile, 'rb')\n            plot = PyPDF2.PdfFileReader(plotfile_f).getPage(0)\n            overlayfile_f = open(overlayfile, 'rb')\n            overlaypdf = PyPDF2.PdfFileReader(overlayfile_f).getPage(0)\n            xshift = overlaypdf.artBox[2] - plot.artBox[2]\n            yshift = (barheight + barspacing) * len(overlay) - 0.5 * barspacing\n            overlaypdf.mergeTranslatedPage(plot, xshift, \n                    yshift * int(underlay), expand=True)\n            overlaypdf.compressContentStreams() \n            output = PyPDF2.PdfFileWriter()\n            output.addPage(overlaypdf)\n            output.write(fmerged)\n            fmerged.close()\n            shutil.move(mergedfile, plotfile)\n        finally:\n            try:\n                plotfile_f.close()\n            except:\n                pass\n            try:\n                overlayfile_f.close()\n            except:\n                pass\n            try:\n                foverlay.close()\n            except:\n                pass\n            try:\n                fmerged.close()\n            except:\n                pass\n            for fname in [overlayfile, mergedfile]:\n                if os.path.isfile(fname):\n                    os.remove(fname)", "response": "Creates a sequence logo plot for the given set of sites and data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _my_pdf_formatter(data, format, ordered_alphabets) :\n    eps = _my_eps_formatter(data, format, ordered_alphabets).decode()\n    gs = weblogolib.GhostscriptAPI()    \n    return gs.convert('pdf', eps, format.logo_width, format.logo_height)", "response": "Generate a logo in PDF format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a new logo in Encapsulated Postscript format.", "response": "def _my_eps_formatter(logodata, format, ordered_alphabets) :\n    \"\"\" Generate a logo in Encapsulated Postscript (EPS)\n    \n    Modified from weblogo version 3.4 source code. \n\n    *ordered_alphabets* is a dictionary keyed by zero-indexed\n    consecutive sites, with values giving order of characters\n    from bottom to top.\n    \"\"\"\n    substitutions = {}\n    from_format =[\n        \"creation_date\",    \"logo_width\",           \"logo_height\",      \n        \"lines_per_logo\",   \"line_width\",           \"line_height\",\n        \"line_margin_right\",\"line_margin_left\",     \"line_margin_bottom\",\n        \"line_margin_top\",  \"title_height\",         \"xaxis_label_height\",\n        \"creator_text\",     \"logo_title\",           \"logo_margin\",\n        \"stroke_width\",     \"tic_length\",           \n        \"stacks_per_line\",  \"stack_margin\",\n        \"yaxis_label\",      \"yaxis_tic_interval\",   \"yaxis_minor_tic_interval\",\n        \"xaxis_label\",      \"xaxis_tic_interval\",   \"number_interval\",\n        \"fineprint\",        \"shrink_fraction\",      \"errorbar_fraction\",\n        \"errorbar_width_fraction\",\n        \"errorbar_gray\",    \"small_fontsize\",       \"fontsize\",\n        \"title_fontsize\",   \"number_fontsize\",      \"text_font\",\n        \"logo_font\",        \"title_font\",          \n        \"logo_label\",       \"yaxis_scale\",          \"end_type\",\n        \"debug\",            \"show_title\",           \"show_xaxis\",\n        \"show_xaxis_label\", \"show_yaxis\",           \"show_yaxis_label\",\n        \"show_boxes\",       \"show_errorbars\",       \"show_fineprint\",\n        \"rotate_numbers\",   \"show_ends\",            \"stack_height\",\n        \"stack_width\"\n        ]\n   \n    for s in from_format :\n        substitutions[s] = getattr(format,s)\n\n    substitutions[\"shrink\"] = str(format.show_boxes).lower()\n\n\n    # --------- COLORS --------------\n    def format_color(color):\n        return  \" \".join( (\"[\",str(color.red) , str(color.green), \n            str(color.blue), \"]\"))  \n\n    substitutions[\"default_color\"] = format_color(format.default_color)\n\n    colors = []  \n    if hasattr(format.color_scheme, 'rules'):\n        grouplist = format.color_scheme.rules\n    else:\n        # this line needed for weblogo 3.4\n        grouplist = format.color_scheme.groups\n    for group in grouplist:\n        cf = format_color(group.color)\n        for s in group.symbols :\n            colors.append( \"  (\"+s+\") \" + cf )\n    substitutions[\"color_dict\"] = \"\\n\".join(colors)\n        \n    data = []\n    \n    # Unit conversion. 'None' for probability units\n    conv_factor = None #JDB\n    #JDB conv_factor = std_units[format.unit_name]\n    \n    data.append(\"StartLine\")\n\n    seq_from = format.logo_start- format.first_index\n    seq_to = format.logo_end - format.first_index +1\n\n    # seq_index : zero based index into sequence data\n    # logo_index : User visible coordinate, first_index based\n    # stack_index : zero based index of visible stacks\n    for seq_index in range(seq_from, seq_to) :\n        logo_index = seq_index + format.first_index \n        stack_index = seq_index - seq_from\n        \n        if stack_index!=0 and (stack_index % format.stacks_per_line) ==0 :\n            data.append(\"\")\n            data.append(\"EndLine\")\n            data.append(\"StartLine\")\n            data.append(\"\")\n        \n        data.append(\"(%s) StartStack\" % format.annotate[seq_index] )\n\n        if conv_factor: \n            stack_height = logodata.entropy[seq_index] * std_units[format.unit_name]\n        else :\n            stack_height = 1.0 # Probability\n\n        # The following code modified by JDB to use ordered_alphabets\n        # and also to replace the \"blank\" characters 'b' and 'B'\n        # by spaces.\n        s_d = dict(zip(logodata.alphabet, logodata.counts[seq_index]))\n        s = []\n        for aa in ordered_alphabets[seq_index]:\n            if aa not in ['B', 'b']:\n                s.append((s_d[aa], aa))\n            else:\n                s.append((s_d[aa], ' '))\n#        s = [(s_d[aa], aa) for aa in ordered_alphabets[seq_index]]\n\n        # Sort by frequency. If equal frequency then reverse alphabetic\n        # (So sort reverse alphabetic first, then frequencty)\n        # TODO: doublecheck this actual works\n        #s = list(zip(logodata.counts[seq_index], logodata.alphabet))\n        #s.sort(key= lambda x: x[1])\n        #s.reverse()\n        #s.sort(key= lambda x: x[0])\n        #if not format.reverse_stacks: s.reverse()\n\n        C = float(sum(logodata.counts[seq_index])) \n        if C > 0.0 :\n            fraction_width = 1.0\n            if format.scale_width :\n                fraction_width = logodata.weight[seq_index] \n            # print(fraction_width, file=sys.stderr)\n            for c in s:\n                data.append(\" %f %f (%s) ShowSymbol\" % (fraction_width, c[0]*stack_height/C, c[1]) )\n\n        # Draw error bar on top of logo. Replaced by DrawErrorbarFirst above.\n        if logodata.entropy_interval is not None and conv_factor and C>0.0:\n\n            low, high = logodata.entropy_interval[seq_index]\n            center = logodata.entropy[seq_index]\n            low *= conv_factor\n            high *= conv_factor\n            center *=conv_factor\n            if high> format.yaxis_scale : high = format.yaxis_scale \n\n            down = (center - low) \n            up   = (high - center) \n            data.append(\" %f %f DrawErrorbar\" % (down, up) )\n            \n        data.append(\"EndStack\")\n        data.append(\"\")\n               \n    data.append(\"EndLine\")\n    substitutions[\"logo_data\"] = \"\\n\".join(data)  \n\n\n    # Create and output logo\n    template = corebio.utils.resource_string( __name__, '_weblogo_template.eps', __file__).decode()\n    logo = string.Template(template).substitute(substitutions)\n\n    return logo.encode()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking an overlay for the given sites and overlayfile.", "response": "def LogoOverlay(sites, overlayfile, overlay, nperline, sitewidth, rmargin, logoheight, barheight, barspacing, fix_limits={}, fixlongname=False, overlay_cmap=None, underlay=False, scalebar=False):\n    \"\"\"Makes overlay for *LogoPlot*.\n\n    This function creates colored bars overlay bars showing up to two\n    properties.\n    The trick of this function is to create the bars the right\n    size so they align when they overlay the logo plot. \n\n    CALLING VARIABLES:\n\n    * *sites* : same as the variable of this name used by *LogoPlot*.\n\n    * *overlayfile* is a string giving the name of created PDF file containing\n      the overlay. It must end in the extension ``.pdf``.\n\n    * *overlay* : same as the variable of this name used by *LogoPlot*.\n\n    * *nperline* : same as the variable of this name used by *LogoPlot*.\n\n    * *sitewidth* is the width of each site in points.\n\n    * *rmargin* is the right margin in points.\n\n    * *logoheight* is the total height of each logo row in points.\n\n    * *barheight* is the total height of each bar in points.\n\n    * *barspacing* is the vertical spacing between bars in points.\n\n    * *fix_limits* has the same meaning of the variable of this name used by *LogoPlot*.\n\n    * *fixlongname* has the same meaning of the variable of this name used by *LogoPlot*.\n\n    * *overlay_cmap* has the same meaning of the variable of this name used by *LogoPlot*.\n\n    * *underlay* is a bool. If `True`, make an underlay rather than an overlay.\n\n    * *scalebar: if not `False`, is 2-tuple `(scalebarheight, scalebarlabel)`\n      where `scalebarheight` is in points.\n    \"\"\"\n    if os.path.splitext(overlayfile)[1] != '.pdf':\n        raise ValueError(\"overlayfile must end in .pdf: %s\" % overlayfile)\n    if not overlay_cmap:\n        (cmap, mapping_d, mapper) = KyteDoolittleColorMapping()\n    else:\n        mapper = pylab.cm.ScalarMappable(cmap=overlay_cmap)\n        cmap = mapper.get_cmap()\n    pts_per_inch = 72.0 # to convert between points and inches\n    # some general properties of the plot\n    matplotlib.rc('text', usetex=True)\n    matplotlib.rc('xtick', labelsize=8)\n    matplotlib.rc('xtick', direction='out')\n    matplotlib.rc('ytick', direction='out')\n    matplotlib.rc('axes', linewidth=0.5)\n    matplotlib.rc('ytick.major', size=3)\n    matplotlib.rc('xtick.major', size=2.5)\n    # define sizes (still in points)\n    colorbar_bmargin = 20 # margin below color bars in points\n    colorbar_tmargin = 15 # margin above color bars in points\n    nlines = int(math.ceil(len(sites) / float(nperline)))\n    lmargin = 25 # left margin in points\n    barwidth = nperline * sitewidth\n    figwidth = lmargin + rmargin + barwidth\n    figheight = nlines * (logoheight + len(overlay) * (barheight +\n            barspacing)) + (barheight + colorbar_bmargin + colorbar_tmargin) + (\n            int(underlay) * len(overlay) * (barheight + barspacing))\n    # set up the figure and axes\n    fig = pylab.figure(figsize=(figwidth / pts_per_inch, figheight / pts_per_inch))\n    # determine property types\n    prop_types = {}\n    for (prop_d, shortname, longname) in overlay:\n        if shortname == longname == 'wildtype':\n            assert all([(isinstance(prop, str) and len(prop) == 1) for \n                    prop in prop_d.values()]), 'prop_d does not give letters'\n            proptype = 'wildtype'\n            (vmin, vmax) = (0, 1) # not used, but need to be assigned\n            propcategories = None # not used, but needs to be assigned\n        elif all([isinstance(prop, str) for prop in prop_d.values()]):\n            proptype = 'discrete'\n            propcategories = list(set(prop_d.values()))\n            propcategories.sort()\n            (vmin, vmax) = (0, len(propcategories) - 1)\n        elif all ([isinstance(prop, (int, float)) for prop in prop_d.values()]):\n            proptype = 'continuous'\n            propcategories = None\n            (vmin, vmax) = (min(prop_d.values()), max(prop_d.values()))\n            # If vmin is slightly greater than zero, set it to zero. This helps for RSA properties.\n            if vmin >= 0 and vmin / float(vmax - vmin) < 0.05:\n                vmin = 0.0\n                # And if vmax is just a bit less than one, set it to that...\n                if 0.9 <= vmax <= 1.0:\n                    vmax = 1.0\n        else:\n            raise ValueError(\"Property %s is neither continuous or discrete. Values are:\\n%s\" % (shortname, str(prop_d.items())))\n        if shortname in fix_limits:\n            (vmin, vmax) = (min(fix_limits[shortname][0]), max(fix_limits[shortname][0]))\n        assert vmin < vmax, \"vmin >= vmax, did you incorrectly use fix_vmin and fix_vmax?\"\n        prop_types[shortname] = (proptype, vmin, vmax, propcategories)\n    assert len(prop_types) == len(overlay), \"Not as many property types as overlays. Did you give the same name (shortname) to multiple properties in the overlay?\"\n    # loop over each line of the multi-lined plot\n    prop_image = {}\n    for iline in range(nlines):\n        isites = sites[iline * nperline : min(len(sites), (iline + 1) * nperline)]\n        xlength = len(isites) * sitewidth\n        logo_ax = pylab.axes([lmargin / figwidth, ((nlines - iline - 1) * (logoheight + len(overlay) * (barspacing + barheight))) / figheight, xlength / figwidth, logoheight / figheight], frameon=False)\n        logo_ax.yaxis.set_ticks_position('none')\n        logo_ax.xaxis.set_ticks_position('none')\n        pylab.yticks([])\n        pylab.xlim(0.5, len(isites) + 0.5)\n        pylab.xticks([])\n        for (iprop, (prop_d, shortname, longname)) in enumerate(overlay):\n            (proptype, vmin, vmax, propcategories) = prop_types[shortname]\n            prop_ax = pylab.axes([\n                    lmargin / figwidth, \n                    ((nlines - iline - 1) * (logoheight + \n                        len(overlay) * (barspacing + barheight)) + \n                        (1 - int(underlay)) * logoheight + int(underlay) * \n                        barspacing + iprop * (barspacing + barheight)) \n                        / figheight, \n                    xlength / figwidth, \n                    barheight / figheight], \n                    frameon=(proptype != 'wildtype'))\n            prop_ax.xaxis.set_ticks_position('none')\n            pylab.xticks([])\n            pylab.xlim((0, len(isites)))\n            pylab.ylim(-0.5, 0.5)\n            if proptype == 'wildtype':\n                pylab.yticks([])\n                prop_ax.yaxis.set_ticks_position('none')\n                for (isite, site) in enumerate(isites):\n                    pylab.text(isite + 0.5, -0.5, prop_d[site], size=9, \n                            horizontalalignment='center', family='monospace')\n                continue\n            pylab.yticks([0], [shortname], size=8)\n            prop_ax.yaxis.set_ticks_position('left')\n            propdata = pylab.zeros(shape=(1, len(isites)))\n            propdata[ : ] = pylab.nan # set to nan for all entries\n            for (isite, site) in enumerate(isites):\n                if site in prop_d:\n                    if proptype == 'continuous':\n                        propdata[(0, isite)] = prop_d[site]\n                    elif proptype == 'discrete':\n                        propdata[(0, isite)] = propcategories.index(prop_d[site])\n                    else:\n                        raise ValueError('neither continuous nor discrete')\n            prop_image[shortname] = pylab.imshow(propdata, interpolation='nearest', aspect='auto', extent=[0, len(isites), 0.5, -0.5], cmap=cmap, vmin=vmin, vmax=vmax)\n            pylab.yticks([0], [shortname], size=8)\n\n    # set up colorbar axes, then color bars\n    ncolorbars = len([p for p in prop_types.values() if p[0] != 'wildtype'])\n    if scalebar:\n        ncolorbars += 1\n    if ncolorbars == 1:\n        colorbarwidth = 0.4\n        colorbarspacingwidth = 1.0 - colorbarwidth\n    elif ncolorbars:\n        colorbarspacingfrac = 0.5 # space between color bars is this fraction of bar width\n        colorbarwidth = 1.0 / (ncolorbars * (1.0 + colorbarspacingfrac)) # width of color bars in fraction of figure width\n        colorbarspacingwidth = colorbarwidth * colorbarspacingfrac # width of color bar spacing in fraction of figure width\n    # bottom of color bars\n    ybottom = 1.0 - (colorbar_tmargin + barheight) / figheight\n    propnames = {}\n    icolorbar = -1\n    icolorbarshift = 0\n    while icolorbar < len(overlay):\n        if icolorbar == -1:\n            # show scale bar if being used\n            icolorbar += 1\n            if scalebar:\n                (scalebarheight, scalebarlabel) = scalebar\n                xleft = (colorbarspacingwidth * 0.5 + icolorbar *\n                        (colorbarwidth + colorbarspacingwidth))\n                ytop = 1 - colorbar_tmargin / figheight\n                scalebarheightfrac = scalebarheight / figheight\n                # follow here for fig axes: https://stackoverflow.com/a/5022412\n                fullfigax = pylab.axes([0, 0, 1, 1], facecolor=(1, 1, 1, 0))\n                fullfigax.axvline(x=xleft, ymin=ytop - scalebarheightfrac,\n                        ymax=ytop, color='black', linewidth=1.5)\n                pylab.text(xleft + 0.005, ytop - scalebarheightfrac / 2.0,\n                        scalebarlabel, verticalalignment='center',\n                        horizontalalignment='left',\n                        transform=fullfigax.transAxes)\n            continue\n\n        (prop_d, shortname, longname) = overlay[icolorbar]\n        icolorbar += 1\n        (proptype, vmin, vmax, propcategories) = prop_types[shortname]\n        if proptype == 'wildtype':\n            icolorbarshift += 1\n            continue\n        if shortname == longname or not longname:\n            propname = shortname\n        elif fixlongname:\n            propname = longname\n        else:\n            propname = \"%s (%s)\" % (longname, shortname)\n        colorbar_ax = pylab.axes([colorbarspacingwidth * 0.5 + (icolorbar - icolorbarshift - int(not bool(scalebar))) * (colorbarwidth + colorbarspacingwidth), ybottom, colorbarwidth, barheight / figheight], frameon=True)\n        colorbar_ax.xaxis.set_ticks_position('bottom')\n        colorbar_ax.yaxis.set_ticks_position('none')\n        pylab.xticks([])\n        pylab.yticks([])\n        pylab.title(propname, size=9)\n        if proptype == 'continuous':\n            cb = pylab.colorbar(prop_image[shortname], cax=colorbar_ax, orientation='horizontal')\n            # if range is close to zero to one, manually set tics to 0, 0.5, 1. This helps for RSA\n            if -0.1 <= vmin <= 0 and 1.0 <= vmax <= 1.15:\n                cb.set_ticks([0, 0.5, 1])\n                cb.set_ticklabels(['0', '0.5', '1'])\n            # if it seems plausible, set integer ticks\n            if 4 < (vmax - vmin) <= 11:\n                fixedticks = [itick for itick in range(int(vmin), int(vmax) + 1)]\n                cb.set_ticks(fixedticks)\n                cb.set_ticklabels([str(itick) for itick in fixedticks])\n        elif proptype == 'discrete':\n            cb = pylab.colorbar(prop_image[shortname], cax=colorbar_ax, orientation='horizontal', boundaries=[i for i in range(len(propcategories) + 1)], values=[i for i in range(len(propcategories))])\n            cb.set_ticks([i + 0.5 for i in range(len(propcategories))])\n            cb.set_ticklabels(propcategories)\n        else:\n            raise ValueError(\"Invalid proptype\")\n        if shortname in fix_limits:\n            (ticklocs, ticknames) = fix_limits[shortname]\n            cb.set_ticks(ticklocs)\n            cb.set_ticklabels(ticknames)\n\n    # save the plot\n    pylab.savefig(overlayfile, transparent=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a sequence matrix from a file.", "response": "def read_transfac( fin, alphabet = None) :\n        \"\"\" Parse a sequence matrix from a file. \n        Returns a tuple of (alphabet, matrix)\n        \"\"\"\n   \n        items = []\n\n        start=True\n        for line in fin :\n            if line.isspace() or line[0] =='#' : continue\n            stuff = line.split()\n            if start and stuff[0] != 'PO' and stuff[0] != 'P0': continue\n            if stuff[0]=='XX' or stuff[0]=='//': break\n            start = False\n            items.append(stuff)\n        if len(items) < 2  :\n            raise ValueError(\"Vacuous file.\")\n\n        # Is the first line a header line?\n        header = items.pop(0)\n        hcols = len(header)\n        rows = len(items)\n        cols = len(items[0])\n        if not( header[0] == 'PO' or header[0] =='P0' or hcols == cols-1 or hcols == cols-2) :\n            raise ValueError(\"Missing header line!\")\n\n        # Do all lines (except the first) contain the same number of items?\n        cols = len(items[0])\n        for i in range(1, len(items)) :\n            if cols != len(items[i]) :\n                raise ValueError(\"Inconsistant length, row %d: \" % i)\n\n        # Vertical or horizontal arrangement?\n        if header[0] == 'PO' or header[0] == 'P0': header.pop(0)\n\n        position_header = True    \n        alphabet_header = True    \n        for h in header :\n            if not corebio.utils.isint(h) : position_header = False\n#allow non-alphabetic            if not str.isalpha(h) : alphabet_header = False\n\n        if not position_header and not alphabet_header :\n            raise ValueError(\"Can't parse header: %s\" % str(header))\n\n        if position_header and alphabet_header :\n            raise ValueError(\"Can't parse header\")\n\n\n        # Check row headers\n        if alphabet_header :\n            for i,r in enumerate(items) :\n                if not corebio.utils.isint(r[0]) and r[0][0]!='P' : \n                    raise ValueError(\n                        \"Expected position as first item on line %d\" % i)\n                r.pop(0)\n                defacto_alphabet = ''.join(header)\n        else :\n            a = []\n            for i,r in enumerate(items) :\n                if not ischar(r[0]) and r[0][0]!='P' : \n                    raise ValueError(\n                        \"Expected position as first item on line %d\" % i)\n                a.append(r.pop(0))\n            defacto_alphabet = ''.join(a)                \n\n        # Check defacto_alphabet\n        defacto_alphabet = corebio.seq.Alphabet(defacto_alphabet)\n\n        if alphabet :\n            if not defacto_alphabet.alphabetic(alphabet) :\n                raise ValueError(\"Incompatible alphabets: %s , %s (defacto)\"\n                                 % (alphabet, defacto_alphabet))\n        else :            \n            alphabets = (unambiguous_rna_alphabet,\n                        unambiguous_dna_alphabet,                      \n                        unambiguous_protein_alphabet,\n                      )\n            for a in alphabets :\n                if defacto_alphabet.alphabetic(a) :\n                    alphabet = a\n                    break\n            if not alphabet :\n                alphabet = defacto_alphabet\n   \n\n        # The last item of each row may be extra cruft. Remove\n        if len(items[0]) == len(header) +1 :\n            for r in items :\n                r.pop()\n\n        # items should now be a list of lists of numbers (as strings) \n        rows = len(items)\n        cols = len(items[0])\n        matrix = numpy.zeros( (rows,cols) , dtype=numpy.float64) \n        for r in range( rows) :\n            for c in range(cols):\n                matrix[r,c] = float( items[r][c]) \n\n        if position_header :\n            matrix.transpose() \n\n        return _my_Motif(defacto_alphabet, matrix).reindex(alphabet)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pruneToAtoms(self, atoms):\n        _atoms = self.atoms[:]\n        for atom in _atoms:\n            if atom not in atoms:\n                self.remove_atom(atom)", "response": "Prune the molecule to the specified atoms."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poll(connection: connection, timeout: float=1.0) -> Iterable[Event]:\n\n    if timeout > 0.0:\n        log('Polling for events (Blocking, {} seconds)...'.format(timeout), logger_name=_LOGGER_NAME)\n    else:\n        log('Polling for events (Non-Blocking)...', logger_name=_LOGGER_NAME)\n    if select.select([connection], [], [], timeout) == ([], [], []):\n        log('...No events found', logger_name=_LOGGER_NAME)\n        return\n    else:\n        log('Events', logger_name=_LOGGER_NAME)\n        log('------', logger_name=_LOGGER_NAME)\n        connection.poll()\n        while connection.notifies:\n            event = connection.notifies.pop(0)\n            log(str(event), logger_name=_LOGGER_NAME)\n            yield Event.fromjson(event.payload)", "response": "Poll the connection for events and return them as an iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Event from a psycopg2 - pgevent event JSON.", "response": "def fromjson(cls, json_string: str) -> 'Event':\n        \"\"\"Create a new Event from a from a psycopg2-pgevent event JSON.\n\n        Parameters\n        ----------\n        json_string: str\n            Valid psycopg2-pgevent event JSON.\n\n        Returns\n        -------\n        Event\n            Event created from JSON deserialization.\n\n        \"\"\"\n        obj = json.loads(json_string)\n        return cls(\n            UUID(obj['event_id']),\n            obj['event_type'],\n            obj['schema_name'],\n            obj['table_name'],\n            obj['row_id']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes an Event into JSON.", "response": "def tojson(self) -> str:\n        \"\"\"Serialize an Event into JSON.\n\n        Returns\n        -------\n        str\n            JSON-serialized Event.\n\n        \"\"\"\n        return json.dumps({\n            'event_id': str(self.id),\n            'event_type': self.type,\n            'schema_name': self.schema_name,\n            'table_name': self.table_name,\n            'row_id': self.row_id\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef input_data(self):\n        ordererd = np.sort(np.asarray(list(self.plot_info.keys())).astype(int))\n\n        trans_cont_dict = OrderedDict()\n        for i in ordererd:\n            trans_cont_dict[str(i)] = self.plot_info[str(i)]\n\n        self.plot_info = trans_cont_dict\n\n        # set empty lists for x,y,z\n        x = [[]for i in np.arange(len(self.plot_info.keys()))]\n        y = [[] for i in np.arange(len(self.plot_info.keys()))]\n        z = [[] for i in np.arange(len(self.plot_info.keys()))]\n\n        # read in base files/data\n        for k, axis_string in enumerate(self.plot_info.keys()):\n\n            if 'file' not in self.plot_info[axis_string].keys():\n                continue\n            for j, file_dict in enumerate(self.plot_info[axis_string]['file']):\n                data_class = ReadInData(**{**self.general, **file_dict,\n                                           **self.plot_info[axis_string]['limits']})\n\n                x[k].append(data_class.x_append_value)\n                y[k].append(data_class.y_append_value)\n                z[k].append(data_class.z_append_value)\n\n            # print(axis_string)\n\n        # add data from plots to current plot based on index\n        for k, axis_string in enumerate(self.plot_info.keys()):\n\n            # takes first file from plot\n            if 'indices' in self.plot_info[axis_string]:\n                if type(self.plot_info[axis_string]['indices']) == int:\n                    self.plot_info[axis_string]['indices'] = (\n                        [self.plot_info[axis_string]['indices']])\n\n                for index in self.plot_info[axis_string]['indices']:\n\n                    index = int(index)\n\n                    x[k].append(x[index][0])\n                    y[k].append(y[index][0])\n                    z[k].append(z[index][0])\n\n        # read or append control values for ratio plots\n        for k, axis_string in enumerate(self.plot_info.keys()):\n            if 'control' in self.plot_info[axis_string]:\n                if ('name' in self.plot_info[axis_string]['control'] or\n                        'label' in self.plot_info[axis_string]['control']):\n                    file_dict = self.plot_info[axis_string]['control']\n                    if 'limits' in self.plot_info[axis_string].keys():\n                        liimits_dict = self.plot_info[axis_string]['limits']\n\n                    data_class = ReadInData(**{**self.general, **file_dict,\n                                               **self.plot_info[axis_string]['limits']})\n                    x[k].append(data_class.x_append_value)\n                    y[k].append(data_class.y_append_value)\n                    z[k].append(data_class.z_append_value)\n\n                elif 'index' in self.plot_info[axis_string]['control']:\n                    index = int(self.plot_info[axis_string]['control']['index'])\n\n                    x[k].append(x[index][0])\n                    y[k].append(y[index][0])\n                    z[k].append(z[index][0])\n\n            # print(axis_string)\n\n        # transfer lists in PlotVals class.\n        value_classes = []\n        for k in range(len(x)):\n            value_classes.append(PlotVals(x[k], y[k], z[k]))\n\n        self.value_classes = value_classes\n        return", "response": "Function to extract data from files according to pid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting up the initial figure on to which every plot is added.", "response": "def setup_figure(self):\n        \"\"\"Sets up the initial figure on to which every plot is added.\n\n        \"\"\"\n\n        # declare figure and axes environments\n        fig, ax = plt.subplots(nrows=int(self.num_rows),\n                               ncols=int(self.num_cols),\n                               sharex=self.sharex,\n                               sharey=self.sharey)\n\n        fig.set_size_inches(self.figure_width, self.figure_height)\n\n        # create list of ax. Catch error if it is a single plot.\n        try:\n            ax = ax.ravel()\n        except AttributeError:\n            ax = [ax]\n\n        # create list of plot types\n        self.plot_types = [self.plot_info[str(i)]['plot_type'] for i in range(len(ax))]\n\n        if len(self.plot_types) == 1:\n            if self.plot_types[0] not in self.colorbars:\n                self.colorbars[self.plot_types[0]] = {'cbar_pos': 5}\n            else:\n                if 'cbar_pos' not in self.colorbars[self.plot_types[0]]:\n                    self.colorbars[self.plot_types[0]]['cbar_pos'] = 5\n\n        # prepare colorbar classes\n        self.colorbar_classes = {}\n        for plot_type in self.plot_types:\n            if plot_type in self.colorbar_classes:\n                continue\n            if plot_type == 'Horizon':\n                self.colorbar_classes[plot_type] = None\n\n            elif plot_type in self.colorbars:\n                self.colorbar_classes[plot_type] = FigColorbar(fig, plot_type,\n                                                               **self.colorbars[plot_type])\n\n            else:\n                self.colorbar_classes[plot_type] = FigColorbar(fig, plot_type)\n\n        # set subplots_adjust settings\n        if 'Ratio' in self.plot_types or 'Waterfall':\n            self.subplots_adjust_kwargs['right'] = 0.79\n\n        # adjust figure sizes\n        fig.subplots_adjust(**self.subplots_adjust_kwargs)\n\n        if 'fig_y_label' in self.__dict__.keys():\n            fig.text(self.fig_y_label_x,\n                     self.fig_y_label_y,\n                     r'{}'.format(self.fig_y_label),\n                     **self.fig_y_label_kwargs)\n\n        if 'fig_x_label' in self.__dict__.keys():\n            fig.text(self.fig_x_label_x,\n                     self.fig_x_label_y,\n                     r'{}'.format(self.fig_x_label),\n                     **self.fig_x_label_kwargs)\n\n        if 'fig_title' in self.__dict__.keys():\n            fig.text(self.fig_title_kwargs['x'],\n                     self.fig_title_kwargs['y'],\n                     r'{}'.format(self.fig_title),\n                     **self.fig_title_kwargs)\n\n        self.fig, self.ax = fig, ax\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating plots according to each plotting class.", "response": "def create_plots(self):\n        \"\"\"Creates plots according to each plotting class.\n\n        \"\"\"\n        for i, axis in enumerate(self.ax):\n            # plot everything. First check general dict for parameters related to plots.\n            trans_plot_class_call = globals()[self.plot_types[i]]\n            trans_plot_class = trans_plot_class_call(self.fig, axis,\n                                                     self.value_classes[i].x_arr_list,\n                                                     self.value_classes[i].y_arr_list,\n                                                     self.value_classes[i].z_arr_list,\n                                                     colorbar=(\n                                                        self.colorbar_classes[self.plot_types[i]]),\n                                                     **{**self.general,\n                                                        **self.figure,\n                                                        **self.plot_info[str(i)],\n                                                        **self.plot_info[str(i)]['limits'],\n                                                        **self.plot_info[str(i)]['label'],\n                                                        **self.plot_info[str(i)]['extra'],\n                                                        **self.plot_info[str(i)]['legend']})\n\n            # create the plot\n            trans_plot_class.make_plot()\n\n            # setup the plot\n            trans_plot_class.setup_plot()\n\n            # print(\"Axis\", i, \"Complete\")\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a table for n - gram of a give cardinality.", "response": "def create_ngram_table(self, cardinality):\n        \"\"\"\n        Creates a table for n-gram of a give cardinality. The table name is\n        constructed from this parameter, for example for cardinality `2` there\n        will be a table `_2_gram` created.\n\n        Parameters\n        ----------\n        cardinality : int\n            The cardinality to create a table for.\n\n        \"\"\"\n        query = \"CREATE TABLE IF NOT EXISTS _{0}_gram (\".format(cardinality)\n        unique = \"\"\n        for i in reversed(range(cardinality)):\n            if i != 0:\n                unique += \"word_{0}, \".format(i)\n                query += \"word_{0} TEXT, \".format(i)\n            else:\n                unique += \"word\"\n                query += \"word TEXT, count INTEGER, UNIQUE({0}) );\".format(\n                    unique)\n\n        self.execute_sql(query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an index for the table with the given cardinality.", "response": "def create_index(self, cardinality):\n        \"\"\"\n        Create an index for the table with the given cardinality.\n\n        Parameters\n        ----------\n        cardinality : int\n            The cardinality to create a index for.\n\n        \"\"\"\n        for i in reversed(range(cardinality)):\n            if i != 0:\n                query = \"CREATE INDEX idx_{0}_gram_{1} ON _{0}_gram(word_{1});\".format(cardinality, i)\n                self.execute_sql(query)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all ngrams in the table.", "response": "def ngrams(self, with_counts=False):\n        \"\"\"\n        Returns all ngrams that are in the table.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        ngrams : generator\n            A generator for ngram tuples.\n\n        \"\"\"\n        query = \"SELECT \"\n        for i in reversed(range(self.cardinality)):\n            if i != 0:\n                query += \"word_{0}, \".format(i)\n            elif i == 0:\n                query += \"word\"\n\n        if with_counts:\n            query += \", count\"\n\n        query += \" FROM _{0}_gram;\".format(self.cardinality)\n\n        result = self.execute_sql(query)\n        for row in result:\n            yield tuple(row)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ngram_count(self, ngram):\n        query = \"SELECT count FROM _{0}_gram\".format(len(ngram))\n        query += self._build_where_clause(ngram)\n        query += \";\"\n\n        result = self.execute_sql(query)\n\n        return self._extract_first_integer(result)", "response": "Gets the count of a given ngram from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_ngram(self, ngram, count):\n        query = \"INSERT INTO _{0}_gram {1};\".format(len(ngram),\n            self._build_values_clause(ngram, count))\n        self.execute_sql(query)", "response": "Inserts a given n - gram with count into the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a given ngram in the database.", "response": "def update_ngram(self, ngram, count):\n        \"\"\"\n        Updates a given ngram in the database. The ngram has to be in the\n        database, otherwise this method will stop with an error.\n\n        Parameters\n        ----------\n        ngram : iterable of str\n            A list, set or tuple of strings.\n        count : int\n            The count for the given n-gram.\n\n        \"\"\"\n        query = \"UPDATE _{0}_gram SET count = {1}\".format(len(ngram), count)\n        query += self._build_where_clause(ngram)\n        query += \";\"\n        self.execute_sql(query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_ngram(self, ngram):\n        query = \"DELETE FROM _{0}_gram\".format(len(ngram))\n        query += self._build_where_clause(ngram)\n        query += \";\"\n        self.execute_sql(query)", "response": "Removes a given ngram from the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_sql(self, query):\n        c = self.con.cursor()\n        c.execute(query)\n        result = c.fetchall()\n        return result", "response": "Executes a given SQL query on an open sqlite database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an empty database if it does not exist.", "response": "def create_database(self):\n        \"\"\"\n        Creates an empty database if not exists.\n        \n        \"\"\"\n        if not self._database_exists():\n            con = psycopg2.connect(host=self.host, database=\"postgres\",\n                user=self.user, password=self.password, port=self.port)\n            con.set_isolation_level(\n                psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n            query = \"CREATE DATABASE {0};\".format(self.dbname)\n            c = con.cursor()\n            c.execute(query)            \n            con.close()\n\n\n            if self.normalize:\n                self.open_database()\n                query = \"CREATE EXTENSION IF NOT EXISTS \\\"plperlu\\\";\"\n                self.execute_sql(query)\n    #            query = \"\"\"CREATE OR REPLACE FUNCTION normalize(str text)\n    #RETURNS text\n    #AS $$\n    #import unicodedata\n    #return ''.join(c for c in unicodedata.normalize('NFKD', str)\n    #if unicodedata.category(c) != 'Mn')\n    #$$ LANGUAGE plpython3u IMMUTABLE;\"\"\"\n    #             query = \"\"\"CREATE OR REPLACE FUNCTION normalize(mystr text)\n    #   RETURNS text\n    # AS $$\n    #     from unidecode import unidecode\n    #     return unidecode(mystr.decode(\"utf-8\"))\n    # $$ LANGUAGE plpythonu IMMUTABLE;\"\"\"\n                query = \"\"\"CREATE OR REPLACE FUNCTION normalize(text)\n      RETURNS text\n    AS $$\n        use Text::Unidecode;\n        return unidecode(shift);\n    $$ LANGUAGE plperlu IMMUTABLE;\"\"\"\n                self.execute_sql(query)\n                self.commit()\n                self.close_database()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_index(self, cardinality):\n        DatabaseConnector.create_index(self, cardinality)\n        query = \"CREATE INDEX idx_{0}_gram_varchar ON _{0}_gram(word varchar_pattern_ops);\".format(cardinality)\n        self.execute_sql(query)\n\n        if self.lowercase:\n\n            for i in reversed(range(cardinality)):\n                if i != 0:\n                    query = \"CREATE INDEX idx_{0}_gram_{1}_lower ON _{0}_gram(LOWER(word_{1}));\".format(cardinality, i)\n                    self.execute_sql(query)\n\n            if self.normalize:\n\n                query = \"CREATE INDEX idx_{0}_gram_lower_normalized_varchar ON _{0}_gram(NORMALIZE(LOWER(word)) varchar_pattern_ops);\".format(cardinality)\n                self.execute_sql(query)\n\n            else:\n\n                query = \"CREATE INDEX idx_{0}_gram_lower_varchar ON _{0}_gram(LOWER(word) varchar_pattern_ops);\".format(cardinality)\n                self.execute_sql(query)\n\n        elif self.normalize:\n\n            query = \"CREATE INDEX idx_{0}_gram_normalized_varchar ON _{0}_gram(NORMALIZE(word) varchar_pattern_ops);\".format(cardinality)\n            self.execute_sql(query)", "response": "Create an index for the table with the given cardinality."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_index(self, cardinality):\n        DatabaseConnector.delete_index(self, cardinality)\n\n        query = \"DROP INDEX IF EXISTS idx_{0}_gram_varchar;\".format(cardinality)\n        self.execute_sql(query)\n        query = \"DROP INDEX IF EXISTS idx_{0}_gram_normalized_varchar;\".format(\n            cardinality)\n        self.execute_sql(query)\n        query = \"DROP INDEX IF EXISTS idx_{0}_gram_lower_varchar;\".format(\n            cardinality)\n        self.execute_sql(query)\n        query = \"DROP INDEX IF EXISTS idx_{0}_gram_lower_normalized_varchar;\".\\\n            format(cardinality)\n        self.execute_sql(query)\n        for i in reversed(range(cardinality)):\n            if i != 0:\n                query = \"DROP INDEX IF EXISTS idx_{0}_gram_{1}_lower;\".format(\n                    cardinality, i)\n                self.execute_sql(query)", "response": "Delete the index for the given cardinality."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening the sqlite database.", "response": "def open_database(self):\n        \"\"\"\n        Opens the sqlite database.\n\n        \"\"\"\n        if not self.con:\n            try:\n                self.con = psycopg2.connect(host=self.host,\n                    database=self.dbname, user=self.user,\n                    password=self.password, port=self.port)\n            except psycopg2.Error as e:\n                print(\"Error while opening database:\")\n                print(e.pgerror)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_sql(self, query):\n        c = self.con.cursor()\n        c.execute(query)\n        result = []\n        if c.rowcount > 0:\n            try:\n                result = c.fetchall()\n            except psycopg2.ProgrammingError:\n                pass\n        return result", "response": "Executes a given SQL query on an open postgres database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _database_exists(self):\n        con = psycopg2.connect(host=self.host, database=\"postgres\",\n            user=self.user, password=self.password, port=self.port)\n        query_check = \"select datname from pg_catalog.pg_database\"\n        query_check += \" where datname = '{0}';\".format(self.dbname)\n        c = con.cursor()\n        c.execute(query_check)\n        result = c.fetchall()\n        if len(result) > 0:\n            return True\n        return False", "response": "Check if the database exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getChirality(self, order):\n        indices = tuple([self._initialOrder.index(atom.handle)\n                         for atom in order])\n        same = chiral_table[indices]\n        if same:\n            return self.chirality\n        else:\n            if self.chirality == \"@\": return \"@@\"\n            else: return \"@\"", "response": "getChirality - returns the chirality of a given order of the chiral table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging a message to the CEF log.", "response": "def log_cef(name, severity=logging.INFO, env=None, username='none',\n            signature=None, **kwargs):\n    \"\"\"\n    Wraps cef logging function so we don't need to pass in the config\n    dictionary every time. See bug 707060. ``env`` can be either a request\n    object or just the request.META dictionary.\n    \"\"\"\n\n    cef_logger = commonware.log.getLogger('cef')\n\n    c = {'product': settings.CEF_PRODUCT,\n         'vendor': settings.CEF_VENDOR,\n         'version': settings.CEF_VERSION,\n         'device_version': settings.CEF_DEVICE_VERSION}\n\n    # The CEF library looks for some things in the env object like\n    # REQUEST_METHOD and any REMOTE_ADDR stuff.  Django not only doesn't send\n    # half the stuff you'd expect, but it specifically doesn't implement\n    # readline on its FakePayload object so these things fail.  I have no idea\n    # if that's outdated code in Django or not, but andym made this\n    # <strike>awesome</strike> less crappy so the tests will actually pass.\n    # In theory, the last part of this if() will never be hit except in the\n    # test runner.  Good luck with that.\n    if isinstance(env, HttpRequest):\n        r = env.META.copy()\n    elif isinstance(env, dict):\n        r = env\n    else:\n        r = {}\n\n    # Drop kwargs into CEF config array, then log.\n    c['environ'] = r\n    c.update({\n        'username': username,\n        'signature': signature,\n        'data': kwargs,\n    })\n\n    cef_logger.log(severity, name, c)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the changedir for the current environment", "response": "def get_changedir(env):\n    \"changedir = {envdir}\"\n    from ctox.subst import replace_braces\n    changedir = _get_env_maybe(env, 'testenv', 'changedir')\n    if changedir:\n        return replace_braces(changedir, env)\n    else:\n        return env.toxinidir"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self, listener, pass_signal=False):\n        info = listenerinfo(listener, pass_signal)\n        self._listeners.append(info)\n        _logger.debug(\"connect %r to %r\", str(listener), self._name)\n        # Track listeners in the instances only\n        if inspect.ismethod(listener):\n            listener_object = listener.__self__\n            # Ensure that the instance has __listeners__ property\n            if not hasattr(listener_object, \"__listeners__\"):\n                listener_object.__listeners__ = collections.defaultdict(list)\n            # Append the signals a listener is connected to\n            listener_object.__listeners__[listener].append(self)", "response": "Connect a new listener to this signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisconnects an existing listener from this signal.", "response": "def disconnect(self, listener, pass_signal=False):\n        \"\"\"\n        Disconnect an existing listener from this signal\n\n        :param listener:\n            The listener (callable) to remove\n        :param pass_signal:\n            An optional argument that controls if the signal object is\n            explicitly passed to this listener when it is being fired.\n            If enabled, a ``signal=`` keyword argument is passed to the\n            listener function.\n\n            Here, this argument simply aids in disconnecting the right\n            listener. Make sure to pass the same value as was passed to\n            :meth:`connect()`\n        :raises ValueError:\n            If the listener (with the same value of pass_signal) is not present\n        :returns:\n            None\n        \"\"\"\n        info = listenerinfo(listener, pass_signal)\n        self._listeners.remove(info)\n        _logger.debug(\n            \"disconnect %r from %r\", str(listener), self._name)\n        if inspect.ismethod(listener):\n            listener_object = listener.__self__\n            if hasattr(listener_object, \"__listeners__\"):\n                listener_object.__listeners__[listener].remove(self)\n                # Remove the listener from the list if any signals connected\n                if (len(listener_object.__listeners__[listener])) == 0:\n                    del listener_object.__listeners__[listener]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfire this signal with the specified arguments and keyword arguments.", "response": "def fire(self, args, kwargs):\n        \"\"\"\n        Fire this signal with the specified arguments and keyword arguments.\n\n        Typically this is used by using :meth:`__call__()` on this object which\n        is more natural as it does all the argument packing/unpacking\n        transparently.\n        \"\"\"\n        for info in self._listeners[:]:\n            if info.pass_signal:\n                info.listener(*args, signal=self, **kwargs)\n            else:\n                info.listener(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef watchSignal(self, signal):\n        self._extend_state()\n\n        def signal_handler(*args, **kwargs):\n            self._events_seen.append((signal, args, kwargs))\n        signal.connect(signal_handler)\n        if hasattr(self, 'addCleanup'):\n            self.addCleanup(signal.disconnect, signal_handler)", "response": "Setup provisions to watch a specified signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasserting that a signal was fired with appropriate arguments.", "response": "def assertSignalFired(self, signal, *args, **kwargs):\n        \"\"\"\n        Assert that a signal was fired with appropriate arguments.\n\n        :param signal:\n            The :class:`Signal` that should have been fired.\n            Typically this is ``SomeClass.on_some_signal`` reference\n        :param args:\n            List of positional arguments passed to the signal handler\n        :param kwargs:\n            List of keyword arguments passed to the signal handler\n        :returns:\n            A 3-tuple (signal, args, kwargs) that describes that event\n        \"\"\"\n        event = (signal, args, kwargs)\n        self.assertIn(\n            event, self._events_seen,\n            \"\\nSignal unexpectedly not fired: {}\\n\".format(event))\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assertSignalNotFired(self, signal, *args, **kwargs):\n        event = (signal, args, kwargs)\n        self.assertNotIn(\n            event, self._events_seen,\n            \"\\nSignal unexpectedly fired: {}\\n\".format(event))", "response": "Assert that a signal was not fired with appropriate arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts that a signals were fired in a specific sequence. :param expected_events: A (varadic) list of events describing the signals that were fired Each element is a 3-tuple (signal, args, kwargs) that describes the event. .. note:: If you are using :meth:`assertSignalFired()` then the return value of that method is a single event that can be passed to this method", "response": "def assertSignalOrdering(self, *expected_events):\n        \"\"\"\n        Assert that a signals were fired in a specific sequence.\n\n        :param expected_events:\n            A (varadic) list of events describing the signals that were fired\n            Each element is a 3-tuple (signal, args, kwargs) that describes\n            the event.\n\n        .. note::\n            If you are using :meth:`assertSignalFired()` then the return value\n            of that method is a single event that can be passed to this method\n        \"\"\"\n        expected_order = [self._events_seen.index(event)\n                          for event in expected_events]\n        actual_order = sorted(expected_order)\n        self.assertEqual(\n            expected_order, actual_order,\n            \"\\nExpected order of fired signals:\\n{}\\n\"\n            \"Actual order observed:\\n{}\".format(\n                \"\\n\".join(\n                    \"\\t{}: {}\".format(i, event)\n                    for i, event in enumerate(expected_events, 1)),\n                \"\\n\".join(\n                    \"\\t{}: {}\".format(i, event)\n                    for i, event in enumerate(\n                        (self._events_seen[idx] for idx in actual_order), 1))))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef url(viewname, *args, **kwargs):\n    return reverse(viewname, args=args, kwargs=kwargs)", "response": "Helper for Django s reverse in templates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef urlparams(url_, hash=None, **query):\n    url = urlparse.urlparse(url_)\n    fragment = hash if hash is not None else url.fragment\n\n    # Use dict(parse_qsl) so we don't get lists of values.\n    q = url.query\n    query_dict = dict(urlparse.parse_qsl(smart_str(q))) if q else {}\n    query_dict.update((k, v) for k, v in query.items())\n\n    query_string = _urlencode([(k, v) for k, v in query_dict.items()\n                               if v is not None])\n    new = urlparse.ParseResult(url.scheme, url.netloc, url.path, url.params,\n                               query_string, fragment)\n    return new.geturl()", "response": "Add a fragment and or query paramaters to a URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _urlencode(items):\n    try:\n        return urllib.urlencode(items)\n    except UnicodeEncodeError:\n        return urllib.urlencode([(k, smart_str(v)) for k, v in items])", "response": "A Unicode - safe URLencoder."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef token_handler_str_default(\n        token, dispatcher, node, subnode, sourcepath_stack=(None,)):\n    \"\"\"\n    Standard token handler that will return the value, ignoring any\n    tokens or strings that have been remapped.\n    \"\"\"\n\n    if isinstance(token.pos, int):\n        _, lineno, colno = node.getpos(subnode, token.pos)\n    else:\n        lineno, colno = None, None\n    yield StreamFragment(subnode, lineno, colno, None, sourcepath_stack[-1])", "response": "A token handler that returns the value for anyMacroToken or string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an ES5 file into an AST.", "response": "def read(parser, stream):\n    \"\"\"\n    Return an AST from the input ES5 stream.\n\n    Arguments\n\n    parser\n        A parser instance.\n    stream\n        Either a stream object or a callable that produces one.  The\n        stream object to read from; its 'read' method will be invoked.\n\n        If a callable was provided, the 'close' method on its return\n        value will be called to close the stream.\n    \"\"\"\n\n    source = stream() if callable(stream) else stream\n    try:\n        text = source.read()\n        stream_name = getattr(source, 'name', None)\n        try:\n            result = parser(text)\n        except ECMASyntaxError as e:\n            error_name = repr_compat(stream_name or source)\n            raise type(e)('%s in %s' % (str(e), error_name))\n    finally:\n        if callable(stream):\n            source.close()\n\n    result.sourcepath = stream_name\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites out the node using the unparser into an output stream, and optionally the sourcemap using the sourcemap stream. Ideally, file objects should be passed to the *_stream arguments, so that the name resolution built into the sourcemap builder function will be used. Also, if these file objects are opened using absolute path arguments, enabling the sourcemap_normalize_paths flag will have all paths normalized to their relative form. If the provided streams are not anchored on the filesystem, or that the provide node was generated from a string or in-memory stream, the generation of the sourcemap should be done using the lower level `write` function provided by the sourcemap module, which this method wraps. Alternatively, the top level node should have its sourcepath set to path that this node originated from. Arguments unparser An unparser instance. nodes The Node or list of Nodes to stream to the output stream with the unparser. output_stream Either a stream object or a callable that produces one. The stream object to write to; its 'write' method will be invoked. If a callable was provided, the 'close' method on its return value will be called to close the stream. sourcemap_stream If one is provided, the sourcemap will be written out to it. Like output_stream, it could also be a callable and be handled in the same manner. If this argument is the same as output_stream (note: the return value of any callables are not compared), the stream object that is the same as the output_stream will be used for writing out the source map, and the source map will instead be encoded as a 'data:application/json;base64,' URL. sourcemap_normalize_mappings Flag for the normalization of the sourcemap mappings; Defaults to True to enable a reduction in output size. sourcemap_normalize_paths If set to true, all absolute paths will be converted to the relative form when the sourcemap is generated, if all paths provided are in the absolute form. Defaults to True to enable a reduction in output size. source_mapping_url If unspecified, the default derived path will be written as a sourceMappingURL comment into the output stream. If explicitly specified with a value, that will be written instead. Set to None to disable this.", "response": "def write(\n        unparser, nodes, output_stream, sourcemap_stream=None,\n        sourcemap_normalize_mappings=True,\n        sourcemap_normalize_paths=True,\n        source_mapping_url=NotImplemented):\n    \"\"\"\n    Write out the node using the unparser into an output stream, and\n    optionally the sourcemap using the sourcemap stream.\n\n    Ideally, file objects should be passed to the *_stream arguments, so\n    that the name resolution built into the sourcemap builder function\n    will be used.  Also, if these file objects are opened using absolute\n    path arguments, enabling the sourcemap_normalize_paths flag will\n    have all paths normalized to their relative form.\n\n    If the provided streams are not anchored on the filesystem, or that\n    the provide node was generated from a string or in-memory stream,\n    the generation of the sourcemap should be done using the lower level\n    `write` function provided by the sourcemap module, which this method\n    wraps.  Alternatively, the top level node should have its sourcepath\n    set to path that this node originated from.\n\n    Arguments\n\n    unparser\n        An unparser instance.\n    nodes\n        The Node or list of Nodes to stream to the output stream with\n        the unparser.\n    output_stream\n        Either a stream object or a callable that produces one.  The\n        stream object to write to; its 'write' method will be invoked.\n\n        If a callable was provided, the 'close' method on its return\n        value will be called to close the stream.\n    sourcemap_stream\n        If one is provided, the sourcemap will be written out to it.\n        Like output_stream, it could also be a callable and be handled\n        in the same manner.\n\n        If this argument is the same as output_stream (note: the return\n        value of any callables are not compared), the stream object that\n        is the same as the output_stream will be used for writing out\n        the source map, and the source map will instead be encoded as a\n        'data:application/json;base64,' URL.\n    sourcemap_normalize_mappings\n        Flag for the normalization of the sourcemap mappings; Defaults\n        to True to enable a reduction in output size.\n    sourcemap_normalize_paths\n        If set to true, all absolute paths will be converted to the\n        relative form when the sourcemap is generated, if all paths\n        provided are in the absolute form.\n\n        Defaults to True to enable a reduction in output size.\n    source_mapping_url\n        If unspecified, the default derived path will be written as a\n        sourceMappingURL comment into the output stream.  If explicitly\n        specified with a value, that will be written instead.  Set to\n        None to disable this.\n    \"\"\"\n\n    closer = []\n\n    def get_stream(stream):\n        if callable(stream):\n            result = stream()\n            closer.append(result.close)\n        else:\n            result = stream\n        return result\n\n    def cleanup():\n        for close in reversed(closer):\n            close()\n\n    chunks = None\n    if isinstance(nodes, Node):\n        chunks = unparser(nodes)\n    elif isinstance(nodes, Iterable):\n        raw = [unparser(node) for node in nodes if isinstance(node, Node)]\n        if raw:\n            chunks = chain(*raw)\n\n    if not chunks:\n        raise TypeError('must either provide a Node or list containing Nodes')\n\n    try:\n        out_s = get_stream(output_stream)\n        sourcemap_stream = (\n            out_s if sourcemap_stream is output_stream else sourcemap_stream)\n        mappings, sources, names = sourcemap.write(\n            chunks, out_s, normalize=sourcemap_normalize_mappings)\n        if sourcemap_stream:\n            sourcemap_stream = get_stream(sourcemap_stream)\n            sourcemap.write_sourcemap(\n                mappings, sources, names, out_s, sourcemap_stream,\n                normalize_paths=sourcemap_normalize_paths,\n                source_mapping_url=source_mapping_url,\n            )\n    finally:\n        cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nerases all fixed effects", "response": "def clearFixedEffect(self):\n        \"\"\" erase all fixed effects \"\"\"\n        self.A = []\n        self.F = []\n        self.F_any = np.zeros((self.N,0))\n        self.clear_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addFixedEffect(self,F=None,A=None,index=None):\n        if F is None:\n            F = np.ones((self.N,1))\n        else:\n            assert F.shape[0]==self.N, \"F dimension mismatch\"\n\n        if ((A is None) or ( (A.shape == (self.P,self.P)) and (A==np.eye(self.P)).all() )):\n            #case any effect\n            self.F_any = np.hstack((self.F_any,F))\n        elif (index is not None) and  ((A==self.A[index]).all()):\n            #case common effect\n            self.F[index] = np.hstack((self.F_index,F))\n        else:\n            #case general A\n            assert A.shape[1]==self.P, \"A dimension mismatch\"\n            self.F.append(F)\n            self.A.append(A)\n\n        self.clear_cache()", "response": "Adds fixed effects to the set of sample and trait designs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef XanyKXany(self):\n        result = np.empty((self.P,self.F_any.shape[1],self.F_any.shape[1]), order='C')\n        for p in range(self.P):\n            X1D = self.Fstar_any * self.D[:,p:p+1]\n            X1X2 = X1D.T.dot(self.Fstar_any)\n            result[p] = X1X2\n        return result", "response": "compute self covariance for any ArcGIS class"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes cross covariance for any and rest", "response": "def XanyKX(self):\n        \"\"\"\n        compute cross covariance for any and rest\n        \"\"\"\n        result = np.empty((self.P,self.F_any.shape[1],self.dof), order='C')\n        #This is trivially parallelizable:\n        for p in range(self.P):\n            FanyD = self.Fstar_any * self.D[:,p:p+1]\n            start = 0\n            #This is trivially parallelizable:\n            for term in range(self.len):\n                stop = start + self.F[term].shape[1]*self.A[term].shape[0]\n                result[p,:,start:stop] = self.XanyKX2_single_p_single_term(p=p, F1=FanyD, F2=self.Fstar[term], A2=self.Astar[term])\n                start = stop\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute self covariance for rest", "response": "def XKX(self):\n        \"\"\"\n        compute self covariance for rest\n        \"\"\"\n        cov_beta = np.zeros((self.dof,self.dof))\n        start_row = 0\n        #This is trivially parallelizable:\n        for term1 in range(self.len):\n            stop_row = start_row + self.A[term1].shape[0] * self.F[term1].shape[1]\n            start_col = start_row\n            #This is trivially parallelizable:\n            for term2 in range(term1,self.len):\n                stop_col = start_col + self.A[term2].shape[0] * self.F[term2].shape[1]\n                cov_beta[start_row:stop_row, start_col:stop_col] = compute_X1KX2(Y=self.Ystar(), D=self.D, X1=self.Fstar[term1], X2=self.Fstar[term2], A1=self.Astar[term1], A2=self.Astar[term2])\n                if term1!=term2:\n                    cov_beta[start_col:stop_col, start_row:stop_row] = cov_beta[n_weights1:stop_row, n_weights2:stop_col].T\n                start_col = stop_col\n            start_row = stop_row\n        return cov_beta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting an access token.", "response": "def request_access_token(\n    grant_type,\n    client_id=None,\n    client_secret=None,\n    scopes=None,\n    code=None,\n    refresh_token=None\n):\n    \"\"\"Make an HTTP POST to request an access token.\n    Parameters\n        grant_type (str)\n            Either 'client_credientials' (Client Credentials Grant)\n            or 'authorization_code' (Authorization Code Grant).\n        client_id (str)\n            Your app's Client ID.\n        client_secret (str)\n            Your app's Client Secret.\n        scopes (set)\n            Set of permission scopes to request.\n            (e.g. {'profile', 'history'})\n        code (str)\n            The authorization code to switch for an access token.\n            Only used in Authorization Code Grant.\n        refresh_token (str)\n            Refresh token used to get a new access token.\n            Only used for Authorization Code Grant.\n    Returns\n        (requests.Response)\n            Successful HTTP response from a 'POST' to request\n            an access token.\n    Raises\n        ClientError (APIError)\n            Thrown if there was an HTTP error.\n    \"\"\"\n    url = build_url(auth.SERVER_HOST, auth.ACCESS_TOKEN_PATH)\n\n    if isinstance(scopes, set):\n        scopes = ' '.join(scopes)\n\n    args = {\n        'grant_type': grant_type,\n        'client_id': client_id,\n        'client_secret': client_secret,\n        'scope': scopes,\n        'code': code,\n        'refresh_token': refresh_token,\n    }\n\n    auth_header = HTTPBasicAuth(client_id, client_secret)\n\n    response = post(url=url, auth=auth_header, data=args)\n\n    if response.status_code == codes.ok:\n        return response\n\n    message = 'Failed to request access token: {}.'\n    message = message.format(response.reason)\n    raise ClientError(response, message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing a refresh token to request a new access token.", "response": "def refresh_access_token(credential):\n    \"\"\"Use a refresh token to request a new access token.\n    Not suported for access tokens obtained via Implicit Grant.\n    Parameters\n        credential (OAuth2Credential)\n            An authorized user's OAuth 2.0 credentials.\n    Returns\n        (Session)\n            A new Session object with refreshed OAuth 2.0 credentials.\n    Raises\n        LyftIllegalState (APIError)\n            Raised if OAuth 2.0 grant type does not support\n            refresh tokens.\n    \"\"\"\n    if credential.grant_type == auth.AUTHORIZATION_CODE_GRANT:\n        response = request_access_token(\n            grant_type=auth.REFRESH_TOKEN,\n            client_id=credential.client_id,\n            client_secret=credential.client_secret,\n            refresh_token=credential.refresh_token,\n        )\n\n        oauth2credential = OAuth2Credential.make_from_response(\n            response=response,\n            grant_type=credential.grant_type,\n            client_id=credential.client_id,\n            client_secret=credential.client_secret,\n        )\n\n        return Session(oauth2credential=oauth2credential)\n\n    elif credential.grant_type == auth.CLIENT_CREDENTIAL_GRANT:\n        response = request_access_token(\n            grant_type=auth.CLIENT_CREDENTIAL_GRANT,\n            client_id=credential.client_id,\n            client_secret=credential.client_secret,\n            scopes=credential.scopes,\n        )\n\n        oauth2credential = OAuth2Credential.make_from_response(\n            response=response,\n            grant_type=credential.grant_type,\n            client_id=credential.client_id,\n            client_secret=credential.client_secret,\n        )\n\n        return Session(oauth2credential=oauth2credential)\n\n    message = '{} Grant Type does not support Refresh Tokens.'\n    message = message.format(credential.grant_type)\n    raise LyftIllegalState(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef revoke_access_token(credential):\n    url = build_url(auth.SERVER_HOST, auth.REVOKE_PATH)\n\n    args = {\n        'token': credential.access_token,\n    }\n\n    response = post(url=url, params=args)\n\n    if response.status_code == codes.ok:\n        return\n\n    message = 'Failed to revoke access token: {}.'\n    message = message.format(response.reason)\n    raise ClientError(response, message)", "response": "Revoke an access token."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the URL to request an authorization code or access token.", "response": "def _build_authorization_request_url(\n        self,\n        response_type,\n        state=None\n    ):\n        \"\"\"Form URL to request an auth code or access token.\n        Parameters\n            response_type (str)\n                Only 'code' (Authorization Code Grant) supported at this time\n            state (str)\n                Optional CSRF State token to send to server.\n        Returns\n            (str)\n                The fully constructed authorization request URL.\n        Raises\n            LyftIllegalState (ApiError)\n                Raised if response_type parameter is invalid.\n        \"\"\"\n        if response_type not in auth.VALID_RESPONSE_TYPES:\n            message = '{} is not a valid response type.'\n            raise LyftIllegalState(message.format(response_type))\n\n        args = OrderedDict([\n            ('scope', ' '.join(self.scopes)),\n            ('state', state),\n            ('response_type', response_type),\n            ('client_id', self.client_id),\n        ])\n\n        return build_url(auth.SERVER_HOST, auth.AUTHORIZE_PATH, args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting query parameters from a url.", "response": "def _extract_query(self, redirect_url):\n        \"\"\"Extract query parameters from a url.\n        Parameters\n            redirect_url (str)\n                The full URL that the Lyft server redirected to after\n                the user authorized your app.\n        Returns\n            (dict)\n                A dictionary of query parameters.\n        \"\"\"\n        qs = urlparse(redirect_url)\n\n        # redirect_urls return data after query identifier (?)\n        qs = qs.query\n\n        query_params = parse_qs(qs)\n        query_params = {qp: query_params[qp][0] for qp in query_params}\n\n        return query_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_session(self, redirect_url):\n        query_params = self._extract_query(redirect_url)\n        authorization_code = self._verify_query(query_params)\n\n        response = request_access_token(\n            grant_type=auth.AUTHORIZATION_CODE_GRANT,\n            client_id=self.client_id,\n            client_secret=self.client_secret,\n            code=authorization_code,\n        )\n\n        oauth2credential = OAuth2Credential.make_from_response(\n            response=response,\n            grant_type=auth.AUTHORIZATION_CODE_GRANT,\n            client_id=self.client_id,\n            client_secret=self.client_secret,\n        )\n\n        return Session(oauth2credential=oauth2credential)", "response": "Complete the Authorization Code Grant process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_session(self):\n        response = request_access_token(\n            grant_type=auth.CLIENT_CREDENTIAL_GRANT,\n            client_id=self.client_id,\n            client_secret=self.client_secret,\n            scopes=self.scopes,\n        )\n\n        oauth2credential = OAuth2Credential.make_from_response(\n            response=response,\n            grant_type=auth.CLIENT_CREDENTIAL_GRANT,\n            client_id=self.client_id,\n            client_secret=self.client_secret,\n        )\n\n        return Session(oauth2credential=oauth2credential)", "response": "Create a new session with OAuth 2. 0 credentials."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hello_user(api_client):\n\n    try:\n        response = api_client.get_user_profile()\n\n    except (ClientError, ServerError) as error:\n        fail_print(error)\n        return\n\n    else:\n        profile = response.json\n        user_id = profile.get('id')\n        message = 'Hello. Successfully granted access token to User ID {}.'.format(user_id)\n        success_print(message)", "response": "Use an authorized client to fetch and print profile information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_contour_data(pid):\n    # check if pid is  dicionary or GenInput class\n    # if GenInput, change to dictionary\n    if isinstance(pid, GenInput):\n        pid = pid.return_dict()\n\n    begin_time = time.time()\n\n    WORKING_DIRECTORY = '.'\n    if 'WORKING_DIRECTORY' not in pid['general'].keys():\n        pid['general']['WORKING_DIRECTORY'] = WORKING_DIRECTORY\n\n    # Generate the contour data.\n    running_process = GenProcess(**{**pid, **pid['generate_info']})\n    running_process.set_parameters()\n    running_process.run_snr()\n\n    # Read out\n    file_out = FileReadOut(running_process.xvals, running_process.yvals,\n                           running_process.final_dict,\n                           **{**pid['general'], **pid['generate_info'], **pid['output_info']})\n\n    print('outputing file:', pid['general']['WORKING_DIRECTORY'] + '/'\n          + pid['output_info']['output_file_name'])\n    getattr(file_out, file_out.output_file_type + '_read_out')()\n\n    print(time.time()-begin_time)\n    return", "response": "This function generates the contour data for a given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_message(self, msg):\n\n        if self.socket is not None:\n\n            msg.version = self.PROTOCOL_VERSION\n            serialized = msg.SerializeToString()\n            data = struct.pack(\">I\", len(serialized)) + serialized\n\n            #print(\"Sending message: %s\" % msg)\n            try:\n                self.socket.send(data)\n            except Exception as e:\n                #self.state = \"Disconnected\"\n                pass", "response": "Send a message through the Clementine remote network protocol."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the server defined in the constructor.", "response": "def _connect(self):\n        \"\"\"\n        Connects to the server defined in the constructor.\n        \"\"\"\n\n        self.first_data_sent_complete = False\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((self.host, self.port))\n\n        msg = cr.Message()\n        msg.type = cr.CONNECT\n        msg.request_connect.auth_code = self.auth_code or 0\n        msg.request_connect.send_playlist_songs = False\n        msg.request_connect.downloader = False\n\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a play command to the player.", "response": "def play(self):\n        \"\"\"\n        Sends a \"play\" command to the player.\n        \"\"\"\n        msg = cr.Message()\n        msg.type = cr.PLAY\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pause(self):\n        msg = cr.Message()\n        msg.type = cr.PAUSE\n        self.send_message(msg)", "response": "Send a pause command to the player."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a stop command to the player.", "response": "def stop(self):\n        \"\"\"\n        Sends a \"play\" command to the player.\n        \"\"\"\n        msg = cr.Message()\n        msg.type = cr.STOP\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a playpause command to the player.", "response": "def playpause(self):\n        \"\"\"\n        Sends a \"playpause\" command to the player.\n        \"\"\"\n        msg = cr.Message()\n        msg.type = cr.PLAYPAUSE\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a next command to the player.", "response": "def next(self):\n        \"\"\"\n        Sends a \"next\" command to the player.\n        \"\"\"\n        msg = cr.Message()\n        msg.type = cr.NEXT\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a previous command to the player.", "response": "def previous(self):\n        \"\"\"\n        Sends a \"previous\" command to the player.\n        \"\"\"\n        msg = cr.Message()\n        msg.type = cr.PREVIOUS\n        self.send_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_volume(self, volume):\n        msg = cr.Message()\n        msg.type = cr.SET_VOLUME\n        msg.request_set_volume.volume = int(volume)\n        self.send_message(msg)", "response": "Sets the player volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsolves the T problem for a single class.", "response": "def solve_t(self, Mt):\n        \"\"\"\n        Mt is dim_r x dim_c x d tensor\n        \"\"\"\n        if len(Mt.shape)==2:    _Mt = Mt[:, :, sp.newaxis]\n        else:                   _Mt = Mt\n        M = _Mt.transpose([0,2,1])\n        MLc = sp.tensordot(M, self.Lc().T, (2,0)) \n        MLcLc = sp.tensordot(MLc, self.Lc(), (2,0)) \n        WrMLcWc = sp.tensordot(sp.tensordot(self.Wr(), MLc, (1,0)), self.Wc().T, (2,0))\n        DWrMLcWc = sp.tensordot(self.D()[:,sp.newaxis,:]*WrMLcWc, self.Wc(), (2,0))\n        WrDWrMLcWcLc = sp.tensordot(self.Wr().T, sp.tensordot(DWrMLcWc, self.Lc(), (2,0)), (1,0))\n        RV = (MLcLc - WrDWrMLcWcLc).transpose([0,2,1])\n        if len(Mt.shape)==2:    RV = RV[:, :, 0]\n        return RV"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_commands_list(p):\n    p[0] = p[1]\n\n    # section 3.2: REQUIRE command must come before any other commands\n    if p[2].RULE_IDENTIFIER == 'REQUIRE':\n        if any(command.RULE_IDENTIFIER != 'REQUIRE'\n               for command in p[0].commands):\n            print(\"REQUIRE command on line %d must come before any \"\n                  \"other non-REQUIRE commands\" % p.lineno(2))\n            raise SyntaxError\n\n    # section 3.1: ELSIF and ELSE must follow IF or another ELSIF\n    elif p[2].RULE_IDENTIFIER in ('ELSIF', 'ELSE'):\n        if p[0].commands[-1].RULE_IDENTIFIER not in ('IF', 'ELSIF'):\n            print(\"ELSIF/ELSE command on line %d must follow an IF/ELSIF \"\n                  \"command\" % p.lineno(2))\n            raise SyntaxError\n\n    p[0].commands.append(p[2])", "response": "p is a list of commands"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_command(p):\n    #print(\"COMMAND:\", p[1], p[2], p[3])\n    tests = p[2].get('tests')\n    block = None\n    if p[3] != ';': block = p[3]\n    handler = sifter.handler.get('command', p[1])\n    if handler is None:\n        print(\"No handler registered for command '%s' on line %d\" %\n            (p[1], p.lineno(1)))\n        raise SyntaxError\n    p[0] = handler(arguments=p[2]['args'], tests=tests, block=block)", "response": "command : IDENTIFIER arguments ';'\n               | IDENTIFIER arguments block"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_block(p):\n    # section 3.2: REQUIRE command must come before any other commands,\n    # which means it can't be in the block of another command\n    if any(command.RULE_IDENTIFIER == 'REQUIRE'\n           for command in p[2].commands):\n        print(\"REQUIRE command not allowed inside of a block (line %d)\" %\n            (p.lineno(2)))\n        raise SyntaxError\n    p[0] = p[2]", "response": "block : '{' commands '}'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_arguments(p):\n    p[0] = { 'args' : p[1], }\n    if len(p) > 2:\n        if p[2] == '(':\n            p[0]['tests'] = p[3]\n        else:\n            p[0]['tests'] = [ p[2] ]", "response": "A function to set the arguments of a node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef step_impl03(context):\n    assert len(context.buf) == len(context.fuzzed_buf)\n    count = number_of_modified_bytes(context.buf, context.fuzzed_buf)\n    assert count < 3\n    assert count >= 0", "response": "Check assertions.\n\n    :param context: test context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef step_impl(context, max_modified):\n    assert len(context.buf) == len(context.fuzzed_buf)\n    count = number_of_modified_bytes(context.buf, context.fuzzed_buf)\n    assert count >= 0\n    assert count <= max_modified", "response": "Test that the number of modifications is at most max_modified."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes fuzzer. :param count: number of string variants to generate. :param context: test context.", "response": "def step_impl06(context, count):\n    \"\"\"Execute fuzzer.\n\n    :param count: number of string variants to generate.\n    :param context: test context.\n    \"\"\"\n    fuzz_factor = 11\n    context.fuzzed_string_list = fuzz_string(context.seed, count, fuzz_factor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef step_impl07(context, len_list):\n    assert len(context.fuzzed_string_list) == len_list\n    for fuzzed_string in context.fuzzed_string_list:\n        assert len(context.seed) == len(fuzzed_string)\n        count = number_of_modified_bytes(context.seed, fuzzed_string)\n        assert count >= 0", "response": "Test implementation of NIST."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef step_impl08(context):\n    assert context.table, \"ENSURE: table is provided.\"\n    context.file_list = [row['file_path'] for row in context.table.rows]", "response": "Create file list.\n\n    :param context: test context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates application list. :param context: test context.", "response": "def step_impl09(context):\n    \"\"\"Create application list.\n\n    :param context: test context.\n    \"\"\"\n    assert context.table, \"ENSURE: table is provided.\"\n    context.app_list = [row['application'] for row in context.table.rows]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates application list. :param context: test context.", "response": "def step_impl10(context):\n    \"\"\"Create application list.\n\n    :param context: test context.\n    \"\"\"\n    assert context.app_list and len(\n        context.app_list) > 0, \"ENSURE: app list is provided.\"\n    assert context.file_list and len(\n        context.file_list) > 0, \"ENSURE: file list is provided.\"\n    context.fuzz_executor = FuzzExecutor(context.app_list, context.file_list)\n    assert context.fuzz_executor, \"VERIFY: fuzz executor created.\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute multiple test runs.", "response": "def step_impl11(context, runs):\n    \"\"\"Execute multiple runs.\n\n    :param runs: number of test runs to perform.\n    :param context: test context.\n    \"\"\"\n    executor = context.fuzz_executor\n    executor.run_test(runs)\n    stats = executor.stats\n    count = stats.cumulated_counts()\n    assert count == runs, \"VERIFY: stats available.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests that the number of records in the is equal to the expected number of records.", "response": "def step_impl12(context, runs):\n    \"\"\"Check called apps / files.\n\n    :param runs: expected number of records.\n    :param context: test context.\n    \"\"\"\n    executor_ = context.fuzz_executor\n    stats = executor_.stats\n    count = stats.cumulated_counts()\n    assert count == runs, \"VERIFY: Number of recorded runs.\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck called apps / files. :param runs: expected number of records. :param context: test context.", "response": "def step_impl13(context, runs):\n    \"\"\"Check called apps / files.\n\n    :param runs: expected number of records.\n    :param context: test context.\n    \"\"\"\n    executor_ = context.fuzz_executor\n    stats = executor_.stats\n    count = stats.cumulated_counts()\n    assert count == runs, \"VERIFY: Number of recorded runs.\"\n    successful_runs = stats.cumulated_counts_for_status(Status.SUCCESS)\n    assert successful_runs == runs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef step_impl14(context, runs):\n    executor_ = context.fuzz_executor\n    stats = executor_.stats\n    count = stats.cumulated_counts()\n    assert count == runs, \"VERIFY: Number of recorded runs.\"\n    failed_runs = stats.cumulated_counts_for_status(Status.FAILED)\n    assert failed_runs == runs", "response": "Check called apps / files.\n\n    :param runs: expected number of records.\n    :param context: test context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the number of differing bytes in a buffer.", "response": "def number_of_modified_bytes(buf, fuzzed_buf):\n    \"\"\"Determine the number of differing bytes.\n\n    :param buf: original buffer.\n    :param fuzzed_buf: fuzzed buffer.\n    :return: number of different bytes.\n    :rtype: int\n    \"\"\"\n    count = 0\n    for idx, b in enumerate(buf):\n        if b != fuzzed_buf[idx]:\n            count += 1\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flesh_out(X, W, embed_dim, CC_labels, dist_mult=2.0, angle_thresh=0.2,\r\n              min_shortcircuit=4, max_degree=5, verbose=False):\r\n  '''Given a connected graph adj matrix (W), add edges to flesh it out.'''\r\n  W = W.astype(bool)\r\n  assert np.all(W == W.T), 'graph given to flesh_out must be symmetric'\r\n  D = pairwise_distances(X, metric='sqeuclidean')\r\n\r\n  # compute average edge lengths for each point\r\n  avg_edge_length = np.empty(X.shape[0])\r\n  for i,nbr_mask in enumerate(W):\r\n    avg_edge_length[i] = D[i,nbr_mask].mean()\r\n\r\n  # candidate edges must satisfy edge length for at least one end point\r\n  dist_thresh = dist_mult * avg_edge_length\r\n  dist_mask = (D < dist_thresh) | (D < dist_thresh[:,None])\r\n  # candidate edges must connect points >= min_shortcircuit hops away\r\n  hops_mask = np.isinf(dijkstra(W, unweighted=True, limit=min_shortcircuit-1))\r\n  # candidate edges must not already be connected, or in the same initial CC\r\n  CC_mask = CC_labels != CC_labels[:,None]\r\n  candidate_edges = ~W & dist_mask & hops_mask & CC_mask\r\n  if verbose:  # pragma: no cover\r\n    print('before F:', candidate_edges.sum(), 'potentials')\r\n\r\n  # calc subspaces\r\n  subspaces, _ = cluster_subspaces(X, embed_dim, CC_labels.max()+1, CC_labels)\r\n\r\n  # upper triangular avoids p,q <-> q,p repeats\r\n  ii,jj = np.where(np.triu(candidate_edges))\r\n  # Get angles\r\n  edge_dirs = X[ii] - X[jj]\r\n  ssi = subspaces[CC_labels[ii]]\r\n  ssj = subspaces[CC_labels[jj]]\r\n  F = edge_cluster_angle(edge_dirs, ssi, ssj)\r\n\r\n  mask = F < angle_thresh\r\n  edge_ii = ii[mask]\r\n  edge_jj = jj[mask]\r\n  edge_order = np.argsort(F[mask])\r\n  if verbose:  # pragma: no cover\r\n    print('got', len(edge_ii), 'potential edges')\r\n  # Prevent any one node from getting a really high degree\r\n  degree = W.sum(axis=0)\r\n  sorted_edges = np.column_stack((edge_ii, edge_jj))[edge_order]\r\n  for e in sorted_edges:\r\n    if degree[e].max() < max_degree:\r\n      W[e[0],e[1]] = True\r\n      W[e[1],e[0]] = True\r\n      degree[e] += 1\r\n  return Graph.from_adj_matrix(np.where(W, np.sqrt(D), 0))", "response": "Given a connected graph adj matrix X add edges to flesh it out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the angle between two subspaces.", "response": "def edge_cluster_angle(edge_dirs, subspaces1, subspaces2):\r\n  '''edge_dirs is a (n,D) matrix of edge vectors.\r\n  subspaces are (n,D,d) or (D,d) matrices of normalized orthogonal subspaces.\r\n  Result is an n-length array of angles.'''\r\n  QG = edge_dirs / np.linalg.norm(edge_dirs, ord=2, axis=1)[:,None]\r\n  X1 = np.einsum('...ij,...i->...j', subspaces1, QG)\r\n  X2 = np.einsum('...ij,...i->...j', subspaces2, QG)\r\n  # TODO: check the math on this for more cases\r\n  # angles = np.maximum(1-np.sum(X1**2, axis=1), 1-np.sum(X2**2, axis=1))\r\n  C1 = np.linalg.svd(X1[:,:,None], compute_uv=False)\r\n  C2 = np.linalg.svd(X2[:,:,None], compute_uv=False)\r\n  angles = np.maximum(1-C1**2, 1-C2**2)[:,0]\r\n  angles[np.isnan(angles)] = 0.0  # nan when edge length == 0\r\n  return angles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the surface mass density profile of each cluster halo and returns the sigma of the cluster halo.", "response": "def sigma_nfw(self):\n        \"\"\"Calculate NFW surface mass density profile.\n\n        Generate the surface mass density profiles of each cluster halo,\n        assuming a spherical NFW model. Optionally includes the effect of\n        cluster miscentering offsets, if the parent object was initialized\n        with offsets.\n\n        Returns\n        ----------\n        Quantity\n            Surface mass density profiles (ndarray, in astropy.units of\n            Msun/pc/pc). Each row corresponds to a single cluster halo.\n        \"\"\"\n        def _centered_sigma(self):\n            # perfectly centered cluster case\n\n            # calculate f\n            bigF = np.zeros_like(self._x)\n            f = np.zeros_like(self._x)\n\n            numerator_arg = ((1. / self._x[self._x_small]) +\n                             np.sqrt((1. / (self._x[self._x_small]**2)) - 1.))\n            denominator = np.sqrt(1. - (self._x[self._x_small]**2))\n            bigF[self._x_small] = np.log(numerator_arg) / denominator\n\n            bigF[self._x_big] = (np.arccos(1. / self._x[self._x_big]) /\n                                 np.sqrt(self._x[self._x_big]**2 - 1.))\n\n            f = (1. - bigF) / (self._x**2 - 1.)\n            f[self._x_one] = 1. / 3.\n            if np.isnan(np.sum(f)) or np.isinf(np.sum(f)):\n                print('\\nERROR: f is not all real\\n')\n\n            # calculate & return centered profiles\n            if f.ndim == 2:\n                sigma = 2. * self._rs_dc_rcrit * f\n            else:\n                rs_dc_rcrit_4D = self._rs_dc_rcrit.T.reshape(1, 1,\n                                                             f.shape[2],\n                                                             f.shape[3])\n                sigma = 2. * rs_dc_rcrit_4D * f\n\n            return sigma\n\n        def _offset_sigma(self):\n\n            # size of \"x\" arrays to integrate over\n            numRoff = self._numRoff\n            numTh = self._numTh\n\n            numRbins = self._nbins\n            maxsig = self._sigmaoffset.value.max()\n\n            # inner/outer bin edges\n            roff_1D = np.linspace(0., 4. * maxsig, numRoff)\n            theta_1D = np.linspace(0., 2. * np.pi, numTh)\n            rMpc_1D = self._rbins.value\n\n            # reshape for broadcasting: (numTh,numRoff,numRbins)\n            theta = theta_1D.reshape(numTh, 1, 1)\n            roff = roff_1D.reshape(1, numRoff, 1)\n            rMpc = rMpc_1D.reshape(1, 1, numRbins)\n\n            r_eq13 = np.sqrt(rMpc ** 2 + roff ** 2 -\n                             2. * rMpc * roff * np.cos(theta))\n\n            # 3D array r_eq13 -> 4D dimensionless radius (nlens)\n            _set_dimensionless_radius(self, radii=r_eq13, integration=True)\n\n            sigma = _centered_sigma(self)\n            inner_integrand = sigma.value / (2. * np.pi)\n\n            # INTEGRATE OVER theta\n            sigma_of_RgivenRoff = simps(inner_integrand, x=theta_1D, axis=0,\n                                        even='first')\n\n            # theta is gone, now dimensions are: (numRoff,numRbins,nlens)\n            sig_off_3D = self._sigmaoffset.value.reshape(1, 1, self._nlens)\n            roff_v2 = roff_1D.reshape(numRoff, 1, 1)\n            PofRoff = (roff_v2 / (sig_off_3D**2) *\n                       np.exp(-0.5 * (roff_v2 / sig_off_3D)**2))\n\n            dbl_integrand = sigma_of_RgivenRoff * PofRoff\n\n            # INTEGRATE OVER Roff\n            # (integration axis=0 after theta is gone).\n            sigma_smoothed = simps(dbl_integrand, x=roff_1D, axis=0,\n                                   even='first')\n\n            # reset _x to correspond to input rbins (default)\n            _set_dimensionless_radius(self)\n\n            sigma_sm = np.array(sigma_smoothed.T) * units.solMass / units.pc**2\n\n            return sigma_sm\n\n        if self._sigmaoffset is None:\n            finalsigma = _centered_sigma(self)\n        elif np.abs(self._sigmaoffset).sum() == 0:\n            finalsigma = _centered_sigma(self)\n        else:\n            finalsigma = _offset_sigma(self)\n            self._sigma_sm = finalsigma\n\n        return finalsigma"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deltasigma_nfw(self):\n        def _centered_dsigma(self):\n            # calculate g\n\n            firstpart = np.zeros_like(self._x)\n            secondpart = np.zeros_like(self._x)\n            g = np.zeros_like(self._x)\n\n            small_1a = 4. / self._x[self._x_small]**2\n            small_1b = 2. / (self._x[self._x_small]**2 - 1.)\n            small_1c = np.sqrt(1. - self._x[self._x_small]**2)\n            firstpart[self._x_small] = (small_1a + small_1b) / small_1c\n\n            big_1a = 8. / (self._x[self._x_big]**2 *\n                           np.sqrt(self._x[self._x_big]**2 - 1.))\n            big_1b = 4. / ((self._x[self._x_big]**2 - 1.)**1.5)\n            firstpart[self._x_big] = big_1a + big_1b\n\n            small_2a = np.sqrt((1. - self._x[self._x_small]) /\n                               (1. + self._x[self._x_small]))\n            secondpart[self._x_small] = np.log((1. + small_2a) /\n                                               (1. - small_2a))\n\n            big_2a = self._x[self._x_big] - 1.\n            big_2b = 1. + self._x[self._x_big]\n            secondpart[self._x_big] = np.arctan(np.sqrt(big_2a / big_2b))\n\n            both_3a = (4. / (self._x**2)) * np.log(self._x / 2.)\n            both_3b = 2. / (self._x**2 - 1.)\n            g = firstpart * secondpart + both_3a - both_3b\n\n            g[self._x_one] = (10. / 3.) + 4. * np.log(0.5)\n\n            if np.isnan(np.sum(g)) or np.isinf(np.sum(g)):\n                print('\\nERROR: g is not all real\\n', g)\n\n            # calculate & return centered profile\n            deltasigma = self._rs_dc_rcrit * g\n\n            return deltasigma\n\n        def _offset_dsigma(self):\n            original_rbins = self._rbins.value\n\n            # if offset sigma was already calculated, use it!\n            try:\n                sigma_sm_rbins = self._sigma_sm\n            except AttributeError:\n                sigma_sm_rbins = self.sigma_nfw()\n\n            innermost_sampling = 1.e-10  # stable for anything below 1e-5\n            inner_prec = self._numRinner\n            r_inner = np.linspace(innermost_sampling,\n                                  original_rbins.min(),\n                                  endpoint=False, num=inner_prec)\n            outer_prec = self._factorRouter * self._nbins\n            r_outer = np.linspace(original_rbins.min(),\n                                  original_rbins.max(),\n                                  endpoint=False, num=outer_prec + 1)[1:]\n            r_ext_unordered = np.hstack([r_inner, r_outer, original_rbins])\n            r_extended = np.sort(r_ext_unordered)\n\n            # set temporary extended rbins, nbins, x, rs_dc_rcrit array\n            self._rbins = r_extended * units.Mpc\n            self._nbins = self._rbins.shape[0]\n            _set_dimensionless_radius(self)  # uses _rbins, _nlens\n            rs_dc_rcrit = self._rs * self._delta_c * self._rho_crit\n            self._rs_dc_rcrit = rs_dc_rcrit.reshape(self._nlens,\n                                                    1).repeat(self._nbins, 1)\n\n            sigma_sm_extended = self.sigma_nfw()\n            mean_inside_sigma_sm = np.zeros([self._nlens,\n                                             original_rbins.shape[0]])\n\n            for i, r in enumerate(original_rbins):\n                index_of_rbin = np.where(r_extended == r)[0][0]\n                x = r_extended[0:index_of_rbin + 1]\n                y = sigma_sm_extended[:, 0:index_of_rbin + 1] * x\n\n                integral = simps(y, x=x, axis=-1, even='first')\n\n                # average of sigma_sm at r < rbin\n                mean_inside_sigma_sm[:, i] = (2. / r**2) * integral\n\n            mean_inside_sigma_sm = mean_inside_sigma_sm * (units.Msun /\n                                                           units.pc**2)\n\n            # reset original rbins, nbins, x\n            self._rbins = original_rbins * units.Mpc\n            self._nbins = self._rbins.shape[0]\n            _set_dimensionless_radius(self)\n            rs_dc_rcrit = self._rs * self._delta_c * self._rho_crit\n            self._rs_dc_rcrit = rs_dc_rcrit.reshape(self._nlens,\n                                                    1).repeat(self._nbins, 1)\n            self._sigma_sm = sigma_sm_rbins  # reset to original sigma_sm\n\n            dsigma_sm = mean_inside_sigma_sm - sigma_sm_rbins\n\n            return dsigma_sm\n\n        if self._sigmaoffset is None:\n            finaldeltasigma = _centered_dsigma(self)\n        elif np.abs(self._sigmaoffset).sum() == 0:\n            finaldeltasigma = _centered_dsigma(self)\n        else:\n            finaldeltasigma = _offset_dsigma(self)\n\n        return finaldeltasigma", "response": "Calculates the NFW differential surface mass density profiles of each cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_edge_pairs(pairs, num_vertices=None, symmetric=False, weights=None):\n  '''Constructor for Graph objects based on edges given as pairs of vertices.\n  pairs : integer array-like with shape (num_edges, 2)\n  '''\n  if not symmetric:\n    if weights is None:\n      return EdgePairGraph(pairs, num_vertices=num_vertices)\n    row, col = np.asarray(pairs).T\n    row, weights = np.broadcast_arrays(row, weights)\n    shape = None if num_vertices is None else (num_vertices, num_vertices)\n    adj = ss.coo_matrix((weights, (row, col)), shape=shape)\n    return SparseAdjacencyMatrixGraph(adj)\n  # symmetric case\n  G = SymmEdgePairGraph(pairs, num_vertices=num_vertices)\n  if weights is None:\n    return G\n  # Convert to sparse adj graph with provided edge weights\n  s = G.matrix('coo').astype(float)\n  # shenanigans to assign edge weights in the right order\n  flat_idx = np.ravel_multi_index(s.nonzero(), s.shape)\n  r, c = np.transpose(pairs)\n  rc_idx = np.ravel_multi_index((r,c), s.shape)\n  cr_idx = np.ravel_multi_index((c,r), s.shape)\n  order = np.argsort(flat_idx)\n  flat_idx = flat_idx[order]\n  s.data[order[np.searchsorted(flat_idx, rc_idx)]] = weights\n  s.data[order[np.searchsorted(flat_idx, cr_idx)]] = weights\n  return SparseAdjacencyMatrixGraph(s)", "response": "Constructor for Graph objects based on edges given as pairs of vertices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset fixed effect design", "response": "def W(self,value):\n        \"\"\" set fixed effect design \"\"\"\n        if value is None:   value = sp.zeros((self._N, 0))\n        assert value.shape[0]==self._N, 'Dimension mismatch'\n        self._K = value.shape[1]\n        self._W = value\n        self._notify()\n        self.clear_cache('predict_in_sample','Yres')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting fixed effect design for predictions", "response": "def Wstar(self,value):\n        \"\"\" set fixed effect design for predictions \"\"\"\n        if value is None:\n            self._use_to_predict = False\n        else:\n            assert value.shape[1]==self._K, 'Dimension mismatch'\n            self._use_to_predict = True\n        self._Wstar = value\n        self.clear_cache('predict')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all local symbols here and also of the parents", "response": "def declared_symbols(self):\n        \"\"\"\n        Return all local symbols here, and also of the parents\n        \"\"\"\n\n        return self.local_declared_symbols | (\n            self.parent.declared_symbols if self.parent else set())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of all symbols that are not declared in this scope and are not referenced in any parent scopes.", "response": "def global_symbols(self):\n        \"\"\"\n        These are symbols that have been referenced, but not declared\n        within this scope or any parent scopes.\n        \"\"\"\n\n        declared_symbols = self.declared_symbols\n        return set(\n            s for s in self.referenced_symbols if s not in declared_symbols)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a set of all symbols that are not declared in children.", "response": "def global_symbols_in_children(self):\n        \"\"\"\n        This is based on all children referenced symbols that have not\n        been declared.\n\n        The intended use case is to ban the symbols from being used as\n        remapped symbol values.\n        \"\"\"\n\n        result = set()\n        for child in self.children:\n            result |= (\n                child.global_symbols |\n                child.global_symbols_in_children)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmark the scope as closed.", "response": "def close(self):\n        \"\"\"\n        Mark the scope as closed, i.e. all symbols have been declared,\n        and no further declarations should be done.\n        \"\"\"\n\n        if self._closed:\n            raise ValueError('scope is already marked as closed')\n\n        # By letting parent know which symbols this scope has leaked, it\n        # will let them reserve all lowest identifiers first.\n        if self.parent:\n            for symbol, c in self.leaked_referenced_symbols.items():\n                self.parent.reference(symbol, c)\n\n        self._closed = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new nested scope that is within this instance and binding the provided node to it.", "response": "def nest(self, node, cls=None):\n        \"\"\"\n        Create a new nested scope that is within this instance, binding\n        the provided node to it.\n        \"\"\"\n\n        if cls is None:\n            cls = type(self)\n\n        nested_scope = cls(node, self)\n        self.children.append(nested_scope)\n        return nested_scope"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef declare(self, symbol):\n\n        if symbol != self.catch_symbol:\n            self.parent.declare(symbol)", "response": "Declare a new symbol in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreferences the given catch symbol.", "response": "def reference(self, symbol, count=1):\n        \"\"\"\n        However, if referenced, ensure that the counter is applied to\n        the catch symbol.\n        \"\"\"\n\n        if symbol == self.catch_symbol:\n            self.catch_symbol_usage += count\n        else:\n            self.parent.reference(symbol, count)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the remapping symbols for this scope class.", "response": "def build_remap_symbols(self, name_generator, children_only=None):\n        \"\"\"\n        The children_only flag is inapplicable, but this is included as\n        the Scope class is defined like so.\n\n        Here this simply just place the catch symbol with the next\n        replacement available.\n        \"\"\"\n\n        replacement = name_generator(skip=(self._reserved_symbols))\n        self.remapped_symbols[self.catch_symbol] = next(replacement)\n\n        # also to continue down the children.\n        for child in self.children:\n            child.build_remap_symbols(name_generator, False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering this identifier to the current scope and mark it as referenced in the current scope.", "response": "def register_reference(self, dispatcher, node):\n        \"\"\"\n        Register this identifier to the current scope, and mark it as\n        referenced in the current scope.\n        \"\"\"\n\n        # the identifier node itself will be mapped to the current scope\n        # for the resolve to work\n        # This should probably WARN about the node object being already\n        # assigned to an existing scope that isn't current_scope.\n        self.identifiers[node] = self.current_scope\n        self.current_scope.reference(node.value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshadowing the current value of the node.", "response": "def shadow_reference(self, dispatcher, node):\n        \"\"\"\n        Only simply make a reference to the value in the current scope,\n        specifically for the FuncBase type.\n        \"\"\"\n\n        # as opposed to the previous one, only add the value of the\n        # identifier itself to the scope so that it becomes reserved.\n        self.current_scope.reference(node.identifier.value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve(self, dispatcher, node):\n\n        scope = self.identifiers.get(node)\n        if not scope:\n            return node.value\n        return scope.resolve(node.value)", "response": "Resolve the node into the scope it was declared\n        at."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwalk through the node with a custom dispatcher for extraction of the details that are required.", "response": "def walk(self, dispatcher, node):\n        \"\"\"\n        Walk through the node with a custom dispatcher for extraction of\n        details that are required.\n        \"\"\"\n\n        deferrable_handlers = {\n            Declare: self.declare,\n            Resolve: self.register_reference,\n        }\n        layout_handlers = {\n            PushScope: self.push_scope,\n            PopScope: self.pop_scope,\n            PushCatch: self.push_catch,\n            # should really be different, but given that the\n            # mechanism is within the same tree, the only difference\n            # would be sanity check which should have been tested in\n            # the first place in the primitives anyway.\n            PopCatch: self.pop_scope,\n        }\n\n        if not self.shadow_funcname:\n            layout_handlers[ResolveFuncName] = self.shadow_reference\n\n        local_dispatcher = Dispatcher(\n            definitions=dict(dispatcher),\n            token_handler=None,\n            layout_handlers=layout_handlers,\n            deferrable_handlers=deferrable_handlers,\n        )\n        return list(walk(local_dispatcher, node))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinalizes the run - build the remap symbol tables and close the global scope and close the global scope and build the remap symbol tables.", "response": "def finalize(self):\n        \"\"\"\n        Finalize the run - build the name generator and use it to build\n        the remap symbol tables.\n        \"\"\"\n\n        self.global_scope.close()\n        name_generator = NameGenerator(skip=self.reserved_keywords)\n        self.global_scope.build_remap_symbols(\n            name_generator,\n            children_only=not self.obfuscate_globals,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prewalk_hook(self, dispatcher, node):\n\n        self.walk(dispatcher, node)\n        self.finalize()\n        return node", "response": "This is a hook for the Unparser to use as a prewalk hook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean():\n    \"take out the trash\"\n    src_dir = easy.options.setdefault(\"docs\", {}).get('src_dir', None)\n    if src_dir is None:\n        src_dir = 'src' if easy.path('src').exists() else '.'\n\n    with easy.pushd(src_dir):\n        for pkg in set(easy.options.setup.packages) | set((\"tests\",)):\n            for filename in glob.glob(pkg.replace('.', os.sep) + \"/*.py[oc~]\"):\n                easy.path(filename).remove()", "response": "take out the trash"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_grid_info(self, which, low, high, num, scale, name):\n        setattr(self.generate_info, which + '_low', low)\n        setattr(self.generate_info, which + '_high', high)\n        setattr(self.generate_info, 'num_' + which, num)\n        setattr(self.generate_info, which + 'val_name', name)\n\n        if scale not in ['lin', 'log']:\n            raise ValueError('{} scale must be lin or log.'.format(which))\n        setattr(self.generate_info, which + 'scale', scale)\n        return", "response": "Set the grid values for x or y."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_y_grid_info(self, y_low, y_high, num_y, yscale, yval_name):\n        self._set_grid_info('y', y_low, y_high, num_y, yscale, yval_name)\n        return", "response": "Set the grid values for y."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the grid values for x.", "response": "def set_x_grid_info(self, x_low, x_high, num_x, xscale, xval_name):\n        \"\"\"Set the grid values for x.\n\n        Create information for the grid of x values.\n\n        Args:\n            num_x (int): Number of points on axis.\n            x_low/x_high (float): Lowest/highest value for the axis.\n            xscale (str): Scale of the axis. Choices are 'log' or 'lin'.\n            xval_name (str): Name representing the axis. See GenerateContainer documentation\n                for options for the name.\n\n        \"\"\"\n        self._set_grid_info('x', x_low, x_high, num_x, xscale, xval_name)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_noise_curve(self, name, noise_type='ASD', is_wd_background=False):\n        if is_wd_background:\n            self.sensitivity_input.wd_noise = name\n            self.sensitivity_input.wd_noise_type_in = noise_type\n\n        else:\n            if 'sensitivity_curves' not in self.sensitivity_input.__dict__:\n                self.sensitivity_input.sensitivity_curves = []\n            if 'noise_type_in' not in self.sensitivity_input.__dict__:\n                self.sensitivity_input.noise_type_in = []\n\n            self.sensitivity_input.sensitivity_curves.append(name)\n            self.sensitivity_input.noise_type_in.append(noise_type)\n        return", "response": "Adds a noise curve for an analysis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding White Dwarf Background Noise This adds the White Dwarf (WD) Background noise. This can either do calculations with, without, or with and without WD noise. Args: wd_noise (bool or str, optional): Add or remove WD background noise. First option is to have only calculations with the wd_noise. For this, use `yes` or True. Second option is no WD noise. For this, use `no` or False. For both calculations with and without WD noise, use `both`. Raises: ValueError: Input value is not one of the options.", "response": "def set_wd_noise(self, wd_noise):\n        \"\"\"Add White Dwarf Background Noise\n\n        This adds the White Dwarf (WD) Background noise. This can either do calculations with,\n        without, or with and without WD noise.\n\n        Args:\n            wd_noise (bool or str, optional): Add or remove WD background noise. First option is to\n                have only calculations with the wd_noise. For this, use `yes` or True.\n                Second option is no WD noise. For this, use `no` or False. For both calculations\n                with and without WD noise, use `both`.\n\n        Raises:\n            ValueError: Input value is not one of the options.\n\n        \"\"\"\n        if isinstance(wd_noise, bool):\n            wd_noise = str(wd_noise)\n\n        if wd_noise.lower() == 'yes' or wd_noise.lower() == 'true':\n            wd_noise = 'True'\n        elif wd_noise.lower() == 'no' or wd_noise.lower() == 'false':\n            wd_noise = 'False'\n        elif wd_noise.lower() == 'both':\n            wd_noise = 'Both'\n        else:\n            raise ValueError('wd_noise must be yes, no, True, False, or Both.')\n\n        self.sensitivity_input.add_wd_noise = wd_noise\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the generation type of the weather module.", "response": "def set_generation_type(self, num_processors=-1, num_splits=1000, verbose=-1):\n        \"\"\"Change generation type.\n\n        Choose weather to generate the data in parallel or on a single processor.\n\n        Args:\n            num_processors (int or None, optional): Number of parallel processors to use.\n                If ``num_processors==-1``, this will use multiprocessing module and use\n                available cpus. If single generation is desired, num_processors is set\n                to ``None``. Default is -1.\n            num_splits (int, optional): Number of binaries to run during each process.\n                Default is 1000.\n            verbose (int, optional): Describes the notification of when parallel processes\n                are finished. Value describes cadence of process completion notifications.\n                If ``verbose == -1``, no notifications are given. Default is -1.\n\n        \"\"\"\n        self.parallel_input.num_processors = num_processors\n        self.parallel_input.num_splits = num_splits\n        self.parallel_input.verbose = verbose\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_signal_type(self, sig_type):\n        if isinstance(sig_type, str):\n            sig_type = [sig_type]\n        self.snr_input.signal_type = sig_type\n        return", "response": "Sets the signal type of interest."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dictionary for the contents of the main container class into the main container class.", "response": "def return_dict(self):\n        \"\"\"Output dictionary for :mod:`gwsnrcalc.generate_contour_data` input.\n\n        Iterates through the entire MainContainer class turning its contents\n        into dictionary form. This dictionary becomes the input for\n        :mod:`gwsnrcalc.generate_contour_data`.\n\n        If `print_input` attribute is True, the entire dictionary will be printed\n        prior to returning the dicitonary.\n\n        Returns:\n            - output_dict: Dicitonary for input into\n                :mod:`gwsnrcalc.generate_contour_data`.\n\n        \"\"\"\n        output_dict = {}\n        output_dict['general'] = self._iterate_through_class(self.general.__dict__)\n        output_dict['generate_info'] = self._iterate_through_class(self.generate_info.__dict__)\n        output_dict['sensitivity_input'] = (self._iterate_through_class(\n            self.sensitivity_input.__dict__))\n        output_dict['snr_input'] = self._iterate_through_class(self.snr_input.__dict__)\n        output_dict['parallel_input'] = self._iterate_through_class(self.parallel_input.__dict__)\n        output_dict['output_info'] = self._iterate_through_class(self.output_info.__dict__)\n\n        if self.print_input:\n            print(output_dict)\n        return output_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_print(ast, indent_str='  '):\n\n    return ''.join(chunk.text for chunk in pretty_printer(indent_str)(ast))", "response": "Simple pretty print function that returns a string rendering of an ES5 Program."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a minimum printer that can be used to parse a single entry.", "response": "def minify_printer(\n        obfuscate=False,\n        obfuscate_globals=False,\n        shadow_funcname=False,\n        drop_semi=False):\n    \"\"\"\n    Construct a minimum printer.\n\n    Arguments\n\n    obfuscate\n        If True, obfuscate identifiers nested in each scope with a\n        shortened identifier name to further reduce output size.\n\n        Defaults to False.\n    obfuscate_globals\n        Also do the same to identifiers nested on the global scope; do\n        not enable unless the renaming of global variables in a not\n        fully deterministic manner into something else is guaranteed to\n        not cause problems with the generated code and other code that\n        in the same environment that it will be executed in.\n\n        Defaults to False for the reason above.\n    drop_semi\n        Drop semicolons whenever possible (e.g. the final semicolons of\n        a given block).\n    \"\"\"\n\n    active_rules = [rules.minify(drop_semi=drop_semi)]\n    if obfuscate:\n        active_rules.append(rules.obfuscate(\n            obfuscate_globals=obfuscate_globals,\n            shadow_funcname=shadow_funcname,\n            reserved_keywords=(Lexer.keywords_dict.keys())\n        ))\n    return Unparser(rules=active_rules)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef minify_print(\n        ast,\n        obfuscate=False,\n        obfuscate_globals=False,\n        shadow_funcname=False,\n        drop_semi=False):\n    \"\"\"\n    Simple minify print function; returns a string rendering of an input\n    AST of an ES5 program\n\n    Arguments\n\n    ast\n        The AST to minify print\n    obfuscate\n        If True, obfuscate identifiers nested in each scope with a\n        shortened identifier name to further reduce output size.\n\n        Defaults to False.\n    obfuscate_globals\n        Also do the same to identifiers nested on the global scope; do\n        not enable unless the renaming of global variables in a not\n        fully deterministic manner into something else is guaranteed to\n        not cause problems with the generated code and other code that\n        in the same environment that it will be executed in.\n\n        Defaults to False for the reason above.\n    drop_semi\n        Drop semicolons whenever possible (e.g. the final semicolons of\n        a given block).\n    \"\"\"\n\n    return ''.join(chunk.text for chunk in minify_printer(\n        obfuscate, obfuscate_globals, shadow_funcname, drop_semi)(ast))", "response": "Simple minify print function that returns a string rendering of an ES5 program containing the given tree of ES5 code."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes integer i into a VLQ encoded string.", "response": "def encode_vlq(i):\n    \"\"\"\n    Encode integer `i` into a VLQ encoded string.\n    \"\"\"\n\n    # shift in the sign to least significant bit\n    raw = (-i << 1) + 1 if i < 0 else i << 1\n    if raw < VLQ_MULTI_CHAR:\n        # short-circuit simple case as it doesn't need continuation\n        return INT_B64[raw]\n\n    result = []\n    while raw:\n        # assume continue\n        result.append(raw & VLQ_BASE_MASK | VLQ_CONT)\n        # shift out processed bits\n        raw = raw >> VLQ_SHIFT\n    # discontinue the last unit\n    result[-1] &= VLQ_BASE_MASK\n    return ''.join(INT_B64[i] for i in result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_vlqs(s):\n\n    ints = []\n    i = 0\n    shift = 0\n\n    for c in s:\n        raw = B64_INT[c]\n        cont = VLQ_CONT & raw\n        i = ((VLQ_BASE_MASK & raw) << shift) | i\n        shift += VLQ_SHIFT\n        if not cont:\n            sign = -1 if 1 & i else 1\n            ints.append((i >> 1) * sign)\n            i = 0\n            shift = 0\n\n    return tuple(ints)", "response": "Decode str s into a list of integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the best available json library on demand.", "response": "def require_json():\n    \"\"\" Load the best available json library on demand.\n    \"\"\"\n    # Fails when \"json\" is missing and \"simplejson\" is not installed either\n    try:\n        import json # pylint: disable=F0401\n        return json\n    except ImportError:\n        try:\n            import simplejson # pylint: disable=F0401\n            return simplejson\n        except ImportError as exc:\n            raise ImportError(\"\"\"Please 'pip install \"simplejson>=2.1.6\"' (%s)\"\"\" % (exc,))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the first N primes", "response": "def _findNextPrime(self, N):\n        \"\"\"Generate the first N primes\"\"\"\n        primes = self.primes\n        nextPrime = primes[-1]+1\n        while(len(primes)<N):\n            maximum = nextPrime * nextPrime\n            prime = 1\n            for i in primes:\n                if i > maximum:\n                    break\n                if nextPrime % i == 0:\n                    prime = 0\n                    break\n            if prime:\n                primes.append(nextPrime)\n            nextPrime+=1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the path to the virtualenv s bin directories or a full executable path.", "response": "def venv_bin(name=None):  # pylint: disable=inconsistent-return-statements\n    \"\"\" Get the directory for virtualenv stubs, or a full executable path\n        if C{name} is provided.\n    \"\"\"\n    if not hasattr(sys, \"real_prefix\"):\n        easy.error(\"ERROR: '%s' is not a virtualenv\" % (sys.executable,))\n        sys.exit(1)\n\n    for bindir in (\"bin\", \"Scripts\"):\n        bindir = os.path.join(sys.prefix, bindir)\n        if os.path.exists(bindir):\n            if name:\n                bin_ext = os.path.splitext(sys.executable)[1] if sys.platform == 'win32' else ''\n                return os.path.join(bindir, name + bin_ext)\n            else:\n                return bindir\n\n    easy.error(\"ERROR: Scripts directory not found in '%s'\" % (sys.prefix,))\n    sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a command installed into the active virtualenv.", "response": "def vsh(cmd, *args, **kw):\n    \"\"\" Execute a command installed into the active virtualenv.\n    \"\"\"\n    args = '\" \"'.join(i.replace('\"', r'\\\"') for i in args)\n    easy.sh('\"%s\" \"%s\"' % (venv_bin(cmd), args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls a required tool before using it.", "response": "def install_tools(dependencies):\n    \"\"\" Install a required tool before using it, if it's missing.\n\n        Note that C{dependencies} can be a distutils requirement,\n        or a simple name from the C{tools} task configuration, or\n        a (nested) list of such requirements.\n    \"\"\"\n    tools = getattr(easy.options, \"tools\", {})\n    for dependency in iterutil.flatten(dependencies):\n        dependency = tools.get(dependency, dependency)\n        try:\n            pkg_resources.require(dependency)\n        except pkg_resources.DistributionNotFound:\n            vsh(\"pip\", \"install\", \"-q\", dependency)\n            dependency = pkg_resources.require(dependency)\n            easy.info(\"Installed required tool %s\" % (dependency,))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toplevel_packages():\n    packages = set(easy.options.setup.packages)\n    for pkg in list(packages):\n        packages -= set(p for p in packages if str(p).startswith(pkg + '.'))\n    return list(sorted(packages))", "response": "Get package list without sub - packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_position(self, position, midpoint=False, surface=None):\n\n        # Find the image size and midpoint of the image\n        imagesize = self.image.get_size()\n        imagemidp = (int(imagesize[0] * 0.5), int(imagesize[1] * 0.5))\n\n        # if a midpoint arguement is passed, set the pos to the top left pixel\n        # such that the position passed in is in the middle of the button\n        if midpoint:\n            self.pos = (position[0] - imagemidp[0], position[1] - imagemidp[1])\n        else:\n            self.pos = position\n\n        # set the rectangle to be used for collision detection\n        self.rect = pygame.Rect(self.pos, self.image.get_size())\n\n        # Set up the information that is needed to blit the image to the surface\n        self.blitinfo = (self.image, self.pos)\n\n        # automatically blit the button onto an input surface\n        if surface:\n            surface.blit(*self.blitinfo)", "response": "This method allows the button to be moved manually and keep the click\n        on functionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_offset(self, offset, mid=None):\n\n        if mid:\n            imagesize = self.image.get_size()\n            imagemidp = (int(imagesize[0] * 0.5), int(imagesize[1] * 0.5))\n            if mid == 'x':\n                offset = (offset[0] - imagemidp[0], offset[1])\n            if mid == 'y':\n                offset = (offset[0], offset[1] - imagemidp[1])\n            if mid == 'c':\n                offset = (offset[0] - imagemidp[0], offset[1] - imagemidp[1])\n\n        self.pos = offset\n\n        for i in self.buttonlist:\n                i.rect[0] += offset[0]\n                i.rect[1] += offset[1]\n\n        try:\n            for i in self.widgetlist:\n                i.rect[0] += offset[0]\n                i.rect[1] += offset[1]\n        except AttributeError:\n            pass", "response": "This method will set the offset of the menu in the open\n othewise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef widget_status(self):\n        widget_status_list = []\n        for i in self.widgetlist:\n            widget_status_list += [[i.name, i.status]]\n        return widget_status_list", "response": "This method returns the status of all of the widgets in the available set of resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, screen, clock):\n\n        # If a music file was passed, start playing it on repeat\n        if self.music is not None:\n            pygame.mixer.music.play(-1)\n\n        while True:\n            clock.tick(30)\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    pygame.quit()\n                    sys.exit()\n                # Check if any of the buttons were clicked\n                for i in self.buttonlist:\n                    if (event.type == pygame.MOUSEBUTTONUP and\n                            i.rect.collidepoint(pygame.mouse.get_pos())):\n                        if self.music is not None:\n                            pygame.mixer.music.stop()\n                        if self.widgetlist:\n                            return [i(), self.widget_status()]\n                        else:\n                            return i()\n                # If there is a widget list, check to see if any were clicked\n                if self.widgetlist:\n                    for i in self.widgetlist:\n                        if (event.type == pygame.MOUSEBUTTONDOWN and\n                                i.rect.collidepoint(pygame.mouse.get_pos())):\n                            # Call the widget and give it the menu information\n                            i(self)\n            screen.blit(self.image, self.pos)\n            pygame.display.flip()", "response": "Update the menu with the current state of the screen and the clock."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Screens(self, text, prog, screen, clock):\n        # Initialize the screen class\n        BaseScreen.__init__(self, self.size, self.background)\n\n        # Determine the mid position of the given screen size and the\n        # y button height\n        xmid = self.size[0]//2\n\n        # Create the header text\n        Linesoftext(text, (xmid, 40), xmid=True, surface=self.image,\n                    fontsize=30)\n\n        # Create the buttons\n        self.buttonlist = []\n        if prog == 0:\n            self.buttonlist += [self.nextbutton]\n\n        elif prog == 1:\n            self.buttonlist += [self.nextbutton]\n            self.buttonlist += [self.backbutton]\n\n        elif prog == 2:\n            self.buttonlist += [self.lastbutton]\n            self.buttonlist += [self.backbutton]\n\n        # Draw the buttons to the screen\n        for i in self.buttonlist:\n            self.image.blit(*i.blitinfo)\n\n        # Use the menu update method to run the screen and process button clicks\n        return Menu.update(self, screen, clock)", "response": "Create the screen and draw the buttons and the header text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new commerce instance from environment variables TBK_COMMERCE_ID TBK_COMMERCE_KEY or TBK_COMMERCE_TESTING.", "response": "def create_commerce():\n        \"\"\"\n        Creates commerce from environment variables ``TBK_COMMERCE_ID``, ``TBK_COMMERCE_KEY``\n        or for testing purposes ``TBK_COMMERCE_TESTING``.\n        \"\"\"\n        commerce_id = os.getenv('TBK_COMMERCE_ID')\n        commerce_key = os.getenv('TBK_COMMERCE_KEY')\n        commerce_testing = os.getenv('TBK_COMMERCE_TESTING') == 'True'\n\n        if not commerce_testing:\n            if commerce_id is None:\n                raise ValueError(\"create_commerce needs TBK_COMMERCE_ID environment variable\")\n            if commerce_key is None:\n                raise ValueError(\"create_commerce needs TBK_COMMERCE_KEY environment variable\")\n\n        return Commerce(\n            id=commerce_id or Commerce.TEST_COMMERCE_ID,\n            key=commerce_key,\n            testing=commerce_testing\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_config_tbk(self, confirmation_url):\n        '''\n        Returns a string with the ``TBK_CONFIG.dat``.\n\n        :param confirmation_url: URL where callback is made.\n        '''\n        config = (\n            \"IDCOMERCIO = {commerce_id}\\n\"\n            \"MEDCOM = 1\\n\"\n            \"TBK_KEY_ID = 101\\n\"\n            \"PARAMVERIFCOM = 1\\n\"\n            \"URLCGICOM = {confirmation_path}\\n\"\n            \"SERVERCOM = {confirmation_host}\\n\"\n            \"PORTCOM = {confirmation_port}\\n\"\n            \"WHITELISTCOM = ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 0123456789./:=&?_\\n\"\n            \"HOST = {confirmation_host}\\n\"\n            \"WPORT = {confirmation_port}\\n\"\n            \"URLCGITRA = /filtroUnificado/bp_revision.cgi\\n\"\n            \"URLCGIMEDTRA = /filtroUnificado/bp_validacion.cgi\\n\"\n            \"SERVERTRA = {webpay_server}\\n\"\n            \"PORTTRA = {webpay_port}\\n\"\n            \"PREFIJO_CONF_TR = HTML_\\n\"\n            \"HTML_TR_NORMAL = http://127.0.0.1/notify\\n\"\n        )\n        confirmation_uri = six.moves.urllib.parse.urlparse(confirmation_url)\n        webpay_server = \"https://certificacion.webpay.cl\" if self.testing else \"https://webpay.transbank.cl\"\n        webpay_port = 6443 if self.testing else 443\n        return config.format(commerce_id=self.id,\n                             confirmation_path=confirmation_uri.path,\n                             confirmation_host=confirmation_uri.hostname,\n                             confirmation_port=confirmation_uri.port,\n                             webpay_port=webpay_port,\n                             webpay_server=webpay_server)", "response": "Returns a string with the TBK_CONFIG. dat file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_lang_change(self, request):\n        if 'lang' not in request.GET:\n            return False\n\n        return not any(request.path.endswith(url) for url in self.exempt_urls)", "response": "Return True if the lang param is present and the URL isn t exempt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, *matches, **kw):  # kw=default=None, boolean=False\n        '''Add an argument; this is optional, and mostly useful for setting up aliases or setting boolean=True\n\n        Apparently `def add(self, *matches, default=None, boolean=False):` is invalid syntax in Python. Not only is this absolutely ridiculous, but the alternative `def add(self, default=None, boolean=False, *matches):` does not do what you would expect. This syntax works as intended in Python 3.\n\n        If you provide multiple `matches` that are not dash-prefixed, only the first will be used as a positional argument.\n\n        Specifying any positional arguments and then using `boolean=True` is just weird, and their will be no special consideration for boolean=True in that case for the position-enabled argument.\n        '''\n        # python syntax hack\n        default = kw.get('default', None)\n        boolean = kw.get('boolean', False)\n        del kw\n        # do not use kw after this line! It's a hack; it should never have been there in the first place.\n        positional = None\n        names = []\n        for match in matches:\n            if match.startswith('--'):\n                names.append(match[2:])\n            elif match.startswith('-'):\n                names.append(match[1:])\n            elif positional:\n                # positional has already been filled\n                names.append(match)\n            else:\n                # first positional: becomes canonical positional\n                positional = match\n                names.append(match)\n\n        argument = BooleanArgument(names, default, boolean, positional)\n        self.arguments.append(argument)\n\n        # chainable\n        return self", "response": "Add an argument to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nflattens a nested list by yielding its scalar items.", "response": "def flatten(nested, containers=(list, tuple)):\n    \"\"\" Flatten a nested list by yielding its scalar items.\n    \"\"\"\n    for item in nested:\n        if hasattr(item, \"next\") or isinstance(item, containers):\n            for subitem in flatten(item):\n                yield subitem\n        else:\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of TEMPLATE_CONTEXT_PROCESSORS without the processors listed in exclude and with the processors listed in append.", "response": "def get_template_context_processors(exclude=(), append=(),\n                        current={'processors': TEMPLATE_CONTEXT_PROCESSORS}):\n    \"\"\"\n    Returns TEMPLATE_CONTEXT_PROCESSORS without the processors listed in\n    exclude and with the processors listed in append.\n\n    The use of a mutable dict is intentional, in order to preserve the state of\n    the TEMPLATE_CONTEXT_PROCESSORS tuple across multiple settings files.\n    \"\"\"\n\n    current['processors'] = tuple(\n        [p for p in current['processors'] if p not in exclude]\n    ) + tuple(append)\n\n    return current['processors']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_middleware(exclude=(), append=(),\n                   current={'middleware': MIDDLEWARE_CLASSES}):\n    \"\"\"\n    Returns MIDDLEWARE_CLASSES without the middlewares listed in exclude and\n    with the middlewares listed in append.\n\n    The use of a mutable dict is intentional, in order to preserve the state of\n    the MIDDLEWARE_CLASSES tuple across multiple settings files.\n    \"\"\"\n\n    current['middleware'] = tuple(\n        [m for m in current['middleware'] if m not in exclude]\n    ) + tuple(append)\n    return current['middleware']", "response": "Returns MIDDLEWARE_CLASSES without the middlewares listed in exclude and with the middlewares listed in append."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_apps(exclude=(), append=(), current={'apps': INSTALLED_APPS}):\n\n    current['apps'] = tuple(\n        [a for a in current['apps'] if a not in exclude]\n    ) + tuple(append)\n    return current['apps']", "response": "Returns the list of INSTALLED_APPS without the apps listed in exclude and with the apps listed in append."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve_t(self, Mt):\n        if len(Mt.shape)==2:    _Mt = Mt[:, :, sp.newaxis]\n        else:                   _Mt = Mt\n        LMt = vei_CoR_veX(_Mt, R=self.Lr(), C=self.Lc())\n        DLMt = self.D()[:, :, sp.newaxis] * LMt\n        RV = vei_CoR_veX(DLMt, R=self.Lr().T, C=self.Lc().T)\n        if len(Mt.shape)==2:    RV = RV[:, :, 0]\n        return RV", "response": "solve the T problem for a single tensor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating initial atom identifiers using atomic invariants", "response": "def invariants(mol):\n    \"\"\"Generate initial atom identifiers using atomic invariants\"\"\"\n    atom_ids = {}\n    for a in mol.atoms:\n        components = []\n        components.append(a.number)\n        components.append(len(a.oatoms))\n        components.append(a.hcount)\n        components.append(a.charge)\n        components.append(a.mass)\n        if len(a.rings) > 0:\n            components.append(1)\n\n        atom_ids[a.index] = gen_hash(components)\n\n    return atom_ids"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ecfp(mol, radius=2):\n\n    atom_ids = invariants(mol)\n\n    fp = {}\n    for i in atom_ids.values():\n        fp[i] = fp.get(i, 0) + 1\n\n    neighborhoods = []\n    atom_neighborhoods = [ len(mol.bonds) * bitarray('0') for a in mol.atoms]\n    dead_atoms = len(mol.atoms) * bitarray('0')\n\n    for layer in xrange(1, radius+1):\n        round_ids = {}\n        round_atom_neighborhoods = copy.deepcopy(atom_neighborhoods)\n        neighborhoods_this_round = []\n\n        for a in mol.atoms:\n            if dead_atoms[a.index]: continue\n\n            nbsr = []\n            for b in a.bonds:\n                round_atom_neighborhoods[a.index][b.index] = True\n                oidx = b.xatom(a).index\n                round_atom_neighborhoods[a.index] |= atom_neighborhoods[oidx]\n                nbsr.append((b.bondtype, atom_ids[oidx]))\n\n            nbsr = sorted(nbsr)\n            nbsr = [item for sublist in nbsr for item in sublist]\n            nbsr.insert(0, atom_ids[a.index])\n            nbsr.insert(0, layer)\n\n            round_ids[a.index] = gen_hash(nbsr)\n            neighborhoods_this_round.append(\n                (round_atom_neighborhoods[a.index], round_ids[a.index], a.index)\n            )\n\n        for lst in neighborhoods_this_round:\n            if lst[0] not in neighborhoods:\n                fp[lst[1]] = fp.get(lst[1], 0) + 1\n                neighborhoods.append(lst[0])\n            else:\n                dead_atoms[lst[2]] = True\n\n        atom_ids = round_ids\n        atom_neighborhoods = copy.deepcopy(round_atom_neighborhoods)\n    return fp", "response": "Compute the Extended - Connectivity fingerprint for a molecule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self):\n        scheme, location, path, query, _ = urlparse.urlsplit(self.url)\n        assert scheme in (\"http\", \"https\"), \"Unsupported scheme %r\" % scheme\n\n        content_type, body = self._encode_multipart_formdata()\n        handle = getattr(httplib, scheme.upper() + \"Connection\")(location)\n        if self.mock_http:\n            # Don't actually send anything, print to stdout instead\n            handle.sock = parts.Bunch(\n                sendall=lambda x: sys.stdout.write(fmt.to_utf8(\n                    ''.join((c if 32 <= ord(c) < 127 or ord(c) in (8, 10) else u'\\u27ea%02X\\u27eb' % ord(c)) for c in x)\n                )),\n                makefile=lambda dummy, _: StringIO.StringIO(\"\\r\\n\".join((\n                    \"HTTP/1.0 204 NO CONTENT\",\n                    \"Content-Length: 0\",\n                    \"\",\n                ))),\n                close=lambda: None,\n            )\n\n        handle.putrequest('POST', urlparse.urlunsplit(('', '', path, query, '')))\n        handle.putheader('Content-Type', content_type)\n        handle.putheader('Content-Length', str(len(body)))\n        for key, val in self.headers.items():\n            handle.putheader(key, val)\n        handle.endheaders()\n        handle.send(body)\n        #print handle.__dict__\n\n        return handle.getresponse()", "response": "Send the request to an HTTP server as multipart form - data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _encode_multipart_formdata(self):\n        def get_content_type(filename):\n            \"Helper to get MIME type.\"\n            return mimetypes.guess_type(filename)[0] or 'application/octet-stream'\n\n        boundary = '----------ThIs_Is_tHe_b0uNdaRY_%d$' % (time.time())\n        logical_lines = []\n        for name, value in self.fields:\n            if value is None:\n                continue\n            logical_lines.append('--' + boundary)\n            if hasattr(value, \"read\"):\n                filename = getattr(value, \"name\", str(id(value))+\".dat\")\n                logical_lines.append('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' % (\n                    name,\n                    os.path.basename(filename).replace(\"'\", '_').replace('\"', '_')\n                ))\n                logical_lines.append('Content-Type: %s' % get_content_type(filename))\n                logical_lines.append('Content-Transfer-Encoding: binary')\n                value = value.read()\n            else:\n                logical_lines.append('Content-Disposition: form-data; name=\"%s\"' % name)\n                logical_lines.append('Content-Type: text/plain; charset=\"UTF-8\"')\n                value = fmt.to_utf8(value)\n            #logical_lines.append('Content-Length: %d' % len(value))\n            logical_lines.append('')\n            logical_lines.append(value)\n        logical_lines.append('--' + boundary + '--')\n        logical_lines.append('')\n\n        body = '\\r\\n'.join(logical_lines)\n        content_type = 'multipart/form-data; boundary=%s' % boundary\n        return content_type, body", "response": "Encode POST body.\n            Return (content_type, body) ready for httplib.HTTP instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a graph as the union of several MSTs on perturbed data.", "response": "def perturbed_mst(X, num_perturbations=20, metric='euclidean', jitter=None):\r\n  '''Builds a graph as the union of several MSTs on perturbed data.\r\n  Reference: http://ecovision.mit.edu/~sloop/shao.pdf, page 8\r\n  jitter refers to the scale of the gaussian noise added for each perturbation.\r\n  When jitter is None, it defaults to the 5th percentile interpoint distance.\r\n  Note that metric cannot be 'precomputed', as multiple MSTs are computed.'''\r\n  assert metric != 'precomputed'\r\n  D = pairwise_distances(X, metric=metric)\r\n  if jitter is None:\r\n    jitter = np.percentile(D[D>0], 5)\r\n  W = minimum_spanning_tree(D)\r\n  W = W + W.T\r\n  W.data[:] = 1.0  # binarize\r\n  for i in range(num_perturbations):\r\n    pX = X + np.random.normal(scale=jitter, size=X.shape)\r\n    pW = minimum_spanning_tree(pairwise_distances(pX, metric=metric))\r\n    pW = pW + pW.T\r\n    pW.data[:] = 1.0\r\n    W = W + pW\r\n  # final graph is the average over all pertubed MSTs + the original\r\n  W.data /= (num_perturbations + 1.0)\r\n  return Graph.from_adj_matrix(W)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a graph as the union of several spanning trees and each time removing any edges present in previously - built trees.", "response": "def disjoint_mst(X, num_spanning_trees=3, metric='euclidean'):\r\n  '''Builds a graph as the union of several spanning trees,\r\n  each time removing any edges present in previously-built trees.\r\n  Reference: http://ecovision.mit.edu/~sloop/shao.pdf, page 9.'''\r\n  D = pairwise_distances(X, metric=metric)\r\n  if metric == 'precomputed':\r\n    D = D.copy()\r\n  mst = minimum_spanning_tree(D)\r\n  W = mst.copy()\r\n  for i in range(1, num_spanning_trees):\r\n    ii,jj = mst.nonzero()\r\n    D[ii,jj] = np.inf\r\n    D[jj,ii] = np.inf\r\n    mst = minimum_spanning_tree(D)\r\n    W = W + mst\r\n  # MSTs are all one-sided, so we symmetrize here\r\n  return Graph.from_adj_matrix(W + W.T)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kernelize(self, kernel):\n    '''Re-weight according to a specified kernel function.\n    kernel : str, {none, binary, rbf}\n      none   -> no reweighting\n      binary -> all edges are given weight 1\n      rbf    -> applies a gaussian function to edge weights\n    '''\n    if kernel == 'none':\n      return self\n    if kernel == 'binary':\n      if self.is_weighted():\n        return self._update_edges(1, copy=True)\n      return self\n    if kernel == 'rbf':\n      w = self.edge_weights()\n      r = np.exp(-w / w.std())\n      return self._update_edges(r, copy=True)\n    raise ValueError('Invalid kernel type: %r' % kernel)", "response": "Re - weight according to a specified kernel function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connected_subgraphs(self, directed=True, ordered=False):\n    '''Generates connected components as subgraphs.\n    When ordered=True, subgraphs are ordered by number of vertices.\n    '''\n    num_ccs, labels = self.connected_components(directed=directed)\n    # check the trivial case first\n    if num_ccs == 1:\n      yield self\n      raise StopIteration\n    if ordered:\n      # sort by descending size (num vertices)\n      order = np.argsort(np.bincount(labels))[::-1]\n    else:\n      order = range(num_ccs)\n\n    # don't use self.subgraph() here, because we can reuse adj\n    adj = self.matrix('dense', 'csr', 'csc')\n    for c in order:\n      mask = labels == c\n      sub_adj = adj[mask][:,mask]\n      yield self.__class__.from_adj_matrix(sub_adj)", "response": "Generates connected components as subgraphs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef minimum_spanning_subtree(self):\n    '''Returns the (undirected) minimum spanning tree subgraph.'''\n    dist = self.matrix('dense', copy=True)\n    dist[dist==0] = np.inf\n    np.fill_diagonal(dist, 0)\n    mst = ssc.minimum_spanning_tree(dist)\n    return self.__class__.from_adj_matrix(mst + mst.T)", "response": "Returns the undirected minimum spanning tree subgraph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a subgraph containing only vertices within a given geodesic radius of start_idx.", "response": "def neighborhood_subgraph(self, start_idx, radius=1, weighted=True,\n                            directed=True, return_mask=False):\n    '''Returns a subgraph containing only vertices within a given\n       geodesic radius of start_idx.'''\n    adj = self.matrix('dense', 'csr', 'csc')\n    dist = ssc.dijkstra(adj, directed=directed, indices=start_idx,\n                        unweighted=(not weighted), limit=radius)\n    mask = np.isfinite(dist)\n    sub_adj = adj[mask][:,mask]\n    g = self.__class__.from_adj_matrix(sub_adj)\n    if return_mask:\n      return g, mask\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove short - circuit edges using the Isograph algorithm.", "response": "def isograph(self, min_weight=None):\n    '''Remove short-circuit edges using the Isograph algorithm.\n\n    min_weight : float, optional\n        Minimum weight of edges to consider removing. Defaults to max(MST).\n\n    From \"Isograph: Neighbourhood Graph Construction Based On Geodesic Distance\n          For Semi-Supervised Learning\" by Ghazvininejad et al., 2011.\n    Note: This uses the non-iterative algorithm which removes edges\n        rather than reweighting them.\n    '''\n    W = self.matrix('dense')\n    # get candidate edges: all edges - MST edges\n    tree = self.minimum_spanning_subtree()\n    candidates = np.argwhere((W - tree.matrix('dense')) > 0)\n    cand_weights = W[candidates[:,0], candidates[:,1]]\n    # order by increasing edge weight\n    order = np.argsort(cand_weights)\n    cand_weights = cand_weights[order]\n    # disregard edges shorter than a threshold\n    if min_weight is None:\n      min_weight = tree.edge_weights().max()\n    idx = np.searchsorted(cand_weights, min_weight)\n    cand_weights = cand_weights[idx:]\n    candidates = candidates[order[idx:]]\n    # check each candidate edge\n    to_remove = np.zeros_like(cand_weights, dtype=bool)\n    for i, (u,v) in enumerate(candidates):\n      W_uv = np.where(W < cand_weights[i], W, 0)\n      len_uv = ssc.dijkstra(W_uv, indices=u, unweighted=True, limit=2)[v]\n      if len_uv > 2:\n        to_remove[i] = True\n    ii, jj = candidates[to_remove].T\n    return self.remove_edges(ii, jj, copy=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cycle_cut(self, cycle_len_thresh=12, directed=False, copy=True):\n    '''CycleCut algorithm: removes bottleneck edges.\n    Paper DOI: 10.1.1.225.5335\n    '''\n    symmetric = not directed\n    adj = self.kernelize('binary').matrix('csr', 'dense', copy=True)\n    if symmetric:\n      adj = adj + adj.T\n\n    removed_edges = []\n    while True:\n      c = _atomic_cycle(adj, cycle_len_thresh, directed=directed)\n      if c is None:\n        break\n      # remove edges in the cycle\n      ii, jj = c.T\n      adj[ii,jj] = 0\n      if symmetric:\n        adj[jj,ii] = 0\n      removed_edges.extend(c)\n\n    #XXX: if _atomic_cycle changes, may need to do this on each loop\n    if ss.issparse(adj):\n      adj.eliminate_zeros()\n\n    # select only the necessary cuts\n    ii, jj = _find_cycle_inducers(adj, removed_edges, cycle_len_thresh,\n                                  directed=directed)\n    # remove the bad edges\n    return self.remove_edges(ii, jj, symmetric=symmetric, copy=copy)", "response": "CycleCut algorithm: removes bottleneck edges.\n    Paper DOI: 10.1.1.225.5335"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fuzzer(buffer, fuzz_factor=101):\n    buf = deepcopy(buffer)\n    num_writes = number_of_bytes_to_modify(len(buf), fuzz_factor)\n    for _ in range(num_writes):\n        random_byte = random.randrange(256)\n        random_position = random.randrange(len(buf))\n        buf[random_position] = random_byte\n    return buf", "response": "Fuzz given buffer.\n\n    Take a buffer of bytes, create a copy, and replace some bytes\n    with random values. Number of bytes to modify depends on fuzz_factor.\n    This code is taken from Charlie Miller's fuzzer code.\n\n    :param buffer: the data to fuzz.\n    :type buffer: byte array\n    :param fuzz_factor: degree of fuzzing.\n    :type fuzz_factor: int\n    :return: fuzzed buffer.\n    :rtype: byte array"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef number_of_bytes_to_modify(buf_len, fuzz_factor):\n    return random.randrange(math.ceil((float(buf_len) / fuzz_factor))) + 1", "response": "Calculate number of bytes to modify."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating fuzzed variant of given file.", "response": "def _fuzz_data_file(self, data_file):\n        \"\"\"Generate fuzzed variant of given file.\n\n        :param data_file: path to file to fuzz.\n        :type data_file: str\n        :return: path to fuzzed file.\n        :rtype: str\n        \"\"\"\n        buf = bytearray(open(os.path.abspath(data_file), 'rb').read())\n        fuzzed = fuzzer(buf, self.fuzz_factor)\n        try:\n            _, fuzz_output = mkstemp(prefix='fuzzed_')\n            open(fuzz_output, 'wb').write(fuzzed)\n        finally:\n            pass\n        return fuzz_output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _execute(self, app_, file_):\n        app_name = os.path.basename(app_)\n        args = [app_]\n        args.extend(self.args[app_])\n        args.append(file_)\n        process = subprocess.Popen(args)\n\n        time.sleep(1)\n        status = {True: Status.SUCCESS, False: Status.FAILED}\n        crashed = process.poll()\n        result = status[crashed is None]\n        self.stats_.add(app_name, result)\n        if result is Status.SUCCESS:\n            # process did not crash, so just terminate it\n            process.terminate()", "response": "Execute the application with file as input."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse list of apps for arguments.", "response": "def __parse_app_list(app_list):\n        \"\"\"Parse list of apps for arguments.\n\n        :param app_list: list of apps with optional arguments.\n        :return: list of apps and assigned argument dict.\n        :rtype: [String], {String: [String]}\n        \"\"\"\n        args = {}\n        apps = []\n        for app_str in app_list:\n            parts = app_str.split(\"&\")\n            app_path = parts[0].strip()\n            apps.append(app_path)\n            if len(parts) > 1:\n                args[app_path] = [arg.strip() for arg in parts[1].split()]\n            else:\n                args[app_path] = []\n        return apps, args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_tonnetz_matrix():\n    pi = np.pi\n    chroma = np.arange(12)\n\n    # Define each row of the transform matrix\n    fifth_x = r_fifth*(np.sin((7*pi/6) * chroma))\n    fifth_y = r_fifth*(np.cos((7*pi/6) * chroma))\n    minor_third_x = r_minor_thirds*(np.sin(3*pi/2 * chroma))\n    minor_third_y = r_minor_thirds*(np.cos(3*pi/2 * chroma))\n    major_third_x = r_major_thirds*(np.sin(2*pi/3 * chroma))\n    major_third_y = r_major_thirds*(np.cos(2*pi/3 * chroma))\n\n    # Return the tonnetz matrix\n    return np.vstack((fifth_x, fifth_y,\n                      minor_third_x, minor_third_y,\n                      major_third_x, major_third_y))", "response": "Return the tonnetz projection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _to_tonnetz(chromagram):\n    if np.sum(np.abs(chromagram)) == 0.:\n        # The input is an empty chord, return zero. \n        return np.zeros(6)\n    \n    _tonnetz = np.dot(__TONNETZ_MATRIX, chromagram)\n    one_norm = np.sum(np.abs(_tonnetz))  # Non-zero value\n    _tonnetz = _tonnetz / float(one_norm) # Normalize tonnetz vector\n    return _tonnetz", "response": "Project a chromagram on the tonnetz. Returns value is normalized to prevent numerical instabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distance(a, b):\n    [a_tonnetz, b_tonnetz] = [_to_tonnetz(x) for x in [a, b]]\n    return np.linalg.norm(b_tonnetz - a_tonnetz)", "response": "Compute tonnetz - distance between two chromagrams."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint an error in red text.", "response": "def fail_print(error):\n    \"\"\"Print an error in red text.\n    Parameters\n        error (HTTPError)\n            Error object to print.\n    \"\"\"\n    print(COLORS.fail, error.message, COLORS.end)\n    print(COLORS.fail, error.errors, COLORS.end)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting OAuth 2. 0 session credentials from the given file.", "response": "def import_oauth2_credentials(filename=STORAGE_FILENAME):\n    \"\"\"Import OAuth 2.0 session credentials from storage file.\n    Parameters\n        filename (str)\n            Name of storage file.\n    Returns\n        credentials (dict)\n            All your app credentials and information\n            imported from the configuration file.\n    \"\"\"\n    with open(filename, 'r') as storage_file:\n        storage = safe_load(storage_file)\n\n    # depending on OAuth 2.0 grant_type, these values may not exist\n    client_secret = storage.get('client_secret')\n    refresh_token = storage.get('refresh_token')\n\n    credentials = {\n        'access_token': storage['access_token'],\n        'client_id': storage['client_id'],\n        'client_secret': client_secret,\n        'expires_in_seconds': storage['expires_in_seconds'],\n        'grant_type': storage['grant_type'],\n        'refresh_token': refresh_token,\n        'scopes': storage['scopes'],\n    }\n\n    return credentials"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an authorized LyftRidesClient from OAuth 2. 0 credentials.", "response": "def create_lyft_client(credentials):\n    \"\"\"Create an LyftRidesClient from OAuth 2.0 credentials.\n    Parameters\n        credentials (dict)\n            Dictionary of OAuth 2.0 credentials.\n    Returns\n        (LyftRidesClient)\n            An authorized LyftRidesClient to access API resources.\n    \"\"\"\n    oauth2credential = OAuth2Credential(\n        client_id=credentials.get('client_id'),\n        access_token=credentials.get('access_token'),\n        expires_in_seconds=credentials.get('expires_in_seconds'),\n        scopes=credentials.get('scopes'),\n        grant_type=credentials.get('grant_type'),\n        client_secret=credentials.get('client_secret'),\n        refresh_token=credentials.get('refresh_token'),\n    )\n    session = Session(oauth2credential=oauth2credential)\n    return LyftRidesClient(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spawn_managed_host(config_file, manager, connect_on_start=True):\n\n    data = manager.request_host_status(config_file)\n\n    is_running = data['started']\n\n    # Managed hosts run as persistent processes, so it may already be running\n    if is_running:\n        host_status = json.loads(data['host']['output'])\n        logfile = data['host']['logfile']\n    else:\n        data = manager.start_host(config_file)\n        host_status = json.loads(data['output'])\n        logfile = data['logfile']\n\n    host = JSHost(\n        status=host_status,\n        logfile=logfile,\n        config_file=config_file,\n        manager=manager\n    )\n\n    if not is_running and settings.VERBOSITY >= verbosity.PROCESS_START:\n        print('Started {}'.format(host.get_name()))\n\n    if connect_on_start:\n        host.connect()\n\n    return host", "response": "Spawns a managed host if it is not already running."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the given input and return a datetime object for the UTC timezone.", "response": "def parse_input(s):\n    \"\"\"Parse the given input and intelligently transform it into an absolute,\n    non-naive, timezone-aware datetime object for the UTC timezone.\n\n    The input can be specified as a millisecond-precision UTC timestamp (or\n    delta against Epoch), with or without a terminating 'L'. Alternatively, the\n    input can be specified as a human-readable delta string with unit-separated\n    segments, like '24d6h4m500' (24 days, 6 hours, 4 minutes and 500ms), as\n    long as the segments are in descending unit span order.\"\"\"\n    if isinstance(s, six.integer_types):\n        s = str(s)\n    elif not isinstance(s, six.string_types):\n        raise ValueError(s)\n\n    original = s\n\n    if s[-1:] == 'L':\n        s = s[:-1]\n\n    sign = {'-': -1, '=': 0, '+': 1}.get(s[0], None)\n    if sign is not None:\n        s = s[1:]\n\n    ts = 0\n    for unit in _SORTED_UNITS:\n        pos = s.find(unit[0])\n        if pos == 0:\n            raise ValueError(original)\n        elif pos > 0:\n            # If we find a unit letter, we're dealing with an offset. Default\n            # to positive offset if a sign wasn't specified.\n            if sign is None:\n                sign = 1\n            ts += int(s[:pos]) * __timedelta_millis(unit[1])\n            s = s[min(len(s), pos + 1):]\n\n    if s:\n        ts += int(s)\n\n    return date_from_utc_ts(ts) if not sign else \\\n        utc() + sign * delta(milliseconds=ts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders the given delta as a human - readable delta.", "response": "def render_delta(d):\n    \"\"\"Render the given delta (in milliseconds) as a human-readable delta.\"\"\"\n    s = '' if d >= 0 else '-'\n    d = abs(d)\n\n    for unit in _SORTED_UNITS:\n        span = __timedelta_millis(unit[1])\n        if d >= span:\n            count = int(d // span)\n            s += '{0}{1}'.format(count, unit[0])\n            d -= count * span\n\n    if d or not s:\n        s += str(d)\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_date(date, tz=pytz.utc, fmt=_FULL_OUTPUT_FORMAT):\n    local = date.astimezone(tz)\n    ts = __date_to_millisecond_ts(date)\n    return fmt.format(\n            ts=ts,\n            utc=date.strftime(_DATE_FORMAT),\n            millis=ts % 1000,\n            utc_tz=date.strftime(_TZ_FORMAT),\n            local=local.strftime(_DATE_FORMAT),\n            local_tz=local.strftime(_TZ_FORMAT),\n            delta=render_delta_from_now(date))", "response": "Format the given date for output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a list of arguments returning a dict.", "response": "def parse(self, complete=True, args=None):\n        '''Parse a list of arguments, returning a dict\n\n        See BooleanParser.parse for `args`-related documentation.\n\n        If `complete` is True and there are values in `args` that don't have corresponding arguments,\n        or there are required arguments that don't have args, then raise an error.\n        '''\n        opts = dict()\n        positions = [argument.positional for argument in self.arguments if argument.positional]\n\n        if args is None:\n            import sys\n            # skip over the program name with the [1:] slice\n            args = sys.argv[1:]\n\n        # arglist is a tuple of (is_flag, name) pairs\n        arglist = peekable(parse_tokens(args))\n        for is_flag, name in arglist:\n            if is_flag is True:\n                argument = self.find_argument(name)\n\n                # .peek will return the default argument iff there are no more entries\n                next_is_flag, next_name = arglist.peek(default=(None, None))\n                # next_is_flag will be None if there are no more items, but True/False if there is a next item\n\n                # if this argument looks for a subsequent (is set as boolean), and the subsequent is not a flag, consume it\n                if argument.boolean is False and next_is_flag is False:\n                    opts[name] = next_name\n                    # finally, advance our iterator, but since we already have the next values, just discard it\n                    arglist.next()\n                else:\n                    # if there is no next, or the next thing is a flag all the boolean=False's in the world can't save you then\n                    opts[name] = True\n            else:\n                # add positional argument\n                if len(positions) > 0:\n                    # we pop the positions off from the left\n                    position = positions.pop(0)\n                    opts[position] = name\n                else:\n                    # the rest of the args now end up as a list in '_'\n                    opts.setdefault('_', []).append(name)\n\n        # propagate aliases and defaults:\n        for argument in self.arguments:\n            # merge provided value from aliases\n            for name in argument.names:\n                if name in opts:\n                    value = opts[name]\n                    # we simply break on the first match.\n                    break\n            else:\n                # if we iterate through all names and fine none in opts, use the default\n                value = argument.default\n\n            for name in argument.names:\n                opts[name] = value\n\n        return opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperturb the values of the parameters of the current object.", "response": "def perturbParams(self, pertSize=1e-3):\n        \"\"\"\n        slightly perturbs the values of the parameters\n        \"\"\"\n        params = self.getParams()\n        self.setParams(params+pertSize*sp.randn(params.shape[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Kgrad_param_num(self,i,h=1e-4):\n        params = self.getParams()\n        e = sp.zeros_like(params); e[i] = 1\n        self.setParams(params-h*e)\n        C_L = self.K()\n        self.setParams(params+h*e)\n        C_R = self.K()\n        self.setParams(params)\n        RV = (C_R-C_L)/(2*h)\n        return RV", "response": "calculate the K - gradient of the parameter i"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the SNR of binaries.", "response": "def snr(*args, **kwargs):\n    \"\"\"Compute the SNR of binaries.\n\n    snr is a function that takes binary parameters and sensitivity curves as inputs,\n    and returns snr for chosen phases.\n\n    Warning: All binary parameters must be either scalar, len-1 arrays,\n    or arrays of the same length. All of these can be used at once. However,\n    you cannot input multiple arrays of different lengths.\n\n    Arguments:\n        *args: Arguments for :meth:`gwsnrcalc.utils.pyphenomd.PhenomDWaveforms.__call__`\n        **kwargs: Keyword arguments related to\n            parallel generation (see :class:`gwsnrcalc.utils.parallel`),\n            waveforms (see :class:`gwsnrcalc.utils.pyphenomd`),\n            or sensitivity information (see :class:`gwsnrcalc.utils.sensitivity`).\n\n    Returns:\n        (dict or list of dict): Signal-to-Noise Ratio dictionary for requested phases.\n\n    \"\"\"\n    squeeze = False\n    max_length = 0\n    for arg in args:\n        try:\n            length = len(arg)\n            if length > max_length:\n                max_length = length\n\n        except TypeError:\n            pass\n\n    if max_length == 0:\n        squeeze = True\n\n    kwargs['length'] = max_length\n\n    snr_main = SNR(**kwargs)\n    if squeeze:\n        snr_out = snr_main(*args)\n        return {key: np.squeeze(snr_out[key]) for key in snr_out}\n    return snr_main(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smilin(smiles, transforms=[figueras.sssr, aromaticity.aromatize]):\r\n    builder = BuildMol()\r\n    tokenize(smiles, builder)\r\n    mol = builder.mol\r\n\r\n        \r\n    for transform in transforms:\r\n        mol = transform(mol)\r\n\r\n    ## implicit hcount doesn't make any sense anymore...\r\n    for atom in mol.atoms:\r\n        if not atom.has_explicit_hcount:\r\n            atom.imp_hcount = atom.hcount - atom.explicit_hcount\r\n\r\n    return mol", "response": "Convert a smiles string into a molecule representation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a list of arguments returning a dict.", "response": "def parse(self, args=None):\n        '''Parse a list of arguments, returning a dict.\n\n        Flags are only boolean if they are not followed by a non-flag argument.\n\n        All positional arguments not associable with a flag will be added to the return dictionary's `['_']` field.\n        '''\n        opts = dict()\n\n        if args is None:\n            import sys\n            # skip over the program name with the [1:] slice\n            args = sys.argv[1:]\n\n        # arglist is a tuple of (is_flag, name) pairs\n        arglist = peekable(parse_tokens(args))\n        for is_flag, name in arglist:\n            if is_flag is True:\n                # .peek will return the default argument iff there are no more entries\n                next_is_flag, next_name = arglist.peek(default=(None, None))\n                # next_is_flag will be None if there are no more items, but True/False if there is a next item\n\n                # if this argument looks for a subsequent (is set as boolean),\n                #   and the subsequent is not a flag, consume it\n                if next_is_flag is False:\n                    opts[name] = next_name\n                    # finally, advance our iterator, but since we already have the next values, just discard it\n                    arglist.next()\n                else:\n                    # if there is no next thing, or the next thing is a flag,\n                    #   all the boolean=False's in the world can't save you then\n                    opts[name] = True\n            else:\n                # add positional argument\n                opts.setdefault('_', []).append(name)\n\n        return opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a matrix of all pariwise squared distances", "response": "def sq_dist(X1,X2=None):\n    \"\"\"\n    computes a matrix of all pariwise squared distances\n    \"\"\"\n    if X2==None:\n        X2 = X1\n    assert X1.shape[1]==X2.shape[1], 'dimensions do not match'\n\n    n = X1.shape[0]\n    m = X2.shape[0]\n    d = X1.shape[1]\n    # (X1 - X2)**2 = X1**2 + X2**2 - 2X1X2\n    X1sq = sp.reshape((X1**2).sum(1),n,1)\n    X2sq = sp.reshape((X2**2).sum(1),m,1)\n\n    K = sp.tile((X1*X1).sum(1),(m,1)).T + sp.tile((X2*X2).sum(1),(n,1)) - 2*sp.dot(X1,X2.T)\n    return K"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the double bond orientation for bond1 and bond2 based on this bond", "response": "def setdbo(self, bond1, bond2, dboval):\n        \"\"\"Set the double bond orientation for bond1 and bond2\n        based on this bond\"\"\"\n        # this bond must be a double bond\n        if self.bondtype != 2:\n            raise FrownsError(\"To set double bond order, center bond must be double!\")\n        assert dboval in [DX_CHI_CIS, DX_CHI_TRANS, DX_CHI_NO_DBO], \"bad dboval value\"\n        \n        self.dbo.append(bond1, bond2, dboval)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the symbol of the molecule", "response": "def set_symbol(self, symbol):\n        \"\"\"(symbol, bondorder) -> set the bondsymbol\n        of the molecule\"\"\"\n        raise \"Deprecated\"\n        self.symbol, self.bondtype, bondorder, self.equiv_class = \\\n                     BONDLOOKUP[symbol]\n        if self.bondtype == 4:\n            self.aromatic = 1\n        else:\n            self.aromatic = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the atom at the other end of this bond", "response": "def xatom(self, atom):\n        \"\"\"(atom)->return the atom at the other end of this bond\n        or None if atom is not part of this bond\"\"\"\n        handle = atom.handle\n        \n        if handle == self.atoms[0].handle:\n            return self.atoms[1]\n        elif handle == self.atoms[1].handle:\n            return self.atoms[0]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves detected segments to a. lab file.", "response": "def save_segments(outfile, boundaries, beat_intervals, labels=None):\n    \"\"\"Save detected segments to a .lab file.\n\n    :parameters:\n        - outfile : str\n            Path to output file\n\n        - boundaries : list of int\n            Beat indices of detected segment boundaries\n\n        - beat_intervals : np.ndarray [shape=(n, 2)]\n            Intervals of beats\n\n        - labels : None or list of str\n            Labels of detected segments\n    \"\"\"\n\n    if labels is None:\n        labels = [('Seg#%03d' % idx) for idx in range(1, len(boundaries))]\n\n    times = [beat_intervals[beat, 0] for beat in boundaries[:-1]]\n    times.append(beat_intervals[-1, -1])\n\n    with open(outfile, 'w') as f:\n        for idx, (start, end, lab) in enumerate(zip(times[:-1],\n                                                    times[1:],\n                                                    labels), 1):\n            f.write('%.3f\\t%.3f\\t%s\\n' % (start, end, lab))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping Django s reverse to prepend the correct locale.", "response": "def reverse(viewname, urlconf=None, args=None, kwargs=None, prefix=None):\n    \"\"\"Wraps Django's reverse to prepend the correct locale.\"\"\"\n    prefixer = get_url_prefix()\n\n    if prefixer:\n        prefix = prefix or '/'\n    url = django_reverse(viewname, urlconf, args, kwargs, prefix)\n    if prefixer:\n        url = prefixer.fix(url)\n\n    # Ensure any unicode characters in the URL are escaped.\n    return iri_to_uri(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the requested path into locale and path.", "response": "def split_path(path_):\n    \"\"\"\n    Split the requested path into (locale, path).\n\n    locale will be empty if it isn't found.\n    \"\"\"\n    path = path_.lstrip('/')\n\n    # Use partitition instead of split since it always returns 3 parts\n    first, _, rest = path.partition('/')\n\n    lang = first.lower()\n    if lang in settings.LANGUAGE_URL_MAP:\n        return settings.LANGUAGE_URL_MAP[lang], rest\n    else:\n        supported = find_supported(first)\n        if len(supported):\n            return supported[0], rest\n        else:\n            return '', path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a locale code that we can use for this user.", "response": "def get_language(self):\n        \"\"\"\n        Return a locale code we support on the site using the\n        user's Accept-Language header to determine which is best. This\n        mostly follows the RFCs but read bug 439568 for details.\n        \"\"\"\n        if 'lang' in self.request.GET:\n            lang = self.request.GET['lang'].lower()\n            if lang in settings.LANGUAGE_URL_MAP:\n                return settings.LANGUAGE_URL_MAP[lang]\n\n        if self.request.META.get('HTTP_ACCEPT_LANGUAGE'):\n            best = self.get_best_language(\n                self.request.META['HTTP_ACCEPT_LANGUAGE'])\n            if best:\n                return best\n        return settings.LANGUAGE_CODE"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive an Accept - Language header return the best - matching language.", "response": "def get_best_language(self, accept_lang):\n        \"\"\"Given an Accept-Language header, return the best-matching language.\"\"\"\n        LUM = settings.LANGUAGE_URL_MAP\n        langs = dict(LUM.items() + settings.CANONICAL_LOCALES.items())\n        # Add missing short locales to the list. This will automatically map\n        # en to en-GB (not en-US), es to es-AR (not es-ES), etc. in alphabetical\n        # order. To override this behavior, explicitly define a preferred locale\n        # map with the CANONICAL_LOCALES setting.\n        langs.update((k.split('-')[0], v) for k, v in LUM.items() if\n                     k.split('-')[0] not in langs)\n        try:\n            ranked = parse_accept_lang_header(accept_lang)\n        except ValueError:  # see https://code.djangoproject.com/ticket/21078\n            return\n        else:\n            for lang, _ in ranked:\n                lang = lang.lower()\n                if lang in langs:\n                    return langs[lang]\n                pre = lang.split('-')[0]\n                if pre in langs:\n                    return langs[pre]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extensions():\n    import numpy\n    from Cython.Build import cythonize\n    ext = [\n            Extension('phydmslib.numutils', ['phydmslib/numutils.pyx'],\n                    include_dirs=[numpy.get_include()],\n                    extra_compile_args=['-Wno-unused-function']),\n          ]      \n    return cythonize(ext)", "response": "Returns list of cython extensions for lazy_cythonize."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(connection: connection, statement: str) -> Optional[List[Tuple[str, ...]]]:\n    response = list()  # type: List\n\n    # See the following link for reasoning behind both with statements:\n    #   http://initd.org/psycopg/docs/usage.html#with-statement\n    #\n    # Additionally, the with statement makes this library safer to use with\n    # higher-level libraries (e.g. SQLAlchemy) that don't inherently respect\n    # PostGreSQL's autocommit isolation-level, since the transaction is\n    # properly completed for each statement.\n    with connection:\n        with connection.cursor(cursor_factory=Psycopg2Cursor) as cursor:\n            cursor.execute(statement)\n            connection.commit()\n\n            # Get response\n            try:\n                response = cursor.fetchall()\n                if not response:\n                    # Empty response list\n                    log('<No Response>', logger_name=_LOGGER_NAME)\n                    return None\n            except ProgrammingError as e:\n                if e.args and e.args[0] == 'no results to fetch':\n                    # No response available (i.e. no response given)\n                    log('<No Response>', logger_name=_LOGGER_NAME)\n                    return None\n\n                # Some other programming error; re-raise\n                raise e\n\n            log('Response', logger_name=_LOGGER_NAME)\n            log('--------', logger_name=_LOGGER_NAME)\n            for line in response:\n                log(str(line), logger_name=_LOGGER_NAME)\n\n    return response", "response": "Executes a PGSQL statement and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nappends the given traverse to the current traverse", "response": "def append(self, traverse):\n        \"\"\"(traverse)->append the traverse to the current traverse\"\"\"\n        self.data.extend(traverse.data)\n        self.atoms.extend(traverse.atoms)\n        self.bonds.extend(traverse.bonds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a human - readable representation of a byte size.", "response": "def human_size(size):\n    \"\"\" Return a human-readable representation of a byte size.\n\n        @param size: Number of bytes as an integer or string.\n        @return: String of length 10 with the formatted result.\n    \"\"\"\n    if isinstance(size, string_types):\n        size = int(size, 10)\n\n    if size < 0:\n        return \"-??? bytes\"\n\n    if size < 1024:\n        return \"%4d bytes\" % size\n    for unit in (\"KiB\", \"MiB\", \"GiB\"):\n        size /= 1024.0\n        if size < 1024:\n            return \"%6.1f %s\" % (size, unit)\n\n    return \"%6.1f GiB\" % size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iso_datetime(timestamp=None):\n    if timestamp is None:\n        timestamp = time.time()\n    return datetime.datetime.fromtimestamp(timestamp).isoformat(' ')[:19]", "response": "Convert UNIX timestamp to ISO datetime string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a human - readable representation of a time delta.", "response": "def human_duration(time1, time2=None, precision=0, short=False):\n    \"\"\" Return a human-readable representation of a time delta.\n\n        @param time1: Relative time value.\n        @param time2: Time base (C{None} for now; 0 for a duration in C{time1}).\n        @param precision: How many time units to return (0 = all).\n        @param short: Use abbreviations, and right-justify the result to always the same length.\n        @return: Formatted duration.\n    \"\"\"\n    if time2 is None:\n        time2 = time.time()\n\n    duration = (time1 or 0) - time2\n    direction = (\n        \" ago\" if duration < 0 else\n        (\"+now\" if short else \" from now\") if time2 else \"\"\n    )\n    duration = abs(duration)\n    parts = [\n        (\"weeks\", duration // (7*86400)),\n        (\"days\", duration // 86400 % 7),\n        (\"hours\", duration // 3600 % 24),\n        (\"mins\", duration // 60 % 60),\n        (\"secs\", duration % 60),\n    ]\n\n    # Kill leading zero parts\n    while len(parts) > 1 and parts[0][1] == 0:\n        parts = parts[1:]\n\n    # Limit to # of parts given by precision\n    if precision:\n        parts = parts[:precision]\n\n    numfmt = (\"%d\", \"%d\"), (\"%4d\", \"%2d\")\n    fmt = \"%1.1s\" if short else \" %s\"\n    sep = \" \" if short else \", \"\n    result = sep.join((numfmt[bool(short)][bool(idx)] + fmt) % (val, key[:-1] if val == 1 else key)\n        for idx, (key, val) in enumerate(parts)\n        if val #or (short and precision)\n    ) + direction\n\n    if not time1:\n        result = \"never\" if time2 else \"N/A\"\n\n    if precision and short:\n        return result.rjust(1 + precision*4 + (4 if time2 else 0))\n    else:\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a decoded unicode string.", "response": "def to_unicode(text):\n    \"\"\" Return a decoded unicode string.\n        False values are returned untouched.\n    \"\"\"\n    if not text or isinstance(text, unicode if PY2 else str):\n        return text\n\n    try:\n        # Try UTF-8 first\n        return text.decode(\"UTF-8\")\n    except UnicodeError:\n        try:\n            # Then Windows Latin-1\n            return text.decode(\"CP1252\")\n        except UnicodeError:\n            # Give up, return byte string in the hope things work out\n            return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_program(self, p):\n        p[0] = self.asttypes.ES5Program(p[1])\n        p[0].setpos(p)", "response": "Parse the program tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_block(self, p):\n        p[0] = self.asttypes.Block(p[2])\n        p[0].setpos(p)", "response": "A block is a simple block."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_boolean_literal(self, p):\n        p[0] = self.asttypes.Boolean(p[1])\n        p[0].setpos(p)", "response": "P_boolean_literal is a literal that is a boolean."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_null_literal(self, p):\n        p[0] = self.asttypes.Null(p[1])\n        p[0].setpos(p)", "response": "P_NULL_LITERAL is a literal that is a null value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the reserved_word token.", "response": "def p_reserved_word(self, p):\n        \"\"\"reserved_word : BREAK\n                         | CASE\n                         | CATCH\n                         | CONTINUE\n                         | DEBUGGER\n                         | DEFAULT\n                         | DELETE\n                         | DO\n                         | ELSE\n                         | FINALLY\n                         | FOR\n                         | FUNCTION\n                         | IF\n                         | IN\n                         | INSTANCEOF\n                         | NEW\n                         | RETURN\n                         | SWITCH\n                         | THIS\n                         | THROW\n                         | TRY\n                         | TYPEOF\n                         | VAR\n                         | VOID\n                         | WHILE\n                         | WITH\n                         | NULL\n                         | TRUE\n                         | FALSE\n                         | CLASS\n                         | CONST\n                         | ENUM\n                         | EXPORT\n                         | EXTENDS\n                         | IMPORT\n                         | SUPER\n        \"\"\"\n        p[0] = self.asttypes.Identifier(p[1])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_primary_expr_no_brace_2(self, p):\n        p[0] = self.asttypes.This()\n        p[0].setpos(p)", "response": "Primary expression no brace."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_array_literal_2(self, p):\n        items = p[2]\n        if len(p) == 6:\n            items.extend(p[4])\n        p[0] = self.asttypes.Array(items=items)\n        p[0].setpos(p)", "response": "P array literal | array_literal | array_literal RBRACKET element_list COMMA elision_opt RBRACKET"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_elision(self, p):\n        if len(p) == 2:\n            p[0] = [self.asttypes.Elision(1)]\n            p[0][0].setpos(p)\n        else:\n            # increment the Elision value.\n            p[1][-1].value += 1\n            p[0] = p[1]\n        # TODO there should be a cleaner API for the lexer and their\n        # token types for ensuring that the mappings are available.\n        p[0][0]._token_map = {(',' * p[0][0].value): [\n            p[0][0].findpos(p, 0)]}\n        return", "response": "parse the elision token"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_property_list(self, p):\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[1].append(p[3])\n            p[0] = p[1]", "response": "property_list : property_assignment\n                         | property_list COMMA property_assignment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_new_expr(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.NewExpr(p[2])\n            p[0].setpos(p)", "response": "handle NEW | member_expr\n                    | NEW new_expr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_new_expr_nobf(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.NewExpr(p[2])\n            p[0].setpos(p)", "response": "P new_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | NEW | member_expr_nobf | |"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_postfix_expr(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.PostfixExpr(op=p[2], value=p[1])\n            p[0].setpos(p, 2)", "response": "P = postfix_expr | postfix_expr | left_hand_side_expr PLUSPLUS\n                        | left_hand_side_expr MINUSMINUS\n            | left_hand_side_expr MINUSMINUS\n            | left_hand_side_expr MINUSMINUS\n            | left_hand_side_expr MINUSMINUS\n            | left_hand_side_expr MINUSMINUS\n            | left_hand_side_expr p"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a unary expression common section", "response": "def p_unary_expr_common(self, p):\n        \"\"\"unary_expr_common : DELETE unary_expr\n                             | VOID unary_expr\n                             | TYPEOF unary_expr\n                             | PLUSPLUS unary_expr\n                             | MINUSMINUS unary_expr\n                             | PLUS unary_expr\n                             | MINUS unary_expr\n                             | BNOT unary_expr\n                             | NOT unary_expr\n        \"\"\"\n        p[0] = self.asttypes.UnaryExpr(p[1], p[2])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_multiplicative_expr(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.BinOp(op=p[2], left=p[1], right=p[3])\n            p[0].setpos(p, 2)", "response": "multiplicative_expr : unary_expr\n                               | multiplicative_expr MULT unary_expr\n                               | multiplicative_expr DIV unary_expr\n                               | multiplicative_expr MOD unary_expr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_conditional_expr(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.Conditional(\n                predicate=p[1], consequent=p[3], alternative=p[5])\n            p[0].setpos(p, 2)", "response": "\\ n conditional_expr \\ n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_assignment_expr_noin(self, p):\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = self.asttypes.Assign(left=p[1], op=p[2], right=p[3])\n            p[0].setpos(p, 2)", "response": "Assign expression for noin - cluster entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_variable_declaration_list(self, p):\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[1].append(p[3])\n            p[0] = p[1]", "response": "\\ n variable_declaration_list \\"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_variable_declaration(self, p):\n        if len(p) == 2:\n            p[0] = self.asttypes.VarDecl(p[1])\n            p[0].setpos(p)  # require yacc_tracking\n        else:\n            p[0] = self.asttypes.VarDecl(p[1], p[2])\n            p[0].setpos(p, additional=(('=', 2),))", "response": "variable_declaration | variable_declaration | identifier\n                                | identifier initializer\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_variable_declaration_noin(self, p):\n        if len(p) == 2:\n            p[0] = self.asttypes.VarDecl(p[1])\n            p[0].setpos(p)  # require yacc_tracking\n        else:\n            p[0] = self.asttypes.VarDecl(p[1], p[2])\n            p[0].setpos(p, additional=(('=', 2),))", "response": "variable_declaration_noin is a helper function for setting up the variable declaration in the next version of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_expr_statement(self, p):\n        # In 12.4, expression statements cannot start with either the\n        # 'function' keyword or '{'.  However, the lexing and production\n        # of the FuncExpr nodes can be done through further rules have\n        # been done, so flag this as an exception, but must be raised\n        # like so due to avoid the SyntaxError being flagged by ply and\n        # which would result in an infinite loop in this case.\n\n        if isinstance(p[1], self.asttypes.FuncExpr):\n            _, line, col = p[1].getpos('(', 0)\n            raise ProductionError(ECMASyntaxError(\n                'Function statement requires a name at %s:%s' % (line, col)))\n\n        # The most bare 'block' rule is defined as part of 'statement'\n        # and there are no other bare rules that would result in the\n        # production of such like for 'function_expr'.\n\n        p[0] = self.asttypes.ExprStatement(p[1])\n        p[0].setpos(p)", "response": "Parse an expression statement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the DO statement.", "response": "def p_iteration_statement_1(self, p):\n        \"\"\"\n        iteration_statement \\\n            : DO statement WHILE LPAREN expr RPAREN SEMI\n            | DO statement WHILE LPAREN expr RPAREN AUTOSEMI\n        \"\"\"\n        p[0] = self.asttypes.DoWhile(predicate=p[5], statement=p[2])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_iteration_statement_2(self, p):\n        p[0] = self.asttypes.While(predicate=p[3], statement=p[5])\n        p[0].setpos(p)", "response": "P 2 ) WHILE LPAREN expr RPAREN statement"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_iteration_statement_3(self, p):\n        def wrap(node, key):\n            if node is None:\n                # work around bug with yacc tracking of empty elements\n                # by using the previous token, and increment the\n                # positions\n                node = self.asttypes.EmptyStatement(';')\n                node.setpos(p, key - 1)\n                node.lexpos += 1\n                node.colno += 1\n            else:\n                node = self.asttypes.ExprStatement(expr=node)\n                node.setpos(p, key)\n            return node\n\n        if len(p) == 10:\n            p[0] = self.asttypes.For(\n                init=wrap(p[3], 3), cond=wrap(p[5], 5),\n                count=p[7], statement=p[9])\n        else:\n            init = self.asttypes.VarStatement(p[4])\n            init.setpos(p, 3)\n            p[0] = self.asttypes.For(\n                init=init, cond=wrap(p[6], 6), count=p[8], statement=p[10])\n        p[0].setpos(p)", "response": "P iteration_statement | iteration_statement | for | for | |"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_iteration_statement_5(self, p):\n        vardecl = self.asttypes.VarDeclNoIn(identifier=p[4])\n        vardecl.setpos(p, 3)\n        p[0] = self.asttypes.ForIn(item=vardecl, iterable=p[6], statement=p[8])\n        p[0].setpos(p)", "response": "\\ n iteration_statement : \\ n For In"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_continue_statement_1(self, p):\n        p[0] = self.asttypes.Continue()\n        p[0].setpos(p)", "response": "P_continue_statement is a continue statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_break_statement_1(self, p):\n        p[0] = self.asttypes.Break()\n        p[0].setpos(p)", "response": "Parse a BREAK statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_return_statement_1(self, p):\n        p[0] = self.asttypes.Return()\n        p[0].setpos(p)", "response": "handle RETURN SEMI\n                            | RETURN AUTOSEMI\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling RETURN expr SEMI | RETURN expr AUTOSEMI", "response": "def p_return_statement_2(self, p):\n        \"\"\"return_statement : RETURN expr SEMI\n                            | RETURN expr AUTOSEMI\n        \"\"\"\n        p[0] = self.asttypes.Return(expr=p[2])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses WITH expr RPAREN statement", "response": "def p_with_statement(self, p):\n        \"\"\"with_statement : WITH LPAREN expr RPAREN statement\"\"\"\n        p[0] = self.asttypes.With(expr=p[3], statement=p[5])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_case_block(self, p):\n        statements = []\n        for s in p[2:-1]:\n            if isinstance(s, list):\n                for i in s:\n                    statements.append(i)\n            elif isinstance(s, self.asttypes.Default):\n                statements.append(s)\n        p[0] = self.asttypes.CaseBlock(statements)\n        p[0].setpos(p)", "response": "A case block is a list of statements that are used to generate the log entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_case_clauses(self, p):\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[1].append(p[2])\n            p[0] = p[1]", "response": "case_clauses : case_clause\n                        | case_clauses case_clause"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncase expr COLON source_elements", "response": "def p_case_clause(self, p):\n        \"\"\"case_clause : CASE expr COLON source_elements\"\"\"\n        p[0] = self.asttypes.Case(expr=p[2], elements=p[4])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_default_clause(self, p):\n        p[0] = self.asttypes.Default(elements=p[3])\n        p[0].setpos(p)", "response": "P_DEFAULT_CLAUSE | DEFAULT COLON | source_elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_labelled_statement(self, p):\n        p[0] = self.asttypes.Label(identifier=p[1], statement=p[3])\n        p[0].setpos(p, 2)", "response": "A labelled statement is a colon - separated list of identifiers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a THROW statement.", "response": "def p_throw_statement(self, p):\n        \"\"\"throw_statement : THROW expr SEMI\n                           | THROW expr AUTOSEMI\n        \"\"\"\n        p[0] = self.asttypes.Throw(expr=p[2])\n        p[0].setpos(p)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_try_statement_2(self, p):\n        p[0] = self.asttypes.Try(statements=p[2], fin=p[3])\n        p[0].setpos(p)", "response": "A try statement is a block of code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_catch(self, p):\n        p[0] = self.asttypes.Catch(identifier=p[3], elements=p[5])\n        p[0].setpos(p)", "response": "Catch identifier RPAREN block"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_function_declaration(self, p):\n        if len(p) == 8:\n            p[0] = self.asttypes.FuncDecl(\n                identifier=p[2], parameters=None, elements=p[6])\n        else:\n            p[0] = self.asttypes.FuncDecl(\n                identifier=p[2], parameters=p[4], elements=p[7])\n        p[0].setpos(p)", "response": "P - function declaration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_function_expr_2(self, p):\n        if len(p) == 8:\n            p[0] = self.asttypes.FuncExpr(\n                identifier=p[2], parameters=None, elements=p[6])\n        else:\n            p[0] = self.asttypes.FuncExpr(\n                identifier=p[2], parameters=p[4], elements=p[7])\n        p[0].setpos(p)", "response": "P 2 ) Isolate Local Class Information"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset fixed effect designs", "response": "def setDesigns(self, F, A):\n        \"\"\" set fixed effect designs \"\"\"\n        F = to_list(F)\n        A = to_list(A)\n        assert len(A) == len(F), 'MeanKronSum: A and F must have same length!'\n        n_terms = len(F)\n        n_covs = 0\n        k = 0\n        l = 0\n        for ti in range(n_terms):\n            assert F[ti].shape[0] == self._N, 'MeanKronSum: Dimension mismatch'\n            assert A[ti].shape[1] == self._P, 'MeanKronSum: Dimension mismatch'\n            n_covs += F[ti].shape[1] * A[ti].shape[0]\n            k += F[ti].shape[1]\n            l += A[ti].shape[0]\n        self._n_terms = n_terms\n        self._n_covs = n_covs\n        self._k = k\n        self._l = l\n        self._F = F\n        self._A = A\n        self._b = sp.zeros((n_covs, 1))\n        self.clear_cache('predict_in_sample', 'Yres', 'designs')\n        self._notify('designs')\n        self._notify()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Fstar(self, value):\n        if value is None:\n            self._use_to_predict = False\n        else:\n            assert value.shape[1] == self._K, 'Dimension mismatch'\n            self._use_to_predict = True\n        self._Fstar = value\n        self.clear_cache('predict')", "response": "set fixed effect design for predictions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toRanks(A):\n    AA=sp.zeros_like(A)\n    for i in range(A.shape[1]):\n        AA[:,i] = st.rankdata(A[:,i])\n    AA=sp.array(sp.around(AA),dtype=\"int\")-1\n    return AA", "response": "converts the columns of A to ranks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregress out Y from X from Y", "response": "def regressOut(Y, X, return_b=False):\n    \"\"\"\n    regresses out X from Y\n    \"\"\"\n    Xd = la.pinv(X)\n    b = Xd.dot(Y)\n    Y_out = Y-X.dot(b)\n    if return_b:\n        return Y_out, b\n    else:\n        return Y_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_dependent_cols(M, tol=1e-6, display=False):\n    R = la.qr(M, mode='r')[0][:M.shape[1], :]\n    I = (abs(R.diagonal())>tol)\n    if sp.any(~I) and display:\n        print(('cols ' + str(sp.where(~I)[0]) +\n                ' have been removed because linearly dependent on the others'))\n        R = M[:,I]\n    else:\n        R = M.copy()\n    return R", "response": "Returns a matrix where dependent columsn have been removed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef boxcox(X):\n    X_transformed = sp.zeros_like(X)\n    maxlog = sp.zeros(X.shape[1])\n    for i in range(X.shape[1]):\n        i_nan = sp.isnan(X[:,i])\n        values = X[~i_nan,i]\n        X_transformed[i_nan,i] = X[i_nan,i]\n        X_transformed[~i_nan,i], maxlog[i] = st.boxcox(values-values.min()+1.0)\n    return X_transformed, maxlog", "response": "Gaussianize X using the Box - Cox transformation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setCovariance(self,cov):\n        chol = LA.cholesky(cov,lower=True)\n        params = chol[sp.tril_indices(self.dim)]\n        self.setParams(params)", "response": "set hyperparameters from given covariance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the bias for a single resource in the base system.", "response": "def bias(mass, z, h=h, Om_M=Om_M, Om_L=Om_L):\n    \"\"\"Calculate halo bias, from Seljak & Warren 2004.\n\n    Parameters\n    ----------\n    mass : ndarray or float\n        Halo mass to calculate bias for.\n    z : ndarray or float\n        Halo z, same type and size as mass.\n    h : float, optional\n        Hubble parameter, defaults to astropy.cosmology.Planck13.h\n    Om_M : float, optional\n        Fractional matter density, defaults to\n        astropy.cosmology.Planck13.Om0\n    Om_L : float, optional\n        Fractional dark energy density, defaults to 1-Om_M.\n\n    Returns\n    ----------\n    ndarray or float\n        Returns the halo bias, of same type and size as input mass_halo and\n        z_halo. Calculated according to Seljak & Warren 2004 for z = 0.\n        For halo z > 0, the non-linear mass is adjusted using the input\n        cosmological parameters.\n\n    References\n    ----------\n    Based on fitting formula derived from simulations in:\n\n    U. Seljak and M.S. Warren, \"Large-scale bias and stochasticity of\n    haloes and dark matter,\" Monthly Notices of the Royal Astronomical\n    Society, Volume 355, Issue 1, pp. 129-136 (2004).\n    \"\"\"\n    M_nl_0 = (8.73 / h) * (10. ** 12.)         # nonlinear mass today [M_sun]\n    M_nl = M_nl_0 * (Om_M + Om_L / ((1. + z) ** 3.))  # scaled to z_lens\n    x = mass / M_nl\n    b = 0.53 + 0.39 * (x ** 0.45) + 0.13 / (40. * x +\n                                            1.) + (5.e-4) * (x ** 1.5)\n    return b"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshortens text to MAX_DISPLAY_LEN characters.", "response": "def shorten(text):\n    \"\"\" Reduce text length for displaying / logging purposes.\n    \"\"\"\n    if len(text) >= MAX_DISPLAY_LEN:\n        text = text[:MAX_DISPLAY_LEN//2]+\"...\"+text[-MAX_DISPLAY_LEN//2:]\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_logfile(logger=None):\n    logger = logger or logging.getLogger()\n    handlers = [i for i in logger.handlers if isinstance(i, logging.FileHandler)]\n    return handlers[0].baseFilename if handlers else None", "response": "Return the log file of the first file handler associated with the root logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef paid_at(self):\n        '''Localized at America/Santiago datetime of ``TBK_FECHA_TRANSACCION``.\n        '''\n        fecha_transaccion = self.data['TBK_FECHA_TRANSACCION']\n        hora_transaccion = self.data['TBK_HORA_TRANSACCION']\n        m = int(fecha_transaccion[:2])\n        d = int(fecha_transaccion[2:])\n        h = int(hora_transaccion[:2])\n        i = int(hora_transaccion[2:4])\n        s = int(hora_transaccion[4:])\n\n        santiago = pytz.timezone('America/Santiago')\n        today = santiago.localize(datetime.datetime.today())\n        santiago_dt = santiago.localize(\n            datetime.datetime(today.year, m, d, h, i, s))\n\n        return santiago_dt", "response": "Localized at America / Santiago datetime of TBK_FECHA_TRANSACCION."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the response code is success.", "response": "def is_success(self, check_timeout=True):\n        '''\n        Check if Webpay response ``TBK_RESPUESTA`` is equal to ``0`` and if the lapse between initialization\n        and this call is less than ``self.timeout`` when ``check_timeout`` is ``True`` (default).\n\n        :param check_timeout: When ``True``, check time between initialization and call.\n        '''\n        if check_timeout and self.is_timeout():\n            return False\n        return self.payload.response == self.payload.SUCCESS_RESPONSE_CODE"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_timeout(self):\n        '''\n        Check if the lapse between initialization and now is more than ``self.timeout``.\n        '''\n        lapse = datetime.datetime.now() - self.init_time\n        return lapse > datetime.timedelta(seconds=self.timeout)", "response": "Check if the lapse between initialization and now is more than self. timeout."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef improvise_step(oracle, i, lrs=0, weight=None, prune=False):\n\n    if prune:\n        prune_list = range(i % prune, oracle.n_states - 1, prune)\n        trn_link = [s + 1 for s in oracle.latent[oracle.data[i]] if\n                    (oracle.lrs[s] >= lrs and\n                     (s + 1) < oracle.n_states) and\n                    s in prune_list]\n    else:\n        trn_link = [s + 1 for s in oracle.latent[oracle.data[i]] if\n                    (oracle.lrs[s] >= lrs and (s + 1) < oracle.n_states)]\n    if not trn_link:\n        if i == oracle.n_states - 1:\n            n = 1\n        else:\n            n = i + 1\n    else:\n        if weight == 'lrs':\n            lrs_link = [oracle.lrs[s] for s in\n                        oracle.latent[oracle.data[i]] if\n                        (oracle.lrs[s] >= lrs and (s + 1) < oracle.n_states)]\n            lrs_pop = list(itertools.chain.from_iterable(itertools.chain.from_iterable(\n                [[[i] * _x for (i, _x) in zip(trn_link, lrs_link)]])))\n            n = np.random.choice(lrs_pop)\n        else:\n            n = trn_link[int(np.floor(random.random() * len(trn_link)))]\n    return n", "response": "Given the current time step and an oracle and the current time step length lrs and weight generate the next time step based on the oracle structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef improvise(oracle, seq_len, k=1, LRS=0, weight=None, continuity=1):\n\n    s = []\n    if k + continuity < oracle.n_states - 1:\n        s.extend(range(k, k + continuity))\n        k = s[-1]\n        seq_len -= continuity\n\n    while seq_len > 0:\n        s.append(improvise_step(oracle, k, LRS, weight))\n        k = s[-1]\n        if k + 1 < oracle.n_states - 1:\n            k += 1\n        else:\n            k = 1\n        if k + continuity < oracle.n_states - 1:\n            s.extend(range(k, k + continuity))\n            seq_len -= continuity\n        k = s[-1]\n        seq_len -= 1\n\n    return s", "response": "Given an oracle and length generate an improvised sequence of the given length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate(oracle, seq_len, p=0.5, k=1, LRS=0, weight=None):\n\n    trn = oracle.trn[:]\n    sfx = oracle.sfx[:]\n    lrs = oracle.lrs[:]\n    rsfx = oracle.rsfx[:]\n\n    s = []\n    ktrace = [1]\n\n    for _i in range(seq_len):\n        # generate each state\n        if sfx[k] != 0 and sfx[k] is not None:\n            if (random.random() < p):\n                # copy forward according to transitions\n                I = trn[k]\n                if len(I) == 0:\n                    # if last state, choose a suffix\n                    k = sfx[k]\n                    ktrace.append(k)\n                    I = trn[k]\n                sym = I[int(np.floor(random.random() * len(I)))]\n                s.append(sym)  # Why (sym-1) before?\n                k = sym\n                ktrace.append(k)\n            else:\n                # copy any of the next symbols\n                ktrace.append(k)\n                _k = k\n                k_vec = []\n                k_vec = _find_links(k_vec, sfx, rsfx, _k)\n                k_vec = [_i for _i in k_vec if lrs[_i] >= LRS]\n                lrs_vec = [lrs[_i] for _i in k_vec]\n                if len(k_vec) > 0:  # if a possibility found, len(I)\n                    if weight == 'weight':\n                        max_lrs = np.amax(lrs_vec)\n                        query_lrs = max_lrs - np.floor(random.expovariate(1))\n\n                        if query_lrs in lrs_vec:\n                            _tmp = np.where(lrs_vec == query_lrs)[0]\n                            _tmp = _tmp[int(\n                                np.floor(random.random() * len(_tmp)))]\n                            sym = k_vec[_tmp]\n                        else:\n                            _tmp = np.argmin(abs(\n                                np.subtract(lrs_vec, query_lrs)))\n                            sym = k_vec[_tmp]\n                    elif weight == 'max':\n                        sym = k_vec[np.argmax([lrs[_i] for _i in k_vec])]\n                    else:\n                        sym = k_vec[int(np.floor(random.random() * len(k_vec)))]\n\n                    if sym == len(sfx) - 1:\n                        sym = sfx[sym] + 1\n                    else:\n                        s.append(sym + 1)\n                    k = sym + 1\n                    ktrace.append(k)\n                else:  # otherwise continue\n                    if k < len(sfx) - 1:\n                        sym = k + 1\n                    else:\n                        sym = sfx[k] + 1\n                    s.append(sym)\n                    k = sym\n                    ktrace.append(k)\n        else:\n            if k < len(sfx) - 1:\n                s.append(k + 1)\n                k += 1\n                ktrace.append(k)\n            else:\n                sym = sfx[k] + 1\n                s.append(sym)\n                k = sym\n                ktrace.append(k)\n        if k >= len(sfx) - 1:\n            k = 0\n    kend = k\n    return s, kend, ktrace", "response": "Generate a sequence based on traversing an oracle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_links(k_vec, sfx, rsfx, k):\n    k_vec.sort()\n    if 0 in k_vec:\n        return k_vec\n    else:\n        if sfx[k] not in k_vec:\n            k_vec.append(sfx[k])\n        for i in range(len(rsfx[k])):\n            if rsfx[k][i] not in k_vec:\n                k_vec.append(rsfx[k][i])\n        for i in range(len(k_vec)):\n            k_vec = _find_links(k_vec, sfx, rsfx, k_vec[i])\n            if 0 in k_vec:\n                break\n        return k_vec", "response": "Find sfx and rsfx recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_win(n, mono=False):\n\n    if mono:\n        win = np.hanning(n) + 0.00001\n    else:\n        win = np.array([np.hanning(n) + 0.00001, np.hanning(n) + 0.00001])\n    win = np.transpose(win)\n    return win", "response": "Generate a window for a given length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the single instance of the given class.", "response": "def singleton(wrapped, _, args, kwargs):\n    \"\"\"Return the single instance of wrapped.\n    :param wrapped: the wrapped class.\n    :param _: unused\n    :param args: optional arguments for wrapped object.\n    :param kwargs: optional arguments for wrapped object.\n    \"\"\"\n    if wrapped not in _INSTANCES:\n        _INSTANCES[wrapped] = wrapped(*args, **kwargs)\n    return _INSTANCES[wrapped]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, node, condition):\n\n        if not isinstance(node, Node):\n            raise TypeError('not a node')\n\n        for child in node:\n            if condition(child):\n                yield child\n            for subchild in self.filter(child, condition):\n                yield subchild", "response": "This method returns a generator that yields the nodes that match the condition function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting a single node that matches the condition.", "response": "def extract(self, node, condition, skip=0):\n        \"\"\"\n        Extract a single node that matches the provided condition,\n        otherwise a TypeError is raised.  An optional skip parameter can\n        be provided to specify how many matching nodes are to be skipped\n        over.\n        \"\"\"\n\n        for child in self.filter(node, condition):\n            if not skip:\n                return child\n            skip -= 1\n        raise TypeError('no match found')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(\n            self, node, omit=(\n                'lexpos', 'lineno', 'colno', 'rowno'),\n            indent=0, depth=-1,\n            pos=False,\n            _level=0):\n        \"\"\"\n        Accepts the standard node argument, along with an optional omit\n        flag - it should be an iterable that lists out all attributes\n        that should be omitted from the repr output.\n        \"\"\"\n\n        if not depth:\n            return '<%s ...>' % node.__class__.__name__\n\n        attrs = []\n        children = node.children()\n        ids = {id(child) for child in children}\n\n        indentation = ' ' * (indent * (_level + 1))\n        header = '\\n' + indentation if indent else ''\n        joiner = ',\\n' + indentation if indent else ', '\n        tailer = '\\n' + ' ' * (indent * _level) if indent else ''\n\n        for k, v in vars(node).items():\n            if k.startswith('_'):\n                continue\n            if id(v) in ids:\n                ids.remove(id(v))\n\n            if isinstance(v, Node):\n                attrs.append((k, self.walk(\n                    v, omit, indent, depth - 1, pos, _level)))\n            elif isinstance(v, list):\n                items = []\n                for i in v:\n                    if id(i) in ids:\n                        ids.remove(id(i))\n                    items.append(self.walk(\n                        i, omit, indent, depth - 1, pos, _level + 1))\n                attrs.append(\n                    (k, '[' + header + joiner.join(items) + tailer + ']'))\n            else:\n                attrs.append((k, repr_compat(v)))\n\n        if ids:\n            # for unnamed child nodes.\n            attrs.append(('?children', '[' + header + joiner.join(\n                self.walk(child, omit, indent, depth - 1, pos, _level + 1)\n                for child in children\n                if id(child) in ids) + tailer + ']'))\n\n        position = ('@%s:%s ' % (\n            '?' if node.lineno is None else node.lineno,\n            '?' if node.colno is None else node.colno,\n        ) if pos else '')\n\n        omit_keys = () if not omit else set(omit)\n        return '<%s %s%s>' % (node.__class__.__name__, position, ', '.join(\n            '%s=%s' % (k, v) for k, v in sorted(attrs)\n            if k not in omit_keys\n        ))", "response": "Walks the tree of the node and returns a string representation of the tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndirect graph construction alg. from Johns & Mahadevan ICML - 07.", "response": "def directed_graph(trajectories, k=5, verbose=False, pruning_thresh=0,\n                   return_coords=False):\n  '''Directed graph construction alg. from Johns & Mahadevan, ICML '07.\n  trajectories: list of NxD arrays of ordered states\n  '''\n  X = np.vstack(trajectories)\n  G = neighbor_graph(X, k=k)\n  if pruning_thresh > 0:\n    traj_len = map(len, trajectories)\n    G = _prune_edges(G, X, traj_len, pruning_thresh, verbose=verbose)\n  if return_coords:\n    return G, X\n  return G"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprune edges in graph G via cosine distance with trajectory edges.", "response": "def _prune_edges(G, X, traj_lengths, pruning_thresh=0.1, verbose=False):\n  '''Prune edges in graph G via cosine distance with trajectory edges.'''\n  W = G.matrix('dense', copy=True)\n  degree = G.degree(kind='out', weighted=False)\n  i = 0\n  num_bad = 0\n  for n in traj_lengths:\n    s, t = np.nonzero(W[i:i+n-1])\n    graph_edges = X[t] - X[s+i]\n    traj_edges = np.diff(X[i:i+n], axis=0)\n    traj_edges = np.repeat(traj_edges, degree[i:i+n-1], axis=0)\n    theta = paired_distances(graph_edges, traj_edges, 'cosine')\n    bad_edges = theta > pruning_thresh\n    s, t = s[bad_edges], t[bad_edges]\n    if verbose:  # pragma: no cover\n      num_bad += np.count_nonzero(W[s,t])\n    W[s,t] = 0\n    i += n\n  if verbose:  # pragma: no cover\n    print('removed %d bad edges' % num_bad)\n  return Graph.from_adj_matrix(W)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the ratio contour and the ratio outline.", "response": "def make_plot(self):\n        \"\"\"Creates the ratio plot.\n\n        \"\"\"\n        # sets colormap for ratio comparison plot\n        cmap = getattr(cm, self.colormap)\n\n        # set values of ratio comparison contour\n        normval = 2.0\n        num_contours = 40  # must be even\n        levels = np.linspace(-normval, normval, num_contours)\n        norm = colors.Normalize(-normval, normval)\n\n        # find Loss/Gain contour and Ratio contour\n        self.set_comparison()\n        diff_out, loss_gain_contour = self.find_difference_contour()\n\n        cmap.set_bad(color='white', alpha=0.001)\n        # plot ratio contours\n\n        sc = self.axis.contourf(self.xvals[0], self.yvals[0], diff_out,\n                                levels=levels, norm=norm,\n                                extend='both', cmap=cmap)\n\n        self.colorbar.setup_colorbars(sc)\n\n        # toggle line contours of orders of magnitude of ratio comparisons\n        if self.order_contour_lines:\n                self.axis.contour(self.xvals[0], self.yvals[0], diff_out, np.array(\n                    [-2.0, -1.0, 1.0, 2.0]), colors='black', linewidths=1.0)\n\n        # plot loss gain contour\n        if self.loss_gain_status is True:\n            # if there is no loss/gain contours, this will produce an error,\n            # so we catch the exception.\n            try:\n                # make hatching\n                cs = self.axis.contourf(self.xvals[0], self.yvals[0],\n                                        loss_gain_contour, levels=[-2, -0.5, 0.5, 2], colors='none',\n                                        hatches=['x', None, '+'])\n                # make loss/gain contour outline\n                self.axis.contour(self.xvals[0], self.yvals[0],\n                                  loss_gain_contour, 3, colors='black', linewidths=2)\n\n            except ValueError:\n                pass\n\n        if self.add_legend:\n            loss_patch = Patch(fill=None, label='Loss', hatch='x', linestyle='--', linewidth=2)\n            gain_patch = Patch(fill=None, label='Gain', hatch='+', linestyle='-', linewidth=2)\n            legend = self.axis.legend(handles=[loss_patch, gain_patch], **self.legend_kwargs)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_comparison(self):\n        self.comp1 = self.zvals[0]\n        self.comp2 = self.zvals[1]\n        return", "response": "Defines the comparison values for the ratio."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the ratio and loss/gain contours. This method finds the ratio contour and the Loss/Gain contour values. Its inputs are the two datasets for comparison where the second is the control to compare against the first. The input data sets need to be the same shape. Returns: 2-element tuple containing - **diff** (*2D array of floats*): Ratio contour values. - **loss_gain_contour** (*2D array of floats*): loss/gain contour values.", "response": "def find_difference_contour(self):\n        \"\"\"Find the ratio and loss/gain contours.\n\n        This method finds the ratio contour and the Loss/Gain contour values.\n        Its inputs are the two datasets for comparison where the second is the control\n        to compare against the first.\n\n        The input data sets need to be the same shape.\n\n        Returns:\n            2-element tuple containing\n                - **diff** (*2D array of floats*): Ratio contour values.\n                - **loss_gain_contour** (*2D array of floats*): loss/gain contour values.\n\n        \"\"\"\n        # set contour to test and control contour\n        self.ratio_comp_value = (self.comparison_value if self.ratio_comp_value is None\n                                 else self.ratio_comp_value)\n\n        # indices of loss,gained.\n        inds_gained = np.where((self.comp1 >= self.comparison_value)\n                               & (self.comp2 < self.comparison_value))\n        inds_lost = np.where((self.comp1 < self.comparison_value)\n                             & (self.comp2 >= self.comparison_value))\n\n        self.comp1 = np.ma.masked_where(self.comp1 < self.ratio_comp_value, self.comp1)\n        self.comp2 = np.ma.masked_where(self.comp2 < self.ratio_comp_value, self.comp2)\n\n        # set diff to ratio for purposed of determining raito differences\n        diff = self.comp1/self.comp2\n\n        # the following determines the log10 of the ratio difference.\n        # If it is extremely small, we neglect and put it as zero\n        # (limits chosen to resemble ratios of less than 1.05 and greater than 0.952)\n        diff = (np.log10(diff)*(diff >= 1.05)\n                + (-np.log10(1.0/diff)) * (diff <= 0.952)\n                + 0.0*((diff < 1.05) & (diff > 0.952)))\n\n        # initialize loss/gain\n        loss_gain_contour = np.zeros(np.shape(self.comp1))\n\n        # fill out loss/gain\n        loss_gain_contour[inds_lost] = -1\n        loss_gain_contour[inds_gained] = 1\n\n        return diff, loss_gain_contour"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking the horizon plot.", "response": "def make_plot(self):\n        \"\"\"Make the horizon plot.\n\n        \"\"\"\n\n        self.get_contour_values()\n        # sets levels of main contour plot\n        colors1 = ['blue', 'green', 'red', 'purple', 'orange',\n                   'gold', 'magenta']\n\n        # set contour value. Default is SNR_CUT.\n        self.snr_contour_value = (self.SNR_CUT if self.snr_contour_value is None\n                                  else self.snr_contour_value)\n\n        # plot contours\n        for j in range(len(self.zvals)):\n            hz = self.axis.contour(self.xvals[j], self.yvals[j],\n                                   self.zvals[j], np.array([self.snr_contour_value]),\n                                   colors=colors1[j], linewidths=1., linestyles='solid')\n\n            # plot invisible lines for purpose of creating a legend\n            if self.legend_labels != []:\n                # plot a curve off of the grid with same color for legend label.\n                self.axis.plot([0.1, 0.2], [0.1, 0.2], color=colors1[j],\n                               label=self.legend_labels[j])\n\n        if self.add_legend:\n            self.axis.legend(**self.legend_kwargs)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ave_laplacian(self):\n    '''Another kind of laplacian normalization, used in the matlab PVF code.\n    Uses the formula: L = I - D^{-1} * W'''\n    W = self.matrix('dense')\n    # calculate -inv(D)\n    Dinv = W.sum(axis=0)\n    mask = Dinv!=0\n    Dinv[mask] = -1./Dinv[mask]\n    # calculate -inv(D) * W\n    lap = (Dinv * W.T).T\n    # add I\n    lap.flat[::W.shape[0]+1] += 1\n    # symmetrize\n    return (lap + lap.T) / 2.0", "response": "Another kind of laplacian normalization used in the matlab PVF code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef directed_laplacian(self, D=None, eta=0.99, tol=1e-12, max_iter=500):\n    '''Computes the directed combinatorial graph laplacian.\n    http://www-all.cs.umass.edu/pubs/2007/johns_m_ICML07.pdf\n\n    D: (optional) N-array of degrees\n    eta: probability of not teleporting (see the paper)\n    tol, max_iter: convergence params for Perron vector calculation\n    '''\n    W = self.matrix('dense')\n    n = W.shape[0]\n    if D is None:\n      D = W.sum(axis=1)\n    # compute probability transition matrix\n    with np.errstate(invalid='ignore', divide='ignore'):\n      P = W.astype(float) / D[:,None]\n    P[D==0] = 0\n    # start at the uniform distribution Perron vector (phi)\n    old_phi = np.ones(n) / n\n    # iterate to the fixed point (teleporting random walk)\n    for _ in range(max_iter):\n      phi = eta * old_phi.dot(P) + (1-eta)/n\n      if np.abs(phi - old_phi).max() < tol:\n        break\n      old_phi = phi\n    else:\n      warnings.warn(\"phi failed to converge after %d iterations\" % max_iter)\n    # L = Phi - (Phi P + P' Phi)/2\n    return np.diag(phi) - ((phi * P.T).T + P.T * phi)/2", "response": "Computes the directed combinatorial graph laplacian."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bandwidth(self):\n    return np.abs(np.diff(self.pairs(), axis=1)).max()", "response": "Computes the bandwidth of a graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef profile(self):\n    leftmost_idx = np.argmax(self.matrix('dense').astype(bool), axis=0)\n    return (np.arange(self.num_vertices()) - leftmost_idx).sum()", "response": "Measure of bandedness also known as envelope size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef betweenness(self, kind='vertex', directed=None, weighted=None):\n    '''Computes the betweenness centrality of a graph.\n    kind : string, either 'vertex' (default) or 'edge'\n    directed : bool, defaults to self.is_directed()\n    weighted : bool, defaults to self.is_weighted()\n    '''\n    assert kind in ('vertex', 'edge'), 'Invalid kind argument: ' + kind\n    weighted = weighted is not False and self.is_weighted()\n    directed = directed if directed is not None else self.is_directed()\n    adj = self.matrix('csr')\n    btw = betweenness(adj, weighted, kind=='vertex')\n    # normalize if undirected\n    if not directed:\n      btw /= 2.\n    return btw", "response": "Computes the betweenness centrality of a graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a greedy vertex coloring as an array of ints.", "response": "def color_greedy(self):\n    '''Returns a greedy vertex coloring as an array of ints.'''\n    n = self.num_vertices()\n    coloring = np.zeros(n, dtype=int)\n    for i, nbrs in enumerate(self.adj_list()):\n      nbr_colors = set(coloring[nbrs])\n      for c in count(1):\n        if c not in nbr_colors:\n          coloring[i] = c\n          break\n    return coloring"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an approximate 2 - coloring as an array of booleans.", "response": "def bicolor_spectral(self):\n    '''Returns an approximate 2-coloring as an array of booleans.\n\n    From \"A Multiscale Pyramid Transform for Graph Signals\" by Shuman et al.\n    Note: Assumes a single connected component, and may fail otherwise.\n    '''\n    lap = self.laplacian().astype(float)\n    vals, vecs = eigs(lap, k=1, which='LM')\n    vec = vecs[:,0].real\n    return vec > 0 if vec[0] > 0 else vec < 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef regression(self, y, y_mask, smoothness_penalty=0, kernel='rbf'):\n    '''Perform vertex-valued regression, given partial labels.\n    y : (n,d) array of known labels\n    y_mask : index object such that all_labels[y_mask] == y\n\n    From \"Regularization and Semi-supervised Learning on Large Graphs\"\n      by Belkin, Matveeva, and Niyogi in 2004.\n    Doesn't support multiple labels per vertex, unlike the paper's algorithm.\n    To allow provided y values to change, use a (small) smoothness_penalty.\n    '''\n    n = self.num_vertices()\n\n    # input validation for y\n    y = np.array(y, copy=True)\n    ravel_f = False\n    if y.ndim == 1:\n      y = y[:,None]\n      ravel_f = True\n    if y.ndim != 2 or y.size == 0:\n      raise ValueError('Invalid shape of y array: %s' % (y.shape,))\n    k, d = y.shape\n\n    # input validation for y_mask\n    if not hasattr(y_mask, 'dtype') or y_mask.dtype != 'bool':\n      tmp = np.zeros(n, dtype=bool)\n      tmp[y_mask] = True\n      y_mask = tmp\n\n    # mean-center known y for stability\n    y_mean = y.mean(axis=0)\n    y -= y_mean\n\n    # use the normalized Laplacian for the smoothness matrix\n    S = self.kernelize(kernel).laplacian(normed=True)\n    if ss.issparse(S):\n      S = S.tocsr()\n\n    if smoothness_penalty == 0:\n      # see Algorithm 2: Interpolated Regularization\n      unlabeled_mask = ~y_mask\n      S_23 = S[unlabeled_mask, :]\n      S_3 = S_23[:, unlabeled_mask]\n      rhs = S_23[:, y_mask].dot(y)\n      if ss.issparse(S):\n        f_unlabeled = ss.linalg.spsolve(S_3, rhs)\n        if f_unlabeled.ndim == 1:\n          f_unlabeled = f_unlabeled[:,None]\n      else:\n        f_unlabeled = sl.solve(S_3, rhs, sym_pos=True, overwrite_a=True,\n                               overwrite_b=True)\n      f = np.zeros((n, d))\n      f[y_mask] = y\n      f[unlabeled_mask] = -f_unlabeled\n    else:\n      # see Algorithm 1: Tikhonov Regularization in the paper\n      y_hat = np.zeros((n, d))\n      y_hat[y_mask] = y\n      I = y_mask.astype(float)  # only one label per vertex\n      lhs = k * smoothness_penalty * S\n      if ss.issparse(lhs):\n        lhs.setdiag(lhs.diagonal() + I)\n        f = ss.linalg.lsqr(lhs, y_hat)[0]\n      else:\n        lhs.flat[::n+1] += I\n        f = sl.solve(lhs, y_hat, sym_pos=True, overwrite_a=True,\n                     overwrite_b=True)\n\n    # re-add the mean\n    f += y_mean\n    if ravel_f:\n      return f.ravel()\n    return f", "response": "Perform vertex - valued regression on large graphs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if value is acceptable value for param. Returns None if value is acceptable.", "response": "def _checkParam(param, value, paramlimits, paramtypes):\n    \"\"\"Checks if `value` is allowable value for `param`.\n\n    Raises except if `value` is not acceptable, otherwise\n    returns `None` if value is acceptable.\n\n    `paramlimits` and `paramtypes` are the `PARAMLIMITS`\n    and `PARAMTYPES` attributes of a `Model`.\n    \"\"\"\n    assert param in paramlimits, \"Invalid param: {0}\".format(param)\n    (lowlim, highlim) = paramlimits[param]\n    paramtype = paramtypes[param]\n    if isinstance(paramtype, tuple):\n        (paramtype, paramshape) = paramtype\n        if not (isinstance(value, paramtype)):\n            raise ValueError(\"{0} must be {1}, not {2}\".format(\n                    param, paramtype, type(param)))\n        if value.shape != paramshape:\n            raise ValueError(\"{0} must have shape {1}, not {2}\".format(\n                    param, paramshape, value.shape))\n        if value.dtype != 'float':\n            raise ValueError(\"{0} must have dtype float, not {1}\".format(\n                    param, value.dtype))\n        if not ((lowlim <= value).all() and (value <= highlim).all()):\n            raise ValueError(\"{0} must be >= {1} and <= {2}, not {3}\".format(\n                    param, lowlim, highlim, value))\n    else:\n        if not isinstance(value, paramtype):\n            raise ValueError(\"{0} must be a {1}, not a {2}\".format(\n                    param, paramtype, type(value)))\n        if not (lowlim <= value <= highlim):\n            raise ValueError(\"{0} must be >= {1} and <= {2}, not {3}\".format(\n                    param, lowlim, highlim, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfills diagonals of nsites matrices in m so rows sum to 0.", "response": "def _fill_diagonals(m, diag_indices):\n    \"\"\"Fills diagonals of `nsites` matrices in `m` so rows sum to 0.\"\"\"\n    assert m.ndim == 3, \"M must have 3 dimensions\"\n    assert m.shape[1] == m.shape[2], \"M must contain square matrices\"\n    for r in range(m.shape[0]):\n        scipy.fill_diagonal(m[r], 0)\n        m[r][diag_indices] -= scipy.sum(m[r], axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DiscreteGamma(alpha, beta, ncats):\n    alpha = float(alpha)\n    beta = float(beta)\n    assert alpha > 0\n    assert beta > 0\n    assert ncats > 1\n    scale = 1.0 / beta\n    catmeans = scipy.ndarray(ncats, dtype='float')\n    for k in range(ncats):\n        if k == 0:\n            lower = 0.0\n            gammainc_lower = 0.0\n        else:\n            lower = upper\n            gammainc_lower = gammainc_upper\n        if k == ncats - 1:\n            upper = float('inf')\n            gammainc_upper = 1.0\n        else:\n            upper = scipy.stats.gamma.ppf((k + 1) / float(ncats), alpha,\n                    scale=scale)\n            gammainc_upper = scipy.special.gammainc(alpha + 1, upper * beta)\n        catmeans[k] = alpha * ncats * (gammainc_upper - gammainc_lower) / beta\n    assert scipy.allclose(catmeans.sum() / ncats, alpha / beta), (\n            \"catmeans is {0}, mean of catmeans is {1}, expected mean \"\n            \"alpha / beta = {2} / {3} = {4}\").format(catmeans,\n            catmeans.sum() / ncats, alpha, beta, alpha / beta)\n    return catmeans", "response": "Returns category means for discretized gamma distribution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting new PARAMLIMITS dictionary.", "response": "def PARAMLIMITS(self, value):\n        \"\"\"Set new `PARAMLIMITS` dictionary.\"\"\"\n        assert set(value.keys()) == set(self.PARAMLIMITS.keys()), \"The \\\n                new parameter limits are not defined for the same set \\\n                of parameters as before.\"\n        for param in value.keys():\n            assert value[param][0] < value[param][1], \"The new \\\n                    minimum value for {0}, {1}, is equal to or \\\n                    larger than the new maximum value, {2}\"\\\n                    .format(param, value[param][0], value[param][1])\n        self._PARAMLIMITS = value.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of the parameters in the model.", "response": "def paramsReport(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        report = {}\n        for param in self._REPORTPARAMS:\n            pvalue = getattr(self, param)\n            if isinstance(pvalue, float):\n                report[param] = pvalue\n            elif isinstance(pvalue, scipy.ndarray) and pvalue.shape == (N_NT,):\n                for w in range(N_NT - 1):\n                    report['{0}{1}'.format(param, INDEX_TO_NT[w])] = pvalue[w]\n            elif isinstance(pvalue, scipy.ndarray) and (pvalue.shape ==\n                    (self.nsites, N_AA)):\n                for r in range(self.nsites):\n                    for a in range(N_AA):\n                        report['{0}{1}{2}'.format(param, r + 1, INDEX_TO_AA[a])\n                                ] = pvalue[r][a]\n            else:\n                raise ValueError(\"Unexpected param: {0}\".format(param))\n        return report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseeing docs for `Model` abstract base class.", "response": "def branchScale(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        bs = -(self.prx * scipy.diagonal(self.Prxy, axis1=1, axis2=2)\n                ).sum() * self.mu / float(self.nsites)\n        assert bs > 0\n        return bs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting new mu value.", "response": "def mu(self, value):\n        \"\"\"Set new `mu` value.\"\"\"\n        _checkParam('mu', value, self.PARAMLIMITS, self.PARAMTYPES)\n        if value != self.mu:\n            self._cached = {}\n        self._mu = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateParams(self, newvalues, update_all=False):\n        assert all(map(lambda x: x in self.freeparams, newvalues.keys())),\\\n                \"Invalid entry in newvalues: {0}\\nfreeparams: {1}\".format(\n                ', '.join(newvalues.keys()), ', '.join(self.freeparams))\n        changed = set([]) # contains string names of changed params\n        for (name, value) in newvalues.items():\n            _checkParam(name, value, self.PARAMLIMITS, self.PARAMTYPES)\n            if isinstance(value, scipy.ndarray):\n                if (value != getattr(self, name)).any():\n                    changed.add(name)\n                    setattr(self, name, value.copy())\n            else:\n                if value != getattr(self, name):\n                    changed.add(name)\n                    setattr(self, name, copy.copy(value))\n\n        if update_all or changed:\n            self._cached = {}\n\n        # The order of the updating below is important.\n        # If you change it, you may break either this class\n        # **or** classes that inherit from it.\n        # Note also that not all attributes need to be updated\n        # for all possible parameter changes, but just doing it\n        # this way is much simpler and adds negligible cost.\n        if update_all or (changed and changed != set(['mu'])):\n            self._update_pi_vars()\n            self._update_phi()\n            self._update_prx()\n            self._update_dprx()\n            self._update_Qxy()\n            self._update_Frxy()\n            self._update_Prxy()\n            self._update_Prxy_diag()\n            self._update_dPrxy()\n            self._update_B()", "response": "Update the parameters of the class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the matrix of the current state of the user - specified t in the current state of the system.", "response": "def M(self, t, tips=None, gaps=None):\n        \"\"\"See docs for method in `Model` abstract base class.\"\"\"\n        assert isinstance(t, float) and t > 0, \"Invalid t: {0}\".format(t)\n        with scipy.errstate(under='ignore'): # don't worry if some values 0\n            if ('expD', t) not in self._cached:\n                self._cached[('expD', t)] = scipy.exp(self.D * self.mu * t)\n            expD = self._cached[('expD', t)]\n            if tips is None:\n                # swap axes to broadcast multiply D as diagonal matrix\n                M = broadcastMatrixMultiply((self.A.swapaxes(0, 1) *\n                        expD).swapaxes(1, 0), self.Ainv)\n            else:\n                M = broadcastMatrixVectorMultiply((self.A.swapaxes(0, 1)\n                        * expD).swapaxes(1, 0), broadcastGetCols(\n                        self.Ainv, tips))\n                if gaps is not None:\n                    M[gaps] = scipy.ones(N_CODON, dtype='float')\n        #if M.min() < -0.01:\n        #    warnings.warn(\"Large negative value in M(t) being set to 0. \"\n        #            \"Value is {0}, t is {1}\".format(M.min(), t))\n        M[M < 0] = 0.0\n        return M"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsees docs for method in `Model` abstract base class.", "response": "def dM(self, t, param, Mt, tips=None, gaps=None):\n        \"\"\"See docs for method in `Model` abstract base class.\"\"\"\n        assert isinstance(t, float) and t > 0, \"Invalid t: {0}\".format(t)\n        assert (param == 't') or (param in self.freeparams), (\n                \"Invalid param: {0}\".format(param))\n\n        if Mt is None:\n            Mt = self.M(t, tips=tips, gaps=gaps)\n\n        if (param == 'mu') or (param == 't'):\n            if param == 'mu':\n                alpha = t\n            else:\n                alpha = self.mu\n            if tips is None:\n                dM_param = broadcastMatrixMultiply(self.Prxy, Mt, alpha=alpha)\n            else:\n                dM_param = broadcastMatrixVectorMultiply(self.Prxy, Mt, alpha=alpha)\n                if gaps is not None:\n                    dM_param[gaps] = scipy.zeros(N_CODON, dtype='float')\n            return dM_param\n\n        paramval = getattr(self, param)\n        if isinstance(paramval, float):\n            paramisvec = False\n        else:\n            assert isinstance(paramval, numpy.ndarray) and paramval.ndim == 1\n            paramisvec = True\n            paramlength = paramval.shape[0]\n\n        if ('expD', t) not in self._cached:\n            self._cached[('expD', t)] = scipy.exp(self.D * self.mu * t)\n        expD = self._cached[('expD', t)]\n\n        if ('V', t) not in self._cached:\n            if 'Dxx_Dyy' not in self._cached:\n                Dyy = scipy.tile(self.D, (1, N_CODON)).reshape(\n                        self.nsites, N_CODON, N_CODON)\n                Dxx = scipy.array([Dyy[r].transpose() for r in\n                        range(self.nsites)])\n                self._cached['Dxx_Dyy'] = Dxx - Dyy\n            Dxx_Dyy = self._cached['Dxx_Dyy']\n            if 'Dxx_Dyy_lt_ALMOST_ZERO' not in self._cached:\n                self._cached['Dxx_Dyy_lt_ALMOST_ZERO'] = scipy.fabs(\n                        Dxx_Dyy) < ALMOST_ZERO\n            Dxx_Dyy_lt_ALMOST_ZERO = self._cached['Dxx_Dyy_lt_ALMOST_ZERO']\n            with scipy.errstate(divide='raise', under='ignore',\n                    over='raise', invalid='ignore'):\n                expDyy = scipy.tile(expD,(1, N_CODON)).reshape(\n                        self.nsites, N_CODON, N_CODON)\n                expDxx = scipy.array([expDyy[r].transpose() for r in\n                        range(self.nsites)])\n                V = (expDxx - expDyy) / Dxx_Dyy\n            with scipy.errstate(under='ignore'): # OK if some values 0\n                scipy.copyto(V, self.mu * t * expDxx, where=\n                        Dxx_Dyy_lt_ALMOST_ZERO)\n            self._cached[('V', t)] = V\n        V = self._cached[('V', t)]\n\n        with scipy.errstate(under='ignore'): # don't worry if some values 0\n            if tips is None:\n                if not paramisvec:\n                    dM_param = broadcastMatrixMultiply(self.A,\n                            broadcastMatrixMultiply(self.B[param]\n                            * V, self.Ainv))\n                else:\n                    dM_param = scipy.ndarray((paramlength, self.nsites,\n                            N_CODON, N_CODON), dtype='float')\n                    for j in range(paramlength):\n                        dM_param[j] = broadcastMatrixMultiply(self.A,\n                                broadcastMatrixMultiply(self.B[param][j]\n                                * V, self.Ainv))\n            else:\n                if not paramisvec:\n                    dM_param = broadcastMatrixVectorMultiply(self.A,\n                            broadcastGetCols(broadcastMatrixMultiply(\n                            self.B[param] * V, self.Ainv), tips))\n                else:\n                    dM_param = scipy.ndarray((paramlength, self.nsites,\n                            N_CODON), dtype='float')\n                    for j in range(paramlength):\n                        dM_param[j] = broadcastMatrixVectorMultiply(self.A,\n                            broadcastGetCols(broadcastMatrixMultiply(\n                            self.B[param][j] * V, self.Ainv), tips))\n                if gaps is not None:\n                    if not paramisvec:\n                        dM_param[gaps] = scipy.zeros(N_CODON, dtype='float')\n                    else:\n                        dM_param[:, gaps] = scipy.zeros(N_CODON, dtype='float')\n        return dM_param"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eta_from_phi(self):\n        self.eta = scipy.ndarray(N_NT - 1, dtype='float')\n        etaprod = 1.0\n        for w in range(N_NT - 1):\n            self.eta[w] = 1.0 - self.phi[w] / etaprod\n            etaprod *= self.eta[w]\n        _checkParam('eta', self.eta, self.PARAMLIMITS, self.PARAMTYPES)", "response": "Update the eta attribute of the class instance based on the current phi."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating phi based on current eta.", "response": "def _update_phi(self):\n        \"\"\"Update `phi` using current `eta`.\"\"\"\n        etaprod = 1.0\n        for w in range(N_NT - 1):\n            self.phi[w] = etaprod * (1 - self.eta[w])\n            etaprod *= self.eta[w]\n        self.phi[N_NT - 1] = etaprod"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates Qxy using current kappa and phi.", "response": "def _update_Qxy(self):\n        \"\"\"Update `Qxy` using current `kappa` and `phi`.\"\"\"\n        for w in range(N_NT):\n            scipy.copyto(self.Qxy, self.phi[w], where=CODON_NT_MUT[w])\n        self.Qxy[CODON_TRANSITION] *= self.kappa"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_pi_vars(self):\n        with scipy.errstate(divide='raise', under='raise', over='raise',\n                invalid='raise'):\n            for r in range(self.nsites):\n                self.pi_codon[r] = self.pi[r][CODON_TO_AA]\n                pim = scipy.tile(self.pi_codon[r], (N_CODON, 1)) # [x][y] is piAy\n                self.piAx_piAy[r] = pim.transpose() / pim\n            self.ln_pi_codon = scipy.log(self.pi_codon)\n            self.piAx_piAy_beta = self.piAx_piAy**self.beta\n            self.ln_piAx_piAy_beta = scipy.log(self.piAx_piAy_beta)", "response": "Update variables that depend on pi."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the internal FRxy field.", "response": "def _update_Frxy(self):\n        \"\"\"Update `Frxy` from `piAx_piAy_beta`, `ln_piAx_piAy_beta`, `omega`, `beta`.\"\"\"\n        self.Frxy.fill(1.0)\n        self.Frxy_no_omega.fill(1.0)\n        with scipy.errstate(divide='raise', under='raise', over='raise',\n                        invalid='ignore'):\n            scipy.copyto(self.Frxy_no_omega, -self.ln_piAx_piAy_beta\n                    / (1 - self.piAx_piAy_beta), where=scipy.logical_and(\n                    CODON_NONSYN, scipy.fabs(1 - self.piAx_piAy_beta) >\n                    ALMOST_ZERO))\n        scipy.copyto(self.Frxy, self.Frxy_no_omega * self.omega, where=CODON_NONSYN)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_Prxy(self):\n        self.Prxy = self.Frxy * self.Qxy\n        _fill_diagonals(self.Prxy, self._diag_indices)", "response": "Update self. Prxy using current Frxy and Qxy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate D A Ainv from Prxy and prx.", "response": "def _update_Prxy_diag(self):\n        \"\"\"Update `D`, `A`, `Ainv` from `Prxy`, `prx`.\"\"\"\n        for r in range(self.nsites):\n            pr_half = self.prx[r]**0.5\n            pr_neghalf = self.prx[r]**-0.5\n            #symm_pr = scipy.dot(scipy.diag(pr_half), scipy.dot(self.Prxy[r], scipy.diag(pr_neghalf)))\n            symm_pr = (pr_half * (self.Prxy[r] * pr_neghalf).transpose()).transpose()\n            # assert scipy.allclose(symm_pr, symm_pr.transpose())\n            (evals, evecs) = scipy.linalg.eigh(symm_pr)\n            # assert scipy.allclose(scipy.linalg.inv(evecs), evecs.transpose())\n            # assert scipy.allclose(symm_pr, scipy.dot(evecs, scipy.dot(scipy.diag(evals), evecs.transpose())))\n            self.D[r] = evals\n            self.Ainv[r] = evecs.transpose() * pr_half\n            self.A[r] = (pr_neghalf * evecs.transpose()).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate prx from phi pi_codon and beta.", "response": "def _update_prx(self):\n        \"\"\"Update `prx` from `phi`, `pi_codon`, and `beta`.\"\"\"\n        qx = scipy.ones(N_CODON, dtype='float')\n        for j in range(3):\n            for w in range(N_NT):\n                qx[CODON_NT[j][w]] *= self.phi[w]\n        frx = self.pi_codon**self.beta\n        self.prx = frx * qx\n        with scipy.errstate(divide='raise', under='raise', over='raise',\n                invalid='raise'):\n            for r in range(self.nsites):\n                self.prx[r] /= self.prx[r].sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_dprx(self):\n        if 'beta' in self.freeparams:\n            for r in range(self.nsites):\n                self.dprx['beta'][r] = self.prx[r] * (self.ln_pi_codon[r]\n                        - scipy.dot(self.ln_pi_codon[r], self.prx[r]))\n        if 'eta' in self.freeparams:\n            boolterm = scipy.ndarray(N_CODON, dtype='float')\n            with scipy.errstate(divide='raise', under='raise', over='raise',\n                    invalid='raise'):\n                for i in range(N_NT - 1):\n                    boolterm.fill(0)\n                    for j in range(3):\n                        boolterm += ((i <= CODON_NT_INDEX[j]).astype('float') /\n                                (self.eta[i] - (i == CODON_NT_INDEX[j]).astype(\n                                'float')))\n                    for r in range(self.nsites):\n                        self.dprx['eta'][i][r] = self.prx[r] * (boolterm -\n                                scipy.dot(boolterm, self.prx[r]) / self.prx[r].sum())", "response": "Update dprx with new values from the prx and eta."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fill_diagonals(self, m):\n        assert m.shape == (self.nsites, N_CODON, N_CODON)\n        for r in range(self.nsites):\n            scipy.fill_diagonal(m[r], 0)\n            m[r][self._diag_indices] -= scipy.sum(m[r], axis=1)", "response": "Fills diagonals of nsites matrices in m so rows sum to 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spielman_wr(self, norm=True):\n\n        wr = []\n        for r in range(self.nsites):\n            num = 0\n            den = 0\n            for i in range(N_CODON):\n                j = scipy.intersect1d(scipy.where(CODON_SINGLEMUT[i]==True)[0],\n                        scipy.where(CODON_NONSYN[i]==True)[0])\n                p_i = self.stationarystate[r][i]\n                P_xy = self.Prxy[r][i][j].sum()\n                if norm:\n                    P_xy = P_xy/self.omega\n                Q_xy = self.Qxy[i][j].sum()\n                num += (p_i * P_xy)\n                den += (p_i * Q_xy)\n            result = num/den\n            wr.append(result)\n        return wr", "response": "Returns a list of site - specific omega values calculated from the ExpCM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dlogprior(self, param):\n        assert param in self.freeparams, \"Invalid param: {0}\".format(param)\n        return self._dlogprior[param]", "response": "Value of derivative of prior depends on value of prior."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate variables that depend on pi from zeta.", "response": "def _update_pi_vars(self):\n        \"\"\"Update variables that depend on `pi` from `zeta`.\n\n        The updated variables are: `pi`, `pi_codon`, `ln_pi_codon`, `piAx_piAy`,\n        `piAx_piAy_beta`, `ln_piAx_piAy_beta`, and `_logprior`.\n\n        If `zeta` is undefined (as it will be on the first call), then create\n        `zeta` and `origpi` from `pi` and `origbeta`.\"\"\"\n        minpi = self.PARAMLIMITS['pi'][0]\n        if not hasattr(self, 'zeta'):\n            # should only execute on first call to initialize zeta\n            assert not hasattr(self, 'origpi')\n            self.origpi = self.pi**self._origbeta\n            for r in range(self.nsites):\n                self.origpi[r] /= self.origpi[r].sum()\n                self.origpi[r][self.origpi[r] < 2 * minpi] = 2 * minpi\n                self.origpi[r] /= self.origpi[r].sum()\n            self.pi = self.origpi.copy()\n            self.zeta = scipy.ndarray(self.nsites * (N_AA - 1), dtype='float')\n            self.tildeFrxy = scipy.zeros((self.nsites, N_CODON, N_CODON),\n                    dtype='float')\n            for r in range(self.nsites):\n                zetaprod = 1.0\n                for i in range(N_AA - 1):\n                    zetari = 1.0 - self.pi[r][i] / zetaprod\n                    self.zeta.reshape(self.nsites, N_AA - 1)[r][i] = zetari\n                    zetaprod *= zetari\n            (minzeta, maxzeta) = self.PARAMLIMITS['zeta']\n            self.zeta[self.zeta < minzeta] = minzeta\n            self.zeta[self.zeta > maxzeta] = maxzeta\n            _checkParam('zeta', self.zeta, self.PARAMLIMITS, self.PARAMTYPES)\n        else:\n            # after first call, we are updating pi from zeta\n            for r in range(self.nsites):\n                zetaprod = 1.0\n                for i in range(N_AA - 1):\n                    zetari = self.zeta.reshape(self.nsites, N_AA - 1)[r][i]\n                    self.pi[r][i] = zetaprod * (1 - zetari)\n                    zetaprod *= zetari\n                self.pi[r][N_AA - 1] = zetaprod\n                self.pi[r][self.pi[r] < minpi] = minpi\n                self.pi[r] /= self.pi[r].sum()\n\n        super(ExpCM_fitprefs, self)._update_pi_vars()\n\n        with scipy.errstate(divide='raise', under='raise', over='raise',\n                invalid='ignore'):\n            scipy.copyto(self.tildeFrxy, self.omega * self.beta *\n                    (self.piAx_piAy_beta * (self.ln_piAx_piAy_beta - 1)\n                    + 1) / (1 - self.piAx_piAy_beta)**2,\n                    where=CODON_NONSYN)\n        scipy.copyto(self.tildeFrxy, self.omega * self.beta / 2.0,\n                where=scipy.logical_and(CODON_NONSYN, scipy.fabs(1 -\n                self.piAx_piAy_beta) < ALMOST_ZERO))\n\n        self._logprior = 0.0\n        self._dlogprior = dict([(param, 0.0) for param in self.freeparams])\n        if self.prior is None:\n            pass\n        elif self.prior[0] == 'invquadratic':\n            (priorstr, c1, c2) = self.prior\n            self._dlogprior = dict([(param, 0.0) for param in self.freeparams])\n            self._dlogprior['zeta'] = scipy.zeros(self.zeta.shape, dtype='float')\n            j = 0\n            aaindex = scipy.arange(N_AA)\n            for r in range(self.nsites):\n                pidiffr = self.pi[r] - self.origpi[r]\n                rlogprior = -c2 * scipy.log(1 + c1 * pidiffr**2).sum()\n                self._logprior += rlogprior\n                for i in range(N_AA - 1):\n                    zetari = self.zeta[j]\n                    self._dlogprior['zeta'][j] = -2 * c1 * c2 * (\n                            pidiffr[i : ] / (1 + c1 * pidiffr[i : ]**2) *\n                            self.pi[r][i : ] / (zetari - (aaindex == i).astype(\n                            'float')[i : ])).sum()\n                    j += 1\n        else:\n            raise ValueError(\"Invalid prior: {0}\".format(self.prior))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating dPrxy with the current values of the zeta and tildeFrxy.", "response": "def _update_dPrxy(self):\n        \"\"\"Update `dPrxy`.\"\"\"\n        super(ExpCM_fitprefs, self)._update_dPrxy()\n\n        if 'zeta' in self.freeparams:\n            tildeFrxyQxy = self.tildeFrxy * self.Qxy\n            j = 0\n            zetaxterm = scipy.ndarray((self.nsites, N_CODON, N_CODON), dtype='float')\n            zetayterm = scipy.ndarray((self.nsites, N_CODON, N_CODON), dtype='float')\n            for r in range(self.nsites):\n                for i in range(N_AA - 1):\n                    zetari = self.zeta[j]\n                    zetaxterm.fill(0)\n                    zetayterm.fill(0)\n                    zetaxterm[r][self._aa_for_x > i] = -1.0 / zetari\n                    zetaxterm[r][self._aa_for_x == i] = -1.0 / (zetari - 1.0)\n                    zetayterm[r][self._aa_for_y > i] = 1.0 / zetari\n                    zetayterm[r][self._aa_for_y == i] = 1.0 / (zetari - 1.0)\n                    self.dPrxy['zeta'][j] = tildeFrxyQxy * (zetayterm + zetaxterm)\n                    _fill_diagonals(self.dPrxy['zeta'][j], self._diag_indices)\n                    j += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates dprx with the new values from the current PMF.", "response": "def _update_dprx(self):\n        \"\"\"Update `dprx`.\"\"\"\n        super(ExpCM_fitprefs, self)._update_dprx()\n        j = 0\n        if 'zeta' in self.freeparams:\n            self.dprx['zeta'].fill(0)\n            for r in range(self.nsites):\n                for i in range(N_AA - 1):\n                    zetari = self.zeta[j]\n                    for a in range(i, N_AA):\n                        delta_aAx = (CODON_TO_AA == a).astype('float')\n                        self.dprx['zeta'][j][r] += (delta_aAx - (delta_aAx\n                                * self.prx[r]).sum())/ (zetari - int(i == a))\n                    self.dprx['zeta'][j] *= self.prx[r]\n                    j += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate dPrxy with the current values of the zeta and tildeFrxy.", "response": "def _update_dPrxy(self):\n        \"\"\"Update `dPrxy`.\"\"\"\n        super(ExpCM_fitprefs, self)._update_dPrxy()\n\n        if 'zeta' in self.freeparams:\n            self.dPrxy['zeta'].fill(0.0)\n            tildeFrxyQxy = self.tildeFrxy * self.Qxy\n            j = 0\n            for r in range(self.nsites):\n                for i in range(N_AA - 1):\n                    zetari = self.zeta[j]\n                    self.dPrxy['zeta'][j][r] = tildeFrxyQxy[r] * (\n                            ((i == self._aa_for_y).astype('float') -\n                            (i == self._aa_for_x).astype('float')) / zetari)\n                    j += 1\n            for j in range(self.dPrxy['zeta'].shape[0]):\n                _fill_diagonals(self.dPrxy['zeta'][j], self._diag_indices)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes phi dphi_dbeta and eta from g and frxy.", "response": "def _update_phi(self):\n        \"\"\"Compute `phi`, `dphi_dbeta`, and `eta` from `g` and `frxy`.\"\"\"\n        self.phi = self._compute_empirical_phi(self.beta)\n        _checkParam('phi', self.phi, self.PARAMLIMITS, self.PARAMTYPES)\n        self._eta_from_phi()\n        dbeta = 1.0e-3\n        self.dphi_dbeta = scipy.misc.derivative(self._compute_empirical_phi,\n                self.beta, dx=dbeta, n=1, order=5)\n        dphi_dbeta_halfdx = scipy.misc.derivative(self._compute_empirical_phi,\n                self.beta, dx=dbeta / 2, n=1, order=5)\n        assert scipy.allclose(self.dphi_dbeta, dphi_dbeta_halfdx, atol=1e-5,\n                rtol=1e-4), (\"The numerical derivative dphi_dbeta differs \"\n                \"considerably in value for step dbeta = {0} and a step \"\n                \"half that size, giving values of {1} and {2}.\").format(\n                dbeta, self.dphi_dbeta, dphi_dbeta_halfdx)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_empirical_phi(self, beta):\n\n        def F(phishort):\n            \"\"\"Difference between `g` and expected `g` given `phishort`.\"\"\"\n            phifull = scipy.append(phishort, 1 - phishort.sum())\n            phiprod = scipy.ones(N_CODON, dtype='float')\n            for w in range(N_NT):\n                phiprod *= phifull[w]**CODON_NT_COUNT[w]\n            frx_phiprod = frx * phiprod\n            frx_phiprod_codonsum = frx_phiprod.sum(axis=1)\n            gexpect = []\n            for w in range(N_NT - 1):\n                gexpect.append(\n                        ((CODON_NT_COUNT[w] * frx_phiprod).sum(axis=1) /\n                        frx_phiprod_codonsum).sum() / (3 * self.nsites))\n            gexpect = scipy.array(gexpect, dtype='float')\n            return self.g[ : -1] - gexpect\n\n        frx = self.pi_codon**beta\n        with scipy.errstate(invalid='ignore'):\n            result = scipy.optimize.root(F, self.phi[ : -1].copy(),\n                    tol=1e-8)\n            assert result.success, \"Failed: {0}\".format(result)\n            phishort = result.x\n        return scipy.append(phishort, 1 - phishort.sum())", "response": "Compute empirical phi at the given value of beta."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates dPrxy accounting for dependence of phi on beta.", "response": "def _update_dPrxy(self):\n        \"\"\"Update `dPrxy`, accounting for dependence of `phi` on `beta`.\"\"\"\n        super(ExpCM_empirical_phi, self)._update_dPrxy()\n        if 'beta' in self.freeparams:\n            self.dQxy_dbeta = scipy.zeros((N_CODON, N_CODON), dtype='float')\n            for w in range(N_NT):\n                scipy.copyto(self.dQxy_dbeta, self.dphi_dbeta[w],\n                        where=CODON_NT_MUT[w])\n            self.dQxy_dbeta[CODON_TRANSITION] *= self.kappa\n            self.dPrxy['beta'] += self.Frxy * self.dQxy_dbeta\n            _fill_diagonals(self.dPrxy['beta'], self._diag_indices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates dprx accounting for dependence of phi on beta.", "response": "def _update_dprx(self):\n        \"\"\"Update `dprx`, accounting for dependence of `phi` on `beta`.\"\"\"\n        super(ExpCM_empirical_phi, self)._update_dprx()\n        if 'beta' in self.freeparams:\n            dphi_over_phi = scipy.zeros(N_CODON, dtype='float')\n            for j in range(3):\n                dphi_over_phi += (self.dphi_dbeta / self.phi)[CODON_NT_INDEX[j]]\n            for r in range(self.nsites):\n                self.dprx['beta'][r] += self.prx[r] * (dphi_over_phi\n                        - scipy.dot(dphi_over_phi, self.prx[r]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_dPrxy(self):\n        super(ExpCM_empirical_phi_divpressure, self)._update_dPrxy()\n        if 'omega2' in self.freeparams:\n            with scipy.errstate(divide='raise', under='raise', over='raise',\n                            invalid='ignore'):\n                scipy.copyto(self.dPrxy['omega2'], -self.ln_piAx_piAy_beta\n                        * self.Qxy * self.omega /\n                        (1 - self.piAx_piAy_beta), where=CODON_NONSYN)\n            scipy.copyto(self.dPrxy['omega2'], self.Qxy * self.omega,\n                       where=scipy.logical_and(CODON_NONSYN, scipy.fabs(1 -\n                       self.piAx_piAy_beta) < ALMOST_ZERO))\n            for r in range(self.nsites):\n                self.dPrxy['omega2'][r] *= self.deltar[r]\n            _fill_diagonals(self.dPrxy['omega2'], self._diag_indices)", "response": "Update dPrxy accounting for dependence of Prxy on omega2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_Frxy(self):\n        self.Frxy.fill(1.0)\n        self.Frxy_no_omega.fill(1.0)\n        with scipy.errstate(divide='raise', under='raise', over='raise',\n                invalid='ignore'):\n            scipy.copyto(self.Frxy_no_omega, -self.ln_piAx_piAy_beta\n                    / (1 - self.piAx_piAy_beta), where=scipy.logical_and(\n                    CODON_NONSYN, scipy.fabs(1 - self.piAx_piAy_beta) >\n                    ALMOST_ZERO))\n        for r in range(self.nsites):\n            scipy.copyto(self.Frxy_no_omega[r], self.Frxy_no_omega[r] *\n                    (1 + self.omega2 * self.deltar[r]), where=CODON_NONSYN)\n        scipy.copyto(self.Frxy, self.Frxy_no_omega * self.omega,\n                where=CODON_NONSYN)", "response": "Update the internal FRxy field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of the parameters in the model.", "response": "def paramsReport(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        report = {}\n        for param in self._REPORTPARAMS:\n            pvalue = getattr(self, param)\n            if isinstance(pvalue, float):\n                report[param] = pvalue\n            elif isinstance(pvalue, scipy.ndarray) and pvalue.shape == (3, N_NT):\n                for p in range(3):\n                    for w in range(N_NT - 1):\n                        report['{0}{1}{2}'.format(param, p, INDEX_TO_NT[w])] =\\\n                                pvalue[p][w]\n            else:\n                raise ValueError(\"Unexpected param: {0}\".format(param))\n        return report"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the branch scale of the current model class.", "response": "def branchScale(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        bs = -(self.Phi_x * scipy.diagonal(self.Pxy[0])).sum() * self.mu\n        assert bs > 0\n        return bs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculate_correctedF3X4(self):\n        '''Calculate `phi` based on the empirical `e_pw` values'''\n        def F(phi):\n            phi_reshape = phi.reshape((3, N_NT))\n            functionList = []\n            stop_frequency = []\n\n            for x in range(N_STOP):\n                codonFrequency = STOP_CODON_TO_NT_INDICES[x] * phi_reshape\n                codonFrequency = scipy.prod(codonFrequency.sum(axis=1))\n                stop_frequency.append(codonFrequency)\n            C = scipy.sum(stop_frequency)\n\n            for p in range(3):\n                for w in range(N_NT):\n                    s = 0\n                    for x in range(N_STOP):\n                        if STOP_CODON_TO_NT_INDICES[x][p][w] == 1:\n                            s += stop_frequency[x]\n                    functionList.append((phi_reshape[p][w] - s)/(1 - C)\n                            - self.e_pw[p][w])\n            return functionList\n\n        phi = self.e_pw.copy().flatten()\n        with scipy.errstate(invalid='ignore'):\n            result = scipy.optimize.root(F, phi,\n                    tol=1e-8)\n            assert result.success, \"Failed: {0}\".format(result)\n            return result.x.reshape((3, N_NT))", "response": "Calculate the corrected F3X4 function for the current empirical e_pw values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the stationary state from phi.", "response": "def _calculate_Phi_x(self):\n        \"\"\"Calculate `Phi_x` (stationary state) from `phi`.\"\"\"\n        self.Phi_x = scipy.ones(N_CODON, dtype='float')\n        for codon in range(N_CODON):\n            for pos in range(3):\n                self.Phi_x[codon] *= self.phi[pos][CODON_NT_INDEX[pos][codon]]\n        self.Phi_x /= self.Phi_x.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the parameters of the class.", "response": "def updateParams(self, newvalues, update_all=False):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        assert all(map(lambda x: x in self.freeparams, newvalues.keys())),\\\n                \"Invalid entry in newvalues: {0}\\nfreeparams: {1}\".format(\n                ', '.join(newvalues.keys()), ', '.join(self.freeparams))\n        changed = set([]) # contains string names of changed params\n        for (name, value) in newvalues.items():\n            _checkParam(name, value, self.PARAMLIMITS, self.PARAMTYPES)\n            if isinstance(value, scipy.ndarray):\n                if (value != getattr(self, name)).any():\n                    changed.add(name)\n                    setattr(self, name, value.copy())\n            else:\n                if value != getattr(self, name):\n                    changed.add(name)\n                    setattr(self, name, copy.copy(value))\n\n        if update_all or changed:\n            self._cached = {}\n\n        # The order of the updating below is important.\n        # If you change it, you may break either this class\n        # **or** classes that inherit from it.\n        # Note also that not all attributes need to be updated\n        # for all possible parameter changes, but just doing it\n        # this way is much simpler and adds negligible cost.\n        if update_all or (changed and changed != set(['mu'])):\n            self._update_Pxy()\n            self._update_Pxy_diag()\n            self._update_dPxy()\n            self._update_B()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef M(self, t, tips=None, gaps=None):\n        assert isinstance(t, float) and t > 0, \"Invalid t: {0}\".format(t)\n        with scipy.errstate(under='ignore'): # don't worry if some values 0\n            if ('expD', t) not in self._cached:\n                self._cached[('expD', t)] = scipy.exp(self.D * self.mu * t)\n            expD = self._cached[('expD', t)]\n            # swap axes to broadcast multiply D as diagonal matrix\n            temp = scipy.ascontiguousarray((self.A.swapaxes(0, 1)\n                    * expD).swapaxes(1, 0), dtype=float)\n            M = broadcastMatrixMultiply(temp, self.Ainv)\n            assert M.min() > -1e-3, \"Overly negative M: {0}\".format(M.min())\n            M[M < 0] = 0.0\n            if tips is None:\n                return scipy.tile(M, (self.nsites, 1, 1))\n            else:\n                newM = scipy.zeros((len(tips), N_CODON))\n                for i in range(len(tips)):\n                    newM[i] =(M[0][:,tips[i]])\n                if gaps is not None:\n                    newM[gaps] = scipy.ones(N_CODON, dtype='float')\n                return newM", "response": "Compute the matrix M at a given time t."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dM(self, t, param, Mt, tips=None, gaps=None):\n        assert isinstance(t, float) and t > 0, \"Invalid t: {0}\".format(t)\n        assert (param == 't') or (param in self.freeparams), (\n                \"Invalid param: {0}\".format(param))\n\n        if Mt is None:\n            Mt = self.M(t, tips=tips, gaps=gaps)\n\n        if (param == 'mu') or (param == 't'):\n            if param == 'mu':\n                alpha = t\n            else:\n                alpha = self.mu\n            if tips is None:\n                dM_param = scipy.tile(broadcastMatrixMultiply(self.Pxy,\n                        scipy.tile(Mt[0], (1, 1, 1)), alpha=alpha),\n                        (self.nsites, 1, 1))\n            else:\n                #Pxy is tiled over the number of sites\n                dM_param = broadcastMatrixVectorMultiply(scipy.tile(self.Pxy[0],\n                        (self.nsites, 1, 1)), Mt, alpha=alpha)\n                if gaps is not None:\n                    dM_param[gaps] = scipy.zeros(N_CODON, dtype='float')\n            return dM_param\n\n        paramval = getattr(self, param)\n        assert isinstance(paramval, float), \"All params should be floats\"\n\n        if ('expD', t) not in self._cached:\n            self._cached[('expD', t)] = scipy.exp(self.D * self.mu * t)\n        expD = self._cached[('expD', t)]\n\n        if ('V', t) not in self._cached:\n            if 'Dxx_Dyy' not in self._cached:\n                Dyy = scipy.tile(self.D, (1, N_CODON)).reshape(\n                        1, N_CODON, N_CODON)\n                Dxx = scipy.array([Dyy[r].transpose() for r in\n                        range(1)])\n                self._cached['Dxx_Dyy'] = Dxx - Dyy\n            Dxx_Dyy = self._cached['Dxx_Dyy']\n            if 'Dxx_Dyy_lt_ALMOST_ZERO' not in self._cached:\n                self._cached['Dxx_Dyy_lt_ALMOST_ZERO'] = scipy.fabs(\n                        Dxx_Dyy) < ALMOST_ZERO\n            Dxx_Dyy_lt_ALMOST_ZERO = self._cached['Dxx_Dyy_lt_ALMOST_ZERO']\n            with scipy.errstate(divide='raise', under='ignore',\n                    over='raise', invalid='ignore'):\n                expDyy = scipy.tile(expD, (1, N_CODON)).reshape(\n                        1, N_CODON, N_CODON)\n                expDxx = scipy.array([expDyy[r].transpose() for r in\n                        range(1)])\n                V = (expDxx - expDyy) / Dxx_Dyy\n            with scipy.errstate(under='ignore'): # OK if some values 0\n                scipy.copyto(V, self.mu * t * expDxx, where=\n                        Dxx_Dyy_lt_ALMOST_ZERO)\n            self._cached[('V', t)] = V\n        V = self._cached[('V', t)]\n\n        with scipy.errstate(under='ignore'): # don't worry if some values 0\n            dM_param = broadcastMatrixMultiply(self.A,\n                        broadcastMatrixMultiply(self.B[param]\n                        * V, self.Ainv))\n            if tips is None:\n                return scipy.tile(dM_param, (self.nsites, 1, 1))\n            else:\n                newdM_param = scipy.zeros((len(tips), N_CODON))\n                for i in range(len(tips)):\n                    newdM_param[i] =(dM_param[0][:,tips[i]])\n                if gaps is not None:\n                    newdM_param[gaps] = scipy.zeros(N_CODON, dtype='float')\n                return newdM_param", "response": "Method to compute the dM parameter for a specific time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates Pxy using current omega kappa and Phon_x.", "response": "def _update_Pxy(self):\n        \"\"\"Update `Pxy` using current `omega`, `kappa`, and `Phi_x`.\"\"\"\n        scipy.copyto(self.Pxy_no_omega, self.Phi_x.transpose(),\n                where=CODON_SINGLEMUT)\n        self.Pxy_no_omega[0][CODON_TRANSITION] *= self.kappa\n        self.Pxy = self.Pxy_no_omega.copy()\n        self.Pxy[0][CODON_NONSYN] *= self.omega\n        _fill_diagonals(self.Pxy, self._diag_indices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_Pxy_diag(self):\n        for r in range(1):\n            Phi_x_half = self.Phi_x**0.5\n            Phi_x_neghalf = self.Phi_x**-0.5\n            #symm_p = scipy.dot(scipy.diag(Phi_x_half), scipy.dot(self.Pxy[r], scipy.diag(Phi_x_neghalf)))\n            symm_p = (Phi_x_half * (self.Pxy[r] * Phi_x_neghalf).transpose()).transpose()\n            #assert scipy.allclose(symm_p, symm_p.transpose())\n            (evals, evecs) = scipy.linalg.eigh(symm_p)\n            #assert scipy.allclose(scipy.linalg.inv(evecs), evecs.transpose())\n            #assert scipy.allclose(symm_pr, scipy.dot(evecs, scipy.dot(scipy.diag(evals), evecs.transpose())))\n            self.D[r] = evals\n            self.Ainv[r] = evecs.transpose() * Phi_x_half\n            self.A[r] = (Phi_x_neghalf * evecs.transpose()).transpose()", "response": "Update D A Ainv from Pxy and Phi_x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dlogprior(self, param):\n        assert param in self.freeparams, \"Invalid param: {0}\".format(param)\n        if param in self.distributionparams:\n            return 0.0\n        else:\n            return self._models[0].dlogprior(param)", "response": "Equal to value of basemodel. dlogprior."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef M(self, k, t, tips=None, gaps=None):\n        assert 0 <= k < self.ncats\n        return self._models[k].M(t, tips=tips, gaps=gaps)", "response": "Returns the distribution model for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the distribution model for the given key t and parameter Mkt.", "response": "def dM(self, k, t, param, Mkt, tips=None, gaps=None):\n        \"\"\"See docs for `DistributionModel` abstract base class.\"\"\"\n        assert 0 <= k < self.ncats\n        assert ((param in self.freeparams) or (param == 't') or (\n                param == self.distributedparam))\n        assert param not in self.distributionparams\n        return self._models[k].dM(t, param, Mkt, tips=tips, gaps=gaps)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef d_distributionparams(self):\n        if not self._d_distributionparams:\n            dx = 1.0e-3\n            def f_alpha(alpha):\n                return DiscreteGamma(alpha, self.beta_lambda, self.ncats)\n            def f_beta(beta):\n                return DiscreteGamma(self.alpha_lambda, beta, self.ncats)\n            assert set(self.distributionparams) == {'alpha_lambda', 'beta_lambda'}\n            for (param, f) in [('alpha_lambda', f_alpha), ('beta_lambda', f_beta)]:\n                pvalue = getattr(self, param)\n                dparam = scipy.misc.derivative(f, pvalue, dx, n=1, order=5)\n                assert dparam.shape == (self.ncats,)\n                for stepchange in [0.5, 2]: # make sure robust to step size\n                    dparam2 = scipy.misc.derivative(f, pvalue, stepchange * dx,\n                            n=1, order=5)\n                    assert scipy.allclose(dparam, dparam2, atol=1e-5, rtol=1e-4), (\n                            \"Numerical derivative of {0} at {1} \"\n                            \"differs for step {2} and {3}: {4} and {5}\"\n                            \", respectively.\").format(param, pvalue,\n                            dx, dx * stepchange, dparam, dparam2)\n                self._d_distributionparams[param] = dparam\n        return self._d_distributionparams", "response": "See docs for DistributionModel abstract base class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the parameters of the current object.", "response": "def updateParams(self, newvalues, update_all=False):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        assert all(map(lambda x: x in self.freeparams, newvalues.keys())),\\\n                \"Invalid entry in newvalues: {0}\\nfreeparams: {1}\".format(\n                ', '.join(newvalues.keys()), ', '.join(self.freeparams))\n\n        newvalues_list = [{} for k in range(self.ncats)]\n\n        if update_all or any([param in self.distributionparams for param\n                in newvalues.keys()]):\n            self._d_distributionparams = {}\n            for param in self.distributionparams:\n                if param in newvalues:\n                    _checkParam(param, newvalues[param], self.PARAMLIMITS,\n                            self.PARAMTYPES)\n                    setattr(self, param, copy.copy(newvalues[param]))\n            self._lambdas = DiscreteGamma(self.alpha_lambda, self.beta_lambda,\n                    self.ncats)\n            for (k, l) in enumerate(self._lambdas):\n                newvalues_list[k][self.distributedparam] = l\n        for name in self.freeparams:\n            if name not in self.distributionparams:\n                if name in newvalues:\n                    value = newvalues[name]\n                    _checkParam(name, value, self.PARAMLIMITS, self.PARAMTYPES)\n                    setattr(self, name, copy.copy(value))\n                    for k in range(self.ncats):\n                        newvalues_list[k][name] = value\n                elif update_all:\n                    for k in range(self.ncats):\n                        newvalues_list[k][name] = getattr(self, name)\n\n        assert len(newvalues_list) == len(self._models) == self.ncats\n        for (k, newvalues_k) in enumerate(newvalues_list):\n            self._models[k].updateParams(newvalues_k)\n\n        # check to make sure all models have same parameter values\n        for param in self.freeparams:\n            if param not in self.distributionparams:\n                pvalue = getattr(self, param)\n                assert all([scipy.allclose(pvalue, getattr(model, param))\n                        for model in self._models]), (\"{0}\\n{1}\".format(\n                        pvalue, '\\n'.join([str(getattr(model, param))\n                        for model in self._models])))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paramsReport(self):\n        report = self._models[0].paramsReport\n        del report[self.distributedparam]\n        for param in self.distributionparams:\n            new_name = \"_\".join([param.split(\"_\")[0], self.distributedparam])\n            report[new_name] = getattr(self, param)\n        return report", "response": "See docs for Model abstract base class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsee docs for Model abstract base class.", "response": "def mu(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        mu = self._models[0].mu\n        assert all([mu == model.mu for model in self._models])\n        return mu"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets new mu value.", "response": "def mu(self, value):\n        \"\"\"Set new `mu` value.\"\"\"\n        for k in range(self.ncats):\n            self._models[k].updateParams({'mu':value})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the stationary state of the model with the given id.", "response": "def stationarystate(self, k):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        assert 0 <= k < self.ncats\n        return self._models[k].stationarystate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the dstationarystate of the model with the given parameter.", "response": "def dstationarystate(self, k, param):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        assert param not in self.distributionparams\n        assert param in self.freeparams or param == self.distributedparam\n        ds = self._models[k].dstationarystate(param)\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the branch scale of the current category.", "response": "def branchScale(self):\n        \"\"\"See docs for `Model` abstract base class.\"\"\"\n        bscales = [m.branchScale for m in self._models]\n        return (self.catweights * bscales).sum()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef step_impl04(context):\n    single = context.singleStore\n    general = context.generalStore\n    key = 13\n    item = 42\n    assert single.request(key) == general.request(key)\n    single.add_item(key, item)\n    general.add_item(key, item)\n    assert single.request(key) == general.request(key)", "response": "Compare behavior of singleton vs. non - singleton."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares test for singleton property.", "response": "def step_impl06(context):\n    \"\"\"Prepare test for singleton property.\n\n    :param context: test context.\n    \"\"\"\n    store = context.SingleStore\n    context.st_1 = store()\n    context.st_2 = store()\n    context.st_3 = store()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef step_impl07(context):\n    assert context.st_1 is context.st_2\n    assert context.st_2 is context.st_3", "response": "Test for singleton property.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(cls, dbfile, encoding=None, fieldnames_lower=True, case_sensitive=True):\n        if not case_sensitive:\n            if isinstance(dbfile, string_types):\n                dbfile = pick_name(dbfile, listdir(path.dirname(dbfile)))\n\n        with open(dbfile, 'rb') as f:\n            yield cls(f, encoding=encoding, fieldnames_lower=fieldnames_lower)", "response": "Context manager. Allows opening a. dbf file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_plot(self):\n        if self.tick_label_fontsize is not None:\n            self.x_tick_label_fontsize = self.tick_label_fontsize\n            self.y_tick_label_fontsize = self.tick_label_fontsize\n\n        # setup xticks and yticks and limits\n        # if logspaced, the log values are used.\n        xticks = np.arange(float(self.xlims[0]),\n                           float(self.xlims[1])\n                           + float(self.dx),\n                           float(self.dx))\n\n        yticks = np.arange(float(self.ylims[0]),\n                           float(self.ylims[1])\n                           + float(self.dy),\n                           float(self.dy))\n\n        xlim = [xticks.min(), xticks.max()]\n        ylim = [yticks.min(), yticks.max()]\n\n        if self.reverse_x_axis:\n                xticks = xticks[::-1]\n                xlim = [xticks.max(), xticks.min()]\n\n        if self.reverse_y_axis:\n                    yticks = yticks[::-1]\n                    ylim = [yticks.max(), yticks.min()]\n\n        self.axis.set_xlim(xlim)\n        self.axis.set_ylim(ylim)\n\n        # adjust ticks for spacing. If 'wide' then show all labels, if 'tight' remove end labels.\n        if self.spacing == 'wide':\n            x_inds = np.arange(len(xticks))\n            y_inds = np.arange(len(yticks))\n        else:\n            # remove end labels\n            x_inds = np.arange(1, len(xticks)-1)\n            y_inds = np.arange(1, len(yticks)-1)\n\n        self.axis.set_xticks(xticks[x_inds])\n        self.axis.set_yticks(yticks[y_inds])\n\n        # set tick labels based on scale\n        if self.xscale == 'log':\n            self.axis.set_xticklabels([r'$10^{%i}$' % int(i)\n                                      for i in xticks[x_inds]], fontsize=self.x_tick_label_fontsize)\n        else:\n            self.axis.set_xticklabels([r'$%.3g$' % (i)\n                                      for i in xticks[x_inds]], fontsize=self.x_tick_label_fontsize)\n\n        if self.yscale == 'log':\n            self.axis.set_yticklabels([r'$10^{%i}$' % int(i)\n                                      for i in yticks[y_inds]], fontsize=self.y_tick_label_fontsize)\n        else:\n            self.axis.set_yticklabels([r'$%.3g$' % (i)\n                                      for i in yticks[y_inds]], fontsize=self.y_tick_label_fontsize)\n\n        # add grid\n        if self.add_grid:\n            self.axis.grid(True, linestyle='-', color='0.75')\n\n        # add title\n        if 'title' in self.__dict__.keys():\n            self.axis.set_title(r'{}'.format(self.title), **self.title_kwargs)\n\n        if 'xlabel' in self.__dict__.keys():\n            self.axis.set_xlabel(r'{}'.format(self.xlabel), **self.xlabel_kwargs)\n\n        if 'ylabel' in self.__dict__.keys():\n            self.axis.set_ylabel(r'{}'.format(self.ylabel), **self.ylabel_kwargs)\n        return", "response": "Setup limits and labels for all plot types."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an error response to standardized ErrorDetails.", "response": "def _adapt_response(self, response):\n        \"\"\"Convert error responses to standardized ErrorDetails.\"\"\"\n        if 'application/json' in response.headers['content-type']:\n            body = response.json()\n            status = response.status_code\n\n            if body.get('error'):\n                return self._simple_response_to_error_adapter(status, body)\n\n        raise UnknownHttpError(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a simple response to an error adapter.", "response": "def _simple_response_to_error_adapter(self, status, original_body):\n        \"\"\"Convert a single error response.\"\"\"\n        meta = original_body.get('error')\n        e = []\n\n        if 'error_detail' in original_body:\n            errors = original_body.get('error_detail')\n\n            for error in errors:\n                if type(error) == dict:\n                    for parameter, title in error.iteritems():\n                        e.append(ErrorDetails(parameter, title))\n        elif 'error_description' in original_body:\n            e.append(original_body.get('error_description'))\n\n        return e, meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the environment for the Django app within a manage. py file.", "response": "def setup_environ(manage_file, settings=None, more_pythonic=False):\n    \"\"\"Sets up a Django app within a manage.py file.\n\n    Keyword Arguments\n\n    **settings**\n        An imported settings module. Without this, playdoh tries to import\n        these modules (in order): DJANGO_SETTINGS_MODULE, settings\n\n    **more_pythonic**\n        When True, does not do any path hackery besides adding the vendor dirs.\n        This requires a newer Playdoh layout without top level apps, lib, etc.\n    \"\"\"\n    # sys is global to avoid undefined local\n    global sys, current_settings, execute_from_command_line, ROOT\n\n    ROOT = os.path.dirname(os.path.abspath(manage_file))\n\n    # Adjust the python path and put local packages in front.\n    prev_sys_path = list(sys.path)\n\n    # Make root application importable without the need for\n    # python setup.py install|develop\n    sys.path.append(ROOT)\n\n    if not more_pythonic:\n        warnings.warn(\"You're using an old-style Playdoh layout with a top \"\n                      \"level __init__.py and apps directories. This is error \"\n                      \"prone and fights the Zen of Python. \"\n                      \"See http://playdoh.readthedocs.org/en/latest/\"\n                      \"getting-started/upgrading.html\")\n        # Give precedence to your app's parent dir, which contains __init__.py\n        sys.path.append(os.path.abspath(os.path.join(ROOT, os.pardir)))\n\n        site.addsitedir(path('apps'))\n        site.addsitedir(path('lib'))\n\n    # Local (project) vendor library\n    site.addsitedir(path('vendor-local'))\n    site.addsitedir(path('vendor-local/lib/python'))\n\n    # Global (upstream) vendor library\n    site.addsitedir(path('vendor'))\n    site.addsitedir(path('vendor/lib/python'))\n\n    # Move the new items to the front of sys.path. (via virtualenv)\n    new_sys_path = []\n    for item in list(sys.path):\n        if item not in prev_sys_path:\n            new_sys_path.append(item)\n            sys.path.remove(item)\n    sys.path[:0] = new_sys_path\n\n    from django.core.management import execute_from_command_line  # noqa\n    if not settings:\n        if 'DJANGO_SETTINGS_MODULE' in os.environ:\n            settings = import_mod_by_name(os.environ['DJANGO_SETTINGS_MODULE'])\n        elif os.path.isfile(os.path.join(ROOT, 'settings_local.py')):\n            import settings_local as settings\n            warnings.warn(\"Using settings_local.py is deprecated. See \"\n                          \"http://playdoh.readthedocs.org/en/latest/upgrading.html\",\n                          DeprecationWarning)\n        else:\n            import settings\n    current_settings = settings\n    validate_settings(settings)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nraising an error in prod if we see any insecure settings. This used to warn during development but that was changed in 71718bec324c2561da6cc3990c927ee87362f0f7", "response": "def validate_settings(settings):\n    \"\"\"\n    Raise an error in prod if we see any insecure settings.\n\n    This used to warn during development but that was changed in\n    71718bec324c2561da6cc3990c927ee87362f0f7\n    \"\"\"\n    from django.core.exceptions import ImproperlyConfigured\n    if settings.SECRET_KEY == '':\n        msg = 'settings.SECRET_KEY cannot be blank! Check your local settings'\n        if not settings.DEBUG:\n            raise ImproperlyConfigured(msg)\n\n    if getattr(settings, 'SESSION_COOKIE_SECURE', None) is None:\n        msg = ('settings.SESSION_COOKIE_SECURE should be set to True; '\n               'otherwise, your session ids can be intercepted over HTTP!')\n        if not settings.DEBUG:\n            raise ImproperlyConfigured(msg)\n\n    hmac = getattr(settings, 'HMAC_KEYS', {})\n    if not len(hmac.keys()):\n        msg = 'settings.HMAC_KEYS cannot be empty! Check your local settings'\n        if not settings.DEBUG:\n            raise ImproperlyConfigured(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate an incremental neighbor graph for a given set of data.", "response": "def incremental_neighbor_graph(X, precomputed=False, k=None, epsilon=None,\n                               weighting='none'):\n  '''See neighbor_graph.'''\n  assert ((k is not None) or (epsilon is not None)\n          ), \"Must provide `k` or `epsilon`\"\n  assert (_issequence(k) ^ _issequence(epsilon)\n          ), \"Exactly one of `k` or `epsilon` must be a sequence.\"\n  assert weighting in ('binary','none'), \"Invalid weighting param: \" + weighting\n  is_weighted = weighting == 'none'\n\n  if precomputed:\n    D = X\n  else:\n    D = pairwise_distances(X, metric='euclidean')\n  # pre-sort for efficiency\n  order = np.argsort(D)[:,1:]\n\n  if k is None:\n    k = D.shape[0]\n\n  # generate the sequence of graphs\n  # TODO: convert the core of these loops to Cython for speed\n  W = np.zeros_like(D)\n  I = np.arange(D.shape[0])\n  if _issequence(k):\n    # varied k, fixed epsilon\n    if epsilon is not None:\n      D[D > epsilon] = 0\n    old_k = 0\n    for new_k in k:\n      idx = order[:, old_k:new_k]\n      dist = D[I, idx.T]\n      W[I, idx.T] = dist if is_weighted else 1\n      yield Graph.from_adj_matrix(W)\n      old_k = new_k\n  else:\n    # varied epsilon, fixed k\n    idx = order[:,:k]\n    dist = D[I, idx.T].T\n    old_i = np.zeros(D.shape[0], dtype=int)\n    for eps in epsilon:\n      for i, row in enumerate(dist):\n        oi = old_i[i]\n        ni = oi + np.searchsorted(row[oi:], eps)\n        rr = row[oi:ni]\n        W[i, idx[i,oi:ni]] = rr if is_weighted else 1\n        old_i[i] = ni\n      yield Graph.from_adj_matrix(W)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string with version information.", "response": "def Versions():\n    \"\"\"Returns a string with version information.\n\n    You would call this function if you want a string giving detailed information\n    on the version of ``phydms`` and the associated packages that it uses.\n    \"\"\"\n    s = [\\\n            'Version information:',\n            '\\tTime and date: %s' % time.asctime(),\n            '\\tPlatform: %s' % platform.platform(),\n            '\\tPython version: %s' % sys.version.replace('\\n', ' '),\n            '\\tphydms version: %s' % phydmslib.__version__,\n            ]\n    for modname in ['Bio', 'cython', 'numpy', 'scipy', 'matplotlib',\n            'natsort', 'sympy', 'six', 'pandas', 'pyvolve', 'statsmodels',\n            'weblogolib', 'PyPDF2']:\n        try:\n            v = importlib.import_module(modname).__version__\n            s.append('\\t%s version: %s' % (modname, v))\n        except ImportError:\n            s.append('\\t%s cannot be imported into Python' % modname)\n    return '\\n'.join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ReadCodonAlignment(fastafile, checknewickvalid):\n    codonmatch = re.compile('^[ATCG]{3}$')\n    gapmatch = re.compile('^-+^')\n    seqs = [(seq.description.strip(), str(seq.seq).upper()) for seq\n            in Bio.SeqIO.parse(fastafile, 'fasta')]\n    assert seqs, \"{0} failed to specify any sequences\".format(fastafile)\n\n    seqlen = len(seqs[0][1])\n    if not all([len(seq) == seqlen for (head, seq) in seqs]):\n        raise ValueError((\"All sequences in {0} are not of the same length; \"\n                \"they must not be properly aligned\").format(fastafile))\n    if (seqlen < 3) or (seqlen % 3 != 0):\n        raise ValueError((\"The length of the sequences in {0} is {1} which \"\n                \"is not divisible by 3; they are not valid codon sequences\"\n                ).format(fastafile, seqlen))\n\n    terminalcodon = []\n    codons_by_position = dict([(icodon, []) for icodon in range(seqlen // 3)])\n    for (head, seq) in seqs:\n        assert len(seq) % 3 == 0\n        for icodon in range(seqlen // 3):\n            codon = seq[3 * icodon : 3 * icodon + 3]\n            codons_by_position[icodon].append(codon)\n            if codonmatch.search(codon):\n                aa = str(Bio.Seq.Seq(codon).translate())\n                if aa == '*':\n                    if icodon + 1 != len(seq) // 3:\n                        raise ValueError((\"In {0}, sequence {1}, non-terminal \"\n                                \"codon {2} is stop codon: {3}\").format(\n                                fastafile, head, icodon + 1, codon))\n            elif codon == '---':\n                aa = '-'\n            else:\n                raise ValueError((\"In {0}, sequence {1}, codon {2} is invalid: \"\n                        \"{3}\").format(fastafile, head, icodon + 1, codon))\n        terminalcodon.append(aa)\n\n    for (icodon, codonlist) in codons_by_position.items():\n        if all([codon == '---' for codon in codonlist]):\n            raise ValueError((\"In {0}, all codons are gaps at position {1}\"\n                    ).format(fastafile, icodon + 1))\n\n    if all([aa in ['*', '-'] for aa in terminalcodon]):\n        if len(seq) == 3:\n            raise ValueError((\"The only codon is a terminal stop codon for \"\n                    \"the sequences in {0}\").format(fastafile))\n        seqs = [(head, seq[ : -3]) for (head, seq) in seqs]\n    elif any([aa == '*' for aa in terminalcodon]):\n        raise ValueError((\"Only some sequences in {0} have a terminal stop \"\n                \"codon. All or none must have terminal stop.\").format(fastafile))\n\n    if any([gapmatch.search(seq) for (head, seq) in seqs]):\n        raise ValueError((\"In {0}, at least one sequence is entirely composed \"\n                \"of gaps.\").format(fastafile))\n\n    if checknewickvalid:\n        if len(set([head for (head, seq) in seqs])) != len(seqs):\n            raise ValueError(\"Headers in {0} not all unique\".format(fastafile))\n        disallowedheader = re.compile('[\\s\\:\\;\\(\\)\\[\\]\\,\\'\\\"]')\n        for (head, seq) in seqs:\n            if disallowedheader.search(head):\n                raise ValueError((\"Invalid character in header in {0}:\"\n                        \"\\n{2}\").format(fastafile, head))\n\n    return seqs", "response": "Reads codon alignment from a FASTA file and returns the aligned sequence list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readPrefs(prefsfile, minpref=0, avgprefs=False, randprefs=False,\n        seed=1, sites_as_strings=False):\n    \"\"\"Read preferences from file with some error checking.\n\n    Args:\n        `prefsfile` (string or readable file-like object)\n            File holding amino-acid preferences. Can be\n            comma-, space-, or tab-separated file with column\n            headers of `site` and then all one-letter amino-acid\n            codes, or can be in the more complex format written\n            `dms_tools v1 <http://jbloomlab.github.io/dms_tools/>`_.\n            Must be prefs for consecutively numbered sites starting at 1.\n            Stop codon prefs can be present (stop codons are indicated by\n            ``*``); if so they are removed and prefs re-normalized to sum to 1.\n        `minpref` (float >= 0)\n            Adjust all preferences to be >= this number.\n        `avgprefs`, `randprefs` (bool)\n            Mutually exclusive options specifying to average or\n            randomize prefs across sites.\n        `seed` (int)\n            Seed used to sort random number generator for `randprefs`.\n        `sites_as_strings` (bool)\n            By default, the site numers are coerced to integers.\n            If this option is `True`, then they are kept as strings.\n\n    Returns:\n        `prefs` (dict)\n            `prefs[r][a]` is the preference of site `r` for amino-acid `a`.\n            `r` is an `int` unless `sites_as_strings=True`.\n    \"\"\"\n    assert minpref >= 0, 'minpref must be >= 0'\n\n    aas = set(phydmslib.constants.AA_TO_INDEX.keys())\n\n    try:\n        df = pandas.read_csv(prefsfile, sep=None, engine='python')\n        pandasformat = True\n    except ValueError:\n        pandasformat = False\n    if pandasformat and (set(df.columns) == aas.union(set(['site'])) or\n            set(df.columns) == aas.union(set(['site', '*']))):\n        # read valid preferences as data frame\n        sites = df['site'].tolist()\n        prefs = {}\n        for r in sites:\n            rdf = df[df['site'] == r]\n            prefs[r] = {}\n            for aa in df.columns:\n                if aa != 'site':\n                    prefs[r][aa] = float(rdf[aa])\n    else:\n        # try reading as dms_tools format\n        prefs = phydmslib.file_io.readPrefs_dms_tools_format(prefsfile)[2]\n        sites = list(prefs.keys())\n\n    # error check prefs\n    if not sites_as_strings:\n        try:\n            sites = [int(r) for r in sites]\n        except ValueError:\n            raise ValueError(\"sites not int in prefsfile {0}\".format(prefsfile))\n        assert (min(sites) == 1 and max(sites) - min(sites) == len(sites) - 1),\\\n                \"Sites not consecutive starting at 1\"\n        prefs = dict([(int(r), rprefs) for (r, rprefs) in prefs.items()])\n    else:\n        sites = [str(r) for r in sites]\n        prefs = dict([(str(r), rprefs) for (r, rprefs) in prefs.items()])\n\n    assert len(set(sites)) == len(sites), \"Non-unique sites in prefsfiles\"\n    assert all([all([pi >= 0 for pi in rprefs.values()]) for rprefs in\n            prefs.values()]), \"prefs < 0 in prefsfile {0}\".format(prefsfile)\n    for r in list(prefs.keys()):\n        rprefs = prefs[r]\n        assert sum(rprefs.values()) - 1 <= 0.01, (\n            \"Prefs in prefsfile {0} don't sum to one\".format(prefsfile))\n        if '*' in rprefs:\n            del rprefs['*']\n        assert aas == set(rprefs.keys()), (\"prefsfile {0} does not include \"\n                \"all amino acids at site {1}\").format(prefsfile, r)\n        rsum = float(sum(rprefs.values()))\n        prefs[r] = dict([(aa, pi / rsum) for (aa, pi) in rprefs.items()])\n    assert set(sites) == set(prefs.keys())\n\n    # Iteratively adjust until all prefs exceed minpref after re-scaling.\n    for r in list(prefs.keys()):\n        rprefs = prefs[r]\n        iterations = 0\n        while any([pi < minpref for pi in rprefs.values()]):\n            rprefs = dict([(aa, max(1.1 * minpref,\n                    pi)) for (aa, pi) in rprefs.items()])\n            newsum = float(sum(rprefs.values()))\n            rprefs = dict([(aa, pi / newsum) for (aa, pi) in rprefs.items()])\n            iterations += 1\n            assert iterations <= 3, \"minpref adjustment not converging.\"\n        prefs[r] = rprefs\n\n    if randprefs:\n        assert not avgprefs, \"randprefs and avgprefs are incompatible\"\n        random.seed(seed)\n        sites = sorted([r for r in prefs.keys()])\n        prefs = [prefs[r] for r in sites]\n        random.shuffle(sites)\n        prefs = dict(zip(sites, prefs))\n    elif avgprefs:\n        avg_prefs = dict([(aa, 0.0) for aa in aas])\n        for rprefs in prefs.values():\n            for aa in aas:\n                avg_prefs[aa] += rprefs[aa]\n        for aa in aas:\n            avg_prefs[aa] /= float(len(prefs))\n        for r in list(prefs.keys()):\n            prefs[r] = avg_prefs\n\n    return prefs", "response": "Read preferences from file with some error checking."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readPrefs_dms_tools_format(f):\n    charmatch = re.compile('^PI_([A-z\\*\\-]+)$')\n    if isinstance(f, str):\n        f = open(f)\n        lines = f.readlines()\n        f.close()\n    else:\n        lines = f.readlines()\n    characters = []\n    sites = []\n    wts = {}\n    pi_means = {}\n    pi_95credint = {}\n    h = {}\n    for line in lines:\n        if line.isspace():\n            continue\n        elif line[0] == '#' and not characters:\n            entries = line[1 : ].strip().split()\n            if len(entries) < 4:\n                raise ValueError(\"Insufficient entries in header:\\n%s\" % line)\n            if not (entries[0] in ['POSITION', 'SITE'] and entries[1][ : 2] == 'WT' and entries[2] == 'SITE_ENTROPY'):\n                raise ValueError(\"Not the correct first three header columns:\\n%s\" % line)\n            i = 3\n            while i < len(entries) and charmatch.search(entries[i]):\n                characters.append(charmatch.search(entries[i]).group(1))\n                i += 1\n            if i  == len(entries):\n                pi_95credint = None\n                linelength = len(characters) + 3\n            else:\n                if not len(entries) - i == len(characters):\n                    raise ValueError(\"Header line does not have valid credible interval format:\\n%s\" % line)\n                if not all([entries[i + j] == 'PI_%s_95' % characters[j] for j in range(len(characters))]):\n                    raise ValueError(\"mean and credible interval character mismatch in header:\\n%s\" % line)\n                linelength = 2 * len(characters) + 3\n        elif line[0] == '#':\n            continue\n        elif not characters:\n            raise ValueError(\"Found data lines before encountering a valid header\")\n        else:\n            entries = line.strip().split()\n            if len(entries) != linelength:\n                raise ValueError(\"Line does not have expected %d entries:\\n%s\" % (linelength, line))\n            r = entries[0]\n            assert r not in sites, \"Duplicate site of %s\" % r\n            sites.append(r)\n            wts[r] = entries[1]\n            assert entries[1] in characters or entries[1] == '?', \"Character %s is not one of the valid ones in header. Valid possibilities: %s\" % (entries[1], ', '.join(characters))\n            h[r] = float(entries[2])\n            pi_means[r] = dict([(x, float(entries[3 + i])) for (i, x) in enumerate(characters)])\n            if pi_95credint != None:\n                pi_95credint[r] = dict([(x, (float(entries[3 + len(characters) + i].split(',')[0]), float(entries[3 + len(characters) + i].split(',')[1]))) for (i, x) in enumerate(characters)])\n    return (sites, wts, pi_means, pi_95credint, h)", "response": "Reads the amino - acid preferences written by dms_tools v1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readDivPressure(fileName):\n    try:\n        df = pandas.read_csv(fileName, sep=None, engine='python')\n        pandasformat = True\n    except ValueError:\n        pandasformat = False\n    df.columns = ['site', 'divPressureValue']\n    scaleFactor = max(df[\"divPressureValue\"].abs())\n    if scaleFactor > 0:\n        df[\"divPressureValue\"] = [x / scaleFactor for x in df[\"divPressureValue\"]]\n    assert len(df['site'].tolist()) == len(set(df['site'].tolist())),\"There is at least one non-unique site in {0}\".format(fileName)\n    assert max(df[\"divPressureValue\"].abs()) <= 1, \"The scaling produced a diversifying pressure value with an absolute value greater than one.\"\n    sites = df['site'].tolist()\n    divPressure = {}\n    for r in sites:\n        divPressure[r] = df[df['site'] == r][\"divPressureValue\"].tolist()[0]\n    return divPressure", "response": "Reads in diversifying pressure values from some file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_configuration(conf_path):\n    with open(conf_path) as f:\n        conf_dict = yaml.load(f)\n    validate_config(conf_dict)\n    return conf_dict", "response": "Load and validate test configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating configuration. :param conf_dict: test configuration. :type conf_dict: {} :raise InvalidConfigurationError:", "response": "def validate_config(conf_dict):\n    \"\"\"Validate configuration.\n\n    :param conf_dict: test configuration.\n    :type conf_dict: {}\n    :raise InvalidConfigurationError:\n    \"\"\"\n    # TASK improve validation\n    if APPLICATIONS not in conf_dict.keys():\n        raise InvalidConfigurationError('Missing application configuration.')\n    if SEED_FILES not in conf_dict.keys():\n        raise InvalidConfigurationError('Missing seed file configuration.')\n    if RUNS not in conf_dict.keys():\n        conf_dict[RUNS] = DEFAULT_RUNS\n    if PROCESSES not in conf_dict.keys():\n        conf_dict[PROCESSES] = DEFAULT_PROCESSES\n    if PROCESSORS not in conf_dict.keys():\n        conf_dict[PROCESSORS] = DEFAULT_PROCESSORS\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads configuration and execute test runs.", "response": "def main():\n    \"\"\"Read configuration and execute test runs.\"\"\"\n    parser = argparse.ArgumentParser(description='Stress test applications.')\n    parser.add_argument('config_path', help='Path to configuration file.')\n    args = parser.parse_args()\n    try:\n        configuration = load_configuration(args.config_path)\n    except InvalidConfigurationError:\n        print(\"\\nConfiguration is not valid.\")\n        print('Example:\\n{}'.format(help_configuration))\n        return 1\n    print(\"Starting up ...\")\n    futures = []\n    with ProcessPoolExecutor(configuration[PROCESSORS]) as executor:\n        for _ in range(configuration[PROCESSES]):\n            futures.append(executor.submit(execute_test, configuration))\n    print(\"... finished\")\n    test_stats = combine_test_stats([f.result() for f in futures])\n    show_test_stats(test_stats)\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ctox(arguments, toxinidir):\n    if arguments is None:\n        arguments = []\n    if toxinidir is None:\n        toxinidir = os.getcwd()\n\n    args, options = parse_args(arguments)\n\n    if args.version:\n        print(version)\n        return 0\n\n    # if no conda trigger OSError\n    try:\n        with open(os.devnull, \"w\") as fnull:\n            check_output(['conda', '--version'], stderr=fnull)\n    except OSError:\n        cprint(\"conda not found, you need to install it to use ctox.\\n\"\n               \"The recommended way is to download miniconda,\\n\"\n               \"Do not install conda via pip.\", 'err')\n        return 1\n\n    toxinifile = os.path.join(toxinidir, \"tox.ini\")\n\n    from ctox.config import read_config, get_envlist\n    config = read_config(toxinifile)\n    if args.e == 'ALL':\n        envlist = get_envlist(config)\n    else:\n        envlist = args.e.split(',')\n\n    # TODO configure with option\n    toxdir = os.path.join(toxinidir, \".tox\")\n\n    # create a zip file for the project\n    from ctox.pkg import make_dist, package_name\n    cprint(\"GLOB sdist-make: %s\" % os.path.join(toxinidir, \"setup.py\"))\n    package = package_name(toxinidir)\n    if not make_dist(toxinidir, toxdir, package):\n        cprint(\"    setup.py sdist failed\", 'err')\n        return 1\n\n    # setup each environment and run ctox\n    failing = {}\n    for env_name in envlist:\n        env = Env(name=env_name, config=config, options=options,\n                  toxdir=toxdir, toxinidir=toxinidir, package=package)\n        failing[env_name] = env.ctox()\n\n    # print summary of the outcomes of ctox for each environment\n    cprint('Summary')\n    print(\"-\" * 23)\n    for env_name in envlist:\n        n = failing[env_name]\n        outcome = ('succeeded', 'failed', 'skipped')[n]\n        status = ('ok', 'err', 'warn')[n]\n        cprint(\"%s commands %s\" % (env_name, outcome), status)\n\n    return any(1 == v for v in failing.values())", "response": "Sets up conda environments and runs each environment based on the project s tox. ini configuration file. Returns 1 if either the build or running the commands failed or 0 if either the build or running the commands failed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef positional_args(arguments):\n    # TODO this behaviour probably isn't quite right.\n    if arguments and arguments[0] == '--':\n        for a in arguments[1:]:\n            yield a\n    else:\n        for a in arguments:\n            if a.startswith('-'):\n                break\n            yield a", "response": "Generator for position arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _main():\n    \"ctox: tox with conda\"\n    from sys import argv\n    arguments = argv[1:]\n\n    toxinidir = os.getcwd()\n\n    return main(arguments, toxinidir)", "response": "ctox : tox with conda"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread tokens strings into is_flag and value tuples.", "response": "def parse_tokens(tokens):\n    '''Read tokens strings into (is_flag, value) tuples:\n\n    For this value of `tokens`:\n\n        ['-f', 'pets.txt', '-v', 'cut', '-cz', '--lost', '--delete=sam', '--', 'lester', 'jack']\n\n    `flatten(tokens)` yields an iterable:\n\n        [\n            (True,  'f'),\n            (False, 'pets.txt'),\n            (True,  'v'),\n            (False, 'cut'),\n            (True,  'c'),\n            (True,  'z'),\n            (True,  'lost'),\n            (True,  'delete'),\n            (False, 'sam'),\n            (False, 'lester'),\n            (False, 'jack'),\n        ]\n\n    Todo:\n\n        ensure that 'verbose' in '--verbose -- a b c' is treated as a boolean even if not marked as one.\n    '''\n    # one pass max\n    tokens = iter(tokens)\n    for token in tokens:\n        if token == '--':\n            # bleed out tokens without breaking, since tokens is an iterator\n            for token in tokens:\n                yield False, token\n        elif token.startswith('-'):\n            # this handles both --last=man.txt and -czf=file.tgz\n            # str.partition produces a 3-tuple whether or not the separator is found\n            token, sep, value = token.partition('=')\n            for flag in split_flag_token(token):\n                yield True, flag\n\n            if sep:\n                # we don't re-flatten the 'value' from '--token=value'\n                yield False, value\n        else:\n            yield False, token"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_blankspace(self, char):\n        if len(char) > 1:\n            raise TypeError(\"Expected a char.\")\n        if char in self.blankspaces:\n            return True\n        return False", "response": "Test if a character is a blankspace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_separator(self, char):\n        if len(char) > 1:\n            raise TypeError(\"Expected a char.\")\n        if char in self.separators:\n            return True\n        return False", "response": "Test if a character is a separator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solve_chol(A,B):\n    # X = linalg.solve(A,linalg.solve(A.transpose(),B))\n    # much faster version\n    X = linalg.cho_solve((A, True), B)\n    return X", "response": "Solve cholesky decomposition::\n\n        return A\\(A'\\B)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jitChol(A, maxTries=10, warning=True):\n\n    \"\"\"Do a Cholesky decomposition with jitter.\n\n    Description:\n\n\n    U, jitter = jitChol(A, maxTries, warning) attempts a Cholesky\n     decomposition on the given matrix, if matrix isn't positive\n     definite the function adds 'jitter' and tries again. Thereafter\n     the amount of jitter is multiplied by 10 each time it is added\n     again. This is continued for a maximum of 10 times.  The amount of\n     jitter added is returned.\n     Returns:\n      U - the Cholesky decomposition for the matrix.\n      jitter - the amount of jitter that was added to the matrix.\n     Arguments:\n      A - the matrix for which the Cholesky decomposition is required.\n      maxTries - the maximum number of times that jitter is added before\n       giving up (default 10).\n      warning - whether to give a warning for adding jitter (default is True)\n\n    See also\n    CHOL, PDINV, LOGDET\n\n\n    Copyright (c) 2005, 2006 Neil D. Lawrence\n\n    \"\"\"\n    jitter = 0\n    i = 0\n\n    while(True):\n        try:\n            # Try --- need to check A is positive definite\n            if jitter == 0:\n                jitter = abs(SP.trace(A))/A.shape[0]*1e-6\n                LC = linalg.cholesky(A, lower=True)\n                return LC.T, 0.0\n            else:\n                if warning:\n                    # pdb.set_trace()\n\t\t    # plt.figure()\n\t\t    # plt.imshow(A, interpolation=\"nearest\")\n\t\t    # plt.colorbar()\n\t\t    # plt.show()\n                    logging.error(\"Adding jitter of %f in jitChol().\" % jitter)\n                LC = linalg.cholesky(A+jitter*SP.eye(A.shape[0]), lower=True)\n\n                return LC.T, jitter\n        except linalg.LinAlgError:\n            # Seems to have been non-positive definite.\n            if i<maxTries:\n                jitter = jitter*10\n            else:\n                raise linalg.LinAlgError(\"Matrix non positive definite, jitter of \" +  str(jitter) + \" added but failed after \" + str(i) + \" trials.\")\n        i += 1\n    return LC", "response": "Do a Cholesky decomposition with jitter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jitEigh(A,maxTries=10,warning=True):\n    warning = True\n    jitter = 0\n    i = 0\n\n    while(True):\n        if jitter == 0:\n            jitter = abs(SP.trace(A))/A.shape[0]*1e-6\n            S,U = linalg.eigh(A)\n\n        else:\n            if warning:\n                # pdb.set_trace()\n\t\t# plt.figure()\n\t\t# plt.imshow(A, interpolation=\"nearest\")\n\t\t# plt.colorbar()\n\t\t# plt.show()\n                logging.error(\"Adding jitter of %f in jitEigh().\" % jitter)\n            S,U = linalg.eigh(A+jitter*SP.eye(A.shape[0]))\n\n        if S.min()>1E-10:\n            return S,U\n\n        if i<maxTries:\n            jitter = jitter*10\n        i += 1\n            \n    raise linalg.LinAlgError(\"Matrix non positive definite, jitter of \" +  str(jitter) + \" added but failed after \" + str(i) + \" trials.\")", "response": "Do a Eigenvalue Decomposition with Jitter and return the S and U matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef modelComparisonDataFrame(modelcomparisonfile, splitparams):\n    df = (pandas.read_csv(modelcomparisonfile, sep='|', skiprows=[1])\n            .select(lambda x: 'Unnamed' not in x, axis=1)\n            )\n\n    # strip whitespace\n    df.columns = df.columns.str.strip()\n    for col in df.columns:\n        if pandas.api.types.is_string_dtype(df[col]):\n            df[col] = df[col].str.strip()\n\n    paramsdict = {}\n    if splitparams:\n        for (i, paramstr) in df['ParamValues'].iteritems():\n            paramsdict[i] = dict(map(lambda tup: (tup[0], float(tup[1])),\n                    [param.strip().split('=') for param in paramstr.split(',')]))\n        params_df = pandas.DataFrame.from_dict(paramsdict, orient='index')\n        params_df = params_df[sorted(params_df.columns)]\n        df = (df.join(params_df)\n                .drop('ParamValues', axis=1)\n                )\n\n    return df", "response": "Converts a modelcomparison. md file into a pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef BenjaminiHochbergCorrection(pvals, fdr):\n    num_tests = len(pvals)\n\n    # sort by p-value\n    sorted_tests = sorted(pvals, key=lambda tup: tup[1])\n\n    # find maximum rank for which p <= (rank/num_tests)*FDR\n    max_rank = 0\n    pcutoff = None\n    for (rank, (label, p)) in enumerate(sorted_tests):\n        rank = rank + 1 # rank beginning with 1 for smallest p-value (there is no rank 0)\n        bh_threshold = fdr * float(rank) / num_tests\n        if p <= bh_threshold: \n            assert rank > max_rank\n            max_rank = rank\n            pcutoff = bh_threshold\n\n    # pcutoff to have one significant site if there are none\n    if pcutoff == None:\n        pcutoff = 1.0 / num_tests * fdr\n\n    # collect significant ranks:\n    significantlabels = []\n    for (rank, (label, p)) in enumerate(sorted_tests):\n        rank = rank + 1 # rank beginning with 1 for site with smallest p-vaalue\n        if rank <= max_rank:\n            assert p <= pcutoff\n            significantlabels.append(label)\n\n    return (pcutoff, significantlabels)", "response": "This function is used to control false discovery rate for a set of data points. This function is used to control false discovery rate for a set of data points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting from param dictionary to list", "response": "def param_dict_to_list(dict,skeys=None):\n    \"\"\"convert from param dictionary to list\"\"\"\n    #sort keys\n    RV = SP.concatenate([dict[key].flatten() for key in skeys])\n    return RV\n    pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting from param dictionary to list param_struct", "response": "def param_list_to_dict(list,param_struct,skeys):\n    \"\"\"convert from param dictionary to list\n    param_struct: structure of parameter array\n    \"\"\"\n    RV = []\n    i0= 0\n    for key in skeys:\n        val = param_struct[key]\n        shape = SP.array(val)\n        np = shape.prod()\n        i1 = i0+np\n        params = list[i0:i1].reshape(shape)\n        RV.append((key,params))\n        i0 = i1\n    return dict(RV)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkgrad(f, fprime, x, *args,**kw_args):\n    LG.debug(\"Checking gradient ...\")\n    import numpy as np\n\n    # using machine precision to choose h\n    eps = np.finfo(float).eps\n    step = np.sqrt(eps)*(x.min())\n\n    # shake things up a bit by taking random steps for each x dimension\n    h = step*np.sign(np.random.uniform(-1, 1, x.size))\n\n    f_ph = f(x+h, *args, **kw_args)\n    f_mh = f(x-h, *args, **kw_args)\n    numerical_gradient = (f_ph - f_mh)/(2*h)\n    analytical_gradient = fprime(x, *args, **kw_args)\n    ratio = (f_ph - f_mh)/(2*np.dot(h, analytical_gradient))\n\n    h = np.zeros_like(x)\n    for i in range(len(x)):\n        pdb.set_trace()\n    h[i] = step\n    f_ph = f(x+h, *args, **kw_args)\n    f_mh = f(x-h, *args, **kw_args)\n    numerical_gradient = (f_ph - f_mh)/(2*step)\n    analytical_gradient = fprime(x, *args, **kw_args)[i]\n    ratio = (f_ph - f_mh)/(2*step*analytical_gradient)\n    h[i] = 0\n    LG.debug(\"[%d] numerical: %f, analytical: %f, ratio: %f\" % (i, numerical_gradient,analytical_gradient,ratio))", "response": "A 3 - point method to calculate the analytical gradient of a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef opt_hyper(gpr,Ifilter=None,bounds=None,opts={},*args,**kw_args):\n    if 'gradcheck' in opts:\n        gradcheck = opts['gradcheck']\n    else:\n        gradcheck = False\n    if 'max_iter_opt' in opts:\n        max_iter = opts['max_iter_opt']\n    else:\n        max_iter = 5000\n    if 'pgtol' in opts:\n        pgtol = opts['pgtol']\n    else:\n        pgtol = 1e-10\n\n    params0 = gpr.getParams()\n\n    def f(x):\n        x_ = X0\n        x_[Ifilter_x] = x\n        gpr.setParams(param_list_to_dict(x_,param_struct,skeys))\n        lml = gpr.LML()\n        if SP.isnan(lml):\n            lml=1E6\n        lml_grad = gpr.LML_grad()\n        lml_grad = param_dict_to_list(lml_grad,skeys)\n        if (~SP.isfinite(lml_grad)).any():\n            idx = (~SP.isfinite(lml_grad))\n            lml_grad[idx] = 1E6\n        return lml, lml_grad[Ifilter_x]\n\n    skeys = SP.sort(list(params0.keys()))\n    param_struct = dict([(name,params0[name].shape) for name in skeys])\n\n    # mask params that should not be optimized\n    X0 = param_dict_to_list(params0,skeys)\n    if Ifilter is not None:\n        Ifilter_x = SP.array(param_dict_to_list(Ifilter,skeys),dtype=bool)\n    else:\n        Ifilter_x = SP.ones(len(X0),dtype='bool')\n\n    # add bounds if necessary\n    if bounds is not None:\n        _b = []\n        for key in skeys:\n            if key in list(bounds.keys()):\n                _b.extend(bounds[key])\n            else:\n                _b.extend([[-SP.inf,+SP.inf]]*params0[key].size)\n        bounds = SP.array(_b)\n        bounds = bounds[Ifilter_x]\n\n    LG.info('Starting optimization ...')\n    t = time.time()\n\n    x = X0.copy()[Ifilter_x]\n    RV = optimize(f,x,maxfun=int(max_iter),pgtol=pgtol,bounds=bounds,**kw_args)\n    #RVopt = optimize(f,x,messages=True,maxfun=int(max_iter),pgtol=pgtol,bounds=bounds)\n    #LG.info('%s'%OPT.tnc.RCSTRINGS[RVopt[2]])\n    #LG.info('Optimization is converged at iteration %d'%RVopt[1])\n    #LG.info('Total time: %.2fs'%(time.time()-t))\n\n    info = RV[2]\n    conv = info['warnflag']==0\n\n    if gradcheck:\n        err = OPT.check_grad(f,df,xopt)\n        LG.info(\"check_grad (post): %.2f\"%err)\n\n    return conv,info", "response": "Optimize hyperparameters for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chival(self, bonds):\n        # XXX I'm not sure how this works?\n        order = [bond.xatom(self) for bond in bonds]\n        return self._chirality(order)", "response": "compute the chiral value around an atom given a list of bonds"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes chiral ordering of surrounding atoms", "response": "def setchival(self, bondorder, rotation):\n        \"\"\"compute chiral ordering of surrounding atoms\"\"\"\n        rotation = [None, \"@\", \"@@\"][(rotation % 2)]\n        # check to see if the bonds are attached\n        if not bondorder: # use the default xatoms\n            if len(self.oatoms) < 3 and self.explicit_hcount != 1:\n                raise PinkyError(\"Need to have an explicit hydrogen when specifying \"\\\n                                  \"chirality with less than three bonds\")\n                \n\n            self._chirality = chirality.T(self.oatoms,\n                                            rotation)\n            return\n        if len(bondorder) != len(self.bonds):\n            raise AtomError(\"The order of all bonds must be specified\")\n        \n        for bond in bondorder:\n            if bond not in self.bonds:\n                raise AtomError(\"Specified bonds to assign chirality are not attatched to atom\")\n\n        order = [bond.xatom(self) for bond in bonds]\n        self._chirality = chirality.T(order, rotation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisambiguates a vertex from the atoms around a given vertex", "response": "def disambiguate(self, symclasses):\n        \"\"\"Use the connection to the atoms around a given vertex\n        as a multiplication function to disambiguate a vertex\"\"\"\n        offsets = self.offsets\n        result = symclasses[:]\n\tfor index in self.range:\n\t    try:\n\t\tval = 1\n\t\tfor offset, bondtype in offsets[index]:\n\t\t    val *= symclasses[offset] * bondtype\n\t    except OverflowError:\n                # Hmm, how often does this occur?\n\t\tval = 1L\n\t\tfor offset, bondtype in offsets[index]:\n\t\t    val *= symclasses[offset] * bondtype\n            result[index] = val\n\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rank(self):\n        # XXX FIX ME, should the lowest value be 1 or 0?\n        symclasses = self.symclasses\n        stableSort = map(None, symclasses, range(len(symclasses)))\n        stableSort.sort()\n\n        last = None\n        x = -1\n        for order, i in stableSort:\n            if order != last:\n                x += 1\n                last = order\n            symclasses[i] = x", "response": "convert a list of integers so that the lowest integer\n        is 0 the next lowest is 1..."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef breakRankTies(self, oldsym, newsym):\n        stableSort = map(None, oldsym, newsym, range(len(oldsym)))\n        stableSort.sort()\n\n        lastOld, lastNew = None, None\n        x = -1\n        for old, new, index in stableSort:\n            if old != lastOld:\n                x += 1\n                # the last old value was changed, so update both\n                lastOld = old\n                lastNew = new\n            elif new != lastNew:\n                # break the tie based on the new info (update lastNew)\n                x += 1\n                lastNew = new\n            newsym[index] = x", "response": "break Ties to form a new list with the same integer ordering from high to low"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findLowest(self, symorders):\n        _range = range(len(symorders))\n        stableSymorders = map(None, symorders, _range)\n\n        # XXX FIX ME\n        # Do I need to sort?\n        stableSymorders.sort()\n        lowest = None\n        for index in _range:\n            if stableSymorders[index][0] == lowest:\n                return stableSymorders[index-1][1]\n            lowest = stableSymorders[index][0]\n        return -1", "response": "Find the first lowest tie in a\n            symorder or - 1 if there are no ties"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findInvariant(self, symclasses):\n        get = primes.primes.get\n        disambiguate = self.disambiguate\n        breakRankTies = self.breakRankTies\n\n        while 1:\n            newSyms = map(get, symclasses)\n            newSyms = disambiguate(newSyms)\n            breakRankTies(symclasses, newSyms)\n            if symclasses == newSyms:\n                return newSyms\n            symclasses = newSyms", "response": "find the invariant of a set of symclasses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findInvariantPartitioning(self):\n        \n        symorders = self.symorders[:]\n        _range = range(len(symorders))\n        while 1:\n            pos = self.findLowest(symorders)\n            if pos == -1:\n                self.symorders = symorders\n                return\n            for i in _range:\n                symorders[i] = symorders[i] * 2 + 1\n            symorders[pos] = symorders[pos] - 1\n            \n            symorders = self.findInvariant(symorders)", "response": "Keep the initial ordering of the symmetry orders\n ArcGIS and make all values unique."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a Request object and execute the call to the API Server.", "response": "def _api_call(self, method, target, args=None):\n        \"\"\"Create a Request object and execute the call to the API Server.\n        Parameters\n            method (str)\n                HTTP request (e.g. 'POST').\n            target (str)\n                The target URL with leading slash (e.g. '/v1/products').\n            args (dict)\n                Optional dictionary of arguments to attach to the request.\n        Returns\n            (Response)\n                The server's response to an HTTP request.\n        \"\"\"\n        self.refresh_oauth_credential()\n\n        request = Request(\n            auth_session=self.session,\n            api_host=self.api_host,\n            method=method,\n            path=target,\n            args=args,\n        )\n\n        return request.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget information about the ride types offered by Lyft at a given location.", "response": "def get_ride_types(self, latitude, longitude, ride_type=None):\n        \"\"\"Get information about the Ride Types offered by Lyft at a given location.\n        Parameters\n            latitude (float)\n                The latitude component of a location.\n            longitude (float)\n                The longitude component of a location.\n            ride_type (str)\n                Optional specific ride type information only.\n        Returns\n            (Response)\n                A Response object containing available ride_type(s) information.\n        \"\"\"\n        args = OrderedDict([\n            ('lat', latitude),\n            ('lng', longitude),\n            ('ride_type', ride_type),\n        ])\n\n        return self._api_call('GET', 'v1/ridetypes', args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget pickup time estimates for products at a given location.", "response": "def get_pickup_time_estimates(self, latitude, longitude, ride_type=None):\n        \"\"\"Get pickup time estimates (ETA) for products at a given location.\n        Parameters\n            latitude (float)\n                The latitude component of a location.\n            longitude (float)\n                The longitude component of a location.\n            ride_type (str)\n                Optional specific ride type pickup estimate only.\n        Returns\n            (Response)\n                A Response containing each product's pickup time estimates.\n        \"\"\"\n        args = OrderedDict([\n            ('lat', latitude),\n            ('lng', longitude),\n            ('ride_type', ride_type),\n        ])\n\n        return self._api_call('GET', 'v1/eta', args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget cost estimates for rides at a given location.", "response": "def get_cost_estimates(\n        self,\n        start_latitude,\n        start_longitude,\n        end_latitude=None,\n        end_longitude=None,\n        ride_type=None,\n    ):\n        \"\"\"Get cost estimates (in cents) for rides at a given location.\n        Parameters\n            start_latitude (float)\n                The latitude component of a start location.\n            start_longitude (float)\n                The longitude component of a start location.\n            end_latitude (float)\n                Optional latitude component of a end location.\n                If the destination parameters are not supplied, the endpoint will\n                simply return the Prime Time pricing at the specified location.\n            end_longitude (float)\n                Optional longitude component of a end location.\n                If the destination parameters are not supplied, the endpoint will\n                simply return the Prime Time pricing at the specified location.\n             ride_type (str)\n                Optional specific ride type price estimate only.\n        Returns\n            (Response)\n                A Response object containing each product's price estimates.\n        \"\"\"\n        args = OrderedDict([\n            ('start_lat', start_latitude),\n            ('start_lng', start_longitude),\n            ('end_lat', end_latitude),\n            ('end_lng', end_longitude),\n            ('ride_type', ride_type),\n        ])\n\n        return self._api_call('GET', 'v1/cost', args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets information about the location of drivers available near a location.", "response": "def get_drivers(self, latitude, longitude):\n        \"\"\"Get information about the location of drivers available near a location.\n        A list of 5 locations for a sample of drivers for each ride type will be provided.\n        Parameters\n            latitude (float)\n                The latitude component of a location.\n            longitude (float)\n                The longitude component of a location.\n        Returns\n            (Response)\n                A Response object containing available drivers information\n                near the specified location.\n        \"\"\"\n        args = OrderedDict([\n            ('lat', latitude),\n            ('lng', longitude),\n        ])\n\n        return self._api_call('GET', 'v1/drivers', args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting a ride on behalf of an Lyft user.", "response": "def request_ride(\n        self,\n        ride_type=None,\n        start_latitude=None,\n        start_longitude=None,\n        start_address=None,\n        end_latitude=None,\n        end_longitude=None,\n        end_address=None,\n        primetime_confirmation_token=None,\n    ):\n        \"\"\"Request a ride on behalf of an Lyft user.\n        Parameters\n            ride_type (str)\n                Name of the type of ride you're requesting.\n                E.g., lyft, lyft_plus\n            start_latitude (float)\n                Latitude component of a start location.\n            start_longitude (float)\n                Longitude component of a start location.\n            start_address (str)\n                Optional pickup address.\n            end_latitude (float)\n                Optional latitude component of a end location.\n                Destination would be NULL in this case.\n            end_longitude (float)\n                Optional longitude component of a end location.\n                Destination would be NULL in this case.\n            end_address (str)\n                Optional destination address.\n            primetime_confirmation_token (str)\n                Optional string containing the Prime Time confirmation token\n                to book rides having Prime Time Pricing.\n        Returns\n            (Response)\n                A Response object containing the ride request ID and other\n                details about the requested ride..\n        \"\"\"\n        args = {\n            'ride_type': ride_type,\n            'origin': {\n                'lat': start_latitude,\n                'lng': start_longitude,\n                'address': start_address,\n            },\n            'destination': {\n                'lat': end_latitude,\n                'lng': end_longitude,\n                'address': end_address,\n            },\n            'primetime_confirmation_token': primetime_confirmation_token,\n        }\n\n        return self._api_call('POST', 'v1/rides', args=args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncancel an ongoing ride on behalf of a user.", "response": "def cancel_ride(self, ride_id, cancel_confirmation_token=None):\n        \"\"\"Cancel an ongoing ride on behalf of a user.\n        Params\n            ride_id (str)\n                The unique ID of the Ride Request.\n            cancel_confirmation_token (str)\n                Optional string containing the cancellation confirmation token.\n        Returns\n            (Response)\n                A Response object with successful status_code\n                if ride was canceled.\n        \"\"\"\n        args = {\n            \"cancel_confirmation_token\": cancel_confirmation_token\n        }\n        endpoint = 'v1/rides/{}/cancel'.format(ride_id)\n        return self._api_call('POST', endpoint, args=args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rate_tip_ride(self,\n        ride_id,\n        rating,\n        tip_amount=None,\n        tip_currency=None,\n        feedback=None\n    ):\n        \"\"\"Provide a rating, tip or feedback for the specified ride.\n        Params\n            ride_id (str)\n                The unique ID of the Ride Request.\n            rating (int)\n                An integer between 1 and 5\n            tip_amount\n                Optional integer amount greater than 0 in minor currency units e.g. 200 for $2\n            tip_currency\n                Optional 3-character currency code e.g. 'USD'\n            feedback\n                Optional feedback message\n        Returns\n            (Response)\n                A Response object with successful status_code\n                if rating was submitted.\n        \"\"\"\n        args = {\n            \"rating\": rating,\n            \"tip.amount\": tip_amount,\n            \"tip.currency\": tip_currency,\n            \"feedback\": feedback,\n        }\n\n        endpoint = 'v1/rides/{}/rating'.format(ride_id)\n        return self._api_call('PUT', endpoint, args=args)", "response": "This method allows you to change the tip or feedback of a specific ride."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_user_ride_history(self, start_time, end_time, limit=None):\n        args = {\n            'start_time': start_time,\n            'end_time': end_time,\n            'limit': limit,\n        }\n\n        return self._api_call('GET', 'v1/rides', args=args)", "response": "Get activity about the user s lifetime activity with Lyft."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refresh_oauth_credential(self):\n        credential = self.session.oauth2credential\n\n        if credential.is_stale():\n            refresh_session = refresh_access_token(credential)\n            self.session = refresh_session", "response": "Refresh session s OAuth 2. 0 credentials if they are stale."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets hyperparameters from given covariance", "response": "def setCovariance(self,cov):\n        \"\"\" set hyperparameters from given covariance \"\"\"\n        self.setParams(sp.log(sp.diagonal(cov)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setCovariance(self, cov):\n        assert cov.shape[0]==self.dim, 'Dimension mismatch.'\n        S, U = la.eigh(cov)\n        U = U[:,::-1]\n        S = S[::-1]\n        _X = U[:, :self.rank] * sp.sqrt(S[:self.rank])\n        self.X = _X", "response": "makes lowrank approximation of cov"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects data using random hard - coded policies on MountainCar.", "response": "def mountain_car_trajectories(num_traj):\n  '''Collect data using random hard-coded policies on MountainCar.\n\n  num_traj : int, number of trajectories to collect\n\n  Returns (trajectories, traces)\n  '''\n  domain = MountainCar()\n  slopes = np.random.normal(0, 0.01, size=num_traj)\n  v0s = np.random.normal(0, 0.005, size=num_traj)\n  trajectories = []\n  traces = []\n  norm = np.array((domain.MAX_POS-domain.MIN_POS,\n                   domain.MAX_VEL-domain.MIN_VEL))\n  for m,b in zip(slopes, v0s):\n    mcar_policy = lambda s: 0 if s[0]*m + s[1] + b > 0 else 2\n    start = (np.random.uniform(domain.MIN_POS,domain.MAX_POS),\n             np.random.uniform(domain.MIN_VEL,domain.MAX_VEL))\n    samples = _run_episode(mcar_policy, domain, start, max_iters=40)\n    # normalize\n    samples.state /= norm\n    samples.next_state /= norm\n    traces.append(samples)\n    if samples.reward[-1] == 0:\n      # Don't include the warp to the final state.\n      trajectories.append(samples.state[:-1])\n    else:\n      trajectories.append(samples.state)\n\n  return trajectories, traces"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef before_all(context):\n    lf = LoggerFactory(config_file='../features/resources/test_config.yaml')\n    lf.initialize()\n    ll = lf.get_instance('environment')\n    ll.info('Logger initialized: {}'.format(lf.config))\n    ll.info('Initial test context: {}'.format(context))", "response": "Setup before all tests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nescape given string according to shell rules.", "response": "def shell_escape(text, _safe=re.compile(r\"^[-._,+a-zA-Z0-9]+$\")):\n    \"\"\"Escape given string according to shell rules.\"\"\"\n    if not text or _safe.match(text):\n        return text\n\n    squote = type(text)(\"'\")\n    return squote + text.replace(squote, type(text)(r\"'\\''\")) + squote"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a matrix that can be used to visualize the pattern extracted from a vmo.", "response": "def get_pattern_mat(oracle, pattern):\n    \"\"\"Output a matrix containing patterns in rows from a vmo.\n\n    :param oracle: input vmo object\n    :param pattern: pattern extracted from oracle\n    :return: a numpy matrix that could be used to visualize the pattern extracted.\n    \"\"\"\n\n    pattern_mat = np.zeros((len(pattern), oracle.n_states-1))\n    for i,p in enumerate(pattern):\n        length = p[1]\n        for s in p[0]:\n            pattern_mat[i][s-length:s-1] = 1\n    \n    return pattern_mat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a fake upload data record for the given URL.", "response": "def fake_upload_from_url(url):\n    \"\"\" Return a 'fake' upload data record, so that upload errors\n        can be mitigated by using an original / alternative URL,\n        especially when cross-loading from the web.\n    \"\"\"\n    return parts.Bunch(\n        image=parts.Bunch(\n            animated='false', bandwidth=0, caption=None, views=0, deletehash=None, hash=None,\n            name=(url.rsplit('/', 1) + [url])[1], title=None, type='image/*', width=0, height=0, size=0,\n            datetime=int(time.time()), # XXX was fmt.iso_datetime() - in API v2 this is a UNIX timestamp\n            id='xxxxxxx', link=url, account_id=0, account_url=None, ad_type=0, ad_url='',\n            description=None, favorite=False, in_gallery=False, in_most_viral=False,\n            is_ad=False, nsfw=None, section=None, tags=[], vote=None,\n        ),\n        links=parts.Bunch(\n            delete_page=None, imgur_page=None,\n            original=url, large_thumbnail=url, small_square=url,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling uploader and cache its results.", "response": "def cache_image_data(cache_dir, cache_key, uploader, *args, **kwargs):\n    \"\"\" Call uploader and cache its results.\n    \"\"\"\n    use_cache = True\n    if \"use_cache\" in kwargs:\n        use_cache = kwargs[\"use_cache\"]\n        del kwargs[\"use_cache\"]\n\n    json_path = None\n    if cache_dir:\n        json_path = os.path.join(cache_dir, \"cached-img-%s.json\" % cache_key)\n        if use_cache and os.path.exists(json_path):\n            LOG.info(\"Fetching %r from cache...\" % (args,))\n            try:\n                with closing(open(json_path, \"r\")) as handle:\n                    img_data = json.load(handle)\n\n                return parts.Bunch([(key, parts.Bunch(val))\n                    for key, val in img_data.items() # BOGUS pylint: disable=E1103\n                ])\n            except (EnvironmentError, TypeError, ValueError) as exc:\n                LOG.warn(\"Problem reading cached data from '%s', ignoring cache... (%s)\" % (json_path, exc))\n\n    LOG.info(\"Copying %r...\" % (args,))\n    img_data = uploader(*args, **kwargs)\n\n    if json_path:\n        with closing(open(json_path, \"w\")) as handle:\n            json.dump(img_data, handle)\n\n    return img_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_image_from_url(url, cache_dir=None, use_cache=True):\n    return cache_image_data(cache_dir, hashlib.sha1(url).hexdigest(), ImgurUploader().upload, url, use_cache=use_cache)", "response": "Copy image from given URL and return upload metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _main():\n    import pprint\n    import tempfile\n\n    try:\n        image = sys.argv[1]\n    except IndexError:\n        print(\"Usage: python -m pyrobase.webservice.imgur <url>\")\n    else:\n        try:\n            pprint.pprint(copy_image_from_url(image, cache_dir=tempfile.gettempdir()))\n        except UploadError as exc:\n            print(\"Upload error. %s\" % exc)", "response": "Command line interface for testing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload(self, image, name=None):\n        assert self.client_id, \"imgur client ID is not set! Export the IMGUR_CLIENT_ID environment variable...\"\n        assert self.client_secret, \"imgur client secret is not set! Export the IMGUR_CLIENT_SECRET environment variable...\"\n\n        # Prepare image\n        try:\n            image_data = (image + '')\n        except (TypeError, ValueError):\n            assert hasattr(image, \"read\"), \"Image is neither a string nor an open file handle\"\n            image_type = \"file\"\n            image_data = image  # XXX are streams supported? need a temp file?\n            image_repr = repr(image)\n        else:\n            if image.startswith(\"http:\") or image.startswith(\"https:\"):\n                image_type = \"url\"\n                image_data = image\n                image_repr = image\n            elif all(ord(i) >= 32 for i in image) and os.path.exists(image):\n                image_type = \"file\"\n                image_data = image  # XXX open(image, \"rb\")\n                image_repr = \"file:\" + image\n            else:\n                # XXX Not supported anymore (maybe use a temp file?)\n                image_type = \"base64\"\n                image_data = image_data.encode(image_type)\n                image_repr = \"<binary data>\"\n\n        # Upload image\n        # XXX \"name\",    name or hashlib.md5(str(image)).hexdigest()),\n        client = ImgurClient(self.client_id, self.client_secret)\n        result = (client.upload_from_url if image_type == 'url'\n                  else client.upload_from_path)(image_data)  # XXX config=None, anon=True)\n\n        if result['link'].startswith('http:'):\n            result['link'] = 'https:' + result['link'][5:]\n        result['hash'] = result['id']  # compatibility to API v1\n        result['caption'] = result['description']  # compatibility to API v1\n\n        return parts.Bunch(\n            image=parts.Bunch(result),\n            links=parts.Bunch(\n                delete_page=None,\n                imgur_page=None,\n                original=result['link'],\n                large_thumbnail=\"{0}s.{1}\".format(*result['link'].rsplit('.', 1)),\n                small_square=\"{0}l.{1}\".format(*result['link'].rsplit('.', 1)),\n            ))", "response": "Upload the given image to the imgur API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a format string into a dictionary of color and line style symbols.", "response": "def _parse_fmt(fmt, color_key='colors', ls_key='linestyles',\n               marker_key='marker'):\n  '''Modified from matplotlib's _process_plot_format function.'''\n  try:  # Is fmt just a colorspec?\n    color = mcolors.colorConverter.to_rgb(fmt)\n  except ValueError:\n    pass  # No, not just a color.\n  else:\n    # Either a color or a numeric marker style\n    if fmt not in mlines.lineMarkers:\n      return {color_key:color}\n\n  result = dict()\n  # handle the multi char special cases and strip them from the string\n  if fmt.find('--') >= 0:\n    result[ls_key] = '--'\n    fmt = fmt.replace('--', '')\n  if fmt.find('-.') >= 0:\n    result[ls_key] = '-.'\n    fmt = fmt.replace('-.', '')\n  if fmt.find(' ') >= 0:\n    result[ls_key] = 'None'\n    fmt = fmt.replace(' ', '')\n\n  for c in list(fmt):\n    if c in mlines.lineStyles:\n      if ls_key in result:\n        raise ValueError('Illegal format string; two linestyle symbols')\n      result[ls_key] = c\n    elif c in mlines.lineMarkers:\n      if marker_key in result:\n        raise ValueError('Illegal format string; two marker symbols')\n      result[marker_key] = c\n    elif c in mcolors.colorConverter.colors:\n      if color_key in result:\n        raise ValueError('Illegal format string; two color symbols')\n      result[color_key] = c\n    else:\n      raise ValueError('Unrecognized character %c in format string' % c)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, coordinates, directed=False, weighted=False, fig='current',\n           ax=None, edge_style=None, vertex_style=None, title=None, cmap=None):\n    '''Plot the graph using matplotlib in 2 or 3 dimensions.\n\n    coordinates : (n,2) or (n,3) array of vertex coordinates\n    directed : if True, edges have arrows indicating direction.\n    weighted : if True, edges are colored by their weight.\n    fig : a matplotlib Figure to use, or one of {'new','current'}. Defaults to\n          'current', which will call gcf(). Only used when ax=None.\n    ax : a matplotlib Axes to use. Defaults to gca()\n    edge_style : string or dict of styles for edges. Defaults to 'k-'\n    vertex_style : string or dict of styles for vertices. Defaults to 'ko'\n    title : string to display as the plot title\n    cmap : a matplotlib Colormap to use for edge weight coloring\n    '''\n    X = np.atleast_2d(coordinates)\n    assert 0 < X.shape[1] <= 3, 'too many dimensions to plot'\n    if X.shape[1] == 1:\n      X = np.column_stack((np.arange(X.shape[0]), X))\n    is_3d = (X.shape[1] == 3)\n    if ax is None:\n      ax = _get_axis(is_3d, fig)\n    edge_kwargs = dict(colors='k', linestyles='-', linewidths=1, zorder=1)\n    vertex_kwargs = dict(marker='o', c='k', s=20, edgecolor='none', zorder=2)\n    if edge_style is not None:\n      if not isinstance(edge_style, dict):\n        edge_style = _parse_fmt(edge_style, color_key='colors')\n      edge_kwargs.update(edge_style)\n    if vertex_style is not None:\n      if not isinstance(vertex_style, dict):\n        vertex_style = _parse_fmt(vertex_style, color_key='c')\n      vertex_kwargs.update(vertex_style)\n    if weighted and self.is_weighted():\n      edge_kwargs['array'] = self.edge_weights()\n    if directed and self.is_directed():\n      _directed_edges(self, X, ax, is_3d, edge_kwargs, cmap)\n    else:\n      _undirected_edges(self, X, ax, is_3d, edge_kwargs, cmap)\n    ax.scatter(*X.T, **vertex_kwargs)\n    ax.autoscale_view()\n    if title:\n      ax.set_title(title)\n    return pyplot.show", "response": "Plot the graph using matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_html(self, html_file, directed=False, weighted=False, vertex_ids=None,\n              vertex_colors=None, vertex_labels=None, width=900, height=600,\n              title=None, svg_border='1px solid black'):\n    '''Write the graph as a d3 force-directed layout SVG to an HTML file.\n\n    html_file : str|file-like, writeable destination for the output HTML.\n    vertex_ids : unique IDs for each vertex, defaults to arange(num_vertices).\n    vertex_colors : numeric color mapping for vertices, optional.\n    vertex_labels : class labels for vertices, optional.\n    title : str, written above the SVG as an h1, optional.\n    svg_border : str, CSS for the 'border' attribute of the SVG element.\n    '''\n    if directed:\n      raise NotImplementedError('Directed graphs are NYI for HTML output.')\n    if (vertex_colors is not None) and (vertex_labels is not None):\n      raise ValueError('Supply only one of vertex_colors, vertex_labels')\n\n    # set up vertices\n    if vertex_ids is None:\n      vertex_ids = np.arange(self.num_vertices())\n    elif len(vertex_ids) != self.num_vertices():\n      raise ValueError('len(vertex_ids) != num vertices.')\n\n    if vertex_labels is not None:\n      vlabels, vcolors = np.unique(vertex_labels, return_inverse=True)\n      if len(vcolors) != len(vertex_ids):\n        raise ValueError('len(vertex_labels) != num vertices.')\n    elif vertex_colors is not None:\n      vcolors = np.array(vertex_colors, dtype=float, copy=False)\n      if len(vcolors) != len(vertex_ids):\n        raise ValueError('len(vertex_colors) != num vertices.')\n      vcolors -= vcolors.min()\n      vcolors /= vcolors.max()\n    else:\n      vcolors = []\n\n    node_json = []\n    for name, c in zip_longest(vertex_ids, vcolors):\n      if c is not None:\n        node_json.append('{\"id\": \"%s\", \"color\": %s}' % (name, c))\n      else:\n        node_json.append('{\"id\": \"%s\"}' % name)\n\n    # set up edges\n    pairs = self.pairs(directed=directed)\n    if weighted:\n      weights = self.edge_weights(directed=directed, copy=True).astype(float)\n      weights -= weights.min()\n      weights /= weights.max()\n    else:\n      weights = np.zeros(len(pairs)) + 0.5\n\n    edge_json = []\n    for (i,j), w in zip(pairs, weights):\n      edge_json.append('{\"source\": \"%s\", \"target\": \"%s\", \"weight\": %f}' % (\n          vertex_ids[i], vertex_ids[j], w))\n\n    # emit self-contained HTML\n    if not hasattr(html_file, 'write'):\n      fh = open(html_file, 'w')\n    else:\n      fh = html_file\n    print(u'<!DOCTYPE html><meta charset=\"utf-8\"><style>', file=fh)\n    print(u'svg { border: %s; }' % svg_border, file=fh)\n    if weighted:\n      print(u'.links line { stroke-width: 2px; }', file=fh)\n    else:\n      print(u'.links line { stroke: #000; stroke-width: 2px; }', file=fh)\n    print(u'.nodes circle { stroke: #fff; stroke-width: 1px; }', file=fh)\n    print(u'</style>', file=fh)\n    if title:\n      print(u'<h1>%s</h1>' % title, file=fh)\n    print(u'<svg width=\"%d\" height=\"%d\"></svg>' % (width, height), file=fh)\n    print(u'<script src=\"https://d3js.org/d3.v4.min.js\"></script>', file=fh)\n    print(u'<script>', LAYOUT_JS, sep=u'\\n', file=fh)\n    if vertex_colors is not None:\n      print(u'var vcolor=d3.scaleSequential(d3.interpolateViridis);', file=fh)\n    elif vertex_labels is not None:\n      scale = 'd3.schemeCategory%d' % (10 if len(vlabels) <= 10 else 20)\n      print(u'var vcolor = d3.scaleOrdinal(%s);' % scale, file=fh)\n    else:\n      print(u'function vcolor(){ return \"#1776b6\"; }', file=fh)\n    print(u'var sim=layout_graph({\"nodes\": [%s], \"links\": [%s]});</script>' % (\n        ',\\n'.join(node_json), ',\\n'.join(edge_json)), file=fh)\n    fh.flush()", "response": "Write the graph as a d3 force - directed layout SVG to an HTML file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the given line of sourcemap into a list of values.", "response": "def normalize_mapping_line(mapping_line, previous_source_column=0):\n    \"\"\"\n    Often times the position will remain stable, such that the naive\n    process will end up with many redundant values; this function will\n    iterate through the line and remove all extra values.\n    \"\"\"\n\n    if not mapping_line:\n        return [], previous_source_column\n\n    # Note that while the local record here is also done as a 4-tuple,\n    # element 1 and 2 are never used since they are always provided by\n    # the segments in the mapping line; they are defined for consistency\n    # reasons.\n\n    def regenerate(segment):\n        if len(segment) == 5:\n            result = (record[0], segment[1], segment[2], record[3], segment[4])\n        else:\n            result = (record[0], segment[1], segment[2], record[3])\n        # Ideally the exact location should still be kept, but given\n        # that the sourcemap format is accumulative and permits a lot\n        # of inferred positions, resetting all values to 0 is intended.\n        record[:] = [0, 0, 0, 0]\n        return result\n\n    # first element of the line; sink column (0th element) is always\n    # the absolute value, so always use the provided value sourced from\n    # the original mapping_line; the source column (3rd element) is\n    # never reset, so if a previous counter exists (which is specified\n    # by the optional argument), make use of it to generate the initial\n    # normalized segment.\n    record = [0, 0, 0, previous_source_column]\n    result = []\n    regen_next = True\n\n    for segment in mapping_line:\n        if not segment:\n            # ignore empty records\n            continue\n        # if the line has not changed, and that the increases of both\n        # columns are the same, accumulate the column counter and drop\n        # the segment.\n\n        # accumulate the current record first\n        record[0] += segment[0]\n        if len(segment) == 1:\n            # Mark the termination, as 1-tuple determines the end of the\n            # previous symbol and denote that whatever follows are not\n            # in any previous source files.  So if it isn't recorded,\n            # make note of this if it wasn't done already.\n            if result and len(result[-1]) != 1:\n                result.append((record[0],))\n                record[0] = 0\n                # the next complete segment will require regeneration\n                regen_next = True\n            # skip the remaining processing.\n            continue\n\n        record[3] += segment[3]\n\n        # 5-tuples are always special case with the remapped identifier\n        # name element, and to mark the termination the next token must\n        # also be explicitly written (in our case, regenerated).  If the\n        # filename or source line relative position changed (idx 1 and\n        # 2), regenerate it too.  Finally, if the column offsets differ\n        # between source and sink, regenerate.\n        if len(segment) == 5 or regen_next or segment[1] or segment[2] or (\n                record[0] != record[3]):\n            result.append(regenerate(segment))\n            regen_next = len(segment) == 5\n\n    # must return the consumed/omitted values.\n    return result, record[3]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an iterable of stream fragments, write it to the stream object by using its write method. Returns a 3-tuple, where the first element is the mapping, second element is the list of sources and the third being the original names referenced by the given fragment. Arguments: stream_fragments an iterable that only contains StreamFragments stream an io.IOBase compatible stream object normalize the default True setting will result in the mappings that were returned be normalized to the minimum form. This will reduce the size of the generated source map at the expense of slightly lower quality. Also, if any of the subsequent arguments are provided (for instance, for the multiple calls to this function), the usage of the normalize flag is currently NOT supported. If multiple sets of outputs are to be produced, the recommended method is to chain all the stream fragments together before passing in. Advanced usage arguments book A Book instance; if none is provided an instance will be created from the default_book constructor. The Bookkeeper instance is used for tracking the positions of rows and columns of the input stream. sources a Names instance for tracking sources; if None is provided, an instance will be created for internal use. names a Names instance for tracking names; if None is provided, an instance will be created for internal use. mappings a previously produced mappings. A stream fragment tuple must contain the following - The string to write to the stream - Original starting line of the string; None if not present - Original starting column fo the line; None if not present - Original string that this fragment represents (i.e. for the case where this string fragment was an identifier but got mangled into an alternative form); use None if this was not the case. - The source of the fragment. If the first fragment is unspecified, the INVALID_SOURCE url will be used (i.e. about:invalid). After that, a None value will be treated as the implicit value, and if NotImplemented is encountered, the INVALID_SOURCE url will be used also. If a number of stream_fragments are to be provided, common instances of Book (constructed via default_book) and Names (for sources and names) should be provided if they are not chained together.", "response": "def write(\n        stream_fragments, stream, normalize=True,\n        book=None, sources=None, names=None, mappings=None):\n    \"\"\"\n    Given an iterable of stream fragments, write it to the stream object\n    by using its write method.  Returns a 3-tuple, where the first\n    element is the mapping, second element is the list of sources and\n    the third being the original names referenced by the given fragment.\n\n    Arguments:\n\n    stream_fragments\n        an iterable that only contains StreamFragments\n    stream\n        an io.IOBase compatible stream object\n    normalize\n        the default True setting will result in the mappings that were\n        returned be normalized to the minimum form.  This will reduce\n        the size of the generated source map at the expense of slightly\n        lower quality.\n\n        Also, if any of the subsequent arguments are provided (for\n        instance, for the multiple calls to this function), the usage of\n        the normalize flag is currently NOT supported.\n\n        If multiple sets of outputs are to be produced, the recommended\n        method is to chain all the stream fragments together before\n        passing in.\n\n    Advanced usage arguments\n\n    book\n        A Book instance; if none is provided an instance will be created\n        from the default_book constructor.  The Bookkeeper instance is\n        used for tracking the positions of rows and columns of the input\n        stream.\n    sources\n        a Names instance for tracking sources; if None is provided, an\n        instance will be created for internal use.\n    names\n        a Names instance for tracking names; if None is provided, an\n        instance will be created for internal use.\n    mappings\n        a previously produced mappings.\n\n    A stream fragment tuple must contain the following\n\n    - The string to write to the stream\n    - Original starting line of the string; None if not present\n    - Original starting column fo the line; None if not present\n    - Original string that this fragment represents (i.e. for the case\n      where this string fragment was an identifier but got mangled into\n      an alternative form); use None if this was not the case.\n    - The source of the fragment.  If the first fragment is unspecified,\n      the INVALID_SOURCE url will be used (i.e. about:invalid).  After\n      that, a None value will be treated as the implicit value, and if\n      NotImplemented is encountered, the INVALID_SOURCE url will be used\n      also.\n\n    If a number of stream_fragments are to be provided, common instances\n    of Book (constructed via default_book) and Names (for sources and\n    names) should be provided if they are not chained together.\n    \"\"\"\n\n    def push_line():\n        mappings.append([])\n        book.keeper._sink_column = 0\n\n    if names is None:\n        names = Names()\n\n    if sources is None:\n        sources = Names()\n\n    if book is None:\n        book = default_book()\n\n    if not isinstance(mappings, list):\n        # note that\n        mappings = []\n        # finalize initial states; the most recent list (mappings[-1])\n        # is the current line\n        push_line()\n\n    for chunk, lineno, colno, original_name, source in stream_fragments:\n        # note that lineno/colno are assumed to be both provided or none\n        # provided.\n        lines = chunk.splitlines(True)\n        for line in lines:\n            stream.write(line)\n\n            # Two separate checks are done.  As per specification, if\n            # either lineno or colno are unspecified, it is assumed that\n            # the segment is unmapped - append a termination (1-tuple)\n            #\n            # Otherwise, note that if this segment is the beginning of a\n            # line, and that an implied source colno/linecol were\n            # provided (i.e. value of 0), and that the string is empty,\n            # it can be safely skipped, since it is an implied and\n            # unmapped indentation\n\n            if lineno is None or colno is None:\n                mappings[-1].append((book.keeper.sink_column,))\n            else:\n                name_id = names.update(original_name)\n                # this is a bit of a trick: an unspecified value (None)\n                # will simply be treated as the implied value, hence 0.\n                # However, a NotImplemented will be recorded and be\n                # convereted to the invalid url at the end.\n                source_id = sources.update(source) or 0\n\n                if lineno:\n                    # a new lineno is provided, apply it to the book and\n                    # use the result as the written value.\n                    book.keeper.source_line = lineno\n                    source_line = book.keeper.source_line\n                else:\n                    # no change in offset, do not calculate and assume\n                    # the value to be written is unchanged.\n                    source_line = 0\n\n                # if the provided colno is to be inferred, calculate it\n                # based on the previous line length plus the previous\n                # real source column value, otherwise standard value\n                # for tracking.\n\n                # the reason for using the previous lengths is simply\n                # due to how the bookkeeper class does the calculation\n                # on-demand, and that the starting column for the\n                # _current_ text fragment can only be calculated using\n                # what was written previously, hence the original length\n                # value being added if the current colno is to be\n                # inferred.\n                if colno:\n                    book.keeper.source_column = colno\n                else:\n                    book.keeper.source_column = (\n                        book.keeper._source_column + book.original_len)\n\n                if original_name is not None:\n                    mappings[-1].append((\n                        book.keeper.sink_column, source_id,\n                        source_line, book.keeper.source_column,\n                        name_id\n                    ))\n                else:\n                    mappings[-1].append((\n                        book.keeper.sink_column, source_id,\n                        source_line, book.keeper.source_column\n                    ))\n\n            # doing this last to update the position for the next line\n            # or chunk for the relative values based on what was added\n            if line[-1:] in '\\r\\n':\n                # Note: this HAS to be an edge case and should never\n                # happen, but this has the potential to muck things up.\n                # Since the parent only provided the start, will need\n                # to manually track the chunks internal to here.\n                # This normally shouldn't happen with sane parsers\n                # and lexers, but this assumes that no further symbols\n                # aside from the new lines got inserted.\n                colno = (\n                    colno if colno in (0, None) else\n                    colno + len(line.rstrip()))\n                book.original_len = book.written_len = 0\n                push_line()\n\n                if line is not lines[-1]:\n                    logger.warning(\n                        'text in the generated document at line %d may be '\n                        'mapped incorrectly due to trailing newline character '\n                        'in provided text fragment.', len(mappings)\n                    )\n                    logger.info(\n                        'text in stream fragments should not have trailing '\n                        'characters after a new line, they should be split '\n                        'off into a separate fragment.'\n                    )\n            else:\n                book.written_len = len(line)\n                book.original_len = (\n                    len(original_name) if original_name else book.written_len)\n                book.keeper.sink_column = (\n                    book.keeper._sink_column + book.written_len)\n\n    # normalize everything\n    if normalize:\n        # if this _ever_ supports the multiple usage using existence\n        # instances of names and book and mappings, it needs to deal\n        # with NOT normalizing the existing mappings and somehow reuse\n        # the previously stored value, probably in the book.  It is\n        # most certainly a bad idea to support that use case while also\n        # supporting the default normalize flag due to the complex\n        # tracking of all the existing values...\n        mappings = normalize_mappings(mappings)\n\n    list_sources = [\n        INVALID_SOURCE if s == NotImplemented else s for s in sources\n    ] or [INVALID_SOURCE]\n    return mappings, list_sources, list(names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_sourcemap(filename, mappings, sources, names=[]):\n\n    return {\n        \"version\": 3,\n        \"sources\": sources,\n        \"names\": names,\n        \"mappings\": encode_mappings(mappings),\n        \"file\": filename,\n    }", "response": "Encode a sourcemap file into JSON."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_sourcemap(\n        mappings, sources, names, output_stream, sourcemap_stream,\n        normalize_paths=True, source_mapping_url=NotImplemented):\n    \"\"\"\n    Write out the mappings, sources and names (generally produced by\n    the write function) to the provided sourcemap_stream, and write the\n    sourceMappingURL to the output_stream.\n\n    Arguments\n\n    mappings, sources, names\n        These should be values produced by write function from this\n        module.\n    output_stream\n        The original stream object that was written to; its name will\n        be used for the file target and if sourceMappingURL is resolved,\n        it will be writtened to this stream also as a comment.\n    sourcemap_stream\n        If one is provided, the sourcemap will be written out to it.\n\n        If it is the same stream as the output_stream, the source map\n        will be written as an encoded 'data:application/json;base64'\n        url to the sourceMappingURL comment.  Note that an appropriate\n        encoding must be available as an attribute by the output_stream\n        object so that the correct character set will be used for the\n        base64 encoded JSON serialized string.\n    normalize_paths\n        If set to True, absolute paths found will be turned into\n        relative paths with relation from the stream being written\n        to, and the path separator used will become a '/' (forward\n        slash).\n    source_mapping_url\n        If an explicit value is set, this will be written as the\n        sourceMappingURL into the output_stream.  Note that the path\n        normalization will NOT use this value, so if paths have been\n        manually provided, ensure that normalize_paths is set to False\n        if the behavior is unwanted.\n    \"\"\"\n\n    encode_sourcemap_args, output_js_map = verify_write_sourcemap_args(\n        mappings, sources, names, output_stream, sourcemap_stream,\n        normalize_paths\n    )\n\n    encoded_sourcemap = json.dumps(\n        encode_sourcemap(*encode_sourcemap_args),\n        sort_keys=True, ensure_ascii=False,\n    )\n\n    if sourcemap_stream is output_stream:\n        # encoding will be missing if using StringIO; fall back to\n        # default_encoding\n        encoding = getattr(output_stream, 'encoding', None) or default_encoding\n        output_stream.writelines([\n            '\\n//# sourceMappingURL=data:application/json;base64;charset=',\n            encoding, ',', base64.b64encode(\n                encoded_sourcemap.encode(encoding)).decode('ascii'),\n        ])\n    else:\n        if source_mapping_url is not None:\n            output_stream.writelines(['\\n//# sourceMappingURL=', (\n                output_js_map if source_mapping_url is NotImplemented\n                else source_mapping_url\n            ), '\\n'])\n\n        sourcemap_stream.write(encoded_sourcemap)", "response": "Writes out the sourcemap for the given base64 encoded sourcemap."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the current value of the source map name field.", "response": "def update(self, name):\n        \"\"\"\n        Query a name for the relative index value to be added into the\n        source map name field (optional 5th element).\n        \"\"\"\n\n        if name is None:\n            return\n\n        if name not in self._names:\n            # add the name if it isn't already tracked\n            self._names[name] = len(self._names)\n\n        result = self._names[name] - self._current\n        self._current = self._names[name]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string representation of a sequence of unicode strings.", "response": "def repr_compat(s):\n    \"\"\"\n    Since Python 2 is annoying with unicode literals, and that we are\n    enforcing the usage of unicode, this ensures the repr doesn't spew\n    out the unicode literal prefix.\n    \"\"\"\n\n    if unicode and isinstance(s, unicode):\n        return repr(s)[1:]\n    else:\n        return repr(s)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_tab_names(name):\n\n    package_name, module_name = name.rsplit('.', 1)\n\n    version = ply_dist.version.replace(\n        '.', '_') if ply_dist is not None else 'unknown'\n    data = (package_name, module_name, py_major, version)\n    lextab = '%s.lextab_%s_py%d_ply%s' % data\n    yacctab = '%s.yacctab_%s_py%d_ply%s' % data\n    return lextab, yacctab", "response": "Return the names to lextab and yacctab modules for the given module\n    name. Typical usage should be like so ::\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normrelpath(base, target):\n\n    if not all(map(isabs, [base, target])):\n        return target\n\n    return relpath(normpath(target), dirname(normpath(base)))", "response": "This function takes the base and target arguments as paths and returns an equivalent relative path from base to target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef permute_graph(G, order):\n  '''Reorder the graph's vertices, returning a copy of the input graph.\n  order : integer array-like, some permutation of range(G.num_vertices()).\n  '''\n  adj = G.matrix('dense')\n  adj = adj[np.ix_(order, order)]\n  return Graph.from_adj_matrix(adj)", "response": "Reorder the graph s vertices returning a copy of the input graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef laplacian_reordering(G):\n  '''Reorder vertices using the eigenvector of the graph Laplacian corresponding\n  to the first positive eigenvalue.'''\n  L = G.laplacian()\n  vals, vecs = np.linalg.eigh(L)\n  min_positive_idx = np.argmax(vals == vals[vals>0].min())\n  vec = vecs[:, min_positive_idx]\n  return permute_graph(G, np.argsort(vec))", "response": "Reorder vertices using the eigenvector of the graph Laplacian corresponding\n to the first positive eigenvalue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the XSD for a column to a TreeBuilder.", "response": "def add_column_xsd(self, tb, column, attrs):\n        \"\"\" Add the XSD for a column to tb (a TreeBuilder) \"\"\"\n        if column.nullable:\n            attrs['minOccurs'] = str(0)\n            attrs['nillable'] = 'true'\n        for cls, xsd_type in six.iteritems(self.SIMPLE_XSD_TYPES):\n            if isinstance(column.type, cls):\n                attrs['type'] = xsd_type\n                with tag(tb, 'xsd:element', attrs) as tb:\n                    self.element_callback(tb, column)\n                    return tb\n        if isinstance(column.type, Geometry):\n            geometry_type = column.type.geometry_type\n            xsd_type = self.SIMPLE_GEOMETRY_XSD_TYPES[geometry_type]\n            attrs['type'] = xsd_type\n            with tag(tb, 'xsd:element', attrs) as tb:\n                self.element_callback(tb, column)\n                return tb\n        if isinstance(column.type, sqlalchemy.Enum):\n            with tag(tb, 'xsd:element', attrs) as tb:\n                with tag(tb, 'xsd:simpleType') as tb:\n                    with tag(tb, 'xsd:restriction', {'base': 'xsd:string'}) \\\n                            as tb:\n                        for enum in column.type.enums:\n                            with tag(tb, 'xsd:enumeration', {'value': enum}):\n                                pass\n                self.element_callback(tb, column)\n                return tb\n        if isinstance(column.type, sqlalchemy.Numeric):\n            if column.type.scale is None and column.type.precision is None:\n                attrs['type'] = 'xsd:decimal'\n                with tag(tb, 'xsd:element', attrs) as tb:\n                    self.element_callback(tb, column)\n                    return tb\n            else:\n                with tag(tb, 'xsd:element', attrs) as tb:\n                    with tag(tb, 'xsd:simpleType') as tb:\n                        with tag(tb, 'xsd:restriction',\n                                 {'base': 'xsd:decimal'}) as tb:\n                            if column.type.scale is not None:\n                                with tag(tb, 'xsd:fractionDigits',\n                                         {'value': str(column.type.scale)}) \\\n                                        as tb:\n                                    pass\n                            if column.type.precision is not None:\n                                precision = column.type.precision\n                                with tag(tb, 'xsd:totalDigits',\n                                         {'value': str(precision)}) \\\n                                        as tb:\n                                    pass\n                    self.element_callback(tb, column)\n                    return tb\n        if isinstance(column.type, sqlalchemy.String) \\\n                or isinstance(column.type, sqlalchemy.Text) \\\n                or isinstance(column.type, sqlalchemy.Unicode) \\\n                or isinstance(column.type, sqlalchemy.UnicodeText):\n            if column.type.length is None:\n                attrs['type'] = 'xsd:string'\n                with tag(tb, 'xsd:element', attrs) as tb:\n                    self.element_callback(tb, column)\n                    return tb\n            else:\n                with tag(tb, 'xsd:element', attrs) as tb:\n                    with tag(tb, 'xsd:simpleType') as tb:\n                        with tag(tb, 'xsd:restriction',\n                                 {'base': 'xsd:string'}) as tb:\n                            with tag(tb, 'xsd:maxLength',\n                                     {'value': str(column.type.length)}):\n                                pass\n                    self.element_callback(tb, column)\n                    return tb\n        raise UnsupportedColumnTypeError(column.type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the XSD for a column property to theTreeBuilder.", "response": "def add_column_property_xsd(self, tb, column_property):\n        \"\"\" Add the XSD for a column property to the ``TreeBuilder``. \"\"\"\n        if len(column_property.columns) != 1:\n            raise NotImplementedError  # pragma: no cover\n        column = column_property.columns[0]\n        if column.primary_key and not self.include_primary_keys:\n            return\n        if column.foreign_keys and not self.include_foreign_keys:\n            if len(column.foreign_keys) != 1:  # pragma: no cover\n                # FIXME understand when a column can have multiple\n                # foreign keys\n                raise NotImplementedError()\n            return\n        attrs = {'name': column_property.key}\n        self.add_column_xsd(tb, column, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_class_properties_xsd(self, tb, cls):\n        for p in class_mapper(cls).iterate_properties:\n            if isinstance(p, ColumnProperty):\n                self.add_column_property_xsd(tb, p)\n        if self.sequence_callback:\n            self.sequence_callback(tb, cls)", "response": "Add the XSD for the class properties to theTreeBuilder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the XSD for a mapped class.", "response": "def get_class_xsd(self, io, cls):\n        \"\"\" Returns the XSD for a mapped class. \"\"\"\n        attrs = {}\n        attrs['xmlns:gml'] = 'http://www.opengis.net/gml'\n        attrs['xmlns:xsd'] = 'http://www.w3.org/2001/XMLSchema'\n        tb = TreeBuilder()\n        with tag(tb, 'xsd:schema', attrs) as tb:\n            with tag(tb, 'xsd:complexType', {'name': cls.__name__}) as tb:\n                with tag(tb, 'xsd:complexContent') as tb:\n                    with tag(tb, 'xsd:extension',\n                             {'base': 'gml:AbstractFeatureType'}) as tb:\n                        with tag(tb, 'xsd:sequence') as tb:\n                            self.add_class_properties_xsd(tb, cls)\n\n        ElementTree(tb.close()).write(io, encoding='utf-8')\n        return io"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the database from a gzipped version of the system folder.", "response": "def load_db_from_url(url=\"https://github.com/OpenExoplanetCatalogue/oec_gzip/raw/master/systems.xml.gz\"):\n    \"\"\" Loads the database from a gzipped version of the system folder, by default the one located in the oec_gzip repo\n    in the OpenExoplanetCatalogue GitHub group.\n\n    The database is loaded from the url in memory\n\n    :param url: url to load (must be gzipped version of systems folder)\n    :return: OECDatabase objected initialised with latest OEC Version\n    \"\"\"\n\n    catalogue = gzip.GzipFile(fileobj=io.BytesIO(requests.get(url).content))\n    database = OECDatabase(catalogue, stream=True)\n\n    return database"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef searchPlanet(self, name):\n\n        searchName = compactString(name)\n        returnDict = {}\n\n        for altname, planetObj in self._planetSearchDict.iteritems():\n            if re.search(searchName, altname):\n                returnDict[planetObj.name] = planetObj\n\n        if returnDict:\n            if len(returnDict) == 1:\n                return returnDict.values()[0]\n            else:\n                return returnDict.values()\n\n        else:\n            return False", "response": "Searches the database for a planet."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of transiting planets in the system.", "response": "def transitingPlanets(self):\n        \"\"\" Returns a list of transiting planet objects\n        \"\"\"\n\n        transitingPlanets = []\n\n        for planet in self.planets:\n            try:\n                if planet.isTransiting:\n                    transitingPlanets.append(planet)\n            except KeyError:  # No 'discoverymethod' tag - this also filters Solar System planets\n                pass\n\n        return transitingPlanets"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generatePlanetSearchDict(self):\n\n        planetNameDict = {}\n        for planet in self.planets:\n\n            name = planet.name\n            altnames = planet.params['altnames']\n            altnames.append(name)  # as we also want the default name to be searchable\n\n            for altname in altnames:\n                reducedname = compactString(altname)\n                planetNameDict[reducedname] = planet\n\n        return planetNameDict", "response": "Generates a search dictionary for all planets by taking all names and flattening them to the most compact form\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _loadDatabase(self, databaseLocation, stream=False):\n\n        # Initialise Database\n        self.systems = []\n        self.binaries = []\n        self.stars = []\n        self.planets = []\n\n        if stream:\n            tree = ET.parse(databaseLocation)\n            for system in tree.findall(\".//system\"):\n                self._loadSystem(system)\n        else:\n            databaseXML = glob.glob(os.path.join(databaseLocation, '*.xml'))\n            if not len(databaseXML):\n                raise LoadDataBaseError('could not find the database xml files. Have you given the correct location '\n                                        'to the open exoplanet catalogues /systems folder?')\n\n            for filename in databaseXML:\n                try:\n                    with open(filename, 'r') as f:\n                        tree = ET.parse(f)\n                except ET.ParseError as e:  # this is sometimes raised rather than the root.tag system check\n                    raise LoadDataBaseError(e)\n\n                root = tree.getroot()\n\n                # Process the system\n                if not root.tag == 'system':\n                    raise LoadDataBaseError('file {0} does not contain a valid system - could be an error with your version'\n                                            ' of the catalogue'.format(filename))\n\n                self._loadSystem(root)", "response": "Loads the database from a given file path in the class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a unique slug.", "response": "def generate_slug(self, model_instance):\n        \"\"\"Returns a unique slug.\"\"\"\n        queryset = model_instance.__class__._default_manager.all()\n\n        # Only count slugs that match current length to prevent issues\n        # when pre-existing slugs are a different length.\n        lookup = {'%s__regex' % self.attname: r'^.{%s}$' % self.length}\n        if queryset.filter(**lookup).count() >= len(self.chars)**self.length:\n            raise FieldError(\"No available slugs remaining.\")\n\n        slug = get_random_string(self.length, self.chars)\n\n        # Exclude the current model instance from the queryset used in\n        # finding next valid slug.\n        if model_instance.pk:\n            queryset = queryset.exclude(pk=model_instance.pk)\n\n        # Form a kwarg dict used to impliment any unique_together\n        # contraints.\n        kwargs = {}\n        for params in model_instance._meta.unique_together:\n            if self.attname in params:\n                for param in params:\n                    kwargs[param] = getattr(model_instance, param, None)\n        kwargs[self.attname] = slug\n\n        while queryset.filter(**kwargs):\n            slug = get_random_string(self.length, self.chars)\n            kwargs[self.attname] = slug\n\n        return slug"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef south_field_triple(self):\n        # We'll just introspect the _actual_ field.\n        from south.modelsinspector import introspector\n        field_class = '%s.%s' % (self.__module__, self.__class__.__name__)\n        args, kwargs = introspector(self)\n        kwargs.update({\n            'length': repr(self.length),\n            'exclude_upper': repr(self.exclude_upper),\n            'exclude_lower': repr(self.exclude_lower),\n            'exclude_digits': repr(self.exclude_digits),\n            'exclude_vowels': repr(self.exclude_vowels),\n        })\n        # That's our definition!\n        return (field_class, args, kwargs)", "response": "Returns a suitable description of this field for South."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an object on HelpScout.", "response": "def create(cls, session, record, endpoint_override=None, out_type=None,\n               **add_params):\n        \"\"\"Create an object on HelpScout.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            record (helpscout.BaseModel): The record to be created.\n            endpoint_override (str, optional): Override the default\n                endpoint using this.\n            out_type (helpscout.BaseModel, optional): The type of record to\n                output. This should be provided by child classes, by calling\n                super.\n            **add_params (mixed): Add these to the request parameters.\n\n        Returns:\n            helpscout.models.BaseModel: Newly created record. Will be of the\n        \"\"\"\n        cls._check_implements('create')\n        data = record.to_api()\n        params = {\n            'reload': True,\n        }\n        params.update(**add_params)\n        data.update(params)\n        return cls(\n            endpoint_override or '/%s.json' % cls.__endpoint__,\n            data=data,\n            request_type=RequestPaginator.POST,\n            singleton=True,\n            session=session,\n            out_type=out_type,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a record. Args: session (requests.sessions.Session): Authenticated session. record (helpscout.BaseModel): The record to be deleted. endpoint_override (str, optional): Override the default endpoint using this. out_type (helpscout.BaseModel, optional): The type of record to output. This should be provided by child classes, by calling super. Returns: NoneType: Nothing.", "response": "def delete(cls, session, record, endpoint_override=None, out_type=None):\n        \"\"\"Delete a record.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            record (helpscout.BaseModel): The record to be deleted.\n            endpoint_override (str, optional): Override the default\n                endpoint using this.\n            out_type (helpscout.BaseModel, optional): The type of record to\n                output. This should be provided by child classes, by calling\n                super.\n\n        Returns:\n            NoneType: Nothing.\n        \"\"\"\n        cls._check_implements('delete')\n        return cls(\n            endpoint_override or '/%s/%s.json' % (\n                cls.__endpoint__, record.id,\n            ),\n            request_type=RequestPaginator.DELETE,\n            singleton=True,\n            session=session,\n            out_type=out_type,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a specific record.", "response": "def get(cls, session, record_id, endpoint_override=None):\n        \"\"\"Return a specific record.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            record_id (int): The ID of the record to get.\n            endpoint_override (str, optional): Override the default\n                endpoint using this.\n\n        Returns:\n            helpscout.BaseModel: A record singleton, if existing. Otherwise\n                ``None``.\n        \"\"\"\n        cls._check_implements('get')\n        try:\n            return cls(\n                endpoint_override or '/%s/%d.json' % (\n                    cls.__endpoint__, record_id,\n                ),\n                singleton=True,\n                session=session,\n            )\n        except HelpScoutRemoteException as e:\n            if e.status_code == 404:\n                return None\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(cls, session, endpoint_override=None, data=None):\n        cls._check_implements('list')\n        return cls(\n            endpoint_override or '/%s.json' % cls.__endpoint__,\n            data=data,\n            session=session,\n        )", "response": "Returns a new instance of the base class with the specified endpoint and data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for a record given a domain.", "response": "def search(cls, session, queries, out_type):\n        \"\"\"Search for a record given a domain.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            queries (helpscout.models.Domain or iter): The queries for the\n                domain. If a ``Domain`` object is provided, it will simply be\n                returned. Otherwise, a ``Domain`` object will be generated\n                from the complex queries. In this case, the queries should\n                conform to the interface in\n                :func:`helpscout.domain.Domain.from_tuple`.\n            out_type (helpscout.BaseModel): The type of record to output. This\n                should be provided by child classes, by calling super.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.BaseModel): Results\n                iterator of the ``out_type`` that is defined.\n        \"\"\"\n        cls._check_implements('search')\n        domain = cls.get_search_domain(queries)\n        return cls(\n            '/search/%s.json' % cls.__endpoint__,\n            data={'query': str(domain)},\n            session=session,\n            out_type=out_type,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a record. Args: session (requests.sessions.Session): Authenticated session. record (helpscout.BaseModel): The record to be updated. Returns: helpscout.BaseModel: Freshly updated record.", "response": "def update(cls, session, record):\n        \"\"\"Update a record.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            record (helpscout.BaseModel): The record to\n                be updated.\n\n        Returns:\n            helpscout.BaseModel: Freshly updated record.\n        \"\"\"\n        cls._check_implements('update')\n        data = record.to_api()\n        del data['id']\n        data['reload'] = True\n        return cls(\n            '/%s/%s.json' % (cls.__endpoint__, record.id),\n            data=data,\n            request_type=RequestPaginator.PUT,\n            singleton=True,\n            session=session,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nintegrating each peak naively ; without regard to overlap.", "response": "def simple_integrate(ts, peak_list, base_ts=None, intname='simple'):\n    \"\"\"\n    Integrate each peak naively; without regard to overlap.\n\n    This is used as the terminal step by most of the other integrators.\n    \"\"\"\n    peaks = []\n    for hints in peak_list:\n        t0, t1 = hints['t0'], hints['t1']\n        hints['int'] = intname\n        pk_ts = ts.twin((t0, t1))\n        if base_ts is None:\n            # make a two point baseline\n            base = Trace([hints.get('y0', pk_ts[0]),\n                          hints.get('y1', pk_ts[-1])],\n                         [t0, t1], name=ts.name)\n        else:\n            base = base_ts.twin((t0, t1))\n        peaks.append(PeakComponent(hints, pk_ts, base))\n    return peaks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndrop an integer from the list of peaks.", "response": "def drop_integrate(ts, peak_list):\n    \"\"\"\n    Resolves overlap by breaking at the minimum value.\n    \"\"\"\n    peaks = []\n    for _, pks in _get_windows(peak_list):\n        temp_pks = []\n        pks = sorted(pks, key=lambda p: p['t0'])\n        if 'y0' in pks[0] and 'y1' in pks[-1]:\n            y0, y1 = pks[0]['y0'], pks[-1]['y1']\n        else:\n            y0 = ts.get_point(pks[0]['t0'])\n            y1 = ts.get_point(pks[-1]['t1'])\n        ys = np.array([y0, y1])\n        xs = np.array([pks[0]['t0'], pks[-1]['t1']])\n\n        # go through list of peaks to make sure there's no overlap\n        for hints in pks:\n            t0, t1 = hints['t0'], hints['t1']\n\n            # figure out the y values (using a linear baseline)\n            hints['y0'] = np.interp(t0, xs, ys)\n            hints['y1'] = np.interp(t1, xs, ys)\n\n            # if this peak totally overlaps with an existing one, don't add\n            if sum(1 for p in temp_pks if t1 <= p['t1']) > 0:\n                continue\n            overlap_pks = [p for p in temp_pks if t0 <= p['t1']]\n            if len(overlap_pks) > 0:\n                # find the last of the overlapping peaks\n                overlap_pk = max(overlap_pks, key=lambda p: p['t0'])\n                # get the section of trace and find the lowest point\n                over_ts = ts.twin((t0, overlap_pk['t1']))\n                min_t = over_ts.index[over_ts.values.argmin()]\n\n                # delete the existing overlaping peak\n                for i, p in enumerate(temp_pks):\n                    if p == overlap_pk:\n                        del temp_pks[i]\n                        break\n\n                # interpolate a new y value\n                y_val = np.interp(min_t, xs, ys)\n                overlap_pk['y1'] = y_val\n                hints['y0'] = y_val\n\n                # add the old and new peak in\n                overlap_pk['t1'] = min_t\n                temp_pks.append(overlap_pk)\n                hints['t0'], hints['t1'] = min_t, t1\n                temp_pks.append(hints)\n            else:\n                hints['t0'], hints['t1'] = t0, t1\n                temp_pks.append(hints)\n\n        # none of our peaks should overlap, so we can just use\n        # simple_integrate now\n        peaks += simple_integrate(ts, temp_pks, intname='drop')\n    return peaks"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nintegrates a timeseries and peaks found in it and return peaks.", "response": "def _integrate_mpwrap(ts_and_pks, integrate, fopts):\n    \"\"\"\n    Take a zipped timeseries and peaks found in it\n    and integrate it to return peaks. Used to allow\n    multiprocessing support.\n    \"\"\"\n    ts, tpks = ts_and_pks\n    pks = integrate(ts, tpks, **fopts)\n    # for p in pks:\n    #     p.info['mz'] = str(ts.name)\n    return pks"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bytes(num_bytes):\n    # Is this the way to do it?\n    #s = c_ubyte()\n    # Or this?\n    s = create_string_buffer(num_bytes)\n    # Used to keep track of status. 1 = success, 0 = error.\n    ok = c_int()\n    # Provider?\n    hProv = c_ulong()\n\n    ok = windll.Advapi32.CryptAcquireContextA(byref(hProv), None, None, PROV_RSA_FULL, 0)\n    ok = windll.Advapi32.CryptGenRandom(hProv, wintypes.DWORD(num_bytes), cast(byref(s), POINTER(c_byte)))\n\n    return s.raw", "response": "Returns a random string of num_bytes length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_long():\n    # The C long type to populate.\n    pbRandomData = c_ulong()\n    # Determine the byte size of this machine's long type.\n    size_of_long = wintypes.DWORD(sizeof(pbRandomData))\n    # Used to keep track of status. 1 = success, 0 = error.\n    ok = c_int()\n    # Provider?\n    hProv = c_ulong()\n\n    ok = windll.Advapi32.CryptAcquireContextA(byref(hProv), None, None, PROV_RSA_FULL, 0)\n    ok = windll.Advapi32.CryptGenRandom(hProv, size_of_long, byref(pbRandomData))\n\n    return pbRandomData.value", "response": "Generates a random long."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets function annotations (on Python2 and 3).", "response": "def annotate(*args, **kwargs):\n    \"\"\"Set function annotations (on Python2 and 3).\"\"\"\n    def decorator(f):\n        if not hasattr(f, '__annotations__'):\n            f.__annotations__ = kwargs.copy()\n        else:\n            f.__annotations__.update(kwargs)\n\n        if args:\n            if len(args) != 1:\n                raise ValueError('annotate supports only a single argument.')\n            f.__annotations__['return'] = args[0]\n        return f\n\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a decorator that will automatically instantiate objects based on function archive parameter annotations.", "response": "def auto_instantiate(*classes):\n    \"\"\"Creates a decorator that will instantiate objects based on function\n    parameter annotations.\n\n    The decorator will check every argument passed into ``f``. If ``f`` has an\n    annotation for the specified parameter and the annotation is found in\n    ``classes``, the parameter value passed in will be used to construct a new\n    instance of the expression that is the annotation.\n\n    An example (Python 3):\n\n    .. code-block:: python\n\n        @auto_instantiate(int)\n        def foo(a: int, b: float):\n            pass\n\n    Any value passed in as ``b`` is left unchanged. Anything passed as the\n    parameter for ``a`` will be converted to :class:`int` before calling the\n    function.\n\n    Since Python 2 does not support annotations, the\n    :func:`~data.decorators.annotate` function should can be used:\n\n    .. code-block:: python\n\n        @auto_instantiate(int)\n        @annotate(a=int)\n        def foo(a, b):\n            pass\n\n\n    :param classes: Any number of classes/callables for which\n                    auto-instantiation should be performed. If empty, perform\n                    for all.\n\n    :note: When dealing with data, it is almost always more convenient to use\n           the :func:`~data.decorators.data` decorator instead.\n    \"\"\"\n    def decorator(f):\n        # collect our argspec\n        sig = signature(f)\n\n        @wraps(f)\n        def _(*args, **kwargs):\n            bvals = sig.bind(*args, **kwargs)\n\n            # replace with instance if desired\n            for varname, val in bvals.arguments.items():\n                anno = sig.parameters[varname].annotation\n\n                if anno in classes or (len(classes) == 0 and anno != _empty):\n                    bvals.arguments[varname] = anno(val)\n\n            return f(*bvals.args, **bvals.kwargs)\n\n        # create another layer by wrapping in a FunctionMaker. this is done\n        # to preserve the original signature\n        return FunctionMaker.create(\n            f, 'return _(%(signature)s)', dict(_=_, __wrapped__=f)\n        )\n\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef data(*argnames):\n    # make it work if given only one argument (for Python3)\n    if len(argnames) == 1 and callable(argnames[0]):\n        return data()(argnames[0])\n\n    def decorator(f):\n        f = annotate(**dict((argname, Data) for argname in argnames))(f)\n        f = auto_instantiate(Data)(f)\n        return f\n    return decorator", "response": "Designate an argument as a : class : ~data. Data argument."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SNRPlanet(SNRStar, starPlanetFlux, Nobs, pixPerbin, NVisits=1):\n\n    SNRplanet = SNRStar * starPlanetFlux * \\\n        sqrt(Nobs) * sqrt(pixPerbin) * sqrt(NVisits)\n\n    return SNRplanet", "response": "r Calculates the Signal to Noise Ratio of the planet atmosphere\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates stellar temperature using the main sequence relationship T ~ 5800 * M^0. 65", "response": "def estimateStellarTemperature(M_s):\n    \"\"\"Estimates stellar temperature using the main sequence relationship\n    T ~ 5800*M^0.65 (Cox 2000).\n    \"\"\"\n    # TODO improve with more x and k values from Cox 2000\n    try:\n        temp = (5800 * aq.K * float(M_s.rescale(aq.M_s) ** 0.65)).rescale(aq.K)\n    except AttributeError:\n        temp = np.nan\n    return temp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef estimateDistance(m, M, Av=0.0):\n    try:\n        m = float(m)  # basic value checking as there is no units\n        M = float(M)\n        Av = float(Av)\n    except TypeError:\n        return np.nan\n\n    d = 10 ** ((m - M + 5 - Av) / 5)\n\n    if math.isnan(d):\n        return np.nan\n    else:\n        return d * aq.pc", "response": "estimate the distance to star based on the absolute magnitude apparent magnitude M and Av"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload magnitude_estimation. dat and creates a dictionary of absolute magnitude for each L class", "response": "def _createAbsMagEstimationDict():\n    \"\"\" loads magnitude_estimation.dat which is from\n    http://xoomer.virgilio.it/hrtrace/Sk.htm on 24/01/2014 and based on\n    Schmid-Kaler (1982)\n\n    creates a dict in the form [Classletter][ClassNumber][List of values for\n    each L Class]\n    \"\"\"\n    magnitude_estimation_filepath = resource_filename(\n        __name__, 'data/magnitude_estimation.dat')\n    raw_table = np.loadtxt(magnitude_estimation_filepath, '|S5')\n\n    absMagDict = {\n        'O': {},\n        'B': {},\n        'A': {},\n        'F': {},\n        'G': {},\n        'K': {},\n        'M': {}}\n    for row in raw_table:\n        if sys.hexversion >= 0x03000000:\n            # otherwise we get byte ints or b' caused by 2to3\n            starClass = row[0].decode(\"utf-8\")\n            absMagDict[starClass[0]][int(starClass[1])] = [\n                float(x) for x in row[1:]]\n        else:\n            # dict of spectral type = {abs mag for each luminosity class}\n            absMagDict[row[0][0]][int(row[0][1])] = [float(x) for x in row[1:]]\n\n    # manually typed from table headers - used to match columns with the L\n    # class (header)\n    LClassRef = {\n        'V': 0,\n        'IV': 1,\n        'III': 2,\n        'II': 3,\n        'Ib': 4,\n        'Iab': 5,\n        'Ia': 6,\n        'Ia0': 7}\n\n    return absMagDict, LClassRef"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimateAbsoluteMagnitude(spectralType):\n\n    from .astroclasses import SpectralType\n\n    specType = SpectralType(spectralType)\n\n    if specType.classLetter == '':\n        return np.nan\n    elif specType.classNumber == '':\n        specType.classNumber = 5  # approximation using mid magnitude value\n\n    if specType.lumType == '':\n        specType.lumType = 'V'  # assume main sequence\n\n    LNum = LClassRef[specType.lumType]\n    classNum = specType.classNumber\n    classLet = specType.classLetter\n\n    try:\n        return absMagDict[classLet][classNum][LNum]\n    # value not in table. Assume the number isn't there (Key p2.7, Ind p3+)\n    except (KeyError, IndexError):\n        try:\n            classLookup = absMagDict[classLet]\n            values = np.array(list(classLookup.values()))[\n                :, LNum]  # only select the right L Type\n            return np.interp(classNum, list(classLookup.keys()), values)\n        except (KeyError, ValueError):\n            return np.nan", "response": "Uses the spectral type to lookup an approximate absolute magnitude for\n    the star."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_remote_allowed(remote):\n  if settings.debug:\n    return True\n  if not remote:\n    return False\n  for domain_pattern in settings.node['cors_whitelist_domains']:\n    if domain_pattern.match(remote):\n      return True\n  return False", "response": "Check if the given remote is allowed to make a CORS request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a decorator which allows a function to serve url and only allows the HTTP methods in methods", "response": "def endpoint(url, methods=['GET']):\n  \"\"\"\n  Returns a decorator which when applied a function, causes that function to\n  serve `url` and only allows the HTTP methods in `methods`\n  \"\"\"\n  def decorator(function, methods=methods):\n    # Always allow OPTIONS since CORS requests will need it.\n    methods = set(methods)\n    methods.add('OPTIONS')\n\n    @wraps(function)\n    def wrapper(environment, start_response):\n      try:\n        start_time = time.time()\n\n        if function.func_name not in (_serving_mode_endpoints\n                                      [settings.serving_mode]):\n          start_response('403 Forbidden',\n                         [('Content-Type', 'application/json')])\n          return marshal.dumps({\n            ERRORS_FIELD: ['kronosd is configured to block access to this '\n                           'endpoint.'],\n            SUCCESS_FIELD: False,\n            TOOK_FIELD: '%fms' % (1000 * (time.time() - start_time))\n          })\n        req_method = environment['REQUEST_METHOD']\n\n        # If the request method is not allowed, return 405.\n        if req_method not in methods:\n          start_response('405 Method Not Allowed',\n                         [('Allow', ', '.join(methods)),\n                          ('Content-Type', 'application/json')])\n          return marshal.dumps({\n            ERRORS_FIELD: ['%s method not allowed' % req_method],\n            SUCCESS_FIELD: False,\n            TOOK_FIELD: '%fms' % (1000 * (time.time() - start_time))\n          })\n\n        headers = []\n        remote_origin = environment.get('HTTP_ORIGIN')\n\n        if req_method == 'OPTIONS':\n          # This is a CORS preflight request so check that the remote domain is\n          # allowed and respond with appropriate CORS headers.\n          # http://www.html5rocks.com/static/images/cors_server_flowchart.png\n          if is_remote_allowed(remote_origin):\n            headers.extend([\n              ('Access-Control-Allow-Origin', remote_origin),\n              ('Access-Control-Allow-Credentials', 'true'),\n              ('Access-Control-Allow-Headers', ', '.join(\n                ('Accept', 'Content-Type', 'Origin', 'X-Requested-With'))),\n              ('Access-Control-Allow-Methods', ', '.join(methods))\n            ])\n          # We just tell the client that CORS is ok. Client will follow up\n          # with another request to get the answer.\n          start_response('200 OK', headers)\n          return ''\n\n        # All POST bodies must be json, so decode it here.\n        if req_method == 'POST':\n          try:\n            environment['json'] = marshal.loads(environment['wsgi.input']\n                                                .read())\n          except ValueError:\n            start_response('400 Bad Request',\n                           [('Content-Type', 'application/json')])\n            return marshal.dumps({\n              ERRORS_FIELD: ['Request body must be valid JSON.'],\n              SUCCESS_FIELD: False,\n              TOOK_FIELD: '%fms' % (1000 * (time.time() - start_time))\n            })\n\n        # All responses are JSON.\n        headers.append(('Content-Type', 'application/json'))\n\n        if remote_origin:\n          headers.append(('Access-Control-Allow-Origin', remote_origin))\n\n        response = function(environment, start_response, headers)\n        if not isinstance(response, types.GeneratorType):\n          response[TOOK_FIELD] = '%fms' % (1000 * (time.time() - start_time))\n          response = marshal.dumps(response)\n        return response\n      except Exception, e:\n        log.exception('endpoint: uncaught exception!')\n        start_response('400 Bad Request',\n                       [('Content-Type', 'application/json')])\n        return marshal.dumps({\n          ERRORS_FIELD: [repr(e)],\n          SUCCESS_FIELD: False,\n          TOOK_FIELD: '%fms' % (1000 * (time.time() - start_time))\n        })\n\n    if settings.profile:\n      wrapper = profile(wrapper)\n\n    # Map the URL to serve to this function. Only map certain\n    # endpoints if serving_mode is restrictive.\n    global ENDPOINTS\n    ENDPOINTS[url] = wrapper\n\n    return wrapper\n\n  return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the specified number of challenges.", "response": "def generate_challenges(self, num, root_seed):\n        \"\"\" Generate the specified number of hash challenges.\n\n        :param num: The number of hash challenges we want to generate.\n        :param root_seed: Some value that we use to generate our seeds from.\n        \"\"\"\n\n        # Generate a series of seeds\n        seeds = self.generate_seeds(num, root_seed, self.secret)\n        blocks = self.pick_blocks(num, root_seed)\n\n        # List of 2-tuples (seed, hash_response)\n        self.challenges = []\n\n        # Generate the corresponding hash for each seed\n        for i in range(num):\n            self.challenges.append(Challenge(blocks[i], seeds[i]))\n            response = self.meet_challenge(self.challenges[i])\n            self.challenges[i].response = response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the SHA256 hash of a specific file block plus the provided seed.", "response": "def meet_challenge(self, challenge):\n        \"\"\" Get the SHA256 hash of a specific file block plus the provided\n        seed. The default block size is one tenth of the file. If the file is\n        larger than 10KB, 1KB is used as the block size.\n\n        :param challenge: challenge as a `Challenge <heartbeat.Challenge>`\n        object\n        \"\"\"\n        chunk_size = min(1024, self.file_size // 10)\n        seed = challenge.seed\n\n        h = hashlib.sha256()\n        self.file_object.seek(challenge.block)\n\n        if challenge.block > (self.file_size - chunk_size):\n            end_slice = (\n                challenge.block - (self.file_size - chunk_size)\n            )\n            h.update(self.file_object.read(end_slice))\n            self.file_object.seek(0)\n            h.update(self.file_object.read(chunk_size - end_slice))\n        else:\n            h.update(self.file_object.read(chunk_size))\n\n        h.update(seed)\n\n        return h.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a list of num num_seeds from a root seed.", "response": "def generate_seeds(num, root_seed, secret):\n        \"\"\" Deterministically generate list of seeds from a root seed.\n\n        :param num: Numbers of seeds to generate as int\n        :param root_seed: Seed to start off with.\n        :return: seed values as a list of length num\n        \"\"\"\n        # Generate a starting seed from the root\n        if num < 0:\n            raise HeartbeatError('%s is not greater than 0' % num)\n\n        if secret is None:\n            raise HeartbeatError('secret can not be of type NoneType')\n\n        seeds = []\n        try:\n            tmp_seed = hashlib.sha256(root_seed).digest()\n        except TypeError:\n            tmp_seed = hashlib.sha256(str(root_seed).encode()).digest()\n\n        # Deterministically generate the rest of the seeds\n        for x in range(num):\n            seeds.append(tmp_seed)\n            h = hashlib.sha256(tmp_seed)\n            h.update(secret)\n            tmp_seed = h.digest()\n\n        return seeds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npicks a set of positions to start reading blocks from the file that challenges are created for.", "response": "def pick_blocks(self, num, root_seed):\n        \"\"\" Pick a set of positions to start reading blocks from the file\n        that challenges are created for. This is a deterministic\n        operation. Positions are guaranteed to be within the bounds of the\n        file.\n\n        :param num: Number of blocks to pick\n        :param root_seed: Seed with which begin picking blocks.\n        :return: block values as a list\n        \"\"\"\n        if num < 0:\n            raise HeartbeatError('%s is not greater than 0' % num)\n\n        blocks = []\n        random.seed(root_seed)\n\n        for i in range(num):\n            blocks.append(random.randint(0, self.file_size - 1))\n\n        return blocks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the returned hash is in our challenges list.", "response": "def check_answer(self, hash_answer):\n        \"\"\" Check if the returned hash is in our challenges list.\n\n        :param hash_answer: Hash that we compare to our list of challenges\n        :return: boolean indicating if answer is correct, True, or not, False\n        \"\"\"\n        for challenge in self.challenges:\n            if challenge.response == hash_answer:\n                # If we don't discard a used challenge then a node\n                # could fake having the file because it already\n                # knows the proper response\n                self.delete_challenge(hash_answer)\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_apis(self):\n        helpscout = __import__('helpscout.apis')\n        for class_name in helpscout.apis.__all__:\n            if not class_name.startswith('_'):\n                cls = getattr(helpscout.apis, class_name)\n                api = AuthProxy(self.session, cls)\n                setattr(self, class_name, api)\n                self.__apis__[class_name] = api", "response": "Find available APIs and set instances property auth proxies."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slice_time(begin, end=None, duration=datetime.timedelta(days=2)):\n    duration_ms = int(duration.total_seconds() * 1000)\n    previous = int(unix_time(begin) * 1000)\n    next = previous + duration_ms\n    now_ms = unix_time(datetime.datetime.now())*1000\n    end_slice = now_ms if not end else min(now_ms, int(unix_time(end) * 1000))\n\n    while next < end_slice:\n        yield TimeSlice(previous, next)\n        previous = next\n        next += duration_ms\n        now_ms = unix_time(datetime.datetime.now())*1000\n        end_slice = now_ms if not end else min(now_ms, int(unix_time(end) * 1000))\n    yield TimeSlice(previous, end_slice)", "response": "returns a generator that yields all the timeslices of the given duration between begin and end."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_response(response):\n    if isinstance(response, unicode) or \\\n            isinstance(response, str):\n        response = (response, 'text/html')\n\n    return response", "response": "Make response tuple\n\n    Potential features to be added\n      - Parameters validation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the first UI page.", "response": "def _init_ui(self):\n        \"\"\"Initial the first UI page.\n        - load html from '/' endpoint\n        - if <title> is defined, use as windows title\n        \"\"\"\n        (content, mimetype) = make_response(self._url_map_to_function('/'))\n        try:\n            beautifulsoup = BeautifulSoup(content)\n            self.window.set_title(beautifulsoup.find('title').string)\n        except:\n            pass\n\n        if self.debug is True:\n            print self.app_dir\n\n        # Use load_string instead of load_uri because it shows warning.\n        self.webkit_web_view.load_string(\n            content,\n            mime_type=mimetype,\n            encoding='utf-8',\n            base_uri='/',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a fast - fourier transform on a Trace", "response": "def fft(ts):\n    \"\"\"\n    Perform a fast-fourier transform on a Trace\n    \"\"\"\n    t_step = ts.index[1] - ts.index[0]\n    oc = np.abs(np.fft.fftshift(np.fft.fft(ts.values))) / len(ts.values)\n    t = np.fft.fftshift(np.fft.fftfreq(len(oc), d=t_step))\n    return Trace(oc, t)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef movingaverage(arr, window):\n    m = np.ones(int(window)) / int(window)\n    return scipy.ndimage.convolve1d(arr, m, axis=0, mode='reflect')", "response": "Calculates the moving average of an array of a certain window size."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loads(ast_str):\n    data = zlib.decompress(ast_str)\n    li = struct.unpack('<L', data[0:4])[0]\n    lt = struct.unpack('<L', data[4:8])[0]\n    n = data[8:8 + li].decode('utf-8')\n    t = np.fromstring(data[8 + li:8 + li + lt])\n    d = np.fromstring(data[8 + li + lt:])\n\n    return Trace(d, t, name=n)", "response": "Create a Trace from a suitably compressed string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dumps(asts):\n    d = asts.values.tostring()\n    t = asts.index.values.astype(float).tostring()\n    lt = struct.pack('<L', len(t))\n    i = asts.name.encode('utf-8')\n    li = struct.pack('<L', len(i))\n    try:  # python 2\n        return buffer(zlib.compress(li + lt + i + t + d))\n    except NameError:  # python 3\n        return zlib.compress(li + lt + i + t + d)", "response": "Create a compressed string from an Trace.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef desaturate(c, k=0):\n    from matplotlib.colors import ColorConverter\n    c = ColorConverter().to_rgb(c)\n    intensity = 0.299 * c[0] + 0.587 * c[1] + 0.114 * c[2]\n    return [intensity * k + i * (1 - k) for i in c]", "response": "Utility function to desaturate a color c by an amount k."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_spectrum_match(spec, spec_lib, method='euclidian'):\n    # filter out any points with abundance below 1 %\n    # spec[spec / np.sum(spec) < 0.01] = 0\n    # normalize everything to sum to 1\n    spec = spec / np.max(spec)\n\n    if method == 'dot':\n        d1 = (spec_lib * lil_matrix(spec).T).sum(axis=1).A ** 2\n        d2 = np.sum(spec ** 2) * spec_lib.multiply(spec_lib).sum(axis=1).A\n        dist = d1 / d2\n    elif method == 'euclidian':\n        # st_spc = spectrum[np.newaxis, :].repeat(spec_lib.shape[0], axis=0)\n        st_spc = dia_matrix((spec, [0]), shape=(len(spec), len(spec)))\n        # calculate the residual sum of squares from spectrum to library\n        dist_sp = spec_lib.multiply(spec_lib) - 2 * spec_lib.dot(st_spc)\n        dist = dist_sp.sum(axis=1).A + np.sum(spec ** 2)\n    return (dist.argmin(), dist.min())", "response": "Find the spectrum in spec_lib most similar to spec."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_command(self, ctx, name):\n        if name not in self.daemon_class.list_actions():\n            return None\n\n        # The context object is a Daemon object\n        daemon = ctx.obj\n\n        def subcommand(debug=False):\n            \"\"\"Call a daemonocle action.\"\"\"\n            if daemon.detach and debug:\n                daemon.detach = False\n\n            daemon.do_action(name)\n\n        # Override the docstring for the function so that it shows up\n        # correctly in the help output\n        subcommand.__doc__ = daemon.get_action(name).__doc__\n\n        if name == 'start':\n            # Add a --debug option for start\n            subcommand = click.option(\n                '--debug', is_flag=True,\n                help='Do NOT detach and run in the background.'\n            )(subcommand)\n\n        # Make it into a click command\n        subcommand = click.command(\n            name, options_metavar=self.options_metavar)(subcommand)\n\n        return subcommand", "response": "Get a callable command object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the configuration from a file.", "response": "def load(self, path=None):\n        '''\n        Load configuration (from configuration files).\n\n        Parameters\n        ----------\n        path : ~pathlib.Path or None\n            Path to configuration file, which must exist; or path to directory\n            containing a configuration file; or None.\n\n        Returns\n        -------\n        ~typing.Dict[str, ~typing.Dict[str, str]]\n            The configuration as a dict of sections mapping section name to\n            options. Each options dict maps from option name to option value. The\n            ``default`` section is not included. However, all options from the\n            ``default`` section are included in each returned section.\n\n        Raises\n        ------\n        ValueError\n            If ``path`` is a missing file; or if it is a directory which does not\n            contain the configuration file.\n\n        Examples\n        --------\n        >>> loader.load()\n        {\n            'section1': {\n                'option1': 'value',\n                'option2': 'value2',\n            }\n        }\n        '''\n        # Add path\n        paths = self._paths.copy()\n        if path:\n            if path.is_dir():\n                path /= '{}.conf'.format(self._configuration_name)\n            paths.append(path)\n\n        # Prepend file sys root to abs paths\n        paths = [(path_._root / str(x)[1:] if x.is_absolute() else x) for x in paths]\n        if path:\n            path = paths[-1]\n\n            # Passed path must exist\n            if not path.exists():\n                raise ValueError('Expected configuration file at {}'.format(path))\n\n        # Configure parser\n        config_parser = ConfigParser(\n            inline_comment_prefixes=('#', ';'), \n            empty_lines_in_values=False, \n            default_section='default', \n            interpolation=ExtendedInterpolation()\n        )\n\n        def option_transform(name):\n            return name.replace('-', '_').replace(' ', '_').lower()\n\n        config_parser.optionxform = option_transform\n\n        # Parse defaults and configs\n        with suppress(FileNotFoundError):\n            defaults_contents = resource_string(self._package_name, 'data/{}.defaults.conf'.format(self._configuration_name))\n            config_parser.read_string(defaults_contents.decode('UTF-8'))\n        config_parser.read([str(x) for x in paths])  # reads in given order\n\n        config = {k : dict(v) for k,v in config_parser.items()}\n        del config['default']\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_help_message(self, description):\n        '''\n        Get a user friendly help message that can be dropped in a\n        `click.Command`\\ 's epilog.\n\n        Parameters\n        ----------\n        description : str\n            Description of the configuration file to include in the message.\n\n        Returns\n        -------\n        str\n            A help message that uses :py:mod:`click`\\ 's help formatting\n            constructs (e.g. ``\\b``).\n        '''\n        config_files_listing = '\\n'.join('    {}. {!s}'.format(i, path) for i, path in enumerate(self._paths, 1))\n        text = dedent('''\\\n        {config_file}:\n        \n            {description}\n            \n            {config_file} files are read from the following locations:\n            \n            \\b\n            {config_files_listing}\n            \n            Any configuration file can override options set by previous configuration files. Some \n            configuration file locations can be changed using the XDG standard (http://standards.freedesktop.org/basedir-spec/basedir-spec-0.6.html).\n        ''').format(\n            config_file='{}.conf'.format(self._configuration_name),\n            description=description,\n            config_files_listing=config_files_listing\n        )\n        return text", "response": "Return a user friendly help message that can be dropped in click. Command s epilog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the application initializing your components.", "response": "def start(self):\n        \"\"\"\n        Start the application, initializing your components.\n        \"\"\"\n        current_pedalboard = self.controller(CurrentController).pedalboard\n        if current_pedalboard is None:\n            self.log('Not exists any current pedalboard.')\n            self.log('Use CurrentController to set the current pedalboard')\n        else:\n            self.log('Load current pedalboard - \"{}\"', current_pedalboard.name)\n\n        self.mod_host.pedalboard = current_pedalboard\n\n        for component in self.components:\n            component.init()\n            self.log('Load component - {}', component.__class__.__name__)\n\n        self.log('Components loaded')\n        atexit.register(self.stop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the application closing your components and controllers.", "response": "def stop(self):\n        \"\"\"\n        Stop the application, closing your components.\n        \"\"\"\n        for component in self.components:\n            component.close()\n            self.log('Stopping component - {}', component.__class__.__name__)\n\n        for controller in self.controllers.values():\n            controller.close()\n            self.log('Stopping controller - {}', controller.__class__.__name__)\n\n        atexit.unregister(self.stop)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a dictionary with keys for scale and value into seconds based on scale.", "response": "def get_seconds(value, scale):\n  \"\"\"Convert time scale dict to seconds\n\n  Given a dictionary with keys for scale and value, convert\n  value into seconds based on scale.\n  \"\"\"\n  scales = {\n    'seconds': lambda x: x,\n    'minutes': lambda x: x * 60,\n    'hours': lambda x: x * 60 * 60,\n    'days': lambda x: x * 60 * 60 * 24,\n    'weeks': lambda x: x * 60 * 60 * 24 * 7,\n    'months': lambda x: x * 60 * 60 * 24 * 30,\n    'years': lambda x: x * 60 * 60 * 24 * 365,\n  }\n  return scales[scale](value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_col_epsg(mapped_class, geom_attr):\n    col = class_mapper(mapped_class).get_property(geom_attr).columns[0]\n    return col.type.srid", "response": "Get the EPSG code associated with a geometry attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_geom_filter(request, mapped_class, geom_attr):\n    tolerance = float(request.params.get('tolerance', 0.0))\n    epsg = None\n    if 'epsg' in request.params:\n        epsg = int(request.params['epsg'])\n    box = request.params.get('bbox')\n    shape = None\n    if box is not None:\n        box = [float(x) for x in box.split(',')]\n        shape = Polygon(((box[0], box[1]), (box[0], box[3]),\n                         (box[2], box[3]), (box[2], box[1]),\n                         (box[0], box[1])))\n    elif 'lon' in request.params and 'lat' in request.params:\n        shape = Point(float(request.params['lon']),\n                      float(request.params['lat']))\n    elif 'geometry' in request.params:\n        shape = loads(request.params['geometry'],\n                      object_hook=GeoJSON.to_instance)\n        shape = asShape(shape)\n    if shape is None:\n        return None\n    column_epsg = _get_col_epsg(mapped_class, geom_attr)\n    geom_attr = getattr(mapped_class, geom_attr)\n    epsg = column_epsg if epsg is None else epsg\n    if epsg != column_epsg:\n        geom_attr = func.ST_Transform(geom_attr, epsg)\n    geometry = from_shape(shape, srid=epsg)\n    return func.ST_DWITHIN(geom_attr, geometry, tolerance)", "response": "Create a spatial filter based on the request params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an SQLAlchemy filter based on the request params.", "response": "def create_attr_filter(request, mapped_class):\n    \"\"\"Create an ``and_`` SQLAlchemy filter (a ClauseList object) based\n    on the request params (``queryable``, ``eq``, ``ne``, ...).\n\n    Arguments:\n\n    request\n        the request.\n\n    mapped_class\n        the SQLAlchemy mapped class.\n    \"\"\"\n\n    mapping = {\n        'eq': '__eq__',\n        'ne': '__ne__',\n        'lt': '__lt__',\n        'lte': '__le__',\n        'gt': '__gt__',\n        'gte': '__ge__',\n        'like': 'like',\n        'ilike': 'ilike'\n    }\n    filters = []\n    if 'queryable' in request.params:\n        queryable = request.params['queryable'].split(',')\n        for k in request.params:\n            if len(request.params[k]) <= 0 or '__' not in k:\n                continue\n            col, op = k.split(\"__\")\n            if col not in queryable or op not in mapping:\n                continue\n            column = getattr(mapped_class, col)\n            f = getattr(column, mapping[op])(request.params[k])\n            filters.append(f)\n    return and_(*filters) if len(filters) > 0 else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_filter(request, mapped_class, geom_attr, **kwargs):\n    attr_filter = create_attr_filter(request, mapped_class)\n    geom_filter = create_geom_filter(request, mapped_class, geom_attr,\n                                     **kwargs)\n    if geom_filter is None and attr_filter is None:\n        return None\n    if geom_filter is None:\n        return attr_filter\n    if attr_filter is None:\n        return geom_filter\n    return and_(geom_filter, attr_filter)", "response": "Create a MapFish filter based on the request params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving some attributes from the feature and set the geometry to None if they are not set.", "response": "def _filter_attrs(self, feature, request):\n        \"\"\" Remove some attributes from the feature and set the geometry to None\n            in the feature based ``attrs`` and the ``no_geom`` parameters. \"\"\"\n        if 'attrs' in request.params:\n            attrs = request.params['attrs'].split(',')\n            props = feature.properties\n            new_props = {}\n            for name in attrs:\n                if name in props:\n                    new_props[name] = props[name]\n            feature.properties = new_props\n        if asbool(request.params.get('no_geom', False)):\n            feature.geometry = None\n        return feature"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_order_by(self, request):\n        attr = request.params.get('sort', request.params.get('order_by'))\n        if attr is None or not hasattr(self.mapped_class, attr):\n            return None\n        if request.params.get('dir', '').upper() == 'DESC':\n            return desc(getattr(self.mapped_class, attr))\n        else:\n            return asc(getattr(self.mapped_class, attr))", "response": "Return an SA order_by"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _query(self, request, filter=None):\n        limit = None\n        offset = None\n        if 'maxfeatures' in request.params:\n            limit = int(request.params['maxfeatures'])\n        if 'limit' in request.params:\n            limit = int(request.params['limit'])\n        if 'offset' in request.params:\n            offset = int(request.params['offset'])\n        if filter is None:\n            filter = create_filter(request, self.mapped_class, self.geom_attr)\n        query = self.Session().query(self.mapped_class)\n        if filter is not None:\n            query = query.filter(filter)\n        order_by = self._get_order_by(request)\n        if order_by is not None:\n            query = query.order_by(order_by)\n        query = query.limit(limit).offset(offset)\n        return query.all()", "response": "Builds a query based on the filter and the request params and sends it to the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of records matching the given filter.", "response": "def count(self, request, filter=None):\n        \"\"\" Return the number of records matching the given filter. \"\"\"\n        if filter is None:\n            filter = create_filter(request, self.mapped_class, self.geom_attr)\n        query = self.Session().query(self.mapped_class)\n        if filter is not None:\n            query = query.filter(filter)\n        return query.count()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, request, filter=None, id=None):\n        ret = None\n        if id is not None:\n            o = self.Session().query(self.mapped_class).get(id)\n            if o is None:\n                return HTTPNotFound()\n            # FIXME: we return a Feature here, not a mapped object, do\n            # we really want that?\n            ret = self._filter_attrs(o.__geo_interface__, request)\n        else:\n            objs = self._query(request, filter)\n            ret = FeatureCollection(\n                [self._filter_attrs(o.__geo_interface__, request)\n                 for o in objs if o is not None])\n        return ret", "response": "Read a single entry from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the GeoJSON feature collection from the request body and create new objects in the database.", "response": "def create(self, request):\n        \"\"\" Read the GeoJSON feature collection from the request body and\n            create new objects in the database. \"\"\"\n        if self.readonly:\n            return HTTPMethodNotAllowed(headers={'Allow': 'GET, HEAD'})\n        collection = loads(request.body, object_hook=GeoJSON.to_instance)\n        if not isinstance(collection, FeatureCollection):\n            return HTTPBadRequest()\n        session = self.Session()\n        objects = []\n        for feature in collection.features:\n            create = False\n            obj = None\n            if hasattr(feature, 'id') and feature.id is not None:\n                obj = session.query(self.mapped_class).get(feature.id)\n            if self.before_create is not None:\n                self.before_create(request, feature, obj)\n            if obj is None:\n                obj = self.mapped_class(feature)\n                create = True\n            else:\n                obj.__update__(feature)\n            if create:\n                session.add(obj)\n            objects.append(obj)\n        session.flush()\n        collection = FeatureCollection(objects) if len(objects) > 0 else None\n        request.response.status_int = 201\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, request, id):\n        if self.readonly:\n            return HTTPMethodNotAllowed(headers={'Allow': 'GET, HEAD'})\n        session = self.Session()\n        obj = session.query(self.mapped_class).get(id)\n        if obj is None:\n            return HTTPNotFound()\n        feature = loads(request.body, object_hook=GeoJSON.to_instance)\n        if not isinstance(feature, Feature):\n            return HTTPBadRequest()\n        if self.before_update is not None:\n            self.before_update(request, feature, obj)\n        obj.__update__(feature)\n        session.flush()\n        request.response.status_int = 200\n        return obj", "response": "Read the GeoJSON feature from the request body and update the corresponding object in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the targeted feature from the database", "response": "def delete(self, request, id):\n        \"\"\" Remove the targeted feature from the database \"\"\"\n        if self.readonly:\n            return HTTPMethodNotAllowed(headers={'Allow': 'GET, HEAD'})\n        session = self.Session()\n        obj = session.query(self.mapped_class).get(id)\n        if obj is None:\n            return HTTPNotFound()\n        if self.before_delete is not None:\n            self.before_delete(request, obj)\n        session.delete(obj)\n        return Response(status_int=204)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store(self, text, tier):\n        store = self._stores.get(tier, None)\n        if not store:\n            store = AutoSplittingFile(self._dir, self._lines_per_store, self._file_name, tier)\n            self._stores[tier] = store\n        store.write(text)", "response": "Writes the given text to the underlying Store mapped at tier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a UUID with the specified time.", "response": "def uuid_from_kronos_time(time, _type=UUIDType.RANDOM):\n  \"\"\"\n  Generate a UUID with the specified time.\n  If `lowest` is true, return the lexicographically first UUID for the specified\n  time.\n  \"\"\"\n  return timeuuid_from_time(int(time) + UUID_TIME_OFFSET, type=_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef by(self, technology):\n        if technology == PluginTechnology.LV2 \\\n        or str(technology).upper() == PluginTechnology.LV2.value.upper():\n            return self.lv2_builder.all\n        else:\n            return []", "response": "Get the plugins registered in PedalPi by technology."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reload_lv2_plugins_data(self):\n        plugins_data = self.lv2_builder.lv2_plugins_data()\n        self._dao.save(plugins_data)", "response": "Reload LV2 audio plugins data from the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invert(self):\n        '''\n        Invert by swapping each value with its key.\n\n        Returns\n        -------\n        MultiDict\n            Inverted multi-dict.\n\n        Examples\n        --------\n        >>> MultiDict({1: {1}, 2: {1,2,3}}, 4: {}).invert()\n        MultiDict({1: {1,2}, 2: {2}, 3: {2}})\n        '''\n        result = defaultdict(set)\n        for k, val in self.items():\n            result[val].add(k)\n        return MultiDict(dict(result))", "response": "Invert by swapping each value with its key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_with_auth(values, secret_key, url):\n\n  data = urllib.urlencode(values)\n\n  # Simulate a Flask request because that is what will be unpacked when the\n  # request is received on the other side\n  request = Request.from_values(\n    content_length=len(data),\n    input_stream=StringIO(data),\n    content_type='application/x-www-form-urlencoded',\n    method='POST')\n\n  # Add the auth_token, re-encode, and send\n  values['auth_token'] = create_token(secret_key, dict(request.form))\n  data = urllib.urlencode(values)\n  req = urllib2.Request(url, data)\n  response = urllib2.urlopen(req)\n  return json.loads(response.read())", "response": "Send a dictionary of JSON serializable values along with an auth_token that s generated from secret_key and values as a POST body to url and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schedule(code, interval, secret_key=None, url=None):\n  if not secret_key:\n    secret_key = default_key()\n  if not url:\n    url = default_url()\n\n  url = '%s/schedule' % url\n  values = {\n    'interval': interval,\n    'code': code,\n  }\n  return _send_with_auth(values, secret_key, url)", "response": "Schedule a string of code to be executed every interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel(task_id, secret_key=None, url=None):\n  if not secret_key:\n    secret_key = default_key()\n  if not url:\n    url = default_url()\n\n  url = '%s/cancel' % url\n  values = {\n    'id': task_id,\n  }\n  return _send_with_auth(values, secret_key, url)", "response": "Cancels scheduled task with task_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_multireg_file(f, title=None):\n    f.seek(0x26)\n    nparts = struct.unpack('<H', f.read(2))[0]\n    foff = 0x2D\n    if title is None:\n        data = []\n        for _ in range(nparts):\n            d = read_reg_file(f, foff)\n            data.append(d)\n            foff = f.tell() + 1\n    else:\n        for _ in range(nparts):\n            d = read_reg_file(f, foff)\n            if d.get('Title') == title:\n                data = d\n                break\n            foff = f.tell() + 1\n        else:\n            data = {}\n    return data", "response": "Reads a multireg file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a single - style Agilent. REG file and return a dict of key - value pairs.", "response": "def read_reg_file(f, foff=0x2D):\n    \"\"\"\n    Given a file handle for an old-style Agilent *.REG file, this\n    will parse that file into a dictonary of key/value pairs\n    (including any tables that are in the *.REG file, which will\n    be parsed into lists of lists).\n    \"\"\"\n    # convenience function for reading in data\n    def rd(st):\n        return struct.unpack(st, f.read(struct.calcsize(st)))\n\n    f.seek(0x19)\n    if f.read(1) != b'A':\n        # raise TypeError(\"Version of REG file is too new.\")\n        return {}\n\n    f.seek(foff)\n    nrecs = rd('<I')[0]  # TODO: should be '<H'\n    rec_tab = [rd('<HHIII') for n in range(nrecs)]\n\n    names = {}\n    f.seek(foff + 20 * nrecs + 4)\n    for r in rec_tab:\n        d = f.read(r[2])\n        if r[1] == 1539:  # '0306'\n            # this is part of the linked list too, but contains a\n            # reference to a table\n            cd = struct.unpack('<HIII21sI', d)\n            names[cd[5]] = cd[4].decode('iso8859').strip('\\x00')\n            # except:\n            #     pass\n        elif r[1] == 32769 or r[1] == 32771:  # b'0180' or b'0380'\n            names[r[4]] = d[:-1].decode('iso8859')\n        elif r[1] == 32774:  # b'0680'\n            # this is a string that is referenced elsewhere (in a table)\n            names[r[4]] = d[2:-1].decode('iso8859')\n        elif r[1] == 32770:  # b'0280'\n            # this is just a flattened numeric array\n            names[r[4]] = np.frombuffer(d, dtype=np.uint32, offset=4)\n\n    data = {}\n    f.seek(foff + 20 * nrecs + 4)\n    for r in rec_tab:\n        d = f.read(r[2])\n        if r[1] == 1538:  # '0206'\n            # this is part of a linked list\n            if len(d) == 43:\n                cd = struct.unpack('<HIII21sd', d)\n                data[cd[4].decode('iso8859').strip('\\x00')] = cd[5]\n            else:\n                pass\n        elif r[1] == 1537:  # b'0106'\n            # name of property\n            n = d[14:30].split(b'\\x00')[0].decode('iso8859')\n            # with value from names\n            data[n] = names.get(struct.unpack('<I', d[35:39])[0], '')\n        elif r[1] == 1793:  # b'0107'\n            # this is a table of values\n            nrow = struct.unpack('<H', d[4:6])[0]\n            ncol = struct.unpack('<H', d[16:18])[0]\n            if ncol != 0:\n                cols = [struct.unpack('<16sHHHHHI', d[20 + 30 * i:50 + 30 * i])\n                        for i in range(ncol)]\n                colnames = [c[0].split(b'\\x00')[0].decode('iso8859')\n                            for c in cols]\n                # TODO: type 2 is not a constant size? 31, 17\n                rty2sty = {1: 'H', 3: 'I', 4: 'f', 5: 'H',\n                           7: 'H', 8: 'd', 11: 'H', 12: 'H',\n                           13: 'I', 14: 'I', 16: 'H'}\n                coltype = '<' + ''.join([rty2sty.get(c[3], str(c[2]) + 's')\n                                         for c in cols])\n                lencol = struct.calcsize(coltype)\n                tab = []\n                for i in reversed(range(2, nrow + 2)):\n                    rawrow = struct.unpack(coltype,\n                                           d[-i * lencol: (1 - i) * lencol])\n                    row = []\n                    for j, p in enumerate(rawrow):\n                        if cols[j][3] == 3:\n                            row.append(names.get(p, str(p)))\n                        else:\n                            row.append(p)\n                    tab.append(row)\n                data[names[r[4]]] = [colnames, tab]\n        elif r[1] == 1281 or r[1] == 1283:  # b'0105' or b'0305'\n            fm = '<HHBIIhIdII12shIddQQB8sII12shIddQQB8s'\n            m = struct.unpack(fm, d)\n            nrecs = m[4]  # number of points in table\n\n            # x_units = names.get(m[8], '')\n            x_arr = m[14] * names.get(m[9], np.arange(nrecs - 1))\n            y_arr = m[25] * names.get(m[20])\n            y_units = names.get(m[19], '')\n            if y_units == 'bar':\n                y_arr *= 0.1  # convert to MPa\n            # TODO: what to call this?\n            data['Trace'] = Trace(y_arr, x_arr, name='')\n        # elif r[1] == 1025:  # b'0104'\n        #     # lots of zeros? maybe one or two numbers?\n        #     # only found in REG entries that have long 0280 records\n        #     fm = '<HQQQIHHHHIIHB'\n        #     m = struct.unpack(fm, d)\n        #     print(m)\n        #     #print(r[1], len(d), binascii.hexlify(d))\n        #     pass\n        # elif r[1] == 512:  # b'0002'\n        #     # either points to two null pointers or two other pointers\n        #     # (indicates start of linked list?)\n        #     print(r[1], len(d), binascii.hexlify(d))\n        # elif r[1] == 769 or r[1] == 772:  # b'0103' or b'0403'\n        #     # points to 2nd, 3rd & 4th records (two 0002 records and a 0180)\n        #     b = binascii.hexlify\n        #     print(b(d[10:14]), b(d[14:18]), b(d[18:22]))\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a specific team.", "response": "def get(cls, session, team_id):\n        \"\"\"Return a specific team.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            team_id (int): The ID of the team to get.\n\n        Returns:\n            helpscout.models.Person: A person singleton representing the team,\n                if existing. Otherwise ``None``.\n        \"\"\"\n        return cls(\n            '/teams/%d.json' % team_id,\n            singleton=True,\n            session=session,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the members for the team.", "response": "def get_members(cls, session, team_or_id):\n        \"\"\"List the members for the team.\n\n        Args:\n            team_or_id (helpscout.models.Person or int): Team or the ID of\n                the team to get the folders for.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.models.Users): Users\n                iterator.\n        \"\"\"\n        if isinstance(team_or_id, Person):\n            team_or_id = team_or_id.id\n        return cls(\n            '/teams/%d/members.json' % team_or_id,\n            session=session,\n            out_type=User,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert XML xs : duration type to decimal minutes", "response": "def t_to_min(x):\n    \"\"\"\n    Convert XML 'xs: duration type' to decimal minutes, e.g.:\n    t_to_min('PT1H2M30S') == 62.5\n    \"\"\"\n    g = re.match('PT(?:(.*)H)?(?:(.*)M)?(?:(.*)S)?', x).groups()\n    return sum(0 if g[i] is None else float(g[i]) * 60. ** (1 - i)\n               for i in range(3))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_mzxml(filename, df, info=None, precision='f'):\n    for r in df.values:\n        df.columns\n        pass", "response": "Write a MZXML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_binary(self, ba, param_groups=None):\n        if ba is None:\n            return []\n\n        pgr = ba.find('m:referenceableParamGroupRef', namespaces=self.ns)\n        if pgr is not None and param_groups is not None:\n            q = 'm:referenceableParamGroup[@id=\"' + pgr.get('ref') + '\"]'\n            pg = param_groups.find(q, namespaces=self.ns)\n        else:\n            pg = ba\n\n        if pg.find('m:cvParam[@accession=\"MS:1000574\"]',\n                   namespaces=self.ns) is not None:\n            compress = True\n        elif pg.find('m:cvParam[@accession=\"MS:1000576\"]',\n                     namespaces=self.ns) is not None:\n            compress = False\n        else:\n            # TODO: no info? should check the other record?\n            pass\n\n        if pg.find('m:cvParam[@accession=\"MS:1000521\"]',\n                   namespaces=self.ns) is not None:\n            dtype = 'f'\n        elif pg.find('m:cvParam[@accession=\"MS:1000523\"]',\n                     namespaces=self.ns) is not None:\n            dtype = 'd'\n        else:\n            # TODO: no info? should check the other record?\n            pass\n\n        datatext = ba.find('m:binary', namespaces=self.ns).text\n        if compress:\n            rawdata = zlib.decompress(base64.b64decode(datatext))\n        else:\n            rawdata = base64.b64decode(datatext)\n        return np.fromstring(rawdata, dtype=dtype)", "response": "Reads the binary data array from the XML node ba."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_memory_info():\n    '''\n    Pretty format memory info.\n\n    Returns\n    -------\n    str\n        Memory info.\n\n    Examples\n    --------\n    >>> pretty_memory_info()\n    '5MB memory usage'\n    '''\n    process = psutil.Process(os.getpid())\n    return '{}MB memory usage'.format(int(process.memory_info().rss / 2**20))", "response": "Pretty format memory info.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninverts a series into a single index.", "response": "def invert(series):\n    '''\n    Swap index with values of series.\n\n    Parameters\n    ----------\n    series : ~pandas.Series\n        Series to swap on, must have a name.\n\n    Returns\n    -------\n    ~pandas.Series\n        Series after swap.\n\n    See also\n    --------\n    pandas.Series.map\n        Joins series ``a -> b`` and ``b -> c`` into ``a -> c``.\n    '''\n    df = series.reset_index() #TODO alt is to to_frame and then use som dataframe methods\n    df.set_index(series.name, inplace=True)\n    return df[df.columns[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(series):\n    '''\n    Split values.\n\n    The index is dropped, but this may change in the future.\n\n    Parameters\n    ----------\n    series : ~pandas.Series[~pytil.numpy.ArrayLike]\n        Series with array-like values.\n\n    Returns\n    -------\n    ~pandas.Series\n        Series with values split across rows.\n\n    Examples\n    --------\n    >>> series = pd.Series([[1,2],[1,2],[3,4,5]])\n    >>> series\n    0       [1, 2]\n    1       [1, 2]\n    2    [3, 4, 5]\n    dtype: object\n    >>> split(series)\n    0    1\n    1    2\n    2    1\n    3    2\n    4    3\n    5    4\n    6    5\n    dtype: object\n    '''\n    s = df_.split_array_like(series.apply(list).to_frame('column'), 'column')['column']\n    s.name = series.name\n    return s", "response": "Splits a series into a single tree of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef equals(series1, series2, ignore_order=False, ignore_index=False, all_close=False, _return_reason=False):\n    '''\n    Get whether 2 series are equal.\n\n    ``NaN`` is considered equal to ``NaN`` and `None`.\n\n    Parameters\n    ----------\n    series1 : pandas.Series\n        Series to compare.\n    series2 : pandas.Series\n        Series to compare.\n    ignore_order : bool\n        Ignore order of values (and index).\n    ignore_index : bool\n        Ignore index values and name.\n    all_close : bool\n        If `False`, values must match exactly, if `True`, floats are compared as if\n        compared with `numpy.isclose`.\n    _return_reason : bool\n        Internal. If `True`, `equals` returns a tuple containing the reason, else\n        `equals` only returns a bool indicating equality (or equivalence\n        rather).\n\n    Returns\n    -------\n    bool\n        Whether they are equal (after ignoring according to the parameters).\n\n        Internal note: if ``_return_reason``, ``Tuple[bool, str or None]`` is\n        returned. The former is whether they're equal, the latter is `None` if\n        equal or a short explanation of why the series aren't equal, otherwise.\n\n    Notes\n    -----\n    All values (including those of indices) must be copyable and ``__eq__`` must\n    be such that a copy must equal its original. A value must equal itself\n    unless it's ``NaN``. Values needn't be orderable or hashable (however\n    pandas requires index values to be orderable and hashable). By consequence,\n    this is not an efficient function, but it is flexible.\n    '''\n    result = _equals(series1, series2, ignore_order, ignore_index, all_close)\n    if _return_reason:\n        return result\n    else:\n        return result[0]", "response": "Returns True if two series are equal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_equals(actual, expected, ignore_order=False, ignore_index=False, all_close=False):\n    '''\n    Assert 2 series are equal.\n\n    Like ``assert equals(series1, series2, ...)``, but with better hints at\n    where the series differ. See `equals` for\n    detailed parameter doc.\n\n    Parameters\n    ----------\n    actual : ~pandas.Series\n    expected : ~pandas.Series\n    ignore_order : bool\n    ignore_index : bool\n    all_close : bool\n    '''\n    equals_, reason = equals(actual, expected, ignore_order, ignore_index, all_close, _return_reason=True)\n    assert equals_, '{}\\n\\n{}\\n\\n{}'.format(reason, actual.to_string(), expected.to_string())", "response": "A basic assertion that two series are equal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenize(self, message, max_length, mentions=None):\n        mention_text = ''\n        mention_length = 0\n        if mentions:\n            formatted_mentions = ['@{0}'.format(mention) for mention in mentions]\n            mention_text = \" \".join(formatted_mentions)\n            message = '{0} {1}'.format(mention_text, message)\n            mention_length = len(mention_text) + 1\n        if len(message) <= max_length:\n            return [message]\n\n        tokens = message.split(' ')\n        indices = []\n        index = 1\n        length = len(tokens[0])\n        while index < len(tokens):\n            # 1 for leading space, 4 for trailing \" ...\"\n            if length + 1 + len(tokens[index]) + 4 > max_length:\n                indices.append(index)\n                # 4 for leading \"... \"\n                length = 4 + mention_length + len(tokens[index])\n            else:\n                # 1 for leading space\n                length += 1 + len(tokens[index])\n            index += 1\n        indices.append(index)\n\n        messages = [\" \".join(tokens[0:indices[0]])]\n        for i in range(1, len(indices)):\n            messages[i - 1] += ' ...'\n            parts = []\n            if mention_text:\n                parts.append(mention_text)\n            parts.append(\"...\")\n            parts.extend(tokens[indices[i - 1]:indices[i]])\n            messages.append(\" \".join(parts))\n\n        return messages", "response": "Tokenize a message into a list of messages of no more than max_length including mentions\n            in each message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a message to twitter with appropriate mentions.", "response": "def send_message(self, message, mention_id=None, mentions=[]):\n        \"\"\"\n        Send the specified message to twitter, with appropriate mentions, tokenized as necessary\n        :param message: Message to be sent\n        :param mention_id: In-reply-to mention_id (to link messages to a previous message)\n        :param mentions: List of usernames to mention in reply\n        :return:\n        \"\"\"\n        messages = self.tokenize(message, self.MESSAGE_LENGTH, mentions)\n        code = 0\n        for message in messages:\n            if self.dry_run:\n                mention_message = ''\n                if mention_id:\n                    mention_message = \" to mention_id '{0}'\".format(mention_id)\n                logging.info(\"Not posting to Twitter because DRY_RUN is set. Would have posted \"\n                             \"the following message{0}:\\n{1}\".format(mention_message, message))\n            else:\n                try:\n                    self.twitter.statuses.update(status=message,\n                                                 in_reply_to_status_id=mention_id)\n                except TwitterHTTPError as e:\n                    logging.error('Unable to post to twitter: {0}'.format(e))\n                    code = e.response_data['errors'][0]['code']\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a sorted list of unique usernames mentioned in the message excluding the bot s own name", "response": "def get_reply_to_names(self, mention):\n        \"\"\"\n        Get a sorted list of unique usernames mentioned in the message, excluding the bot's own name\n        :param mention: JSON mention object from twitter\n        :return: list of usernames\n        \"\"\"\n        mention_list = [user['screen_name'] for user in mention['entities']['user_mentions']]\n        mention_list.append(mention['user']['screen_name'])\n        reply_to_names = set(mention_list)\n        # Do not include bot's own name\n        reply_to_names.discard(self.screen_name)\n        return sorted(list(reply_to_names))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reply_to_mentions(self):\n        since_id = self.since_id.get()\n\n        kwargs = {'count': 200}\n        if since_id:\n            kwargs['since_id'] = since_id\n\n        mentions_list = []\n        try:\n            mentions_list = self.twitter.statuses.mentions_timeline(**kwargs)\n        except TwitterHTTPError as e:\n            logging.error('Unable to retrieve mentions from twitter: {0}'.format(e))\n\n        logging.info(\"Retrieved {0} mentions\".format(len(mentions_list)))\n\n        mentions_processed = 0\n        # We want to process least recent to most recent, so that since_id is set properly\n        for mention in reversed(mentions_list):\n            mention_id = mention['id']\n            reply_to_names = self.get_reply_to_names(mention)\n\n            error_code = self.DUPLICATE_CODE\n            tries = 0\n            message = ''\n            while error_code == self.DUPLICATE_CODE:\n                if tries > 10:\n                    logging.error('Unable to post duplicate message to {0}: {1}'.format(\n                                  reply_to_names, message))\n                    break\n                elif tries == 10:\n                    # Tried 10 times to post a message, but all were duplicates\n                    message = 'No unique messages found.'\n                else:\n                    message = self.messages.create(mention, self.MESSAGE_LENGTH)\n                error_code = self.send_message(message, mention_id, reply_to_names)\n                tries += 1\n\n            mentions_processed += 1\n            self.since_id.set('{0}'.format(mention_id))\n\n        return mentions_processed", "response": "Get a list of mentions and reply to the names of the mentions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the specified command using a TwitterBot created with the provided settings", "response": "def go(self, settings, command):\n        \"\"\"\n        Run the specified command using a TwitterBot created with the provided settings\n        :param settings: Settings class\n        :param command: Command to run, either 'post_message' or 'reply_to_mentions'\n        :return: Result of running the command\n        \"\"\"\n        bot = TwitterBot(settings)\n\n        result = 1\n        if command == 'post_message':\n            result = bot.post_message()\n        elif command == 'reply_to_mentions':\n            result = bot.reply_to_mentions()\n        else:\n            print(\"Command must be either 'post_message' or 'reply_to_mentions'\")\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a slice of the incoming array filtered between the two times specified.", "response": "def _slice_idxs(df, twin=None):\n    \"\"\"\n    Returns a slice of the incoming array filtered between\n    the two times specified. Assumes the array is the same\n    length as self.data. Acts in the time() and trace() functions.\n    \"\"\"\n    if twin is None:\n        return 0, df.shape[0]\n\n    tme = df.index\n\n    if twin[0] is None:\n        st_idx = 0\n    else:\n        st_idx = (np.abs(tme - twin[0])).argmin()\n    if twin[1] is None:\n        en_idx = df.shape[0]\n    else:\n        en_idx = (np.abs(tme - twin[1])).argmin() + 1\n    return st_idx, en_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _apply_data(self, f, ts, reverse=False):\n        # TODO: needs to catch np numeric types?\n        if isinstance(ts, (int, float)):\n            d = ts * np.ones(self.shape[0])\n        elif ts is None:\n            d = None\n        elif np.array_equal(ts.index, self.index):\n            d = ts.values\n        else:\n            d = ts._retime(self.index)\n\n        if not reverse:\n            new_data = np.apply_along_axis(f, 0, self.values, d)\n        else:\n            new_data = np.apply_along_axis(f, 0, d, self.values)\n        return Trace(new_data, self.index, name=self.name)", "response": "Applies the data f to the trace and returns a Trace object with the new data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traces(self):\n        traces = []\n        for v, c in zip(self.values.T, self.columns):\n            traces.append(Trace(v, self.index, name=c))\n        return traces", "response": "Returns a list of Traces."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, style='heatmap', legend=False, cmap=None, ax=None):\n        # styles: 2d, colors, otherwise interpret as trace?\n        if ax is None:\n            import matplotlib.pyplot as plt\n            ax = plt.gca()\n\n        if style == 'heatmap':\n            ions = self.columns\n            ext = (self.index[0], self.index[-1], min(ions), max(ions))\n            grid = self.values[:, np.argsort(self.columns)].transpose()\n            if isinstance(self.values, scipy.sparse.spmatrix):\n                grid = grid.toarray()\n            img = ax.imshow(grid, origin='lower', aspect='auto',\n                            extent=ext, cmap=cmap)\n            if legend:\n                ax.figure.colorbar(img)\n        elif style == 'colors':\n            # TODO: importing gaussian at the top leads to a whole\n            # mess of dependency issues => fix somehow?\n            from aston.peak.peak_models import gaussian\n            from matplotlib.colors import ListedColormap\n\n            wvs = np.genfromtxt(np.array(self.columns).astype(bytes))\n            # wvs = self.columns.astype(float)\n\n            # http://www.ppsloan.org/publications/XYZJCGT.pdf\n            vis_filt = np.zeros((3, len(wvs)))\n            vis_filt[0] = 1.065 * gaussian(wvs, x=595.8, w=33.33) + \\\n                0.366 * gaussian(wvs, x=446.8, w=19.44)\n            vis_filt[1] = 1.014 * gaussian(np.log(wvs), x=np.log(556.3),\n                                           w=0.075)\n            vis_filt[2] = 1.839 * gaussian(np.log(wvs), x=np.log(449.8),\n                                           w=0.051)\n            if isinstance(self.values, scipy.sparse.spmatrix):\n                xyz = np.dot(self.values.toarray(), vis_filt.T)\n            else:\n                xyz = np.dot(self.values.copy(), vis_filt.T)\n\n            # http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n            xyz_rgb = [[3.2404542, -1.5371385, -0.4985314],\n                       [-0.9692660, 1.8760108, 0.0415560],\n                       [0.0556434, -0.2040259, 1.0572252]]\n            xyz_rgb = np.array(xyz_rgb)\n            rgb = np.dot(xyz_rgb, xyz.T).T\n\n            # normalize\n            rgb[rgb < 0] = 0\n            rgb /= np.max(rgb)\n            rgb = 1 - np.abs(rgb)\n\n            # plot\n            cmask = np.meshgrid(np.arange(rgb.shape[0]), 0)[0]\n            ax.imshow(cmask, cmap=ListedColormap(rgb), aspect='auto',\n                      extent=(self.index[0], self.index[-1], 0, 1))\n            ax.yaxis.set_ticks([])\n        else:\n            if cmap is not None:\n                color = cmap(0, 1)\n            else:\n                color = 'k'\n            self.trace().plot(color=color, ax=ax)", "response": "Plots the AstonFrame using matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the AstonFrame into a sound file.", "response": "def as_sound(self, filename, speed=60, cutoff=50):\n        \"\"\"\n        Convert AstonFrame into a WAV file.\n\n        Parameters\n        ----------\n        filename : str\n            Name of wavfile to create.\n        speed : float, optional\n            How much to speed up for sound recording, e.g. a value of 60\n            will turn an hour-long AstonFrame into a minute-long sound clip.\n        cutoff : float, optional\n            m/z's under this value will be clipped out.\n        \"\"\"\n        # make a 1d array for the sound\n        def to_t(t):\n            return (t - self.index[0]) / speed\n\n        wav_len = int(to_t(self.index[-1]) * 60 * 44100)\n        wav = np.zeros(wav_len)\n\n        # create an artificial array to interpolate times out of\n        tmask = np.linspace(0, 1, self.shape[0])\n\n        # come up with a mapping from mz to tone\n        min_hz, max_hz = 50, 1000\n        min_mz, max_mz = min(self.columns), max(self.columns)\n\n        def mz_to_wv(mz):\n            \"\"\"\n            Maps a wavelength/mz to a tone.\n            \"\"\"\n            try:\n                mz = float(mz)\n            except:\n                return 100\n            wv = (mz * (max_hz - min_hz) -\n                  max_hz * min_mz + min_hz * max_mz) / (max_mz - min_mz)\n            return int(44100 / wv)\n\n        # go through each trace and map it into the sound array\n        for i, mz in enumerate(self.columns):\n            if float(mz) < cutoff:\n                # clip out mz/wv below a certain threshold\n                # handy if data has low level noise\n                continue\n            print(str(i) + '/' + str(self.shape[1]))\n            inter_x = np.linspace(0, 1, wav[::mz_to_wv(mz)].shape[0])\n            wav[::mz_to_wv(mz)] += np.interp(inter_x, tmask, self.values[:, i])\n\n        # scale the new array and write it out\n        scaled = wav / np.max(np.abs(wav))\n        scaled = scipy.signal.fftconvolve(scaled, np.ones(5) / 5, mode='same')\n        scaled = np.int16(scaled * 32767)\n        scipy.io.wavfile.write(filename, 44100, scaled)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan(self, t, dt=None, aggfunc=None):\n        idx = (np.abs(self.index - t)).argmin()\n\n        if dt is None:\n            # only take the spectra at the nearest time\n            mz_abn = self.values[idx, :].copy()\n        else:\n            # sum up all the spectra over a range\n            en_idx = (np.abs(self.index - t - dt)).argmin()\n            idx, en_idx = min(idx, en_idx), max(idx, en_idx)\n            if aggfunc is None:\n                mz_abn = self.values[idx:en_idx + 1, :].copy().sum(axis=0)\n            else:\n                mz_abn = aggfunc(self.values[idx:en_idx + 1, :].copy())\n        if isinstance(mz_abn, scipy.sparse.spmatrix):\n            mz_abn = mz_abn.toarray()[0]\n        return Scan(self.columns, mz_abn)", "response": "Returns the spectrum from a specific time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_cassandra(self, namespaces):\n    connections_to_shutdown = []\n    self.cluster = Cluster(self.hosts)\n\n    for namespace_name in namespaces:\n      keyspace = '%s_%s' % (self.keyspace_prefix, namespace_name)\n      namespace = Namespace(self.cluster, keyspace,\n                            self.replication_factor, self.read_size)\n      connections_to_shutdown.append(namespace.session)\n      self.namespaces[namespace_name] = namespace\n\n    # Shutdown Cluster instance after shutting down all Sessions.\n    connections_to_shutdown.append(self.cluster)\n\n    # Shutdown all connections to Cassandra before exiting Python interpretter.\n    atexit.register(lambda: map(lambda c: c.shutdown(),\n                                connections_to_shutdown))", "response": "Setup a connection to the specified Cassandra cluster and create the specified keyspaces if they don t exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _delete(self, namespace, stream, start_id, end_time, configuration):\n    stream = self.get_stream(namespace, stream, configuration)\n    return stream.delete(start_id,\n                         uuid_from_kronos_time(end_time,\n                                               _type=UUIDType.HIGHEST))", "response": "Delete events for a given stream between start_id and end_time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _retrieve(self, namespace, stream, start_id, end_time, order, limit,\n                configuration):\n    \"\"\"\n    Retrieve events for `stream` between `start_id` and `end_time`.\n    `stream` : The stream to return events for.\n    `start_id` : Return events with id > `start_id`.\n    `end_time` : Return events ending <= `end_time`.\n    `order` : Whether to return the results in ResultOrder.ASCENDING\n              or ResultOrder.DESCENDING time-order.\n    `configuration` : A dictionary of settings to override any default\n                      settings, such as number of shards or width of a\n                      time interval.\n    \"\"\"\n    stream = self.get_stream(namespace, stream, configuration)\n    events = stream.iterator(start_id,\n                             uuid_from_kronos_time(end_time,\n                                                   _type=UUIDType.HIGHEST),\n                             order == ResultOrder.DESCENDING, limit)\n    events = events.__iter__()\n    event = events.next()\n    # If first event's ID is equal to `start_id`, skip it.\n    if event.id != start_id:\n      yield event.json\n    while True:\n      yield events.next().json", "response": "Retrieve events for a given stream between start_id and end_time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(environment, start_response, headers):\n  response = {'service': 'kronosd',\n              'version': kronos.__version__,\n              'id': settings.node['id'],\n              'storage': {},\n              SUCCESS_FIELD: True}\n\n  # Check if each backend is alive\n  for name, backend in router.get_backends():\n    response['storage'][name] = {'alive': backend.is_alive(),\n                                 'backend': settings.storage[name]['backend']}\n\n  start_response('200 OK', headers)\n  return response", "response": "Return the status of this Kronos instance + its backends >\n Doesn t expect any URL parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstore events in backends POST body should contain a JSON encoded version of the event.", "response": "def put_events(environment, start_response, headers):\n  \"\"\"\n  Store events in backends\n  POST body should contain a JSON encoded version of:\n    { namespace: namespace_name (optional),\n      events: { stream_name1 : [event1, event2, ...],\n                stream_name2 : [event1, event2, ...],\n                ... }\n    }\n  Where each event is a dictionary of keys and values.\n  \"\"\"\n  errors = []\n  events_to_insert = defaultdict(list)\n  request_json = environment['json']\n  namespace = request_json.get('namespace', settings.default_namespace)\n\n  # Validate streams and events\n  for stream, events in request_json.get('events', {}).iteritems():\n    try:\n      validate_stream(stream)\n    except Exception, e:\n      log.exception('put_events: stream validation failed for `%s`', stream)\n      errors.append(repr(e))\n      continue\n\n    for event in events:\n      try:\n        events_to_insert[stream].append(validate_event_and_assign_id(event))\n      except Exception, e:\n        log.exception('put_events: event validation failed for `%s`', event)\n        errors.append(repr(e))\n\n  results = {}\n  for stream, events in events_to_insert.iteritems():\n    backends = router.backends_to_mutate(namespace, stream)\n    for backend, configuration in backends.iteritems():\n      results[(stream, backend.name)] = execute_greenlet_async(\n        backend.insert, namespace, stream, events, configuration)\n  wait(results.values())\n\n  # Did any insertion fail?\n  success = True\n  response = defaultdict(dict)\n  for (stream, backend), result in results.iteritems():\n    try:\n      result.get()\n      response[stream][backend] = {\n        'num_inserted': len(events_to_insert[stream])\n      }\n    except Exception, e:\n      log.exception('put_events: insertion to backend `%s` failed.', backend)\n      success = False\n      response[stream][backend] = {'num_inserted': -1,\n                                   ERRORS_FIELD: [repr(e)]}\n\n  response[SUCCESS_FIELD] = success and not errors\n  if errors:\n    response[ERRORS_FIELD] = errors\n\n  start_response('200 OK', headers)\n  return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_events(environment, start_response, headers):\n  request_json = environment['json']\n  try:\n    stream = request_json['stream']\n    validate_stream(stream)\n  except Exception, e:\n    log.exception('get_events: stream validation failed for `%s`',\n                  request_json.get('stream'))\n    start_response('400 Bad Request', headers)\n    yield marshal.dumps({ERRORS_FIELD: [repr(e)],\n                         SUCCESS_FIELD: False})\n    return\n\n  namespace = request_json.get('namespace', settings.default_namespace)\n  limit = int(request_json.get('limit', MAX_LIMIT))\n  if limit <= 0:\n    events = []\n  else:\n    backend, configuration = router.backend_to_retrieve(namespace, stream)\n    events = backend.retrieve(\n      namespace,\n      stream,\n      long(request_json.get('start_time', 0)),\n      long(request_json['end_time']),\n      request_json.get('start_id'),\n      configuration,\n      order=request_json.get('order', ResultOrder.ASCENDING),\n      limit=limit)\n\n  start_response('200 OK', headers)\n\n  string_buffer = StringIO()\n  for event in events:\n    # TODO(usmanm): Once all backends start respecting limit, remove this check.\n    if limit <= 0:\n      break\n    if string_buffer.tell() >= settings.node.flush_size:\n      yield string_buffer.getvalue()\n      string_buffer.close()\n      string_buffer = StringIO()\n    string_buffer.write(event)\n    string_buffer.write('\\r\\n')\n    limit -= 1\n  if string_buffer.tell():\n    yield string_buffer.getvalue()\n  string_buffer.close()\n  yield ''", "response": "Retrieves events from Kronos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete events from a stream.", "response": "def delete_events(environment, start_response, headers):\n  \"\"\"\n  Delete events\n  POST body should contain a JSON encoded version of:\n    { namespace: namespace_name (optional),\n      stream : stream_name,\n      start_time : starting_time_as_kronos_time,\n      end_time : ending_time_as_kronos_time,\n      start_id : only_delete_events_with_id_gte_me,\n    }\n  Either start_time or start_id should be specified.\n  \"\"\"\n  request_json = environment['json']\n  try:\n    stream = request_json['stream']\n    validate_stream(stream)\n  except Exception, e:\n    log.exception('delete_events: stream validation failed for `%s`.',\n                  request_json.get('stream'))\n    start_response('400 Bad Request', headers)\n    return {ERRORS_FIELD: [repr(e)]}\n\n  namespace = request_json.get('namespace', settings.default_namespace)\n  backends = router.backends_to_mutate(namespace, stream)\n  statuses = {}\n  for backend, conf in backends.iteritems():\n    statuses[backend.name] = execute_greenlet_async(\n      backend.delete,\n      namespace,\n      stream,\n      long(request_json.get('start_time', 0)),\n      long(request_json['end_time']),\n      request_json.get('start_id'),\n      conf)\n  wait(statuses.values())\n\n  success = True\n  response = {}\n  for backend, status in statuses.iteritems():\n    try:\n      num_deleted, errors = status.get()\n      response[backend] = {'num_deleted': num_deleted}\n      if errors:\n        success = False\n        response[ERRORS_FIELD] = errors\n    except Exception, e:\n      log.exception('delete_events: delete from backend `%s` failed.', backend)\n      success = False\n      response[backend] = {'num_deleted': -1,\n                           ERRORS_FIELD: [repr(e)]}\n\n  response = {request_json['stream']: response,\n              SUCCESS_FIELD: success}\n\n  start_response('200 OK', headers)\n  return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all streams that can be read from Kronos right now.", "response": "def get_streams(environment, start_response, headers):\n  \"\"\"\n  List all streams that can be read from Kronos right now.\n  POST body should contain a JSON encoded version of:\n    { namespace: namespace_name (optional)\n    }\n  \"\"\"\n  start_response('200 OK', headers)\n  streams_seen_so_far = set()\n  namespace = environment['json'].get('namespace', settings.default_namespace)\n  for prefix, backend in router.get_read_backends(namespace):\n    for stream in backend.streams(namespace):\n      if stream.startswith(prefix) and stream not in streams_seen_so_far:\n        streams_seen_so_far.add(stream)\n        yield '{0}\\r\\n'.format(stream)\n  yield ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninferring schema of the requested stream.", "response": "def infer_schema(environment, start_response, headers):\n  \"\"\"\n  Return the inferred schema of the requested stream.\n  POST body should contain a JSON encoded version of:\n    { stream: stream_name,\n      namespace: namespace_name (optional)\n    }\n  \"\"\"\n  stream = environment['json']['stream']\n  namespace = environment['json'].get('namespace') or settings.default_namespace\n\n  start_response('200 OK', headers)\n  schema = _infer_schema(namespace, stream)\n  response = {\n    'stream': stream,\n    'namespace': namespace,\n    'schema': schema,\n    SUCCESS_FIELD: True\n  }\n  return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef devices(self):\n        devices = None\n        with self.socket.Connect():\n            devices = self._command(\"host:devices\")\n\n        return parse_device_list(devices)", "response": "Return a list of connected devices in the form serial status unauthorized device can\n "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_leaf(self, leaf_blob):\n        self.leaves.append(MerkleLeaf(len(self.leaves), leaf_blob))", "response": "Adds a leaf to the list of leaves."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the tree from the leaves that have been added. This function populates the tree from the leaves that have been added.", "response": "def build(self):\n        \"\"\"Builds the tree from the leaves that have been added.\n\n        This function populates the tree from the leaves down non-recursively\n        \"\"\"\n        self.order = MerkleTree.get_order(len(self.leaves))\n        n = 2 ** self.order\n        self.nodes = [b''] * 2 * n\n\n        # populate lowest nodes with leaf hashes\n        for j in range(0, n):\n            if (j < len(self.leaves)):\n                self.nodes[j + n - 1] = self.leaves[j].get_hash()\n            else:\n                break\n\n        # now populate the entire tree\n        for i in range(1, self.order + 1):\n            p = 2 ** (self.order - i)\n            for j in range(0, p):\n                k = p + j - 1\n                h = hashlib.sha256()\n                l = self.nodes[MerkleTree.get_left_child(k)]\n                if (len(l) > 0):\n                    h.update(l)\n                r = self.nodes[MerkleTree.get_right_child(k)]\n                if (len(r) > 0):\n                    h.update(r)\n                self.nodes[k] = h.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_branch(self, i):\n        branch = MerkleBranch(self.order)\n        j = i + 2 ** self.order - 1\n\n        for k in range(0, self.order):\n            if (self.is_left(j)):\n                branch.set_row(k, (self.nodes[j], self.nodes[j + 1]))\n            else:\n                branch.set_row(k, (self.nodes[j - 1], self.nodes[j]))\n            j = MerkleTree.get_parent(j)\n\n        return branch", "response": "This will trace the tree that contains the leaf i and returns a list of tuples that represent the pairs of nodes that are all the way from the leaves down to the root."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_branch(leaf, branch, root):\n        # just check the hashes are correct\n        try:\n            lh = leaf.get_hash()\n        except:\n            return False\n        for i in range(0, branch.get_order()):\n            if (branch.get_left(i) != lh and branch.get_right(i) != lh):\n                return False\n            h = hashlib.sha256()\n            if (len(branch.get_left(i)) > 0):\n                h.update(branch.get_left(i))\n            if (len(branch.get_right(i)) > 0):\n                h.update(branch.get_right(i))\n            lh = h.digest()\n        if (root != lh):\n            return False\n        return True", "response": "This function checks that the given branch fits the given leaf and root and returns True if the branch fits the root and root node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self):\n        id = self.id or self.objects.id(self.name)\n        self.objects[id] = self.prepare_save(dict(self))\n        self.id = id\n        self.post_save()\n        return id", "response": "Save this entry.\n\n        If the entry does not have an :attr:`id`, a new id will be assigned,\n        and the :attr:`id` attribute set accordingly.\n\n        Pre-save processing of the fields saved can be done by\n        overriding the :meth:`prepare_save` method.\n\n        Additional actions to be done after the save operation\n        has been completed can be added by defining the\n        :meth:`post_save` method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_many(self, ids):\n        return [self.instance(id, **fields)\n                    for id, fields in zip(ids, self.api.mget(ids))]", "response": "Get several entries at once."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, **fields):\n        entry = self.instance(**fields)\n        entry.save()\n        return entry", "response": "Create a new entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a urlparse. ParseResult object with the results of parsing the given URI.", "response": "def generous_parse_uri(uri):\n    \"\"\"Return a urlparse.ParseResult object with the results of parsing the\n    given URI. This has the same properties as the result of parse_uri.\n\n    When passed a relative path, it determines the absolute path, sets the\n    scheme to file, the netloc to localhost and returns a parse of the result.\n    \"\"\"\n\n    parse_result = urlparse(uri)\n\n    if parse_result.scheme == '':\n        abspath = os.path.abspath(parse_result.path)\n        if IS_WINDOWS:\n            abspath = windows_to_unix_path(abspath)\n        fixed_uri = \"file://{}\".format(abspath)\n        parse_result = urlparse(fixed_uri)\n\n    return parse_result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the username of the current node.", "response": "def getuser():\n    \"\"\"Return the username.\"\"\"\n    is_windows = platform.system() == \"Windows\"\n    no_username_in_env = os.environ.get(\"USERNAME\") is None\n    return cross_platform_getuser(is_windows, no_username_in_env)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the content of the config file.", "response": "def _get_config_dict_from_file(config_path=None):\n    \"\"\"Return value if key exists in file.\n\n    Return empty string (\"\") if key or file does not exist.\n    \"\"\"\n    if config_path is None:\n        config_path = DEFAULT_CONFIG_PATH\n\n    # Default (empty) content will be used if config file does not exist.\n    config_content = {}\n\n    # If the config file exists we use that content.\n    if os.path.isfile(config_path):\n        with open(config_path) as fh:\n            config_content = json.load(fh)\n\n    return config_content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_config_value_to_file(key, value, config_path=None):\n    if config_path is None:\n        config_path = DEFAULT_CONFIG_PATH\n\n    # Get existing config.\n    config = _get_config_dict_from_file(config_path)\n\n    # Add/update the key/value pair.\n    config[key] = value\n\n    # Create parent directories if they are missing.\n    mkdir_parents(os.path.dirname(config_path))\n\n    # Write the content\n    with open(config_path, \"w\") as fh:\n        json.dump(config, fh, sort_keys=True, indent=2)\n\n    # Set 600 permissions on the config file.\n    os.chmod(config_path, 33216)\n\n    return get_config_value_from_file(key, config_path)", "response": "Write key value pair to config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a value from a config file.", "response": "def get_config_value_from_file(key, config_path=None, default=None):\n    \"\"\"Return value if key exists in file.\n\n    Return default if key not in config.\n    \"\"\"\n    config = _get_config_dict_from_file(config_path)\n    if key not in config:\n        return default\n    return config[key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config_value(key, config_path=None, default=None):\n    if config_path is None:\n        config_path = DEFAULT_CONFIG_PATH\n\n    # Start by setting default value\n    value = default\n\n    # Update from config file\n    value = get_config_value_from_file(\n        key=key,\n        config_path=config_path,\n        default=value\n    )\n\n    # Update from environment variable\n    value = os.environ.get(key, value)\n    return value", "response": "Get a configuration value from a file or environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mkdir_parents(path):\n\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            pass\n        else:\n            raise", "response": "Create the given directory path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning Unix timestamp as float.", "response": "def timestamp(datetime_obj):\n    \"\"\"Return Unix timestamp as float.\n\n    The number of seconds that have elapsed since January 1, 1970.\n    \"\"\"\n    start_of_time = datetime.datetime(1970, 1, 1)\n    diff = datetime_obj - start_of_time\n    return diff.total_seconds()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef name_is_valid(name):\n    # The name can only be 80 characters long.\n    if len(name) > MAX_NAME_LENGTH:\n        return False\n    return bool(NAME_VALID_CHARS_REGEX.match(name))", "response": "Return True if the dataset name is valid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the admin metadata as a dictionary.", "response": "def get_admin_metadata(self):\n        \"\"\"Return the admin metadata as a dictionary.\"\"\"\n        logger.debug(\"Getting admin metdata\")\n        text = self.get_text(self.get_admin_metadata_key())\n        return json.loads(text)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the manifest as a dictionary.", "response": "def get_manifest(self):\n        \"\"\"Return the manifest as a dictionary.\"\"\"\n        logger.debug(\"Getting manifest\")\n        text = self.get_text(self.get_manifest_key())\n        return json.loads(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_overlay(self, overlay_name):\n        logger.debug(\"Getting overlay: {}\".format(overlay_name))\n        overlay_key = self.get_overlay_key(overlay_name)\n        text = self.get_text(overlay_key)\n        return json.loads(text)", "response": "Return overlay as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_admin_metadata(self, admin_metadata):\n        logger.debug(\"Putting admin metdata\")\n        text = json.dumps(admin_metadata)\n        key = self.get_admin_metadata_key()\n        self.put_text(key, text)", "response": "Store the admin metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores the readme descriptive metadata.", "response": "def put_readme(self, content):\n        \"\"\"Store the readme descriptive metadata.\"\"\"\n        logger.debug(\"Putting readme\")\n        key = self.get_readme_key()\n        self.put_text(key, content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the readme descriptive metadata.", "response": "def update_readme(self, content):\n        \"\"\"Update the readme descriptive metadata.\"\"\"\n        logger.debug(\"Updating readme\")\n        key = self.get_readme_key()\n\n        # Back up old README content.\n        backup_content = self.get_readme_content()\n        backup_key = key + \"-{}\".format(\n            timestamp(datetime.datetime.now())\n        )\n        logger.debug(\"README.yml backup key: {}\".format(backup_key))\n        self.put_text(backup_key, backup_content)\n\n        self.put_text(key, content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the properties of the item with the given handle.", "response": "def item_properties(self, handle):\n        \"\"\"Return properties of the item with the given handle.\"\"\"\n        logger.debug(\"Getting properties for handle: {}\".format(handle))\n        properties = {\n            'size_in_bytes': self.get_size_in_bytes(handle),\n            'utc_timestamp': self.get_utc_timestamp(handle),\n            'hash': self.get_hash(handle),\n            'relpath': self.get_relpath(handle)\n        }\n        logger.debug(\"{} properties: {}\".format(handle, properties))\n        return properties"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _document_structure(self):\n        logger.debug(\"Documenting dataset structure\")\n        key = self.get_structure_key()\n        text = json.dumps(self._structure_parameters, indent=2, sort_keys=True)\n        self.put_text(key, text)\n\n        key = self.get_dtool_readme_key()\n        self.put_text(key, self._dtool_readme_txt)", "response": "Document the structure of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_dataset_uris(cls, base_uri, config_path):\n\n        parsed_uri = generous_parse_uri(base_uri)\n        uri_list = []\n\n        path = parsed_uri.path\n        if IS_WINDOWS:\n            path = unix_to_windows_path(parsed_uri.path, parsed_uri.netloc)\n\n        for d in os.listdir(path):\n            dir_path = os.path.join(path, d)\n\n            if not os.path.isdir(dir_path):\n                continue\n\n            storage_broker = cls(dir_path, config_path)\n\n            if not storage_broker.has_admin_metadata():\n                continue\n\n            uri = storage_broker.generate_uri(\n                name=d,\n                uuid=None,\n                base_uri=base_uri\n            )\n            uri_list.append(uri)\n\n        return uri_list", "response": "Return list containing URIs in location given by base_uri."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nputs the text into the storage associated with the key.", "response": "def put_text(self, key, text):\n        \"\"\"Put the text into the storage associated with the key.\"\"\"\n        with open(key, \"w\") as fh:\n            fh.write(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the size in bytes of the file associated with the given handle.", "response": "def get_size_in_bytes(self, handle):\n        \"\"\"Return the size in bytes.\"\"\"\n        fpath = self._fpath_from_handle(handle)\n        return os.stat(fpath).st_size"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_utc_timestamp(self, handle):\n        fpath = self._fpath_from_handle(handle)\n        datetime_obj = datetime.datetime.utcfromtimestamp(\n            os.stat(fpath).st_mtime\n        )\n        return timestamp(datetime_obj)", "response": "Return the UTC timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the hash of the file with the given handle.", "response": "def get_hash(self, handle):\n        \"\"\"Return the hash.\"\"\"\n        fpath = self._fpath_from_handle(handle)\n        return DiskStorageBroker.hasher(fpath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of overlay names.", "response": "def list_overlay_names(self):\n        \"\"\"Return list of overlay names.\"\"\"\n        overlay_names = []\n        if not os.path.isdir(self._overlays_abspath):\n            return overlay_names\n        for fname in os.listdir(self._overlays_abspath):\n            name, ext = os.path.splitext(fname)\n            overlay_names.append(name)\n        return overlay_names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning absolute path at which the item content can be accessed.", "response": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"\n        manifest = self.get_manifest()\n        item = manifest[\"items\"][identifier]\n        item_abspath = os.path.join(self._data_abspath, item[\"relpath\"])\n        return item_abspath"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating necessary structure to hold a dataset.", "response": "def _create_structure(self):\n        \"\"\"Create necessary structure to hold a dataset.\"\"\"\n\n        # Ensure that the specified path does not exist and create it.\n        if os.path.exists(self._abspath):\n            raise(StorageBrokerOSError(\n                \"Path already exists: {}\".format(self._abspath)\n            ))\n\n        # Make sure the parent directory exists.\n        parent, _ = os.path.split(self._abspath)\n        if not os.path.isdir(parent):\n            raise(StorageBrokerOSError(\n                \"No such directory: {}\".format(parent)))\n\n        os.mkdir(self._abspath)\n\n        # Create more essential subdirectories.\n        for abspath in self._essential_subdirectories:\n            if not os.path.isdir(abspath):\n                os.mkdir(abspath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_item(self, fpath, relpath):\n\n        # Define the destination path and make any missing parent directories.\n        dest_path = os.path.join(self._data_abspath, relpath)\n        dirname = os.path.dirname(dest_path)\n        mkdir_parents(dirname)\n\n        # Copy the file across.\n        shutil.copyfile(fpath, dest_path)\n\n        return relpath", "response": "Put an item with content from fpath at relpath in dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn iterator over item handles.", "response": "def iter_item_handles(self):\n        \"\"\"Return iterator over item handles.\"\"\"\n\n        path = self._data_abspath\n        path_length = len(path) + 1\n\n        for dirpath, dirnames, filenames in os.walk(path):\n            for fn in filenames:\n                path = os.path.join(dirpath, fn)\n                relative_path = path[path_length:]\n                if IS_WINDOWS:\n                    relative_path = windows_to_unix_path(relative_path)\n                yield relative_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_item_metadata(self, handle, key, value):\n        if not os.path.isdir(self._metadata_fragments_abspath):\n            os.mkdir(self._metadata_fragments_abspath)\n\n        prefix = self._handle_to_fragment_absprefixpath(handle)\n        fpath = prefix + '.{}.json'.format(key)\n\n        with open(fpath, 'w') as fh:\n            json.dump(value, fh)", "response": "Store the given key value pair for the item associated with handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_item_metadata(self, handle):\n\n        if not os.path.isdir(self._metadata_fragments_abspath):\n            return {}\n\n        prefix = self._handle_to_fragment_absprefixpath(handle)\n\n        def list_abspaths(dirname):\n            for f in os.listdir(dirname):\n                yield os.path.join(dirname, f)\n\n        files = [f for f in list_abspaths(self._metadata_fragments_abspath)\n                 if f.startswith(prefix)]\n\n        metadata = {}\n        for f in files:\n            key = f.split('.')[-2]  # filename: identifier.key.json\n            with open(f) as fh:\n                value = json.load(fh)\n            metadata[key] = value\n\n        return metadata", "response": "Return dictionary containing all the metadata associated with handle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npost :meth:`dtoolcore.ProtoDataSet.freeze` cleanup actions. This method is called at the end of the :meth:`dtoolcore.ProtoDataSet.freeze` method. In the :class:`dtoolcore.storage_broker.DiskStorageBroker` it removes the temporary directory for storing item metadata fragment files.", "response": "def post_freeze_hook(self):\n        \"\"\"Post :meth:`dtoolcore.ProtoDataSet.freeze` cleanup actions.\n\n        This method is called at the end of the\n        :meth:`dtoolcore.ProtoDataSet.freeze` method.\n\n        In the :class:`dtoolcore.storage_broker.DiskStorageBroker` it removes\n        the temporary directory for storing item metadata fragment files.\n        \"\"\"\n        if os.path.isdir(self._metadata_fragments_abspath):\n            shutil.rmtree(self._metadata_fragments_abspath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacquire a token for futher API calls", "response": "def get_token(self):\n        \"\"\" Acquires a token for futher API calls, unless you already have a token this will be the first thing\n            you do before you use this.\n            :param email: string, the username for your EinsteinVision service, usually in email form\n            :para pem_file: string, file containing your Secret key. Copy contents of relevant Config Var\n            on Heroku to a file locally.\n            attention: this will set self.token on success\n            attention: currently spitting out results via a simple print\n            returns: requests object\n        \"\"\"\n        payload = {\n            'aud': API_OAUTH,\n            'exp': time.time()+600, # 10 minutes\n            'sub': self.email\n        }\n\n        header = {'Content-type':'application/x-www-form-urlencoded'}\n\n        assertion = jwt.encode(payload, self.private_key, algorithm='RS256')\n        assertion = assertion.decode('utf-8')\n\n        response = requests.post(\n            url=API_OAUTH,\n            headers=header,\n            data='grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer&assertion='+assertion\n        )\n\n        print(response.text)\n\n        if response.status_code == 200:\n            print('status 200 ok for Token')\n            self.token = response.json()['access_token']\n        else:\n            print('Could not get Token. Status: ' + str(response.status_code))\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_model_info(self, model_id, token=None, url=API_GET_MODEL_INFO):\n        auth = 'Bearer ' + self.check_for_token(token)\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url + '/' + model_id\n        r = requests.get(the_url, headers=h)\n\n        return r", "response": "Get information about a specific previously trained model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting information on all datasets for this account", "response": "def get_datasets_info(self, token=None, url=API_GET_DATASETS_INFO):\n        \"\"\" Gets information on all datasets for this account\n            returns: requests object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n        r = requests.get(the_url, headers=h)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_url_image_prediction(self, model_id, picture_url, token=None, url=API_GET_PREDICTION_IMAGE_URL):\n        auth = 'Bearer ' + self.check_for_token(token)\n        m = MultipartEncoder(fields={'sampleLocation':picture_url, 'modelId':model_id})\n        h = {'Authorization': auth, 'Cache-Control':'no-cache', 'Content-Type':m.content_type}\n        the_url = url\n        r = requests.post(the_url, headers=h, data=m)\n\n        return r", "response": "Gets a prediction from a supplied picture url based on a previously trained model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a prediction from a supplied image on your machine, by encoding the image data as b64 and posting to the API. :param model_id: string, once you train a model you'll be given a model id to use. :param filename: string, the name of a file to be posted to the api. returns: requests object", "response": "def get_fileb64_image_prediction(self, model_id, filename, token=None, url=API_GET_PREDICTION_IMAGE_URL):\n        \"\"\" Gets a prediction from a supplied image on your machine, by encoding the image data as b64\n            and posting to the API.\n            :param model_id: string, once you train a model you'll be given a model id to use.\n            :param filename: string, the name of a file to be posted to the api.\n            returns: requests object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)        \n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n\n        with open(filename, \"rb\") as image_file:\n            encoded_string = base64.b64encode(image_file.read())\n\n        m = MultipartEncoder(fields={'sampleBase64Content':encoded_string, 'modelId':model_id})\n        h = {'Authorization': auth, 'Cache-Control':'no-cache', 'Content-Type':m.content_type}\n        r = requests.post(the_url, headers=h, data=m)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_b64_image_prediction(self, model_id, b64_encoded_string, token=None, url=API_GET_PREDICTION_IMAGE_URL):\n        auth = 'Bearer ' + self.check_for_token(token)        \n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n    \n        encoded_string = b64_encoded_string\n\n        m = MultipartEncoder(fields={'sampleBase64Content':encoded_string, 'modelId':model_id})\n        h = {'Authorization': auth, 'Cache-Control':'no-cache', 'Content-Type':m.content_type}\n        r = requests.post(the_url, headers=h, data=m)\n\n        return r", "response": "This method returns a prediction from a supplied b64 string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_dataset_synchronous(self, file_url, dataset_type='image', token=None, url=API_CREATE_DATASET):\n        auth = 'Bearer ' + self.check_for_token(token)\n        m = MultipartEncoder(fields={'type':dataset_type, 'path':file_url})\n        h = {'Authorization': auth, 'Cache-Control':'no-cache', 'Content-Type':m.content_type}\n        the_url = url\n        r = requests.post(the_url, headers=h, data=m)\n\n        return r", "response": "Create a dataset so you can train models from it\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef train_model(self, dataset_id, model_name, token=None, url=API_TRAIN_MODEL):\n        auth = 'Bearer ' + self.check_for_token(token)\n        m = MultipartEncoder(fields={'name':model_name, 'datasetId':dataset_id})\n        h = {'Authorization': auth, 'Cache-Control':'no-cache', 'Content-Type':m.content_type}\n        the_url = url\n        r = requests.post(the_url, headers=h, data=m)\n\n        return r", "response": "Train a model given a specifi dataset previously created\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_training_status(self, model_id, token=None, url=API_TRAIN_MODEL):\n        auth = 'Bearer ' + self.check_for_token(token)\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url + '/' + model_id\n        r = requests.get(the_url, headers=h)\n\n        return r", "response": "Get the training status of a model in the given url"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting metadata on all models available for a given dataset.", "response": "def get_models_info_for_dataset(self, dataset_id, token=None, url=API_GET_MODELS):\n        \"\"\" Gets metadata on all models available for given dataset id\n            :param dataset_id: string, previously obtained dataset id\n            warning: if providing your own url here, also include the dataset_id in the right place\n            as this method will not include it for you. Otherwise use the dataset_id attribute as \n            per usual\n            returns: a requests object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        if url != API_GET_MODELS:\n            r = requests.get(the_url, headers=h)\n            return r\n\n        the_url = url.replace('<dataset_id>', dataset_id)\n        r = requests.get(the_url, headers=h)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a dataset from a publicly accessible file stored on the cloud.", "response": "def create_language_dataset_from_url(self, file_url, token=None, url=API_CREATE_LANGUAGE_DATASET):\n        \"\"\" Creates a dataset from a publicly accessible file stored in the cloud.\n            :param file_url: string, in the form of a URL to a file accessible on the cloud. \n            Popular options include Dropbox, AWS S3, Google Drive.\n            warning: Google Drive by default gives you a link to a web ui that allows you to download a file\n            NOT to the file directly. There is a way to change the link to point directly to the file as of 2018\n            as this may change, please search google for a solution.\n            returns: a request object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)\n        dummy_files = {'type': (None, 'text-intent'), 'path':(None, file_url)}\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n        r = requests.post(the_url, headers=h, files=dummy_files)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train_language_model_from_dataset(self, dataset_id, name, token=None, url=API_TRAIN_LANGUAGE_MODEL):\n        auth = 'Bearer ' + self.check_for_token(token)\n        dummy_files = {'name': (None, name), 'datasetId':(None, dataset_id)}\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n        r = requests.post(the_url, headers=h, files=dummy_files)\n\n        return r", "response": "Trains a language model from a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the status of a language model.", "response": "def get_language_model_status(self, model_id, token=None, url=API_TRAIN_LANGUAGE_MODEL):\n        \"\"\" Gets the status of your model, including whether the training has finished.\n            :param model_id: string, the ID for a model you created previously.            \n            returns: a request object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)        \n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url + '/' + model_id\n        r = requests.get(the_url, headers=h)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a prediction based on a body of text of text you send to a trained model.", "response": "def get_language_prediction_from_model(self, model_id, document, token=None, url=API_GET_LANGUAGE_PREDICTION):\n        \"\"\" Gets a prediction based on a body of text you send to a trained model you created previously.\n            :param model_id: string, the ID for a model you created previously.\n            :param document: string, a body of text to be classified.\n            returns: a request object\n        \"\"\"\n        auth = 'Bearer ' + self.check_for_token(token)\n        dummy_files = {'modelId': (None, model_id), 'document':(None, document)}\n        h = {'Authorization': auth, 'Cache-Control':'no-cache'}\n        the_url = url\n        r = requests.post(the_url, headers=h, files=dummy_files)\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_rectlabel_app_output(self):\n        # get json files only\n        files = []\n        files = [f for f in os.listdir() if f[-5:] == '.json']\n\n        if len(files) == 0:\n            print('No json files found in this directory')\n            return None\n\n        max_boxes = 0        \n        rows = []\n\n        for each_file in files:\n            f = open(each_file, 'r')\n            j = f.read()            \n            j = json.loads(j)            \n            f.close()\n\n            # running count of the # of boxes.\n            if len(j['objects']) > max_boxes:\n                max_boxes = len(j['objects'])\n\n            # Each json file will end up being a row\n            # set labels\n            row = []\n\n            for o in j['objects']:\n                labels = {}\n                labels['label'] = o['label']\n                labels['x'] = o['x_y_w_h'][0]\n                labels['y'] = o['x_y_w_h'][1]\n                labels['width'] = o['x_y_w_h'][2]\n                labels['height'] = o['x_y_w_h'][3]\n\n                # String manipulation for csv\n                labels_right_format = '\\\"' + json.dumps(labels).replace('\"', '\\\"\\\"') + '\\\"'\n\n                row.append(labels_right_format)\n\n            row.insert(0, '\\\"' + j['filename'] + '\\\"')        \n\n            rows.append(row)\n\n        # one array element per row\n        rows = [','.join(i) for i in rows]\n\n        header = '\\\"image\\\"'\n        \n        for box_num in range(0, max_boxes):\n            header += ', \\\"box\\\"' + str(box_num)\n\n        rows.insert(0, header)\n        return rows", "response": "Internal use mostly finds all. json files in the current directory and parses each file into a row and a column containing the values for each row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noutput a csv file in accordance with parse_rectlabel_app_output method.", "response": "def save_parsed_data_to_csv(self, output_filename='output.csv'):\n        \"\"\" Outputs a csv file in accordance with parse_rectlabel_app_output method. This csv file is meant to accompany a set of pictures files\n            in the creation of an Object Detection dataset.\n            :param output_filename string, default makes sense, but for your convenience.\n        \"\"\"\n        result = self.parse_rectlabel_app_output()\n\n        ff = open(output_filename, 'w', encoding='utf8')\n\n        for line in result:\n            ff.write(line + '\\n')\n\n        ff.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noutputting a csv file in accordance with parse_rectlabel_app_output method.", "response": "def XML_save_parsed_data_to_csv(self, output_filename='output.csv'):\n        \"\"\" Outputs a csv file in accordance with parse_rectlabel_app_output method. This csv file is meant to accompany a set of pictures files\n            in the creation of an Object Detection dataset.\n            :param output_filename string, default makes sense, but for your convenience.\n        \"\"\"\n        result = self.XML_parse_rectlabel_app_output()\n\n        ff = open(output_filename, 'w', encoding='utf8')\n\n        for line in result:\n            ff.write(line + '\\n')\n\n        ff.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive an ion specification determine its type e. g. 1D Events etc.", "response": "def istr_type(istr):\n    \"\"\"\n    Given an \"ion\" specification, determine its \"type\", e.g. 1D, Events, etc.\n    \"\"\"\n    data = set(i.rstrip('0123456789') for i in tokens(istr))\n    has_events = not data.isdisjoint(istr_type_evts)\n    has_2d = not data.isdisjoint(istr_type_2d)\n    has_1d = data.difference(istr_type_evts).difference(istr_type_2d) != set()\n\n    if has_events and not (has_1d or has_2d):\n        return 'events'\n    elif has_1d and not has_events:\n        return '1d'\n    elif has_2d and not (has_events or has_1d):\n        return '2d'\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if an expression is a valid function call or a valid parameter call.", "response": "def is_parans_exp(istr):\n    \"\"\"\n    Determines if an expression is a valid function \"call\"\n    \"\"\"\n    fxn = istr.split('(')[0]\n    if (not fxn.isalnum() and fxn != '(') or istr[-1] != ')':\n        return False\n    plevel = 1\n    for c in '('.join(istr[:-1].split('(')[1:]):\n        if c == '(':\n            plevel += 1\n        elif c == ')':\n            plevel -= 1\n        if plevel == 0:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the current Pedalboard for the pedalboard.", "response": "def set_pedalboard(self, pedalboard, notify=True, force=False):\n        \"\"\"\n        Set the current :class:`.Pedalboard` for the pedalboard\n        only if the ``pedalboard != current_pedalboard or force``\n\n        Is also possible unload the current pedalboard data with replacing it for ``None``::\n\n            >>> current_controller.set_pedalboard(None)\n\n        .. warning::\n\n            Changing the current pedalboard to Nonw, will not be able to call\n            methods to change the pedalboard based in the current, like\n            :meth:`.to_before_pedalboard`, :meth:`.to_next_pedalboard`,\n            :meth:`.to_before_bank` and :meth:`.to_next_bank`\n\n        :param Pedalboard pedalboard: New current pedalboard\n        :param bool notify: If false, not notify change for :class:`.UpdatesObserver`\n                            instances registered in :class:`.Application`\n        :param bool force: Force set pedalboard\n        \"\"\"\n        if pedalboard is not None and pedalboard.bank is None:\n            raise CurrentPedalboardError('Pedalboard {} has not added in any bank'.format(pedalboard))\n\n        if pedalboard == self.pedalboard and not force:\n            return\n\n        self._pedalboard = pedalboard\n        self._device_controller.pedalboard = pedalboard\n        self._save_current_pedalboard()\n\n        if notify:\n            self.app.components_observer.on_current_pedalboard_changed(self.pedalboard)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the current pedalboard for the previous pedalboard.", "response": "def to_before_pedalboard(self):\n        \"\"\"\n        Change the current :class:`.Pedalboard` for the previous pedalboard.\n\n        If the current pedalboard is the first in the current :class:`Bank`,\n        the current pedalboard is will be the **last of the current Bank**.\n\n        .. warning::\n\n            If the current :attr:`.pedalboard` is ``None``, a :class:`.CurrentPedalboardError` is raised.\n        \"\"\"\n        if self.pedalboard is None:\n            raise CurrentPedalboardError('The current pedalboard is None')\n\n        before_index = self.pedalboard.index - 1\n        if before_index == -1:\n            before_index = len(self.bank.pedalboards) - 1\n\n        self.set_pedalboard(self.bank.pedalboards[before_index])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_next_pedalboard(self):\n        if self.pedalboard is None:\n            raise CurrentPedalboardError('The current pedalboard is None')\n\n        next_index = self.pedalboard.index + 1\n        if next_index == len(self.bank.pedalboards):\n            next_index = 0\n\n        self.set_pedalboard(self.bank.pedalboards[next_index])", "response": "Change the current Pedalboard for the next Pedalboard."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the current bank for the before bank.", "response": "def to_before_bank(self):\n        \"\"\"\n        Change the current :class:`Bank` for the before bank. If the current\n        bank is the first, the current bank is will be the last bank.\n\n        The current pedalboard will be the first pedalboard of the new current bank\n        **if it contains any pedalboard**, else will be ``None``.\n\n        .. warning::\n\n            If the current :attr:`.pedalboard` is ``None``, a :class:`.CurrentPedalboardError` is raised.\n        \"\"\"\n        if self.pedalboard is None:\n            raise CurrentPedalboardError('The current pedalboard is None')\n\n        before_index = self.bank.index - 1\n        if before_index == -1:\n            before_index = len(self._manager.banks) - 1\n\n        self.set_bank(self._manager.banks[before_index])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_next_bank(self):\n        if self.pedalboard is None:\n            raise CurrentPedalboardError('The current pedalboard is None')\n\n        next_index = self.next_bank_index(self.bank.index)\n\n        self.set_bank(self._manager.banks[next_index])", "response": "Change the current bank for the next bank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_bank(self, bank, try_preserve_index=False):\n        if bank not in self._manager:\n            raise CurrentPedalboardError('Bank {} has not added in banks manager'.format(bank))\n\n        if self.bank == bank:\n            return\n\n        if bank.pedalboards:\n            pedalboard = self._equivalent_pedalboard(bank) if try_preserve_index else bank.pedalboards[0]\n            self.set_pedalboard(pedalboard)\n        else:\n            self.set_pedalboard(None)", "response": "Sets the bank for the current bank."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_and_get_value(options, options_name, key, _type):\n  if isinstance(options, dict):\n    has = lambda k: k in options\n    get = lambda k: options[k]\n  elif isinstance(options, object):\n    has = lambda k: hasattr(options, k)\n    get = lambda k: getattr(options, k)\n  else:\n    raise ImproperlyConfigured(\n        '`{}` must be a dictionary-like object.'.format(options_name))\n\n  if not has(key):\n    raise ImproperlyConfigured(\n        '`{}` must be specified in `{}`'.format(key, options_name))\n\n  value = get(key)\n  if not isinstance(value, _type):\n    raise ImproperlyConfigured(\n        '`{}` in `{}` must be a {}'.format(key, options_name, repr(_type)))\n\n  return value", "response": "Check that options has a value for key with type\n _type. Return that value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_event_and_assign_id(event):\n  event_time = event.get(TIMESTAMP_FIELD)\n\n  if event_time is None:\n    event[TIMESTAMP_FIELD] = event_time = epoch_time_to_kronos_time(time.time())\n  elif type(event_time) not in (int, long):\n    raise InvalidEventTime(event_time)\n\n  # Generate a uuid1-like sequence from the event time with the non-time bytes\n  # set to random values.\n  _id = uuid_from_kronos_time(event_time)\n  event[ID_FIELD] = str(_id)\n  return _id, event", "response": "Validate an event and assign a random UUID based on the event time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the stream name is well - formed.", "response": "def validate_stream(stream):\n  \"\"\"\n  Check that the stream name is well-formed.\n  \"\"\"\n  if not STREAM_REGEX.match(stream) or len(stream) > MAX_STREAM_LENGTH:\n    raise InvalidStreamName(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_storage_settings(storage_class, settings):\n  if not isinstance(settings, dict):\n    raise ImproperlyConfigured(\n        '{}: storage class settings must be a dict'.format(storage_class))\n\n  if not hasattr(storage_class, 'SETTINGS_VALIDATORS'):\n    raise NotImplementedError(\n      '{}: storage class must define `SETTINGS_VALIDATORS`'.format(\n        storage_class))\n\n  settings_validators = getattr(storage_class, 'SETTINGS_VALIDATORS')\n  settings = settings.copy()\n  settings.pop('backend', None)  # No need to validate the `backend` key.\n  invalid_settings = set(settings.keys()) - set(settings_validators.keys())\n  if invalid_settings:\n    raise ImproperlyConfigured(\n        '{}: invalid settings: {}'.format(storage_class, invalid_settings))\n\n  for setting, value in settings.iteritems():\n    if not settings_validators[setting](value):\n      raise ImproperlyConfigured(\n          '{}: invalid value for {}'.format(storage_class, setting))", "response": "This method verifies that all the settings are valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_settings(settings):\n\n  # Validate `storage`\n  storage = _validate_and_get_value(settings, 'settings', 'storage', dict)\n  for name, options in storage.iteritems():\n    if 'backend' not in options:\n      raise ImproperlyConfigured(\n          '`storage[\\'{}\\'] must contain a `backend` key'.format(name))\n\n    path = options['backend']\n    module, cls = path.rsplit('.', 1)\n    module = import_module(module)\n    if not hasattr(module, cls):\n      raise NotImplementedError('`{}` not implemented.'.format(cls))\n    validate_storage_settings(getattr(module, cls), options)\n\n  # Validate `streams_to_backends`\n  namespace_to_streams_configuration = _validate_and_get_value(\n      settings, 'settings', 'namespace_to_streams_configuration', dict)\n  for namespace, prefix_confs in namespace_to_streams_configuration.iteritems():\n    if '' not in prefix_confs:\n      raise ImproperlyConfigured(\n          'Must specify backends for the null prefix')\n\n    for prefix, options in prefix_confs.iteritems():\n      if prefix != '':\n        # Validate stream prefix.\n        validate_stream(prefix)\n\n      backends = _validate_and_get_value(\n          options,\n          \"namespace_to_streams_configuration['{}']['{}']\".format(namespace,\n                                                                  prefix),\n          'backends', dict)\n      for backend in backends.keys():\n        if backend not in storage:\n          raise ImproperlyConfigured(\n              \"`{}` backend for `namespace_to_streams_configuration['{}']\"\n              \"['{}']` is not configured in `storage`\"\n              .format(backend, namespace, prefix))\n\n      read_backend = _validate_and_get_value(\n          options,\n          \"namespace_to_streams_configuration['{}']['{}']\".format(namespace,\n                                                                  prefix),\n          'read_backend', str)\n      if read_backend not in storage:\n          raise ImproperlyConfigured(\n              \"`{}` backend for `namespace_to_streams_configuration['{}']\"\n              \"['{}']` is not configured in `storage`\"\n              .format(read_backend, namespace, prefix))\n\n  # Validate `stream`\n  stream = getattr(settings, 'stream', dict)\n  _validate_and_get_value(stream, 'stream', 'format', re._pattern_type)\n\n  # Validate `node`\n  node = getattr(settings, 'node', dict)\n  _validate_and_get_value(node, 'node', 'greenlet_pool_size', int)\n  _validate_and_get_value(node, 'node', 'id', str)", "response": "Validate all the required settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new instance from API arguments.", "response": "def from_api(cls, **kwargs):\n        \"\"\"Create a new instance from API arguments.\n\n        This will switch camelCase keys into snake_case for instantiation.\n\n        It will also identify any ``Instance`` or ``List`` properties, and\n        instantiate the proper objects using the values. The end result being\n        a fully Objectified and Pythonified API response.\n\n        Returns:\n            BaseModel: Instantiated model using the API values.\n        \"\"\"\n\n        vals = cls.get_non_empty_vals({\n            cls._to_snake_case(k): v for k, v in kwargs.items()\n        })\n        remove = []\n        for attr, val in vals.items():\n            try:\n                vals[attr] = cls._parse_property(attr, val)\n            except HelpScoutValidationException:\n                remove.append(attr)\n                logger.info(\n                    'Unexpected property received in API response',\n                    exc_info=True,\n                )\n        for attr in remove:\n            del vals[attr]\n        return cls(**cls.get_non_empty_vals(vals))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary that can be sent to the API.", "response": "def to_api(self):\n        \"\"\"Return a dictionary to send to the API.\n\n        Returns:\n            dict: Mapping representing this object that can be sent to the\n             API.\n        \"\"\"\n        vals = {}\n        for attribute, attribute_type in self._props.items():\n            prop = getattr(self, attribute)\n            vals[self._to_camel_case(attribute)] = self._to_api_value(\n                attribute_type, prop,\n            )\n        return vals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a parsed value for the API.", "response": "def _to_api_value(self, attribute_type, value):\n        \"\"\"Return a parsed value for the API.\"\"\"\n\n        if not value:\n            return None\n\n        if isinstance(attribute_type, properties.Instance):\n            return value.to_api()\n\n        if isinstance(attribute_type, properties.List):\n            return self._parse_api_value_list(value)\n\n        return attribute_type.serialize(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_api_value_list(self, values):\n        try:\n            return [v.to_api() for v in values]\n        # Not models\n        except AttributeError:\n            return list(values)", "response": "Return a list field compatible with the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_property(cls, name, value):\n\n        prop = cls._props.get(name)\n        return_value = value\n\n        if not prop:\n            logger.debug(\n                '\"%s\" with value \"%s\" is not a valid property for \"%s\".' % (\n                    name, value, cls,\n                ),\n            )\n            return_value = None\n\n        elif isinstance(prop, properties.Instance):\n            return_value = prop.instance_class.from_api(**value)\n\n        elif isinstance(prop, properties.List):\n            return_value = cls._parse_property_list(prop, value)\n\n        elif isinstance(prop, properties.Color):\n            return_value = cls._parse_property_color(value)\n\n        return return_value", "response": "Parse a property received from the API into an internal object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a list property and return a list of the results.", "response": "def _parse_property_list(prop, value):\n        \"\"\"Parse a list property and return a list of the results.\"\"\"\n        attributes = []\n        for v in value:\n            try:\n                attributes.append(\n                    prop.prop.instance_class.from_api(**v),\n                )\n            except AttributeError:\n                attributes.append(v)\n        return attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_snake_case(string):\n        sub_string = r'\\1_\\2'\n        string = REGEX_CAMEL_FIRST.sub(sub_string, string)\n        return REGEX_CAMEL_SECOND.sub(sub_string, string).lower()", "response": "Return a snake cased version of the input string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a camel cased version of the input string.", "response": "def _to_camel_case(string):\n        \"\"\"Return a camel cased version of the input string.\n\n        Args:\n            string (str): A snake cased string.\n\n        Returns:\n            str: A camel cased string.\n        \"\"\"\n        components = string.split('_')\n        return '%s%s' % (\n            components[0],\n            ''.join(c.title() for c in components[1:]),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, output=None):\n        if output is None:\n            if self.dbfile is None:\n                return\n            output = self.dbfile\n\n        with open(output, 'w') as f:\n            f.write(self._dump())", "response": "Save the database to a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feed(self, text=None, source=None):\n        if text is not None:\n            words = re.split(r'\\s+', text)\n            wlen = len(words)\n            for i in range(wlen - 2):\n                self._insert(words[i:i+3])\n\n        if source is not None:\n            with open(source, 'r') as f:\n                self.feed(f.read())", "response": "Feed some text to the database either from a string or a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates some text from the database.", "response": "def generate(self, **kwargs):\n        \"\"\"\n        Generate some text from the database. By default only 70 words are\n        generated, but you can change this using keyword arguments.\n\n        Keyword arguments:\n\n            - ``wlen``: maximum length (words)\n            - ``words``: a list of words to use to begin the text with\n        \"\"\"\n        words = list(map(self._sanitize, kwargs.get('words', [])))\n        max_wlen = kwargs.get('wlen', 70)\n\n        wlen = len(words)\n\n        if wlen < 2:\n            if not self._db:\n                return ''\n\n            if wlen == 0:\n                words = sample(self._db.keys(), 1)[0].split(self._WSEP)\n            elif wlen == 1:\n                spl = [k for k in self._db.keys()\n                       if k.startswith(words[0]+self._WSEP)]\n                words.append(sample(spl, 1)[0].split(self._WSEP)[1])\n\n            wlen = 2\n\n        while wlen < max_wlen:\n            next_word = self._get(words[-2], words[-1])\n            if next_word is None:\n                break\n\n            words.append(next_word)\n            wlen += 1\n\n        return ' '.join(words)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load(self):\n        if self.dbfile is not None:\n            with open(self.dbfile, 'r') as f:\n                self._db = json.loads(f.read())\n        else:\n            self._db = {}", "response": "Load the database from its dbfile if it has one"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get(self, word1, word2):\n        key = self._WSEP.join([self._sanitize(word1), self._sanitize(word2)])\n        key = key.lower()\n        if key not in self._db:\n            return\n\n        return sample(self._db[key], 1)[0]", "response": "Return a possible next word after word1 and word2 or None if there s no possibility."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _insert(self, trigram):\n        words = list(map(self._sanitize, trigram))\n\n        key = self._WSEP.join(words[:2]).lower()\n        next_word = words[2]\n\n        self._db.setdefault(key, [])\n        # we could use a set here, but sets are not serializables in JSON. This\n        # is the same reason we use dicts instead of defaultdicts.\n        if next_word not in self._db[key]:\n            self._db[key].append(next_word)", "response": "Insert a trigram into the DB."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cohort_queryplan(plan):\n  cohort = plan['cohort']\n  action = plan['action']\n  source = plan['source']\n\n  # Calculate the start and end dates, in Kronos time, of the\n  # beginning and end of the cohort and action streams that will be\n  # relevant.\n  cohort_start = datetime_to_kronos_time(_date_to_datetime(cohort['start']))\n  cohort_span = timedelta(**{cohort['unit']: cohort['cohorts']})\n  cohort_end = cohort['start'] + cohort_span\n  action_span = timedelta(**{action['unit']: action['repetitions']})\n  action_end = cohort_end + action_span\n  cohort_end = datetime_to_kronos_time(_date_to_datetime(cohort_end)) + 1\n  action_end = datetime_to_kronos_time(_date_to_datetime(action_end)) + 1\n\n  left = _cohort_stream_transform(source,\n                                  cohort['stream'], cohort_start, cohort_end,\n                                  cohort.get('transform'),\n                                  cohort['grouping_key'], cohort['unit'])\n  right = _cohort_stream_transform(source,\n                                   action['stream'], cohort_start, action_end,\n                                   action.get('transform'),\n                                   action['grouping_key'], action['unit'])\n\n  additional_action_time = (DateUnit.unit_to_kronos_time(action['unit']) *\n                            action['repetitions'])\n\n  left.alias = 'cohort'\n  right.alias = 'action'\n\n  joined = Join(left,\n                right,\n                (Condition(Condition.Op.EQ,\n                           Property('cohort.%s' % cohort['grouping_key']),\n                           Property('action.%s' % action['grouping_key'])) &\n                 Condition(Condition.Op.GTE,\n                           Property('action.%s' % TIMESTAMP_FIELD),\n                           Property('cohort.%s' % TIMESTAMP_FIELD)) &\n                 Condition(Condition.Op.LT,\n                           Property('action.%s' % TIMESTAMP_FIELD),\n                           Add([Property('cohort.%s' % TIMESTAMP_FIELD),\n                                Constant(additional_action_time)]))))\n\n  user_aggregated = Aggregate(\n    joined,\n    GroupBy([Property('cohort.date', alias=TIMESTAMP_FIELD),\n             Property('cohort.%s' % cohort['grouping_key'], alias='group'),\n             Floor([Subtract([Property('action.%s' % TIMESTAMP_FIELD),\n                              Property('cohort.%s' % TIMESTAMP_FIELD)]),\n                    Constant(DateUnit.unit_to_kronos_time(action['unit']))],\n                   alias='action_step')]),\n    [Count([], alias='count')]\n  )\n\n  aggregated = Aggregate(\n    user_aggregated,\n    GroupBy([Property(TIMESTAMP_FIELD, alias=TIMESTAMP_FIELD),\n             Property('action_step', alias='action_step')]),\n    [Count([], alias='cohort_actions')])\n\n  # TODO(marcua): Also sum up the cohort sizes, join with the plan.\n  return aggregated.to_dict()", "response": "Returns a metis - compatible query plan for a cohort."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef planetMassType(mass):\n\n    if mass is np.nan:\n        return None\n\n    for massLimit, massType in planetAssumptions['massType']:\n\n        if mass < massLimit:\n            return massType", "response": "Returns the planet masstype given the mass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef planetRadiusType(radius):\n\n    if radius is np.nan:\n        return None\n\n    for radiusLimit, radiusType in planetAssumptions['radiusType']:\n\n        if radius < radiusLimit:\n            return radiusType", "response": "Returns the planet radiustype given the mass and using planetAssumptions. radiusType"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef planetType(temperature, mass, radius):\n\n    if mass is not np.nan:\n        sizeType = planetMassType(mass)\n    elif radius is not np.nan:\n        sizeType = planetRadiusType(radius)\n    else:\n        return None\n\n    return '{0} {1}'.format(planetTempType(temperature), sizeType)", "response": "Returns the planet type as temperatureType massType radiusType"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a dataframe into two arrays.", "response": "def split_array_like(df, columns=None): #TODO rename TODO if it's not a big performance hit, just make them arraylike? We already indicated the column explicitly (sort of) so...\n    '''\n    Split cells with array-like values along row axis.\n\n    Column names are maintained. The index is dropped.\n\n    Parameters\n    ----------\n    df : ~pandas.DataFrame\n        Data frame ``df[columns]`` should contain :py:class:`~pytil.numpy.ArrayLike`\n        values.\n    columns : ~typing.Collection[str] or str or None\n        Columns (or column) whose values to split. Defaults to ``df.columns``.\n\n    Returns\n    -------\n    ~pandas.DataFrame\n        Data frame with array-like values in ``df[columns]`` split across rows,\n        and corresponding values in other columns repeated.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1,[1,2],[1]],[1,[1,2],[3,4,5]],[2,[1],[1,2]]], columns=('check', 'a', 'b'))\n    >>> df\n       check       a          b\n    0      1  [1, 2]        [1]\n    1      1  [1, 2]  [3, 4, 5]\n    2      2     [1]     [1, 2]\n    >>> split_array_like(df, ['a', 'b'])\n      check  a  b\n    0     1  1  1\n    1     1  2  1\n    2     1  1  3\n    3     1  1  4\n    4     1  1  5\n    5     1  2  3\n    6     1  2  4\n    7     1  2  5\n    8     2  1  1\n    9     2  1  2\n    '''\n    # TODO could add option to keep_index by using reset_index and eventually\n    # set_index. index names trickery: MultiIndex.names, Index.name. Both can be\n    # None. If Index.name can be None in which case it translates to 'index' or\n    # if that already exists, it translates to 'level_0'. If MultiIndex.names is\n    # None, it translates to level_0,... level_N\n    dtypes = df.dtypes\n\n    if columns is None:\n        columns = df.columns\n    elif isinstance(columns, str):\n        columns = [columns]\n\n    for column in columns:\n        expanded = np.repeat(df.values, df[column].apply(len).values, axis=0)\n        expanded[:, df.columns.get_loc(column)] = np.concatenate(df[column].tolist())\n        df = pd.DataFrame(expanded, columns=df.columns)\n\n    # keep types unchanged\n    for i, dtype in enumerate(dtypes):\n        df.iloc[:,i] = df.iloc[:,i].astype(dtype)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef equals(df1, df2, ignore_order=set(), ignore_indices=set(), all_close=False, _return_reason=False):\n    '''\n    Get whether 2 data frames are equal.\n\n    ``NaN`` is considered equal to ``NaN`` and `None`.\n\n    Parameters\n    ----------\n    df1 : ~pandas.DataFrame\n        Data frame to compare.\n    df2 : ~pandas.DataFrame\n        Data frame to compare.\n    ignore_order : ~typing.Set[int]\n        Axi in which to ignore order.\n    ignore_indices : ~typing.Set[int]\n        Axi of which to ignore the index. E.g. ``{1}`` allows differences in\n        ``df.columns.name`` and ``df.columns.equals(df2.columns)``.\n    all_close : bool\n        If `False`, values must match exactly, if `True`, floats are compared as if\n        compared with `numpy.isclose`.\n    _return_reason : bool\n        Internal. If `True`, `equals` returns a tuple containing the reason, else\n        `equals` only returns a bool indicating equality (or equivalence\n        rather).\n\n    Returns\n    -------\n    bool\n        Whether they are equal (after ignoring according to the parameters).\n\n        Internal note: if ``_return_reason``, ``Tuple[bool, str or None]`` is\n        returned. The former is whether they're equal, the latter is `None` if\n        equal or a short explanation of why the data frames aren't equal,\n        otherwise.\n\n    Notes\n    -----\n    All values (including those of indices) must be copyable and ``__eq__`` must\n    be such that a copy must equal its original. A value must equal itself\n    unless it's ``NaN``. Values needn't be orderable or hashable (however\n    pandas requires index values to be orderable and hashable). By consequence,\n    this is not an efficient function, but it is flexible.\n\n    Examples\n    --------\n    >>> from pytil import data_frame as df_\n    >>> import pandas as pd\n    >>> df = pd.DataFrame([\n    ...        [1, 2, 3],\n    ...        [4, 5, 6],\n    ...        [7, 8, 9]\n    ...    ],\n    ...    index=pd.Index(('i1', 'i2', 'i3'), name='index1'),\n    ...    columns=pd.Index(('c1', 'c2', 'c3'), name='columns1')\n    ... )\n    >>> df\n    columns1  c1  c2  c3\n    index1              \n    i1         1   2   3\n    i2         4   5   6\n    i3         7   8   9\n    >>> df2 = df.reindex(('i3', 'i1', 'i2'), columns=('c2', 'c1', 'c3'))\n    >>> df2\n    columns1  c2  c1  c3\n    index1              \n    i3         8   7   9\n    i1         2   1   3\n    i2         5   4   6\n    >>> df_.equals(df, df2)\n    False\n    >>> df_.equals(df, df2, ignore_order=(0,1))\n    True\n    >>> df2 = df.copy()\n    >>> df2.index = [1,2,3]\n    >>> df2\n    columns1  c1  c2  c3\n    1          1   2   3\n    2          4   5   6\n    3          7   8   9\n    >>> df_.equals(df, df2)\n    False\n    >>> df_.equals(df, df2, ignore_indices={0})\n    True\n    >>> df2 = df.reindex(('i3', 'i1', 'i2'))\n    >>> df2\n    columns1  c1  c2  c3\n    index1              \n    i3         7   8   9\n    i1         1   2   3\n    i2         4   5   6\n    >>> df_.equals(df, df2, ignore_indices={0})  # does not ignore row order!\n    False\n    >>> df_.equals(df, df2, ignore_order={0})\n    True\n    >>> df2 = df.copy()\n    >>> df2.index.name = 'other'\n    >>> df_.equals(df, df2)  # df.index.name must match as well, same goes for df.columns.name\n    False\n    '''\n    result = _equals(df1, df2, ignore_order, ignore_indices, all_close)\n    if _return_reason:\n        return result\n    else:\n        return result[0]", "response": "Returns True if two data frames are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmasks first row in values", "response": "def _try_mask_first_row(row, values, all_close, ignore_order):\n    '''\n    mask first row in 2d array\n\n    values : 2d masked array\n        Each row is either fully masked or not masked at all\n    ignore_order : bool\n        Ignore column order\n\n    Return whether masked a row. If False, masked nothing.\n    '''\n    for row2 in values:\n        mask = ma.getmaskarray(row2)\n        assert mask.sum() in (0, len(mask))  # sanity check: all or none masked\n        if mask[0]: # Note: at this point row2's mask is either all False or all True\n            continue\n\n        # mask each value of row1 in row2\n        if _try_mask_row(row, row2, all_close, ignore_order):\n            return True\n    # row did not match\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _try_mask_row(row1, row2, all_close, ignore_order):\n    '''\n    if each value in row1 matches a value in row2, mask row2\n\n    row1\n        1d array\n    row2\n        1d masked array whose mask is all False\n    ignore_order : bool\n        Ignore column order\n    all_close : bool\n        compare with np.isclose instead of ==\n\n    Return whether masked the row\n    '''\n    if ignore_order:\n        for value1 in row1:\n            if not _try_mask_first_value(value1, row2, all_close):\n                row2.mask = ma.nomask\n                return False\n    else:\n        for value1, value2 in zip(row1, row2):\n            if not _value_equals(value1, value2, all_close):\n                return False\n        row2[:] = ma.masked\n    assert row2.mask.all()  # sanity check\n    return True", "response": "Try to mask the elements in row1 and row2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to mask first value in row.", "response": "def _try_mask_first_value(value, row, all_close):\n    '''\n    mask first value in row\n\n    value1 : ~typing.Any\n    row : 1d masked array\n    all_close : bool\n        compare with np.isclose instead of ==\n\n    Return whether masked a value\n    '''\n    # Compare value to row\n    for i, value2 in enumerate(row):\n        if _value_equals(value, value2, all_close):\n            row[i] = ma.masked\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _value_equals(value1, value2, all_close):\n    '''\n    Get whether 2 values are equal\n\n    value1, value2 : ~typing.Any\n    all_close : bool\n        compare with np.isclose instead of ==\n    '''\n    if value1 is None:\n        value1 = np.nan\n    if value2 is None:\n        value2 = np.nan\n\n    are_floats = np.can_cast(type(value1), float) and np.can_cast(type(value2), float)\n    if all_close and are_floats:\n        return np.isclose(value1, value2, equal_nan=True)\n    else:\n        if are_floats:\n            return value1 == value2 or (value1 != value1 and value2 != value2)\n        else:\n            return value1 == value2", "response": "Return True if two values are equal to the base object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assert_equals(df1, df2, ignore_order=set(), ignore_indices=set(), all_close=False, _return_reason=False):\n    '''\n    Assert 2 data frames are equal\n\n    A more verbose form of ``assert equals(df1, df2, ...)``. See `equals` for an explanation of the parameters.\n\n    Parameters\n    ----------\n    df1 : ~pandas.DataFrame\n        Actual data frame.\n    df2 : ~pandas.DataFrame\n        Expected data frame.\n    ignore_order : ~typing.Set[int]\n    ignore_indices : ~typing.Set[int]\n    all_close : bool\n    '''\n    equals_, reason = equals(df1, df2, ignore_order, ignore_indices, all_close, _return_reason=True)\n    assert equals_, '{}\\n\\n{}\\n\\n{}'.format(reason, df1.to_string(), df2.to_string())", "response": "A more verbose form of assert equals"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_to_json(self):\r\n\r\n        requestvalues = {\r\n            'id': self.dataset,\r\n            'publicationDate': self.publication_date.strftime('%Y-%m-%d'),\r\n            'source': self.source,\r\n            'refUrl': self.refernce_url,\r\n        }        \r\n        return json.dumps(requestvalues)", "response": "The method saves data to json from object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keyspace(self, keyspace):\n        if FORMAT_SPEC.search(keyspace):\n            return KeyspacedProxy(self, keyspace)\n        else:\n            return KeyspacedProxy(self, self._keyspaces[keyspace])", "response": "Returns a new instance of the given keyspace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare two directory trees content. Return False if they are not the same.", "response": "def is_same(dir1, dir2):\n    \"\"\"\n    Compare two directory trees content.\n    Return False if they differ, True is they are the same.\n    :param dir2:\n    :param dir1:\n    \"\"\"\n    compared = dircmp(dir1, dir2, IGNORED_FILES)  # ignore the OS X file\n    if compared.left_only or compared.right_only or compared.diff_files or compared.funny_files:\n        return False\n    for subdir in compared.common_dirs:\n        if not is_same(os.path.join(dir1, subdir), os.path.join(dir2, subdir)):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind out differences between common files and common files.", "response": "def phase3(self):\n        \"\"\"\n        Find out differences between common files.\n        Ensure we are using content comparison with shallow=False.\n        \"\"\"\n        fcomp = filecmp.cmpfiles(self.left, self.right, self.common_files,\n                                 shallow=False)\n        self.same_files, self.diff_files, self.funny_files = fcomp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert events into the index.", "response": "def _insert(self, namespace, stream, events, configuration):\n    \"\"\"\n    `namespace` acts as db for different streams\n    `stream` is the name of a stream and `events` is a list of events to\n    insert.\n    \"\"\"\n    index = self.index_manager.get_index(namespace)\n    start_dts_to_add = set()\n\n    def actions():\n      for _id, event in events:\n        dt = kronos_time_to_datetime(uuid_to_kronos_time(_id))\n        start_dts_to_add.add(_round_datetime_down(dt))\n        event['_index'] = index\n        event['_type'] = stream\n        event[LOGSTASH_TIMESTAMP_FIELD] = dt.isoformat()\n\n        yield event\n\n    list(es_helpers.streaming_bulk(self.es, actions(), chunk_size=1000,\n                                   refresh=self.force_refresh))\n    self.index_manager.add_aliases(namespace,\n                                   index,\n                                   start_dts_to_add)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete events from Elasticsearch.", "response": "def _delete(self, namespace, stream, start_id, end_time, configuration):\n    \"\"\"\n    Delete events with id > `start_id` and end_time <= `end_time`.\n    \"\"\"\n    start_time = uuid_to_kronos_time(start_id)\n    body_query = {\n      'query': {\n        'filtered': {\n          'query': {'match_all': {}},\n          'filter': {\n            'bool': {\n              'should': [\n                {\n                  'range': {TIMESTAMP_FIELD: {'gt': start_time,\n                                              'lte': end_time}}\n                },\n                {\n                  'bool': {\n                    'must': [\n                      {'range': {ID_FIELD: {'gt': str(start_id)}}},\n                      {'term': {TIMESTAMP_FIELD: start_time}}\n                    ]\n                  }\n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n    query = {'index': self.index_manager.get_index(namespace),\n             'doc_type': stream,\n             'body': body_query,\n             'ignore': 404,\n             'allow_no_indices': True,\n             'ignore_unavailable': True}\n    try:\n      # XXX: ElasticSearch does not return stats on deletions.\n      # https://github.com/elasticsearch/elasticsearch/issues/6519\n      count = self.es.count(**query).get('count', 0)\n      if count:\n        self.es.delete_by_query(**query)\n      return count, []\n    except Exception, e:\n      return 0, [repr(e)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _retrieve(self, namespace, stream, start_id, end_time, order, limit,\n                configuration):\n    \"\"\"\n    Yield events from stream starting after the event with id `start_id` until\n    and including events with timestamp `end_time`.\n    \"\"\"\n    indices = self.index_manager.get_aliases(namespace,\n                                             uuid_to_kronos_time(start_id),\n                                             end_time)\n    if not indices:\n      return\n\n    end_id = uuid_from_kronos_time(end_time, _type=UUIDType.HIGHEST)\n    end_id.descending = start_id.descending = descending = (\n      order == ResultOrder.DESCENDING)\n\n    start_time = uuid_to_kronos_time(start_id)\n    body_query = {\n      'query': {\n        'filtered': {\n          'query': {'match_all': {}},\n          'filter': {\n            'range': {TIMESTAMP_FIELD: {'gte': start_time, 'lte': end_time}}\n          }\n        }\n      }\n    }\n    order = 'desc' if descending else 'asc'\n    sort_query = [\n      '%s:%s' % (TIMESTAMP_FIELD, order),\n      '%s:%s' % (ID_FIELD, order)\n    ]\n\n    last_id = end_id if descending else start_id\n    scroll_id = None\n    while True:\n      size = max(min(limit, configuration['read_size']) / self.shards, 10)\n      if scroll_id is None:\n        res = self.es.search(index=indices,\n                             doc_type=stream,\n                             size=size,\n                             body=body_query,\n                             sort=sort_query,\n                             _source=True,\n                             scroll='1m',\n                             ignore=[400, 404],\n                             allow_no_indices=True,\n                             ignore_unavailable=True)\n      else:\n        res = self.es.scroll(scroll_id, scroll='1m')\n      if '_scroll_id' not in res:\n        break\n      scroll_id = res['_scroll_id']\n      hits = res.get('hits', {}).get('hits')\n      if not hits:\n        break\n\n      for hit in hits:\n        _id = TimeUUID(hit['_id'], descending=descending)\n        if _id <= last_id:\n          continue\n        last_id = _id\n        event = hit['_source']\n        del event[LOGSTASH_TIMESTAMP_FIELD]\n        yield json.dumps(event)\n        limit -= 1\n        if limit == 0:\n          break\n\n    if scroll_id is not None:\n      self.es.clear_scroll(scroll_id)", "response": "Retrieve events from stream starting after start_id and including events with timestamp end_time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_tables(self):\n        cdir = os.path.dirname( os.path.realpath(__file__) )\n        \n        # table schemas -------------------------------------\n        schema = os.path.join(cdir,\"data\",\"partsmaster.sql\")\n        if self.debug:\n            print(self.hdr,\"parts master schema is \",schema)\n        \n        self.populate(schema)\n        \n        schema = os.path.join(cdir,\"data\",\"approvedmfg.sql\")\n        if self.debug:\n            print(self.hdr,\"approved mfg list schema is \",schema)\n        \n        self.populate(schema)\n        \n        schema = os.path.join(cdir,\"data\",\"attachment.sql\")\n        if self.debug:\n            print(self.hdr,\"attachment schema is \",schema)\n        \n        self.populate(schema)\n\n        schema = os.path.join(cdir,\"data\",\"bom.sql\")\n        if self.debug:\n            print(self.hdr,\"bill of materials schema is \",schema)\n        \n        self.populate(schema)\n        \n        return", "response": "create tables in database"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_c_serialized(f):\n    # TODO: rewrite to use re library\n    f.seek(0)\n    try:\n        p_rec_type = None\n        while True:\n            rec_off = f.tell()\n            while True:\n                if f.read(2) == b'\\xff\\xff':\n                    h = struct.unpack('<HH', f.read(4))\n                    if h[1] < 64 and h[1] != 0:\n                        rec_type = f.read(h[1])\n                        if rec_type[0] == 67:  # starts with 'C'\n                            break\n                if f.read(1) == b'':\n                    raise EOFError\n                f.seek(f.tell() - 2)\n            if p_rec_type is not None:\n                rec_len = f.tell() - 6 - len(rec_type) - rec_off\n                f.seek(rec_off)\n                yield p_rec_type, f.read(rec_len)\n                f.seek(f.tell() + 6 + len(rec_type))\n            # p_type = h[0]\n            p_rec_type = rec_type\n    except EOFError:\n        rec_len = f.tell() - 6 - len(rec_type) - rec_off\n        f.seek(rec_off)\n        yield p_rec_type, f.read(rec_len)", "response": "Reads in a binary file created by a C ++ serializer and returns tuples of header name and data following the header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scan(self, t, dt=None, aggfunc=None):\n        return self.data.scan(t, dt, aggfunc)", "response": "Returns the spectrum from a specific time or range of times."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning dictionary of available storage brokers.", "response": "def _generate_storage_broker_lookup():\n    \"\"\"Return dictionary of available storage brokers.\"\"\"\n    storage_broker_lookup = dict()\n    for entrypoint in iter_entry_points(\"dtool.storage_brokers\"):\n        StorageBroker = entrypoint.load()\n        storage_broker_lookup[StorageBroker.key] = StorageBroker\n    return storage_broker_lookup"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_storage_broker(uri, config_path):\n    uri = dtoolcore.utils.sanitise_uri(uri)\n    storage_broker_lookup = _generate_storage_broker_lookup()\n    parsed_uri = dtoolcore.utils.generous_parse_uri(uri)\n    StorageBroker = storage_broker_lookup[parsed_uri.scheme]\n    return StorageBroker(uri, config_path)", "response": "Helper function to enable use lookup of appropriate storage brokers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _admin_metadata_from_uri(uri, config_path):\n    uri = dtoolcore.utils.sanitise_uri(uri)\n    storage_broker = _get_storage_broker(uri, config_path)\n    admin_metadata = storage_broker.get_admin_metadata()\n    return admin_metadata", "response": "Helper function for getting admin metadata from a URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_admin_metadata(name, creator_username=None):\n\n    if not dtoolcore.utils.name_is_valid(name):\n        raise(DtoolCoreInvalidNameError())\n\n    if creator_username is None:\n        creator_username = dtoolcore.utils.getuser()\n\n    datetime_obj = datetime.datetime.utcnow()\n\n    admin_metadata = {\n        \"uuid\": str(uuid.uuid4()),\n        \"dtoolcore_version\": __version__,\n        \"name\": name,\n        \"type\": \"protodataset\",\n        \"creator_username\": creator_username,\n        \"created_at\": dtoolcore.utils.timestamp(datetime_obj)\n    }\n    return admin_metadata", "response": "Generate admin metadata as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning dataset URI. :param admin_metadata: dataset administrative metadata :param base_uri: base URI from which to derive dataset URI :returns: dataset URI", "response": "def _generate_uri(admin_metadata, base_uri):\n    \"\"\"Return dataset URI.\n\n    :param admin_metadata: dataset administrative metadata\n    :param base_uri: base URI from which to derive dataset URI\n    :returns: dataset URI\n    \"\"\"\n    name = admin_metadata[\"name\"]\n    uuid = admin_metadata[\"uuid\"]\n    # storage_broker_lookup = _generate_storage_broker_lookup()\n    # parse_result = urlparse(base_uri)\n    # storage = parse_result.scheme\n    StorageBroker = _get_storage_broker(base_uri, config_path=None)\n    return StorageBroker.generate_uri(name, uuid, base_uri)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a ProtoDataSet instance from admin_metadata.", "response": "def generate_proto_dataset(admin_metadata, base_uri, config_path=None):\n    \"\"\"Return :class:`dtoolcore.ProtoDataSet` instance.\n\n    :param admin_metadata: dataset administrative metadata\n    :param base_uri: base URI for proto dataset\n    :param config_path: path to dtool configuration file\n    \"\"\"\n    uri = _generate_uri(admin_metadata, base_uri)\n    return ProtoDataSet(uri, admin_metadata, config_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy(src_uri, dest_base_uri, config_path=None, progressbar=None):\n    dataset = DataSet.from_uri(src_uri)\n\n    proto_dataset = _copy_create_proto_dataset(\n        dataset,\n        dest_base_uri,\n        config_path,\n        progressbar\n    )\n    _copy_content(dataset, proto_dataset, progressbar)\n    proto_dataset.freeze(progressbar=progressbar)\n\n    return proto_dataset.uri", "response": "Copy a dataset to another location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresume copying a dataset to another location.", "response": "def copy_resume(src_uri, dest_base_uri, config_path=None, progressbar=None):\n    \"\"\"Resume coping a dataset to another location.\n\n    Items that have been copied to the destination and have the same size\n    as in the source dataset are skipped. All other items are copied across\n    and the dataset is frozen.\n\n    :param src_uri: URI of dataset to be copied\n    :param dest_base_uri: base of URI for copy target\n    :param config_path: path to dtool configuration file\n    :returns: URI of new dataset\n    \"\"\"\n    dataset = DataSet.from_uri(src_uri)\n\n    # Generate the URI of the destination proto dataset.\n    dest_uri = _generate_uri(dataset._admin_metadata, dest_base_uri)\n\n    proto_dataset = ProtoDataSet.from_uri(dest_uri)\n\n    _copy_content(dataset, proto_dataset, progressbar)\n    proto_dataset.freeze(progressbar=progressbar)\n\n    return proto_dataset.uri"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_name(self, new_name):\n\n        if not dtoolcore.utils.name_is_valid(new_name):\n            raise(DtoolCoreInvalidNameError())\n\n        self._admin_metadata['name'] = new_name\n        if self._storage_broker.has_admin_metadata():\n            self._storage_broker.put_admin_metadata(self._admin_metadata)", "response": "Update the name of the proto dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _put_overlay(self, overlay_name, overlay):\n        if not isinstance(overlay, dict):\n            raise TypeError(\"Overlay must be dict\")\n\n        if set(self._identifiers()) != set(overlay.keys()):\n            raise ValueError(\"Overlay keys must be dataset identifiers\")\n\n        self._storage_broker.put_overlay(overlay_name, overlay)", "response": "Store the given overlay so that it is accessible by the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_manifest(self, progressbar=None):\n        items = dict()\n\n        if progressbar:\n            progressbar.label = \"Generating manifest\"\n\n        for handle in self._storage_broker.iter_item_handles():\n            key = dtoolcore.utils.generate_identifier(handle)\n            value = self._storage_broker.item_properties(handle)\n            items[key] = value\n            if progressbar:\n                progressbar.item_show_func = lambda x: handle\n                progressbar.update(1)\n\n        manifest = {\n            \"items\": items,\n            \"dtoolcore_version\": __version__,\n            \"hash_function\": self._storage_broker.hasher.name\n        }\n\n        return manifest", "response": "Generate manifest from knowledge about contents."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn iterable of dataset item identifiers.", "response": "def _identifiers(self):\n        \"\"\"Return iterable of dataset item identifiers.\"\"\"\n        for handle in self._storage_broker.iter_item_handles():\n            yield dtoolcore.utils.generate_identifier(handle)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the required directory structure and admin metadata.", "response": "def create(self):\n        \"\"\"Create the required directory structure and admin metadata.\"\"\"\n        self._storage_broker.create_structure()\n        self._storage_broker.put_admin_metadata(self._admin_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_item_metadata(self, handle, key, value):\n        self._storage_broker.add_item_metadata(handle, key, value)", "response": "Add metadata to a specific item in the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_overlays(self):\n        overlays = defaultdict(dict)\n        for handle in self._storage_broker.iter_item_handles():\n            identifier = dtoolcore.utils.generate_identifier(handle)\n            item_metadata = self._storage_broker.get_item_metadata(handle)\n            for k, v in item_metadata.items():\n                overlays[k][identifier] = v\n\n        return overlays", "response": "Generate overlays generated from added item metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfreeing the item store.", "response": "def freeze(self, progressbar=None):\n        \"\"\"\n        Convert :class:`dtoolcore.ProtoDataSet` to :class:`dtoolcore.DataSet`.\n        \"\"\"\n        # Call the storage broker pre_freeze hook.\n        self._storage_broker.pre_freeze_hook()\n\n        if progressbar:\n            progressbar.label = \"Freezing dataset\"\n\n        # Generate and persist the manifest.\n        manifest = self.generate_manifest(progressbar=progressbar)\n        self._storage_broker.put_manifest(manifest)\n\n        # Generate and persist overlays from any item metadata that has been\n        # added.\n\n        overlays = self._generate_overlays()\n        for overlay_name, overlay in overlays.items():\n            self._put_overlay(overlay_name, overlay)\n\n        # Change the type of the dataset from \"protodataset\" to \"dataset\" and\n        # add a \"frozen_at\" time stamp to the administrative metadata.\n        datetime_obj = datetime.datetime.utcnow()\n        metadata_update = {\n            \"type\": \"dataset\",\n            \"frozen_at\": dtoolcore.utils.timestamp(datetime_obj)\n        }\n        self._admin_metadata.update(metadata_update)\n        self._storage_broker.put_admin_metadata(self._admin_metadata)\n\n        # Clean up using the storage broker's post freeze hook.\n        self._storage_broker.post_freeze_hook()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a Papyrus handler to the base URL that will be used to access the MapFish Taxonomy.", "response": "def add_papyrus_handler(self, route_name_prefix, base_url, handler):\n    \"\"\" Add a Papyrus handler, i.e. a handler defining the MapFish\n    HTTP interface.\n\n    Example::\n\n        import papyrus\n        config.include(papyrus)\n        config.add_papyrus_handler(\n            'spots', '/spots', 'mypackage.handlers.SpotHandler')\n\n    Arguments:\n\n    ``route_name_prefix`` The prefix used for the route names\n    passed to ``config.add_handler``.\n\n    ``base_url`` The web service's base URL, e.g. ``/spots``. No\n    trailing slash!\n\n    ``handler`` a dotted name or a reference to a handler class,\n    e.g. ``'mypackage.handlers.MyHandler'``.\n    \"\"\"\n    route_name = route_name_prefix + '_read_many'\n    self.add_handler(route_name, base_url, handler,\n                     action='read_many', request_method='GET')\n    route_name = route_name_prefix + '_read_one'\n    self.add_handler(route_name, base_url + '/{id}', handler,\n                     action='read_one', request_method='GET')\n    route_name = route_name_prefix + '_count'\n    self.add_handler(route_name, base_url + '/count', handler,\n                     action='count', request_method='GET')\n    route_name = route_name_prefix + '_create'\n    self.add_handler(route_name, base_url, handler,\n                     action='create', request_method='POST')\n    route_name = route_name_prefix + '_update'\n    self.add_handler(route_name, base_url + '/{id}', handler,\n                     action='update', request_method='PUT')\n    route_name = route_name_prefix + '_delete'\n    self.add_handler(route_name, base_url + '/{id}', handler,\n                     action='delete', request_method='DELETE')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef peak_model(f):\n    @wraps(f)\n    def wrapped_f(t, **kw):\n        # load kwargs with default values\n        # do this here instead of in the def because we want to parse\n        # all of kwargs later to copy values to pass into f\n        def_vals = {'v': 0.0, 'h': 1.0, 'x': 0.0, 'w': 1.0, 's': 1.1, 'e': 1.0}\n        for v in def_vals:\n            if v not in kw:\n                kw[v] = def_vals[v]\n\n        # this copies all of the defaults into what the peak function needs\n        anames, _, _, _ = inspect.getargspec(f)\n        fkw = dict([(arg, kw[arg]) for arg in anames if arg in kw])\n\n        # some functions use location or width parameters explicitly\n        # if not, adjust the timeseries accordingly\n        ta = t\n        if 'x' not in anames:\n            ta = ta - kw['x']\n        if 'w' not in anames:\n            ta = ta / kw['w']\n\n        # finally call the function\n        mod = f(ta, **fkw)\n        # recalcualte, making the peak maximize at x\n        mod = f(ta + ta[mod.argmax()], **fkw)\n        return kw['v'] + kw['h'] / max(mod) * mod\n\n    args = set(['v', 'h', 'x', 'w'])\n    anames, _, _, _ = inspect.getargspec(f)\n    wrapped_f._peakargs = list(args.union([a for a in anames\n                                           if a not in ('t', 'r')]))\n    return wrapped_f", "response": "Given a function that models a peak add scale and location arguments to\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads matches from the archive.", "response": "def download_matches(match_downloaded_callback, on_exit_callback, conf, synchronize_callback= True):\n    \"\"\"\n    :param match_downloaded_callback:       function       when a match is downloaded function is called with the match\n                                                            and the tier (league) of the lowest player in the match\n                                                            as parameters\n\n    :param on_exit_callback:                function        when this function is terminating on_exit_callback is called\n                                                            with the remaining players to download, the downloaded\n                                                            players, the id of the remaining matches to download and\n                                                            the id of the downloaded matches\n\n    :param conf:                            dict           a dictionary containing all the configuration parameters\n\n    :param synchronize_callback:            bool            Synchronize the calls to match_downloaded_callback\n                                                            If set to True the calls are wrapped by a lock, so that only\n                                                            one at a time is executing\n\n    :return:                                None\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    if conf['logging_level'] != logging.NOTSET:\n        logger.setLevel(conf['logging_level'])\n    else:\n        # possibly set the level to warning\n        pass\n\n    def checkpoint(players_to_analyze, analyzed_players, matches_to_download, downloaded_matches):\n        logger.info(\"Reached the checkpoint.\"\n                    .format(datetime.datetime.now().strftime(\"%m-%d %H:%M:%S\"), len(downloaded_matches)))\n        if on_exit_callback:\n            on_exit_callback(players_to_analyze, analyzed_players, matches_to_download, downloaded_matches)\n\n    players_to_analyze = set(conf['seed_players_id'])\n    downloaded_matches = set(conf['downloaded_matches'])\n    logger.info(\"{} previously downloaded matches\".format(len(downloaded_matches)))\n    matches_to_download = set(conf['matches_to_download'])\n    logger.info(\"{} matches to download\".format(len(matches_to_download)))\n\n    analyzed_players = set()\n    pta_lock = threading.Lock()\n    players_available_condition = threading.Condition(pta_lock)\n    mtd_lock = threading.Lock()\n    matches_Available_condition = threading.Condition(mtd_lock)\n    user_function_lock = threading.Lock() if synchronize_callback else NoOpContextManager()\n    logger_lock = threading.Lock()\n    player_downloader_threads = []\n    match_downloader_threads = []\n\n    try:\n\n        def create_thread():\n            if len(player_downloader_threads) < max_players_download_threads:\n                player_downloader = PlayerDownloader(conf, players_to_analyze, analyzed_players, pta_lock, players_available_condition,\n                                         matches_to_download , mtd_lock, matches_Available_condition,\n                                         logger, logger_lock)\n                player_downloader.start()\n                player_downloader_threads.append(player_downloader)\n                with logger_lock:\n                    logger.info(\"Adding a player download thread. Threads: \" + str(len(player_downloader_threads)))\n            else:\n                with logger_lock:\n                    logger.debug(\"Tried adding a player download thread, but there are already the maximum number:\"\n                                \" \" + str(max_players_download_threads))\n\n        def shutdown_thread():\n            if len(player_downloader_threads) > 1:\n                player_downloader_threads.pop().shutdown()\n                with logger_lock:\n                    logger.info(\"Removing a player downloader thread. Threads: \" + str(len(player_downloader_threads)))\n            else:\n                with logger_lock:\n                    logger.debug(\"Tried removing a player download thread, but there is only one left\")\n\n\n        logger.info(\"Starting fetching..\")\n        # Start one player downloader thread\n        create_thread()\n\n        for _ in range(matches_download_threads):\n            match_downloader = MatchDownloader(conf, players_to_analyze, pta_lock, players_available_condition,\n                                               matches_to_download, downloaded_matches, mtd_lock, matches_Available_condition,\n                                               match_downloaded_callback, user_function_lock,\n                                               logger, logger_lock)\n            match_downloader.start()\n            match_downloader_threads.append(match_downloader)\n\n        auto_tuner = ThreadAutoTuner(create_thread, shutdown_thread)\n\n        for i, _ in enumerate(do_every(1)):\n            # Pool the exit flag every second\n            if conf.get('exit', False):\n                break\n\n            if i % 5 == 0:\n                with mtd_lock:\n                    matches_in_queue = len(matches_to_download)\n\n                # The lock happens in the property. Since it is not re-entrant, do not lock now\n                total_players = sum(th.total_downloads for th in player_downloader_threads)\n\n                auto_tuner.update_thread_number(total_players, matches_in_queue)\n\n            # Execute every LOGGING_INTERVAL seconds\n            if i % logging_interval == 0:\n                with mtd_lock:\n                    matches_in_queue = len(matches_to_download)\n                total_matches = sum(th.total_downloads for th in match_downloader_threads)\n                with pta_lock:\n                    players_in_queue = len(players_to_analyze)\n                total_players = sum(th.total_downloads for th in player_downloader_threads)\n                with logger_lock:\n                    logger.info(\"Players in queue: {}. Downloaded players: {}. Matches in queue: {}. Downloaded matches: {}\"\n                                    .format(players_in_queue, total_players, matches_in_queue, total_matches))\n\n        # Notify all the waiting threads so they can exit\n        with pta_lock:\n            players_available_condition.notify_all()\n        with mtd_lock:\n            matches_Available_condition.notify_all()\n        logger.info(\"Terminating fetching\")\n\n    finally:\n        conf['exit'] = True\n        # Joining threads before saving the state\n        for thread in player_downloader_threads + match_downloader_threads:\n            thread.join()\n        # Always call the checkpoint, so that we can resume the download in case of exceptions.\n        logger.info(\"Calling checkpoint callback\")\n        checkpoint(players_to_analyze, analyzed_players, matches_to_download, downloaded_matches)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the value in arr that value is closest to", "response": "def _findNearest(arr, value):\n    \"\"\" Finds the value in arr that value is closest to\n    \"\"\"\n    arr = np.array(arr)\n    # find nearest value in array\n    idx = (abs(arr-value)).argmin()\n    return arr[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _createMagConversionDict():\n    magnitude_conversion_filepath = resource_stream(__name__, 'data/magnitude_conversion.dat')\n    raw_table = np.loadtxt(magnitude_conversion_filepath, '|S5')\n\n    magDict = {}\n    for row in raw_table:\n        if sys.hexversion >= 0x03000000:\n            starClass = row[1].decode(\"utf-8\")  # otherwise we get byte ints or b' caused by 2to3\n            tableData = [x.decode(\"utf-8\") for x in row[3:]]\n        else:\n            starClass = row[1]\n            tableData = row[3:]\n        magDict[starClass] = tableData\n\n    return magDict", "response": "loads magnitude_conversion. dat which is table A% 1995ApJS.. 101..117K\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getParentClass(self, startClass, parentClass):\n        try:\n            if not startClass:  # reached system with no hits\n                raise AttributeError\n        except AttributeError:  # i.e calling binary on an object without one\n                raise HierarchyError('This object ({0}) has no {1} as a parent object'.format(self.name, parentClass))\n\n        if startClass.classType == parentClass:\n            return startClass\n        else:\n            return self._getParentClass(startClass.parent, parentClass)", "response": "getParentClass - Gets the parent class by calling successive parent classes with. parent until parentclass is matched."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef T(self):\n        paramTemp = self.getParam('temperature')\n\n        if not paramTemp is np.nan:\n            return paramTemp\n        elif ed_params.estimateMissingValues:\n            self.flags.addFlag('Calculated Temperature')\n            return self.calcTemperature()\n        else:\n            return np.nan", "response": "Returns the temperature in the catalogue"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef d(self):\n        # TODO this will only work from a star or below. good thing?\n        d = self.parent.d\n        if ed_params.estimateMissingValues:\n            if d is np.nan:\n                d = self.estimateDistance()\n                if d is not np.nan:\n                    self.flags.addFlag('Estimated Distance')\n            return d\n        else:\n            return np.nan", "response": "Returns the distance of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_or_convert_magnitude(self, mag_letter):\n        allowed_mags = \"UBVJIHKLMN\"\n        catalogue_mags = 'BVIJHK'\n\n        if mag_letter not in allowed_mags or not len(mag_letter) == 1:\n            raise ValueError(\"Magnitude letter must be a single letter in {0}\".format(allowed_mags))\n\n        mag_str = 'mag'+mag_letter\n        mag_val = self.getParam(mag_str)\n\n        if isNanOrNone(mag_val) and ed_params.estimateMissingValues:  # then we need to estimate it!\n            # old style dict comprehension for python 2.6\n            mag_dict = dict(('mag'+letter, self.getParam('mag'+letter)) for letter in catalogue_mags)\n            mag_class = Magnitude(self.spectralType, **mag_dict)\n            try:\n                mag_conversion = mag_class.convert(mag_letter)\n                # logger.debug('Star Class: Conversion to {0} successful, got {1}'.format(mag_str, mag_conversion))\n                self.flags.addFlag('Estimated mag{0}'.format(mag_letter))\n                return mag_conversion\n            except ValueError as e:  # cant convert\n                logger.exception(e)\n                # logger.debug('Cant convert to {0}'.format(mag_letter))\n                return np.nan\n        else:\n            # logger.debug('returning {0}={1} from catalogue'.format(mag_str, mag_val))\n            return mag_val", "response": "Returns the value of the given magnitude letter and returns a converted value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getLimbdarkeningCoeff(self, wavelength=1.22):  # TODO replace with pylightcurve\n        # TODO check this returns correct value - im not certain\n        # The intervals of values in the tables\n        tempind = [ 3500., 3750., 4000., 4250., 4500., 4750., 5000., 5250., 5500., 5750., 6000., 6250.,\n                 6500., 6750., 7000., 7250., 7500., 7750., 8000., 8250., 8500., 8750., 9000., 9250.,\n                 9500., 9750., 10000., 10250., 10500., 10750., 11000., 11250., 11500., 11750., 12000., 12250.,\n                 12500., 12750., 13000., 14000., 15000., 16000., 17000., 19000., 20000., 21000., 22000., 23000.,\n                 24000., 25000., 26000., 27000., 28000., 29000., 30000., 31000., 32000., 33000., 34000., 35000.,\n                 36000., 37000., 38000., 39000., 40000., 41000., 42000., 43000., 44000., 45000., 46000., 47000.,\n                 48000., 49000., 50000.]\n        lggind = [0., 0.5, 1., 1.5, 2., 2.5, 3., 3.5, 4., 4.5, 5.]\n        mhind = [-5., -4.5, -4., -3.5, -3., -2.5, -2., -1.5, -1., -0.5, -0.3, -0.2, -0.1, 0., 0.1, 0.2, 0.3, 0.5, 1.]\n\n        # Choose the values in the table nearest our parameters\n        tempselect = _findNearest(tempind, float(self.T))\n        lgselect = _findNearest(lggind, float(self.calcLogg()))\n        mhselect = _findNearest(mhind, float(self.Z))\n\n        quadratic_filepath = resource_stream(__name__, 'data/quadratic.dat')\n        coeffTable = np.loadtxt(quadratic_filepath)\n\n        foundValues = False\n        for i in range(len(coeffTable)):\n            if coeffTable[i, 2] == lgselect and coeffTable[i, 3] == tempselect and coeffTable[i, 4] == mhselect:\n                if coeffTable[i, 0] == 1:\n                    u1array = coeffTable[i, 8:]  # Limb darkening parameter u1 for each wl in waveind\n                    u2array = coeffTable[i+1, 8:]\n                    foundValues = True\n                    break\n\n        if not foundValues:\n            raise ValueError('No limb darkening values could be found')  # TODO replace with better exception\n\n        waveind = [0.365, 0.445, 0.551, 0.658, 0.806, 1.22, 1.63, 2.19, 3.45]  # Wavelengths available in table\n\n        # Interpolates the value at wavelength from values in the table (waveind)\n        u1AtWavelength = np.interp(wavelength, waveind, u1array, left=0, right=0)\n        u2AtWavelength = np.interp(wavelength, waveind, u2array, left=0, right=0)\n\n        return u1AtWavelength, u2AtWavelength", "response": "Returns the quadratic limb darkening coefficients for the given wavelength."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the istransiting tag to see if the planet transits.", "response": "def isTransiting(self):\n        \"\"\" Checks the the istransiting tag to see if the planet transits. Note that this only works as of catalogue\n        version  ee12343381ae4106fd2db908e25ffc537a2ee98c (11th March 2014) where the istransiting tag was implemented\n        \"\"\"\n        try:\n            isTransiting = self.params['istransiting']\n        except KeyError:\n            return False\n\n        if isTransiting == '1':\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the primary transit time assuming a circular orbit.", "response": "def calcTransitDuration(self, circular=False):\n        \"\"\" Estimation of the primary transit time assuming a circular orbit (see :py:func:`equations.transitDuration`)\n        \"\"\"\n\n        try:\n            if circular:\n                return eq.transitDurationCircular(self.P, self.star.R, self.R, self.a, self.i)\n            else:\n                return eq.TransitDuration(self.P, self.a, self.R, self.star.R, self.i, self.e, self.periastron).Td\n        except (ValueError,\n                AttributeError,  # caused by trying to rescale nan i.e. missing i value\n                HierarchyError):  # i.e. planets that dont orbit stars\n            return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the temperature using which uses equations. MeanPlanetTemp and equations. starTemperature.", "response": "def calcTemperature(self):\n        \"\"\" Calculates the temperature using which uses equations.MeanPlanetTemp, albedo assumption and potentially\n        equations.starTemperature.\n\n        issues\n        - you cant get the albedo assumption without temp but you need it to calculate the temp.\n        \"\"\"\n        try:\n            return eq.MeanPlanetTemp(self.albedo, self.star.T, self.star.R, self.a).T_p\n        except (ValueError, HierarchyError):  # ie missing value (.a) returning nan\n            return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the semi - major axis from Keplers Third Law", "response": "def calcSMA(self):\n        \"\"\" Calculates the semi-major axis from Keplers Third Law\n        \"\"\"\n        try:\n            return eq.KeplersThirdLaw(None, self.star.M, self.P).a\n        except HierarchyError:\n            return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calcSMAfromT(self, epsilon=0.7):\n\n        return eq.MeanPlanetTemp(self.albedo(), self.star.T, self.star.R, epsilon, self.T).a", "response": "Calculates the semi - major axis based on planet temperature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calcPeriod(self):\n\n        return eq.KeplersThirdLaw(self.a, self.star.M).P", "response": "calculates period using a and stellar mass\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addParam(self, key, value, attrib=None):\n\n        if key in self.rejectTags:\n            return False  # TODO Replace with exception\n\n        # Temporary code to handle the seperation tag than can occur several times with different units.\n        # TODO code a full multi unit solution (github issue #1)\n        if key == 'separation':\n            if attrib is None:\n                return False  # reject seperations without a unit\n            try:\n                if not attrib['unit'] == 'AU':\n                    return False  # reject for now\n            except KeyError:  # a seperation attribute exists but not one for units\n                return False\n\n        if key in self.params:  # if already exists\n\n            if key == 'name':\n                try:  # if flagged as a primary or popular name use this one, an option should be made to use either\n                    if attrib['type'] == 'pri':  # first names or popular names.\n                        oldname = self.params['name']\n                        self.params['altnames'].append(oldname)\n                        self.params['name'] = value\n                    else:\n                        self.params['altnames'].append(value)\n                except (KeyError, TypeError):  # KeyError = no type key in attrib dict, TypeError = not a dict\n                    self.params['altnames'].append(value)\n            elif key == 'list':\n                self.params['list'].append(value)\n            else:\n                try:\n                    name = self.params['name']\n                except KeyError:\n                    name = 'Unnamed'\n                print('rejected duplicate {0}: {1} in {2}'.format(key, value, name))  # TODO: log rejected value\n                return False  # TODO Replace with exception\n\n        else:  # If the key doesn't already exist and isn't rejected\n\n            # Some tags have no value but a upperlimit in the attributes\n            if value is None and attrib is not None:\n                try:\n                    value = attrib['upperlimit']\n                except KeyError:\n                    try:\n                        value = attrib['lowerlimit']\n                    except KeyError:\n                        return False\n\n            if key == 'rightascension':\n                value = _ra_string_to_unit(value)\n            elif key == 'declination':\n                value = _dec_string_to_unit(value)\n            elif key in self._defaultUnits:\n                try:\n                    value = float(value) * self._defaultUnits[key]\n                except:\n                    print('caught an error with {0} - {1}'.format(key, value))\n            self.params[key] = value", "response": "Adds a parameter to the list of parameters for the current seperation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef roundedSpecClass(self):\n        try:\n            classnumber = str(int(np.around(self.classNumber)))\n        except TypeError:\n            classnumber = str(self.classNumber)\n\n        return self.classLetter + classnumber", "response": "Spectral class with rounded class number ie A8. 5V is A9"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert all the magnitudes from one UBV to another.", "response": "def convert(self, to_mag, from_mag=None):\n        \"\"\" Converts magnitudes using UBVRIJHKLMNQ photometry in Taurus-Auriga (Kenyon+ 1995)\n         ReadMe+ftp1995ApJS..101..117K Colors for main-sequence stars\n\n         If from_mag isn't specified the program will cycle through provided magnitudes and choose one. Note that all\n         magnitudes are first converted to V, and then to the requested magnitude.\n\n        :param to_mag: magnitude to convert to\n        :param from_mag: magnitude to convert from\n        :return:\n        \"\"\"\n        allowed_mags = \"UBVJIHKLMN\"\n\n        if from_mag:\n            if to_mag == 'V':  # If V mag is requested (1/3) - from mag specified\n                return self._convert_to_from('V', from_mag)\n            if from_mag == 'V':\n                magV = self.magV\n            else:\n                magV = self._convert_to_from('V', from_mag)\n\n            return self._convert_to_from(to_mag, 'V', magV)\n\n        # if we can convert from any magnitude, try V first\n        elif not isNanOrNone(self.magV):\n            if to_mag == 'V':  # If V mag is requested (2/3) - no need to convert\n                return self.magV\n            else:\n                return self._convert_to_from(to_mag, 'V', self.magV)\n        else:  # Otherwise lets try all other magnitudes in turn\n            order = \"UBJHKLMN\"  # V is the intermediate step from the others, done by default if possible\n            for mag_letter in order:\n                try:\n                    magV = self._convert_to_from('V', mag_letter)\n                    if to_mag == 'V':  # If V mag is requested (3/3) - try all other mags to convert\n                        logging.debug('Converted to magV from {0} got {1}'.format(mag_letter, magV))\n                        return magV\n                    else:\n                        mag_val = self._convert_to_from(to_mag, 'V', magV)\n                        logging.debug('Converted to mag{0} from {1} got {2}'.format(to_mag, mag_letter, mag_val))\n                        return mag_val\n                except ValueError:\n                    continue  # this conversion may not be possible, try another\n\n            raise ValueError('Could not convert from any provided magnitudes')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_to_from(self, to_mag, from_mag, fromVMag=None):\n        lumtype = self.spectral_type.lumType\n\n        # rounds decimal types, TODO perhaps we should interpolate?\n        specClass = self.spectral_type.roundedSpecClass\n\n        if not specClass:  # TODO investigate implications of this\n            raise ValueError('Can not convert when no spectral class is given')\n\n        if lumtype not in ('V', ''):\n            raise ValueError(\"Can only convert for main sequence stars. Got {0} type\".format(lumtype))\n\n        if to_mag == 'V':\n            col, sign = self.column_for_V_conversion[from_mag]\n\n            try:  # TODO replace with pandas table\n                offset = float(magDict[specClass][col])\n            except KeyError:\n                raise ValueError('No data available to convert those magnitudes for that spectral type')\n\n            if math.isnan(offset):\n                raise ValueError('No data available to convert those magnitudes for that spectral type')\n            else:\n                from_mag_val = self.__dict__['mag'+from_mag]  # safer than eval\n                if isNanOrNone(from_mag_val):\n                    # logger.debug('2 '+from_mag)\n                    raise ValueError('You cannot convert from a magnitude you have not specified in class')\n                return from_mag_val + (offset*sign)\n        elif from_mag == 'V':\n            if fromVMag is None:\n                # trying to second guess here could mess up a K->B calulation by using the intermediate measured V. While\n                # this would probably be preferable it is not was was asked and therefore could give unexpected results\n                raise ValueError('Must give fromVMag, even if it is self.magV')\n\n            col, sign = self.column_for_V_conversion[to_mag]\n            try:\n                offset = float(magDict[specClass][col])\n            except KeyError:\n                raise ValueError('No data available to convert those magnitudes for that spectral type')\n\n            if math.isnan(offset):\n                raise ValueError('No data available to convert those magnitudes for that spectral type')\n            else:\n                return fromVMag + (offset*sign*-1)  # -1 as we are now converting the other way\n        else:\n            raise ValueError('Can only convert from and to V magnitude. Use .convert() instead')", "response": "Convert from or to V mag using the conversion tables"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_str(self, f, off):\n        f.seek(off)\n        return f.read(2 * struct.unpack('>B', f.read(1))[0]).decode('utf-16')", "response": "Convenience function to quickly pull out strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delta13c_constants():\n    # possible values for constants (from NIST)\n    cst = OrderedDict()\n    cst['Craig'] = {'S13': 0.0112372, 'S18': 0.002079,\n                    'K': 0.008333, 'A': 0.5}\n    cst['IAEA'] = {'S13': 0.0112372, 'S18': 0.00206716068,\n                   'K': 0.0091993, 'A': 0.5}\n    cst['Werner'] = {'S13': 0.0112372, 'S18': 0.0020052,\n                     'K': 0.0093704, 'A': 0.516}\n    cst['Santrock'] = {'S13': 0.0112372, 'S18': 0.0020052,\n                       'K': 0.0099235, 'A': 0.516}\n    cst['Assonov'] = {'S13': 0.0112372, 'S18': 0.0020052,\n                      'K': 0.0102819162, 'A': 0.528}\n    cst['Assonov2'] = {'S13': 0.0111802, 'S18': 0.0020052,\n                       'K': 0.0102819162, 'A': 0.528}\n    cst['Isodat'] = {'S13': 0.0111802, 'S18': 0.0020052,\n                     'K': 0.0099235, 'A': 0.516}\n    return cst", "response": "Return a dict of delta13C constants for calculating delta13C values from ratios."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the delta - 13C of a sample using the delta - 13C algorithm from Santrock and Studley Hayes 1985 Anal. Chem.", "response": "def delta13c_santrock(r45sam, r46sam, d13cstd, r45std, r46std,\n                      ks='Santrock', d18ostd=23.5):\n    \"\"\"\n    Given the measured isotope signals of a sample and a\n    standard and the delta-13C of that standard, calculate\n    the delta-13C of the sample.\n\n    Algorithm from Santrock, Studley & Hayes 1985 Anal. Chem.\n    \"\"\"\n    k = delta13c_constants()[ks]\n\n    # function for calculating 17R from 18R\n    def c17(r):\n        return k['K'] * r ** k['A']\n    rcpdb, rosmow = k['S13'], k['S18']\n\n    # known delta values for the ref peak\n    r13std = (d13cstd / 1000. + 1) * rcpdb\n    r18std = (d18ostd / 1000. + 1) * rosmow\n\n    # determine the correction factors\n    c45 = r13std + 2 * c17(r18std)\n    c46 = c17(r18std) ** 2 + 2 * r13std * c17(r18std) + 2 * r18std\n\n    # correct the voltage ratios to ion ratios\n    r45 = (r45sam / r45std) * c45\n    r46 = (r46sam / r46std) * c46\n\n    def rf(r18):\n        return -3 * c17(r18) ** 2 + 2 * r45 * c17(r18) + 2 * r18 - r46\n    # r18 = scipy.optimize.root(rf, r18std).x[0]  # use with scipy 0.11.0\n    r18 = fsolve(rf, r18std)[0]\n    r13 = r45 - 2 * c17(r18)\n    return 1000 * (r13 / rcpdb - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resource_copy(package_or_requirement, resource_name, destination):\n    '''\n    Copy file/dir resource to destination.\n\n    Parameters\n    ----------\n    package_or_requirement : str\n    resource_name : str\n    destination : ~pathlib.Path\n        Path to copy to, it must not exist.\n    '''\n    args = package_or_requirement, resource_name\n    if resource_isdir(*args):\n        destination.mkdir()\n        for name in resource_listdir(*args):\n            resource_copy(\n                package_or_requirement,\n                str(Path(resource_name) / name),\n                destination / name\n            )\n    else:\n        with destination.open('wb') as f:\n            with resource_stream(*args) as source:\n                shutil.copyfileobj(source, f)", "response": "Copy file or dir resource to destination."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a Series, return a list of tuples indicating when peaks start and stop and what their baseline is. [(t_start, t_end, hints) ...]", "response": "def simple_peak_find(s, init_slope=500, start_slope=500, end_slope=200,\n                     min_peak_height=50, max_peak_width=1.5):\n    \"\"\"\n    Given a Series, return a list of tuples indicating when\n    peaks start and stop and what their baseline is.\n    [(t_start, t_end, hints) ...]\n    \"\"\"\n    point_gap = 10\n\n    def slid_win(itr, size=2):\n        \"\"\"Returns a sliding window of size 'size' along itr.\"\"\"\n        itr, buf = iter(itr), []\n        for _ in range(size):\n            buf += [next(itr)]\n        for l in itr:\n            yield buf\n            buf = buf[1:] + [l]\n        yield buf\n\n    # TODO: check these smoothing defaults\n    y, t = s.values, s.index.astype(float)\n    smooth_y = movingaverage(y, 9)\n    dxdt = np.gradient(smooth_y) / np.gradient(t)\n    # dxdt = -savitzkygolay(ts, 5, 3, deriv=1).y / np.gradient(t)\n\n    init_slopes = np.arange(len(dxdt))[dxdt > init_slope]\n    if len(init_slopes) == 0:\n        return []\n    # get the first points of any \"runs\" as a peak start\n    # runs can have a gap of up to 10 points in them\n    peak_sts = [init_slopes[0]]\n    peak_sts += [j for i, j in slid_win(init_slopes, 2) if j - i > 10]\n    peak_sts.sort()\n\n    en_slopes = np.arange(len(dxdt))[dxdt < -end_slope]\n    if len(en_slopes) == 0:\n        return []\n    # filter out any lone points farther than 10 away from their neighbors\n    en_slopes = [en_slopes[0]]\n    en_slopes += [i[1] for i in slid_win(en_slopes, 3)\n                  if i[1] - i[0] < point_gap or i[2] - i[1] < point_gap]\n    en_slopes += [en_slopes[-1]]\n    # get the last points of any \"runs\" as a peak end\n    peak_ens = [j for i, j in slid_win(en_slopes[::-1], 2)\n                if i - j > point_gap] + [en_slopes[-1]]\n    peak_ens.sort()\n    # avals = np.arange(len(t))[np.abs(t - 0.675) < 0.25]\n    # print([i for i in en_slopes if i in avals])\n    # print([(t[i], i) for i in peak_ens if i in avals])\n\n    peak_list = []\n    pk2 = 0\n    for pk in peak_sts:\n        # don't allow overlapping peaks\n        if pk < pk2:\n            continue\n\n        # track backwards to find the true start\n        while dxdt[pk] > start_slope and pk > 0:\n            pk -= 1\n\n        # now find where the peak ends\n        dist_to_end = np.array(peak_ens) - pk\n        pos_end = pk + dist_to_end[dist_to_end > 0]\n        for pk2 in pos_end:\n            if (y[pk2] - y[pk]) / (t[pk2] - t[pk]) > start_slope:\n                # if the baseline beneath the peak is too large, let's\n                # keep going to the next dip\n                peak_list.append({'t0': t[pk], 't1': t[pk2]})\n                pk = pk2\n            elif t[pk2] - t[pk] > max_peak_width:\n                # make sure that peak is short enough\n                pk2 = pk + np.abs(t[pk:] - t[pk] - max_peak_width).argmin()\n                break\n            else:\n                break\n        else:\n            # if no end point is found, the end point\n            # is the end of the timeseries\n            pk2 = len(t) - 1\n\n        if pk == pk2:\n            continue\n        pk_hgt = max(y[pk:pk2]) - min(y[pk:pk2])\n        if pk_hgt < min_peak_height:\n            continue\n        peak_list.append({'t0': t[pk], 't1': t[pk2]})\n    return peak_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(zk, path='/'):\n    children = zk.get_children(path)\n    yield path\n    for child in children:\n        if path == '/':\n            subpath = \"/%s\" % child\n        else:\n            subpath = \"%s/%s\" % (path, child)\n\n        for child in walk(zk, subpath):\n            yield child", "response": "Yields all paths under path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invert(dict_): #TODO return a MultiDict right away\n    '''\n    Invert dict by swapping each value with its key.\n\n    Parameters\n    ----------\n    dict_ : ~typing.Dict[~typing.Hashable, ~typing.Hashable]\n        Dict to invert.\n\n    Returns\n    -------\n    ~typing.Dict[~typing.Hashable, ~typing.Set[~typing.Hashable]]\n        Dict with keys and values swapped.\n\n    See also\n    --------\n    pytil.multi_dict.MultiDict : Multi-dict view of a ``Dict[Hashable, Set[Hashable]]`` dict.\n\n    Notes\n    -----\n    If your dict never has 2 keys mapped to the same value, you can convert it\n    to a ``Dict[Hashable, Hashable]`` dict using::\n\n        from pytil.multi_dict import MultiDict\n        inverted_dict = dict(MultiDict(inverted_dict))\n\n    Examples\n    --------\n    >>> invert({1: 2, 3: 4})\n    {2: {1}, 4: {3}}\n\n    >>> invert({1: 2, 3: 2, 4: 5})\n    {2: {1,3}, 5: {4}}\n    '''\n    result = defaultdict(lambda: set())\n    for k, val in dict_.items():\n        result[val].add(k)\n    return dict(result)", "response": "Invert a dict by swapping each value with its key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, json=None):\n        return self._call('delete', url=self.endpoint, json=json)", "response": "Send a DELETE request and return the JSON decoded result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a POST request and return the JSON decoded result.", "response": "def get(self, params=None):\n        \"\"\"Send a POST request and return the JSON decoded result.\n\n        Args:\n            params (dict, optional): Mapping of parameters to send in request.\n\n        Returns:\n            mixed: JSON decoded response data.\n        \"\"\"\n        return self._call('get', url=self.endpoint, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, json=None):\n        return self._call('post', url=self.endpoint, json=json)", "response": "Send a POST request and return the JSON decoded result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put(self, json=None):\n        return self._call('put', url=self.endpoint, json=json)", "response": "Send a PUT request and return the JSON decoded result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _call(self, method, *args, **kwargs):\n\n        assert self.session\n\n        if not kwargs.get('verify'):\n            kwargs['verify'] = self.SSL_VERIFY\n\n        response = self.session.request(method, *args, **kwargs)\n        response_json = response.text and response.json() or {}\n\n        if response.status_code < 200 or response.status_code >= 300:\n            message = response_json.get('error', response_json.get('message'))\n            raise HelpScoutRemoteException(response.status_code, message)\n\n        self.page_current = response_json.get(self.PAGE_CURRENT, 1)\n        self.page_total = response_json.get(self.PAGE_TOTAL, 1)\n\n        try:\n            return response_json[self.PAGE_DATA_MULTI]\n        except KeyError:\n            pass\n\n        try:\n            return [response_json[self.PAGE_DATA_SINGLE]]\n        except KeyError:\n            pass\n\n        return None", "response": "Call the remote service and return the response data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _locate_bin(bins, n):\n    while bins[n] != n:\n        n = bins[n]\n    return n", "response": "Find the bin where list n has ended up."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a RequestPaginator that returns the users that are associated to a mailbox.", "response": "def find_in_mailbox(cls, session, mailbox_or_id):\n        \"\"\"Get the users that are associated to a Mailbox.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            mailbox_or_id (MailboxRef or int): Mailbox of the ID of the\n                mailbox to get the folders for.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.models.User): Users\n                iterator.\n        \"\"\"\n        if hasattr(mailbox_or_id, 'id'):\n            mailbox_or_id = mailbox_or_id.id\n        return cls(\n            '/mailboxes/%d/users.json' % mailbox_or_id,\n            session=session,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nschedules a precompute task for a panel.", "response": "def enable_precompute(panel):\n  \"\"\"Schedule a precompute task for `panel`\"\"\"\n  use_metis = panel['data_source']['source_type'] == 'querybuilder'\n  if use_metis:\n    query = panel['data_source']['query']\n  else:\n    query = \"u'''%s'''\" % panel['data_source']['code']\n  precompute = panel['data_source']['precompute']\n  timeframe = panel['data_source']['timeframe']\n  bucket_width = precompute['bucket_width']['value']\n  time_scale = precompute['bucket_width']['scale']['name']\n  bucket_width_seconds = get_seconds(bucket_width, time_scale)\n\n  if timeframe['mode']['value'] == 'recent':\n    untrusted_time = precompute['untrusted_time']['value']\n    untrusted_time_scale = precompute['untrusted_time']['scale']['name']\n    untrusted_time_seconds = get_seconds(untrusted_time, untrusted_time_scale)\n    # Schedule the task with an interval equal to the bucket_width\n    interval = bucket_width_seconds\n  elif timeframe['mode']['value'] == 'range':\n    untrusted_time_seconds = 0\n    # Schedule the task with an interval of 0 so it only runs once\n    interval = 0\n\n  task_code = PRECOMPUTE_INITIALIZATION_CODE % (query, timeframe,\n                                                bucket_width_seconds,\n                                                untrusted_time_seconds,\n                                                use_metis)\n  result = scheduler_client.schedule(task_code, interval)\n\n  if result['status'] != 'success':\n    raise RuntimeError(result.get('reason'))\n\n  return result['id']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disable_precompute(panel):\n  task_id = panel['data_source']['precompute']['task_id']\n  result = scheduler_client.cancel(task_id)\n  if result['status'] != 'success':\n    raise RuntimeError(result.get('reason'))", "response": "Cancel precomputation for a panel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_timeframe_bounds(self, timeframe, bucket_width):\n    if bucket_width:\n      bucket_width_seconds = bucket_width\n      bucket_width = epoch_time_to_kronos_time(bucket_width)\n\n    # TODO(derek): Potential optimization by setting the end_time equal to the\n    # untrusted_time if end_time > untrusted_time and the results are not being\n    # output to the user (only for caching)\n    if timeframe['mode']['value'] == 'recent':\n      # Set end_time equal to now and align to bucket width\n      end_time = kronos_time_now()\n      original_end_time = end_time\n      duration = get_seconds(timeframe['value'], timeframe['scale']['name'])\n      duration = epoch_time_to_kronos_time(duration)\n      start_time = original_end_time - duration\n\n      if bucket_width:\n        # Align values to the bucket width\n        # TODO(derek): Warn the user that the timeframe has been altered to fit\n        # the bucket width\n        if (end_time % bucket_width) != 0:\n          end_time += bucket_width - (end_time % bucket_width)\n\n        if (start_time % bucket_width) != 0:\n          start_time -= (start_time % bucket_width)\n\n      start = kronos_time_to_datetime(start_time)\n      end = kronos_time_to_datetime(end_time)\n    elif timeframe['mode']['value'] == 'range':\n      end = datetime.datetime.strptime(timeframe['to'], DT_FORMAT)\n      end_seconds = datetime_to_epoch_time(end)\n\n      start = datetime.datetime.strptime(timeframe['from'], DT_FORMAT)\n      start_seconds = datetime_to_epoch_time(start)\n\n      if bucket_width:\n        # Align values to the bucket width\n        # TODO(derek): Warn the user that the timeframe has been altered to fit\n        # the bucket width\n        start_bump = start_seconds % bucket_width_seconds\n        start -= datetime.timedelta(seconds=start_bump)\n        if (end_seconds % bucket_width_seconds) != 0:\n          end_bump = bucket_width_seconds - (end_seconds % bucket_width_seconds)\n          end += datetime.timedelta(seconds=end_bump)\n    else:\n      raise ValueError(\"Timeframe mode must be 'recent' or 'range'\")\n\n    return start, end", "response": "Get a list of kronos objects that are aligned to the given timeframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_query(self, start_time, end_time, unique_id=None):\n    # XXX(derek): DEPRECATION WARNING\n    # Use of the implicit Kronos client in pycode queries is deprecated\n    client = KronosClient(self._app.config['KRONOS_URL'],\n                          namespace=self._app.config['KRONOS_NAMESPACE'],\n                          blocking=False,\n                          sleep_block=0.2)\n    locals_dict = {\n      'kronos_client': client,\n      'events': [],\n      'start_time': start_time,\n      'end_time': end_time,\n    }\n    try:\n      exec self._query in {}, locals_dict  # No globals.\n    except:\n      _, exception, tb = sys.exc_info()\n      raise PyCodeError(exception, traceback.format_tb(tb))\n\n    # Retrieve the `events` variable as computed by the pycode.\n    events = locals_dict.get('events', [])\n\n    return events", "response": "Executes a Python query string and returns events"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute(self, use_cache=True):\n    if use_cache:\n      if not self._bucket_width:\n        raise ValueError('QueryCompute must be initialized with a bucket_width'\n                         ' to use caching features.')\n      return list(self._query_cache.retrieve_interval(self._start_time,\n                                                      self._end_time,\n                                                      compute_missing=True))\n    else:\n      if self._metis:\n        return self._run_metis(self._start_time, self._end_time)\n      else:\n        return self._run_query(self._start_time, self._end_time)", "response": "Call a user defined query and return events with optional help from\n    the cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cache(self):\n    if not self._bucket_width or self._untrusted_time is None:\n      raise ValueError('QueryCompute must be initialized with a bucket_width '\n                       'and an untrusted_time in order to write to the cache.')\n\n    now = datetime.datetime.now()\n    untrusted_time = now - datetime.timedelta(seconds=self._untrusted_time)\n    list(self._query_cache.compute_and_cache_missing_buckets(\n        self._start_time,\n        self._end_time,\n        untrusted_time))", "response": "Call a user defined query and cache the results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(cls, session, first_name=None, last_name=None, email=None,\n             modified_since=None):\n        \"\"\"List the customers.\n\n        Customers can be filtered on any combination of first name, last name,\n        email, and modifiedSince.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            first_name (str, optional): First name of customer.\n            last_name (str, optional): Last name of customer.\n            email (str, optional): Email address of customer.\n            modified_since (datetime.datetime, optional): If modified after\n                this date.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.models.Customer): Customers\n                iterator.\n        \"\"\"\n        return super(Customers, cls).list(\n            session,\n            data=cls.__object__.get_non_empty_vals({\n                'firstName': first_name,\n                'lastName': last_name,\n                'email': email,\n                'modifiedSince': modified_since,\n            })\n        )", "response": "List the customers. Customers can be filtered on any combination of first name last name email and modifiedSince."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for a customer given a domain.", "response": "def search(cls, session, queries):\n        \"\"\"Search for a customer given a domain.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            queries (helpscout.models.Domain or iter): The queries for the\n                domain. If a ``Domain`` object is provided, it will simply be\n                returned. Otherwise, a ``Domain`` object will be generated\n                from the complex queries. In this case, the queries should\n                conform to the interface in\n                :func:`helpscout.domain.Domain.from_tuple`.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.models.SearchCustomer):\n                SearchCustomer iterator.\n        \"\"\"\n        return super(Customers, cls).search(session, queries, SearchCustomer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates auth token generator", "response": "def create_token(key, payload):\n  \"\"\"Auth token generator\n\n  payload should be a json encodable data structure\n  \"\"\"\n  token = hmac.new(key)\n  token.update(json.dumps(payload))\n  return token.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_app(settings_file=None):\n  global _APP\n  if _APP:\n    return _APP\n  _APP = Flask(__name__)\n  db.init_app(_APP)\n  migrate = Migrate(_APP, db, directory='scheduler/migrations')\n\n  _APP.config.from_pyfile('../jia/conf/default_settings.py')\n  if settings_file:\n    if not settings_file.startswith('/'):\n      settings_file = os.path.join(os.pardir, settings_file)\n    _APP.config.from_pyfile(settings_file, silent=True)\n    \n  _APP.config.update(PORT=_APP.config['SCHEDULER_PORT'])\n  _APP.config.update(SQLALCHEMY_DATABASE_URI=_APP.config['SCHEDULER_DATABASE_URI'])\n\n  _APP.secret_key = _APP.config['SECRET_KEY']\n\n  from scheduler.views import scheduler\n  _APP.register_blueprint(scheduler)\n\n  return _APP", "response": "Get the scheduler app singleton\n Returns the app instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new conversation.", "response": "def create(cls, session, record, imported=False, auto_reply=False):\n        \"\"\"Create a conversation.\n\n        Please note that conversation cannot be created with more than 100\n        threads, if attempted the API will respond with HTTP 412.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            record (helpscout.models.Conversation): The conversation\n             to be created.\n            imported (bool, optional): The ``imported`` request parameter\n             enables conversations to be created for historical purposes (i.e.\n             if moving from a different platform, you can import your\n             history). When ``imported`` is set to ``True``, no outgoing\n             emails or notifications will be generated.\n            auto_reply (bool): The ``auto_reply`` request parameter enables\n             auto replies to be sent when a conversation is created via the\n             API. When ``auto_reply`` is set to ``True``, an auto reply will\n             be sent as long as there is at least one ``customer`` thread in\n             the conversation.\n\n        Returns:\n            helpscout.models.Conversation: Newly created conversation.\n        \"\"\"\n        return super(Conversations, cls).create(\n            session,\n            record,\n            imported=imported,\n            auto_reply=auto_reply,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_attachment(cls, session, attachment):\n        return super(Conversations, cls).create(\n            session,\n            attachment,\n            endpoint_override='/attachments.json',\n            out_type=Attachment,\n        )", "response": "Create an attachment.\n\n        An attachment must be sent to the API before it can be used in a\n        thread. Use this method to create the attachment, then use the\n        resulting hash when creating a thread.\n\n        Note that HelpScout only supports attachments of 10MB or lower.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            attachment (helpscout.models.Attachment): The attachment to be\n             created.\n\n        Returns:\n            helpscout.models.Attachment: The newly created attachment (hash\n             property only). Use this hash when associating the attachment with\n             a new thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_thread(cls, session, conversation, thread, imported=False):\n        return super(Conversations, cls).create(\n            session,\n            thread,\n            endpoint_override='/conversations/%s.json' % conversation.id,\n            imported=imported,\n        )", "response": "Create a new thread in a conversation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_attachment(cls, session, attachment):\n        return super(Conversations, cls).delete(\n            session,\n            attachment,\n            endpoint_override='/attachments/%s.json' % attachment.id,\n            out_type=Attachment,\n        )", "response": "Delete an attachment.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            attachment (helpscout.models.Attachment): The attachment to\n                be deleted.\n\n        Returns:\n            NoneType: Nothing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_customer(cls, session, mailbox, customer):\n        return cls(\n            '/mailboxes/%d/customers/%s/conversations.json' % (\n                mailbox.id, customer.id,\n            ),\n            session=session,\n        )", "response": "Returns a new paginator of conversations for a specific customer in a mailbox."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_user(cls, session, mailbox, user):\n        return cls(\n            '/mailboxes/%d/users/%s/conversations.json' % (\n                mailbox.id, user.id,\n            ),\n            session=session,\n        )", "response": "Return conversations for a specific user in a mailbox."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_attachment_data(cls, session, attachment_id):\n        return cls(\n            '/attachments/%d/data.json' % attachment_id,\n            singleton=True,\n            session=session,\n            out_type=AttachmentData,\n        )", "response": "Return a specific attachment s data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(cls, session, mailbox):\n        endpoint = '/mailboxes/%d/conversations.json' % mailbox.id\n        return super(Conversations, cls).list(session, endpoint)", "response": "Returns a list of conversations in a mailbox."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_folder(cls, session, mailbox, folder):\n        return cls(\n            '/mailboxes/%d/folders/%s/conversations.json' % (\n                mailbox.id, folder.id,\n            ),\n            session=session,\n        )", "response": "Returns a new iterator that returns conversations in a specific folder of a mailbox."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(cls, session, queries):\n        return super(Conversations, cls).search(\n            session, queries, SearchConversation,\n        )", "response": "Search for a conversation given a domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_thread(cls, session, conversation, thread):\n        data = thread.to_api()\n        data['reload'] = True\n        return cls(\n            '/conversations/%s/threads/%d.json' % (\n                conversation.id, thread.id,\n            ),\n            data=data,\n            request_type=RequestPaginator.PUT,\n            singleton=True,\n            session=session,\n        )", "response": "Update a thread.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            conversation (helpscout.models.Conversation): The conversation\n                that the thread belongs to.\n            thread (helpscout.models.Thread): The thread to be updated.\n\n        Returns:\n            helpscout.models.Conversation: Conversation including freshly\n                updated thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring root logger to log INFO to stderr and DEBUG to log file.", "response": "def configure(log_file):\n    '''\n    Configure root logger to log INFO to stderr and DEBUG to log file.\n\n    The log file is appended to. Stderr uses a terse format, while the log file\n    uses a verbose unambiguous format.\n\n    Root level is set to INFO.\n\n    Parameters\n    ----------\n    log_file : ~pathlib.Path\n        File to log to.\n\n    Returns\n    -------\n    ~typing.Tuple[~logging.StreamHandler, ~logging.FileHandler]\n        Stderr and file handler respectively.\n    '''\n    # Note: do not use logging.basicConfig as it does not play along with caplog in testing\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.INFO)\n\n    # log info to stderr in terse format\n    stderr_handler = logging.StreamHandler() # to stderr\n    stderr_handler.setLevel(logging.INFO)\n    stderr_handler.setFormatter(logging.Formatter('{levelname[0]}: {message}', style='{'))\n    root_logger.addHandler(stderr_handler)\n\n    # log debug to file in full format\n    file_handler = logging.FileHandler(str(log_file))\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter('{levelname[0]} {asctime} {name} ({module}:{lineno}):\\n{message}\\n', style='{'))\n    root_logger.addHandler(file_handler)\n\n    return stderr_handler, file_handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pandasframe(self):\r\n        if self.dataset:\r\n            self._load_dimensions()\r\n            return self._get_pandasframe_one_dataset()\r\n        return self._get_pandasframe_across_datasets()", "response": "The method loads data from dataset returns a pandas. DataFrame object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new value to the list of values and index points.", "response": "def add_value(self, value, index_point):\r\n        \"\"\"The function is addeing new value to provied index. If index does not exist\"\"\"\r\n        if index_point not in self.index:\r\n            self.values.append(value)\r\n            self.index.append(index_point)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pandas_series(self):\r\n        return pandas.Series(self.values, self.index, name=self.name)", "response": "The function creates pandas series based on index and values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a file or directory or directory recursively.", "response": "def remove(path, force=False):\n    '''\n    Remove file or directory (recursively), if it exists.\n\n    On NFS file systems, if a directory contains :file:`.nfs*` temporary files\n    (sometimes created when deleting a file), it waits for them to go away.\n\n    Parameters\n    ----------\n    path : ~pathlib.Path\n        Path to remove.\n    force : bool\n        If True, will remove files and directories even if they are read-only\n        (as if first doing ``chmod -R +w``).\n    '''\n    if not path.exists():\n        return\n    else:\n        if force:\n            with suppress(FileNotFoundError):\n                chmod(path, 0o700, '+', recursive=True)\n        if path.is_dir() and not path.is_symlink():\n            # Note: shutil.rmtree did not handle NFS well\n\n            # First remove all files\n            for dir_, dirs, files in os.walk(str(path), topdown=False): # bottom-up walk\n                dir_ = Path(dir_)\n                for file in files:\n                    with suppress(FileNotFoundError):\n                        (dir_ / file).unlink()\n                for file in dirs:  # Note: os.walk treats symlinks to directories as directories\n                    file = dir_ / file\n                    if file.is_symlink():\n                        with suppress(FileNotFoundError):\n                            file.unlink()\n\n            # Now remove all dirs, being careful of any lingering .nfs* files\n            for dir_, _, _ in os.walk(str(path), topdown=False): # bottom-up walk\n                dir_ = Path(dir_)\n                with suppress(FileNotFoundError):\n                    # wait for .nfs* files\n                    children = list(dir_.iterdir())\n\n                    while children:\n                        # only wait for nfs temporary files\n                        if any(not child.name.startswith('.nfs') for child in children):\n                            dir_.rmdir()  # raises dir not empty\n\n                        # wait and go again\n                        time.sleep(.1)\n                        children = list(dir_.iterdir())\n\n                    # rm\n                    dir_.rmdir()\n        else:\n            with suppress(FileNotFoundError):\n                path.unlink()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chmod(path, mode, operator='=', recursive=False):\n    '''\n    Change file mode bits.\n\n    When recursively chmodding a directory, executable bits in ``mode`` are\n    ignored when applying to a regular file. E.g. ``chmod(path, mode=0o777,\n    recursive=True)`` would apply ``mode=0o666`` to regular files.\n\n    Symlinks are ignored.\n\n    Parameters\n    ----------\n    path : ~pathlib.Path\n        Path to chmod.\n    mode : int\n        Mode bits to apply, e.g. ``0o777``.\n    operator : str\n        How to apply the mode bits to the file, one of:\n\n        '='\n            Replace mode with given mode.\n        '+'\n            Add to current mode.\n        '-'\n            Subtract from current mode.\n\n    recursive : bool\n        Whether to chmod recursively.\n    '''\n    if mode > 0o777 and operator != '=':\n        raise ValueError('Special bits (i.e. >0o777) only supported when using \"=\" operator')\n\n    # first chmod path\n    if operator == '+':\n        mode_ = path.stat().st_mode | mode\n    elif operator == '-':\n        mode_ = path.stat().st_mode & ~mode\n    else:\n        mode_ = mode\n    if path.is_symlink():\n        # Do not chmod or follow symlinks\n        return\n    path.chmod(mode_)\n\n    # then its children\n    def chmod_children(parent, files, mode_mask, operator):\n        for file in files:\n            with suppress(FileNotFoundError):\n                file = parent / file\n                if not file.is_symlink():\n                    chmod(file, mode & mode_mask, operator)\n    if recursive and path.is_dir():\n        for parent, dirs, files in os.walk(str(path)):\n            parent = Path(parent)\n            chmod_children(parent, dirs, 0o777777, operator)\n            chmod_children(parent, files, 0o777666, operator)", "response": "Change file mode bits."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhashing a file or directory.", "response": "def hash(path, hash_function=hashlib.sha512):  # @ReservedAssignment\n    '''\n    Hash file or directory.\n\n    Parameters\n    ----------\n    path : ~pathlib.Path\n        File or directory to hash.\n    hash_function : ~typing.Callable[[], hash object]\n        Function which creates a hashlib hash object when called. Defaults to\n        ``hashlib.sha512``.\n\n    Returns\n    -------\n    hash object\n        hashlib hash object of file/directory contents. File/directory stat data\n        is ignored. The directory digest covers file/directory contents and\n        their location relative to the directory being digested. The directory\n        name itself is ignored.\n    '''\n    hash_ = hash_function()\n    if path.is_dir():\n        for directory, directories, files in os.walk(str(path), topdown=True):\n            # Note:\n            # - directory: path to current directory in walk relative to current working direcotry\n            # - directories/files: dir/file names\n\n            # Note: file names can contain nearly any character (even newlines).\n\n            # hash like (ignore the whitespace):\n            #\n            #   h(relative-dir-path)\n            #   h(dir_name)\n            #   h(dir_name2)\n            #   ,\n            #   h(file_name) h(file_content)\n            #   h(file_name2) h(file_content2)\n            #   ;\n            #   h(relative-dir-path2)\n            #   ...\n            hash_.update(hash_function(str(Path(directory).relative_to(path)).encode()).digest())\n            for name in sorted(directories):\n                hash_.update(hash_function(name.encode()).digest())\n            hash_.update(b',')\n            for name in sorted(files):\n                hash_.update(hash_function(name.encode()).digest())\n                hash_.update(hash(Path(directory) / name).digest())\n            hash_.update(b';')\n    else:\n        with path.open('rb') as f:\n            while True:\n                buffer = f.read(65536)\n                if not buffer:\n                    break\n                hash_.update(buffer)\n    return hash_"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_orderrun(self, orderrun_id):\n        rc = DKReturnCode()\n        if orderrun_id == 'good':\n            rc.set(rc.DK_SUCCESS, None, None)\n        else:\n            rc.set(rc.DK_FAIL, 'ServingDeleteV2: unable to delete OrderRun')\n        return rc", "response": "This method deletes the order run with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diff_identifiers(a, b):\n\n    a_ids = set(a.identifiers)\n    b_ids = set(b.identifiers)\n\n    difference = []\n\n    for i in a_ids.difference(b_ids):\n        difference.append((i, True, False))\n    for i in b_ids.difference(a_ids):\n        difference.append((i, False, True))\n\n    return difference", "response": "Return list of tuples where identifiers in datasets differ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diff_sizes(a, b, progressbar=None):\n    difference = []\n\n    for i in a.identifiers:\n        a_size = a.item_properties(i)[\"size_in_bytes\"]\n        b_size = b.item_properties(i)[\"size_in_bytes\"]\n        if a_size != b_size:\n            difference.append((i, a_size, b_size))\n        if progressbar:\n            progressbar.update(1)\n\n    return difference", "response": "Returns list of tuples where sizes differ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of tuples where content differ.", "response": "def diff_content(a, reference, progressbar=None):\n    \"\"\"Return list of tuples where content differ.\n\n    Tuple structure:\n    (identifier, hash in a, hash in reference)\n\n    Assumes list of identifiers in a and b are identical.\n\n    Storage broker of reference used to generate hash for files in a.\n\n    :param a: first :class:`dtoolcore.DataSet`\n    :param b: second :class:`dtoolcore.DataSet`\n    :returns: list of tuples for all items with different content\n    \"\"\"\n    difference = []\n\n    for i in a.identifiers:\n        fpath = a.item_content_abspath(i)\n        calc_hash = reference._storage_broker.hasher(fpath)\n        ref_hash = reference.item_properties(i)[\"hash\"]\n        if calc_hash != ref_hash:\n            info = (i, calc_hash, ref_hash)\n            difference.append(info)\n        if progressbar:\n            progressbar.update(1)\n\n    return difference"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _insert(self, namespace, stream, events, configuration):\n    max_items = configuration['max_items']\n    for _id, event in events:\n      while len(self.db[namespace][stream]) >= max_items:\n        self.db[namespace][stream].pop(0)\n      bisect.insort(self.db[namespace][stream], Event(_id, event))", "response": "Insert the events into the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete events with id > = start_id and end_time < end_time.", "response": "def _delete(self, namespace, stream, start_id, end_time, configuration):\n    \"\"\"\n    Delete events with id > `start_id` and end_time <= `end_time`.\n    \"\"\"\n    start_id_event = Event(start_id)\n    end_id_event = Event(uuid_from_kronos_time(end_time,\n                                               _type=UUIDType.HIGHEST))\n    stream_events = self.db[namespace][stream]\n\n    # Find the interval our events belong to.\n    lo = bisect.bisect_left(stream_events, start_id_event)\n    if lo + 1 > len(stream_events):\n      return 0, []\n    if stream_events[lo] == start_id_event:\n      lo += 1\n    hi = bisect.bisect_right(stream_events, end_id_event)\n\n    del stream_events[lo:hi]\n    return max(0, hi - lo), []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the events from the given stream.", "response": "def _retrieve(self, namespace, stream, start_id, end_time, order, limit,\n                configuration):\n    \"\"\"\n    Yield events from stream starting after the event with id `start_id` until\n    and including events with timestamp `end_time`.\n    \"\"\"\n    start_id_event = Event(start_id)\n    end_id_event = Event(uuid_from_kronos_time(end_time,\n                                               _type=UUIDType.HIGHEST))\n    stream_events = self.db[namespace][stream]\n\n    # Find the interval our events belong to.\n    lo = bisect.bisect_left(stream_events, start_id_event)\n    if lo + 1 > len(stream_events):\n      return\n    if stream_events[lo] == start_id_event:\n      lo += 1\n    hi = bisect.bisect_right(stream_events, end_id_event)\n\n    if order == ResultOrder.DESCENDING:\n      index_it = xrange(hi - 1, lo - 1, -1)\n    else:\n      index_it = xrange(lo, hi)\n\n    for i in index_it:\n      if limit <= 0:\n        break\n      limit -= 1\n      yield marshal.dumps(stream_events[i])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse command line options from the command line", "response": "def cli():\n    \"\"\"Parse options from the command line\"\"\"\n    parser = argparse.ArgumentParser(prog=\"sphinx-serve\",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n                                     conflict_handler=\"resolve\",\n                                     description=__doc__\n                                     )\n\n    parser.add_argument(\"-v\", \"--version\", action=\"version\",\n                        version=\"%(prog)s {0}\".format(__version__)\n                        )\n\n    parser.add_argument(\"-h\", \"--host\", action=\"store\",\n                        default=\"0.0.0.0\",\n                        help=\"Listen to the given hostname\"\n                        )\n\n    parser.add_argument(\"-p\", \"--port\", action=\"store\",\n                        type=int, default=8081,\n                        help=\"Listen to given port\"\n                        )\n\n    parser.add_argument(\"-b\", \"--build\", action=\"store\",\n                        default=\"_build\",\n                        help=\"Build folder name\"\n                        )\n\n    parser.add_argument(\"-s\", \"--single\", action=\"store_true\",\n                        help=\"Serve the single-html documentation version\"\n                        )\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_build_dir(path, build=\"_build\"):\n    path = os.path.abspath(os.path.expanduser(path))\n    contents = os.listdir(path)\n    filtered_contents = [directory for directory in contents\n                            if os.path.isdir(os.path.join(path, directory))]\n\n    if build in filtered_contents:\n        return os.path.join(path, build)\n    else:\n        if path == os.path.realpath(\"/\"):\n            return None\n        else:\n            return find_build_dir(\"{0}/..\".format(path), build)", "response": "try to guess the build folder s location"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flip_uuid_parts(uuid):\n  flipped_uuid = uuid.split('-')\n  flipped_uuid[0], flipped_uuid[2] = flipped_uuid[2], flipped_uuid[0]\n  flipped_uuid = '-'.join(flipped_uuid)\n  return flipped_uuid", "response": "Flips high and low segments of the timestamp portion of a UUID string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates example star, if binaryLetter is true creates a parent binary object, if heirarchy is true will create a system and link everything up", "response": "def genExampleStar(binaryLetter='', heirarchy=True):\n    \"\"\" generates example star, if binaryLetter is true creates a parent binary object, if heirarchy is true will create a\n    system and link everything up\n    \"\"\"\n\n    starPar = StarParameters()\n    starPar.addParam('age', '7.6')\n    starPar.addParam('magB', '9.8')\n    starPar.addParam('magH', '7.4')\n    starPar.addParam('magI', '7.6')\n    starPar.addParam('magJ', '7.5')\n    starPar.addParam('magK', '7.3')\n    starPar.addParam('magV', '9.0')\n    starPar.addParam('mass', '0.98')\n    starPar.addParam('metallicity', '0.43')\n    starPar.addParam('name', 'Example Star {0}{1}'.format(ac._ExampleSystemCount, binaryLetter))\n    starPar.addParam('name', 'HD {0}{1}'.format(ac._ExampleSystemCount, binaryLetter))\n    starPar.addParam('radius', '0.95')\n    starPar.addParam('spectraltype', 'G5')\n    starPar.addParam('temperature', '5370')\n\n    exampleStar = Star(starPar.params)\n    exampleStar.flags.addFlag('Fake')\n\n    if heirarchy:\n        if binaryLetter:\n            exampleBinary = genExampleBinary()\n            exampleBinary._addChild(exampleStar)\n            exampleStar.parent = exampleBinary\n        else:\n            exampleSystem = genExampleSystem()\n            exampleSystem._addChild(exampleStar)\n            exampleStar.parent = exampleSystem\n\n    return exampleStar"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a fake planet with some defaults", "response": "def genExamplePlanet(binaryLetter=''):\n    \"\"\" Creates a fake planet with some defaults\n    :param `binaryLetter`: host star is part of a binary with letter binaryletter\n    :return:\n    \"\"\"\n\n    planetPar = PlanetParameters()\n    planetPar.addParam('discoverymethod', 'transit')\n    planetPar.addParam('discoveryyear', '2001')\n    planetPar.addParam('eccentricity', '0.09')\n    planetPar.addParam('inclination', '89.2')\n    planetPar.addParam('lastupdate', '12/12/08')\n    planetPar.addParam('mass', '3.9')\n    planetPar.addParam('name', 'Example Star {0}{1} b'.format(ac._ExampleSystemCount, binaryLetter))\n    planetPar.addParam('period', '111.2')\n    planetPar.addParam('radius', '0.92')\n    planetPar.addParam('semimajoraxis', '0.449')\n    planetPar.addParam('temperature', '339.6')\n    planetPar.addParam('transittime', '2454876.344')\n    planetPar.addParam('separation', '330', {'unit': 'AU'})\n\n    examplePlanet = Planet(planetPar.params)\n    examplePlanet.flags.addFlag('Fake')\n\n    exampleStar = genExampleStar(binaryLetter=binaryLetter)\n    exampleStar._addChild(examplePlanet)\n    examplePlanet.parent = exampleStar\n\n    return examplePlanet"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_list(backend):\n    click.secho('Print Configuration', fg='green')\n    print str(backend.dki.get_config())", "response": "Print the current configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recipe_status(backend):\n    kitchen = DKCloudCommandRunner.which_kitchen_name()\n    if kitchen is None:\n        raise click.ClickException('You are not in a Kitchen')\n    recipe_dir = DKRecipeDisk.find_recipe_root_dir()\n    if recipe_dir is None:\n        raise click.ClickException('You must be in a Recipe folder')\n    recipe_name = DKRecipeDisk.find_recipe_name()\n    click.secho(\"%s - Getting the status of Recipe '%s' in Kitchen '%s'\\n\\tversus directory '%s'\" % (\n        get_datetime(), recipe_name, kitchen, recipe_dir), fg='green')\n    check_and_print(DKCloudCommandRunner.recipe_status(backend.dki, kitchen, recipe_name, recipe_dir))", "response": "Get the status of the current recipe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recipe_conflicts(backend):\n    recipe_dir = DKRecipeDisk.find_recipe_root_dir()\n    if recipe_dir is None:\n        raise click.ClickException('You must be in a Recipe folder.')\n    recipe_name = DKRecipeDisk.find_recipe_name()\n    click.secho(\"%s - Checking for conflicts on Recipe '%s'\" % (\n        get_datetime(),recipe_name))\n    recipe_name = DKRecipeDisk.find_recipe_name()\n    check_and_print(DKCloudCommandRunner.get_unresolved_conflicts(recipe_name, recipe_dir))", "response": "Check if there are unresolved conflicts for this Recipe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kitchen_get(backend, kitchen_name, recipe):\n    found_kitchen = DKKitchenDisk.find_kitchen_name()\n    if found_kitchen is not None and len(found_kitchen) > 0:\n        raise click.ClickException(\"You cannot get a kitchen into an existing kitchen directory structure.\")\n\n    if len(recipe) > 0:\n        click.secho(\"%s - Getting kitchen '%s' and the recipes %s\" % (get_datetime(), kitchen_name, str(recipe)), fg='green')\n    else:\n        click.secho(\"%s - Getting kitchen '%s'\" % (get_datetime(), kitchen_name), fg='green')\n\n    check_and_print(DKCloudCommandRunner.get_kitchen(backend.dki, kitchen_name, os.getcwd(), recipe))", "response": "Get an existing Kitchen"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new kitchen", "response": "def kitchen_create(backend, parent, kitchen):\n    \"\"\"\n    Create a new kitchen\n    \"\"\"\n    click.secho('%s - Creating kitchen %s from parent kitchen %s' % (get_datetime(), kitchen, parent), fg='green')\n    master = 'master'\n    if kitchen.lower() != master.lower():\n        check_and_print(DKCloudCommandRunner.create_kitchen(backend.dki, parent, kitchen))\n    else:\n        raise click.ClickException('Cannot create a kitchen called %s' % master)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kitchen_delete(backend, kitchen):\n    click.secho('%s - Deleting kitchen %s' % (get_datetime(), kitchen), fg='green')\n    master = 'master'\n    if kitchen.lower() != master.lower():\n        check_and_print(DKCloudCommandRunner.delete_kitchen(backend.dki, kitchen))\n    else:\n        raise click.ClickException('Cannot delete the kitchen called %s' % master)", "response": "Provide the name of the kitchen to delete"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kitchen_config(backend, kitchen, add, get, unset, listall):\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    check_and_print(DKCloudCommandRunner.config_kitchen(backend.dki, use_kitchen, add, get, unset, listall))", "response": "Get and Set Kitchen variable overrides"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting the Recipes in a Kitchen", "response": "def recipe_list(backend, kitchen):\n    \"\"\"\n    List the Recipes in a Kitchen\n    \"\"\"\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    click.secho(\"%s - Getting the list of Recipes for Kitchen '%s'\" % (get_datetime(), use_kitchen), fg='green')\n    check_and_print(DKCloudCommandRunner.list_recipe(backend.dki, use_kitchen))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recipe_create(backend, kitchen, name):\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    click.secho(\"%s - Creating Recipe %s for Kitchen '%s'\" % (get_datetime(), name, use_kitchen), fg='green')\n    check_and_print(DKCloudCommandRunner.recipe_create(backend.dki, use_kitchen,name))", "response": "Create a new Recipe"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recipe_get(backend, recipe):\n    recipe_root_dir = DKRecipeDisk.find_recipe_root_dir()\n    if recipe_root_dir is None:\n        if recipe is None:\n            raise click.ClickException(\"\\nPlease change to a recipe folder or provide a recipe name arguement\")\n\n        # raise click.ClickException('You must be in a Recipe folder')\n        kitchen_root_dir = DKKitchenDisk.is_kitchen_root_dir()\n        if not kitchen_root_dir:\n            raise click.ClickException(\"\\nPlease change to a recipe folder or a kitchen root dir.\")\n        recipe_name = recipe\n        start_dir = DKKitchenDisk.find_kitchen_root_dir()\n    else:\n        recipe_name = DKRecipeDisk.find_recipe_name()\n        if recipe is not None:\n            if recipe_name != recipe:\n                raise click.ClickException(\"\\nThe recipe name argument '%s' is inconsistent with the current directory '%s'\" % (recipe, recipe_root_dir))\n        start_dir = recipe_root_dir\n\n    kitchen_name = Backend.get_kitchen_name_soft()\n    click.secho(\"%s - Getting the latest version of Recipe '%s' in Kitchen '%s'\" % (get_datetime(), recipe_name, kitchen_name), fg='green')\n    check_and_print(DKCloudCommandRunner.get_recipe(backend.dki, kitchen_name, recipe_name, start_dir))", "response": "Get the latest files for this recipe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recipe_compile(backend, kitchen, recipe, variation):\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n\n    if recipe is None:\n        recipe = DKRecipeDisk.find_recipe_name()\n        if recipe is None:\n            raise click.ClickException('You must be in a recipe folder, or provide a recipe name.')\n\n    click.secho('%s - Get the Compiled OrderRun of Recipe %s.%s in Kitchen %s' % (get_datetime(), recipe, variation, use_kitchen),\n                fg='green')\n    check_and_print(DKCloudCommandRunner.get_compiled_serving(backend.dki, use_kitchen, recipe, variation))", "response": "Get the Compiled OrderRun of a Recipe"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a newly created file to a Recipe", "response": "def file_add(backend, kitchen, recipe, message, filepath):\n    \"\"\"\n    Add a newly created file to a Recipe\n    \"\"\"\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    if recipe is None:\n        recipe = DKRecipeDisk.find_recipe_name()\n        if recipe is None:\n            raise click.ClickException('You must be in a recipe folder, or provide a recipe name.')\n\n    click.secho('%s - Adding File (%s) to Recipe (%s) in kitchen(%s) with message (%s)' %\n                (get_datetime(), filepath, recipe, use_kitchen, message), fg='green')\n    check_and_print(DKCloudCommandRunner.add_file(backend.dki, use_kitchen, recipe, message, filepath))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file_update_all(backend, message, dryrun):\n    kitchen = DKCloudCommandRunner.which_kitchen_name()\n    if kitchen is None:\n        raise click.ClickException('You must be in a Kitchen')\n    recipe_dir = DKRecipeDisk.find_recipe_root_dir()\n    if recipe_dir is None:\n        raise click.ClickException('You must be in a Recipe folder')\n    recipe = DKRecipeDisk.find_recipe_name()\n\n    if dryrun:\n        click.secho('%s - Display all changed files in Recipe (%s) in Kitchen(%s) with message (%s)' %\n                    (get_datetime(), recipe, kitchen, message), fg='green')\n    else:\n        click.secho('%s - Updating all changed files in Recipe (%s) in Kitchen(%s) with message (%s)' %\n                    (get_datetime(), recipe, kitchen, message), fg='green')\n    check_and_print(DKCloudCommandRunner.update_all_files(backend.dki, kitchen, recipe, recipe_dir, message, dryrun))", "response": "Update all of the changed files for this Recipe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file_resolve(backend, filepath):\n    recipe = DKRecipeDisk.find_recipe_name()\n    if recipe is None:\n        raise click.ClickException('You must be in a recipe folder.')\n\n    click.secho(\"%s - Resolving conflicts\" % get_datetime())\n\n    for file_to_resolve in filepath:\n        if not os.path.exists(file_to_resolve):\n            raise click.ClickException('%s does not exist' % file_to_resolve)\n        check_and_print(DKCloudCommandRunner.resolve_conflict(file_to_resolve))", "response": "Resolve conflicts in a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef active_serving_watcher(backend, kitchen, period):\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    click.secho('%s - Watching Active OrderRun Changes in Kitchen %s' % (get_datetime(), use_kitchen), fg='green')\n    DKCloudCommandRunner.watch_active_servings(backend.dki, use_kitchen, period)\n    while True:\n        try:\n            DKCloudCommandRunner.join_active_serving_watcher_thread_join()\n            if not DKCloudCommandRunner.watcher_running():\n                break\n        except KeyboardInterrupt:\n            print 'KeyboardInterrupt'\n            exit_gracefully(None, None)\n    exit(0)", "response": "Watches all cooking Recipes in a Kitchen and returns a list of all cooking Recipes in a Kitchen folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns an order: cook a recipe variation", "response": "def order_run(backend, kitchen, recipe, variation, node):\n    \"\"\"\n    Run an order: cook a recipe variation\n    \"\"\"\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    if recipe is None:\n        recipe = DKRecipeDisk.find_recipe_name()\n        if recipe is None:\n            raise click.ClickException('You must be in a recipe folder, or provide a recipe name.')\n\n    msg = '%s - Create an Order:\\n\\tKitchen: %s\\n\\tRecipe: %s\\n\\tVariation: %s\\n' % (get_datetime(), use_kitchen, recipe, variation)\n    if node is not None:\n        msg += '\\tNode: %s\\n' % node\n\n    click.secho(msg, fg='green')\n    check_and_print(DKCloudCommandRunner.create_order(backend.dki, use_kitchen, recipe, variation, node))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef order_delete(backend, kitchen, order_id):\n    use_kitchen = Backend.get_kitchen_name_soft(kitchen)\n    print use_kitchen\n    if use_kitchen is None and order_id is None:\n        raise click.ClickException('You must specify either a kitchen or an order_id or be in a kitchen directory')\n\n    if order_id is not None:\n        click.secho('%s - Delete an Order using id %s' % (get_datetime(), order_id), fg='green')\n        check_and_print(DKCloudCommandRunner.delete_one_order(backend.dki, order_id))\n    else:\n        click.secho('%s - Delete all orders in Kitchen %s' % (get_datetime(), use_kitchen), fg='green')\n        check_and_print(DKCloudCommandRunner.delete_all_order(backend.dki, use_kitchen))", "response": "Delete an order or all orders in a kitchen"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop an order - Turn off the serving generation ability of an order. Stop any running jobs. Keep all state around.", "response": "def order_stop(backend, order_id):\n    \"\"\"\n    Stop an order - Turn off the serving generation ability of an order.  Stop any running jobs.  Keep all state around.\n    \"\"\"\n    if order_id is None:\n        raise click.ClickException('invalid order id %s' % order_id)\n    click.secho('%s - Stop order id %s' % (get_datetime(), order_id), fg='green')\n    check_and_print(DKCloudCommandRunner.stop_order(backend.dki, order_id))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping the run of an order", "response": "def order_stop(backend, order_run_id):\n    \"\"\"\n    Stop the run of an order - Stop the running order and keep all state around.\n    \"\"\"\n    if order_run_id is None:\n        raise click.ClickException('invalid order id %s' % order_run_id)\n\n    click.secho('%s - Stop order id %s' % (get_datetime(), order_run_id), fg='green')\n    check_and_print(DKCloudCommandRunner.stop_orderrun(backend.dki, order_run_id.strip()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay information about an Order - Run", "response": "def orderrun_detail(backend, kitchen, summary, nodestatus, runstatus, log, timing, test, all_things,\n                    order_id, order_run_id, disp_order_id, disp_order_run_id):\n    \"\"\"\n    Display information about an Order-Run\n    \"\"\"\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n    # if recipe is None:\n    #     recipe = DKRecipeDisk.find_reciper_name()\n    #     if recipe is None:\n    #         raise click.ClickException('You must be in a recipe folder, or provide a recipe name.')\n    pd = dict()\n    if all_things:\n        pd['summary'] = True\n        pd['logs'] = True\n        pd['timingresults'] = True\n        pd['testresults'] = True\n        # pd['state'] = True\n        pd['status'] = True\n    if summary:\n        pd['summary'] = True\n    if log:\n        pd['logs'] = True\n    if timing:\n        pd['timingresults'] = True\n    if test:\n        pd['testresults'] = True\n    if nodestatus:\n        pd['status'] = True\n\n    if runstatus:\n        pd['runstatus'] = True\n    if disp_order_id:\n        pd['disp_order_id'] = True\n    if disp_order_run_id:\n        pd['disp_order_run_id'] = True\n\n    # if the user does not specify anything to display, show the summary information\n    if not runstatus and \\\n            not all_things and \\\n            not test and \\\n            not timing and \\\n            not log and \\\n            not nodestatus and \\\n            not summary and \\\n            not disp_order_id and \\\n            not disp_order_run_id:\n        pd['summary'] = True\n\n    if order_id is not None and order_run_id is not None:\n        raise click.ClickException(\"Cannot specify both the Order Id and the OrderRun Id\")\n    if order_id is not None:\n        pd[DKCloudCommandRunner.ORDER_ID] = order_id.strip()\n    elif order_run_id is not None:\n        pd[DKCloudCommandRunner.ORDER_RUN_ID] = order_run_id.strip()\n\n    # don't print the green thing if it is just runstatus\n    if not runstatus and not disp_order_id and not disp_order_run_id:\n        click.secho('%s - Display Order-Run details from kitchen %s' % (get_datetime(), use_kitchen), fg='green')\n    check_and_print(DKCloudCommandRunner.orderrun_detail(backend.dki, use_kitchen, pd))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_orderrun(backend, orderrun_id):\n    click.secho('%s - Deleting orderrun %s' % (get_datetime(), orderrun_id), fg='green')\n    check_and_print(DKCloudCommandRunner.delete_orderrun(backend.dki, orderrun_id.strip()))", "response": "Delete the orderrun specified by the argument."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef order_list(backend, kitchen):\n    err_str, use_kitchen = Backend.get_kitchen_from_user(kitchen)\n    if use_kitchen is None:\n        raise click.ClickException(err_str)\n\n\n    click.secho('%s - Get Order information for Kitchen %s' % (get_datetime(), use_kitchen), fg='green')\n\n    check_and_print(\n            DKCloudCommandRunner.list_order(backend.dki, use_kitchen))", "response": "List all order of a Recipe"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef secret_list(backend,path):\n    click.echo(click.style('%s - Getting the list of secrets' % get_datetime(), fg='green'))\n    check_and_print(\n        DKCloudCommandRunner.secret_list(backend.dki,path))", "response": "List all secrets in a folder"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a secret to a file", "response": "def secret_write(backend,entry):\n    \"\"\"\n    Write a secret\n    \"\"\"\n    path,value=entry.split('=')\n\n    if value.startswith('@'):\n        with open(value[1:]) as vfile:\n            value = vfile.read()\n\n    click.echo(click.style('%s - Writing secret' % get_datetime(), fg='green'))\n    check_and_print(\n        DKCloudCommandRunner.secret_write(backend.dki,path,value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint text to stream.", "response": "def puts(text, end=\"\\n\", flush=True, stream=sys.stdout):\n    \"\"\"\n    Print ``text`` to ``stream`` (default: ``sys.stdout``) and auto-flush.\n\n    This is useful for fast loops where Python's default IO buffering would\n    prevent \"realtime\" updating.\n\n    Newlines may be disabled by setting ``end`` to the empty string (``''``).\n    (This intentionally mirrors Python 3's ``print`` syntax.)\n\n    You may disable output flushing by setting ``flush=False``.\n    \"\"\"\n    stream.write(str(text) + end)\n    if flush:\n        stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stream(self):\n\n        if not hasattr(self, '_stream'):\n            if self.file is not None:\n                self._stream = self.file\n            elif self.filename is not None:\n                self._stream = open(self.filename, 'rb')\n            elif self.text is not None:\n                self._stream = StringIO(self.text)\n            elif self.data is not None:\n                self._stream = BytesIO(self.data)\n            else:\n                raise ValueError('Broken Data, all None.')\n        return self._stream", "response": "Returns a stream object on the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readlines(self, *args, **kwargs):\n        return list(iter(partial(self.readline, *args, **kwargs), u''))", "response": "Return list of all lines. Always returns list of unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the data to a file.", "response": "def save_to(self, file):\n        \"\"\"Save data to file.\n\n        Will copy by either writing out the data or using\n        :func:`shutil.copyfileobj`.\n\n        :param file: A file-like object (with a ``write`` method) or a\n                     filename.\"\"\"\n        dest = file\n\n        if hasattr(dest, 'write'):\n            # writing to a file-like\n            # only works when no unicode conversion is done\n            if self.file is not None and\\\n                    getattr(self.file, 'encoding', None) is None:\n                copyfileobj(self.file, dest)\n            elif self.filename is not None:\n                with open(self.filename, 'rb') as inp:\n                    copyfileobj(inp, dest)\n            else:\n                dest.write(self.__bytes__())\n        else:\n            # we do not use filesystem io to make sure we have the same\n            # permissions all around\n            # copyfileobj() should be efficient enough\n\n            # destination is a filename\n            with open(dest, 'wb') as out:\n                return self.save_to(out)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves data to a temporary file and returns the relevant instance of .", "response": "def temp_saved(self, suffix='', prefix='tmp', dir=None):\n        \"\"\"Saves data to temporary file and returns the relevant instance of\n        :func:`~tempfile.NamedTemporaryFile`. The resulting file is not\n        deleted upon closing, but when the context manager exits.\n\n        Other arguments are passed on to :func:`~tempfile.NamedTemporaryFile`.\n        \"\"\"\n        tmp = tempfile.NamedTemporaryFile(\n            suffix=suffix,\n            prefix=prefix,\n            dir=dir,\n            delete=False,\n        )\n\n        try:\n            self.save_to(tmp)\n            tmp.flush()\n            tmp.seek(0)\n            yield tmp\n        finally:\n            try:\n                os.unlink(tmp.name)\n            except OSError as e:\n                if e.errno != 2:\n                    reraise(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ions(self, f):\n        outside_pos = f.tell()\n        doff = find_offset(f, 4 * b'\\xff' + 'HapsSearch'.encode('ascii'))\n        # actual end of prev section is 34 bytes before, but assume 1 rec\n        f.seek(doff - 62)\n        # seek backwards to find the FFFFFFFF header\n        while True:\n            f.seek(f.tell() - 8)\n            if f.read(4) == 4 * b'\\xff':\n                break\n        f.seek(f.tell() + 64)\n        nsegments = struct.unpack('<I', f.read(4))[0]\n        for _ in range(nsegments):\n            # first 32 bytes are segment name, rest are something else?\n            f.seek(f.tell() + 96)\n            nions = struct.unpack('<I', f.read(4))[0]\n            ions = []\n            for _ in range(nions):\n                # TODO: check that itype is actually a SIM/full scan switch\n                i1, i2, _, _, _, _, itype, _ = struct.unpack('<' + 8 * 'I',\n                                                             f.read(32))\n                if itype == 0:  # SIM\n                    ions.append(i1 / 100.)\n                else:  # full scan\n                    # TODO: this might be a little hacky?\n                    #  ideally we would need to know n for this, e.g.:\n                    # ions += np.linspace(i1 / 100, i2 / 100, n).tolist()\n                    ions += np.arange(i1 / 100., i2 / 100. + 1, 1).tolist()\n            # save the file position and load the position\n            # that we were at before we started this code\n            inside_pos = f.tell()\n            f.seek(outside_pos)\n            yield ions\n            outside_pos = f.tell()\n            f.seek(inside_pos)\n        f.seek(outside_pos)", "response": "This is a generator that returns the mzs being measured during a time segment one segment at a time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a message to STDOUT.", "response": "def _emit_message(cls, message):\n        \"\"\"Print a message to STDOUT.\"\"\"\n        sys.stdout.write(message)\n        sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints an error message to STDERR.", "response": "def _emit_error(cls, message):\n        \"\"\"Print an error message to STDERR.\"\"\"\n        sys.stderr.write('ERROR: {message}\\n'.format(message=message))\n        sys.stderr.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _emit_warning(cls, message):\n        sys.stderr.write('WARNING: {message}\\n'.format(message=message))\n        sys.stderr.flush()", "response": "Print an warning message to STDERR."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_piddir(self):\n        if self.pidfile is None:\n            return\n        piddir = os.path.dirname(self.pidfile)\n        if not os.path.isdir(piddir):\n            # Create the directory with sensible mode and ownership\n            os.makedirs(piddir, 0o777 & ~self.umask)\n            os.chown(piddir, self.uid, self.gid)", "response": "Create the directory for the PID file if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the PID file and check to make sure it s not stale.", "response": "def _read_pidfile(self):\n        \"\"\"Read the PID file and check to make sure it's not stale.\"\"\"\n        if self.pidfile is None:\n            return None\n\n        if not os.path.isfile(self.pidfile):\n            return None\n\n        # Read the PID file\n        with open(self.pidfile, 'r') as fp:\n            try:\n                pid = int(fp.read())\n            except ValueError:\n                self._emit_warning('Empty or broken pidfile {pidfile}; '\n                                   'removing'.format(pidfile=self.pidfile))\n                pid = None\n\n        if pid is not None and psutil.pid_exists(pid):\n            return pid\n        else:\n            # Remove the stale PID file\n            os.remove(self.pidfile)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating write to and lock the PID file.", "response": "def _write_pidfile(self):\n        \"\"\"Create, write to, and lock the PID file.\"\"\"\n        flags = os.O_CREAT | os.O_RDWR\n        try:\n            # Some systems don't have os.O_EXLOCK\n            flags = flags | os.O_EXLOCK\n        except AttributeError:\n            pass\n        self._pid_fd = os.open(self.pidfile, flags, 0o666 & ~self.umask)\n        os.write(self._pid_fd, str(os.getpid()).encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose and removes the PID file.", "response": "def _close_pidfile(self):\n        \"\"\"Closes and removes the PID file.\"\"\"\n        if self._pid_fd is not None:\n            os.close(self._pid_fd)\n        try:\n            os.remove(self.pidfile)\n        except OSError as ex:\n            if ex.errno != errno.ENOENT:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prevent_core_dump(cls):\n        try:\n            # Try to get the current limit\n            resource.getrlimit(resource.RLIMIT_CORE)\n        except ValueError:\n            # System doesn't support the RLIMIT_CORE resource limit\n            return\n        else:\n            # Set the soft and hard limits for core dump size to zero\n            resource.setrlimit(resource.RLIMIT_CORE, (0, 0))", "response": "Prevent the process from generating a core dump."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclosing open file descriptors and redirect standard streams to the null file.", "response": "def _reset_file_descriptors(self):\n        \"\"\"Close open file descriptors and redirect standard streams.\"\"\"\n        if self.close_open_files:\n            # Attempt to determine the max number of open files\n            max_fds = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\n            if max_fds == resource.RLIM_INFINITY:\n                # If the limit is infinity, use a more reasonable limit\n                max_fds = 2048\n        else:\n            # If we're not closing all open files, we at least need to\n            # reset STDIN, STDOUT, and STDERR.\n            max_fds = 3\n\n        for fd in range(max_fds):\n            try:\n                os.close(fd)\n            except OSError:\n                # The file descriptor probably wasn't open\n                pass\n\n        # Redirect STDIN, STDOUT, and STDERR to /dev/null\n        devnull_fd = os.open(os.devnull, os.O_RDWR)\n        os.dup2(devnull_fd, 0)\n        os.dup2(devnull_fd, 1)\n        os.dup2(devnull_fd, 2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_socket(cls, stream):\n        try:\n            fd = stream.fileno()\n        except ValueError:\n            # If it has no file descriptor, it's not a socket\n            return False\n\n        sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n        try:\n            # This will raise a socket.error if it's not a socket\n            sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n        except socket.error as ex:\n            if ex.args[0] != errno.ENOTSOCK:\n                # It must be a socket\n                return True\n        else:\n            # If an exception wasn't raised, it's a socket\n            return True", "response": "Check if the given stream is a socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a PID is alive with a timeout.", "response": "def _pid_is_alive(cls, pid, timeout):\n        \"\"\"Check if a PID is alive with a timeout.\"\"\"\n        try:\n            proc = psutil.Process(pid)\n        except psutil.NoSuchProcess:\n            return False\n\n        try:\n            proc.wait(timeout=timeout)\n        except psutil.TimeoutExpired:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_detach_necessary(cls):\n        if os.getppid() == 1:\n            # Process was started by init\n            return False\n\n        if cls._is_socket(sys.stdin):\n            # If STDIN is a socket, the daemon was started by a super-server\n            return False\n\n        return True", "response": "Check if detaching the process is even necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _detach_process(self):\n        # First fork to return control to the shell\n        pid = os.fork()\n        if pid > 0:\n            # Wait for the first child, because it's going to wait and\n            # check to make sure the second child is actually running\n            # before exiting\n            os.waitpid(pid, 0)\n            sys.exit(0)\n\n        # Become a process group and session group leader\n        os.setsid()\n\n        # Fork again so the session group leader can exit and to ensure\n        # we can never regain a controlling terminal\n        pid = os.fork()\n        if pid > 0:\n            time.sleep(1)\n            # After waiting one second, check to make sure the second\n            # child hasn't become a zombie already\n            status = os.waitpid(pid, os.WNOHANG)\n            if status[0] == pid:\n                # The child is already gone for some reason\n                exitcode = status[1] % 255\n                self._emit_failed()\n                self._emit_error('Child exited immediately with exit '\n                                 'code {code}'.format(code=exitcode))\n                sys.exit(exitcode)\n            else:\n                self._emit_ok()\n                sys.exit(0)\n\n        self._reset_file_descriptors()", "response": "Detach the process via the standard double - fork method with\n            some extra magic."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _orphan_this_process(cls, wait_for_parent=False):\n        # The current PID will be the PPID of the forked child\n        ppid = os.getpid()\n\n        pid = os.fork()\n        if pid > 0:\n            # Exit parent\n            sys.exit(0)\n\n        if wait_for_parent and cls._pid_is_alive(ppid, timeout=1):\n            raise DaemonError(\n                'Parent did not exit while trying to orphan process')", "response": "Orphan the current process by forking and then waiting for the parent to exit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fork_and_supervise_child(cls):\n        pid = os.fork()\n        if pid == 0:\n            # Fork again but orphan the child this time so we'll have\n            # the original parent and the second child which is orphaned\n            # so we don't have to worry about it becoming a zombie\n            cls._orphan_this_process()\n            return\n        # Since this process is not going to exit, we need to call\n        # os.waitpid() so that the first child doesn't become a zombie\n        os.waitpid(pid, 0)\n\n        # Generate a list of PIDs to exclude when checking for processes\n        # in the group (exclude all ancestors that are in the group)\n        pgid = os.getpgrp()\n        exclude_pids = set([0, os.getpid()])\n        proc = psutil.Process()\n        while os.getpgid(proc.pid) == pgid:\n            exclude_pids.add(proc.pid)\n            proc = psutil.Process(proc.ppid())\n\n        while True:\n            try:\n                # Look for other processes in this process group\n                group_procs = []\n                for proc in psutil.process_iter():\n                    try:\n                        if (os.getpgid(proc.pid) == pgid and\n                                proc.pid not in exclude_pids):\n                            # We found a process in this process group\n                            group_procs.append(proc)\n                    except (psutil.NoSuchProcess, OSError):\n                        continue\n\n                if group_procs:\n                    psutil.wait_procs(group_procs, timeout=1)\n                else:\n                    # No processes were found in this process group\n                    # so we can exit\n                    cls._emit_message(\n                        'All children are gone. Parent is exiting...\\n')\n                    sys.exit(0)\n            except KeyboardInterrupt:\n                # Don't exit immediatedly on Ctrl-C, because we want to\n                # wait for the child processes to finish\n                cls._emit_message('\\n')\n                continue", "response": "Fork a child and then watch the process group until there are no processes in it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling a signal to terminate.", "response": "def _handle_terminate(self, signal_number, _):\n        \"\"\"Handle a signal to terminate.\"\"\"\n        signal_names = {\n            signal.SIGINT: 'SIGINT',\n            signal.SIGQUIT: 'SIGQUIT',\n            signal.SIGTERM: 'SIGTERM',\n        }\n        message = 'Terminated by {name} ({number})'.format(\n            name=signal_names[signal_number], number=signal_number)\n        self._shutdown(message, code=128+signal_number)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run(self):\n        try:\n            # Run the worker\n            self.worker()\n        except SystemExit as ex:\n            # sys.exit() was called\n            if isinstance(ex.code, int):\n                if ex.code is not None and ex.code != 0:\n                    # A custom exit code was specified\n                    self._shutdown(\n                        'Exiting with non-zero exit code {exitcode}'.format(\n                            exitcode=ex.code),\n                        ex.code)\n            else:\n                # A message was passed to sys.exit()\n                self._shutdown(\n                    'Exiting with message: {msg}'.format(msg=ex.code), 1)\n        except Exception as ex:\n            if self.detach:\n                self._shutdown('Dying due to unhandled {cls}: {msg}'.format(\n                    cls=ex.__class__.__name__, msg=str(ex)), 127)\n            else:\n                # We're not detached so just raise the exception\n                raise\n\n        self._shutdown('Shutting down normally')", "response": "Run the worker function with some custom exception handling."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the status of the daemon.", "response": "def status(self):\n        \"\"\"Get the status of the daemon.\"\"\"\n        if self.pidfile is None:\n            raise DaemonError('Cannot get status of daemon without PID file')\n\n        pid = self._read_pidfile()\n        if pid is None:\n            self._emit_message(\n                '{prog} -- not running\\n'.format(prog=self.prog))\n            sys.exit(1)\n\n        proc = psutil.Process(pid)\n        # Default data\n        data = {\n            'prog': self.prog,\n            'pid': pid,\n            'status': proc.status(),\n            'uptime': '0m',\n            'cpu': 0.0,\n            'memory': 0.0,\n        }\n\n        # Add up all the CPU and memory usage of all the\n        # processes in the process group\n        pgid = os.getpgid(pid)\n        for gproc in psutil.process_iter():\n            try:\n                if os.getpgid(gproc.pid) == pgid and gproc.pid != 0:\n                    data['cpu'] += gproc.cpu_percent(interval=0.1)\n                    data['memory'] += gproc.memory_percent()\n            except (psutil.Error, OSError):\n                continue\n\n        # Calculate the uptime and format it in a human-readable but\n        # also machine-parsable format\n        try:\n            uptime_mins = int(round((time.time() - proc.create_time()) / 60))\n            uptime_hours, uptime_mins = divmod(uptime_mins, 60)\n            data['uptime'] = str(uptime_mins) + 'm'\n            if uptime_hours:\n                uptime_days, uptime_hours = divmod(uptime_hours, 24)\n                data['uptime'] = str(uptime_hours) + 'h ' + data['uptime']\n                if uptime_days:\n                    data['uptime'] = str(uptime_days) + 'd ' + data['uptime']\n        except psutil.Error:\n            pass\n\n        template = ('{prog} -- pid: {pid}, status: {status}, '\n                    'uptime: {uptime}, %cpu: {cpu:.1f}, %mem: {memory:.1f}\\n')\n        self._emit_message(template.format(**data))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of exposed actions that are callable via the do_action method.", "response": "def list_actions(cls):\n        \"\"\"Get a list of exposed actions that are callable via the\n        ``do_action()`` method.\"\"\"\n        # Make sure these are always at the beginning of the list\n        actions = ['start', 'stop', 'restart', 'status']\n        # Iterate over the instance attributes checking for actions that\n        # have been exposed\n        for func_name in dir(cls):\n            func = getattr(cls, func_name)\n            if (not hasattr(func, '__call__') or\n                    not getattr(func, '__daemonocle_exposed__', False)):\n                # Not a function or not exposed\n                continue\n            action = func_name.replace('_', '-')\n            if action not in actions:\n                actions.append(action)\n\n        return actions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a callable action.", "response": "def get_action(self, action):\n        \"\"\"Get a callable action.\"\"\"\n        func_name = action.replace('-', '_')\n        if not hasattr(self, func_name):\n            # Function doesn't exist\n            raise DaemonError(\n                'Invalid action \"{action}\"'.format(action=action))\n\n        func = getattr(self, func_name)\n        if (not hasattr(func, '__call__') or\n                getattr(func, '__daemonocle_exposed__', False) is not True):\n            # Not a function or not exposed\n            raise DaemonError(\n                'Invalid action \"{action}\"'.format(action=action))\n\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload(self):\n        pid = self._read_pidfile()\n        if pid is None or pid != os.getpid():\n            raise DaemonError(\n                'Daemon.reload() should only be called by the daemon process '\n                'itself')\n\n        # Copy the current environment\n        new_environ = os.environ.copy()\n        new_environ['DAEMONOCLE_RELOAD'] = 'true'\n        # Start a new python process with the same arguments as this one\n        subprocess.call(\n            [sys.executable] + sys.argv, cwd=self._orig_workdir,\n            env=new_environ)\n        # Exit this process\n        self._shutdown('Shutting down for reload')", "response": "Make the daemon reload itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_windows(peak_list):\n    win_list = []\n    for t0, t1, hints in peak_list:\n        p_w = (t0, t1)\n        for w in win_list:\n            if p_w[0] <= w[0][1] and p_w[1] >= w[0][0]:\n                w[0] = (min(p_w[0], w[0][0]), max(p_w[1], w[0][1]))\n                w[1].append((t0, t1, hints))\n                break\n        else:\n            win_list.append([p_w, [(t0, t1, hints)]])\n    return win_list", "response": "Given a list of peaks bin them into windows."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_file(self, kitchen, recipe, message, api_file_key, file_contents):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen parameter')\n            return rc\n        if recipe is None or isinstance(recipe, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with recipe parameter')\n            return rc\n        if api_file_key is None or isinstance(api_file_key, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with api_file_key parameter')\n            return rc\n        if file_contents is None or isinstance(file_contents, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with file_contents parameter')\n            return rc\n        pdict = dict()\n        pdict[self.MESSAGE] = message\n        pdict[self.FILEPATH] = api_file_key\n        pdict[self.FILE] = file_contents\n        url = '%s/v2/recipe/create/%s/%s' % (self.get_url_for_direct_rest_call(), kitchen, recipe)\n        try:\n            response = requests.put(url, data=json.dumps(pdict), headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError, TypeError), c:\n            s = \"add_file: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        if DKCloudAPI._valid_response(response):\n            rc.set(rc.DK_SUCCESS, None)\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n        return rc", "response": "This method will add a file to the recipe and return a boolean indicating if the file was successfully added or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_compiled_serving(self, kitchen, recipe_name, variation_name):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen')\n            return rc\n        if recipe_name is None or isinstance(recipe_name, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with recipe_name')\n            return rc\n        if variation_name is None or isinstance(variation_name, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with variation_name')\n            return rc\n        url = '%s/v2/servings/compiled/get/%s/%s/%s' % (self.get_url_for_direct_rest_call(),\n                                                        kitchen, recipe_name, variation_name)\n        try:\n            response = requests.get(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError, TypeError), c:\n            rc.set(rc.DK_FAIL, \"get_compiled_serving: exception: %s\" % str(c))\n            return rc\n        if DKCloudAPI._valid_response(response):\n            rc.set(rc.DK_SUCCESS, None, rdict[rdict.keys()[0]])\n            return rc\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n            return rc", "response": "get the compiled version of arecipe with variables applied for a specific variation in a kitchen"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_kitchens_improved(self, from_kitchen, to_kitchen, resolved_conflicts=None):\n        rc = DKReturnCode()\n        if from_kitchen is None or isinstance(from_kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with from kitchen')\n            return rc\n        if to_kitchen is None or isinstance(to_kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with to kitchen')\n            return rc\n        url = '%s/v2/kitchen/merge/%s/%s' % (self.get_url_for_direct_rest_call(), from_kitchen, to_kitchen)\n        try:\n            if resolved_conflicts is not None and len(resolved_conflicts) > 0:\n                data = dict()\n                data['resolved_conflicts'] = resolved_conflicts\n                response = requests.post(url, data=json.dumps(data), headers=self._get_common_headers())\n            else:\n                response = requests.post(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n        except (RequestException, ValueError, TypeError), c:\n            rc.set(\"merge_kitchens: exception: %s\" % str(c))\n            return rc\n        if DKCloudAPI._valid_response(response):\n            rc.set(rc.DK_SUCCESS, None, rdict)\n            return rc\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n            return rc", "response": "This method performs the merge of kitchens and returns the new version of the merge."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the result of merging a local file with the latest version on the remote. This does not cause any side-effects on the server, and no actual merge is performed in the remote repo. /v2/file/merge/<string:kitchenname>/<string:recipename>/<path:filepath>, methods=['POST'] :param kitchen: name of the kitchen where this file lives :param recipe: name of the recipe that owns this file :param file_path: path to the file, relative to the recipe :param file_contents: contents of the file :param orig_head: sha of commit head of the branch when this file was obtained. :param last_file_sha: The sha of the file when it was obtained from the server. :return: dict", "response": "def merge_file(self, kitchen, recipe, file_path, file_contents, orig_head, last_file_sha):\n        \"\"\"\n        Returns the result of merging a local file with the latest version on the remote.\n        This does not cause any side-effects on the server, and no actual merge is performed in the remote repo.\n        /v2/file/merge/<string:kitchenname>/<string:recipename>/<path:filepath>, methods=['POST']\n        :param kitchen: name of the kitchen where this file lives\n        :param recipe: name of the recipe that owns this file\n        :param file_path: path to the file, relative to the recipe\n        :param file_contents: contents of the file\n        :param orig_head: sha of commit head of the branch when this file was obtained.\n        :param last_file_sha: The sha of the file when it was obtained from the server.\n        :return: dict\n        \"\"\"\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False or \\\n                recipe is None or isinstance(recipe, basestring) is False or \\\n                file_path is None or isinstance(file_path, basestring) is False or \\\n                orig_head is None or isinstance(orig_head, basestring) is False or \\\n                last_file_sha is None or isinstance(last_file_sha, basestring) is False or \\\n                file_contents is None:\n            rc.set(rc.DK_FAIL, 'One or more parameters is invalid. ')\n            return rc\n\n        params = dict()\n        params['orig_head'] = orig_head\n        params['last_file_sha'] = last_file_sha\n        params['content'] = file_contents\n        adjusted_file_path = file_path\n        url = '%s/v2/file/merge/%s/%s/%s' % (self.get_url_for_direct_rest_call(), kitchen, recipe, adjusted_file_path)\n        try:\n            response = requests.post(url, data=json.dumps(params), headers=self._get_common_headers())\n            rdict = self._get_json(response)\n        except (RequestException, ValueError, TypeError), c:\n            print \"merge_file: exception: %s\" % str(c)\n            return None\n        if DKCloudAPI._valid_response(response) is True and rdict is not None and isinstance(rdict, dict) is True:\n            rc.set(rc.DK_SUCCESS, None, rdict)\n            return rc\n        else:\n            rc.set(rc.DK_FAIL, str(rdict))\n            return rc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recipe_status(self, kitchen, recipe, local_dir=None):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen parameter')\n            return rc\n        if recipe is None or isinstance(recipe, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with recipe parameter')\n            return rc\n        url = '%s/v2/recipe/tree/%s/%s' % (self.get_url_for_direct_rest_call(),\n                                           kitchen, recipe)\n        try:\n            response = requests.get(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError, TypeError), c:\n            s = \"get_recipe: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        if DKCloudAPI._valid_response(response):\n            # Now get the local sha.\n            if local_dir is None:\n                check_path = os.getcwd()\n            else:\n                if os.path.isdir(local_dir) is False:\n                    print 'Local path %s does not exist' % local_dir\n                    return None\n                else:\n                    check_path = local_dir\n            local_sha = get_directory_sha(check_path)\n            remote_sha = rdict['recipes'][recipe]\n            rv = compare_sha(remote_sha, local_sha)\n            rc.set(rc.DK_SUCCESS, None, rv)\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n        return rc", "response": "This method gets the status of a recipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the status of a recipe :param self: DKCloudAPI :param kitchen: string :param recipe: string :rtype: dict", "response": "def recipe_tree(self, kitchen, recipe):\n        \"\"\"\n        gets the status of a recipe\n        :param self: DKCloudAPI\n        :param kitchen: string\n        :param recipe: string\n        :rtype: dict\n        \"\"\"\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen parameter')\n            return rc\n        if recipe is None or isinstance(recipe, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with recipe parameter')\n            return rc\n        url = '%s/v2/recipe/tree/%s/%s' % (self.get_url_for_direct_rest_call(),\n                                           kitchen, recipe)\n        try:\n            response = requests.get(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError, TypeError), c:\n            s = \"recipe_tree: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        if DKCloudAPI._valid_response(response):\n            remote_sha = rdict['recipes'][recipe]\n            rc.set(rc.DK_SUCCESS, None, remote_sha)\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_order(self, kitchen, recipe_name, variation_name, node_name=None):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen')\n            return rc\n        if recipe_name is None or isinstance(recipe_name, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with recipe_name')\n            return rc\n        if variation_name is None or isinstance(variation_name, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with variation_name')\n            return rc\n\n        if node_name is None:\n            url = '%s/v2/order/create/%s/%s/%s' % (self.get_url_for_direct_rest_call(),\n                                                   kitchen, recipe_name, variation_name)\n        else:\n            url = '%s/v2/order/create/onenode/%s/%s/%s/%s' % (self.get_url_for_direct_rest_call(),\n                                                              kitchen, recipe_name, variation_name, node_name)\n\n        try:\n            response = requests.put(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError), c:\n            s = \"create_order: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        if DKCloudAPI._valid_response(response):\n            rc.set(rc.DK_SUCCESS, None, rdict['serving_chronos_id'])\n            return rc\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n            return rc", "response": "This method creates an order for a given node and a given variation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef orderrun_detail(self, kitchen, pdict, return_all_data=False):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen')\n            return rc\n        url = '%s/v2/order/details/%s' % (self.get_url_for_direct_rest_call(),\n                                          kitchen)\n        try:\n            response = requests.post(url, data=json.dumps(pdict), headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            if False:\n                import pickle\n                pickle.dump(rdict, open(\"files/orderrun_detail.p\", \"wb\"))\n            pass\n        except (RequestException, ValueError), c:\n            s = \"orderrun_detail: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        if DKCloudAPI._valid_response(response):\n            if return_all_data is False:\n                rc.set(rc.DK_SUCCESS, None, rdict['servings'])\n            else:\n                rc.set(rc.DK_SUCCESS, None, rdict)\n            return rc\n        else:\n            arc = DKAPIReturnCode(rdict, response)\n            rc.set(rc.DK_FAIL, arc.get_message())\n            return rc", "response": "This method is used to create or update an order from an order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_order(self, kitchen, save_to_file=None):\n        rc = DKReturnCode()\n        if kitchen is None or isinstance(kitchen, basestring) is False:\n            rc.set(rc.DK_FAIL, 'issue with kitchen parameter')\n            return rc\n\n        url = '%s/v2/order/status/%s' % (self.get_url_for_direct_rest_call(), kitchen)\n        try:\n            response = requests.get(url, headers=self._get_common_headers())\n            rdict = self._get_json(response)\n            pass\n        except (RequestException, ValueError, TypeError), c:\n            s = \"get_recipe: exception: %s\" % str(c)\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        if not DKCloudAPI._valid_response(response):\n            arc = DKAPIReturnCode(rdict)\n            rc.set(rc.DK_FAIL, arc.get_message())\n        else:\n            if save_to_file is not None:\n                import pickle\n                pickle.dump(rdict, open(save_to_file, \"wb\"))\n\n            rc.set(rc.DK_SUCCESS, None, rdict)\n        return rc", "response": "List the orders for a kitchen or recipe"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _date_trunc(value, timeframe):\n  if isinstance(value, types.StringTypes):\n    value = parse(value)\n    return_as_str = True\n  else:\n    value = kronos_time_to_datetime(value)\n    return_as_str = False\n  timeframes = {\n    DateTrunc.Unit.SECOND: (lambda dt:\n                            dt - timedelta(microseconds=dt.microsecond)),\n    DateTrunc.Unit.MINUTE: (lambda dt:\n                            dt - timedelta(seconds=dt.second,\n                                           microseconds=dt.microsecond)),\n    DateTrunc.Unit.HOUR: (lambda dt:\n                          dt - timedelta(minutes=dt.minute,\n                                         seconds=dt.second,\n                                         microseconds=dt.microsecond)),\n    DateTrunc.Unit.DAY: lambda dt: dt.date(),\n    DateTrunc.Unit.WEEK: lambda dt: dt.date() - timedelta(days=dt.weekday()),\n    DateTrunc.Unit.MONTH: lambda dt: datetime(dt.year, dt.month, 1),\n    DateTrunc.Unit.YEAR: lambda dt: datetime(dt.year, 1, 1)\n  }\n  value = timeframes[timeframe](value)\n  if return_as_str:\n    return value.isoformat()\n  return datetime_to_kronos_time(value)", "response": "A date flooring function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a portion of a datetime.", "response": "def _date_part(value, part):\n  \"\"\"\n  Returns a portion of a datetime.\n\n  Returns the portion of a datetime represented by timeframe.\n  For example, _date_part('2014-08-13 05:00:00', DatePart.Unit.WEEK_DAY)\n  will return 2, for Wednesday.\n  \"\"\"\n  if isinstance(value, types.StringTypes):\n    value = parse(value)\n  else:\n    value = kronos_time_to_datetime(value)\n  parts = {\n    DatePart.Unit.SECOND: lambda dt: dt.second,\n    DatePart.Unit.MINUTE: lambda dt: dt.minute,\n    DatePart.Unit.HOUR: lambda dt: dt.hour,\n    DatePart.Unit.DAY: lambda dt: dt.day,\n    DatePart.Unit.MONTH: lambda dt: dt.month,\n    DatePart.Unit.YEAR: lambda dt: dt.year,\n    DatePart.Unit.WEEK_DAY: lambda dt: dt.weekday(),\n  }\n  result = parts[part](value)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Set(self, name, initial=None):\n        return types.Set(name, self.api, initial)", "response": "Returns a new set object with the specified name and initial members."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SortedSet(self, name, initial=None):\n        return types.SortedSet(name, self.api, initial)", "response": "Returns a new sorted set with the specified name and initial members."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Dict(self, name, initial=None, **extra):\n        return types.Dict(name, self.api, initial=initial, **extra)", "response": "Returns a new object of type Dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Queue(self, name, initial=None, maxsize=None):\n        return types.Queue(name, self.api, initial=initial, maxsize=maxsize)", "response": "Returns a new object of type Queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef LifoQueue(self, name, initial=None, maxsize=None):\n        return types.LifoQueue(name, self.api,\n                               initial=initial, maxsize=maxsize)", "response": "Returns a new object of type LifoQueue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, mapping):\n        return self.api.mset(dict((key, self.prepare_value(value))\n                                for key, value in mapping.items()))", "response": "Update the database with the key - value pairs from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename(self, old_name, new_name):\n        try:\n            self.api.rename(mkey(old_name), mkey(new_name))\n        except ResponseError, exc:\n            if \"no such key\" in exc.args:\n                raise KeyError(old_name)\n            raise", "response": "Rename key to a new name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iteritems(self, pattern=\"*\"):\n        for key in self.keys(pattern):\n            yield (key, self[key])", "response": "An iterator over all the items in the database where the keys match the pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pop(self, name):\n        name = mkey(name)\n        temp = mkey((name, \"__poptmp__\"))\n        self.rename(name, temp)\n        value = self[temp]\n        del(self[temp])\n        return value", "response": "Get and remove key from database ( atomic )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite CSV file. Parameters ---------- file : Path *args csv.DictWriter args (except the f arg) **kwargs csv.DictWriter args Examples -------- with write.csv(file) as writer: writer.writerow((1,2,3))", "response": "def csv(file, *args, **kwargs):\n    '''\n    Write CSV file.\n\n    Parameters\n    ----------\n    file : Path\n    *args\n        csv.DictWriter args (except the f arg)\n    **kwargs\n        csv.DictWriter args\n\n    Examples\n    --------\n    with write.csv(file) as writer:\n        writer.writerow((1,2,3))\n    '''\n    with file.open('w', newline='') as f:\n        yield DictWriter(f, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary fully representing the state of this object", "response": "def todict(self):\n        \"\"\"Returns a dictionary fully representing the state of this object\n        \"\"\"\n        return {\"f_key\": hb_encode(self.f_key),\n                \"alpha_key\": hb_encode(self.alpha_key),\n                \"chunks\": self.chunks,\n                \"encrypted\": self.encrypted,\n                \"iv\": hb_encode(self.iv),\n                \"hmac\": hb_encode(self.hmac)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fromdict(dict):\n        return State(hb_decode(dict[\"f_key\"]),\n                     hb_decode(dict[\"alpha_key\"]),\n                     dict[\"chunks\"],\n                     dict[\"encrypted\"],\n                     hb_decode(dict[\"iv\"]),\n                     hb_decode(dict[\"hmac\"]))", "response": "Takes a dictionary as an argument and returns a new State object containing the contents of the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the keyed HMAC for authentication of this state data.", "response": "def get_hmac(self, key):\n        \"\"\"Returns the keyed HMAC for authentication of this state data.\n\n        :param key: the key for the keyed hash function\n        \"\"\"\n        h = HMAC.new(key, None, SHA256)\n        h.update(self.iv)\n        h.update(str(self.chunks).encode())\n        h.update(self.f_key)\n        h.update(self.alpha_key)\n        h.update(str(self.encrypted).encode())\n        return h.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encrypt(self, key):\n        if (self.encrypted):\n            return\n        # encrypt\n        self.iv = Random.new().read(AES.block_size)\n        aes = AES.new(key, AES.MODE_CFB, self.iv)\n        self.f_key = aes.encrypt(self.f_key)\n        self.alpha_key = aes.encrypt(self.alpha_key)\n        self.encrypted = True\n        # sign\n        self.hmac = self.get_hmac(key)", "response": "This method encrypts and signs the state to make it unreadable by getting the HMAC and the FKEY."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt(self, key):\n        # check signature\n        if (self.get_hmac(key) != self.hmac):\n            raise HeartbeatError(\"Signature invalid on state.\")\n        if (not self.encrypted):\n            return\n        # decrypt\n        aes = AES.new(key, AES.MODE_CFB, self.iv)\n        self.f_key = aes.decrypt(self.f_key)\n        self.alpha_key = aes.decrypt(self.alpha_key)\n        self.encrypted = False\n        self.hmac = self.get_hmac(key)", "response": "This method decrypts the state of the object and returns the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromdict(dict):\n        self = Proof()\n        self.mu = dict[\"mu\"]\n        self.sigma = dict[\"sigma\"]\n        return self", "response": "Takes a dictionary as an argument and returns a new Proof object containing only the mu and sigma keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode(self, file):\n        tag = Tag()\n        tag.sigma = list()\n\n        state = State(Random.new().read(32), Random.new().read(32))\n\n        f = KeyedPRF(state.f_key, self.prime)\n        alpha = KeyedPRF(state.alpha_key, self.prime)\n\n        done = False\n        chunk_id = 0\n\n        while (not done):\n            sigma = f.eval(chunk_id)\n            for j in range(0, self.sectors):\n                buffer = file.read(self.sectorsize)\n\n                if (len(buffer) > 0):\n                    sigma += alpha.eval(j) * number.bytes_to_long(buffer)\n\n                if (len(buffer) != self.sectorsize):\n                    done = True\n                    break\n            sigma %= self.prime\n            tag.sigma.append(sigma)\n            chunk_id += 1\n\n        state.chunks = chunk_id\n        state.encrypt(self.key)\n\n        return (tag, state)", "response": "This function encodes the file for a new tag and returns a tuple that is returned by the function that is calculated for the new tag and state. The returned tuple is a tuple that is returned by the function that is used to generate the new state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_challenge(self, state):\n        state.decrypt(self.key)\n\n        chal = Challenge(state.chunks, self.prime, Random.new().read(32))\n\n        return chal", "response": "This function generates a challenge for given state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prove(self, file, chal, tag):\n        chunk_size = self.sectors * self.sectorsize\n\n        index = KeyedPRF(chal.key, len(tag.sigma))\n        v = KeyedPRF(chal.key, chal.v_max)\n\n        proof = Proof()\n        proof.mu = [0] * self.sectors\n        proof.sigma = 0\n\n        for i in range(0, chal.chunks):\n            for j in range(0, self.sectors):\n                pos = index.eval(i) * chunk_size + j * self.sectorsize\n                file.seek(pos)\n                buffer = file.read(self.sectorsize)\n                if (len(buffer) > 0):\n                    proof.mu[j] += v.eval(i) * number.bytes_to_long(buffer)\n\n                if (len(buffer) != self.sectorsize):\n                    break\n\n        for j in range(0, self.sectors):\n            proof.mu[j] %= self.prime\n\n        for i in range(0, chal.chunks):\n            proof.sigma += v.eval(i) * tag.sigma[index.eval(i)]\n\n        proof.sigma %= self.prime\n\n        return proof", "response": "This function returns a proof calculated from the file the challenge and the tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a dictionary of events to the server.", "response": "def put(self, event_dict, namespace=None):\n    \"\"\"\n    Sends a dictionary of `event_dict` of the form {stream_name:\n    [event, ...], ...}  to the server.\n    \"\"\"\n    # Copy the input, in case we need to modify it by adding a timestamp.\n    event_dict = copy.deepcopy(event_dict)\n\n    # Ensure that all events have a timestamp.\n    timestamp = kronos_time_now()\n    for events in event_dict.itervalues():\n      for event in events:\n        if TIMESTAMP_FIELD not in event:\n          event[TIMESTAMP_FIELD] = timestamp\n        else:\n          if isinstance(event[TIMESTAMP_FIELD], types.StringTypes):\n            event[TIMESTAMP_FIELD] = parse(event[TIMESTAMP_FIELD])\n          if isinstance(event[TIMESTAMP_FIELD], datetime):\n            event[TIMESTAMP_FIELD] = datetime_to_kronos_time(\n              event[TIMESTAMP_FIELD])\n        event[LIBRARY_FIELD] = {\n          'version': pykronos.__version__,\n          'name': 'pykronos'\n        }\n\n    namespace = namespace or self.namespace\n\n    if self._blocking:\n      return self._put(namespace, event_dict)\n    else:\n      with self._put_lock:\n        self._put_queue.append((namespace, event_dict))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying a stream with name stream for all events between start_time and end_time.", "response": "def get(self, stream, start_time, end_time, start_id=None, limit=None,\n          order=ResultOrder.ASCENDING, namespace=None, timeout=None):\n    \"\"\"\n    Queries a stream with name `stream` for all events between `start_time` and\n    `end_time` (both inclusive).  An optional `start_id` allows the client to\n    restart from a failure, specifying the last ID they read; all events that\n    happened after that ID will be returned. An optional `limit` limits the\n    maximum number of events returned.  An optional `order` requests results in\n    `ASCENDING` or `DESCENDING` order.\n    \"\"\"\n    if isinstance(start_time, types.StringTypes):\n      start_time = parse(start_time)\n    if isinstance(end_time, types.StringTypes):\n      end_time = parse(end_time)\n    if isinstance(start_time, datetime):\n      start_time = datetime_to_kronos_time(start_time)\n    if isinstance(end_time, datetime):\n      end_time = datetime_to_kronos_time(end_time)\n\n    request_dict = {\n      'stream': stream,\n      'end_time': end_time,\n      'order': order,\n    }\n    if start_id is not None:\n      request_dict['start_id'] = start_id\n    else:\n      request_dict['start_time'] = start_time\n\n    if limit is not None:\n      request_dict['limit'] = limit\n\n    namespace = namespace or self.namespace\n    if namespace is not None:\n      request_dict['namespace'] = namespace\n\n    errors = []\n    last_id = None\n    while True:\n      try:\n        response = self._make_request(self._get_url,\n                                      data=request_dict,\n                                      stream=True,\n                                      timeout=timeout)\n        for line in response.iter_lines(chunk_size=self._chunk_size):\n          if line:\n            # Python's json adds a lot of overhead when decoding a large\n            # number of events; ujson fares better. However ujson won't work\n            # on PyPy since it's a C extension.\n            event = ujson.loads(line, precise_float=True)\n            last_id = event[ID_FIELD]\n            yield event\n        break\n      except Exception, e:\n        if isinstance(e, requests.exceptions.Timeout):\n          raise KronosClientError('Request timed out.')\n        errors.append(e)\n        if len(errors) == 10:\n          raise KronosClientError(errors)\n        if last_id is not None:\n          request_dict.pop('start_time', None)\n          request_dict['start_id'] = last_id\n        time.sleep(len(errors) * 0.1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, stream, start_time, end_time, start_id=None, namespace=None):\n    if isinstance(start_time, types.StringTypes):\n      start_time = parse(start_time)\n    if isinstance(end_time, types.StringTypes):\n      end_time = parse(end_time)\n    if isinstance(start_time, datetime):\n      start_time = datetime_to_kronos_time(start_time)\n    if isinstance(end_time, datetime):\n      end_time = datetime_to_kronos_time(end_time)\n    request_dict = {\n      'stream': stream,\n      'end_time': end_time\n    }\n    if start_id:\n      request_dict['start_id'] = start_id\n    else:\n      request_dict['start_time'] = start_time\n\n    namespace = namespace or self.namespace\n    if namespace is not None:\n      request_dict['namespace'] = namespace\n\n    return self._make_request(self._delete_url, data=request_dict)", "response": "Delete events in the specified stream between start_time and end_time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_streams(self, namespace=None):\n    request_dict = {}\n    namespace = namespace or self.namespace\n    if namespace is not None:\n      request_dict['namespace'] = namespace\n    response = self._make_request(self._streams_url,\n                                  data=request_dict,\n                                  stream=True)\n    for line in response.iter_lines():\n      if line:\n        yield line", "response": "Retrieves a list of streams available to be read in a Kronos namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_schema(self, stream, namespace=None):\n    return self._make_request(self._infer_schema_url,\n                              data={'stream': stream,\n                                    'namespace': namespace or self.namespace})", "response": "Infer the schema for the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging each call to the function as an event in the stream with name `stream_name`. If `log_stack_trace` is set, it will log the stack trace under the `stack_trace` key. `properties` is an optional mapping fron key name to some function which expects the same arguments as the function `function` being decorated. The event will be populated with keys in `properties` mapped to the return values of the `properties[key_name](*args, **kwargs)`. Usage: @kronos_client.log_function('mystreamname', properties={'a': lambda x, y: x, 'b': lambda x, y: y}) def myfunction(a, b): <some code here>", "response": "def log_function(self, stream_name, properties={},\n                   log_function_stack_trace=False,\n                   log_exception_stack_trace=False,\n                   namespace=None):\n    \"\"\"\n    Logs each call to the function as an event in the stream with name\n    `stream_name`. If `log_stack_trace` is set, it will log the stack trace\n    under the `stack_trace` key. `properties` is an optional mapping fron key\n    name to some function which expects the same arguments as the function\n    `function` being decorated. The event will be populated with keys in\n    `properties` mapped to the return values of the\n    `properties[key_name](*args, **kwargs)`.\n    Usage:\n\n      @kronos_client.log_function('mystreamname',\n                                  properties={'a': lambda x, y: x,\n                                              'b': lambda x, y: y})\n      def myfunction(a, b):\n        <some code here>\n    \"\"\"\n    namespace = namespace or self.namespace\n\n    def decorator(function):\n      @functools.wraps(function)\n      def wrapper(*args, **kwargs):\n        event = {}\n        start_time = time.time()\n        if log_function_stack_trace:\n          event['stack_trace'] = traceback.extract_stack()\n        try:\n          return function(*args, **kwargs)\n        except Exception as exception:\n          self._log_exception(event, exception,\n                              (sys.last_traceback if log_exception_stack_trace\n                               else None))\n          raise exception\n        finally:\n          event['duration'] = time.time() - start_time\n          for key, value_getter in properties.iteritems():\n            event[key] = value_getter(*args, **kwargs)\n          self.put({stream_name: [event]}, namespace=namespace)\n      return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _nodemap_changed(self, data, stat):\n\n        if not stat:\n            raise EnvironmentNotFoundException(self.nodemap_path)\n\n        try:\n            conf_path = self._deserialize_nodemap(data)[self.hostname]\n        except KeyError:\n            conf_path = '/services/%s/conf' % self.service\n\n        self.config_watcher = DataWatch(\n            self.zk, conf_path,\n            self._config_changed\n        )", "response": "Called when the nodemap changes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _config_changed(self, data, stat):\n\n        self.config = json.loads(data)\n\n        if self.cb:\n            self.cb(self.config)", "response": "Called when config changes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving all the events for a given stream from start_time to end_time.", "response": "def retrieve(self, namespace, stream, start_time, end_time, start_id,\n               configuration, order=ResultOrder.ASCENDING, limit=sys.maxint):\n    \"\"\"\n    Retrieves all the events for `stream` from `start_time` (inclusive) till\n    `end_time` (inclusive). Alternatively to `start_time`, `start_id` can be\n    provided, and then all events from `start_id` (exclusive) till `end_time`\n    (inclusive) are returned. `start_id` should be used in cases when the client\n    got disconnected from the server before all the events in the requested\n    time window had been returned. `order` can be one of ResultOrder.ASCENDING\n    or ResultOrder.DESCENDING.\n\n    Returns an iterator over all JSON serialized (strings) events.\n    \"\"\"\n    if not start_id:\n      start_id = uuid_from_kronos_time(start_time, _type=UUIDType.LOWEST)\n    else:\n      start_id = TimeUUID(start_id)\n    if uuid_to_kronos_time(start_id) > end_time:\n      return []\n    return self._retrieve(namespace, stream, start_id, end_time, order, limit,\n                          configuration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends an email to the Mandrill API.", "response": "def send_email(self, **kwargs):\n        \"\"\"\n        Sends an email using Mandrill's API. Returns a\n        Requests :class:`Response` object.\n        At a minimum kwargs must contain the keys to, from_email, and text.\n        Everything passed as kwargs except for the keywords 'key', 'async',\n        and 'ip_pool' will be sent as key-value pairs in the message object.\n        Reference https://mandrillapp.com/api/docs/messages.JSON.html#method=send\n        for all the available options.\n        \"\"\"\n        endpoint = self.messages_endpoint\n\n        data = {\n            'async': kwargs.pop('async', False),\n            'ip_pool': kwargs.pop('ip_pool', ''),\n            'key': kwargs.pop('key', self.api_key),\n        }\n\n        if not data.get('key', None):\n            raise ValueError('No Mandrill API key has been configured')\n\n        # Sending a template through Mandrill requires a couple extra args\n        # and a different endpoint.\n        if kwargs.get('template_name', None):\n            data['template_name'] = kwargs.pop('template_name')\n            data['template_content'] = kwargs.pop('template_content', [])\n            endpoint = self.templates_endpoint\n\n        data['message'] = kwargs\n\n        if self.app:\n            data['message'].setdefault(\n                'from_email',\n                self.app.config.get('MANDRILL_DEFAULT_FROM', None)\n            )\n\n        if endpoint != self.templates_endpoint and not data['message'].get('from_email', None):\n            raise ValueError(\n                'No from email was specified and no default was configured')\n\n\n        response = requests.post(endpoint,\n                                 data=json.dumps(data),\n                                 headers={'Content-Type': 'application/json'})\n        response.raise_for_status()\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading a feed s most recent enclosure that we don t have", "response": "def put_a_hit_out(name):\n    \"\"\"Download a feed's most recent enclosure that we don't have\"\"\"\n\n    feed = resolve_name(name)\n    if six.PY3:\n        feed = feed.decode()\n    d = feedparser.parse(feed)\n    # logger.info(d)\n    # logger.info(feed)\n    print(d['feed']['title'])\n    if d.entries[0].enclosures:\n        with Database(\"settings\") as s:\n            if 'verbose' in s:\n                print(d.entries[0].enclosures[0])\n\n        # print d.feed.updated_parsed\n        # Doesn't work everywhere, may nest in try or\n        # use .headers['last-modified']\n        url = str(d.entries[0].enclosures[0]['href'])\n        with Database(\"downloads\") as db:\n            if url.split('/')[-1] not in db:\n                with Database(\"settings\") as settings:\n                    if 'dl' in settings:\n                        dl_dir = settings['dl']\n                    else:\n                        dl_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n                requests_get(url, dl_dir)\n                db[url.split('/')[-1]] = json.dumps({'url': url, 'date': time.ctime(), 'feed': feed})\n                growl(\"Mission Complete: %s downloaded\" % d.feed.title)\n                print(\"Mission Complete: %s downloaded\" % d.feed.title)\n            else:\n                growl(\"Mission Aborted: %s already downloaded\" % d.feed.title)\n                print(\"Mission Aborted: %s already downloaded\" % d.feed.title)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef selective_download(name, oldest, newest):\n    if six.PY3:\n        name = name.encode(\"utf-8\")\n    feed = resolve_name(name)\n    if six.PY3:\n        feed = feed.decode()\n    d = feedparser.parse(feed)\n    logger.debug(d)\n    try:\n        d.entries[int(oldest)]\n    except IndexError:\n        print(\"Error feed does not contain this many items.\")\n        print(\"Hitman thinks there are %d items in this feed.\" % len(d.entries))\n        return\n    for url in [q.enclosures[0]['href'] for q in d.entries[int(newest):int(oldest)]]:\n        # iterate over urls in feed from newest to oldest feed items.\n        url = str(url)\n        with Database(\"downloads\") as db:\n            if url.split('/')[-1] not in db:\n                # download(url, name, feed)\n                with Database(\"settings\") as settings:\n                    if 'dl' in settings:\n                        dl_dir = settings['dl']\n                    else:\n                        dl_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n                requests_get(url, dl_dir)", "response": "Download the given feed from newest to newest."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a given input from a user and finds the url for it", "response": "def resolve_name(name):\n    \"\"\"Takes a given input from a user and finds the url for it\"\"\"\n    logger.debug(\"resolve_name: %s\", name)\n    with Database(\"feeds\") as feeds, Database(\"aliases\") as aliases:\n        if name in aliases.keys():\n            return feeds[aliases[name]]\n        elif name in feeds.keys():\n            return feeds[name]\n        else:\n            print(\"Cannot find feed named: %s\" % name)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hitsquad(ctx):\n    with Database(\"feeds\") as feeds:\n        for name, feed in zip(list(feeds.keys()), list(feeds.values())):\n            logger.debug(\"calling put_a_hit_out: %s\", name)\n            ctx.invoke(put_a_hit_out, name=name)\n        if len(list(feeds.keys())) == 0:\n            ctx.get_help()", "response": "put a hit out on all known rss feeds [ Default action without arguements"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending native notifications where supported. Growl is gone.", "response": "def growl(text):\n    \"\"\"send native notifications where supported. Growl is gone.\"\"\"\n    if platform.system() == 'Darwin':\n        import pync\n        pync.Notifier.notify(text, title=\"Hitman\")\n\n    elif platform.system() == 'Linux':\n        notified = False\n        try:\n            logger.debug(\"Trying to import pynotify\")\n            import pynotify\n            pynotify.init(\"Hitman\")\n            n = pynotify.Notification(\"Hitman Status Report\", text)\n            n.set_timeout(pynotify.EXPIRES_DEFAULT)\n            n.show()\n            notified = True\n        except ImportError:\n            logger.debug(\"Trying notify-send\")\n            # print(\"trying to notify-send\")\n            if Popen(['which', 'notify-send'], stdout=PIPE).communicate()[0]:\n                # Do an OSD-Notify\n                # notify-send \"Totem\" \"This is a superfluous notification\"\n                os.system(\"notify-send \\\"Hitman\\\" \\\"%r\\\" \" % str(text))\n                notified = True\n        if not notified:\n            try:\n                logger.info(\"notificatons gnome gi???\")\n                import gi\n                gi.require_version('Notify', '0.7')\n                from gi.repository import Notify\n                Notify.init(\"Hitman\")\n                # TODO have Icon as third argument.\n                notification = Notify.Notification.new(\"Hitman\", text)\n                notification.show()\n                Notify.uninit()\n                notified = True\n            except ImportError:\n                logger.exception()\n    elif platform.system() == 'Haiku':\n        os.system(\"notify --type information --app Hitman --title 'Status Report' '%s'\" % str(text))\n    elif platform.system() == 'Windows':\n        try:\n            from win10toast import ToastNotifier\n            toaster = ToastNotifier()\n            toaster.show_toast(text, \"Hitman\")\n            # gntplib.publish(\"Hitman\", \"Status Update\", \"Hitman\", text=text)\n        except Exception:\n            logger.exception()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_feed(url):\n    with Database(\"feeds\") as db:\n        title = feedparser.parse(url).feed.title\n        name = str(title)\n        db[name] = url\n        return name", "response": "add a feed to the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving from database and delete aliases", "response": "def del_feed(name):\n    \"\"\"remove from database (and delete aliases)\"\"\"\n    with Database(\"aliases\") as aliases, Database(\"feeds\") as feeds:\n        if aliases[name]:\n            proper_name = aliases[name]\n        elif feeds[name]:\n            proper_name = feeds[name]\n        for k, v in aliases:\n            if v == proper_name:\n                del aliases[k]\n        # deleted from aliases\n        del feeds[proper_name]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef del_alias(alias):\n    with Database(\"aliases\") as mydb:\n        try:\n            print(\"removing alias of %s to %s\" % (alias, mydb.pop(alias)))\n        except KeyError:\n            print(\"No such alias key\")\n            print(\"Check alias db:\")\n            print(zip(list(mydb.keys()), list(mydb.values())))", "response": "delete an alias from the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alias_feed(name, alias):\n    with Database(\"aliases\") as db:\n        if alias in db:\n            print(\"Something has gone horribly wrong with your aliases! Try deleting the %s entry.\" % name)\n            return\n        else:\n            db[alias] = name", "response": "write aliases to db"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all feeds in plain text and give their aliases", "response": "def list_feeds():\n    \"\"\"List all feeds in plain text and give their aliases\"\"\"\n    with Database(\"feeds\") as feeds, Database(\"aliases\") as aliases_db:\n        for feed in feeds:\n            name = feed\n            url = feeds[feed]\n            aliases = []\n            for k, v in zip(list(aliases_db.keys()), list(aliases_db.values())):\n                if v == name:\n                    aliases.append(k)\n            if aliases:\n                print(name, \" : %s Aliases: %s\" % (url, aliases))\n            else:\n                print(name, \" : %s\" % url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_opml():\n    with Database(\"feeds\") as feeds:\n        # Thanks to the canto project- used under the GPL\n        print(\"\"\"<opml version=\"1.0\">\"\"\")\n        print(\"\"\"<body>\"\"\")\n        # Accurate but slow.\n        for name in list(feeds.keys()):\n            kind = feedparser.parse(feeds[name]).version\n            if kind[:4] == 'atom':\n                t = 'pie'\n            elif kind[:3] == 'rss':\n                t = 'rss'\n            print(\"\"\"\\t<outline text=\"%s\" xmlUrl=\"%s\" type=\"%s\" />\"\"\" % (name, feeds[name], t))\n        print(\"\"\"</body>\"\"\")\n        print(\"\"\"</opml>\"\"\")", "response": "Export an OPML feed list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports an OPML file locally or from a URL. Uses your text attributes as aliases.", "response": "def import_opml(url):\n    \"\"\"Import an OPML file locally or from a URL. Uses your text attributes as aliases.\"\"\"\n    # Test if URL given is local, then open, parse out feed urls,\n    # add feeds, set text= to aliases and report success, list feeds added\n    from bs4 import BeautifulSoup\n    try:\n        f = file(url).read()\n    except IOError:\n        f = requests.get(url).text\n    soup = BeautifulSoup(f, \"xml\")\n    links = soup.find_all('outline', type=\"rss\" or \"pie\")\n    # This is very slow, might cache this info on add\n    for link in links:\n        # print link\n        add_feed(link['xmlUrl'])\n        print(\"Added \" + link['text'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing hitman_dir from os name", "response": "def directory():\n    \"\"\"Construct hitman_dir from os name\"\"\"\n    home = os.path.expanduser('~')\n    if platform.system() == 'Linux':\n        hitman_dir = os.path.join(home, '.hitman')\n    elif platform.system() == 'Darwin':\n        hitman_dir = os.path.join(home, 'Library', 'Application Support',\n                                  'hitman')\n    elif platform.system() == 'Windows':\n        hitman_dir = os.path.join(os.environ['appdata'], 'hitman')\n    else:\n        hitman_dir = os.path.join(home, '.hitman')\n    if not os.path.isdir(hitman_dir):\n        os.mkdir(hitman_dir)\n    return hitman_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(url, force=False):\n    if url[-3:] == 'xml' or url[1][-4:] == 'atom':\n        print(\"Added your feed as %s\" % str(add_feed(url)))\n    elif is_feed(url):\n        print(\"Added your feed as %s\" % str(add_feed(url)))\n    elif force:\n        print(\"Added your feed as %s\" % str(add_feed(url)))\n    else:\n        print(\"Hitman doesn't think that url is a feed; if you're sure it is rerun with --force\")", "response": "Add a feed by url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_settings(key, value):\n    with Database(\"settings\") as settings:\n        if value in ['0', 'false', 'no', 'off', 'False']:\n            del settings[key]\n            print(\"Disabled setting\")\n        else:\n            print(value)\n            settings[key] = value\n            print(\"Setting saved\")", "response": "Set Hitman internal settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_settings(all,key):\n    with Database(\"settings\") as s:\n        if all:\n            for k, v in zip(list(s.keys()), list(s.values())):\n                print(\"{} = {}\".format(k, v))\n        elif key:\n            print(\"{} = {}\".format(key, s[key]))\n        else:\n            print(\"Don't know what you want? Try --all\")", "response": "View Hitman internal settings. Use 'all' for all keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a dictionary as an argument and returns a new Challenge object from the dictionary.", "response": "def fromdict(dict):\n        \"\"\"Takes a dictionary as an argument and returns a new Challenge\n        object from the dictionary.\n\n        :param dict: the dictionary to convert\n        \"\"\"\n        seed = hb_decode(dict['seed'])\n        index = dict['index']\n        return Challenge(seed, index)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a dictionary as an argument and returns a new Tag object containing the contents of the dictionary.", "response": "def fromdict(dict):\n        \"\"\"Takes a dictionary as an argument and returns a new Tag object\n        from the dictionary.\n\n        :param dict: the dictionary to convert\n        \"\"\"\n        tree = MerkleTree.fromdict(dict['tree'])\n        chunksz = dict['chunksz']\n        filesz = dict['filesz']\n        return Tag(tree, chunksz, filesz)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary fully representing the state of this object", "response": "def todict(self):\n        \"\"\"Returns a dictionary fully representing the state of this object\n        \"\"\"\n        return {'index': self.index,\n                'seed': hb_encode(self.seed),\n                'n': self.n,\n                'root': hb_encode(self.root),\n                'hmac': hb_encode(self.hmac),\n                'timestamp': self.timestamp}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a dictionary as an argument and returns a new State object containing the object s keys and values.", "response": "def fromdict(dict):\n        \"\"\"Takes a dictionary as an argument and returns a new State object\n        from the dictionary.\n\n        :param dict: the dictionary to convert\n        \"\"\"\n        index = dict['index']\n        seed = hb_decode(dict['seed'])\n        n = dict['n']\n        root = hb_decode(dict['root'])\n        hmac = hb_decode(dict['hmac'])\n        timestamp = dict['timestamp']\n        self = State(index, seed, n, root, hmac, timestamp)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_hmac(self, key):\n        h = HMAC.new(key, None, SHA256)\n        h.update(str(self.index).encode())\n        h.update(self.seed)\n        h.update(str(self.n).encode())\n        h.update(self.root)\n        h.update(str(self.timestamp).encode())\n        return h.digest()", "response": "Returns the keyed HMAC for authentication of this state data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a dictionary as an argument and returns a new Proof object containing the key and check_fraction.", "response": "def fromdict(dict):\n        \"\"\"Takes a dictionary as an argument and returns a new Proof object\n        from the dictionary.\n\n        :param dict: the dictionary to convert\n        \"\"\"\n        key = hb_decode(dict['key'])\n        check_fraction = dict['check_fraction']\n        return Merkle(check_fraction, key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_challenge(self, state):\n        state.checksig(self.key)\n        if (state.index >= state.n):\n            raise HeartbeatError(\"Out of challenges.\")\n        state.seed = MerkleHelper.get_next_seed(self.key, state.seed)\n        chal = Challenge(state.seed, state.index)\n        state.index += 1\n        state.sign(self.key)\n        return chal", "response": "returns the next Challenge and increments the seed and index in the state."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a proof of ownership of the given file based on the given challenge and tag.", "response": "def prove(self, file, challenge, tag):\n        \"\"\"Returns a proof of ownership of the given file based on the\n        challenge.  The proof consists of a hash of the specified file chunk\n        and the complete merkle branch.\n\n        :param file: a file that supports `read()`, `seek()` and `tell()`\n        :param challenge: the challenge to use for generating this proof\n        :param tag: the file tag as provided from the client\n        :param filesz: optional filesz parameter.  if not specified, the\n            filesz will be detected by seeking to the end of the stream\n        \"\"\"\n        leaf = MerkleLeaf(challenge.index,\n                          MerkleHelper.get_chunk_hash(file,\n                                                      challenge.seed,\n                                                      filesz=tag.filesz,\n                                                      chunksz=tag.chunksz))\n        return Proof(leaf, tag.tree.get_branch(challenge.index))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify(self, proof, challenge, state):\n        state.checksig(self.key)\n        if (proof.leaf.index != challenge.index):\n            return False\n        return MerkleTree.verify_branch(proof.leaf,\n                                        proof.branch,\n                                        state.root)", "response": "returns true if the proof matches the challenge. returns false otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_hash(file, seed, bufsz=DEFAULT_BUFFER_SIZE):\n        h = hmac.new(seed, None, hashlib.sha256)\n        while (True):\n            buffer = file.read(bufsz)\n            h.update(buffer)\n            if (len(buffer) != bufsz):\n                break\n        return h.digest()", "response": "This method generates a secure hash of the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a hash of a chunk of the file provided.", "response": "def get_chunk_hash(file,\n                       seed,\n                       filesz=None,\n                       chunksz=DEFAULT_CHUNK_SIZE,\n                       bufsz=DEFAULT_BUFFER_SIZE):\n        \"\"\"returns a hash of a chunk of the file provided.  the position of\n        the chunk is determined by the seed.  additionally, the hmac of the\n        chunk is calculated from the seed.\n\n        :param file: a file like object to get the chunk hash from.  should\n        support `read()`, `seek()` and `tell()`.\n        :param seed: the seed to use for calculating the chunk position and\n        chunk hash\n        :param chunksz: the size of the chunk to check\n        :param bufsz: an optional buffer size to use for reading the file.\n        \"\"\"\n        if (filesz is None):\n            file.seek(0, 2)\n            filesz = file.tell()\n        if (filesz < chunksz):\n            chunksz = filesz\n        prf = KeyedPRF(seed, filesz - chunksz + 1)\n        i = prf.eval(0)\n        file.seek(i)\n        h = hmac.new(seed, None, hashlib.sha256)\n        while (True):\n            if (chunksz < bufsz):\n                bufsz = chunksz\n            buffer = file.read(bufsz)\n            h.update(buffer)\n            chunksz -= len(buffer)\n            assert(chunksz >= 0)\n            if (chunksz == 0):\n                break\n        return h.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new domain from a set of complex query tuples.", "response": "def from_tuple(cls, queries):\n        \"\"\"Create a ``Domain`` given a set of complex query tuples.\n\n        Args:\n            queries (iter): An iterator of complex queries. Each iteration\n                should contain either:\n\n                * A data-set compatible with :func:`~domain.Domain.add_query`\n                * A string to switch the join type\n\n                Example::\n\n                    [('subject', 'Test1'),\n                     'OR',\n                     ('subject', 'Test2')',\n                     ('subject', 'Test3')',\n                     ]\n                    # The above is equivalent to:\n                    #    subject:'Test1' OR subject:'Test2' OR subject:'Test3'\n\n                    [('modified_at', datetime(2017, 01, 01)),\n                     ('status', 'active'),\n                     ]\n                    # The above is equivalent to:\n                    #    modified_at:[2017-01-01T00:00:00Z TO *]\n                    #    AND status:\"active\"\n\n        Returns:\n            Domain: A domain representing the input queries.\n        \"\"\"\n        domain = cls()\n        join_with = cls.AND\n        for query in queries:\n            if query in [cls.OR, cls.AND]:\n                join_with = query\n            else:\n                domain.add_query(query, join_with)\n        return domain"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new query to the stack.", "response": "def add_query(self, query, join_with=AND):\n        \"\"\"Join a new query to existing queries on the stack.\n\n        Args:\n            query (tuple or list or DomainCondition): The condition for the\n                query. If a ``DomainCondition`` object is not provided, the\n                input should conform to the interface defined in\n                :func:`~.domain.DomainCondition.from_tuple`.\n            join_with (str): The join string to apply, if other queries are\n                already on the stack.\n        \"\"\"\n        if not isinstance(query, DomainCondition):\n            query = DomainCondition.from_tuple(query)\n        if len(self.query):\n            self.query.append(join_with)\n        self.query.append(query)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a domain condition from a query tuple.", "response": "def from_tuple(cls, query):\n        \"\"\"Create a condition from a query tuple.\n\n        Args:\n            query (tuple or list): Tuple or list that contains a query domain\n                in the format of ``(field_name, field_value,\n                field_value_to)``. ``field_value_to`` is only applicable in\n                the case of a date search.\n\n        Returns:\n            DomainCondition: An instance of a domain condition. The specific\n                type will depend on the data type of the first value provided\n                in ``query``.\n        \"\"\"\n\n        field, query = query[0], query[1:]\n\n        try:\n            cls = TYPES[type(query[0])]\n        except KeyError:\n            # We just fallback to the base class if unknown type.\n            pass\n\n        return cls(field, *query)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading all the backends setup in settings. py.", "response": "def load_backends(self):\n    \"\"\"\n    Loads all the backends setup in settings.py.\n    \"\"\"\n    for name, backend_settings in settings.storage.iteritems():\n      backend_path = backend_settings['backend']\n      backend_module, backend_cls = backend_path.rsplit('.', 1)\n      backend_module = import_module(backend_module)\n      # Create an instance of the configured backend.\n      backend_constructor = getattr(backend_module, backend_cls)\n      self.backends[name] = backend_constructor(name,\n                                                self.namespaces,\n                                                **backend_settings)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_matching_prefix(self, namespace, stream):\n    validate_stream(stream)\n    default_prefix = ''\n    longest_prefix = default_prefix\n    for prefix in self.prefix_confs[namespace]:\n      if prefix == default_prefix:\n        continue\n      if not stream.startswith(prefix):\n        continue\n      if len(prefix) <= len(longest_prefix):\n        continue\n      longest_prefix = prefix\n    return longest_prefix", "response": "Get the longest prefix for a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backends_to_mutate(self, namespace, stream):\n    if namespace not in self.namespaces:\n      raise NamespaceMissing('`{}` namespace is not configured'\n                             .format(namespace))\n    return self.prefix_confs[namespace][self.get_matching_prefix(namespace,\n                                                                 stream)]", "response": "Returns all the backends enabled for writing for the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning backend to retrieve for given namespace and stream.", "response": "def backend_to_retrieve(self, namespace, stream):\n    \"\"\"\n    Return backend enabled for reading for `stream`.\n    \"\"\"\n    if namespace not in self.namespaces:\n      raise NamespaceMissing('`{}` namespace is not configured'\n                             .format(namespace))\n    stream_prefix = self.get_matching_prefix(namespace, stream)\n    read_backend = self.prefix_read_backends[namespace][stream_prefix]\n    return (read_backend,\n            self.prefix_confs[namespace][stream_prefix][read_backend])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new web hook.", "response": "def create(cls, session, web_hook):\n        \"\"\"Create a web hook.\n\n        Note that creating a new web hook will overwrite the web hook that is\n        already configured for this company. There is also no way to\n        programmatically determine if a web hook already exists for the\n        company. This is a limitation of the HelpScout API and cannot be\n        circumvented.\n\n        Args:\n            session (requests.sessions.Session): Authenticated session.\n            web_hook (helpscout.models.WebHook): The web hook to be created.\n\n        Returns:\n            bool: ``True`` if the creation was a success. Errors otherwise.\n        \"\"\"\n        cls(\n            '/hooks.json',\n            data=web_hook.to_api(),\n            request_type=RequestPaginator.POST,\n            session=session,\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the base64 encoded data using a raw value or file object.", "response": "def raw_data(self, value):\n        \"\"\"Set the base64 encoded data using a raw value or file object.\"\"\"\n        if value:\n            try:\n                value = value.read()\n            except AttributeError:\n                pass\n            b64 = base64.b64encode(value.encode('utf-8'))\n            self.data = b64.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schedule():\n  code = request.form['code']\n  interval = int(request.form['interval'])\n\n  task_id = binascii.b2a_hex(os.urandom(5))\n  new_task = Task(id=task_id)\n  new_task.active = True\n  new_task.code = code\n  new_task.interval = interval\n\n  # TODO(derek): Assert there is only one other_task\n  other_task = Task.query.filter_by(code=code, active=True).first()\n\n  if other_task:\n    if other_task.interval <= new_task.interval:\n      new_task.active = False\n    else:\n      other_task.active = False\n      other_task.save()\n      current_app.scheduler.cancel(other_task.id)\n\n  if new_task.active:\n    print current_app.scheduler.schedule\n    current_app.scheduler.schedule({\n      'id': task_id,\n      'code': new_task.code,\n      'interval': new_task.interval\n    })\n\n  new_task.save()\n\n  return json.dumps({\n    'status': 'success',\n    'id': task_id,\n  })", "response": "HTTP endpoint for scheduling tasks with the same code and interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cancel():\n  task_id = request.form['id']\n  task = Task.query.get(task_id)\n\n  if not task:\n    return json.dumps({\n      'status': 'success',\n      'id': None,\n    })\n\n  task.delete()\n\n  if task.active:\n    current_app.scheduler.cancel(task_id)\n\n    code = task.code\n    other_task = Task.query.filter_by(code=code).order_by('interval').first()\n    if other_task:\n      other_task.active = True\n      other_task.save()\n      current_app.scheduler.schedule({\n        'id': other_task.id,\n        'code': other_task.code,\n        'interval': other_task.interval\n      })\n\n  return json.dumps({\n    'status': 'success',\n    'id': task_id,\n  })", "response": "HTTP endpoint for canceling tasks\n\n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert(self, ns, docid, raw, **kw):\n        try:\n            self._dest_coll(ns).insert(raw['o'], safe=True)\n        except DuplicateKeyError, e:\n            logging.warning(e)", "response": "Perform a single insert operation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, ns, docid, raw, **kw):\n        self._dest_coll(ns).update(raw['o2'], raw['o'], safe=True)", "response": "Perform a single update operation on the base database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, ns, docid, raw, **kw):\n        self._dest_coll(ns).remove(raw['o'], safe=True)", "response": "Perform a single delete operation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a drop index command.", "response": "def drop_index(self, raw):\n        \"\"\" Executes a drop index command.\n\n            { \"op\" : \"c\",\n              \"ns\" : \"testdb.$cmd\",\n              \"o\" : { \"dropIndexes\" : \"testcoll\",\n            \t\t  \"index\" : \"nuie_1\" } }\n        \"\"\"\n        dbname = raw['ns'].split('.', 1)[0]\n        collname = raw['o']['dropIndexes']\n        self.dest[dbname][collname].drop_index(raw['o']['index'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute command. { \"op\" : \"c\", \"ns\" : \"testdb.$cmd\", \"o\" : { \"drop\" : \"fs.files\"} }", "response": "def command(self, ns, raw, **kw):\n        \"\"\" Executes command.\n\n            { \"op\" : \"c\",\n              \"ns\" : \"testdb.$cmd\",\n              \"o\" : { \"drop\" : \"fs.files\"}\n            }\n        \"\"\"\n        try:\n            dbname = raw['ns'].split('.', 1)[0]\n            self.dest[dbname].command(raw['o'], check=True)\n        except OperationFailure, e:\n            logging.warning(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator over the folders for the mailbox.", "response": "def get_folders(cls, session, mailbox_or_id):\n        \"\"\"List the folders for the mailbox.\n\n        Args:\n            mailbox_or_id (helpscout.models.Mailbox or int): Mailbox or the ID\n                of the mailbox to get the folders for.\n\n        Returns:\n            RequestPaginator(output_type=helpscout.models.Folder): Folders\n                iterator.\n        \"\"\"\n        if isinstance(mailbox_or_id, Mailbox):\n            mailbox_or_id = mailbox_or_id.id\n        return cls(\n            '/mailboxes/%d/folders.json' % mailbox_or_id,\n            session=session,\n            out_type=Folder,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _slice(start, stop, step):\n    if step == 0:\n        raise ValueError(\"slice() arg 3 must not be zero\")\n    if start==stop:\n        raise StopIteration\n\n    previous = start\n    next = start + step\n    while next < stop:\n        yield previous, next\n        previous += step\n        next += step\n    yield previous, stop", "response": "Generate pairs so that you can slice from start to stop step elements at a time\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef leagues_by_summoner_ids(summoner_ids, queue=Queue.RANKED_SOLO_5x5):\n    summoners_league = defaultdict(set)\n    for start, end in _slice(0, len(summoner_ids), 10):\n        for id, leagues in get_league_entries_by_summoner(summoner_ids[start:end]).items():\n            for league in leagues:\n                if Queue[league.queue]==queue:\n                    summoners_league[Tier.parse(league.tier)].add(int(id))\n    return summoners_league", "response": "Takes in a list of players ids and divide them by league tiers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the tier of the lowest tier and the participantsIDs divided by tier player in the queue", "response": "def get_tier_from_participants(participantsIdentities, minimum_tier=Tier.bronze, queue=Queue.RANKED_SOLO_5x5):\n    \"\"\"\n    Returns the tier of the lowest tier and the participantsIDs divided by tier\n    player in the match\n    :param participantsIdentities: the match participants\n    :param minimum_tier: the minimum tier that a participant must be in order to be added\n    :param queue: the queue over which the tier of the player is considered\n    :return: the tier of the lowest tier player in the match\n    \"\"\"\n    leagues = leagues_by_summoner_ids([p.player.summonerId for p in participantsIdentities], queue)\n    match_tier = max(leagues.keys(), key=operator.attrgetter('value'))\n    return match_tier, {league: ids for league, ids in leagues.items() if league.is_better_or_equal(minimum_tier)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of summoners names and returns a dictionary mapping the player name to his / her summoner id", "response": "def summoner_names_to_id(summoners):\n    \"\"\"\n    Gets a list of summoners names and return a dictionary mapping the player name to his/her summoner id\n    :param summoners: a list of player names\n    :return: a dictionary name -> id\n    \"\"\"\n    ids = {}\n    for start, end in _slice(0, len(summoners), 40):\n        result = get_summoners_by_name(summoners[start:end])\n        for name, summoner in result.items():\n            ids[name] = summoner.id\n    return ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self):\n    if self.board_data:\n      board_dict = json.loads(self.board_data)\n      board_dict['id'] = self.id\n      del board_dict['__version__']\n    else:\n      board_dict = {\n        'id': self.id,\n        'title': '',\n        'panels': []\n      }\n    return board_dict\n    \"\"\"    pycode = self.pycodes.first() or PyCode()\n    return {'id': self.id,\n            'pycode': pycode.json()}\n            \"\"\"", "response": "A JSON - encoded description of this board."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses this function to get data from Knoema dataset.", "response": "def get(dataset = None, include_metadata = False, mnemonics = None, **dim_values):\r\n    \"\"\"Use this function to get data from Knoema dataset.\"\"\"\r\n\r\n    if not dataset and not mnemonics:\r\n        raise ValueError('Dataset id is not specified')\r\n\r\n    if mnemonics and dim_values:\r\n        raise ValueError('The function does not support specifying mnemonics and selection in a single call')\r\n\r\n    config = ApiConfig()\r\n    client = ApiClient(config.host, config.app_id, config.app_secret)\r\n    client.check_correct_host()\r\n\r\n    ds = client.get_dataset(dataset) if dataset else None\r\n    reader =  MnemonicsDataReader(client, mnemonics) if mnemonics else StreamingDataReader(client, dim_values) if ds.type == 'Regular' else PivotDataReader(client, dim_values)\r\n    reader.include_metadata = include_metadata\r\n    reader.dataset = ds\r\n\r\n    return reader.get_pandasframe()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload(file_path, dataset=None, public=False):\r\n\r\n    config = ApiConfig()\r\n    client = ApiClient(config.host, config.app_id, config.app_secret)\r\n    return client.upload(file_path, dataset, public)", "response": "Use this function to upload data to Knoema dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(dataset):\r\n    \r\n    config = ApiConfig()\r\n    client = ApiClient(config.host, config.app_id, config.app_secret)\r\n    client.check_correct_host()\r\n    client.delete(dataset)\r\n    return ('Dataset {} has been deleted successfully'.format(dataset))", "response": "Use this function to delete dataset by it s id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify(dataset, publication_date, source, refernce_url):\r\n\r\n    config = ApiConfig()\r\n    client = ApiClient(config.host, config.app_id, config.app_secret)\r\n    client.check_correct_host()\r\n    client.verify(dataset, publication_date, source, refernce_url)", "response": "Use this function to verify a dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_op(self, ns, raw):\n        # Compute the document id of the document that will be altered\n        # (in case of insert, update or delete).\n        docid = self.__get_id(raw)\n\n        op = raw['op']\n        if op == 'i':\n            self.insert(ns=ns, docid=docid, raw=raw)\n        elif op == 'u':\n            self.update(ns=ns, docid=docid, raw=raw)\n        elif op == 'd':\n            self.delete(ns=ns, docid=docid, raw=raw)\n        elif op == 'c':\n            self.command(ns=ns, raw=raw)\n        elif op == 'db':\n            self.db_declare(ns=ns, raw=raw)\n        elif op == 'n':\n            self.noop()\n        else:\n            logging.error(\"Unknown op: %r\" % op)\n\n        # Save timestamp of last processed oplog.\n        self.ts = raw['ts']", "response": "Processes a single operation from the oplog."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toset_from_tosets(*tosets):  # Note: a setlist is perfect representation of a toset as it's totally ordered and it's a set, i.e. a toset\n    '''\n    Create totally ordered set (toset) from tosets.\n\n    These tosets, when merged, form a partially ordered set. The linear\n    extension of this poset, a toset, is returned.\n\n    .. warning:: untested\n\n    Parameters\n    ----------\n    tosets : Iterable[~collections_extended.setlist]\n        Tosets to merge.\n\n    Raises\n    ------\n    ValueError\n        If the tosets (derived from the lists) contradict each other. E.g. \n        ``[a, b]`` and ``[b, c, a]`` contradict each other.\n\n    Returns\n    -------\n    toset : ~collectiontions_extended.setlist\n        Totally ordered set.\n    '''\n    # Construct directed graph with: a <-- b iff a < b and adjacent in a list\n    graph = nx.DiGraph()\n    for toset in tosets:\n        graph.add_nodes_from(toset)\n        graph.add_edges_from(windowed(reversed(toset)))\n\n    # No cycles allowed\n    if not nx.is_directed_acyclic_graph(graph): #TODO could rely on NetworkXUnfeasible https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.algorithms.dag.topological_sort.html\n        raise ValueError('Given tosets contradict each other')  # each cycle is a contradiction, e.g. a > b > c > a\n\n    # Topological sort\n    return setlist(nx.topological_sort(graph, reverse=True))", "response": "Create totally ordered set from a list of tosets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the value into a single group", "response": "def _sortValueIntoGroup(groupKeys, groupLimits, value):\n    \"\"\" returns the Key of the group a value belongs to\n    :param groupKeys: a list/tuple of keys ie ['1-3', '3-5', '5-8', '8-10', '10+']\n    :param groupLimits: a list of the limits for the group [1,3,5,8,10,float('inf')] note the first value is an absolute\n    minimum and the last an absolute maximum. You can therefore use float('inf')\n    :param value:\n    :return:\n    \"\"\"\n\n    if not len(groupKeys) == len(groupLimits)-1:\n        raise ValueError('len(groupKeys) must equal len(grouplimits)-1 got \\nkeys:{0} \\nlimits:{1}'.format(groupKeys,\n                                                                                                         groupLimits))\n\n    if math.isnan(value):\n        return 'Uncertain'\n\n    # TODO add to other if bad value or outside limits\n    keyIndex = None\n\n    if value == groupLimits[0]:  # if value is == minimum skip the comparison\n        keyIndex = 1\n    elif value == groupLimits[-1]:  # if value is == minimum skip the comparison\n        keyIndex = len(groupLimits)-1\n    else:\n        for i, limit in enumerate(groupLimits):\n            if value < limit:\n                keyIndex = i\n                break\n\n    if keyIndex == 0:  # below the minimum\n        raise BelowLimitsError('Value {0} below limit {1}'.format(value, groupLimits[0]))\n\n    if keyIndex is None:\n        raise AboveLimitsError('Value {0} above limit {1}'.format(value, groupLimits[-1]))\n\n    return groupKeys[keyIndex-1]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchoosing a preset size for the plot", "response": "def set_size(self, size):\n        \"\"\" choose a preset size for the plot\n        :param size: 'small' for documents or 'large' for presentations\n        \"\"\"\n        if size == 'small':\n            self._set_size_small()\n        elif size == 'large':\n            self._set_size_large()\n        else:\n            raise ValueError('Size must be large or small')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the color of the frame major ticks axis labels title and legend patches.", "response": "def set_foregroundcolor(self, color):\n         '''For the specified axes, sets the color of the frame, major ticks,\n             tick labels, axis labels, title and legend\n         '''\n\n         ax = self.ax\n\n         for tl in ax.get_xticklines() + ax.get_yticklines():\n             tl.set_color(color)\n         for spine in ax.spines:\n             ax.spines[spine].set_edgecolor(color)\n         for tick in ax.xaxis.get_major_ticks():\n             tick.label1.set_color(color)\n         for tick in ax.yaxis.get_major_ticks():\n             tick.label1.set_color(color)\n         ax.axes.xaxis.label.set_color(color)\n         ax.axes.yaxis.label.set_color(color)\n         ax.axes.xaxis.get_offset_text().set_color(color)\n         ax.axes.yaxis.get_offset_text().set_color(color)\n         ax.axes.title.set_color(color)\n         lh = ax.get_legend()\n         if lh != None:\n             lh.get_title().set_color(color)\n             lh.legendPatch.set_edgecolor('none')\n             labels = lh.get_texts()\n             for lab in labels:\n                 lab.set_color(color)\n         for tl in ax.get_xticklabels():\n             tl.set_color(color)\n         for tl in ax.get_yticklabels():\n             tl.set_color(color)\n         plt.draw()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the background color of the current axes and legend.", "response": "def set_backgroundcolor(self, color):\n         '''Sets the background color of the current axes (and legend).\n             Use 'None' (with quotes) for transparent. To get transparent\n             background on saved figures, use:\n             pp.savefig(\"fig1.svg\", transparent=True)\n         '''\n         ax = self.ax\n         ax.patch.set_facecolor(color)\n         lh = ax.get_legend()\n         if lh != None:\n             lh.legendPatch.set_facecolor(color)\n\n         plt.draw()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getParLabelAndUnit(self, param):\n\n        firstObject = self.objectList[0]\n\n        if isinstance(firstObject, ac.Planet):\n            if 'star.' in param:\n                return _starPars[param[5:]]  # cut off star. part\n            else:\n                return _planetPars[param]\n        elif isinstance(firstObject, ac.Star):\n            return _starPars[param]\n        else:\n            raise TypeError('Only Planets and Star object are currently supported, you gave {0}'.format(type(firstObject)))", "response": "checks if the parameter is a planet or star then returns the correct unit and label for the the\n job from the parDicts\n "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of all the results that can meet SNR requirments", "response": "def _processResults(self):\n        \"\"\" Checks each result can meet SNR requirments, adds to count\n        :return:\n        \"\"\"\n\n        resultsByClass = self._genEmptyResults()\n\n        for astroObject in self.objectList:\n            sortKey = self._getSortKey(astroObject)\n            resultsByClass[sortKey] += 1\n\n        return resultsByClass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getPlotData(self):\n        resultsByClass = self.resultsByClass\n\n        try:\n            if resultsByClass['Uncertain'] == 0:  # remove uncertain tag if present and = 0\n                resultsByClass.pop('Uncertain', None)\n        except KeyError:\n            pass\n\n        plotData = list(zip(*resultsByClass.items()))  # (labels, ydata)\n\n        return plotData", "response": "Turn the resultsByClass Dict into a list of bin groups skipping the uncertain group"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _genEmptyResults(self):\n\n        allowedKeys = self._allowedKeys\n\n        keysDict = OrderedDict()  # Note: list comprehension take 0 then 2 then 1 then 3 etc for some reason. we want strict order\n        for k in allowedKeys:\n            keysDict[k] = 0\n\n\n        resultsByClass = keysDict\n\n        return resultsByClass", "response": "Generate an empty dict for the current set of keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getSortKey(self, planet):\n\n        value = eval('planet.'+self._planetProperty)\n\n        # TODO some sort of data validation, either before or using try except\n\n        if self.unit is not None:\n            try:\n                value = value.rescale(self.unit)\n            except AttributeError:  # either nan or unitless\n                pass\n\n        return _sortValueIntoGroup(self._allowedKeys[:-1], self._binlimits, value)", "response": "Takes a planet and turns it into a key to be sorted by\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _genKeysBins(self):\n        binlimits = self._binlimits\n\n        allowedKeys = []\n        midbinlimits = binlimits\n\n        if binlimits[0] == -float('inf'):\n            midbinlimits = binlimits[1:]  # remove the bottom limit\n            allowedKeys.append('<{0}'.format(midbinlimits[0]))\n\n        if binlimits[-1] == float('inf'):\n            midbinlimits = midbinlimits[:-1]\n\n        lastbin = midbinlimits[0]\n\n        for binlimit in midbinlimits[1:]:\n            if lastbin == binlimit:\n                allowedKeys.append('{0}'.format(binlimit))\n            else:\n                allowedKeys.append('{0} to {1}'.format(lastbin, binlimit))\n            lastbin = binlimit\n\n        if binlimits[-1] == float('inf'):\n            allowedKeys.append('{0}+'.format(binlimits[-2]))\n\n        allowedKeys.append('Uncertain')\n        self._allowedKeys = allowedKeys", "response": "Generates keys from binlimits and sets self. _allowedKeys normally set in _classVariables\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of use on the x axis", "response": "def set_xaxis(self, param, unit=None, label=None):\n        \"\"\" Sets the value of use on the x axis\n        :param param: value to use on the xaxis, should be a variable or function of the objects in objectList. ie 'R'\n        for the radius variable and 'calcDensity()' for the calcDensity function\n\n        :param unit: the unit to scale the values to, None will use the default\n        :type unit: quantities unit or None\n\n        :param label: axis label to use, if None \"Parameter (Unit)\" is generated here and used\n        :type label: str\n        \"\"\"\n\n        if unit is None:\n            unit = self._getParLabelAndUnit(param)[1]  # use the default unit defined in this class\n        self._xaxis_unit = unit\n\n        self._xaxis = self._set_axis(param, unit)\n        if label is None:\n            self.xlabel = self._gen_label(param, unit)\n        else:\n            self.xlabel = label"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of use on the yaxis", "response": "def set_yaxis(self, param, unit=None, label=None):\n        \"\"\" Sets the value of use on the yaxis\n        :param param: value to use on the yaxis, should be a variable or function of the objects in objectList. ie 'R'\n        for the radius variable and 'calcDensity()' for the calcDensity function\n\n        :param unit: the unit to scale the values to\n        :type unit: quantities unit or None\n\n        :param label: axis label to use, if None \"Parameter (Unit)\" is generated here and used\n        :type label: str\n        \"\"\"\n        if unit is None:\n            unit = self._getParLabelAndUnit(param)[1]  # use the default unit defined in this class\n        self._yaxis_unit = unit\n\n        self._yaxis = self._set_axis(param, unit)\n        if label is None:\n            self.ylabel = self._gen_label(param, unit)\n        else:\n            self.ylabel = label"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_axis(self, param, unit):\n        axisValues = []\n        for astroObject in self.objectList:\n            try:\n                value = eval('astroObject.{0}'.format(param))\n            except ac.HierarchyError:  # ie trying to call planet.star and one planet is a lone ranger\n                value = np.nan\n\n            if unit is None:  # no unit to rescale (a aq.unitless quanitity would otherwise fail with ValueError)\n                axisValues.append(value)\n            else:\n                try:\n                    axisValues.append(value.rescale(unit))\n                except AttributeError:  # either nan or unitless\n                    axisValues.append(value)\n\n        return axisValues", "response": "this method will take a variable or a function and turn it into a list by evaluating on each planet\n            and then rescale the object to the given unit"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_marker_color(self, color='#3ea0e4', edgecolor='k'):\n        # TODO allow a colour set per another variable\n        self._marker_color = color\n        self._edge_color = edgecolor", "response": "set the marker color used in the plot\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the initial data dictionary to store the values", "response": "def setup_keys(self):\n        \"\"\" Build the initial data dictionary to store the values\n        \"\"\"\n\n        discovery_methods = {}\n        discovery_years = {}\n        nan_list = []\n\n        # Initial Loop to get keys\n        for planet in self.planet_list:\n            if 'Solar System' in planet.params['list'] and self.skip_solar_system_planets:\n                continue\n            try:\n                discovery_methods[planet.discoveryMethod] += 1\n            except KeyError:\n                discovery_methods[planet.discoveryMethod] = 1\n\n            try:\n                discovery_years[planet.discoveryYear] += 1\n            except KeyError:\n                discovery_years[planet.discoveryYear] = 1\n\n            if planet.discoveryMethod is np.nan:\n                nan_list.append(planet)\n\n        self.nan_list = nan_list\n\n        return discovery_years"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the current state of the object.", "response": "def plot(self, method_labels=None, exodata_credit=True):\n        \"\"\"\n        :param method_labels: list of legend labels for the methods, i.e. give\n        'Radial Velocity' rather than 'RV'\n\n        :return:\n        \"\"\"\n\n        if method_labels is None:\n            method_labels = self.methods_to_plot\n\n        plot_matrix, year_list, discovery_years = self.generate_data()\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        ind = np.arange(len(year_list))\n\n        # output from seaborn.color_palette(\"Set1\", n_colors=8)\n        colors = [(0.89411765336990356, 0.10196078568696976, 0.10980392247438431),\n                  (0.21602460800432691, 0.49487120380588606, 0.71987698697576341),\n                  (0.30426760128900115, 0.68329106055054012, 0.29293349969620797),\n                  (0.60083047361934883, 0.30814303335021526, 0.63169552298153153),\n                  (1.0, 0.50591311045721465, 0.0031372549487095253),\n                  (0.99315647868549117, 0.9870049982678657, 0.19915417450315812),\n                  (0.65845446095747107, 0.34122261685483596, 0.1707958535236471),\n                  (0.95850826852461868, 0.50846600392285513, 0.74492888871361229)]\n\n        width = 0.9\n        bottom = np.zeros_like(year_list)\n        for i, method in enumerate(self.methods_to_plot):\n            bar_data = [plot_matrix[year][method] for year in year_list]\n            plt.bar(ind, bar_data, width, color=colors[i], bottom=bottom, label=method_labels[i], linewidth=0)\n\n            bottom += np.array(bar_data) # plot the next bar stacked\n\n        plt.ylabel('Number of planets', fontsize=14)\n        plt.xlabel('Year', fontsize=14)\n        plt.xticks(ind+width/2., year_list, rotation=70, fontsize=13)\n        plt.xlim(0, ind[-1]+1)\n        plt.legend(loc=2, fontsize=13)\n\n        if exodata_credit is True:\n            ax.text(2, 420, 'Generated by ExoData on {}'.format(time.strftime(\"%d/%m/%Y\")), fontsize=11)\n\n        for i, year in enumerate(year_list):\n            try:\n                total_planets = discovery_years[year]\n            except KeyError:  # year with no planets discovered\n                continue  # Dont label\n            y_pos = bottom[i]+10\n            x_pos = i\n\n            ax.text(x_pos, y_pos, '{}'.format(total_planets), fontsize=9)\n\n        plt.grid(b=True, which='both', color='0.9', linestyle='-')\n\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')", "response": "This function prints and plots the confusion matrix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrim the list to the specified range of elements.", "response": "def trim(self, start, stop):\n        \"\"\"Trim the list to the specified range of elements.\"\"\"\n        return self.client.ltrim(self.name, start, stop - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, value, count=1):\n        count = self.client.lrem(self.name, value, num=count)\n        if not count:\n            raise ValueError(\"%s not in list\" % value)\n        return count", "response": "Removes occurences of value from the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove an element from the set ; it must be a member.", "response": "def remove(self, member):\n        \"\"\"Remove element from set; it must be a member.\n\n        :raises KeyError: if the element is not a member.\n\n        \"\"\"\n        if not self.client.srem(self.name, member):\n            raise KeyError(member)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove and return an arbitrary set element.", "response": "def pop(self):\n        \"\"\"Remove and return an arbitrary set element.\n\n        :raises KeyError: if the set is empty.\n\n        \"\"\"\n        member = self.client.spop(self.name)\n        if member is not None:\n            return member\n        raise KeyError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef union(self, other):\n        if isinstance(other, self.__class__):\n            return self.client.sunion([self.name, other.name])\n        else:\n            return self._as_set().union(other)", "response": "Return the union of sets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating this set with the union of itself and others.", "response": "def update(self, other):\n        \"\"\"Update this set with the union of itself and others.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.client.sunionstore(self.name, [self.name, other.name])\n        else:\n            return map(self.add, other)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intersection(self, other):\n        if isinstance(other, self.__class__):\n            return self.client.sinter([self.name, other.name])\n        else:\n            return self._as_set().intersection(other)", "response": "Return the intersection of two sets as a new set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the set with the intersection of itself and another.", "response": "def intersection_update(self, other):\n        \"\"\"Update the set with the intersection of itself and another.\"\"\"\n        return self.client.sinterstore(self.name, [self.name, other.name])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef difference(self, *others):\n        if all([isinstance(a, self.__class__) for a in others]):\n            return self.client.sdiff([self.name] + [other.name for other in others])\n        else:\n            othersets = filter(lambda x: isinstance(x, set), others)\n            otherTypes = filter(lambda x: isinstance(x, self.__class__), others)\n            return self.client.sdiff([self.name] + [other.name for other in otherTypes]).difference(*othersets)", "response": "Return the difference of two sets as a new set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef difference_update(self, other):\n        return self.client.sdiffstore(self.name, [self.name, other.name])", "response": "Remove all elements of this set from another set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, member, score):\n        return self.client.zadd(self.name, member, score)", "response": "Add the specified member to the sorted set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a member from the set.", "response": "def remove(self, member):\n        \"\"\"Remove member.\"\"\"\n        if not self.client.zrem(self.name, member):\n            raise KeyError(member)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the score of a member by amount.", "response": "def increment(self, member, amount=1):\n        \"\"\"Increment the score of ``member`` by ``amount``.\"\"\"\n        return self.client.zincrby(self.name, member, amount)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all the elements with score between min and max.", "response": "def range_by_score(self, min, max, num=None, withscores=False):\n        \"\"\"Return all the elements with score >= min and score <= max\n        (a range query) from the sorted set.\"\"\"\n        return self.client.zrangebyscore(self.name, min, max, num=num,\n                                         withscores=withscores)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves specified key and return the corresponding value.", "response": "def pop(self, key, *args, **kwargs):\n        \"\"\"Remove specified key and return the corresponding value.\n\n        :keyword default: If key is not found, ``default`` is returned if given,\n            otherwise :exc:`KeyError` is raised.\n\n        \"\"\"\n        try:\n            val = self[key]\n        except KeyError:\n            if len(args):\n                return args[0]\n            if \"default\" in kwargs:\n                return kwargs[\"default\"]\n            raise\n\n        try:\n            del(self[key])\n        except KeyError:\n            pass\n\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef full(self):\n        return self.maxsize and len(self.list) >= self.maxsize or False", "response": "Return True if the queue is full False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nputting an item into the queue.", "response": "def put(self, item, **kwargs):\n        \"\"\"Put an item into the queue.\"\"\"\n        if self.full():\n            raise Full()\n        self._append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef increment(self, member, amount=1):\n        self._dict[member] += amount\n        return self._dict[member]", "response": "Increment the score of a member by amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range_by_score(self, min, max):\n        data = self.items()\n        keys = [r[1] for r in data] \n        start = bisect.bisect_left(keys, min)\n        end = bisect.bisect_right(keys, max, start)\n        return self._as_set()[start:end]", "response": "Return all the elements with score between min and max."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self):\n        if not self.connected:\n            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.socket.connect(self.address)\n            self.connected = True", "response": "Connect to the given socket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a formatted message to the ADB server", "response": "def send(self, data):\n        \"\"\"Send a formatted message to the ADB server\"\"\"\n        self._send_data(int_to_hex(len(data)))\n\n        self._send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend data to the ADB server", "response": "def _send_data(self, data):\n        \"\"\"Send data to the ADB server\"\"\"\n        total_sent = 0\n\n        while total_sent < len(data):\n            # Send only the bytes that haven't been\n            # sent yet\n            sent = self.socket.send(data[total_sent:].encode(\"ascii\"))\n\n            if sent == 0:\n                self.close()\n                raise RuntimeError(\"Socket connection dropped, \"\n                                   \"send failed\")\n            total_sent += sent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive_until_end(self, timeout=None):\n        if self.receive_fixed_length(4) != \"OKAY\":\n            raise SocketError(\"Socket communication failed: \"\n                              \"the server did not return a valid response\")\n\n        # The time at which the receive starts\n        start_time = time.clock()\n\n        output = \"\"\n\n        while True:\n            if timeout is not None:\n                self.socket.settimeout(timeout - (time.clock() - start_time))\n            \n            chunk = ''\n            try:\n                chunk = self.socket.recv(4096).decode(\"ascii\")\n            except socket.timeout:\n                return output            \n\n            if not chunk:\n                return output\n\n            output += chunk", "response": "Reads and blocks until the socket closes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef defaults(f, self, *args, **kwargs):\n    for name, data in PARAMETERS.iteritems():\n        kwargs[name] = kwargs.get(name) or getattr(self, name)\n        if 'transform' in data:\n            kwargs[name] = data['transform'](kwargs[name])\n    return f(self, *args, **kwargs)", "response": "Decorator for creating a new object with default values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef requires(*params):\n    def requires(f, self, *args, **kwargs):\n        missing = filter(lambda x: kwargs.get(x) is None, params)\n        if missing:\n            msgs = \", \".join([PARAMETERS[x]['msg'] for x in missing])\n            raise ValueError(\"Missing the following parameters: %s\" % msgs)\n        return f(self, *args, **kwargs)\n    return decorator(requires)", "response": "Decorator to specify which methods are required to be called on a resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the security group ID for a given security group name.", "response": "def get_security_group_id(self, name):\n        \"\"\"\n        Take name string, give back security group ID.\n\n        To get around VPC's API being stupid.\n        \"\"\"\n        # Memoize entire list of groups\n        if not hasattr(self, '_security_groups'):\n            self._security_groups = {}\n            for group in self.get_all_security_groups():\n                self._security_groups[group.name] = group.id\n        return self._security_groups[name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_instance_subnet_name(self, instance):\n        # TODO: we have to do this here since we are monkeypatching Instance.\n        # If we switch to custom Instance (sub)class then we could do it in the\n        # object, provided it has access to the configuration data.\n        if instance.subnet_id:\n            # Account for omitted 'subnet-'\n            subnet = self.config['subnets'][instance.subnet_id[7:]]\n        else:\n            subnet = BLANK\n        return subnet", "response": "Returns a human readable name for given instance s subnet or None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the subnet ID for given name.", "response": "def get_subnet_id(self, name):\n        \"\"\"\n        Return subnet ID for given ``name``, if it exists.\n\n        E.g. with a subnet mapping of ``{'abc123': 'ops', '67fd56': 'prod'}``,\n        ``get_subnet_id('ops')`` would return ``'abc123'``. If the map has\n        non-unique values, the first matching key will be returned.\n\n        If no match is found, the given ``name`` is returned as-is. This works\n        well for e.g. normalizing names-or-IDs to just IDs.\n        \"\"\"\n        for subnet_id, subnet_name in self.config['subnets'].iteritems():\n            if subnet_name == name:\n                return subnet_id\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, hostname, **kwargs):\n        # Create\n        creating = \"Creating '%s' (a %s instance of %s)\" % (\n            hostname, kwargs['size'], kwargs['ami']\n        )\n        with self.msg(creating):\n            instance = self._create(hostname, kwargs)\n\n        # Name\n        with self.msg(\"Tagging as '%s'\" % hostname):\n            try:\n                instance.rename(hostname)\n            # One-time retry for API errors when setting tags\n            except _ResponseError:\n                time.sleep(1)\n                instance.rename(hostname)\n\n        # Wait for it to finish booting\n        with self.msg(\"Waiting for boot: \"):\n            tick = 5\n            while instance.state != 'running':\n                self.log(\".\", end='')\n                time.sleep(tick)\n                instance.update()\n\n        return instance", "response": "Create a new EC2 instance with the given hostname and return the instance object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, arg):\n        try:\n            reservations = self.get_all_instances(filters={'tag:Name': [arg]})\n            instance = reservations[0].instances[0]\n        except IndexError:\n            try:\n                instance = self.get_all_instances([arg])[0].instances[0]\n            except (_ResponseError, IndexError):\n                # TODO: encapsulate actual exception for debugging\n                err = \"Can't find any instance with name or ID '%s'\" % arg\n                raise ValueError(err)\n        return instance", "response": "Get an instance with given EC2 ID or nametag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_volumes_for_instance(self, arg, device=None):\n        instance = self.get(arg)\n        filters = {'attachment.instance-id': instance.id}\n        if device is not None:\n            filters['attachment.device'] = device\n        return self.get_all_volumes(filters=filters)", "response": "Get all EC2 Volume objects attached to an instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nterminating the instance with given EC2 ID or nametag.", "response": "def terminate(self, arg):\n        \"\"\"\n        Terminate instance with given EC2 ID or nametag.\n        \"\"\"\n        instance = self.get(arg)\n        with self.msg(\"Terminating %s (%s): \" % (instance.name, instance.id)):\n            instance.rename(\"old-%s\" % instance.name)\n            instance.terminate()\n            while instance.state != 'terminated':\n                time.sleep(5)\n                self.log(\".\", end='')\n                instance.update()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get(self):\n\n        data, stat = self.zk.get(self.path)\n        if not len(data):\n            return {}, stat.version\n        if self.OLD_SEPARATOR in data:\n            return self._get_old()\n        return json.loads(data), stat.version", "response": "get and parse data stored in self. path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes and set data to self. path", "response": "def _set(self, data, version):\n        \"\"\"serialize and set data to self.path.\"\"\"\n\n        self.zk.set(self.path, json.dumps(data), version)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets and parse data stored in self. path", "response": "def _get_old(self):\n        \"\"\"get and parse data stored in self.path.\"\"\"\n\n        def _deserialize(d):\n            if not len(d):\n                return {}\n            return dict(l.split(self.OLD_SEPARATOR) for l in d.split('\\n'))\n\n        data, stat = self.zk.get(self.path)\n        return _deserialize(data.decode('utf8')), stat.version"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new config for the specified environment", "response": "def create_config(self, env, conf):\n        \"\"\"\n        Set conf to env under service.\n\n        pass None to env for root.\n        \"\"\"\n\n        if not isinstance(conf, collections.Mapping):\n            raise ValueError(\"conf must be a collections.Mapping\")\n\n        self.zk.ensure_path(self.view_path)\n\n        self._create(\n            self._get_env_path(env),\n            conf\n        )\n\n        self._update_view(env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets conf to env under root.", "response": "def set_config(self, env, conf, version):\n        \"\"\"\n        Set conf to env under service.\n\n        pass None to env for root.\n        \"\"\"\n\n        if not isinstance(conf, collections.Mapping):\n            raise ValueError(\"conf must be a collections.Mapping\")\n\n        self._set(\n            self._get_env_path(env),\n            conf,\n            version\n        )\n        path = self._get_env_path(env)\n        \"\"\"Update env's children with new config.\"\"\"\n        for child in zkutil.walk(self.zk, path):\n            self._update_view(Env(child[len(self.conf_path)+1:]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(self, hostname):\n        version, config = self._get(\n            self.associations.get(hostname)\n        )\n        return config", "response": "Returns a configuration for hostname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the view of env.", "response": "def get_view_by_env(self, env):\n        \"\"\"\n        Returns the view of `env`.\n\n        \"\"\"\n        version, data = self._get(self._get_view_path(env))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assoc_host(self, hostname, env):\n\n        dest = self._get_view_path(env)\n        self.associations.set(hostname, dest)", "response": "Associate a host with an environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_associations(self, env):\n\n        if env.is_root:\n            return None\n\n        associations = self.associations.get_all()\n        return [assoc for assoc in associations\n                if associations[assoc] == self._get_view_path(env)]", "response": "Get all the associations for this environment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _flatten_from_root(self, env):\n\n        nodes = env.components\n\n        # Path through the znode graph from root ('') to env\n        path = [nodes[:n] for n in xrange(len(nodes) + 1)]\n\n        # Expand path and map it to the root\n        path = map(\n            self._get_env_path,\n            [Env('/'.join(p)) for p in path]\n        )\n\n        data = {}\n        for n in path:\n            _, config = self._get(n)\n            data.update(config)\n\n        return data", "response": "Flatten values from root down in to new view."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls the package via pip pin it only to requirements file.", "response": "def install(packagename, save, save_dev, save_test, filename):\n    \"\"\"\n    Install the package via pip, pin the package only to requirements file.\n    Use option to decide which file the package will be pinned to.\n    \"\"\"\n    print('Installing ', packagename)\n    print(sh_pip.install(packagename))\n    if not filename:\n        filename = get_filename(save, save_dev, save_test)\n    try:\n        add_requirements(packagename, filename)\n    except AssertionError:\n        print('Package already pinned in ', filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(packagename, save, save_dev, save_test, filename):\n    print(sh_pip.uninstall(packagename, \"-y\"))\n    if not filename:\n        filename = get_filename(save, save_dev, save_test)\n    remove_requirements(packagename, filename)", "response": "Remove the package from the requirements file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting path into a list of directories.", "response": "def os_path_split_asunder(path, debug=False):\n        \"\"\"\n        http://stackoverflow.com/a/4580931/171094\n        \"\"\"\n        parts = []\n        while True:\n            newpath, tail = os.path.split(path)\n            if debug: print repr(path), (newpath, tail)\n            if newpath == path:\n                assert not tail\n                if path: parts.append(path)\n                break\n            parts.append(tail)\n            path = newpath\n        parts.reverse()\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nis the first argument a sub - directory of the second argument?", "response": "def is_subdirectory(potential_subdirectory, expected_parent_directory):\n        \"\"\"\n        Is the first argument a sub-directory of the second argument?\n\n        :param potential_subdirectory:\n        :param expected_parent_directory:\n        :return: True if the potential_subdirectory is a child of the expected parent directory\n\n        \"\"\"\n\n        def _get_normalized_parts(path):\n            return DKCloudCommandRunner.os_path_split_asunder(os.path.realpath(os.path.abspath(os.path.normpath(path))))\n\n        # make absolute and handle symbolic links, split into components\n        sub_parts = _get_normalized_parts(potential_subdirectory)\n        parent_parts = _get_normalized_parts(expected_parent_directory)\n\n        if len(parent_parts) > len(sub_parts):\n            # a parent directory never has more path segments than its child\n            return False\n\n        # we expect the zip to end with the short path, which we know to be the parent\n        return all(part1 == part2 for part1, part2 in zip(sub_parts, parent_parts))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_all_files(dk_api, kitchen, recipe_name, recipe_dir, message, dryrun=False):\n        rc = DKReturnCode()\n        if kitchen is None or recipe_name is None or message is None:\n            s = 'ERROR: DKCloudCommandRunner bad input parameters'\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        rc = dk_api.recipe_status(kitchen, recipe_name, recipe_dir)\n        if not rc.ok():\n            rs = 'DKCloudCommand.update_all_files failed\\nmessage: %s' % rc.get_message()\n            rc.set_message(rs)\n            return rc\n\n        rl = rc.get_payload()\n        if (len(rl['different']) + len(rl['only_local']) + len(rl['only_remote'])) == 0:\n            rs = 'DKCloudCommand.update_all_files no files changed.'\n            rc.set_message(rs)\n            return rc\n\n        rc = DKCloudCommandRunner._update_changed_files(dk_api, rl['different'], kitchen, recipe_name, message, dryrun)\n        if not rc.ok():\n            return rc\n        msg_differences = rc.get_message()\n\n        rc = DKCloudCommandRunner._add_new_files(dk_api, rl['only_local'], kitchen, recipe_name, message, dryrun)\n        if not rc.ok():\n            return rc\n        msg_additions = rc.get_message()\n\n        rc = DKCloudCommandRunner._remove_deleted_files(dk_api, rl['only_remote'], kitchen, recipe_name, message,\n                                                        dryrun)\n        if not rc.ok():\n            return rc\n        msg_deletions = rc.get_message()\n\n        msg = ''\n        if len(msg_differences) > 0:\n            if len(msg) > 0:\n                msg += '\\n'\n            msg += msg_differences + '\\n'\n        if len(msg_additions) > 0:\n            if len(msg) > 0:\n                msg += '\\n'\n            msg += msg_additions + '\\n'\n        if len(msg_deletions) > 0:\n            if len(msg) > 0:\n                msg += '\\n'\n            msg += msg_deletions + '\\n'\n        rc.set_message(msg)\n        return rc", "response": "update all files in a kitchen"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_file(dk_api, kitchen, recipe_name, message, files_to_update_param):\n        rc = DKReturnCode()\n        if kitchen is None or recipe_name is None or message is None or files_to_update_param is None:\n            s = 'ERROR: DKCloudCommandRunner bad input parameters'\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        # Take a simple string or an array\n        if isinstance(files_to_update_param, basestring):\n            files_to_update = [files_to_update_param]\n        else:\n            files_to_update = files_to_update_param\n\n        msg = ''\n        for file_to_update in files_to_update:\n            try:\n                with open(file_to_update, 'r') as f:\n                    file_contents = f.read()\n            except IOError as e:\n                if len(msg) != 0:\n                    msg += '\\n'\n                msg += '%s' % (str(e))\n                rc.set(rc.DK_FAIL, msg)\n                return rc\n            except ValueError as e:\n                if len(msg) != 0:\n                    msg += '\\n'\n                msg += 'ERROR: %s' % e.message\n                rc.set(rc.DK_FAIL, msg)\n                return rc\n            rc = dk_api.update_file(kitchen, recipe_name, message, file_to_update, file_contents)\n            if not rc.ok():\n                if len(msg) != 0:\n                    msg += '\\n'\n                msg += 'DKCloudCommand.update_file for %s failed\\n\\tmessage: %s' % (file_to_update, rc.get_message())\n                rc.set_message(msg)\n                return rc\n            else:\n                if len(msg) != 0:\n                    msg += '\\n'\n                msg += 'DKCloudCommand.update_file for %s succeeded' % file_to_update\n\n        rc.set_message(msg)\n        return rc", "response": "This function will update the contents of a file in a Kitchen Recipe"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a file to the recipe", "response": "def add_file(dk_api, kitchen, recipe_name, message, api_file_key):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param kitchen: string\n        :param recipe_name: string\n        :param message: string  -- commit message, string\n        :param api_file_key: string  -- directory where the recipe file lives\n        :rtype: DKReturnCode\n        \"\"\"\n        rc = DKReturnCode()\n        if kitchen is None or recipe_name is None or message is None or api_file_key is None:\n            s = 'ERROR: DKCloudCommandRunner bad input parameters'\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        ig = DKIgnore()\n        if ig.ignore(api_file_key):\n            rs = 'DKCloudCommand.add_file ignoring %s' % api_file_key\n            rc.set_message(rs)\n            return rc\n\n        if not os.path.exists(api_file_key):\n            s = \"'%s' does not exist\" % api_file_key\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        try:\n            with open(api_file_key, 'r') as f:\n                file_contents = f.read()\n        except ValueError as e:\n            s = 'ERROR: %s' % e.message\n            rc.set(rc.DK_FAIL, s)\n            return rc\n        rc = dk_api.add_file(kitchen, recipe_name, message, api_file_key, file_contents)\n        if rc.ok():\n            rs = 'DKCloudCommand.add_file for %s succeed' % api_file_key\n        else:\n            rs = 'DKCloudCommand.add_file for %s failed\\nmessage: %s' % (api_file_key, rc.get_message())\n        rc.set_message(rs)\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_file(dk_api, kitchen, recipe_name, message, files_to_delete_param):\n        rc = DKReturnCode()\n        if kitchen is None or recipe_name is None or message is None or files_to_delete_param is None:\n            s = 'ERROR: DKCloudCommandRunner bad input parameters'\n            rc.set(rc.DK_FAIL, s)\n            return rc\n\n        # Take a simple string or an array\n        if isinstance(files_to_delete_param, basestring):\n            files_to_delete = [files_to_delete_param]\n        else:\n            files_to_delete = files_to_delete_param\n        msg = ''\n        for file_to_delete in files_to_delete:\n            basename = os.path.basename(file_to_delete)\n            rc = dk_api.delete_file(kitchen, recipe_name, message, file_to_delete, basename)\n            if not rc.ok():\n                msg += '\\nDKCloudCommand.delete_file for %s failed\\nmessage: %s' % (file_to_delete, rc.get_message())\n                rc.set_message(msg)\n                return rc\n            else:\n                msg += 'DKCloudCommand.delete_file for %s succeed' % file_to_delete\n        rc.set_message(msg)\n        return rc", "response": "This function deletes a file from the specified recipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch_active_servings(dk_api, kitchen, period):\n        print 'period', period\n\n        # try:\n        #     p = int(period)\n        # except ValueError:\n        #     return 'DKCloudCommand.watch_active_servings requires an integer for the period'\n        if period <= 0:\n            return 'DKCloudCommand.watch_active_servings requires a positive period'\n\n        DKActiveServingWatcherSingleton().set_sleep_time(period)\n        DKActiveServingWatcherSingleton().set_api(dk_api)\n        DKActiveServingWatcherSingleton().set_kitchen(kitchen)\n        DKActiveServingWatcherSingleton().start_watcher()\n        return \"\"", "response": "watch the active servings for the given period"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string. :param dk_api: -- api object :param kitchen: string :param recipe_name: string -- kitchen name, string :param variation_name: string -- name of the recipe variation_name to be used :rtype: DKReturnCode", "response": "def get_compiled_serving(dk_api, kitchen, recipe_name, variation_name):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param kitchen: string\n        :param recipe_name: string  -- kitchen name, string\n        :param variation_name: string -- name of the recipe variation_name to be used\n        :rtype: DKReturnCode\n        \"\"\"\n        rc = dk_api.get_compiled_serving(kitchen, recipe_name, variation_name)\n        if rc.ok():\n            rs = 'DKCloudCommand.get_compiled_serving succeeded %s\\n' % json.dumps(rc.get_payload(), indent=4)\n        else:\n            m = rc.get_message()\n            e = m.split('the logfile errors are:nn')\n            if len(e) > 1:\n                e2 = DKCloudCommandRunner._decompress(e[len(e) - 1])\n                errors = e2.split('|')\n                re = e[0] + \" \" + 'the logfile errors are: '\n                for e in errors:\n                    re += '\\n%s' % e\n            else:\n                re = m\n            rs = 'DKCloudCommand.get_compiled_serving failed\\nmessage: %s\\n' % re\n        rc.set_message(rs)\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string. :param dk_api: -- api object :param from_kitchen: string :param to_kitchen: string -- kitchen name, string :rtype: DKReturnCode", "response": "def merge_kitchens_improved(dk_api, from_kitchen, to_kitchen):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param from_kitchen: string\n        :param to_kitchen: string  -- kitchen name, string\n        :rtype: DKReturnCode\n        \"\"\"\n        unresolved_conflicts = DKKitchenDisk.get_unresolved_conflicts(from_kitchen, to_kitchen)\n        if unresolved_conflicts is not None and len(unresolved_conflicts) != 0:\n            msg = DKCloudCommandRunner._print_unresolved_conflicts(unresolved_conflicts)\n            rc = DKReturnCode()\n            rc.set(DKReturnCode.DK_FAIL, msg)\n            return rc\n\n        resolved_conflicts = DKKitchenDisk.get_resolved_conflicts(from_kitchen, to_kitchen)\n        # if resolved_conflicts is not None and len(resolved_conflicts) != 0:\n\n        md = dk_api.merge_kitchens_improved(from_kitchen, to_kitchen, resolved_conflicts)\n        if not md.ok():\n            md.set_message('merge_kitchens_improved error from %s to Kitchen %s\\nmessage: %s' %\n                           (from_kitchen, to_kitchen, md.get_message()))\n            return md\n        merge_no_conflicts = DKCloudCommandRunner._check_no_merge_conflicts(md.get_payload())\n        if merge_no_conflicts:\n            msg = DKCloudCommandRunner._print_merge_success(md.get_payload())\n            current_kitchen = DKKitchenDisk.find_kitchen_name()\n            md.set_message(msg)\n        else:\n            # Found conflicts\n            recipe_name = DKRecipeDisk.find_recipe_name()\n            kitchen_name = DKKitchenDisk.find_kitchen_name()\n            if recipe_name is None and kitchen_name is None:\n                # We are not in a kitchen or recipe folder, so just report the findings\n                rs = DKCloudCommandRunner.print_merge_conflicts(md.get_payload())\n                md.set_message(rs)\n            else:\n                # We are in a recipe folder, so let's write out the conflicted files.\n                rc = DKCloudCommandRunner.write_merge_conflicts(md.get_payload())\n                if rc.ok():\n                    md.set_message(rc.get_message())\n                else:\n                    md = rc\n        return md"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new order", "response": "def create_order(dk_api, kitchen, recipe_name, variation_name, node_name=None):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param kitchen: string\n        :param recipe_name: string  -- kitchen name, string\n        :param variation_name: string -- name of the recipe variation_name to be run\n        :param node_name: string -- name of the single node to run\n        :rtype: DKReturnCode\n        \"\"\"\n        rc = dk_api.create_order(kitchen, recipe_name, variation_name, node_name)\n        if rc.ok():\n            s = 'Order ID is: %s' % rc.get_payload()\n        else:\n            m = rc.get_message().replace('\\\\n','\\n')\n            e = m.split('the logfile errors are:')\n            if len(e) > 1:\n                e2 = DKCloudCommandRunner._decompress(e[-1])\n                errors = e2.split('|')\n                re = e[0] + \" \" + 'the logfile errors are: '\n                for e in errors:\n                    re += '\\n%s' % e\n            else:\n                re = m\n            s = 'DKCloudCommand.create_order failed\\nmessage: %s\\n' % re\n        rc.set_message(s)\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string. :param dk_api: -- api object :param kitchen: string :param pd: dict :rtype: DKReturnCode", "response": "def orderrun_detail(dk_api, kitchen, pd):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param kitchen: string\n        :param pd: dict\n        :rtype: DKReturnCode\n        \"\"\"\n        if DKCloudCommandRunner.SUMMARY in pd:\n            display_summary = True\n        else:\n            display_summary = False\n        # always get summary information\n        pd[DKCloudCommandRunner.SUMMARY] = True\n        rc = dk_api.orderrun_detail(kitchen, pd)\n        s = ''\n        if not rc.ok() or not isinstance(rc.get_payload(), list):\n            s = 'Issue with getting order run details\\nmessage: %s' % rc.get_message()\n            rc.set_message(s)\n            return rc\n\n        # we have a list of servings, find the right dict\n        serving_list = rc.get_payload()\n        serving = None\n        if DKCloudCommandRunner.ORDER_RUN_ID in pd:\n            order_run_id = pd[DKCloudCommandRunner.ORDER_RUN_ID]\n            for serv in serving_list:\n                if serv[DKCloudCommandRunner.ORDER_RUN_ID] == order_run_id:\n                    serving = serv\n                    break\n        elif DKCloudCommandRunner.ORDER_ID in pd:\n            order_id = pd[DKCloudCommandRunner.ORDER_ID]\n            for serv in serving_list:\n                if serv[DKCloudCommandRunner.ORDER_ID] == order_id:\n                    serving = serv\n                    break\n        else:\n            # find the newest serving\n            dex = -1\n            latest = None\n            for i, serving in enumerate(serving_list):\n                if DKCloudCommandRunner.ORDER_ID in serving and serving[DKCloudCommandRunner.ORDER_ID] > latest:\n                    latest = serving[DKCloudCommandRunner.ORDER_ID]\n                    dex = i\n            if dex != -1:\n                serving = serving_list[dex]\n\n        if serving is None:\n            rc.set(rc.DK_FAIL,\n                   \"No OrderRun information.  Try using 'dk order-list -k %s' to see what is available.\" % kitchen)\n            return rc\n\n        # serving now contains the dictionary of the serving to display\n        # pull out the information and put it in the message string of the rc\n\n        if serving and display_summary:\n            s += '\\nORDER RUN SUMMARY\\n\\n'\n            summary = None\n            if DKCloudCommandRunner.SUMMARY in serving:\n                summary = serving[DKCloudCommandRunner.SUMMARY]\n            pass\n            s += 'Order ID:\\t%s\\n' % serving[DKCloudCommandRunner.ORDER_ID]\n            orid_from_serving = serving[DKCloudCommandRunner.ORDER_RUN_ID]\n            s += 'Order Run ID:\\t%s\\n' % orid_from_serving\n            s += 'Status:\\t\\t%s\\n' % serving['status']\n            s += 'Kitchen:\\t%s\\n' % kitchen\n\n            if summary and 'name' in summary:\n                s += 'Recipe:\\t\\t%s\\n' % summary['name']\n            else:\n                s += 'Recipe:\\t\\t%s\\n' % 'Not available'\n\n            # variation name is inside the order id, pull it out\n            s += 'Variation:\\t%s\\n' % orid_from_serving.split('#')[3]\n\n            if summary and 'start-time' in summary:\n                start_time = summary['start-time']\n                if isinstance(start_time, basestring):\n                    s += 'Start time:\\t%s\\n' % summary['start-time'].split('.')[0]\n                else:\n                    s += 'Start time:\\t%s\\n' % 'Not available 1'\n            else:\n                s += 'Start time:\\t%s\\n' % 'Not available 2'\n\n            run_time = None\n            if summary and 'total-recipe-time' in summary:\n                run_time = summary['total-recipe-time']\n            if isinstance(run_time, basestring):  # Active recipes don't have a run-duration\n                s += 'Run duration:\\t%s (H:M:S)\\n' % run_time.split('.')[0]\n            else:\n                s += 'Run duration:\\t%s\\n' % 'Not available'\n\n        if serving and DKCloudCommandRunner.TESTRESULTS in serving and \\\n                isinstance(serving[DKCloudCommandRunner.TESTRESULTS], basestring):\n            s += '\\nTEST RESULTS'\n            s += serving[DKCloudCommandRunner.TESTRESULTS]\n        if serving and DKCloudCommandRunner.TIMINGRESULTS in serving and \\\n                isinstance(serving[DKCloudCommandRunner.TIMINGRESULTS], basestring):\n            s += '\\n\\nTIMING RESULTS\\n\\n'\n            s += serving[DKCloudCommandRunner.TIMINGRESULTS]\n        if serving and DKCloudCommandRunner.LOGS in serving and \\\n                isinstance(serving[DKCloudCommandRunner.LOGS], basestring):\n            s += '\\n\\nLOG\\n\\n'\n            s += DKCloudCommandRunner._decompress(serving[DKCloudCommandRunner.LOGS])\n        if 'status' in pd and serving and DKCloudCommandRunner.SUMMARY in serving and \\\n                isinstance(serving[DKCloudCommandRunner.SUMMARY], dict):\n            s += '\\nSTEP STATUS\\n\\n'\n            summary = serving[DKCloudCommandRunner.SUMMARY]\n            # loop through the sorted keys\n            for key in sorted(summary):\n                value = summary[key]\n                if isinstance(value, dict):\n                    # node/step info is stored as a dictionary, print the node name (key) and status\n                    if 'status' in value:\n                        status = value['status']\n                    else:\n                        status = 'unknown'\n                    s += '%s\\t%s\\n' % (key, status)\n\n        if serving and 'runstatus' in pd:\n            s += serving['status']\n\n        if serving and 'disp_order_id' in pd and DKCloudCommandRunner.ORDER_ID in serving:\n            s += serving[DKCloudCommandRunner.ORDER_ID]\n\n        if serving and 'disp_order_run_id' in pd and DKCloudCommandRunner.ORDER_RUN_ID in serving:\n            s += serving[DKCloudCommandRunner.ORDER_RUN_ID]\n\n        rc.set_message(s)\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_one_end(path):\n        s = path.rsplit('/', 1)\n        if len(s) == 1:\n            return s[0], ''\n        else:\n            return tuple(s)", "response": "Utility function for splitting off the very end part of a path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the config directory for the current platform.", "response": "def get_system_config_directory():\r\n    \"\"\"\r\n    Return platform specific config directory.\r\n    \"\"\"\r\n    if platform.system().lower() == 'windows':\r\n        _cfg_directory = Path(os.getenv('APPDATA') or '~')\r\n    elif platform.system().lower() == 'darwin':\r\n        _cfg_directory = Path('~', 'Library', 'Preferences')\r\n    else:\r\n        _cfg_directory = Path(os.getenv('XDG_CONFIG_HOME') or '~/.config')\r\n\r\n    logger.debug('Fetching configt directory for {}.'\r\n                 .format(platform.system()))\r\n    return _cfg_directory.joinpath(Path('mayalauncher/.config'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind valid application version from given path object and return a mapping of version executable.", "response": "def get_version_exec_mapping_from_path(path):\r\n    \"\"\"\r\n    Find valid application version from given path object and return\r\n    a mapping of version, executable.\r\n    \"\"\"\r\n    version_executable = {}\r\n    logger.debug('Getting exes from path: {}'.format(path))\r\n\r\n    for sub_dir in path.iterdir():\r\n        if not sub_dir.name.startswith(APPLICATION_NAME):\r\n            continue\r\n\r\n        release = sub_dir.name.split(APPLICATION_NAME)[-1]\r\n        executable = Path(sub_dir, 'bin').glob('maya.exe').next()\r\n        version_executable[release] = str(executable)\r\n    logger.debug('Found exes for: {}'.format(version_executable.keys()))\r\n    return version_executable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding maya versions from the system PATH if exists else try looking for custom executable paths from config file.", "response": "def find_applications_on_system():\r\n    \"\"\"\r\n    Collect maya version from Autodesk PATH if exists, else try looking\r\n    for custom executable paths from config file.\r\n    \"\"\"\r\n    # First we collect maya versions from the Autodesk folder we presume\r\n    # is addeed to the system environment \"PATH\"\r\n    path_env = os.getenv('PATH').split(os.pathsep)\r\n    versions = {}\r\n    for each in path_env:\r\n        path = Path(os.path.expandvars(each))\r\n        if not path.exists():\r\n            continue\r\n        if path.name.endswith(DEVELOPER_NAME):\r\n            if not path.exists():\r\n                continue\r\n            versions.update(get_version_exec_mapping_from_path(path))\r\n    return versions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_config(config_file=get_system_config_directory()):\r\n    config = Config(config_file, allow_no_value=True)\r\n    application_versions = find_applications_on_system()\r\n\r\n    # Add found versions to config if they don't exist. Versions found\r\n    # in the config file takes precedence over versions found in PATH.\r\n    for item in application_versions.iteritems():\r\n        if not config.has_option(Config.EXECUTABLES, item[0]):\r\n            config.set(Config.EXECUTABLES, item[0], item[1])\r\n    return config", "response": "Build a config object from necessary elements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_environment_paths(config, env):\r\n    if env is None:\r\n        return config.get(Config.DEFAULTS, 'environment')\r\n\r\n    # Config option takes precedence over environment key.\r\n    if config.has_option(Config.ENVIRONMENTS, env):\r\n        env = config.get(Config.ENVIRONMENTS, env).replace(' ', '').split(';')\r\n    else:\r\n        env = os.getenv(env)\r\n        if env:\r\n            env = env.split(os.pathsep)\r\n    return [i for i in env if i]", "response": "Get environment paths from given environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_maya_environment(config, env=None, arg_paths=None):\r\n    maya_env = MayaEnvironment()\r\n    maya_env.exclude_pattern = config.get_list(Config.PATTERNS, 'exclude')\r\n    maya_env.icon_extensions = config.get_list(Config.PATTERNS, 'icon_ext')\r\n\r\n    env = get_environment_paths(config, env)\r\n    if not env and arg_paths is None:\r\n        return logger.info('Using maya factory environment setup.')\r\n\r\n    logger.debug('Launching with addon paths: {}'.format(arg_paths))\r\n    logger.debug('Launching with environment paths: {}'.format(env))\r\n\r\n    if arg_paths:\r\n        arg_paths = arg_paths.split(' ')\r\n    for directory in flatten_combine_lists(env, arg_paths or ''):\r\n        maya_env.traverse_path_for_valid_application_paths(directory)\r\n    return maya_env", "response": "Construct a new maya environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the default config file.", "response": "def _create_default_config_file(self):\r\n        \"\"\"\r\n        If config file does not exists create and set default values.\r\n        \"\"\"\r\n        logger.info('Initialize Maya launcher, creating config file...\\n')\r\n        self.add_section(self.DEFAULTS)\r\n        self.add_section(self.PATTERNS)\r\n        self.add_section(self.ENVIRONMENTS)\r\n        self.add_section(self.EXECUTABLES)\r\n        self.set(self.DEFAULTS, 'executable', None)\r\n        self.set(self.DEFAULTS, 'environment', None)\r\n        self.set(self.PATTERNS, 'exclude', ', '.join(self.EXLUDE_PATTERNS))\r\n        self.set(self.PATTERNS, 'icon_ext', ', '.join(self.ICON_EXTENSIONS))\r\n\r\n        self.config_file.parent.mkdir(exist_ok=True)\r\n        self.config_file.touch()\r\n        with self.config_file.open('wb') as f:\r\n            self.write(f)\r\n\r\n        # If this function is run inform the user that a new file has been\r\n        # created.\r\n        sys.exit('Maya launcher has successfully created config file at:\\n'\r\n                 ' \"{}\"'.format(str(self.config_file)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting string value to list object.", "response": "def get_list(self, section, option):\r\n        \"\"\"\r\n        Convert string value to list object.\r\n        \"\"\"\r\n        if self.has_option(section, option):\r\n            return self.get(section, option).replace(' ', '').split(',')\r\n        else:\r\n            raise KeyError('{} with {} does not exist.'.format(section,\r\n                                                               option))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edit(self):\r\n        if platform.system().lower() == 'windows':\r\n            os.startfile(str(self.config_file))\r\n        else:\r\n            if platform.system().lower() == 'darwin':\r\n                call = 'open'\r\n            else:\r\n                call = 'xdg-open'\r\n            subprocess.call([call, self.config_file])", "response": "Edit file with default os application."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_excluded(self, path, exclude=None):\r\n        for pattern in (exclude or self.exclude_pattern):\r\n            if path.match(pattern):\r\n                return True\r\n        else:\r\n            return False", "response": "Return True if path is in exclude pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of directories with image extensions in given directory and optionally a list of extensions.", "response": "def get_directories_with_extensions(self, start, extensions=None):\r\n        \"\"\"\r\n        Look for directories with image extensions in given directory and\r\n        return a list with found dirs.\r\n\r\n        .. note:: In deep file structures this might get pretty slow.\r\n        \"\"\"\r\n        return set([p.parent for ext in extensions for p in start.rglob(ext)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a path find the directories that belong to it and append it.", "response": "def put_path(self, path):\r\n        \"\"\"\r\n        Given path identify in which environment the path belong to and\r\n        append it.\r\n        \"\"\"\r\n        if self.is_package(path):\r\n            logger.debug('PYTHON PACKAGE: {}'.format(path))\r\n            self.python_paths.append(path.parent)\r\n            site.addsitedir(str(path.parent))\r\n            xbmdirs = self.get_directories_with_extensions(\r\n                path,\r\n                self.icon_extensions,\r\n            )\r\n            self.xbmlang_paths.extend(xbmdirs)\r\n            return\r\n\r\n        if self.has_next(path.glob('*.' + self.MEL)):\r\n            logger.debug('MEL: {}'.format(str(path)))\r\n            self.script_paths.append(path)\r\n\r\n        if self.has_next(path.glob('*.' + self.PYTHON)):\r\n            logger.debug('PYTHONPATH: {}'.format(str(path)))\r\n            self.python_paths.append(path)\r\n            site.addsitedir(str(path))\r\n\r\n        if self.PLUGIN in list(path.iterdir()):\r\n            logger.debug('PLUG-IN: {}'.format(str(path)))\r\n            self.plug_in_paths.append(path)\r\n\r\n        for ext in self.icon_extensions:\r\n            if self.has_next(path.glob('*.' + ext)):\r\n                logger.debug('XBM: {}.'.format(str(path)))\r\n                self.xbmlang_paths.append(path)\r\n                break"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traverse_path_for_valid_application_paths(self, top_path):\r\n        self.put_path(Path(top_path))\r\n        for p in self._walk(top_path):\r\n            self.put_path(p)", "response": "Traverse the given path and put all the application paths that are not in the exclude list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_correct_host(self):\r\n        if self._host == 'knoema.com':\r\n            return\r\n        url = self._get_url('/api/1.0/frontend/tags')\r\n        req = urllib.request.Request(url)\r\n        try:\r\n            resp = urllib.request.urlopen(req)\r\n        except:\r\n            raise ValueError('The specified host {} does not exist'.format(self._host))", "response": "Checks whether the host is correctly set and whether it can configure the connection to this host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dataset(self, datasetid):\r\n\r\n        path = '/api/1.0/meta/dataset/{}'\r\n        return self._api_get(definition.Dataset, path.format(datasetid))", "response": "The method is getting information about a dataset by it s id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dimension(self, dataset, dimension):\r\n\r\n        path = '/api/1.0/meta/dataset/{}/dimension/{}'\r\n        return self._api_get(definition.Dimension, path.format(dataset, dimension))", "response": "The method is getting information about a dimension with items"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data_raw(self, request):\r\n        path = '/api/1.0/data/raw/'\r\n        res = self._api_post(definition.RawDataResponse, path, request)\r\n        token = res.continuation_token\r\n        while token is not None:\r\n           res2 = self.get_data_raw_with_token(token)\r\n           res.series += res2.series\r\n           token = res2.continuation_token \r\n        return res", "response": "The method is getting data by raw request"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mnemonics (self, mnemonics):\r\n        path = '/api/1.0/data/mnemonics?mnemonics={0}'\r\n        return self._api_get(definition.MnemonicsResponseList, path.format(mnemonics))", "response": "The method get series by mnemonics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload_file(self, file):\r\n\r\n        url = self._get_url('/api/1.0/upload/post')\r\n\r\n        fcontent = FileContent(file)\r\n        binary_data = fcontent.get_binary()\r\n\r\n        headers = self._get_request_headers()\r\n        req = urllib.request.Request(url, binary_data, headers)\r\n        req.add_header('Content-type', fcontent.get_content_type())\r\n        req.add_header('Content-length', len(binary_data))\r\n        resp = urllib.request.urlopen(req)   \r\n\r\n        return definition.UploadPostResponse(_response_to_json(resp))", "response": "The method is uploading a file to the remote server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload(self, file_path, dataset=None, public=False):\r\n\r\n        upload_status = self.upload_file(file_path)\r\n        err_msg = 'Dataset has not been uploaded to the remote host'\r\n        if not upload_status.successful:\r\n            msg = '{}, because of the following error: {}'.format(err_msg, upload_status.error)\r\n            raise ValueError(msg)\r\n\r\n        err_msg = 'File has not been verified'\r\n        upload_ver_status = self.upload_verify(upload_status.properties.location, dataset)\r\n        if not upload_ver_status.successful:\r\n            ver_err = '\\r\\n'.join(upload_ver_status.errors)\r\n            msg = '{}, because of the following error(s): {}'.format(err_msg, ver_err)\r\n            raise ValueError(msg)\r\n\r\n        ds_upload = definition.DatasetUpload(upload_ver_status, upload_status, dataset, public)\r\n        ds_upload_submit_result = self.upload_submit(ds_upload)\r\n        err_msg = 'Dataset has not been saved to the database'\r\n        if ds_upload_submit_result.status == 'failed':\r\n            ver_err = '\\r\\n'.join(ds_upload_submit_result.errors)\r\n            msg = '{}, because of the following error(s): {}'.format(err_msg, ver_err)\r\n            raise ValueError(msg)\r\n\r\n        ds_upload_result = None\r\n        while True:\r\n            ds_upload_result = self.upload_status(ds_upload_submit_result.submit_id)\r\n            if ds_upload_result.status == 'pending' or ds_upload_result.status == 'processing':\r\n                time.sleep(5)\r\n            else:\r\n                break\r\n\r\n        if ds_upload_result.status != 'successful':\r\n            ver_err = '\\r\\n'.join(ds_upload_result.errors)\r\n            msg = '{}, because of the following error(s): {}'.format(err_msg, ver_err)\r\n            raise ValueError(msg)\r\n\r\n        return ds_upload_result.dataset", "response": "Use this function to upload data to Knoema dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_binary(self):\r\n\r\n        content_disp = 'Content-Disposition: form-data; name=\"file\"; filename=\"{}\"'\r\n\r\n        stream = io.BytesIO()\r\n        stream.write(_string_to_binary('--{}'.format(self.boundary)))\r\n        stream.write(_crlf())\r\n        stream.write(_string_to_binary(content_disp.format(self.file_name)))\r\n        stream.write(_crlf())\r\n        stream.write(_crlf())\r\n        stream.write(self.body)\r\n        stream.write(_crlf())\r\n        stream.write(_string_to_binary('--{}--'.format(self.boundary)))\r\n        stream.write(_crlf())\r\n\r\n        return stream.getvalue()", "response": "Return a binary buffer containing the file content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a unique cache stream name for this QueryCache.", "response": "def _scratch_stream_name(self):\n    \"\"\"\n    A unique cache stream name for this QueryCache.\n\n    Hashes the necessary facts about this QueryCache to generate a\n    unique cache stream name.  Different `query_function`\n    implementations at different `bucket_width` values will be cached\n    to different streams.\n\n    TODO(marcua): This approach won't work for dynamically-generated\n    functions.  We will want to either:\n      1) Hash the function closure/containing scope.\n      2) Ditch this approach and rely on the caller to tell us all the\n         information that makes this function unique.\n    \"\"\"\n    query_details = [\n      str(QueryCache.QUERY_CACHE_VERSION),\n      str(self._bucket_width),\n      binascii.b2a_hex(marshal.dumps(self._query_function.func_code)),\n      str(self._query_function_args),\n      str(self._query_function_kwargs),\n    ]\n    return hashlib.sha512('$'.join(query_details)).hexdigest()[:20]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the time that the event bucket is at.", "response": "def _bucket_time(self, event_time):\n    \"\"\"\n    The seconds since epoch that represent a computed bucket.\n\n    An event bucket is the time of the earliest possible event for\n    that `bucket_width`.  Example: if `bucket_width =\n    timedelta(minutes=10)`, bucket times will be the number of seconds\n    since epoch at 12:00, 12:10, ...  on each day.\n    \"\"\"\n    event_time = kronos_time_to_epoch_time(event_time)\n    return event_time - (event_time % self._bucket_width)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bucket_events(self, event_iterable):\n\n    current_bucket_time = None\n    current_bucket_events = None\n    for event in event_iterable:\n      event_bucket_time = self._bucket_time(event[TIMESTAMP_FIELD])\n      if current_bucket_time is None or current_bucket_time < event_bucket_time:\n        if current_bucket_events is not None:\n          yield current_bucket_events\n        current_bucket_time = event_bucket_time\n        current_bucket_events = []\n      current_bucket_events.append(event)\n    if current_bucket_events is not None and current_bucket_events != []:\n      yield current_bucket_events", "response": "Convert an iterable of events into an iterable of lists of events\n    per bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields the cached results for the given start and end times.", "response": "def _cached_results(self, start_time, end_time):\n    \"\"\"\n    Retrieves cached results for any bucket that has a single cache entry.\n\n    If a bucket has two cache entries, there is a chance that two\n    different writers previously computed and cached a result since\n    Kronos has no transaction semantics.  While it might be safe to\n    return one of the cached results if there are multiple, we\n    currently do the safe thing and pretend we have no previously\n    computed data for this bucket.\n    \"\"\"\n    cached_buckets = self._bucket_events(\n      self._client.get(self._scratch_stream, start_time, end_time,\n                       namespace=self._scratch_namespace))\n    for bucket_events in cached_buckets:\n      # If we have multiple cache entries for the same bucket, pretend\n      # we have no results for that bucket.\n      if len(bucket_events) == 1:\n        first_result = bucket_events[0]\n        yield (kronos_time_to_epoch_time(first_result[TIMESTAMP_FIELD]),\n               first_result[QueryCache.CACHE_KEY])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_and_cache_missing_buckets(self, start_time, end_time,\n                                        untrusted_time, force_recompute=False):\n    \"\"\"\n    Return the results for `query_function` on every `bucket_width`\n    time period between `start_time` and `end_time`.  Look for\n    previously cached results to avoid recomputation.  For any buckets\n    where all events would have occurred before `untrusted_time`,\n    cache the results.\n\n    :param start_time: A datetime for the beginning of the range,\n    aligned with `bucket_width`.\n    :param end_time: A datetime for the end of the range, aligned with\n    `bucket_width`.\n    :param untrusted_time: A datetime after which to not trust that\n    computed data is stable.  Any buckets that overlap with or follow\n    this untrusted_time will not be cached.\n    :param force_recompute: A boolean that, if True, will force\n    recompute and recaching of even previously cached data.\n    \"\"\"\n    if untrusted_time and not untrusted_time.tzinfo:\n      untrusted_time = untrusted_time.replace(tzinfo=tzutc())\n\n    events = self._compute_buckets(start_time, end_time, compute_missing=True,\n                                   cache=True, untrusted_time=untrusted_time,\n                                   force_recompute=force_recompute)\n\n    for event in events:\n      yield event", "response": "Compute the missing buckets for a given time period between start_time and end_time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the results for query_function on every bucket_width between start_time and end_time.", "response": "def retrieve_interval(self, start_time, end_time, compute_missing=False):\n    \"\"\"\n    Return the results for `query_function` on every `bucket_width`\n    time period between `start_time` and `end_time`.  Look for\n    previously cached results to avoid recomputation.\n\n    :param start_time: A datetime for the beginning of the range,\n    aligned with `bucket_width`.\n    :param end_time: A datetime for the end of the range, aligned with\n    `bucket_width`.\n    :param compute_missing: A boolean that, if True, will compute any\n    non-cached results.\n    \"\"\"\n    events = self._compute_buckets(start_time, end_time,\n                                   compute_missing=compute_missing)\n\n    for event in events:\n      yield event"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef token_protected_endpoint(function):\n  @wraps(function)\n  def decorated(*args, **kwargs):\n    auth_token = request.form.get('auth_token')\n    if not auth_token:\n      return json.dumps({\n        'status': 'fail',\n        'reason': 'You must provide an auth_token',\n      })\n\n    data = dict(request.form)\n    del data['auth_token']\n    correct_token = create_token(current_app.config['SECRET_KEY'], data)\n\n    if _compare_digest(auth_token, correct_token):\n      return function(*args, **kwargs)\n\n    else:\n      return json.dumps({\n        'status': 'fail',\n        'reason': 'Incorrect auth_token',\n      })\n\n  return decorated", "response": "Decorator that validates auth_token in POST to access\n\n"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Trace of the mz of the molecular ion.", "response": "def molmz(df, noise=10000):\n    \"\"\"\n    The mz of the molecular ion.\n    \"\"\"\n    d = ((df.values > noise) * df.columns).max(axis=1)\n    return Trace(d, df.index, name='molmz')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DataFrame with the abundances of ions which are minus below the molecular ion.", "response": "def mzminus(df, minus=0, noise=10000):\n    \"\"\"\n    The abundances of ions which are minus below the molecular ion.\n    \"\"\"\n    mol_ions = ((df.values > noise) * df.columns).max(axis=1) - minus\n    mol_ions[np.abs(mol_ions) < 0] = 0\n    d = np.abs(np.ones(df.shape) * df.columns -\n               (mol_ions[np.newaxis].T * np.ones(df.shape))) < 1\n    d = (df.values * d).sum(axis=1)\n    return Trace(d, df.index, name='m-' + str(minus))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the mz of the most abundant ion.", "response": "def basemz(df):\n    \"\"\"\n    The mz of the most abundant ion.\n    \"\"\"\n    # returns the\n    d = np.array(df.columns)[df.values.argmax(axis=1)]\n    return Trace(d, df.index, name='basemz')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coda(df, window, level):\n    # pull out the data\n    d = df.values\n\n    # smooth the data and standardize it\n    smooth_data = movingaverage(d, df.index, window)[0]\n    stand_data = (smooth_data - smooth_data.mean()) / smooth_data.std()\n\n    # scale the data to have unit length\n    scale_data = d / np.sqrt(np.sum(d ** 2, axis=0))\n\n    # calculate the \"mass chromatographic quality\" (MCQ) index\n    mcq = np.sum(stand_data * scale_data, axis=0) / np.sqrt(d.shape[0] - 1)\n\n    # filter out ions with an mcq below level\n    good_ions = [i for i, q in zip(df.columns, mcq) if q >= level]\n    return good_ions", "response": "CODA processing from Windig Phalp & Payne 1996 Anal Chem\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tfclasses():\n    # automatically find any subclasses of TraceFile in the same\n    # directory as me\n    classes = {}\n    mydir = op.dirname(op.abspath(inspect.getfile(get_mimetype)))\n    tfcls = {\"<class 'aston.tracefile.TraceFile'>\",\n             \"<class 'aston.tracefile.ScanListFile'>\"}\n    for filename in glob(op.join(mydir, '*.py')):\n        name = op.splitext(op.basename(filename))[0]\n        module = import_module('aston.tracefile.' + name)\n        for clsname in dir(module):\n            cls = getattr(module, clsname)\n            if hasattr(cls, '__base__'):\n                if str(cls.__base__) in tfcls:\n                    classes[cls.mime] = cls\n    return classes", "response": "A mapping of mimetypes to every class for reading data files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nguess the initc for a given set of peaks.", "response": "def guess_initc(ts, f, rts=[]):\n    \"\"\"\n    ts - An AstonSeries that's being fitted with peaks\n    f - The functional form of the peaks (e.g. gaussian)\n    rts - peak maxima to fit; each number corresponds to one peak\n    \"\"\"\n    def find_side(y, loc=None):\n        if loc is None:\n            loc = y.argmax()\n        ddy = np.diff(np.diff(y))\n        lft_loc, rgt_loc = loc - 2, loc + 1\n        while rgt_loc >= 0 and rgt_loc < len(ddy):\n            if ddy[rgt_loc] < ddy[rgt_loc - 1]:\n                break\n            rgt_loc += 1\n        while lft_loc >= 0 and lft_loc < len(ddy):\n            if ddy[lft_loc] < ddy[lft_loc + 1]:\n                break\n            lft_loc -= 1\n        return lft_loc + 1, rgt_loc + 1\n\n    # weight_mom = lambda m, a, w: \\\n    #   np.sum(w * (a - np.sum(w * a) / np.sum(w)) ** m) / np.sum(w)\n    # sig = np.sqrt(weight_mom(2, ts.index, ts.values))  # sigma\n    # peak_params['s'] = weight_mom(3, ts.index, ts.values) / sig ** 3\n    # peak_params['e'] = weight_mom(4, ts.index, ts.values) / sig ** 4 - 3\n    # TODO: better method of calculation of these?\n    all_params = []\n    for rt in rts:\n        peak_params = {'x': rt}  # ts.index[ts.values.argmax()]\n        top_idx = np.abs(ts.index - rt).argmin()\n        side_idx = find_side(ts.values, top_idx)\n        peak_params['h'] = ts.values[top_idx]\n        # - min(ts.y[side_idx[0]], ts.y[side_idx[1]])\n        peak_params['w'] = ts.index[side_idx[1]] - ts.index[side_idx[0]]\n        peak_params['s'] = 1.1\n        peak_params['e'] = 1.\n        peak_params['a'] = 1.\n        all_params.append(peak_params)\n    return all_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit a set of analytical functions to a set of analytical data structures.", "response": "def fit(ts, fs=[], all_params=[], fit_vars=None,\n        alg='leastsq', make_bounded=True):\n    \"\"\"\n    Use a minimization algorithm to fit a AstonSeries with\n    analytical functions.\n    \"\"\"\n    if fit_vars is None:\n        fit_vars = [f._peakargs for f in fs]\n    initc = [min(ts.values)]\n    for f, peak_params, to_fit in zip(fs, all_params, fit_vars):\n        if 'v' in to_fit:\n            to_fit.remove('v')\n\n        if make_bounded and hasattr(f, '_pbounds'):\n            new_v = _to_unbnd_p({i: peak_params[i] for i in to_fit},\n                                f._pbounds)\n            initc += [new_v[i] for i in to_fit]\n        else:\n            initc += [peak_params[i] for i in to_fit]\n\n    def errfunc_lsq(fit_params, t, y, all_params):\n        # first value in fit_params is baseline\n        # fit_y = np.ones(len(t)) * fit_params[0]\n        fit_y = np.zeros(len(t))\n        param_i = 1\n        for f, peak_params, to_fit in zip(fs, all_params, fit_vars):\n            for k in to_fit:\n                peak_params[k] = fit_params[param_i]\n                param_i += 1\n            if make_bounded and hasattr(f, '_pbounds'):\n                fit_y += f(t, **_to_bound_p(peak_params, f._pbounds))\n            else:\n                fit_y += f(t, **peak_params)\n        return fit_y - y\n\n    def errfunc(p, t, y, all_params):\n        return np.sum(errfunc_lsq(p, t, y, all_params) ** 2)\n\n    if alg == 'simplex':\n        fit_p, _ = fmin(errfunc, initc, args=(ts.index, ts.values,\n                                              peak_params))\n#    elif alg == 'anneal':\n#        fit_p, _ = anneal(errfunc, initc, args=(ts.index, ts.values,\n#                                                peak_params))\n    elif alg == 'lbfgsb':\n        # TODO: use bounds param\n        fitp, _ = fmin_l_bfgs_b(errfunc, fit_p,\n                                args=(ts.index, ts.values, peak_params),\n                                approx_grad=True)\n    elif alg == 'leastsq':\n        fit_p, _ = leastsq(errfunc_lsq, initc, args=(ts.index, ts.values,\n                                                     all_params))\n    # else:\n    #     r = minimize(errfunc, initc, \\\n    #                  args=(ts.index, ts.values, all_params), \\\n    #                  jac=False, gtol=1e-2)\n    #     #if not r['success']:\n    #     #    print('Fail:' + str(f))\n    #     #    print(r)\n    #     #if np.nan in r['x']:  # not r['success']?\n    #     #    fit_p = initc\n    #     #else:\n    #     #    fit_p = r['x']\n\n    fit_pl = fit_p.tolist()\n    v = fit_pl.pop(0)  # noqa\n    fitted_params = []\n    for f, to_fit in zip(fs, fit_vars):\n        fit_p_dict = {v: fit_pl.pop(0) for v in to_fit}\n        # fit_p_dict['v'] = v\n        if make_bounded and hasattr(f, '_pbounds'):\n            fitted_params.append(_to_bound_p(fit_p_dict, f._pbounds))\n        else:\n            fitted_params.append(fit_p_dict)\n\n    # calculate r^2 of the fit\n    ss_err = errfunc(fit_p, ts.index, ts.values, fitted_params)\n    ss_tot = np.sum((ts.values - np.mean(ts.values)) ** 2)\n    r2 = 1 - ss_err / ss_tot\n    res = {'r^2': r2}\n\n    return fitted_params, res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_greenlet_async(func, *args, **kwargs):\n  global _GREENLET_EXECUTOR\n  if _GREENLET_EXECUTOR is None:\n    _GREENLET_EXECUTOR = GreenletExecutor(\n      num_greenlets=settings.node.greenlet_pool_size)\n  return _GREENLET_EXECUTOR.submit(func, *args, **kwargs)", "response": "Executes a function in a separate greenlet in the same process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a function in a separate process.", "response": "def execute_process_async(func, *args, **kwargs):\n  \"\"\"\n  Executes `func` in a separate process. Memory and other resources are not\n  available. This gives true concurrency at the cost of losing access to\n  these resources. `args` and `kwargs` are\n  \"\"\"\n  global _GIPC_EXECUTOR\n  if _GIPC_EXECUTOR is None:\n    _GIPC_EXECUTOR = GIPCExecutor(\n      num_procs=settings.node.gipc_pool_size,\n      num_greenlets=settings.node.greenlet_pool_size)\n  return _GIPC_EXECUTOR.submit(func, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait(results, num=None, timeout=None):\n  return AbstractExecutor.wait(results, num=num, timeout=timeout)", "response": "Wait for the results of async executions to become available or ready."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, mention, max_message_length):\n        message = []\n\n        def message_len():\n            return sum([len(w) + 1 for w in message])\n\n        while message_len() < max_message_length:\n            message.append(self.a_random_word(message[-1] if message else None))\n\n        return ' '.join(message[:-1])", "response": "Create a new message from a Markov chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive(self, event_type, signature, data_str):\n\n        if not self.validate_signature(signature, data_str):\n            raise HelpScoutSecurityException(\n                'The signature provided by this request was invalid.',\n            )\n\n        return HelpScoutWebHookEvent(\n            event_type=event_type,\n            record=json.loads(data_str),\n        )", "response": "Receive a web hook for the event and signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_signature(self, signature, data, encoding='utf8'):\n\n        if isinstance(data, string_types):\n            data = bytearray(data, encoding)\n        if isinstance(signature, string_types):\n            signature = bytearray(signature, encoding)\n\n        secret_key = bytearray(self.secret_key, 'utf8')\n        hashed = hmac.new(secret_key, data, sha1)\n        encoded = b64encode(hashed.digest())\n\n        return encoded.strip() == signature.strip()", "response": "Validate the signature for the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hook(name=None, priority=-1):\n\n    def _hook(hook_func):\n        return register_hook(name, hook_func=hook_func, priority=priority)\n\n    return _hook", "response": "Decorator for registering a hook function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train_doc2vec(paths, out='data/model.d2v', tokenizer=word_tokenize, sentences=False, **kwargs):\n    kwargs = {\n        'size': 400,\n        'window': 8,\n        'min_count': 2,\n        'workers': 8\n    }.update(kwargs)\n\n    n = 0\n    for path in paths:\n        print('Counting lines for {0}...'.format(path))\n        n += sum(1 for line in open(path, 'r'))\n    print('Processing {0} lines...'.format(n))\n\n    print('Training doc2vec model...')\n    m = Doc2Vec(_doc2vec_doc_stream(paths, n, tokenizer=tokenizer, sentences=sentences), **kwargs)\n\n    print('Saving...')\n    m.save(out)", "response": "Train a doc2vec model on a list of files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _doc2vec_doc_stream(paths, n, tokenizer=word_tokenize, sentences=True):\n    i = 0\n    p = Progress()\n    for path in paths:\n        with open(path, 'r') as f:\n            for line in f:\n                i += 1\n                p.print_progress(i/n)\n\n                # We do minimal pre-processing here so the model can learn\n                # punctuation\n                line = line.lower()\n\n                if sentences:\n                    for sent in sent_tokenize(line):\n                        tokens = tokenizer(sent)\n                        yield LabeledSentence(tokens, ['SENT_{}'.format(i)])\n                else:\n                    tokens = tokenizer(line)\n                    yield LabeledSentence(tokens, ['SENT_{}'.format(i)])", "response": "Generator to feed sentences to the dov2vec model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the info line from the log file.", "response": "def _parse_info(line):\n        \"\"\"\n        The output can be:\n        - [LaCrosseITPlusReader.10.1s (RFM12B f:0 r:17241)]\n        - [LaCrosseITPlusReader.10.1s (RFM12B f:0 t:10~3)]\n        \"\"\"\n        re_info = re.compile(\n            r'\\[(?P<name>\\w+).(?P<ver>.*) ' +\n            r'\\((?P<rfm1name>\\w+) (\\w+):(?P<rfm1freq>\\d+) ' +\n            r'(?P<rfm1mode>.*)\\)\\]')\n\n        info = {\n            'name': None,\n            'version': None,\n            'rfm1name': None,\n            'rfm1frequency': None,\n            'rfm1datarate': None,\n            'rfm1toggleinterval': None,\n            'rfm1togglemask': None,\n        }\n        match = re_info.match(line)\n        if match:\n            info['name'] = match.group('name')\n            info['version'] = match.group('ver')\n            info['rfm1name'] = match.group('rfm1name')\n            info['rfm1frequency'] = match.group('rfm1freq')\n            values = match.group('rfm1mode').split(':')\n            if values[0] == 'r':\n                info['rfm1datarate'] = values[1]\n            elif values[0] == 't':\n                toggle = values[1].split('~')\n                info['rfm1toggleinterval'] = toggle[0]\n                info['rfm1togglemask'] = toggle[1]\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_info(self):\n        re_info = re.compile(r'\\[.*\\]')\n\n        self._write_cmd('v')\n        while True:\n            line = self._serial.readline()\n            try:\n                line = line.encode().decode('utf-8')\n            except AttributeError:\n                line = line.decode('utf-8')\n\n            match = re_info.match(line)\n            if match:\n                return self._parse_info(line)", "response": "Get current configuration info from v command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_frequency(self, frequency, rfm=1):\n        cmds = {1: 'f', 2: 'F'}\n        self._write_cmd('{}{}'.format(frequency, cmds[rfm]))", "response": "Set the frequency in kHz."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_datarate(self, rate, rfm=1):\n        cmds = {1: 'r', 2: 'R'}\n        self._write_cmd('{}{}'.format(rate, cmds[rfm]))", "response": "Set the baud rate for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_toggle_interval(self, interval, rfm=1):\n        cmds = {1: 't', 2: 'T'}\n        self._write_cmd('{}{}'.format(interval, cmds[rfm]))", "response": "Set the toggle interval."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the baudrate mask for the current locale.", "response": "def set_toggle_mask(self, mode_mask, rfm=1):\n        \"\"\"Set toggle baudrate mask.\n\n        The baudrate mask values are:\n          1: 17.241 kbps\n          2 : 9.579 kbps\n          4 : 8.842 kbps\n        These values can be or'ed.\n        \"\"\"\n        cmds = {1: 'm', 2: 'M'}\n        self._write_cmd('{}{}'.format(mode_mask, cmds[rfm]))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _refresh(self):\n\n        while not self._stopevent.isSet():\n            line = self._serial.readline()\n            #this is for python2/python3 compatibility. Is there a better way?\n            try:\n                line = line.encode().decode('utf-8')\n            except AttributeError:\n                line = line.decode('utf-8')\n\n            if LaCrosseSensor.re_reading.match(line):\n                sensor = LaCrosseSensor(line)\n                self.sensors[sensor.sensorid] = sensor\n\n                if self._callback:\n                    self._callback(sensor, self._callback_data)\n\n                if sensor.sensorid in self._registry:\n                    for cbs in self._registry[sensor.sensorid]:\n                        cbs[0](sensor, cbs[1])", "response": "Refreshes the internal state of the thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_callback(self, sensorid, callback, user_data=None):\n        if sensorid not in self._registry:\n            self._registry[sensorid] = list()\n        self._registry[sensorid].append((callback, user_data))", "response": "Register a callback for the specified sensor id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a callback for all sensors.", "response": "def register_all(self, callback, user_data=None):\n        \"\"\"Register a callback for all sensors.\"\"\"\n        self._callback = callback\n        self._callback_data = user_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef not_completed(f):\n\n    @wraps(f)\n    def check_if_complete(cls, *args, **kwargs):\n        if cls.is_complete:\n            raise FMBaseError('Transfer already completed.')\n\n        return f(cls, *args, **kwargs)\n\n    return check_if_complete", "response": "Decorator to check if user is logged in."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_recipients(self, to):\n\n        if to is None:\n            return None\n\n        if isinstance(to, list):\n            recipients = []\n\n            for recipient in to:\n                if isinstance(recipient, dict):\n                    if 'contactgroupname' in recipient:\n                        recipients.append(recipient['contactgroupname'])\n\n                    else:\n                        recipients.append(recipient.get('email'))\n\n                else:\n                    recipients.append(recipient)\n\n        elif isinstance(to, basestring):\n            if ',' in to:\n                recipients = to.strip().split(',')\n\n            else:\n                recipients = [to]\n\n        return ', '.join(recipients)", "response": "Make sure we have a comma separated list of recipients"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_files(self, files):\n\n        if isinstance(files, basestring):\n            files = [files]\n\n        zip_file = None\n        if self.zip_:\n            zip_filename = self._get_zip_filename()\n            zip_file = ZipFile(zip_filename, 'w')\n\n        for filename in files:\n            if os.path.isdir(filename):\n                for dirname, subdirs, filelist in os.walk(filename):\n                    if dirname:\n                        if self.zip_:\n                            zip_file.write(dirname)\n\n                    for fname in filelist:\n                        filepath = os.path.join(dirname, fname)\n                        if self.zip_:\n                            zip_file.write(filepath)\n\n                        else:\n                            fmfile = self.get_file_specs(filepath,\n                                                         keep_folders=True)\n                            if fmfile['totalsize'] > 0:\n                                self._files.append(fmfile)\n\n            else:\n                if self.zip_:\n                    zip_file.write(filename)\n\n                else:\n                    fmfile = self.get_file_specs(filename)\n                    self._files.append(fmfile)\n\n        if self.zip_:\n            zip_file.close()\n            filename = zip_filename\n            fmfile = self.get_file_specs(filename)\n            self._files.append(fmfile)", "response": "Adds files and / or folders to the transfer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngathers information on files needed for valid transfer.", "response": "def get_file_specs(self, filepath, keep_folders=False):\n        \"\"\"Gather information on files needed for valid transfer.\n\n        :param filepath: Path to file in question\n        :param keep_folders: Whether or not to maintain folder structure\n        :type keep_folders: bool\n        :type filepath: str, unicode\n        :rtype: ``dict``\n        \"\"\"\n\n        path, filename = os.path.split(filepath)\n\n        fileid = str(uuid4()).replace('-', '')\n\n        if self.checksum:\n            with open(filepath, 'rb') as f:\n                md5hash = md5(f.read()).digest().encode('base64')[:-1]\n        else:\n            md5hash = None\n\n        specs = {\n            'transferid': self.transfer_id,\n            'transferkey': self.transfer_info['transferkey'],\n            'fileid': fileid,\n            'filepath': filepath,\n            'thefilename': keep_folders and filepath or filename,\n            'totalsize': os.path.getsize(filepath),\n            'md5': md5hash,\n            'content-type': guess_type(filepath)[0]\n            }\n\n        return specs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets information on files in a filemail.", "response": "def get_files(self):\n        \"\"\"Get information on file in transfer from Filemail.\n\n        :rtype: ``list`` of ``dict`` objects with info on files\n        \"\"\"\n\n        method, url = get_URL('get')\n        payload = {\n            'apikey': self.session.cookies.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'transferid': self.transfer_id,\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            transfer_data = res.json()['transfer']\n            files = transfer_data['files']\n\n            for file_data in files:\n                self._files.append(file_data)\n\n            return self.files\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a filename for zip file when Transfer. compress isTrue", "response": "def _get_zip_filename(self):\n        \"\"\"Create a filename for zip file when :class:Transfer.compress is\n        set to ``True``\n\n        :rtype: str\n        \"\"\"\n\n        date = datetime.datetime.now().strftime('%Y_%m_%d-%H%M%S')\n        zip_file = 'filemail_transfer_{date}.zip'.format(date=date)\n\n        return zip_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, auto_complete=True, callback=None):\n\n        tot = len(self.files)\n        url = self.transfer_info['transferurl']\n\n        for index, fmfile in enumerate(self.files):\n\n            msg = 'Uploading: \"{filename}\" ({cur}/{tot})'\n            logger.debug(\n                msg.format(\n                    filename=fmfile['thefilename'],\n                    cur=index + 1,\n                    tot=tot)\n                )\n\n            with open(fmfile['filepath'], 'rb') as file_obj:\n                fields = {\n                    fmfile['thefilename']: (\n                        'filename',\n                        file_obj,\n                        fmfile['content-type']\n                        )\n                    }\n\n                def pg_callback(monitor):\n                    if pm.COMMANDLINE:\n                        bar.show(monitor.bytes_read)\n\n                    elif callback is not None:\n                        callback(fmfile['totalsize'], monitor.bytes_read)\n\n                m_encoder = encoder.MultipartEncoder(fields=fields)\n                monitor = encoder.MultipartEncoderMonitor(m_encoder,\n                                                          pg_callback\n                                                          )\n                label = fmfile['thefilename'] + ': '\n\n                if pm.COMMANDLINE:\n                    bar = ProgressBar(label=label,\n                                      expected_size=fmfile['totalsize'])\n\n                headers = {'Content-Type': m_encoder.content_type}\n\n                res = self.session.post(url,\n                                        params=fmfile,\n                                        data=monitor,\n                                        headers=headers)\n\n                if res.status_code != 200:\n                    hellraiser(res)\n\n        #logger.info('\\r')\n        if auto_complete:\n            return self.complete()\n\n        return res", "response": "Uploads files and sends them to recipient."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the transfer is complete False otherwise.", "response": "def is_complete(self):\n        \"\"\":rtype: ``bool`` ``True`` if transfer is complete\"\"\"\n\n        if 'status' in self.transfer_info:\n            self._complete = self.transfer_info['status'] == 'STATUS_COMPLETE'\n\n        return self._complete"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward(self, to):\n\n        method, url = get_URL('forward')\n\n        payload = {\n            'apikey': self.session.cookies.get('apikey'),\n            'transferid': self.transfer_id,\n            'transferkey': self.transfer_info.get('transferkey'),\n            'to': self._parse_recipients(to)\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)", "response": "Forward prior transfer to new recipient."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshare transfer with new message to new people.", "response": "def share(self, to, sender=None, message=None):\n        \"\"\"Share transfer with new message to new people.\n\n        :param to: receiver(s)\n        :param sender: Alternate email address as sender\n        :param message: Meggase to new recipients\n        :type to: ``list`` or ``str`` or ``unicode``\n        :type sender: ``str`` or ``unicode``\n        :type message: ``str`` or ``unicode``\n        :rtyep: ``bool``\n        \"\"\"\n\n        method, url = get_URL('share')\n\n        payload = {\n            'apikey': self.session.cookies.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'transferid': self.transfer_id,\n            'to': self._parse_recipients(to),\n            'from': sender or self.fm_user.username,\n            'message': message or ''\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cancel(self):\n\n        method, url = get_URL('cancel')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'transferid': self.transfer_id,\n            'transferkey': self.transfer_info.get('transferkey')\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            self._complete = True\n            return True\n\n        hellraiser(res)", "response": "Cancel the current transfer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rename_file(self, fmfile, newname):\n\n        if not isinstance(fmfile, dict):\n            raise FMBaseError('fmfile must be a <dict>')\n\n        method, url = get_URL('file_rename')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'fileid': fmfile.get('fileid'),\n            'filename': newname\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n        if res.status_code == 200:\n            self._complete = True\n            return True\n\n        hellraiser(res)", "response": "Rename file in transfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_file(self, fmfile):\n\n        if not isinstance(fmfile, dict):\n            raise FMFileError('fmfile must be a <dict>')\n\n        method, url = get_URL('file_delete')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'fileid': fmfile.get('fileid')\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            self._complete = True\n            return True\n\n        hellraiser(res)", "response": "Delete a file from a filemail containing fileid\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self,\n               message=None,\n               subject=None,\n               days=None,\n               downloads=None,\n               notify=None):\n        \"\"\"Update properties for a transfer.\n\n        :param message: updated message to recipient(s)\n        :param subject: updated subject for trasfer\n        :param days: updated amount of days transfer is available\n        :param downloads: update amount of downloads allowed for transfer\n        :param notify: update whether to notifiy on downloads or not\n        :type message: ``str`` or ``unicode``\n        :type subject: ``str`` or ``unicode``\n        :type days: ``int``\n        :type downloads: ``int``\n        :type notify: ``bool``\n        :rtype: ``bool``\n        \"\"\"\n\n        method, url = get_URL('update')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'transferid': self.transfer_id,\n            }\n\n        data = {\n            'message': message or self.transfer_info.get('message'),\n            'message': subject or self.transfer_info.get('subject'),\n            'days': days or self.transfer_info.get('days'),\n            'downloads': downloads or self.transfer_info.get('downloads'),\n            'notify': notify or self.transfer_info.get('notify')\n            }\n\n        payload.update(data)\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code:\n            self.transfer_info.update(data)\n            return True\n\n        hellraiser(res)", "response": "Update properties for a single trasfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(self,\n                 files=None,\n                 destination=None,\n                 overwrite=False,\n                 callback=None):\n\n        \"\"\"Download file or files.\n\n        :param files: file or files to download\n        :param destination: destination path (defaults to users home directory)\n        :param overwrite: replace existing files?\n        :param callback: callback function that will receive total file size\n         and written bytes as arguments\n        :type files: ``list`` of ``dict`` with file data from filemail\n        :type destination: ``str`` or ``unicode``\n        :type overwrite: ``bool``\n        :type callback: ``func``\n        \"\"\"\n\n        if files is None:\n            files = self.files\n\n        elif not isinstance(files, list):\n            files = [files]\n\n        if destination is None:\n            destination = os.path.expanduser('~')\n\n        for f in files:\n            if not isinstance(f, dict):\n                raise FMBaseError('File must be a <dict> with file data')\n\n            self._download(f, destination, overwrite, callback)", "response": "Download file or files from filemail."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _download(self, fmfile, destination, overwrite, callback):\n\n        fullpath = os.path.join(destination, fmfile.get('filename'))\n        path, filename = os.path.split(fullpath)\n\n        if os.path.exists(fullpath):\n            msg = 'Skipping existing file: {filename}'\n            logger.info(msg.format(filename=filename))\n            return\n\n        filesize = fmfile.get('filesize')\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        url = fmfile.get('downloadurl')\n        stream = self.session.get(url, stream=True)\n\n        def pg_callback(bytes_written):\n            if pm.COMMANDLINE:\n                bar.show(bytes_written)\n\n            elif callback is not None:\n                callback(filesize, bytes_written)\n\n        if pm.COMMANDLINE:\n            label = fmfile['filename'] + ': '\n            bar = ProgressBar(label=label, expected_size=filesize)\n\n        bytes_written = 0\n        with open(fullpath, 'wb') as f:\n            for chunk in stream.iter_content(chunk_size=1024 * 1024):\n                if not chunk:\n                    break\n\n                f.write(chunk)\n                bytes_written += len(chunk)\n\n                # Callback\n                pg_callback(bytes_written)", "response": "The actual downloader for the filemail."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompresses files on the server side after transfer complete and make zip available for download.", "response": "def compress(self):\n        \"\"\"Compress files on the server side after transfer complete\n         and make zip available for download.\n\n        :rtype: ``bool``\n        \"\"\"\n\n        method, url = get_URL('compress')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'transferid': self.transfer_id\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a sentence from a PIN.", "response": "def from_pin(self, pin, timeout=5):\n        \"\"\"\n        Generate a sentence from PIN\n\n        :param str pin: a string of digits\n        :param float timeout: total time in seconds\n        :return dict: {\n            'sentence': sentence corresponding to the PIN,\n            'overlap': overlapping positions, starting for 0\n        }\n\n        >>> ToSentence().from_pin('3492')\n        [(\"Helva's\", False), ('masking', True), ('was', False), ('not', False), ('without', False), ('real', True), (',', False), ('pretty', True), ('novels', True)]\n        \"\"\"\n        return self.keyword_parse.from_initials_list([self.mnemonic.reality_to_starter('major_system', number)\n                                                      for number in pin],\n                                                     timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a list of tuples from a list of keywords.", "response": "def from_keywords(self, keyword_list, strictness=2, timeout=3):\n        \"\"\"\n        Generate a sentence from initial_list.\n\n        :param list keyword_list: a list of keywords to be included in the sentence.\n        :param int | None strictness: None for highest strictness. 2 or 1 for a less strict POS matching\n        :param float timeout: timeout of this function\n        :return list of tuple:\n\n        >>> ToSentence().from_keywords(['gains', 'grew', 'pass', 'greene', 'escort', 'illinois'])\n        [('The', False), ('gains', True), ('of', False), ('Bienville', False), ('upon', False), ('grew', True), ('liberal', False), ('pass', True), ('to', False), ('the', False), ('Indians', False), (',', False), ('in', False), ('greene', True), ('to', False), ('drive', False), ('back', False), ('the', False), ('Carolina', False), ('escort', True), (',', False), ('was', False), ('probably', False), ('a', False), ('illinois', True)]\n        \"\"\"\n        return self.keyword_parse.from_keyword_list(keyword_list, strictness, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _define_helper(flag_name, default_value, docstring, flagtype, required):\n    option_name = flag_name if required else \"--%s\" % flag_name\n    get_context_parser().add_argument(\n        option_name, default=default_value, help=docstring, type=flagtype)", "response": "Registers flag_name with default_value and docstring."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine a flag of type string.", "response": "def DEFINE_string(flag_name, default_value, docstring, required=False):  # pylint: disable=invalid-name\n    \"\"\"Defines a flag of type 'string'.\n    Args:\n        flag_name: The name of the flag as a string.\n        default_value: The default value the flag should take as a string.\n        docstring: A helpful message explaining the use of the flag.\n    \"\"\"\n    _define_helper(flag_name, default_value, docstring, str, required)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DEFINE_integer(flag_name, default_value, docstring, required=False):  # pylint: disable=invalid-name\n    _define_helper(flag_name, default_value, docstring, int, required)", "response": "Defines a flag of type int."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DEFINE_boolean(flag_name, default_value, docstring):  # pylint: disable=invalid-name\n\n    # Register a custom function for 'bool' so --flag=True works.\n    def str2bool(bool_str):\n        \"\"\"Return a boolean value from a give string.\"\"\"\n        return bool_str.lower() in ('true', 't', '1')\n\n    get_context_parser().add_argument(\n        '--' + flag_name,\n        nargs='?',\n        const=True,\n        help=docstring,\n        default=default_value,\n        type=str2bool)\n\n    # Add negated version, stay consistent with argparse with regard to\n    # dashes in flag names.\n    get_context_parser().add_argument(\n        '--no' + flag_name,\n        action='store_false',\n        dest=flag_name.replace('-', '_'))", "response": "Defines a flag of type boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DEFINE_float(flag_name, default_value, docstring, required=False):  # pylint: disable=invalid-name\n    _define_helper(flag_name, default_value, docstring, float, required)", "response": "Defines a flag of type float."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_subparser(self, name, dest=\"subcommand\", **kwargs):\n        if name not in self.children:\n            # Create the subparser.\n            subparsers = self._get_subparsers(dest)\n            parser = subparsers.add_parser(name, **kwargs)\n            self.children[name] = NamedParser(name, parser)\n        return self.children[name]", "response": "Get or create a subparser."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a string that will turn any ANSI shell output the desired colour.", "response": "def build_attr_string(attrs, supported=True):\r\n    '''Build a string that will turn any ANSI shell output the desired\r\n    colour.\r\n\r\n    attrs should be a list of keys into the term_attributes table.\r\n\r\n    '''\r\n    if not supported:\r\n        return ''\r\n    if type(attrs) == str:\r\n        attrs = [attrs]\r\n    result = '\\033['\r\n    for attr in attrs:\r\n        result += term_attributes[attr] + ';'\r\n    return result[:-1] + 'm'"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list of string widths a width of the minimum gap to place between them and the maximum width of the output. Returns the number of columns and rows and the width of each row.", "response": "def get_num_columns_and_rows(widths, gap_width, term_width):\r\n    '''Given a list of string widths, a width of the minimum gap to place\r\n    between them, and the maximum width of the output (such as a terminal\r\n    width), calculate the number of columns and rows, and the width of each\r\n    column, for the optimal layout.\r\n\r\n    '''\r\n    def calc_longest_width(widths, gap_width, ncols):\r\n        longest = 0\r\n        rows = [widths[s:s + ncols] for s in range(0, len(widths), ncols)]\r\n        col_widths = rows[0] # Column widths start at the first row widths\r\n        for r in rows:\r\n            for ii, c in enumerate(r):\r\n                if c > col_widths[ii]:\r\n                    col_widths[ii] = c\r\n            length = sum(col_widths) + gap_width * (ncols - 1)\r\n            if length > longest:\r\n                longest = length\r\n        return longest, col_widths\r\n\r\n    def calc_num_rows(num_items, cols):\r\n        div, mod = divmod(num_items, cols)\r\n        return div + (mod != 0)\r\n\r\n    # Start with one row\r\n    ncols = len(widths)\r\n    # Calculate the width of the longest row as the longest set of item widths\r\n    # ncols long and gap widths (gap_width * ncols - 1) that fits within the\r\n    # terminal width.\r\n    while ncols > 0:\r\n        longest_width, col_widths = calc_longest_width(widths, gap_width, ncols)\r\n        if longest_width < term_width:\r\n            # This number of columns fits\r\n            return calc_num_rows(len(widths), ncols), ncols, col_widths\r\n        else:\r\n            # This number of columns doesn't fit, so try one less\r\n            ncols -= 1\r\n    # If got here, it all has to go in one column\r\n    return len(widths), 1, 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_terminal_size():\r\n    '''Finds the width of the terminal, or returns a suitable default value.'''\r\n    def read_terminal_size_by_ioctl(fd):\r\n        try:\r\n            import struct, fcntl, termios\r\n            cr = struct.unpack('hh', fcntl.ioctl(1, termios.TIOCGWINSZ,\r\n                                                            '0000'))\r\n        except ImportError:\r\n            return None\r\n        except IOError as e:\r\n            return None\r\n        return cr[1], cr[0]\r\n\r\n    cr = read_terminal_size_by_ioctl(0) or \\\r\n            read_terminal_size_by_ioctl(1) or \\\r\n            read_terminal_size_by_ioctl(2)\r\n    if not cr:\r\n        try:\r\n            import os\r\n            fd = os.open(os.ctermid(), os.O_RDONLY)\r\n            cr = read_terminal_size_by_ioctl(fd)\r\n            os.close(fd)\r\n        except:\r\n            pass\r\n    if not cr:\r\n        import os\r\n        cr = [80, 25] # 25 rows, 80 columns is the default value\r\n        if os.getenv('ROWS'):\r\n            cr[1] = int(os.getenv('ROWS'))\r\n        if os.getenv('COLUMNS'):\r\n            cr[0] = int(os.getenv('COLUMNS'))\r\n\r\n    return cr[1], cr[0]", "response": "Finds the width of the terminal or returns a suitable default value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_to_nvlist(dict):\r\n    '''Convert a dictionary into a CORBA namevalue list.'''\r\n    result = []\r\n    for item in list(dict.keys()):\r\n        result.append(SDOPackage.NameValue(item, omniORB.any.to_any(dict[item])))\r\n    return result", "response": "Convert a dictionary into a CORBA namevalue list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a CORBA namevalue list into a dictionary.", "response": "def nvlist_to_dict(nvlist):\r\n    '''Convert a CORBA namevalue list into a dictionary.'''\r\n    result = {}\r\n    for item in nvlist :\r\n        result[item.name] = item.value.value()\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filtered(path, filter):\r\n    '''Check if a path is removed by a filter.\r\n\r\n    Check if a path is in the provided set of paths, @ref filter. If\r\n    none of the paths in filter begin with @ref path, then True is\r\n    returned to indicate that the path is filtered out. If @ref path is\r\n    longer than the filter, and starts with the filter, it is\r\n    considered unfiltered (all paths below a filter are unfiltered).\r\n\r\n    An empty filter ([]) is treated as not filtering any.\r\n\r\n    '''\r\n    if not filter:\r\n        return False\r\n    for p in filter:\r\n        if len(path) > len(p):\r\n            if path[:len(p)] == p:\r\n                return False\r\n        else:\r\n            if p[:len(path)] == path:\r\n                return False\r\n    return True", "response": "Check if a path is removed by a filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trim_filter(filter, levels=1):\r\n    '''Trim @ref levels levels from the front of each path in @filter.'''\r\n    trimmed = [f[levels:] for f in filter]\r\n    return [f for f in trimmed if f]", "response": "Trim @ref levels from the front of each path in filter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, bug_number):\n        bug = self.request(\n            'bug/%s' % bug_number,\n            params={\"include_fields\": self. DEFAULT_SEARCH}\n        )\n        return Bug(self, **bug['bugs'][0])", "response": "Get a specific bug from Bugzilla."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request(self, path, method='GET', headers=None, **kwargs):\n        headers = {} if headers is None else headers.copy()\n        headers[\"User-Agent\"] = \"Bugsy\"\n        kwargs['headers'] = headers\n        url = '%s/%s' % (self.bugzilla_url, path)\n        return self._handle_errors(self.session.request(method, url, **kwargs))", "response": "Perform a HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate possible eps values for a given distance matrix.", "response": "def estimate_eps(dist_mat, n_closest=5):\n    \"\"\"\n    Estimates possible eps values (to be used with DBSCAN)\n    for a given distance matrix by looking at the largest distance \"jumps\"\n    amongst the `n_closest` distances for each item.\n\n    Tip: the value for `n_closest` is important - set it too large and you may only get\n    really large distances which are uninformative. Set it too small and you may get\n    premature cutoffs (i.e. select jumps which are really not that big).\n\n    TO DO this could be fancier by calculating support for particular eps values,\n    e.g. 80% are around 4.2 or w/e\n    \"\"\"\n    dist_mat = dist_mat.copy()\n\n    # To ignore i == j distances\n    dist_mat[np.where(dist_mat == 0)] = np.inf\n    estimates = []\n    for i in range(dist_mat.shape[0]):\n        # Indices of the n closest distances\n        row = dist_mat[i]\n        dists = sorted(np.partition(row, n_closest)[:n_closest])\n        difs = [(x, y,\n                (y - x)) for x, y in zip(dists, dists[1:])]\n        eps_candidate, _, jump = max(difs, key=lambda x: x[2])\n\n        estimates.append(eps_candidate)\n    return sorted(estimates)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef estimate_k(X, max_k):\n    ks = range(1, max_k)\n    fs = np.zeros(len(ks))\n\n    # Special case K=1\n    fs[0], Sk = _fK(1)\n\n    # Rest of Ks\n    for k in ks[1:]:\n        fs[k-1], Sk = _fK(k, Skm1=Sk)\n    return np.argmin(fs) + 1", "response": "Estimate k for K - Means."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _clean_fields(allowed_fields: dict, fields: FieldsParam) -> Iterable[str]:\n    if fields == ALL:\n        fields = allowed_fields.keys()\n    else:\n        fields = tuple(fields)\n        unknown_fields = set(fields) - allowed_fields.keys()\n        if unknown_fields:\n            raise ValueError('Unknown fields: {}'.format(unknown_fields))\n    return fields", "response": "Clean lookup fields and check for errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms an arbitrary SQL SELECT on the anime table.", "response": "def select(\n        db,\n        where_query: str,\n        where_params: SQLParams,\n        fields: FieldsParam = ALL,\n        episode_fields: FieldsParam = (),\n) -> Iterator[Anime]:\n    \"\"\"Perform an arbitrary SQL SELECT WHERE on the anime table.\n\n    By nature of \"arbitrary query\", this is vulnerable to injection, use only\n    trusted values for `where_query`.\n\n    This will \"lazily\" fetch the requested fields as needed.  For example,\n    episodes (which require a separate query per anime) will only be fetched if\n    `episode_fields` is provided.  Anime status will be cached only if status\n    fields are requested.\n\n    :param str where_query: SELECT WHERE query\n    :param where_params: parameters for WHERE query\n    :param fields: anime fields to get.  If :const:`ALL`, get all fields.\n        Default is :const:`ALL`.\n    :param episode_fields: episode fields to get.\n        If :const:`ALL`, get all fields.  If empty, don't get episodes.\n        `fields` must contain 'aid' to get episodes.\n    :param bool force_status: whether to force status calculation.\n    :returns: iterator of Anime\n\n    \"\"\"\n\n    logger.debug(\n        'select(%r, %r, %r, %r, %r)',\n        db, where_query, where_params, fields, episode_fields)\n    fields = _clean_fields(ANIME_FIELDS, fields)\n    if not fields:\n        raise ValueError('Fields cannot be empty')\n    if set(fields) & STATUS_FIELDS.keys():\n        cur = db.cursor().execute(\n            ANIME_QUERY.format('aid', where_query),\n            where_params)\n        for row in cur:\n            cache_status(db, row[0])\n\n    if 'aid' in fields:\n        episode_fields = _clean_fields(EPISODE_FIELDS, episode_fields)\n    else:\n        episode_fields = ()\n\n    with db:\n        anime_query = ANIME_QUERY.format(\n            ','.join(ANIME_FIELDS[field] for field in fields),\n            where_query,\n        )\n        anime_rows = db.cursor().execute(anime_query, where_params)\n        for row in anime_rows:\n            anime = Anime(**{\n                field: value\n                for field, value in zip(fields, row)})\n\n            if episode_fields:\n                episode_query = 'SELECT {} FROM episode WHERE aid=?'\n                episode_query = episode_query.format(\n                    ','.join(EPISODE_FIELDS[field] for field in episode_fields))\n\n                episode_rows = db.cursor().execute(episode_query, (anime.aid,))\n                episodes = [\n                    Episode(**{\n                        field: value\n                        for field, value in zip(episode_fields, row)})\n                    for row in episode_rows]\n                anime.episodes = episodes\n\n            yield anime"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlook up information for a single anime.", "response": "def lookup(\n        db,\n        aid: int,\n        fields: FieldsParam = ALL,\n        episode_fields: FieldsParam = (),\n) -> Anime:\n    \"\"\"Look up information for a single anime.\n\n    :param fields: anime fields to get.  If ``None``, get all fields.\n    :param episode_fields: episode fields to get.\n        If ``None``, get all fields.  If empty, don't get episodes.\n\n    \"\"\"\n    return next(select(\n        db,\n        'aid=?',\n        (aid,),\n        fields=fields,\n        episode_fields=episode_fields,\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the set of letters defined in the configuration variable DEFAULT_ALPHABET.", "response": "def _get_default_letters(model_admin=None):\n    \"\"\"\n    Returns the set of letters defined in the configuration variable\n    DEFAULT_ALPHABET. DEFAULT_ALPHABET can be a callable, string, tuple, or\n    list and returns a set.\n\n    If a ModelAdmin class is passed, it will look for a DEFAULT_ALPHABET\n    attribute and use it instead.\n    \"\"\"\n    from django.conf import settings\n    import string\n    default_ltrs = string.digits + string.ascii_uppercase\n    default_letters = getattr(settings, 'DEFAULT_ALPHABET', default_ltrs)\n    if model_admin and hasattr(model_admin, 'DEFAULT_ALPHABET'):\n        default_letters = model_admin.DEFAULT_ALPHABET\n    if callable(default_letters):\n        return set(default_letters())\n    elif isinstance(default_letters, str):\n        return set([x for x in default_letters])\n    elif isinstance(default_letters, str):\n        return set([x for x in default_letters.decode('utf8')])\n    elif isinstance(default_letters, (tuple, list)):\n        return set(default_letters)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_available_letters(field_name, queryset):\n    if django.VERSION[1] <= 4:\n        result = queryset.values(field_name).annotate(\n            fl=FirstLetter(field_name)\n        ).values('fl').distinct()\n        return set([res['fl'] for res in result if res['fl'] is not None])\n    else:\n        from django.db import connection\n        qn = connection.ops.quote_name\n        db_table = queryset.model._meta.db_table\n        sql = \"SELECT DISTINCT UPPER(SUBSTR(%s, 1, 1)) as letter FROM %s\" % (qn(field_name), qn(db_table))\n        cursor = connection.cursor()\n        cursor.execute(sql)\n        rows = cursor.fetchall() or ()\n        return set([row[0] for row in rows if row[0] is not None])", "response": "Returns a set of available letters for the given field and table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alphabet(cl):\n    if not getattr(cl.model_admin, 'alphabet_filter', False):\n        return\n    field_name = cl.model_admin.alphabet_filter\n    alpha_field = '%s__istartswith' % field_name\n    alpha_lookup = cl.params.get(alpha_field, '')\n\n    letters_used = _get_available_letters(field_name, cl.model.objects.all())\n    all_letters = list(_get_default_letters(cl.model_admin) | letters_used)\n    all_letters.sort()\n\n    choices = [{\n        'link': cl.get_query_string({alpha_field: letter}),\n        'title': letter,\n        'active': letter == alpha_lookup,\n        'has_entries': letter in letters_used, } for letter in all_letters]\n    all_letters = [{\n        'link': cl.get_query_string(None, [alpha_field]),\n        'title': _('All'),\n        'active': '' == alpha_lookup,\n        'has_entries': True\n    }, ]\n    return {'choices': all_letters + choices}", "response": "Returns a dictionary that can be used to display a list of alphabet items in the admin."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the alphabet filter for the current queryset.", "response": "def qs_alphabet_filter(parser, token):\n    \"\"\"\n    The parser/tokenizer for the queryset alphabet filter.\n\n    {% qs_alphabet_filter <queryset> <field name> [<template name>] [strip_params=comma,delim,list] %}\n\n    {% qs_alphabet_filter objects lastname myapp/template.html %}\n\n    The template name is optional and uses alphafilter/alphabet.html if not\n    specified\n    \"\"\"\n    bits = token.split_contents()\n    if len(bits) == 3:\n        return AlphabetFilterNode(bits[1], bits[2])\n    elif len(bits) == 4:\n        if \"=\" in bits[3]:\n            key, val = bits[3].split('=')\n            return AlphabetFilterNode(bits[1], bits[2], strip_params=val)\n        else:\n            return AlphabetFilterNode(bits[1], bits[2], template_name=bits[3])\n    elif len(bits) == 5:\n        key, val = bits[4].split('=')\n        return AlphabetFilterNode(bits[1], bits[2], bits[3], bits[4])\n    else:\n        raise TemplateSyntaxError(\"%s is called with a queryset and field \"\n                                  \"name, and optionally a template.\" % bits[0])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of viable installed plugins keyed off of the names of the installed plugins.", "response": "def get_installed_classes(cls):\n        \"\"\"\n        Iterates over installed plugins associated with the `entry_point` and\n        returns a dictionary of viable ones keyed off of their names.\n\n        A viable installed plugin is one that is both loadable *and* a subclass\n        of the Pluggable subclass in question.\n        \"\"\"\n        installed_classes = {}\n        for entry_point in pkg_resources.iter_entry_points(cls.entry_point):\n            try:\n                plugin = entry_point.load()\n            except ImportError as e:\n                logger.error(\n                    \"Could not load plugin %s: %s\", entry_point.name, str(e)\n                )\n                continue\n\n            if not issubclass(plugin, cls):\n                logger.error(\n                    \"Could not load plugin %s:\" +\n                    \" %s class is not subclass of %s\",\n                    entry_point.name, plugin.__class__.__name__, cls.__name__\n                )\n                continue\n\n            if not plugin.validate_dependencies():\n                logger.error(\n                    \"Could not load plugin %s:\" +\n                    \" %s class dependencies not met\",\n                    entry_point.name, plugin.__name__\n                )\n                continue\n\n            installed_classes[entry_point.name] = plugin\n\n        return installed_classes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an instance of the class with the given name and config.", "response": "def from_config(cls, name, config):\n        \"\"\"\n        Behaves like the base Configurable class's `from_config()` except this\n        makes sure that the `Pluggable` subclass with the given name is\n        actually a properly installed plugin first.\n        \"\"\"\n        installed_classes = cls.get_installed_classes()\n\n        if name not in installed_classes:\n            raise ValueError(\"Unknown/unavailable %s\" % cls.__name__.lower())\n\n        pluggable_class = installed_classes[name]\n\n        pluggable_class.validate_config(config)\n\n        instance = pluggable_class()\n        if not instance.name:\n            instance.name = name\n        instance.apply_config(config)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the basename of the affected file.", "response": "def file_name(self, event):\n        \"\"\"\n        Helper method for determining the basename of the affected file.\n        \"\"\"\n        name = os.path.basename(event.src_path)\n        name = name.replace(\".yaml\", \"\")\n        name = name.replace(\".yml\", \"\")\n\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmodify config file handler.", "response": "def on_modified(self, event):\n        \"\"\"\n        Modified config file handler.\n\n        If a config file is modified, the yaml contents are parsed and the\n        new results are validated by the target class.  Once validated, the\n        new config is passed to the on_update callback.\n        \"\"\"\n        if os.path.isdir(event.src_path):\n            return\n\n        logger.debug(\"file modified: %s\", event.src_path)\n\n        name = self.file_name(event)\n\n        try:\n            config = yaml.load(open(event.src_path))\n            self.target_class.from_config(name, config)\n        except Exception:\n            logger.exception(\n                \"Error when loading updated config file %s\", event.src_path,\n            )\n            return\n\n        self.on_update(self.target_class, name, config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when a file is deleted.", "response": "def on_deleted(self, event):\n        \"\"\"\n        Deleted config file handler.\n\n        Simply fires the on_delete callback with the name of the deleted item.\n        \"\"\"\n        logger.debug(\"file removed: %s\", event.src_path)\n        name = self.file_name(event)\n\n        self.on_delete(self.target_class, name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_moved(self, event):\n        self.on_deleted(events.FileDeletedEvent(event.src_path))\n        self.on_created(events.FileCreatedEvent(event.dest_path))", "response": "A move event is proxied to the appropriate handlers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef regenerate(self):\n\n        oldhash = self.session_hash\n        self.new_session_id()\n        try:\n            self.rdb.rename(oldhash,self.session_hash)\n            self.rdb.expire(self.session_hash,self.ttl)\n        except:\n            pass", "response": "Regenerate the session id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a value from the dictionary.", "response": "def get(self,key,default=None):\n        \"\"\"Get a value from the dictionary.\n\n        Args:\n            key (str): The dictionary key.\n            default (any): The default to return if the key is not in the\n                dictionary. Defaults to None.\n\n        Returns:\n            str or any: The dictionary value or the default if the key is not\n                in the dictionary.\n        \"\"\"\n\n        retval = self.__getitem__(key)\n        if not retval:\n            retval = default\n\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all the key value pair tuples in the dictionary.", "response": "def items(self):\n        \"\"\"Return a list of all the key, value pair tuples in the dictionary.\n\n        Returns:\n            list of tuples: [(key1,value1),(key2,value2),...,(keyN,valueN)]\n        \"\"\"\n        all_items = [(k.decode('utf-8'),v.decode('utf-8')) for k,v in self.rdb.hgetall(self.session_hash).items()]\n        return all_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all keys in the dictionary.", "response": "def keys(self):\n        \"\"\"Return a list of all keys in the dictionary.\n\n        Returns:\n            list of str: [key1,key2,...,keyN]\n        \"\"\"\n        all_keys = [k.decode('utf-8') for k,v in self.rdb.hgetall(self.session_hash).items()]\n        return all_keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all values in the dictionary.", "response": "def values(self):\n        \"\"\"Returns a list of all values in the dictionary.\n\n        Returns:\n            list of str: [value1,value2,...,valueN]\n        \"\"\"\n        all_values = [v.decode('utf-8') for k,v in self.rdb.hgetall(self.session_hash).items()]\n        return all_values"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a vector of spherical bessel functions yn = 0 to N - 1", "response": "def sbessely(x, N):\r\n    \"\"\"Returns a vector of spherical bessel functions yn:\r\n\r\n        x:   The argument.\r\n        N:   values of n will run from 0 to N-1.\r\n\r\n    \"\"\"\r\n\r\n    out = np.zeros(N, dtype=np.float64)\r\n\r\n    out[0] = -np.cos(x) / x\r\n    out[1] = -np.cos(x) / (x ** 2) - np.sin(x) / x\r\n\r\n    for n in xrange(2, N):\r\n        out[n] = ((2.0 * n - 1.0) / x) * out[n - 1] - out[n - 2]\r\n\r\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sbesselj(x, N):\r\n\r\n    nmax = N - 1;\r\n    out = np.zeros(N, dtype=np.float64)\r\n    z = x ** 2\r\n\r\n    out[0] = np.sin(x) / x\r\n    j1 = np.sin(x) / z - np.cos(x) / x\r\n\r\n    u = 1\r\n    v = x / (2.0 * nmax + 1.0)\r\n    w = v\r\n    n = nmax\r\n\r\n    while(np.abs(v / w) > 1e-20):\r\n        n = n + 1\r\n        u = 1 / (1 - z * u / (4.0 * n ** 2 - 1.0))\r\n        v *= u - 1\r\n        w += v\r\n\r\n    out[nmax] = w\r\n\r\n    for n in xrange(nmax - 1, 0, -1):\r\n        out[n] = 1.0 / ((2.0 * n + 1.0) / x - out[n + 1])\r\n\r\n    if(np.abs(out[0]) >= np.abs(j1)):\r\n        out[1] *= out[0]\r\n    else:\r\n        out[1] = j1\r\n\r\n    for n in xrange(1, nmax):\r\n        out[n + 1] *= out[n]\r\n\r\n    return out", "response": "Returns a vector of spherical bessel functions jn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sbesselh1(x, N):\r\n    \"Spherical Hankel of the first kind\"\r\n    \r\n    jn = sbesselj(x, N)\r\n    yn = sbessely(x, N)\r\n\r\n    return jn + 1j * yn", "response": "Spherical Hankel of the first kind"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an array where each column is a vector of sbessel values. This is useful for plotting a set of Spherical Bessel functions.", "response": "def sbesselj_array(xvec, N):\r\n    \"\"\"Outputs an array where each column is a vector of sbessel values. This\r\n    is useful for plotting a set of Spherical Bessel Functions:\r\n\r\n        A = sbessel.sbessely_array(np.linspace(.1,20,100),40)\r\n        for sb in A:\r\n            plot(sb)\r\n        show()\r\n    \"\"\"\r\n\r\n    first_time = True  \r\n    for x in xvec:\r\n        a = sbesselj(x, N)\r\n        if first_time:\r\n            out = np.array([a])\r\n            first_time = False\r\n        else:\r\n            out = np.concatenate([out, [a]], axis=0)\r\n            \r\n    return out.T"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sbessely_array(xvec, N):\r\n\r\n    first_time = True  \r\n    for x in xvec:\r\n        a = sbessely(x, N)\r\n        if first_time:\r\n            out = np.array([a])\r\n            first_time = False\r\n        else:\r\n            out = np.concatenate([out, [a]], axis=0)\r\n            \r\n    return out.T", "response": "Returns an array where each column is a vector of sbessel values. This is useful for plotting a set of Spherical Bessel functions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sbesselj_sum(z, N):\r\n\r\n    b = sbesselj(z, N)\r\n    vvv = 2.0 * np.array(range(0, N), dtype=np.float64) + 1.0\r\n    sm = np.sum(np.sort(vvv * (b ** 2)))\r\n    return np.abs((sm - 1.0) / sm) + np.spacing(1)", "response": "Tests the Spherical Bessel function jn using the sum"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of dicts summing their values.", "response": "def merge(dicts):\n    \"\"\"\n    Merges a list of dicts, summing their values.\n    (Parallelized wrapper around `_count`)\n    \"\"\"\n    chunks = [args for args in np.array_split(dicts, 20)]\n    results = parallel(_count, chunks, n_jobs=-1)\n    return _count(results)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging a list of dicts summing their values.", "response": "def _count(dicts):\n    \"\"\"\n    Merge a list of dicts, summing their values.\n    \"\"\"\n    counts = defaultdict(int)\n    for d in dicts:\n        for k, v in d.items():\n            counts[k] += v\n    return counts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _chunks(iterable, n):\n    iterable = iter(iterable)\n    while True:\n        # store one line in memory,\n        # chain it to an iterator on the rest of the chunk\n        yield chain([next(iterable)], islice(iterable, n-1))", "response": "Splits an iterable into n chunks of size n."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit the specified file into smaller files.", "response": "def split_file(path, chunk_size=50000):\n    \"\"\"\n    Splits the specified file into smaller files.\n    \"\"\"\n    with open(path) as f:\n        for i, lines in enumerate(_chunks(f, chunk_size)):\n            file_split = '{}.{}'.format(os.path.basename(path), i)\n            chunk_path = os.path.join('/tmp', file_split)\n            with open(chunk_path, 'w') as f:\n                f.writelines(lines)\n            yield chunk_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a given line string to the list of lines validating the line string first.", "response": "def add_line(self, line):\n        \"\"\"\n        Adds a given line string to the list of lines, validating the line\n        first.\n        \"\"\"\n        if not self.is_valid_line(line):\n            logger.warn(\n                \"Invalid line for %s section: '%s'\",\n                self.section_name, line\n            )\n            return\n\n        self.lines.append(line)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_valid_line(self, line):\n        adjusted_line = line.strip().lower()\n\n        return any([\n            adjusted_line.startswith(directive)\n            for directive in directives_by_section[self.section_name]\n        ])", "response": "Checks if a given line is a valid look - up - key - value line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef param_redirect(request, viewname, *args):\n    url = reverse(viewname, PARAMS_URL_CONF, args)\n    params = request.GET.urlencode().split('&')\n\n    if hasattr(request, 'cparam'):\n        for k, v in request.cparam.items():\n            params.append('{0}={1}'.format(k, v))\n\n    new_params = '&'.join(x for x in params if x != '')\n    if len(new_params) > 0:\n        return HttpResponseRedirect('{0}?{1}'.format(url, new_params))\n    return HttpResponseRedirect(url)", "response": "Redirect and keep URL parameters if any."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backup_db(release=None, limit=5):\n\n    assert \"mysql_user\" in env, \"Missing mysqL_user in env\"\n    assert \"mysql_password\" in env, \"Missing mysql_password in env\"\n    assert \"mysql_host\" in env, \"Missing mysql_host in env\"\n    assert \"mysql_db\" in env, \"Missing mysql_db in env\"\n\n    if not release:\n        release = paths.get_current_release_name()\n\n    max_versions = limit+1\n\n    if not release:\n        return\n\n    env.run(\"mkdir -p %s\" % paths.get_backup_path(\"mysql\"))\n\n    backup_file = \"mysql/%s.sql.gz\" % release\n    backup_path = paths.get_backup_path(backup_file)\n\n    env.run(\"mysqldump -u %s -p%s -h %s %s | gzip -c > %s\" %\n            (env.mysql_user, env.mysql_password, env.mysql_host, env.mysql_db,\n             backup_path))\n\n    # Remove older releases\n    env.run(\"ls -dt %s/* | tail -n +%s | xargs rm -rf\" % (\n        paths.get_backup_path(\"mysql\"),\n        max_versions)\n    )", "response": "Backup database and associate it with current release"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restore_db(release=None):\n\n    assert \"mysql_user\" in env, \"Missing mysqL_user in env\"\n    assert \"mysql_password\" in env, \"Missing mysql_password in env\"\n    assert \"mysql_host\" in env, \"Missing mysql_host in env\"\n    assert \"mysql_db\" in env, \"Missing mysql_db in env\"\n\n    if not release:\n        release = paths.get_current_release_name()\n\n    if not release:\n        raise Exception(\"Release %s was not found\" % release)\n\n    backup_file = \"mysql/%s.sql.gz\" % release\n    backup_path = paths.get_backup_path(backup_file)\n\n    if not env.exists(backup_path):\n        raise Exception(\"Backup file %s not found\" % backup_path)\n\n    env.run(\"gunzip < %s | mysql -u %s -p%s -h %s %s\" %\n            (backup_path, env.mysql_user, env.mysql_password, env.mysql_host,\n             env.mysql_db))", "response": "Restores backup to version using current version by default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the type of this context as an optionally coloured string.", "response": "def kind_as_string(self, add_colour=True):\n        '''Get the type of this context as an optionally coloured string.\n\n        @param add_colour If True, ANSI colour codes will be added.\n        @return A string describing the kind of execution context this is.\n\n        '''\n        with self._mutex:\n            if self.kind == self.PERIODIC:\n                result = 'Periodic', ['reset']\n            elif self.kind == self.EVENT_DRIVEN:\n                result = 'Event-driven', ['reset']\n            elif self.kind == self.OTHER:\n                result = 'Other', ['reset']\n        if add_colour:\n            return utils.build_attr_string(result[1], supported=add_colour) + \\\n                    result[0] + utils.build_attr_string('reset', supported=add_colour)\n        else:\n            return result[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef running_as_string(self, add_colour=True):\n        '''Get the state of this context as an optionally coloured string.\n\n        @param add_colour If True, ANSI colour codes will be added.\n        @return A string describing this context's running state.\n\n        '''\n        with self._mutex:\n            if self.running:\n                result = 'Running', ['bold', 'green']\n            else:\n                result = 'Stopped', ['reset']\n        if add_colour:\n            return utils.build_attr_string(result[1], supported=add_colour) + \\\n                    result[0] + utils.build_attr_string('reset', supported=add_colour)\n        else:\n            return result[0]", "response": "Get the state of this context as an optionally coloured string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kind(self):\n        '''The kind of this execution context.'''\n        with self._mutex:\n            kind = self._obj.get_kind()\n            if kind == RTC.PERIODIC:\n                return self.PERIODIC\n            elif kind == RTC.EVENT_DRIVEN:\n                return self.EVENT_DRIVEN\n            else:\n                return self.OTHER", "response": "The kind of this execution context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef owner_name(self):\n        '''The name of the RTObject that owns this context.'''\n        with self._mutex:\n            if self._owner:\n                return self._owner.get_component_profile().instance_name\n            else:\n                return ''", "response": "The name of the RTObject that owns this context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_manager(self, instance):\n        return self.create_manager(instance,\n                                   self.rating_model._base_manager.__class__)", "response": "Returns a queryset based on the related model s base manager. Used by get_base_manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_manager(self, instance, superclass):\n        rel_model = self.rating_model\n        rated_model = self.rated_model\n\n        class RelatedManager(superclass):\n            def get_query_set(self):\n                qs = RatingsQuerySet(rel_model, rated_model=rated_model)\n                return qs.filter(**(self.core_filters))\n\n            def add(self, *objs):\n                lookup_kwargs = rel_model.lookup_kwargs(instance)\n                for obj in objs:\n                    if not isinstance(obj, self.model):\n                        raise TypeError(\"'%s' instance expected\" %\n                                        self.model._meta.object_name)\n                    for (k, v) in lookup_kwargs.iteritems():\n                        setattr(obj, k, v)\n                    obj.save()\n            add.alters_data = True\n\n            def create(self, **kwargs):\n                kwargs.update(rel_model.lookup_kwargs(instance))\n                return super(RelatedManager, self).create(**kwargs)\n            create.alters_data = True\n\n            def get_or_create(self, **kwargs):\n                kwargs.update(rel_model.lookup_kwargs(instance))\n                return super(RelatedManager, self).get_or_create(**kwargs)\n            get_or_create.alters_data = True\n\n            def remove(self, *objs):\n                for obj in objs:\n                    # Is obj actually part of this descriptor set?\n                    if obj in self.all():\n                        obj.delete()\n                    else:\n                        raise rel_model.DoesNotExist(\n                            \"%r is not related to %r.\" % (obj, instance))\n            remove.alters_data = True\n\n            def clear(self):\n                self.all().delete()\n            clear.alters_data = True\n\n            def rate(self, user, score):\n                rating, created = self.get_or_create(user=user)\n                if created or score != rating.score:\n                    rating.score = score\n                    rating.save()\n                return rating\n\n            def unrate(self, user):\n                return self.filter(user=user,\n                                   **rel_model.lookup_kwargs(instance)\n                                   ).delete()\n\n            def perform_aggregation(self, aggregator):\n                score = self.all().aggregate(agg=aggregator('score'))\n                return score['agg']\n\n            def cumulative_score(self):\n                # simply the sum of all scores, useful for +1/-1\n                return self.perform_aggregation(models.Sum)\n\n            def average_score(self):\n                # the average of all the scores, useful for 1-5\n                return self.perform_aggregation(models.Avg)\n\n            def standard_deviation(self):\n                # the standard deviation of all the scores, useful for 1-5\n                return self.perform_aggregation(models.StdDev)\n\n            def variance(self):\n                # the variance of all the scores, useful for 1-5\n                return self.perform_aggregation(models.Variance)\n\n            def similar_items(self):\n                return SimilarItem.objects.get_for_item(instance)\n\n        manager = RelatedManager()\n        manager.core_filters = rel_model.lookup_kwargs(instance)\n        manager.model = rel_model\n\n        return manager", "response": "Dynamically create a related manager for the related class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hash(self, value):\n        result = 0\n        for i in range(len(value)):\n            result += self.seed * result + ord(value[i])\n        return (self.capacity - 1) % result", "response": "hash function that implements to acquire hash value that use simply method that weighted sum of the value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_contains(self, data):\n        if not data:\n            return False\n        data = self._compress_by_md5(data)\n        result = True\n        # cut the first two place,route to different block by block_num\n        name = self.key + str(int(data[0:2], 16) % self.block_num)\n        for h in self.hash_function:\n            local_hash = h.hash(data)\n            result = result & self.server.getbit(name, local_hash)\n        return result", "response": "Check if the data contains the key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate HAProxy config file content based on a list of clusters.", "response": "def generate(self, clusters, version=None):\n        \"\"\"\n        Generates HAProxy config file content based on a given list of\n        clusters.\n        \"\"\"\n        now = datetime.datetime.now()\n\n        sections = [\n            Section(\n                \"Auto-generated by Lighthouse (%s)\" % now.strftime(\"%c\"),\n                self.global_stanza,\n                self.defaults_stanza\n            )\n        ]\n\n        meta_stanzas = [\n            MetaFrontendStanza(\n                name, self.meta_clusters[name][\"port\"],\n                self.meta_clusters[name].get(\"frontend\", []), members,\n                self.bind_address\n            )\n            for name, members\n            in six.iteritems(self.get_meta_clusters(clusters))\n        ]\n        frontend_stanzas = [\n            FrontendStanza(cluster, self.bind_address)\n            for cluster in clusters\n            if \"port\" in cluster.haproxy\n        ]\n        backend_stanzas = [BackendStanza(cluster) for cluster in clusters]\n\n        if version and version >= (1, 5, 0):\n            peers_stanzas = [PeersStanza(cluster) for cluster in clusters]\n        else:\n            peers_stanzas = []\n\n        sections.extend([\n            Section(\"Frontend stanzas for ACL meta clusters\", *meta_stanzas),\n            Section(\"Per-cluster frontend definitions\", *frontend_stanzas),\n            Section(\"Per-cluster backend definitions\", *backend_stanzas),\n            Section(\"Per-cluster peer listings\", *peers_stanzas),\n            Section(\"Individual proxy definitions\", *self.proxy_stanzas),\n        ])\n        if self.stats_stanza:\n            sections.append(\n                Section(\"Listener for stats web interface\", self.stats_stanza)\n            )\n\n        return \"\\n\\n\\n\".join([str(section) for section in sections]) + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_meta_clusters(self, clusters):\n        meta_clusters = collections.defaultdict(list)\n\n        for cluster in clusters:\n            if not cluster.meta_cluster:\n                continue\n            meta_clusters[cluster.meta_cluster].append(cluster)\n\n        unconfigured_meta_clusters = [\n            name for name in meta_clusters.keys()\n            if name not in self.meta_clusters\n        ]\n\n        for name in unconfigured_meta_clusters:\n            logger.error(\"Meta cluster %s not configured!\")\n            del meta_clusters[name]\n\n        return meta_clusters", "response": "Returns a dictionary keyed off of meta cluster names where the values\n        are lists of clusters associated with the meta cluster name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random image of size with random background color.", "response": "def generate(cls, size, string, filetype=\"JPEG\"):\n        \"\"\"\n            Generates a squared avatar with random background color.\n            :param size: size of the avatar, in pixels\n            :param string: string to be used to print text and seed the random\n            :param filetype: the file format of the image (i.e. JPEG, PNG)\n        \"\"\"\n        render_size = max(size, GenAvatar.MAX_RENDER_SIZE)\n        image = Image.new('RGB', (render_size, render_size),\n                          cls._background_color(string))\n        draw = ImageDraw.Draw(image)\n        font = cls._font(render_size)\n        text = cls._text(string)\n        draw.text(\n            cls._text_position(render_size, text, font),\n            text,\n            fill=cls.FONT_COLOR,\n            font=font)\n        stream = BytesIO()\n        image = image.resize((size, size), Image.ANTIALIAS)\n        image.save(stream, format=filetype, optimize=True)\n        # return stream.getvalue()\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a random background color.", "response": "def _background_color(s):\n        \"\"\"\n            Generate a random background color.\n            Brighter colors are dropped, because the text is white.\n            :param s: Seed used by the random generator\n            (same seed will produce the same color).\n        \"\"\"\n        seed(s)\n        r = v = b = 255\n        while r + v + b > 255 * 2:\n            r = randint(0, 255)\n            v = randint(0, 255)\n            b = randint(0, 255)\n        return (r, v, b)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _font(size):\n        # path = '/usr/share/fonts/wenquanyi/wqy-microhei/wqy-microhei.ttc'\n        path = os.path.join(\n            os.path.dirname(__file__), 'data', \"wqy-microhei.ttc\")\n        return ImageFont.truetype(path, size=int(0.65 * size), index=0)", "response": "Returns a PIL ImageFont instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the left - top point where the text should be positioned.", "response": "def _text_position(size, text, font):\n        \"\"\"\n            Returns the left-top point where the text should be positioned.\n        \"\"\"\n        width, height = font.getsize(text)\n        left = (size - width) / 2.0\n        top = (size - height) / 3.0\n        return left, top"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef age(self):\n        if self.Death:\n            age =  self.Death - self.Born\n        else:    \n            age =  datetime.date.today() - self.Born\n        return age.days", "response": "Calculates the animals age relative to the current date of death or today."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef breeding_female_location_type(self):\n        try:\n            self.breeding_females.all()[0].Cage\n            if int(self.breeding_females.all()[0].Cage) == int(self.Cage):\n                type = \"resident-breeder\"\n            else:\n                type = \"non-resident-breeder\"                \n        except IndexError:\n            type = \"unknown-breeder\"\n        except ValueError:\n            type = \"unknown-breeder\"            \n        return type", "response": "This attribute defines whether a female s current location is the same as the breeding cage to which it belongs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self):\n        if self.Death:\n            self.Alive = False\n        super(Animal, self).save()", "response": "The save method for Animal class is over - ridden to set Alive = False when a Death date is entered."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the breeding cage s duration.", "response": "def duration(self):\n        \"\"\"Calculates the breeding cage's duration.\n\n        This is relative to the current date (if alive) or the date of inactivation (if not).\n        The duration is formatted in days.\"\"\"\n        if self.End:\n            age =  self.End - self.Start\n        else:    \n            age =  datetime.date.today() - self.Start\n        return age.days"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unweaned(self):\n        return Animal.objects.filter(Breeding=self, Weaned__isnull=True, Alive=True)", "response": "This attribute generates a queryset of unweaned animals for this breeding cage. It is filtered for only Alive animals."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef male_breeding_location_type(self):\n        if int(self.Male.all()[0].Cage) == int(self.Cage):\n            type = \"resident breeder\"\n        else:\n            type = \"non-resident breeder\"\n        return type", "response": "This attribute defines whether a breeding male s current location is the same as the breeding cage."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self, on_add, on_update, on_delete):\n        handler = ConfigFileChangeHandler(\n            self.target_class, on_add, on_update, on_delete\n        )\n\n        for file_name in os.listdir(self.file_path):\n            if os.path.isdir(os.path.join(self.file_path, file_name)):\n                continue\n            if (\n                not self.target_class.config_subdirectory and\n                not (\n                    file_name.endswith(\".yaml\") or file_name.endswith(\".yml\")\n                )\n            ):\n                continue\n\n            handler.on_created(\n                events.FileCreatedEvent(\n                    os.path.join(self.file_path, file_name)\n                )\n            )\n\n        observer = observers.Observer()\n        observer.schedule(handler, self.file_path)\n        observer.start()\n\n        return observer", "response": "Starts monitoring the file path passing along on_add update and delete callbacks to a watchdog observer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matplotlibensure(func):\r\n    @wraps(func)\r\n    def wrap(*args):\r\n        if MPLINSTALLED == False:\r\n            raise ImportError(msg)\r\n        \r\n        return func(*args)   \r\n        \r\n    return wrap", "response": "Decorator that raises ImportError if matplotlib isn t installed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting date to timestamp.", "response": "def to_ts(date: datetime.date) -> float:\n    \"\"\"Convert date to timestamp.\n\n    >>> to_ts(datetime.date(2001, 1, 2))\n    978393600.0\n    \"\"\"\n    return datetime.datetime(\n        date.year, date.month, date.day,\n        tzinfo=datetime.timezone.utc).timestamp()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_date(ts: float) -> datetime.date:\n    return datetime.datetime.fromtimestamp(\n        ts, tz=datetime.timezone.utc).date()", "response": "Convert timestamp to date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_response(sock, buffer_size=4096):\n    response = \"\"\n    extra = \"\"\n\n    while True:\n        try:\n            chunk = sock.recv(buffer_size)\n            if chunk:\n                response += chunk\n        except socket.error as e:\n            if e.errno not in [errno.EAGAIN, errno.EINTR]:\n                raise\n\n        if not response:\n            break\n\n        if \"\\n\" in response:\n            response, extra = response.split(\"\\n\", 1)\n            break\n\n    return response, extra", "response": "Returns a tuple of the response line and extra data after the newline."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a name server and add its contents to the tree.", "response": "def add_name_server(self, server, filter=[], dynamic=None):\n        '''Parse a name server, adding its contents to the tree.\n\n        @param server The address of the name server, in standard\n                      address format. e.g. 'localhost',\n                      'localhost:2809', '59.7.0.1'.\n        @param filter Restrict the parsed objects to only those in this\n                      path. For example, setting filter to [['/',\n                      'localhost', 'host.cxt', 'comp1.rtc']] will\n                      prevent 'comp2.rtc' in the same naming context\n                      from being parsed.\n        @param dynamic Override the tree-wide dynamic setting. If not provided,\n                       the value given when the tree was created will be used.\n\n        '''\n        if dynamic == None:\n            dynamic = self._dynamic\n        self._parse_name_server(server, filter, dynamic=dynamic)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nis the node pointed to by path a component?", "response": "def is_component(self, path):\n        '''Is the node pointed to by @ref path a component?'''\n        node = self.get_node(path)\n        if not node:\n            return False\n        return node.is_component"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_directory(self, path):\n        '''Is the node pointed to by @ref path a directory (name servers and\n        naming contexts)?\n\n        '''\n        node = self.get_node(path)\n        if not node:\n            return False\n        return node.is_directory", "response": "Is the node pointed to by path a directory?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbeing the node pointed to by path a manager?", "response": "def is_manager(self, path):\n        '''Is the node pointed to by @ref path a manager?'''\n        node = self.get_node(path)\n        if not node:\n            return False\n        return node.is_manager"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbeing the node pointed to by path a name server?", "response": "def is_nameserver(self, path):\n        '''Is the node pointed to by @ref path a name server (specialisation\n        of directory nodes)?\n\n        '''\n        node = self.get_node(path)\n        if not node:\n            return False\n        return node.is_nameserver"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_unknown(self, path):\n        '''Is the node pointed to by @ref path an unknown object?'''\n        node = self.get_node(path)\n        if not node:\n            return True\n        return node.is_unknown", "response": "Is the node pointed to by path an unknown object?"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_zombie(self, path):\n        '''Is the node pointed to by @ref path a zombie object?'''\n        node = self.get_node(path)\n        if not node:\n            return False\n        return node.is_zombie", "response": "Is the node pointed to by path a zombie object?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall a function on the root node and recursively all its children.", "response": "def iterate(self, func, args=None, filter=[]):\n        '''Call a function on the root node, and recursively all its children.\n\n        This is a depth-first iteration.\n\n        @param func The function to call. Its declaration must be\n                    'def blag(node, args)', where 'node' is the current node\n                    in the iteration and args is the value of @ref args.\n        @param args Extra arguments to pass to the function at each iteration.\n                    Pass multiple arguments in as a tuple.\n        @param filter A list of filters to apply before calling func for each\n                      node in the iteration. If the filter is not True,\n                      @ref func will not be called for that node. Each filter\n                      entry should be a string, representing on of the is_*\n                      properties (is_component, etc), or a function object.\n        @return The results of the calls to @ref func in a list.\n\n        '''\n        return self._root.iterate(func, args, filter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_servers_from_env(self, filter=[], dynamic=None):\n        '''Load the name servers environment variable and parse each server in\n        the list.\n\n        @param filter Restrict the parsed objects to only those in this\n                      path. For example, setting filter to [['/',\n                      'localhost', 'host.cxt', 'comp1.rtc']] will\n                      prevent 'comp2.rtc' in the same naming context\n                      from being parsed.\n        @param dynamic Override the tree-wide dynamic setting. If not provided,\n                       the value given when the tree was created will be used.\n\n        '''\n        if dynamic == None:\n            dynamic = self._dynamic\n        if NAMESERVERS_ENV_VAR in os.environ:\n            servers = [s for s in os.environ[NAMESERVERS_ENV_VAR].split(';') \\\n                         if s]\n            self._parse_name_servers(servers, filter, dynamic)", "response": "Load the name servers environment variable and parse each server in\n                      the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_port(port_obj, owner):\n    '''Create a port object of the correct type.\n\n    The correct port object type is chosen based on the port.port_type\n    property of port_obj.\n\n    @param port_obj The CORBA PortService object to wrap.\n    @param owner The owner of this port. Should be a Component object or None.\n    @return The created port object.\n\n    '''\n    profile = port_obj.get_port_profile()\n    props = utils.nvlist_to_dict(profile.properties)\n    if props['port.port_type'] == 'DataInPort':\n        return DataInPort(port_obj, owner)\n    elif props['port.port_type'] == 'DataOutPort':\n        return DataOutPort(port_obj, owner)\n    elif props['port.port_type'] == 'CorbaPort':\n        return CorbaPort(port_obj, owner)\n    else:\n        return Port(port_obj, owner)", "response": "Create a correct type of port object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects this port to another ports.", "response": "def connect(self, dests=[], name=None, id='', props={}):\n        '''Connect this port to other ports.\n\n        After the connection has been made, a delayed reparse of the\n        connections for this and the destination port will be triggered.\n\n        @param dests A list of the destination Port objects. Must be provided.\n        @param name The name of the connection. If None, a suitable default\n                    will be created based on the names of the two ports.\n        @param id The ID of this connection. If None, one will be generated by\n               the RTC implementation.\n        @param props Properties of the connection. Required values depend on\n                     the type of the two ports being connected.\n        @raises IncompatibleDataPortConnectionPropsError, FailedToConnectError\n\n        '''\n        with self._mutex:\n            if self.porttype == 'DataInPort' or self.porttype == 'DataOutPort':\n                for prop in props:\n                    if prop in self.properties:\n                        if props[prop] not in [x.strip() for x in self.properties[prop].split(',')] and \\\n                                'any' not in self.properties[prop].lower():\n                            # Invalid property selected\n                            raise exceptions.IncompatibleDataPortConnectionPropsError\n                    for d in dests:\n                        if prop in d.properties:\n                            if props[prop] not in [x.strip() for x in d.properties[prop].split(',')] and \\\n                                    'any' not in d.properties[prop].lower():\n                                # Invalid property selected\n                                raise exceptions.IncompatibleDataPortConnectionPropsError\n            if not name:\n                name = self.name + '_'.join([d.name for d in dests])\n            props = utils.dict_to_nvlist(props)\n            profile = RTC.ConnectorProfile(name, id,\n                    [self._obj] + [d._obj for d in dests], props)\n            return_code, profile = self._obj.connect(profile)\n            if return_code != RTC.RTC_OK:\n                raise exceptions.FailedToConnectError(return_code)\n            self.reparse_connections()\n            for d in dests:\n                d.reparse_connections()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisconnect all connections to this port.", "response": "def disconnect_all(self):\n        '''Disconnect all connections to this port.'''\n        with self._mutex:\n            for conn in self.connections:\n                self.object.disconnect(conn.id)\n            self.reparse_connections()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection_by_dest(self, dest):\n        '''DEPRECATED. Search for a connection between this and another port.'''\n        with self._mutex:\n            for conn in self.connections:\n                if conn.has_port(self) and conn.has_port(dest):\n                    return conn\n            return None", "response": "DEPRECATED. Search for a connection between this and dest."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for all connections between this and dest.", "response": "def get_connections_by_dest(self, dest):\n        '''Search for all connections between this and another port.'''\n        with self._mutex:\n            res = []\n            for c in self.connections:\n                if c.has_port(self) and c.has_port(dest):\n                    res.append(c)\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for all connections involving this and all other ports.", "response": "def get_connections_by_dests(self, dests):\n        '''Search for all connections involving this and all other ports.'''\n        with self._mutex:\n            res = []\n            for c in self.connections:\n                if not c.has_port(self):\n                    continue\n                has_dest = False\n                for d in dests:\n                    if c.has_port(d):\n                        has_dest = True\n                        break\n                if has_dest:\n                    res.append(c)\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching for a connection on this port by its ID.", "response": "def get_connection_by_id(self, id):\n        '''Search for a connection on this port by its ID.'''\n        with self._mutex:\n            for conn in self.connections:\n                if conn.id == id:\n                    return conn\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for a connection to or from this port by name.", "response": "def get_connection_by_name(self, name):\n        '''Search for a connection to or from this port by name.'''\n        with self._mutex:\n            for conn in self.connections:\n                if conn.name == name:\n                    return conn\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects this port to another DataPorts.", "response": "def connect(self, dests=[], name=None, id='', props={}):\n        '''Connect this port to other DataPorts.\n\n        After the connection has been made, a delayed reparse of the\n        connections for this and the destination port will be triggered.\n\n        @param dests A list of the destination Port objects. Must be provided.\n        @param name The name of the connection. If None, a suitable default\n                    will be created based on the names of the two ports.\n        @param id The ID of this connection. If None, one will be generated by\n               the RTC implementation.\n        @param props Properties of the connection. Suitable defaults will be\n                     set for required values if they are not already present.\n        @raises WrongPortTypeError\n\n        '''\n        # Data ports can only connect to opposite data ports\n        with self._mutex:\n            new_props = props.copy()\n            ptypes = [d.porttype for d in dests]\n            if self.porttype == 'DataInPort':\n                if 'DataOutPort' not in ptypes:\n                    raise exceptions.WrongPortTypeError\n            if self.porttype == 'DataOutPort':\n                if 'DataInPort' not in ptypes:\n                    raise exceptions.WrongPortTypeError\n            if 'dataport.dataflow_type' not in new_props:\n                new_props['dataport.dataflow_type'] = 'push'\n            if 'dataport.interface_type' not in new_props:\n                new_props['dataport.interface_type'] = 'corba_cdr'\n            if 'dataport.subscription_type' not in new_props:\n                new_props['dataport.subscription_type'] = 'new'\n            if 'dataport.data_type' not in new_props:\n                new_props['dataport.data_type'] = \\\n                        self.properties['dataport.data_type']\n            super(DataPort, self).connect(dests=dests, name=name, id=id,\n                                          props=new_props)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, dests=None, name=None, id='', props={}):\n        '''Connect this port to other CorbaPorts.\n\n        After the connection has been made, a delayed reparse of the\n        connections for this and the destination port will be triggered.\n\n        @param dests A list of the destination Port objects. Must be provided.\n        @param name The name of the connection. If None, a suitable default\n                    will be created based on the names of the two ports.\n        @param id The ID of this connection. If None, one will be generated by\n               the RTC implementation.\n        @param props Properties of the connection. Suitable defaults will be\n                     set for required values if they are not already present.\n        @raises WrongPortTypeError, MismatchedInterfacesError,\n                MismatchedPolarityError\n\n        '''\n        with self._mutex:\n            # Corba ports can only connect to corba ports of the opposite\n            # polarity\n            for d in dests:\n                if not d.porttype == 'CorbaPort':\n                    raise exceptions.WrongPortTypeError\n            # Check the interfaces and their respective polarities match\n            if self.interfaces:\n                for d in dests:\n                    if not d.interfaces:\n                        raise exceptions.MismatchedInterfacesError\n                for intf in self.interfaces:\n                    for d in dests:\n                        match = d.get_interface_by_instance_name(\n                                    intf.instance_name)\n                        if not match:\n                            raise exceptions.MismatchedInterfacesError\n                        if intf.polarity == match.polarity:\n                            # Polarity should be opposite\n                            raise exceptions.MismatchedPolarityError\n            else:\n                for d in dests:\n                    if d.interfaces:\n                        raise exceptions.MismatchedInterfacesError\n            # Make the connection\n            new_props = props.copy()\n            if 'port.port_type' not in new_props:\n                new_props['port.port_type'] = 'CorbaPort'\n            super(CorbaPort, self).connect(dests=dests, name=name, id=id,\n                                           props=new_props)", "response": "Connect this port to another CorbaPorts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an interface of this port by instance name.", "response": "def get_interface_by_instance_name(self, name):\n        '''Get an interface of this port by instance name.'''\n        with self._mutex:\n            for intf in self.interfaces:\n                if intf.instance_name == name:\n                    return intf\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the polarity of this interface as a string.", "response": "def polarity_as_string(self, add_colour=True):\n        '''Get the polarity of this interface as a string.\n\n        @param add_colour If True, ANSI colour codes will be added to the\n                          string.\n        @return A string describing the polarity of this interface.\n\n        '''\n        with self._mutex:\n            if self.polarity == self.PROVIDED:\n                result = 'Provided', ['reset']\n            elif self.polarity == self.REQUIRED:\n                result = 'Required', ['reset']\n            if add_colour:\n                return utils.build_attr_string(result[1], supported=add_colour) + \\\n                        result[0] + utils.build_attr_string('reset',\n                                supported=add_colour)\n            else:\n                return result[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if this connection involves the given Port object.", "response": "def has_port(self, port):\n        '''Return True if this connection involves the given Port object.\n\n        @param port The Port object to search for in this connection's ports.\n\n        '''\n        with self._mutex:\n            for p in self.ports:\n                if not p[1]:\n                    # Port owner not in tree, so unknown\n                    continue\n                if port.object._is_equivalent(p[1].object):\n                    return True\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets musical analysis of the song using the librosa library.", "response": "def analysis(self):\n        \"\"\"Get musical analysis of the song using the librosa library\n        \"\"\"\n        if self._analysis is not None:\n            return self._analysis\n\n        if self.cache_dir is not None:\n            path = os.path.join(self.cache_dir, self.checksum)\n            try:\n                if self.refresh_cache: raise IOError\n                with open(path + '.pickle', 'rb') as pickle_file:\n                    self._analysis = pickle.load(pickle_file)\n            except IOError:\n                self._analysis = librosa_analysis.analyze_frames(self.all_as_mono(), self.samplerate)\n                with open(path + '.pickle', 'wb') as pickle_file:\n                    pickle.dump(self._analysis, pickle_file, pickle.HIGHEST_PROTOCOL)\n        else:\n            self._analysis = librosa_analysis.analyze_frames(self.all_as_mono(), self.samplerate)\n        return self._analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef breeding_pups(request, breeding_id):\n    breeding = get_object_or_404(Breeding, pk=breeding_id)\n    PupsFormSet = inlineformset_factory(Breeding, Animal, extra=10, exclude=('Notes','Alive', 'Death', 'Cause_of_Death', 'Father', 'Mother', 'Breeding'))\n    if request.method == \"POST\":\n        formset = PupsFormSet(request.POST, instance=breeding)\n        if formset.is_valid():\n            formset.save()\n            return HttpResponseRedirect( breeding.get_absolute_url() )            \n    else:\n        formset = PupsFormSet(instance=breeding)\n    return render(\"breeding_pups.html\", {\"formset\":formset, 'breeding':breeding})", "response": "This view is used to generate a form that can be used to add pups to a breeding set. It returns a form that can be used to add initial information about pups."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef breeding_wean(request, breeding_id):\n    breeding = Breeding.objects.get(id=breeding_id)\n    strain = breeding.Strain\n    PupsFormSet = inlineformset_factory(Breeding, Animal, extra=0, exclude=('Alive','Father', 'Mother', 'Breeding', 'Notes','Rack','Rack_Position','Strain','Background','Genotype','Death','Cause_of_Death','Backcross','Generation'))\n    if request.method ==\"POST\":\n        formset = PupsFormSet(request.POST, instance=breeding, queryset=Animal.objects.filter(Alive=True, Weaned__isnull=True))\n        if formset.is_valid():\n            formset.save()\n            return HttpResponseRedirect( breeding.get_absolute_url() )\n    else:\n        formset = PupsFormSet(instance=breeding, queryset=Animal.objects.filter(Alive=True, Weaned__isnull=True))\n    return render(request, \"breeding_wean.html\", {\"formset\":formset, 'breeding':breeding})", "response": "This view generates a formset that can be used to wean existing pups."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dispatch(self, *args, **kwargs):\n        return super(AnimalList, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context_data(self, **kwargs):\n        \n        context = super(AnimalListAlive, self).get_context_data(**kwargs)\n        context['list_type'] = 'Alive'\n        return context", "response": "This add in the context of list_type and returns this as Alive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dispatch(self, *args, **kwargs):\n        return super(AnimalUpdate, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_context_data(self, **kwargs):\n        \n        context = super(StrainList, self).get_context_data(**kwargs)\n        context['strain_list_alive'] = Strain.objects.filter(animal__Alive=True).annotate(alive=Count('animal'))\n        context['cages'] = Animal.objects.filter(Alive=True).values(\"Cage\")        \n        return context", "response": "This add in the context of strain_list_alive which filters for all alive animals and cages which filters for the number of current cages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_context_data(self, **kwargs):\n        \n        strain = super(StrainDetail, self).get_object()\n        context = super(StrainDetail, self).get_context_data(**kwargs)\n        context['breeding_cages'] = Breeding.objects.filter(Strain=strain)\n        context['animal_list'] = Animal.objects.filter(Strain=strain).order_by('Background','Genotype')\n        context['cages'] = Animal.objects.filter(Strain=strain).values(\"Cage\").distinct()\n        context['active'] = False        \n        return context", "response": "This adds into the context of strain_list_all which filters for all alive : class : ~mousedb. animal. models. Animal objects and active cages which filters for all alive : class : ~mousedb. animal. models. Animal objects and active cages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dispatch(self, *args, **kwargs):\n        return super(StrainUpdate, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dispatch(self, *args, **kwargs):\n        return super(StrainDelete, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_context_data(self, **kwargs):\n        \n        context = super(BreedingList, self).get_context_data(**kwargs)\n        context['breeding_type'] = \"Active\" \n        return context", "response": "This adds into the context of breeding_type and sets it to Active."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dispatch(self, *args, **kwargs):\n        return super(BreedingCreate, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch(self, *args, **kwargs):\n        return super(BreedingUpdate, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dispatch(self, *args, **kwargs):\n        return super(AnimalYearArchive, self).dispatch(*args, **kwargs)", "response": "This decorator sets this view to have restricted permissions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef queryset(self):\n        \n        from mousedb.animal.models import CROSS_TYPE    \n        crosstype_reverse = dict((v, k) for k, v in CROSS_TYPE)\n        try:\n            crosstype = crosstype_reverse[self.kwargs['breeding_type']]\n        except KeyError:\n            raise Http404\n        strain = get_object_or_404(Strain, Strain_slug=self.kwargs['strain_slug'])\n        if strain:\n            return Animal.objects.filter(Strain=strain,Breeding__Crosstype=crosstype)\n        else:\n            raise Http404", "response": "This function sets the queryset according to the keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_context_data(self, **kwargs):\n        \n        context = super(CrossTypeAnimalList, self).get_context_data(**kwargs)\n        context['list_type'] = self.kwargs['breeding_type']\n        return context", "response": "This add in the context of list_type and returns this as whatever the crosstype was."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd to the context all issues conditions and treatments.", "response": "def get_context_data(self, **kwargs):\n        '''Adds to the context all issues, conditions and treatments.'''\n        context = super(VeterinaryHome, self).get_context_data(**kwargs)\n        context['medical_issues'] = MedicalIssue.objects.all()\n        context['medical_conditions'] = MedicalCondition.objects.all()\n        context['medical_treatments'] = MedicalTreatment.objects.all()               \n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads n frames from the track starting with the current frame.", "response": "def read_frames(self, n, channels=None):\n        \"\"\"Read ``n`` frames from the track, starting\n        with the current frame\n\n        :param integer n: Number of frames to read\n        :param integer channels: Number of channels to return (default\n            is number of channels in track)\n        :returns: Next ``n`` frames from the track, starting with ``current_frame``\n        :rtype: numpy array\n        \"\"\"\n        if channels is None:\n            channels = self.channels\n\n        if channels == 1:\n            out = np.zeros(n)\n        elif channels == 2:\n            out = np.zeros((n, 2))\n        else:\n            print \"Input needs to be 1 or 2 channels\"\n            return\n        if n > self.remaining_frames():\n            print \"Trying to retrieve too many frames!\"\n            print \"Asked for\", n\n            n = self.remaining_frames()\n            print \"Returning\", n\n\n        if self.channels == 1 and channels == 1:\n            out = self.sound.read_frames(n)\n        elif self.channels == 1 and channels == 2:\n            frames = self.sound.read_frames(n)\n            out = np.vstack((frames.copy(), frames.copy())).T\n        elif self.channels == 2 and channels == 1:\n            frames = self.sound.read_frames(n)\n            out = np.mean(frames, axis=1)\n        elif self.channels == 2 and channels == 2:\n            out[:n, :] = self.sound.read_frames(n)\n\n        self.current_frame += n\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_frame(self, n):\n        self.sound.seek(n)\n        self._current_frame = n", "response": "Sets the current frame to n."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef range_as_mono(self, start_sample, end_sample):\n        tmp_current = self.current_frame\n        self.current_frame = start_sample\n        tmp_frames = self.read_frames(end_sample - start_sample)\n        if self.channels == 2:\n            frames = np.mean(tmp_frames, axis=1)\n        elif self.channels == 1:\n            frames = tmp_frames\n        else:\n            raise IOError(\"Input audio must have either 1 or 2 channels\")\n        self.current_frame = tmp_current\n        return frames", "response": "Get a range of frames as 1 combined channel\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loudest_time(self, start=0, duration=0):\n        if duration == 0:\n            duration = self.sound.nframes\n        self.current_frame = start\n        arr = self.read_frames(duration)\n        # get the frame of the maximum amplitude\n        # different names for the same thing...\n        # max_amp_sample = a.argmax(axis=0)[a.max(axis=0).argmax()]\n        max_amp_sample = int(np.floor(arr.argmax()/2)) + start\n        return max_amp_sample", "response": "Find the loudest time in the window given by start and duration."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind nearest zero crossing in waveform before frame n", "response": "def zero_crossing_before(self, n):\n        \"\"\"Find nearest zero crossing in waveform before frame ``n``\"\"\"\n        n_in_samples = int(n * self.samplerate)\n\n        search_start = n_in_samples - self.samplerate\n        if search_start < 0:\n            search_start = 0\n\n        frame = zero_crossing_last(\n            self.range_as_mono(search_start, n_in_samples)) + search_start\n\n        return frame / float(self.samplerate)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zero_crossing_after(self, n):\n        n_in_samples = int(n * self.samplerate)\n        search_end = n_in_samples + self.samplerate\n        if search_end > self.duration:\n            search_end = self.duration\n\n        frame = zero_crossing_first(\n            self.range_as_mono(n_in_samples, search_end)) + n_in_samples\n\n        return frame / float(self.samplerate)", "response": "Find nearest zero crossing in waveform after frame n."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label(self, t):\n        if self.labels is None:\n            return None\n        prev_label = None\n        for l in self.labels:\n            if l.time > t:\n                break\n            prev_label = l\n        if prev_label is None:\n            return None\n        return prev_label.name", "response": "Get the label of the song at a given time in seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_param(self, param, value):\n        '''Set a parameter in this configuration set.'''\n        self.data[param] = value\n        self._object.configuration_data = utils.dict_to_nvlist(self.data)", "response": "Set a parameter in this configuration set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reload(self, object, description, data):\n        '''Reload the configuration set data.'''\n        self._object = object\n        self._description = description\n        self._data = data", "response": "Reload the configuration set data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setting(self):\n        prog_type = self.__program.program_type\n        return self._setting / self.SETTING_DIVIDES[prog_type]", "response": "Load setting of the current process"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef partial_steps_data(self, start=0):\n        cnt = 0\n        if len(self._prog_steps) >= start:\n            # yields actual steps for encoding\n            for step in self._prog_steps[start:start+5]:\n                yield((step.raw_data))\n                cnt += 1\n        while cnt < 5:\n            yield((0, 0))\n            cnt += 1", "response": "Yields 5 steps from start position and provides tuple for packing into buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a step to the program.", "response": "def add_step(self, setting, duration):\n        \"\"\"\n        Adds steps to a program.\n        :param setting: Current, Wattage or Resistance, depending on program mode.\n        :param duration: Length of step in seconds.\n        :return: None\n        \"\"\"\n        if len(self._prog_steps) < 10:\n            self._prog_steps.append(ProgramStep(self, setting, duration))\n        else:\n            raise IndexError(\"Maximum of 10 steps are allowed\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_buffer_one_to_five(self, out_buffer):\n        struct.pack_into(b\"< 2B\", out_buffer, 3, self._program_type, len(self._prog_steps))\n        offset = 5\n        for ind, step in enumerate(self.partial_steps_data(0)):\n            struct.pack_into(b\"< 2H\", out_buffer, offset + ind*4, step[0], step[1])", "response": "Load the first 5 bytes of a program buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the second program buffer with everything but first three bytes and checksum", "response": "def load_buffer_six_to_ten(self, out_buffer):\n        \"\"\"\n        Loads second program buffer (0x94) with everything but\n        first three bytes and checksum\n        \"\"\"\n        offset = 3\n        for ind, step in enumerate(self.partial_steps_data(5)):\n            struct.pack_into(b\"< 2H\", out_buffer, offset + ind*4, step[0], step[1])\n        struct.pack_into(b\"< B x\", out_buffer, 23, self._program_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_load_resistance(self, resistance):\n        new_val = int(round(resistance * 100))\n        if not 0 <= new_val <= 50000:\n            raise ValueError(\"Load Resistance should be between 0-500 ohms\")\n        self._load_mode = self.SET_TYPE_RESISTANCE\n        self._load_value = new_val\n        self.__set_parameters()", "response": "Changes load to resistance mode and sets resistance value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_load_power(self, power_watts):\n        new_val = int(round(power_watts * 10))\n        if not 0 <= new_val <= 2000:\n            raise ValueError(\"Load Power should be between 0-200 W\")\n        self._load_mode = self.SET_TYPE_POWER\n        self._load_value = new_val\n        self.__set_parameters()", "response": "Changes load to power mode and sets power value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange load mode and sets current value.", "response": "def set_load_current(self, current_amps):\n        \"\"\"\n        Changes load to current mode and sets current value.\n        Rounds to nearest mA.\n\n        :param current_amps: Current in Amps (0-30A)\n        :return: None\n        \"\"\"\n        new_val = int(round(current_amps * 1000))\n        if not 0 <= new_val <= 30000:\n            raise ValueError(\"Load Current should be between 0-30A\")\n        self._load_mode = self.SET_TYPE_CURRENT\n        self._load_value = new_val\n        self.__set_parameters()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the checksum on the last byte of buffer based on values in the buffer", "response": "def __set_checksum(self):\n        \"\"\"\n        Sets the checksum on the last byte of buffer,\n        based on values in the buffer\n        :return: None\n        \"\"\"\n        checksum = self.__get_checksum(self.__out_buffer.raw)\n        self.STRUCT_CHECKSUM.pack_into(self.__out_buffer, self.OFFSET_CHECKSUM, checksum)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __clear_in_buffer(self):\n        self.__in_buffer.value = bytes(b'\\0' * len(self.__in_buffer))", "response": "Clear the in buffer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __send_buffer(self):\n        bytes_written = self.serial.write(self.__out_buffer.raw)\n        if self.DEBUG_MODE:\n            print(\"Wrote: '{}'\".format(binascii.hexlify(self.__out_buffer.raw)))\n        if bytes_written != len(self.__out_buffer):\n            raise IOError(\"{} bytes written for output buffer of size {}\".format(bytes_written,\n                                                                                 len(self.__out_buffer)))\n        return bytes_written", "response": "Sends the contents of self. __out_buffer to the serial device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __send_receive_buffer(self):\n        self.__clear_in_buffer()\n        self.__send_buffer()\n        read_string = self.serial.read(len(self.__in_buffer))\n        if self.DEBUG_MODE:\n            print(\"Read: '{}'\".format(binascii.hexlify(read_string)))\n        if len(read_string) != len(self.__in_buffer):\n            raise IOError(\"{} bytes received for input buffer of size {}\".format(len(read_string),\n                                                                                 len(self.__in_buffer)))\n        if not self.__is_valid_checksum(read_string):\n            raise IOError(\"Checksum validation failed on received data\")\n        self.__in_buffer.value = read_string", "response": "Sends a read from the input buffer and then clears the in_buffer and then calls the send_receive_buffer method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __set_parameters(self):\n        self.__set_buffer_start(self.CMD_SET_PARAMETERS)\n        # Can I send 0xFF as address to not change it each time?\n        # Worry about writing to EEPROM or Flash with each address change.\n        # Would then implement a separate address only change function.\n        self.STRUCT_SET_PARAMETERS.pack_into(self.__out_buffer, self.OFFSET_PAYLOAD,\n                                             self._max_current, self._max_power, self.address,\n                                             self._load_mode, self._load_value)\n        self.__set_checksum()\n        self.__send_buffer()\n        self.update_status()", "response": "Sets Load Parameters from class values including max current power address load mode load value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the status of the current system.", "response": "def update_status(self, retry_count=2):\n        \"\"\"\n        Updates current values from load.\n        Must be called to get latest values for the following properties of class:\n          current\n          voltage\n          power\n          max current\n          max power\n          resistance\n          local_control\n          load_on\n          wrong_polarity\n          excessive_temp\n          excessive_voltage\n          excessive_power\n\n        :param retry_count: Number of times to ignore IOErrors and retry update\n        :return: None\n        \"\"\"\n        # I think retry should be in here.\n        # Throw exceptions in __update_status and handle here\n        cur_count = max(retry_count, 0)\n        while cur_count >= 0:\n            try:\n                self.__update_status()\n            except IOError as err:\n                if self.print_errors:\n                    print(\"IOError: {}\".format(err))\n            else:\n                if not self.__is_valid_checksum(self.__in_buffer.raw):\n                    if self.print_errors:\n                        raise IOError(\"Checksum validation failed.\")\n                values = self.STRUCT_READ_VALUES_IN.unpack_from(self.__in_buffer, self.OFFSET_FRONT)\n                (self._current,\n                 self._voltage,\n                 self._power,\n                 self._max_current,\n                 self._max_power,\n                 self._resistance,\n                 output_state) = values[3:-1]\n\n                self._remote_control = (output_state & 0b00000001) > 0\n                self._load_on = (output_state & 0b00000010) > 0\n                self.wrong_polarity = (output_state & 0b00000100) > 0\n                self.excessive_temp = (output_state & 0b00001000) > 0\n                self.excessive_voltage = (output_state & 0b00010000) > 0\n                self.excessive_power = (output_state & 0b00100000) > 0\n                return None\n            cur_count -= 1\n        raise IOError(\"Retry count exceeded with serial IO.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the program sequence for the specified array of classes.", "response": "def set_program_sequence(self, array_program):\n        \"\"\"\n        Sets program up in load.\n        :param array_program: Populated Array3710Program object\n        :return: None\n        \"\"\"\n        self.__set_buffer_start(self.CMD_DEFINE_PROG_1_5)\n        array_program.load_buffer_one_to_five(self.__out_buffer)\n        self.__set_checksum()\n        self.__send_buffer()\n\n        self.__set_buffer_start(self.CMD_DEFINE_PROG_6_10)\n        array_program.load_buffer_six_to_ten(self.__out_buffer)\n        self.__set_checksum()\n        self.__send_buffer()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart running programmed test sequence.", "response": "def start_program(self, turn_on_load=True):\n        \"\"\"\n        Starts running programmed test sequence\n        :return: None\n        \"\"\"\n        self.__set_buffer_start(self.CMD_START_PROG)\n        self.__set_checksum()\n        self.__send_buffer()\n        # Turn on Load if not on\n        if turn_on_load and not self.load_on:\n            self.load_on = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops running programmed test sequence.", "response": "def stop_program(self, turn_off_load=True):\n        \"\"\"\n        Stops running programmed test sequence\n        :return: None\n        \"\"\"\n        self.__set_buffer_start(self.CMD_STOP_PROG)\n        self.__set_checksum()\n        self.__send_buffer()\n        if turn_off_load and self.load_on:\n            self.load_on = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the check on the base class and stores the result in a results deque.", "response": "def run(self):\n        \"\"\"\n        Calls the `perform()` method defined by subclasses and stores the\n        result in a `results` deque.\n\n        After the result is determined the `results` deque is analyzed to see\n        if the `passing` flag should be updated.  If the check was considered\n        passing and the previous `self.fall` number of checks failed, the check\n        is updated to not be passing.  If the check was not passing and the\n        previous `self.rise` number of checks passed, the check is updated to\n        be considered passing.\n        \"\"\"\n        logger.debug(\"Running %s check\", self.name)\n\n        try:\n            result = self.perform()\n        except Exception:\n            logger.exception(\"Error while performing %s check\", self.name)\n            result = False\n\n        logger.debug(\"Result: %s\", result)\n\n        self.results.append(result)\n        if self.passing and not any(self.last_n_results(self.fall)):\n            logger.info(\n                \"%s check failed %d time(s), no longer passing.\",\n                self.name, self.fall,\n            )\n            self.passing = False\n        if not self.passing and all(self.last_n_results(self.rise)):\n            logger.info(\n                \"%s check passed %d time(s), is now passing.\",\n                self.name, self.rise\n            )\n            self.passing = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef last_n_results(self, n):\n        return list(\n            itertools.islice(\n                self.results, len(self.results) - n, len(self.results)\n            )\n        )", "response": "Returns a list of the last n results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_config(self, config):\n        self.rise = int(config[\"rise\"])\n        self.fall = int(config[\"fall\"])\n\n        self.apply_check_config(config)\n\n        if self.results.maxlen == max(self.rise, self.fall):\n            return\n\n        results = list(self.results)\n        while len(results) > max(self.rise, self.fall):\n            results.pop(0)\n        while len(results) < max(self.rise, self.fall):\n            results.insert(0, False)\n\n        self.results = deque(\n            results,\n            maxlen=max(self.rise, self.fall)\n        )", "response": "Applies the given config to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_config(cls, config):\n        if \"rise\" not in config:\n            raise ValueError(\"No 'rise' configured\")\n        if \"fall\" not in config:\n            raise ValueError(\"No 'fall' configured\")\n\n        cls.validate_check_config(config)", "response": "Validate that required config entries are present."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vectorize(self, docs):\n        doc_core_sems, all_concepts = self._extract_core_semantics(docs)\n\n        shape = (len(docs), len(all_concepts))\n        vecs = np.zeros(shape)\n        for i, core_sems in enumerate(doc_core_sems):\n            for con, weight in core_sems:\n                j = all_concepts.index(con)\n                vecs[i,j] = weight\n\n        # Normalize\n        return vecs/np.max(vecs)", "response": "Vectorizes a list of documents using their DCS representations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply DCS to a document to extract its core concepts and their weights.", "response": "def _process_doc(self, doc):\n        \"\"\"\n        Applies DCS to a document to extract its core concepts and their weights.\n        \"\"\"\n        # Prep\n        doc = doc.lower()\n        tagged_tokens = [(t, penn_to_wordnet(t.tag_)) for t in spacy(doc, tag=True, parse=False, entity=False)]\n        tokens = [t for t, tag in tagged_tokens]\n        term_concept_map = self._disambiguate_doc(tagged_tokens)\n        concept_weights = self._weight_concepts(tokens, term_concept_map)\n\n        # Compute core semantics\n        lexical_chains = self._lexical_chains(doc, term_concept_map)\n        core_semantics = self._core_semantics(lexical_chains, concept_weights)\n        core_concepts = [c for chain in core_semantics for c in chain]\n\n        return [(con, concept_weights[con]) for con in core_concepts]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _disambiguate_doc(self, tagged_tokens):\n\n        # Group tokens by PoS\n        pos_groups = {pos: [] for pos in [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]}\n        for tok, tag in tagged_tokens:\n            if tag in pos_groups:\n                pos_groups[tag].append(tok)\n\n        #print(pos_groups)\n\n        # Map of final term -> concept mappings\n        map = {}\n        for tag, toks in pos_groups.items():\n            map.update(self._disambiguate_pos(toks, tag))\n\n        #nice_map = {k: map[k].lemma_names() for k in map.keys()}\n        #print(json.dumps(nice_map, indent=4, sort_keys=True))\n\n        return map", "response": "Disambiguates a list of tagged tokens and returns a dictionary mapping terms to their disambiguated concepts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _disambiguate_pos(self, terms, pos):\n        # Map the terms to candidate concepts\n        # Consider only the top 3 most common senses\n        candidate_map = {term: wn.synsets(term, pos=pos)[:3] for term in terms}\n\n        # Filter to unique concepts\n        concepts = set(c for cons in candidate_map.values() for c in cons)\n\n        # Back to list for consistent ordering\n        concepts = list(concepts)\n        sim_mat = self._similarity_matrix(concepts)\n\n        # Final map of terms to their disambiguated concepts\n        map = {}\n\n        # This is terrible\n        # For each term, select the candidate concept\n        # which has the maximum aggregate similarity score against\n        # all other candidate concepts of all other terms sharing the same PoS\n        for term, cons in candidate_map.items():\n            # Some words may not be in WordNet\n            # and thus have no candidate concepts, so skip\n            if not cons:\n                continue\n            scores = []\n            for con in cons:\n                i = concepts.index(con)\n                scores_ = []\n                for term_, cons_ in candidate_map.items():\n                    # Some words may not be in WordNet\n                    # and thus have no candidate concepts, so skip\n                    if term == term_ or not cons_:\n                        continue\n                    cons_idx = [concepts.index(c) for c in cons_]\n                    top_sim = max(sim_mat[i,cons_idx])\n                    scores_.append(top_sim)\n                scores.append(sum(scores_))\n            best_idx = np.argmax(scores)\n            map[term] = cons[best_idx]\n\n        return map", "response": "Disambiguates a list of tokens of a given PoS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _similarity_matrix(self, concepts):\n        n_cons = len(concepts)\n        sim_mat = np.zeros((n_cons, n_cons))\n        for i, c1 in enumerate(concepts):\n            for j, c2 in enumerate(concepts):\n                # Just build the lower triangle\n                if i >= j:\n                    sim_mat[i,j] = self._semsim(c1, c2) if i != j else 1.\n        return sim_mat + sim_mat.T - np.diag(sim_mat.diagonal())", "response": "Computes a semantic similarity matrix for a set of concepts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the semantic similarity between two concepts.", "response": "def _semsim(self, c1, c2):\n        \"\"\"\n        Computes the semantic similarity between two concepts.\n\n        The semantic similarity is a combination of two sem sims:\n\n            1. An \"explicit\" sem sim metric, that is, one which is directly\n            encoded in the WordNet graph. Here it is just Wu-Palmer similarity.\n\n            2. An \"implicit\" sem sim metric. See `_imp_semsim`.\n\n        Note we can't use the NLTK Wu-Palmer similarity implementation because we need to\n        incorporate the implicit sem sim, but it's fairly straightforward --\n        leaning on <http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html#Synset.wup_similarity>,\n        see that for more info. Though...the formula in the paper includes an extra term in the denominator,\n        which is wrong, so we leave it out.\n        \"\"\"\n        if c1 == c2:\n            return 1.\n\n        if (c1, c2) in self.concept_sims:\n            return self.concept_sims[(c1, c2)]\n\n        elif (c2, c1) in self.concept_sims:\n            return self.concept_sims[(c2, c1)]\n\n        else:\n            need_root = c1._needs_root()\n            subsumers = c1.lowest_common_hypernyms(c2, simulate_root=need_root)\n\n            if not subsumers:\n                # For relationships not in WordNet, fallback on just implicit semsim.\n                return self._imp_semsim(c1, c2)\n\n            subsumer = subsumers[0]\n            depth = subsumer.max_depth() + 1\n            len1 = c1.shortest_path_distance(subsumer, simulate_root=need_root)\n            len2 = c2.shortest_path_distance(subsumer, simulate_root=need_root)\n\n            if len1 is None or len2 is None:\n                # See above\n                return self._imp_semsim(c1, c2)\n\n            len1 += depth\n            len2 += depth\n\n            imp_score = self._imp_semsim(c1, c2)\n\n            sim = (2.*depth + imp_score)/(len1 + len2 + imp_score)\n            self.concept_sims[(c1, c2)] = sim\n            return sim"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _imp_semsim(self, c1, c2):\n\n        desc1 = self._description(c1)\n        desc2 = self._description(c2)\n\n        raw_sim = 1/(sift4(desc1, desc2) + 1)\n        return math.log(raw_sim + 1)", "response": "Returns the implicit semantic similarity between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _core_semantics(self, lex_chains, concept_weights):\n        chain_scores = [self._score_chain(lex_chain, adj_submat, concept_weights) for lex_chain, adj_submat in lex_chains]\n        scored_chains = zip(lex_chains, chain_scores)\n        scored_chains = sorted(scored_chains, key=lambda x: x[1], reverse=True)\n\n        thresh = (self.alpha/len(lex_chains)) * sum(chain_scores)\n        return [chain for (chain, adj_mat), score in scored_chains if score >= thresh][:self.n_chains]", "response": "Returns the n representative lexical chains for a document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_core_semantics(self, docs):\n        all_concepts = []\n        doc_core_sems = []\n        for doc in docs:\n            core_sems = self._process_doc(doc)\n            doc_core_sems.append(core_sems)\n            all_concepts += [con for con, weight in core_sems]\n        return doc_core_sems, list(set(all_concepts))", "response": "Extracts the core semantics for a list of documents returning them along with the concepts represented."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lexical_chains(self, doc, term_concept_map):\n        concepts = list({c for c in term_concept_map.values()})\n\n        # Build an adjacency matrix for the graph\n        # Using the encoding:\n        # 1 = identity/synonymy, 2 = hypernymy/hyponymy, 3 = meronymy, 0 = no edge\n        n_cons = len(concepts)\n        adj_mat = np.zeros((n_cons, n_cons))\n\n        for i, c in enumerate(concepts):\n            # TO DO can only do i >= j since the graph is undirected\n            for j, c_ in enumerate(concepts):\n                edge = 0\n                if c == c_:\n                    edge = 1\n                # TO DO when should simulate root be True?\n                elif c_ in c._shortest_hypernym_paths(simulate_root=False).keys():\n                    edge = 2\n                elif c in c_._shortest_hypernym_paths(simulate_root=False).keys():\n                    edge = 2\n                elif c_ in c.member_meronyms() + c.part_meronyms() + c.substance_meronyms():\n                    edge = 3\n                elif c in c_.member_meronyms() + c_.part_meronyms() + c_.substance_meronyms():\n                    edge = 3\n\n                adj_mat[i,j] = edge\n\n        # Group connected concepts by labels\n        concept_labels = connected_components(adj_mat, directed=False)[1]\n        lexical_chains = [([], []) for i in range(max(concept_labels) + 1)]\n        for i, concept in enumerate(concepts):\n            label = concept_labels[i]\n            lexical_chains[label][0].append(concept)\n            lexical_chains[label][1].append(i)\n\n        # Return the lexical chains as (concept list, adjacency sub-matrix) tuples\n        return [(chain, adj_mat[indices][:,indices]) for chain, indices in lexical_chains]", "response": "Builds the lexical chains for a given document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the score for a given lexical chain.", "response": "def _score_chain(self, lexical_chain, adj_submat, concept_weights):\n        \"\"\"\n        Computes the score for a lexical chain.\n        \"\"\"\n        scores = []\n\n        # Compute scores for concepts in the chain\n        for i, c in enumerate(lexical_chain):\n            score = concept_weights[c] * self.relation_weights[0]\n            rel_scores = []\n            for j, c_ in enumerate(lexical_chain):\n                if adj_submat[i,j] == 2:\n                    rel_scores.append(self.relation_weights[1] * concept_weights[c_])\n\n                elif adj_submat[i,j] == 3:\n                    rel_scores.append(self.relation_weights[2] * concept_weights[c_])\n\n            scores.append(score + sum(rel_scores))\n\n        # The chain's score is just the sum of its concepts' scores\n        return sum(scores)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating weights for concepts in a document.", "response": "def _weight_concepts(self, tokens, term_concept_map):\n        \"\"\"\n        Calculates weights for concepts in a document.\n\n        This is just the frequency of terms which map to a concept.\n        \"\"\"\n\n        weights = {c: 0 for c in term_concept_map.values()}\n        for t in tokens:\n            # Skip terms that aren't one of the PoS we used\n            if t not in term_concept_map:\n                continue\n            con = term_concept_map[t]\n            weights[con] += 1\n\n        # TO DO paper doesn't mention normalizing these weights...should we?\n        return weights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a description of a concept", "response": "def _description(self, concept):\n        \"\"\"\n        Returns a \"description\" of a concept,\n        as defined in the paper.\n\n        The paper describes the description as a string,\n        so this is a slight modification where we instead represent\n        the definition as a list of tokens.\n        \"\"\"\n        if concept not in self.descriptions:\n            lemmas = concept.lemma_names()\n            gloss = self._gloss(concept)\n            glosses = [self._gloss(rel) for rel in self._related(concept)]\n            raw_desc = ' '.join(lemmas + [gloss] + glosses)\n            desc = [w for w in raw_desc.split() if w not in stops]\n            self.descriptions[concept] = desc\n        return self.descriptions[concept]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _related(self, concept):\n        return concept.hypernyms() + \\\n                concept.hyponyms() + \\\n                concept.member_meronyms() + \\\n                concept.substance_meronyms() + \\\n                concept.part_meronyms() + \\\n                concept.member_holonyms() + \\\n                concept.substance_holonyms() + \\\n                concept.part_holonyms() + \\\n                concept.attributes() + \\\n                concept.also_sees() + \\\n                concept.similar_tos()", "response": "Returns a list of related concepts for a concept."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the base setup before any of the tasks are run.", "response": "def init_tasks():\n    \"\"\"\n    Performs basic setup before any of the tasks are run. All tasks needs to\n    run this before continuing. It only fires once.\n    \"\"\"\n\n    # Make sure exist are set\n    if \"exists\" not in env:\n        env.exists = exists\n\n    if \"run\" not in env:\n        env.run = run\n\n    if \"cd\" not in env:\n        env.cd = cd\n\n    if \"max_releases\" not in env:\n        env.max_releases = 5\n\n    if \"public_path\" in env:\n        public_path = env.public_path.rstrip(\"/\")\n        env.public_path = public_path\n\n    run_hook(\"init_tasks\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup():\n\n    init_tasks()\n\n    run_hook(\"before_setup\")\n\n    # Create shared folder\n    env.run(\"mkdir -p %s\" % (paths.get_shared_path()))\n    env.run(\"chmod 755 %s\" % (paths.get_shared_path()))\n\n    # Create backup folder\n    env.run(\"mkdir -p %s\" % (paths.get_backup_path()))\n    env.run(\"chmod 750 %s\" % (paths.get_backup_path()))\n\n    # Create uploads folder\n    env.run(\"mkdir -p %s\" % (paths.get_upload_path()))\n    env.run(\"chmod 775 %s\" % (paths.get_upload_path()))\n\n    run_hook(\"setup\")\n    run_hook(\"after_setup\")", "response": "Creates shared and upload directories then fires setup to recipes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new release and copies it to the current directory and runs the necessary hooks.", "response": "def deploy():\n    \"\"\"\n    Performs a deploy by invoking copy, then generating next release name and\n    invoking necessary hooks.\n    \"\"\"\n\n    init_tasks()\n\n    if not has_hook(\"copy\"):\n        return report(\"No copy method has been defined\")\n\n    if not env.exists(paths.get_shared_path()):\n        return report(\"You need to run setup before running deploy\")\n\n    run_hook(\"before_deploy\")\n\n    release_name = int(time.time()*1000)\n    release_path = paths.get_releases_path(release_name)\n\n    env.current_release = release_path\n\n    try:\n        run_hook(\"copy\")\n    except Exception as e:\n        return report(\"Error occurred on copy. Aborting deploy\", err=e)\n\n    if not env.exists(paths.get_source_path(release_name)):\n        return report(\"Source path not found '%s'\" %\n                      paths.get_source_path(release_name))\n\n    try:\n        run_hook(\"deploy\")\n    except Exception as e:\n        message = \"Error occurred on deploy, starting rollback...\"\n\n        logger.error(message)\n        logger.error(e)\n\n        run_task(\"rollback\")\n        return report(\"Error occurred on deploy\")\n\n    # Symlink current folder\n    paths.symlink(paths.get_source_path(release_name),\n                  paths.get_current_path())\n\n    # Clean older releases\n    if \"max_releases\" in env:\n        cleanup_releases(int(env.max_releases))\n\n    run_hook(\"after_deploy\")\n\n    if \"public_path\" in env:\n        paths.symlink(paths.get_source_path(release_name), env.public_path)\n\n    logger.info(\"Deploy complete\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nroll back to previous release", "response": "def rollback():\n    \"\"\"\n    Rolls back to previous release\n    \"\"\"\n\n    init_tasks()\n\n    run_hook(\"before_rollback\")\n\n    # Remove current version\n    current_release = paths.get_current_release_path()\n    if current_release:\n        env.run(\"rm -rf %s\" % current_release)\n\n    # Restore previous version\n    old_release = paths.get_current_release_name()\n    if old_release:\n        paths.symlink(paths.get_source_path(old_release),\n                      paths.get_current_path())\n\n    run_hook(\"rollback\")\n    run_hook(\"after_rollback\")\n\n    logger.info(\"Rollback complete\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_db(cls, db, force=False):\n        if force or db not in cls._cache:\n            cls._cache[db] = cls._new_from_db(db)\n        return cls._cache[db]", "response": "Make an instance of the class from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pipe_shell_cmds(shell_CMDs):\n    '''\n        shell_CMDs = {}\n        shell_CMDs[1] = 'netstat -n'\n        shell_CMDs[2] = \"awk {'print $6'}\"\n    '''\n    len = shell_CMDs.__len__()\n    p = {}\n    p[1] = subprocess.Popen(shlex.split(shell_CMDs[1]), stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n    for i in range(2,len):\n        p[i] = subprocess.Popen(shlex.split(shell_CMDs[i]), stdin=p[i-1].stdout, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n    if(len > 1):\n        p[len] = subprocess.Popen(shlex.split(shell_CMDs[len]), stdin=p[len-1].stdout, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n    result = p[len].communicate()\n    if(len > 1):\n        for i in range(2,len+1):\n            returncode = p[i].wait()\n    else:\n        returncode = p[len].wait()\n    return(result)", "response": "pipe shell commands to get all available resources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprune terms which are totally subsumed by a phrase.", "response": "def prune(tdocs):\n    \"\"\"\n    Prune terms which are totally subsumed by a phrase\n\n    This could be better if it just removes the individual keywords\n    that occur in a phrase for each time that phrase occurs.\n    \"\"\"\n    all_terms = set([t for toks in tdocs for t in toks])\n    terms = set()\n    phrases = set()\n    for t in all_terms:\n        if gram_size(t) > 1:\n            phrases.add(t)\n        else:\n            terms.add(t)\n\n    # Identify candidates for redundant terms (1-gram terms found in a phrase)\n    redundant = set()\n    for t in terms:\n        if any(t in ph for ph in phrases):\n            redundant.add(t)\n\n    # Search all documents to check that these terms occur\n    # only in a phrase. If not, remove it as a candidate.\n    # This could be more efficient\n    cleared = set()\n    for t in redundant:\n        if any(check_term(d, term=t) for d in tdocs):\n            cleared.add(t)\n\n    redundant = redundant.difference(cleared)\n\n    pruned_tdocs = []\n    for doc in tdocs:\n        pruned_tdocs.append([t for t in doc if t not in redundant])\n\n    return pruned_tdocs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_CLASS(prefix):\n    # latest class version and download link\n    args = (package_basedir, package_basedir, CLASS_VERSION, os.path.abspath(prefix))\n    command = 'sh %s/depends/install_class.sh %s %s %s' %args\n\n    ret = os.system(command)\n    if ret != 0:\n        raise ValueError(\"could not build CLASS v%s\" %CLASS_VERSION)", "response": "Function to dowwnload CLASS from github and build the library\n "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a number in the National September - 2001 format.", "response": "def format_number(number):\n    \"\"\"\n    >>> format_number(1)\n    1\n    >>> format_number(22)\n    22\n    >>> format_number(333)\n    333\n    >>> format_number(4444)\n    '4,444'\n    >>> format_number(55555)\n    '55,555'\n    >>> format_number(666666)\n    '666,666'\n    >>> format_number(7777777)\n    '7,777,777'\n    \"\"\"\n    char_list = list(str(number))\n    length = len(char_list)\n    if length <= 3:\n        return number\n\n    result = ''\n    if length % 3 != 0:\n        while len(char_list) % 3 != 0:\n            c = char_list[0]\n            result += c\n            char_list.remove(c)\n        result += ','\n\n    i = 0\n    while len(char_list) > 0:\n        c = char_list[0]\n        result += c\n        char_list.remove(c)\n        i += 1\n        if i % 3 == 0:\n            result += ','\n\n    return result[0:-1] if result[-1] == ',' else result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_key(url, page_number):\n    index = url.rfind('page')\n    if index != -1:\n        result = url[0:index]\n        result += 'page=%s' % page_number\n    else:\n        result = url\n        result += '&page=%s' % page_number\n    return result", "response": "Generate a key for the page number in the url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_pagination(total_page_num, current_page_num):\n    pagination = {'start': 1, 'end': PAGE_SIZE, 'current': current_page_num}\n\n    if total_page_num <= PAGE_SIZE:\n        pagination['end'] = total_page_num\n    else:\n        # base on front four and back five\n        pagination['start'] = current_page_num - 4\n        pagination['end'] = current_page_num + 5\n\n        if pagination['start'] < 1:\n            pagination['start'] = 1\n            pagination['end'] = PAGE_SIZE\n\n        if pagination['end'] > total_page_num:\n            pagination['end'] = total_page_num\n            pagination['start'] = total_page_num - 9\n\n    return pagination", "response": "Generate pagination for the current page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a PDF document by embedding the first page from the given image and writes some text to it.", "response": "def main(filename):\n    \"\"\"\n    Creates a PDF by embedding the first page from the given image and\n    writes some text to it.\n\n    @param[in] filename\n        The source filename of the image to embed.\n    \"\"\"\n\n    # Prepare font.\n    font_family = 'arial'\n    font = Font(font_family, bold=True)\n    if not font:\n        raise RuntimeError('No font found for %r' % font_family)\n\n    # Initialize PDF document on a stream.\n    with Document('output.pdf') as document:\n\n        # Initialize a new page and begin its context.\n        with document.Page() as ctx:\n\n            # Open the image to embed.\n            with Image(filename) as embed:\n\n                # Set the media box for the page to the same as the\n                # image to embed.\n                ctx.box = embed.box\n\n                # Embed the image.\n                ctx.embed(embed)\n\n            # Write some text.\n            ctx.add(Text('Hello World', font, size=14, x=100, y=60))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs-in to filemail as the current user.", "response": "def login(self, password):\n        \"\"\"Login to filemail as the current user.\n\n        :param password:\n        :type password: ``str``\n        \"\"\"\n\n        method, url = get_URL('login')\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'username': self.username,\n            'password': password,\n            'source': 'Desktop'\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if all transfers are complete.", "response": "def transfers_complete(self):\n        \"\"\"Check if all transfers are completed.\"\"\"\n\n        for transfer in self.transfers:\n            if not transfer.is_complete:\n                error = {\n                    'errorcode': 4003,\n                    'errormessage': 'You must complete transfer before logout.'\n                    }\n                hellraiser(error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sent(self, expired=False, for_all=False):\n\n        method, url = get_URL('get_sent')\n\n        payload = {\n            'apikey': self.session.cookies.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'getexpired': expired,\n            'getforallusers': for_all\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return self._restore_transfers(res)\n\n        hellraiser(res.json())", "response": "Retreve information on previously sent transfers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_user_info(self, save_to_config=True):\n\n        method, url = get_URL('user_get')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken')\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            settings = res.json()['user']\n\n            if save_to_config:\n                self.config.update(settings)\n\n            return settings\n\n        hellraiser(res)", "response": "Get user info and settings from Filemail."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_user_info(self, **kwargs):\n\n        if kwargs:\n            self.config.update(kwargs)\n\n        method, url = get_URL('user_update')\n\n        res = getattr(self.session, method)(url, params=self.config)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)", "response": "Update user info and settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves a list of transfers sent to you or your company members from other people.", "response": "def get_received(self, age=None, for_all=True):\n        \"\"\"Retrieve a list of transfers sent to you or your company\n         from other people.\n\n        :param age: between 1 and 90 days.\n        :param for_all: If ``True`` will return received files for\n         all users in the same business. (Available for business account\n         members only).\n        :type age: ``int``\n        :type for_all: ``bool``\n        :rtype: ``list`` of :class:`Transfer` objects.\n        \"\"\"\n\n        method, url = get_URL('received_get')\n\n        if age:\n            if not isinstance(age, int) or age < 0 or age > 90:\n                raise FMBaseError('Age must be <int> between 0-90')\n\n            past = datetime.utcnow() - timedelta(days=age)\n            age = timegm(past.utctimetuple())\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'getForAllUsers': for_all,\n            'from': age\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return self._restore_transfers(res)\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _restore_transfers(self, response):\n\n        transfers = []\n        for transfer_data in response.json()['transfers']:\n            transfer = Transfer(self, _restore=True)\n            transfer.transfer_info.update(transfer_data)\n            transfer.get_files()\n            transfers.append(transfer)\n\n        return transfers", "response": "Restore transfers from josn retreived Filemail\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting Filemail contact based on email.", "response": "def get_contact(self, email):\n        \"\"\"Get Filemail contact based on email.\n\n        :param email: address of contact\n        :type email: ``str``, ``unicode``\n        :rtype: ``dict`` with contact information\n        \"\"\"\n\n        contacts = self.get_contacts()\n        for contact in contacts:\n            if contact['email'] == email:\n                return contact\n\n        msg = 'No contact with email: \"{email}\" found.'\n        raise FMBaseError(msg.format(email=email))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates name and email for a specific contact.", "response": "def update_contact(self, contact):\n        \"\"\"Update name and/or email for contact.\n\n        :param contact: with updated info\n        :type contact: ``dict``\n        :rtype: ``bool``\n        \"\"\"\n\n        if not isinstance(contact, dict):\n            raise AttributeError('contact must be a <dict>')\n\n        method, url = get_URL('contacts_update')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'contactid': contact.get('contactid'),\n            'name': contact.get('name'),\n            'email': contact.get('email')\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets contact group by name.", "response": "def get_group(self, name):\n        \"\"\"Get contact group by name\n\n        :param name: name of group\n        :type name: ``str``, ``unicode``\n        :rtype: ``dict`` with group data\n        \"\"\"\n\n        groups = self.get_groups()\n        for group in groups:\n            if group['contactgroupname'] == name:\n                return group\n\n        msg = 'No group named: \"{name}\" found.'\n        raise FMBaseError(msg.format(name=name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_group(self, name):\n\n        group = self.get_group(name)\n\n        method, url = get_URL('group_delete')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'contactgroupid': group['contactgroupid']\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)", "response": "Delete contact group with name name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename contact group :param group: group data or name of group :param newname: of group :type group: ``str``, ``unicode``, ``dict`` :type newname: ``str``, ``unicode`` :rtype: ``bool``", "response": "def rename_group(self, group, newname):\n        \"\"\"Rename contact group\n\n        :param group: group data or name of group\n        :param newname: of group\n        :type group: ``str``, ``unicode``, ``dict``\n        :type newname: ``str``, ``unicode``\n        :rtype: ``bool``\n        \"\"\"\n\n        if isinstance(group, basestring):\n            group = self.get_contact(group)\n\n        method, url = get_URL('group_update')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'contactgroupid': group['contactgroupid'],\n            'name': newname\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_contact_to_group(self, contact, group):\n\n        if isinstance(contact, basestring):\n            contact = self.get_contact(contact)\n\n        if isinstance(group, basestring):\n            group = self.get_group(group)\n\n        method, url = get_URL('contacts_add_to_group')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'contactid': contact['contactid'],\n            'contactgroupid': group['contactgroupid']\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)", "response": "Add contact to group"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets company settings from Filemail", "response": "def get_company_info(self):\n        \"\"\"Get company settings from Filemail\n\n        :rtype: ``dict`` with company data\n        \"\"\"\n\n        method, url = get_URL('company_get')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken')\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return res.json()['company']\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_company(self, company):\n\n        if not isinstance(company, dict):\n            raise AttributeError('company must be a <dict>')\n\n        method, url = get_URL('company_update')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken')\n            }\n\n        payload.update(company)\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)", "response": "Update company settings\n\n        :param company: updated settings\n        :type company: ``dict``\n        :rtype: ``bool``"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_company_user(self, email):\n\n        users = self.get_company_users()\n        for user in users:\n            if user['email'] == email:\n                return user\n\n        msg = 'No user with email: \"{email}\" associated with this company.'\n        raise FMBaseError(msg.format(email=email))", "response": "Get company user based on email."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a user to the company account.", "response": "def company_add_user(self, email, name, password, receiver, admin):\n        \"\"\"Add a user to the company account.\n\n        :param email:\n        :param name:\n        :param password: Pass without storing in plain text\n        :param receiver: Can user receive files\n        :param admin:\n        :type email: ``str`` or ``unicode``\n        :type name: ``str`` or ``unicode``\n        :type password: ``str`` or ``unicode``\n        :type receiver: ``bool``\n        :type admin: ``bool``\n        :rtype: ``bool``\n        \"\"\"\n\n        method, url = get_URL('company_add_user')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'email': email,\n            'name': name,\n            'password': password,\n            'canreceivefiles': receiver,\n            'admin': admin\n            }\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating company user settings", "response": "def update_company_user(self, email, userdata):\n        \"\"\"Update a company users settings\n\n        :param email: current email address of user\n        :param userdata: updated settings\n        :type email: ``str`` or ``unicode``\n        :type userdata: ``dict``\n        :rtype: ``bool``\n        \"\"\"\n\n        if not isinstance(userdata, dict):\n            raise AttributeError('userdata must be a <dict>')\n\n        method, url = get_URL('company_update_user')\n\n        payload = {\n            'apikey': self.config.get('apikey'),\n            'logintoken': self.session.cookies.get('logintoken'),\n            'useremail': email\n            }\n\n        payload.update(userdata)\n\n        res = getattr(self.session, method)(url, params=payload)\n\n        if res.status_code == 200:\n            return True\n\n        hellraiser(res)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef col_widths(self):\n        # type: () -> defaultdict\n        \"\"\"Get MAX possible width of each column in the table.\n\n        :return: defaultdict\n        \"\"\"\n        _widths = defaultdict(int)\n\n        all_rows = [self.headers]\n        all_rows.extend(self._rows)\n\n        for row in all_rows:\n            for idx, col in enumerate(row):\n                _col_l = len(col)\n                if _col_l > _widths[idx]:\n                    _widths[idx] = _col_l\n\n        return _widths", "response": "Get MAX possible width of each column in the table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a correctly sized marker line. e. g. A marker line is a line that is appended to the end of the string.", "response": "def _marker_line(self):\n        # type: () -> str\n        \"\"\"Generate a correctly sized marker line.\n\n        e.g.\n\n        '+------------------+---------+----------+---------+'\n\n        :return: str\n        \"\"\"\n        output = ''\n        for col in sorted(self.col_widths):\n            line = self.COLUMN_MARK + (self.DASH * (self.col_widths[col] + self.PADDING * 2))\n            output += line\n        output += self.COLUMN_MARK + '\\n'\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _row_to_str(self, row):\n        # type: (List[str]) -> str\n        \"\"\"Converts a list of strings to a correctly spaced and formatted\n        row string.\n\n        e.g.\n\n        ['some', 'foo', 'bar'] --> '| some | foo |  bar  |'\n\n        :param row: list\n        :return: str\n        \"\"\"\n        _row_text = ''\n        for col, width in self.col_widths.items():\n            _row_text += self.COLUMN_SEP\n            l_pad, r_pad = self._split_int(width - len(row[col]))\n            _row_text += '{0}{1}{2}'.format(' ' * (l_pad + self.PADDING),\n                                            row[col],\n                                            ' ' * (r_pad + self.PADDING))\n\n        _row_text += self.COLUMN_SEP + '\\n'\n\n        return _row_text", "response": "Converts a list of strings to a correctly spaced and formatted unicode row string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _table_to_str(self):\n        # type: () -> str\n        \"\"\"Return single formatted table string.\n\n        :return: str\n        \"\"\"\n        _marker_line = self._marker_line()\n        output = _marker_line + self._row_to_str(self.headers) + _marker_line\n\n        for row in self._rows:\n            output += self._row_to_str(row)\n\n        output += _marker_line\n\n        return output", "response": "Return a string representation of the table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sift4(s1, s2, max_offset=5):\n    t1, t2 = list(s1), list(s2)\n    l1, l2 = len(t1), len(t2)\n\n    if not s1:\n        return l2\n\n    if not s2:\n        return l1\n\n    # Cursors for each string\n    c1, c2 = 0, 0\n\n    # Largest common subsequence\n    lcss = 0\n\n    # Local common substring\n    local_cs = 0\n\n    # Number of transpositions ('ab' vs 'ba')\n    trans = 0\n\n    # Offset pair array, for computing the transpositions\n    offsets = []\n\n    while c1 < l1 and c2 < l2:\n        if t1[c1] == t2[c2]:\n            local_cs += 1\n\n            # Check if current match is a transposition\n            is_trans = False\n            i = 0\n            while i < len(offsets):\n                ofs = offsets[i]\n                if c1 <= ofs['c1'] or c2 <= ofs['c2']:\n                    is_trans = abs(c2-c1) >= abs(ofs['c2'] - ofs['c1'])\n                    if is_trans:\n                        trans += 1\n                    elif not ofs['trans']:\n                        ofs['trans'] = True\n                        trans += 1\n                    break\n                elif c1 > ofs['c2'] and c2 > ofs['c1']:\n                    del offsets[i]\n                else:\n                    i += 1\n            offsets.append({\n                'c1': c1,\n                'c2': c2,\n                'trans': is_trans\n            })\n\n        else:\n            lcss += local_cs\n            local_cs = 0\n            if c1 != c2:\n                c1 = c2 = min(c1, c2)\n\n            for i in range(max_offset):\n                if c1 + i >= l1 and c2 + i >= l2:\n                    break\n                elif c1 + i < l1 and s1[c1+i] == s2[c2]:\n                    c1 += i - 1\n                    c2 -= 1\n                    break\n\n                elif c2 + i < l2 and s1[c1] == s2[c2 + i]:\n                    c2 += i - 1\n                    c1 -= 1\n                    break\n\n        c1 += 1\n        c2 += 1\n\n        if c1 >= l1 or c2 >= l2:\n            lcss += local_cs\n            local_cs = 0\n            c1 = c2 = min(c1, c2)\n\n    lcss += local_cs\n    return round(max(l1, l2) - lcss + trans)", "response": "This function is an implementation of general Sift4."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pick(self, filenames: Iterable[str]) -> str:\n        filenames = sorted(filenames, reverse=True)  # e.g., v2 before v1\n        for priority in sorted(self.rules.keys(), reverse=True):\n            patterns = self.rules[priority]\n            for pattern in patterns:\n                for filename in filenames:\n                    if pattern.search(filename):\n                        return filename\n        return filenames[0]", "response": "Pick one filename based on priority rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train_tf(tokens_stream, out=None, **kwargs):\n    print('Counting terms...')\n    results = parallel(count_tf, tokens_stream, n_jobs=-1)\n\n    print('Merging...')\n    tf = merge(results)\n\n    if out is not None:\n        with open(out, 'w') as f:\n            json.dump(tf, f)\n\n    return tf", "response": "Train a map of term frequencies on a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts the number of term frequencies for a single file.", "response": "def count_tf(tokens_stream):\n    \"\"\"\n    Count term frequencies for a single file.\n    \"\"\"\n    tf = defaultdict(int)\n    for tokens in tokens_stream:\n        for token in tokens:\n            tf[token] += 1\n    return tf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating a config dictionary parsed from a cluster config file.", "response": "def validate_config(cls, config):\n        \"\"\"\n        Validates a config dictionary parsed from a cluster config file.\n\n        Checks that a discovery method is defined and that at least one of\n        the balancers in the config are installed and available.\n        \"\"\"\n        if \"discovery\" not in config:\n            raise ValueError(\"No discovery method defined.\")\n\n        installed_balancers = Balancer.get_installed_classes().keys()\n\n        if not any([balancer in config for balancer in installed_balancers]):\n            raise ValueError(\"No available balancer configs defined.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies the given configuration to the current object.", "response": "def apply_config(self, config):\n        \"\"\"\n        Sets the `discovery` and `meta_cluster` attributes, as well as the\n        configured + available balancer attributes from a given validated\n        config.\n        \"\"\"\n        self.discovery = config[\"discovery\"]\n        self.meta_cluster = config.get(\"meta_cluster\")\n        for balancer_name in Balancer.get_installed_classes().keys():\n            if balancer_name in config:\n                setattr(self, balancer_name, config[balancer_name])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks filename extension to see if it s a video file.", "response": "def _is_video(filepath) -> bool:\n    \"\"\"Check filename extension to see if it's a video file.\"\"\"\n    if os.path.exists(filepath):  # Could be broken symlink\n        extension = os.path.splitext(filepath)[1]\n        return extension in ('.mkv', '.mp4', '.avi')\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_files(dirpath: str) -> 'Iterable[str]':\n    for dirpath, dirnames, filenames in os.walk(dirpath, topdown=True,\n                                                followlinks=True):\n        if os.path.basename(dirpath).startswith('.'):\n            del dirnames[:]\n        for filename in filenames:\n            yield os.path.join(dirpath, filename)", "response": "Find files recursively.\n\n    Returns a generator that yields paths in no particular order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new instance of that is the current peer of whichever host we re running on.", "response": "def current(cls):\n        \"\"\"\n        Helper method for getting the current peer of whichever host we're\n        running on.\n        \"\"\"\n        name = socket.getfqdn()\n        ip = socket.gethostbyname(name)\n\n        return cls(name, ip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self):\n        return json.dumps({\n            \"name\": self.name,\n            \"ip\": self.ip,\n            \"port\": self.port\n        }, sort_keys=True)", "response": "Serializes the peer data as a simple JSON map string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deserialize(cls, value):\n        parsed = json.loads(value)\n\n        if \"name\" not in parsed:\n            raise ValueError(\"No peer name.\")\n        if \"ip\" not in parsed:\n            raise ValueError(\"No peer IP.\")\n        if \"port\" not in parsed:\n            parsed[\"port\"] = DEFAULT_PEER_PORT\n\n        return cls(parsed[\"name\"], parsed[\"ip\"], parsed[\"port\"])", "response": "Deserialize a peer class from a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a default value for the object.", "response": "def default(self, obj):\n        \"\"\"\n        if input object is a ndarray it will be converted into a dict holding dtype, shape and the data base64 encoded\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            data_b64 = base64.b64encode(obj.data).decode('utf-8')\n            return dict(__ndarray__=data_b64,\n                        dtype=str(obj.dtype),\n                        shape=obj.shape)\n        elif sps.issparse(obj):\n            data_b64 = base64.b64encode(obj.data).decode('utf-8')\n            return dict(__ndarray__=data_b64,\n                        dtype=str(obj.dtype),\n                        shape=obj.shape,\n                        indices=obj.indices,\n                        indptr=obj.indptr)\n        elif hasattr(obj, '__dict__'):\n            return obj.__dict__\n        # Let the base class default method raise the TypeError\n        return json.JSONEncoder.default(self, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting a target value from meta data.", "response": "def _extract_meta_value(self, tag):\n        # type: (str, List[str]) -> str\n        \"\"\"Find a target value by `tag` from given meta data.\n\n        :param tag: str\n        :param meta_data: list\n        :return: str\n        \"\"\"\n        try:\n            return [l[len(tag):] for l in self.meta_data if l.startswith(tag)][0]\n        except IndexError:\n            return '* Not Found *'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting available markup options as list.", "response": "def get_markup_choices():\n    \"\"\"\n    Receives available markup options as list.\n    \"\"\"\n    available_reader_list = []\n    module_dir = os.path.realpath(os.path.dirname(__file__))\n    module_names = filter(\n        lambda x: x.endswith('_reader.py'), os.listdir(module_dir))\n\n    for module_name in module_names:\n        markup = module_name.split('_')[0]\n        reader = get_reader(markup=markup)\n\n        if reader.enabled is True:\n            available_reader_list.append((markup, reader.name))\n\n    return available_reader_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying the given config dictionary to the current instance s hosts and base_path attributes.", "response": "def apply_config(self, config):\n        \"\"\"\n        Takes the given config dictionary and sets the hosts and base_path\n        attributes.\n\n        If the kazoo client connection is established, its hosts list is\n        updated to the newly configured value.\n        \"\"\"\n        self.hosts = config[\"hosts\"]\n        old_base_path = self.base_path\n        self.base_path = config[\"path\"]\n        if not self.connected.is_set():\n            return\n\n        logger.debug(\"Setting ZK hosts to %s\", self.hosts)\n        self.client.set_hosts(\",\".join(self.hosts))\n\n        if old_base_path and old_base_path != self.base_path:\n            logger.critical(\n                \"ZNode base path changed!\" +\n                \" Lighthouse will need to be restarted\" +\n                \" to watch the right znodes\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self):\n        self.client = client.KazooClient(hosts=\",\".join(self.hosts))\n\n        self.client.add_listener(self.handle_connection_change)\n        self.client.start_async()", "response": "Connects to the Zookeeper server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisconnecting from the Zookeeper server.", "response": "def disconnect(self):\n        \"\"\"\n        Stops and closes the kazoo connection.\n        \"\"\"\n        logger.info(\"Disconnecting from Zookeeper.\")\n        self.client.stop()\n        self.client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_connection_change(self, state):\n        if state == client.KazooState.LOST:\n            if not self.shutdown.is_set():\n                logger.info(\"Zookeeper session lost!\")\n            self.connected.clear()\n        elif state == client.KazooState.SUSPENDED:\n            logger.info(\"Zookeeper connection suspended!\")\n            self.connected.clear()\n        else:\n            logger.info(\"Zookeeper connection (re)established.\")\n            self.connected.set()", "response": "Callback for handling connection changes in the kazoo client s state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_watching(self, cluster, callback):\n        logger.debug(\"starting to watch cluster %s\", cluster.name)\n        wait_on_any(self.connected, self.shutdown)\n        logger.debug(\"done waiting on (connected, shutdown)\")\n        znode_path = \"/\".join([self.base_path, cluster.name])\n\n        self.stop_events[znode_path] = threading.Event()\n\n        def should_stop():\n            return (\n                znode_path not in self.stop_events or\n                self.stop_events[znode_path].is_set() or\n                self.shutdown.is_set()\n            )\n\n        while not should_stop():\n            try:\n                if self.client.exists(znode_path):\n                    break\n            except exceptions.ConnectionClosedError:\n                break\n\n            wait_on_any(\n                self.stop_events[znode_path], self.shutdown,\n                timeout=NO_NODE_INTERVAL\n            )\n\n        logger.debug(\"setting up ChildrenWatch for %s\", znode_path)\n\n        @self.client.ChildrenWatch(znode_path)\n        def watch(children):\n            if should_stop():\n                return False\n\n            logger.debug(\"znode children changed! (%s)\", znode_path)\n\n            new_nodes = []\n            for child in children:\n                child_path = \"/\".join([znode_path, child])\n                try:\n                    new_nodes.append(\n                        Node.deserialize(self.client.get(child_path)[0])\n                    )\n                except ValueError:\n                    logger.exception(\"Invalid node at path '%s'\", child)\n                    continue\n\n            cluster.nodes = new_nodes\n\n            callback()", "response": "Starts watching the children of a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstop watching the given cluster.", "response": "def stop_watching(self, cluster):\n        \"\"\"\n        Causes the thread that launched the watch of the cluster path\n        to end by setting the proper stop event found in `self.stop_events`.\n        \"\"\"\n        znode_path = \"/\".join([self.base_path, cluster.name])\n        if znode_path in self.stop_events:\n            self.stop_events[znode_path].set()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef report_up(self, service, port):\n        wait_on_any(self.connected, self.shutdown)\n\n        node = Node.current(service, port)\n\n        path = self.path_of(service, node)\n        data = node.serialize().encode()\n\n        znode = self.client.exists(path)\n\n        if not znode:\n            logger.debug(\"ZNode at %s does not exist, creating new one.\", path)\n            self.client.create(path, value=data, ephemeral=True, makepath=True)\n        elif znode.owner_session_id != self.client.client_id[0]:\n            logger.debug(\"ZNode at %s not owned by us, recreating.\", path)\n            txn = self.client.transaction()\n            txn.delete(path)\n            txn.create(path, value=data, ephemeral=True)\n            txn.commit()\n        else:\n            logger.debug(\"Setting node value to %r\", data)\n            self.client.set(path, data)", "response": "Report the given service s present node as up by creating or updating its znode s data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report_down(self, service, port):\n        wait_on_any(self.connected, self.shutdown)\n\n        node = Node.current(service, port)\n\n        path = self.path_of(service, node)\n        try:\n            logger.debug(\"Deleting znode at %s\", path)\n            self.client.delete(path)\n        except exceptions.NoNodeError:\n            pass", "response": "Report the given service s present node as down by deleting the znode in Zookeeper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef path_of(self, service, node):\n        return \"/\".join([self.base_path, service.name, node.name])", "response": "Returns the Zookeeper path for a given cluster\n        member node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rating_score(obj, user):\n    if not user.is_authenticated() or not hasattr(obj, '_ratings_field'):\n        return False\n\n    ratings_descriptor = getattr(obj, obj._ratings_field)\n    try:\n        rating = ratings_descriptor.get(user=user).score\n    except ratings_descriptor.model.DoesNotExist:\n        rating = None\n\n    return rating", "response": "Returns the score a user has given an object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a link to rate the given object with the given score.", "response": "def rate_url(obj, score=1):\n    \"\"\"\n    Generates a link to \"rate\" the given object with the provided score - this\n    can be used as a form target or for POSTing via Ajax.\n    \"\"\"\n    return reverse('ratings_rate_object', args=(\n        ContentType.objects.get_for_model(obj).pk,\n        obj.pk,\n        score,\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unrate_url(obj):\n    return reverse('ratings_unrate_object', args=(\n        ContentType.objects.get_for_model(obj).pk,\n        obj.pk,\n    ))", "response": "Generates a link to un - rate the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef command(state, args):\n    args = parser.parse_args(args[1:])\n    aid = state.results.parse_aid(args.aid, default_key='db')\n    query.update.reset(state.db, aid, args.episode)", "response": "Reset anime watched episodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncancel a job. If the job is pending, it will be removed. If the job is running, it will be terminated.", "response": "def cancel_job(agent, project_name, job_id):\n    \"\"\"\n    cancel a job.\n    If the job is pending, it will be removed. If the job is running, it will be terminated.\n    \"\"\"\n    prevstate = agent.cancel(project_name, job_id)['prevstate']\n    if prevstate == 'pending':\n        sqllite_agent.execute(ScrapydJobExtInfoSQLSet.DELETE_BY_ID, (job_id,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef packing_job_ext_info(job_lsit_DO):\n    ext_info = sqllite_agent.execute(ScrapydJobExtInfoSQLSet.SELECT_BY_ID, (job_lsit_DO.job_id,))\n    if ext_info is None or len(ext_info) <= 0: return\n    ext_info = ext_info[0]\n    job_lsit_DO.args = ext_info[1]\n    job_lsit_DO.priority = ext_info[2]\n    job_lsit_DO.creation_time = ext_info[3]\n    job_lsit_DO.logs_name = str_to_list(ext_info[4], ',')\n    job_lsit_DO.logs_url = str_to_list(ext_info[5], ',')", "response": "Packing additional information of the job into the job_lsit_DO"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_job_list(agent):\n    project_list = agent.get_project_list()\n    if project_list['status'] == 'error':\n        raise ScrapydTimeoutException\n    project_list = project_list['projects']\n    pending_job_list = []\n    running_job_list = []\n    finished_job_list = []\n    for project_name in project_list:\n        job_list = agent.get_job_list(project_name)\n        # Extract latest version\n        project_version = agent.get_version_list(project_name)['versions'][-1:]\n        for pending_job in job_list['pending']:\n            pending_job_list.append(JobListDO(project_name=project_name,\n                                              project_version=project_version,\n                                              job_id=pending_job['id'],\n                                              spider_name=pending_job['spider'],\n                                              job_status=JobStatus.PENDING\n                                              ))\n        for running_job in job_list['running']:\n            running_job_list.append(JobListDO(project_name=project_name,\n                                              project_version=project_version,\n                                              job_id=running_job['id'],\n                                              spider_name=running_job['spider'],\n                                              start_time=running_job['start_time'],\n                                              job_status=JobStatus.RUNNING\n                                              ))\n        for finished_job in job_list['finished']:\n            finished_job_list.append(JobListDO(project_name=project_name,\n                                               project_version=project_version,\n                                               job_id=finished_job['id'],\n                                               spider_name=finished_job['spider'],\n                                               start_time=finished_job['start_time'],\n                                               end_time=finished_job['end_time'],\n                                               job_status=JobStatus.FINISHED\n                                               ))\n\n    return pending_job_list, running_job_list, finished_job_list", "response": "Get all job list by each project name then\n    Return the list of all jobs in the order they were created."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_job_amounts(agent, project_name, spider_name=None):\n    job_list = agent.get_job_list(project_name)\n    pending_job_list = job_list['pending']\n    running_job_list = job_list['running']\n    finished_job_list = job_list['finished']\n    job_amounts = {}\n    if spider_name is None:\n        job_amounts['pending'] = len(pending_job_list)\n        job_amounts['running'] = len(running_job_list)\n        job_amounts['finished'] = len(finished_job_list)\n    else:\n        job_amounts['pending'] = len([j for j in pending_job_list if j['spider'] == spider_name])\n        job_amounts['running'] = len([j for j in running_job_list if j['spider'] == spider_name])\n        job_amounts['finished'] = len([j for j in finished_job_list if j['spider'] == spider_name])\n\n    return job_amounts", "response": "Get amounts that pending job amount running job amount finished job amount."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef corba_name_to_string(name):\n    '''Convert a CORBA CosNaming.Name to a string.'''\n    parts = []\n    if type(name) is not list and type(name) is not tuple:\n        raise NotCORBANameError(name)\n    if len(name) == 0:\n        raise NotCORBANameError(name)\n\n    for nc in name:\n        if not nc.kind:\n            parts.append(nc.id)\n        else:\n            parts.append('{0}.{1}'.format(nc.id, nc.kind))\n    return '/'.join(parts)", "response": "Convert a CORBA CosNaming. Name to a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reparse(self):\n        '''Reparse all children of this directory.\n\n        This effectively rebuilds the tree below this node.\n\n        This operation takes an unbounded time to complete; if there are a lot\n        of objects registered below this directory's context, they will all\n        need to be parsed.\n\n        '''\n        self._remove_all_children()\n        self._parse_context(self._context, self.orb)", "response": "Reparse all children of this directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunbind an object from the context represented by this directory.", "response": "def unbind(self, name):\n        '''Unbind an object from the context represented by this directory.\n\n        Warning: this is a dangerous operation. You may unlink an entire\n        section of the tree and be unable to recover it. Be careful what you\n        unbind.\n\n        The name should be in the format used in paths. For example,\n        'manager.mgr' or 'ConsoleIn0.rtc'.\n\n        '''\n        with self._mutex:\n            id, sep, kind = name.rpartition('.')\n            if not id:\n                id = kind\n                kind = ''\n            name = CosNaming.NameComponent(id=str(id), kind=str(kind))\n            try:\n                self.context.unbind([name])\n            except CosNaming.NamingContext.NotFound:\n                raise exceptions.BadPathError(name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenize(self, docs):\n        if self.n_jobs == 1:\n            return [self._tokenize(doc) for doc in docs]\n        else:\n            return parallel(self._tokenize, docs, self.n_jobs)", "response": "Tokenizes a text document using a lemmatizer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(codelabel, submit):\n    code = Code.get_from_string(codelabel)\n\n    # set up calculation\n    calc = code.new_calc()\n    calc.label = \"compute rips from distance matrix\"\n    calc.set_max_wallclock_seconds(1 * 60)\n    calc.set_withmpi(False)\n    calc.set_resources({\"num_machines\": 1, \"num_mpiprocs_per_machine\": 1})\n\n    # Prepare input parameters\n    from aiida.orm import DataFactory\n    Parameters = DataFactory('gudhi.rdm')\n    parameters = Parameters(dict={'max-edge-length': 4.2})\n    calc.use_parameters(parameters)\n\n    SinglefileData = DataFactory('singlefile')\n    distance_matrix = SinglefileData(\n        file=os.path.join(gt.TEST_DIR, 'sample_distance.matrix'))\n    calc.use_distance_matrix(distance_matrix)\n\n    if submit:\n        calc.store_all()\n        calc.submit()\n        print(\"submitted calculation; calc=Calculation(uuid='{}') # ID={}\"\\\n                .format(calc.uuid,calc.dbnode.pk))\n    else:\n        subfolder, script_filename = calc.submit_test()\n        path = os.path.relpath(subfolder.abspath)\n        print(\"submission test successful\")\n        print(\"Find remote folder in {}\".format(path))\n        print(\"In order to actually submit, add '--submit'\")", "response": "This script extends submit. py and adds flexibility in the selected code and computer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering watching regexp for an anime.", "response": "def command(state, args):\n    \"\"\"Register watching regexp for an anime.\"\"\"\n    args = parser.parse_args(args[1:])\n    aid = state.results.parse_aid(args.aid, default_key='db')\n    if args.query:\n        # Use regexp provided by user.\n        regexp = '.*'.join(args.query)\n    else:\n        # Make default regexp.\n        title = query.select.lookup(state.db, aid, fields=['title']).title\n        # Replace non-word, non-whitespace with whitespace.\n        regexp = re.sub(r'[^\\w\\s]', ' ', title)\n        # Split on whitespace and join with wildcard regexp.\n        regexp = '.*?'.join(re.escape(x) for x in regexp.split())\n        # Append episode matching regexp.\n        regexp = '.*?'.join((\n            regexp,\n            r'\\b(?P<ep>[0-9]+)(v[0-9]+)?',\n        ))\n    query.files.set_regexp(state.db, aid, regexp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd all columns from the specified table added in the specified version", "response": "def from_definition(self, table: Table, version: int):\n        \"\"\"Add all columns from the table added in the specified version\"\"\"\n        self.table(table)\n        self.add_columns(*table.columns.get_with_version(version))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keygen(self, keyBitLength, rawKey):\n    if keyBitLength not in ACCEPTABLE_KEY_LENGTHS:\n      raise Exception(\"keyBitLength must be 128, 192, or 256\")\n    self.bitlen = keyBitLength\n    if len(rawKey) <= 0 or len(rawKey) > self.bitlen/8:\n      raise Exception(\"rawKey must be less than or equal to keyBitLength/8 (%d) characters long\" % (self.bitlen/8))\n    rawKey = zero_pad(rawKey, self.bitlen/8)\n    keytable = ctypes.create_string_buffer(TABLE_BYTE_LEN)\n    self.ekeygen(self.bitlen, rawKey, keytable)\n    self.keytable = keytable\n    self.initialized = True", "response": "This method is called by the encryption and decryption methods to generate a key for the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencrypts an arbitrary - length block of data.", "response": "def encrypt(self, plainText):\n    \"\"\"Encrypt an arbitrary-length block of data.\n\n    NOTE: This function formerly worked only on 16-byte blocks of `plainText`.\n    code that assumed this should still work fine, but can optionally be\n    modified to call `encrypt_block` instead.\n\n    Args:\n        plainText (str): data to encrypt. If the data is not a multiple of 16\n            bytes long, it will be padded with null (0x00) bytes until it is.\n\n    Returns:\n        encrypted data. Note that this will always be a multiple of 16 bytes\n            long.\n    \"\"\"\n    encryptedResult = ''\n    for index in range(0, len(plainText), BLOCK_SIZE):\n      block = plainText[index:index + BLOCK_SIZE]\n      # Pad to required length if needed\n      if len(block) < BLOCK_SIZE:\n        block = zero_pad(block, BLOCK_SIZE)\n      encryptedResult += self.encrypt_block(block)\n    return encryptedResult"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencrypting a 16 - byte block of data.", "response": "def encrypt_block(self, plainText):\n    \"\"\"Encrypt a 16-byte block of data.\n\n    NOTE: This function was formerly called `encrypt`, but was changed when\n    support for encrypting arbitrary-length strings was added.\n\n    Args:\n        plainText (str): 16-byte data.\n\n    Returns:\n        16-byte str.\n\n    Raises:\n        TypeError if CamCrypt object has not been initialized.\n        ValueError if `plainText` is not BLOCK_SIZE (i.e. 16) bytes.\n    \"\"\"\n    if not self.initialized:\n      raise TypeError(\"CamCrypt object has not been initialized\")\n    if len(plainText) != BLOCK_SIZE:\n      raise ValueError(\"plainText must be %d bytes long (received %d bytes)\" %\n                       (BLOCK_SIZE, len(plainText)))\n    cipher = ctypes.create_string_buffer(BLOCK_SIZE)\n    self.encblock(self.bitlen, plainText, self.keytable, cipher)\n    return cipher.raw"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef use_backup_if_fail(app, key):\n    lock.acquire()\n    try:\n        if key not in backup:\n            backup[key] = {}\n        if key in fail_times and fail_times[key] % app.config[MAX_FAILURE_TIMES] == 0:\n            logger.error(\n                '<SERVER KEY %s> At present already reaching the upper limit of the max failure times, failure times: %s' % (\n                    key, fail_times[key]))\n            message = {app.config[MAX_FAILURE_MESSAGE_KEY]: MAX_FAILURE_MESSAGE % key}\n            if alarm_email is not None:\n                _send_alarm_email('Happened fault!', MAX_FAILURE_MESSAGE % key)\n            return unite_dict(backup[key], message)\n        else:\n            logger.info('<SERVER KEY %s> Request fail or in a status of sleep time window and return backup data %s' % (\n                key, backup[key]))\n            return backup[key]\n    finally:\n        lock.release()", "response": "Return a flag for prompt message in front - end if the failure times exceeded the maximum failure times."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_sleep(key):\n    lock.acquire()\n    try:\n        if key not in sleep_record:\n            return False\n        return time.time() < sleep_record[key]\n    finally:\n        lock.release()", "response": "Determines if a key is in the sleep time window"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an anime from an AniDB search.", "response": "def command(state, args):\n    \"\"\"Add an anime from an AniDB search.\"\"\"\n    args = parser.parse_args(args[1:])\n    if args.watching:\n        rows = query.select.select(state.db, 'regexp IS NOT NULL', [], ['aid'])\n        aids = [anime.aid for anime in rows]\n    elif args.incomplete:\n        rows = query.select.select(state.db, 'enddate IS NULL', [], ['aid'])\n        aids = [anime.aid for anime in rows]\n    else:\n        aid = state.results.parse_aid(args.aid, default_key='db')\n        aids = [aid]\n    if not aids:\n        return\n    anime = request_anime(aids.pop())\n    query.update.add(state.db, anime)\n    print('Updated {} {}'.format(anime.aid, anime.title))\n    for aid in aids:\n        time.sleep(2)\n        anime = request_anime(aid)\n        query.update.add(state.db, anime)\n        print('Updated {} {}'.format(anime.aid, anime.title))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestart the HAProxy process.", "response": "def restart(self):\n        \"\"\"\n        Performs a soft reload of the HAProxy process.\n        \"\"\"\n        version = self.get_version()\n\n        command = [\n            \"haproxy\",\n            \"-f\", self.config_file_path, \"-p\", self.pid_file_path\n        ]\n        if version and version >= (1, 5, 0):\n            command.extend([\"-L\", self.peer.name])\n        if os.path.exists(self.pid_file_path):\n            with open(self.pid_file_path) as fd:\n                command.extend([\"-sf\", fd.read().replace(\"\\n\", \"\")])\n\n        try:\n            output = subprocess.check_output(command)\n        except subprocess.CalledProcessError as e:\n            logger.error(\"Failed to restart HAProxy: %s\", str(e))\n            return\n\n        if output:\n            logging.error(\"haproxy says: %s\", output)\n\n        logger.info(\"Gracefully restarted HAProxy.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_version(self):\n        command = [\"haproxy\", \"-v\"]\n        try:\n            output = subprocess.check_output(command)\n            version_line = output.split(\"\\n\")[0]\n        except subprocess.CalledProcessError as e:\n            logger.error(\"Could not get HAProxy version: %s\", str(e))\n            return None\n\n        match = version_re.match(version_line)\n        if not match:\n            logger.error(\"Could not parse version from '%s'\", version_line)\n            return None\n\n        version = (\n            int(match.group(\"major\")),\n            int(match.group(\"minor\")),\n            int(match.group(\"patch\"))\n        )\n\n        logger.debug(\"Got HAProxy version: %s\", version)\n\n        return version", "response": "Returns a tuple representing the installed HAProxy version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a simple dictionary of the current state of the ACS Azimuth.", "response": "def get_info(self):\n        \"\"\"\n        Parses the output of a \"show info\" HAProxy command and returns a\n        simple dictionary of the results.\n        \"\"\"\n        info_response = self.send_command(\"show info\")\n\n        if not info_response:\n            return {}\n\n        def convert_camel_case(string):\n            return all_cap_re.sub(\n                r'\\1_\\2',\n                first_cap_re.sub(r'\\1_\\2', string)\n            ).lower()\n\n        return dict(\n            (convert_camel_case(label), value)\n            for label, value in [\n                line.split(\": \")\n                for line in info_response.split(\"\\n\")\n            ]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of lists where the key is the name of a service and the value is the active nodes associated with that service.", "response": "def get_active_nodes(self):\n        \"\"\"\n        Returns a dictionary of lists, where the key is the name of a service\n        and the list includes all active nodes associated with that service.\n        \"\"\"\n        # the -1 4 -1 args are the filters <proxy_id> <type> <server_id>,\n        # -1 for all proxies, 4 for servers only, -1 for all servers\n        stats_response = self.send_command(\"show stat -1 4 -1\")\n        if not stats_response:\n            return []\n\n        lines = stats_response.split(\"\\n\")\n        fields = lines.pop(0).split(\",\")\n        # the first field is the service name, which we key off of so\n        # it's not included in individual node records\n        fields.pop(0)\n\n        active_nodes = collections.defaultdict(list)\n\n        for line in lines:\n            values = line.split(\",\")\n            service_name = values.pop(0)\n            active_nodes[service_name].append(\n                dict(\n                    (fields[i], values[i])\n                    for i in range(len(fields))\n                )\n            )\n\n        return active_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_node(self, service_name, node_name):\n        logger.info(\"Enabling server %s/%s\", service_name, node_name)\n        return self.send_command(\n            \"enable server %s/%s\" % (service_name, node_name)\n        )", "response": "Enable a given node name for the given service name via the availabe enable server command."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisable a given node name for the given service name via the HAProxy command.", "response": "def disable_node(self, service_name, node_name):\n        \"\"\"\n        Disables a given node name for the given service name via the\n        \"disable server\" HAProxy command.\n        \"\"\"\n        logger.info(\"Disabling server %s/%s\", service_name, node_name)\n        return self.send_command(\n            \"disable server %s/%s\" % (service_name, node_name)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_command(self, command):\n        logger.debug(\"Connecting to socket %s\", self.socket_file_path)\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        try:\n            sock.connect(self.socket_file_path)\n        except IOError as e:\n            if e.errno == errno.ECONNREFUSED:\n                logger.error(\"Connection refused.  Is HAProxy running?\")\n                return\n            else:\n                raise\n\n        sock.sendall((command + \"\\n\").encode())\n\n        response = b\"\"\n        while True:\n            try:\n                chunk = sock.recv(SOCKET_BUFFER_SIZE)\n                if chunk:\n                    response += chunk\n                else:\n                    break\n            except IOError as e:\n                if e.errno not in (errno.EAGAIN, errno.EINTR):\n                    raise\n\n        sock.close()\n\n        return self.process_command_response(command, response)", "response": "Sends a given command to the HAProxy control socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a command response from the HAProxy socket.", "response": "def process_command_response(self, command, response):\n        \"\"\"\n        Takes an HAProxy socket command and its response and either raises\n        an appropriate exception or returns the formatted response.\n        \"\"\"\n        if response.startswith(b\"Unknown command.\"):\n            raise UnknownCommandError(command)\n        if response == b\"Permission denied.\\n\":\n            raise PermissionError(command)\n        if response == b\"No such backend.\\n\":\n            raise UnknownServerError(command)\n\n        response = response.decode()\n        return response.rstrip(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef levenshtein(source, target):\n    if len(source) < len(target):\n        return levenshtein(target, source)\n\n    # So now we have len(source) >= len(target).\n    if len(target) == 0:\n        return len(source)\n\n    # We call tuple() to force strings to be used as sequences\n    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n    source = np.array(tuple(source))\n    target = np.array(tuple(target))\n\n    # We use a dynamic programming algorithm, but with the\n    # added optimization that we only need the last two rows\n    # of the matrix.\n    previous_row = np.arange(target.size + 1)\n    for s in source:\n        # Insertion (target grows longer than source):\n        current_row = previous_row + 1\n\n        # Substitution or matching:\n        # Target and source items are aligned, and either\n        # are different (cost of 1), or are the same (cost of 0).\n        current_row[1:] = np.minimum(\n                current_row[1:],\n                np.add(previous_row[:-1], target != s))\n\n        # Deletion (target grows shorter than source):\n        current_row[1:] = np.minimum(\n                current_row[1:],\n                current_row[0:-1] + 1)\n\n        previous_row = current_row\n\n    return previous_row[-1]", "response": "This function returns the Levenshtein distance between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef alphafilter(request, queryset, template):\n\n    qs_filter = {}\n    for key in list(request.GET.keys()):\n        if '__istartswith' in key:\n            qs_filter[str(key)] = request.GET[key]\n            break\n\n    return render_to_response(\n        template,\n        {'objects': queryset.filter(**qs_filter),\n         'unfiltered_objects': queryset},\n        context_instance=RequestContext(request)\n    )", "response": "Render the template with the filtered queryset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the array of multipliers for the dynamic", "response": "def to_array(self, channels=2):\n        \"\"\"Return the array of multipliers for the dynamic\"\"\"\n        if channels == 1:\n            return self.volume_frames.reshape(-1, 1)\n        if channels == 2:\n            return np.tile(self.volume_frames, (2, 1)).T\n        raise Exception(\n            \"RawVolume doesn't know what to do with %s channels\" % channels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a simhash based on the given item.", "response": "def generate_simhash(self, item):\n        \"\"\"\n        Generate simhash based on title, description, keywords, p_texts and links_text.\n        \"\"\"\n        list = item['p_texts'] + item['links_text']\n        list.append(item['title'])\n        list.append(item['description'])\n        list.append(item['keywords'])\n        return Simhash(','.join(list).strip()).hash"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train_phrases(paths, out='data/bigram_model.phrases', tokenizer=word_tokenize, **kwargs):\n    n = 0\n    for path in paths:\n        print('Counting lines for {0}...'.format(path))\n        n += sum(1 for line in open(path, 'r'))\n    print('Processing {0} lines...'.format(n))\n\n    # Change to use less memory. Default is 40m.\n    kwargs = {\n        'max_vocab_size': 40000000,\n        'threshold': 8.\n    }.update(kwargs)\n\n    print('Training bigrams...')\n    bigram = Phrases(_phrase_doc_stream(paths, n, tokenizer=word_tokenize), **kwargs)\n\n    print('Saving...')\n    bigram.save(out)", "response": "Train a bigram phrase model on a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefaulting hash function is variable - length version of Python s builtin hash.", "response": "def _default_hashfunc(content, hashbits):\n    \"\"\"\n    Default hash function is variable-length version of Python's builtin hash.\n\n    :param content: data that needs to hash.\n    :return: return a decimal number.\n    \"\"\"\n    if content == \"\":\n        return 0\n\n    x = ord(content[0]) << 7\n    m = 1000003\n    mask = 2 ** hashbits - 1\n    for c in content:\n        x = ((x * m) ^ ord(c)) & mask\n    x ^= len(content)\n    if x == -1:\n        x = -2\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefaulting tokenizer function that uses jieba tokenizer.", "response": "def _default_tokenizer_func(content, keyword_weight_pair):\n    \"\"\"\n    Default tokenizer function that uses jieba tokenizer.\n\n    :param keyword_weight_pair: maximum pair number of the keyword-weight list.\n    :return: return keyword-weight list. Example: [('Example',0.4511233019962264),('Hello',0.25548051420382073),...].\n    \"\"\"\n    seg_list = jieba.lcut_for_search(content)\n    # Extract keyword-weight list by TF-IDF algorithms and by sorted maximum weight\n    return jieba.analyse.extract_tags(\"\".join(seg_list), topK=keyword_weight_pair, withWeight=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simhash(self, content):\n        if content is None:\n            self.hash = -1\n            return\n\n        if isinstance(content, str):\n            features = self.tokenizer_func(content, self.keyword_weight_pari)\n            self.hash = self.build_from_features(features)\n        elif isinstance(content, collections.Iterable):\n            self.hash = self.build_from_features(content)\n        elif isinstance(content, int):\n            self.hash = content\n        else:\n            raise Exception(\"Unsupported parameter type %s\" % type(content))", "response": "Select policies for simhash on the different types of content."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_from_features(self, features):\n        v = [0] * self.hash_bit_number\n        if isinstance(features, dict):\n            features = features.items()\n\n        # Starting longitudinal accumulation of bits, current bit add current weight\n        # when the current bits equal 1 and else current bit minus the current weight.\n        for f in features:\n            if isinstance(f, str):\n                h = self.hashfunc(f, self.hash_bit_number)\n                w = 1\n            else:\n                assert isinstance(f, collections.Iterable)\n                h = self.hashfunc(f[0], self.hash_bit_number)\n                w = f[1]\n            for i in range(self.hash_bit_number):\n                bitmask = 1 << i\n                v[i] += w if h & bitmask else -w\n\n        # Just record weight of the non-negative\n        fingerprint = 0\n        for i in range(self.hash_bit_number):\n            if v[i] >= 0:\n                fingerprint += 1 << i\n\n        return fingerprint", "response": "Builds a fingerprint from a list of features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if two simhashes are similar or not similar.", "response": "def is_equal(self, another, limit=0.8):\n        \"\"\"\n        Determine two simhash are similar or not similar.\n\n        :param another: another simhash.\n        :param limit: a limit of the similarity.\n        :return: if similarity greater than limit return true and else return false.\n        \"\"\"\n        if another is None:\n            raise Exception(\"Parameter another is null\")\n\n        if isinstance(another, int):\n            distance = self.hamming_distance(another)\n        elif isinstance(another, Simhash):\n            assert self.hash_bit_number == another.hash_bit_number\n            distance = self.hamming_distance(another.hash)\n        else:\n            raise Exception(\"Unsupported parameter type %s\" % type(another))\n\n        similarity = float(self.hash_bit_number - distance) / self.hash_bit_number\n        if similarity > limit:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hamming_distance(self, another):\n        x = (self.hash ^ another) & ((1 << self.hash_bit_number) - 1)\n        result = 0\n        while x:\n            result += 1\n            x &= x - 1\n        return result", "response": "Compute the hamming distance between two simhashes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_date_str(str_):\n\n    if not str_:\n        return None\n\n    # Convert to datetime so we can validate it's a real date that exists then\n    # convert it back to the string.\n    try:\n        date = datetime.strptime(str_, DATE_FMT)\n    except ValueError:\n        msg = 'Invalid date format, should be YYYY-MM-DD'\n        raise argparse.ArgumentTypeError(msg)\n\n    return date.strftime(DATE_FMT)", "response": "Validate str as a date and return string version of date"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_args():\n\n    token_file = os.path.expanduser('~/.nikeplus_access_token')\n\n    parser = argparse.ArgumentParser(description='Export NikePlus data to CSV')\n\n    parser.add_argument('-t', '--token', required=False, default=None,\n                        help=('Access token for API, can also store in file %s'\n                        ' to avoid passing via command line' % (token_file)))\n    parser.add_argument('-s', '--since', type=_validate_date_str,\n                        help=('Only process entries starting with YYYY-MM-DD '\n                              'and newer'))\n\n    args = vars(parser.parse_args())\n\n    if args['token'] is None:\n        try:\n            with open(token_file, 'r') as _file:\n                access_token = _file.read().strip()\n        except IOError:\n            print 'Must pass access token via command line or store in file %s' % (\n                                                                    token_file)\n            sys.exit(-1)\n\n        args['token'] = access_token\n\n    return args", "response": "Parse sys. argv arguments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute a similarity score for two documents.", "response": "def similarity(self, d, d_):\n        \"\"\"\n        Compute a similarity score for two documents.\n\n        Optionally pass in a `term_sim_ref` dict-like, which should be able\n        to take `term1, term2` as args and return their similarity.\n        \"\"\"\n        es = set([e.name for e in d.entities])\n        es_ = set([e.name for e in d_.entities])\n        e_weight = (len(es) + len(es_) - abs(len(es) - len(es_)))/2\n        e_score = sum(self.idf_entity[t] for t in es & es_)\n\n        toks = set(d.tokens)\n        toks_ = set(d_.tokens)\n        t_weight = (len(toks) + len(toks_) - abs(len(toks) - len(toks_)))/2\n\n        # If no term similarity reference is passed,\n        # look only at surface form overlap (i.e. exact overlap)\n        shared_toks = toks & toks_\n        overlap = [(t, t, self.idf[t]) for t in shared_toks]\n        t_score = sum(self.idf[t] for t in shared_toks)\n        if self.term_sim_ref is not None:\n            # Double-count exact overlaps b/c we are\n            # comparing bidirectional term pairs here\n            t_score *= 2\n            for toks1, toks2 in [(toks, toks_), (toks_, toks)]:\n                for t in toks1 - shared_toks:\n                    best_match = max(toks2, key=lambda t_: self.term_sim_ref[t, t_])\n                    sim = self.term_sim_ref[t, best_match]\n                    t_score += sim * ((self.idf[t] + self.idf[best_match])/2)\n                    if sim > 0:\n                        overlap.append((t, best_match, sim * ((self.idf[t] + self.idf[best_match])/2)))\n\n            # Adjust term weight\n            #t_weight /= 2\n\n        t_weight = 1/t_weight if t_weight != 0 else 0\n        e_weight = 1/e_weight if e_weight != 0 else 0\n        t_score *= t_weight\n        e_score *= e_weight\n\n        if self.debug:\n            print('\\n-------------------------')\n            print((d.id, d_.id))\n            print('DOC:', d.id)\n            print('DOC:', d_.id)\n            print('\\tEntities:')\n            print('\\t', es)\n            print('\\t', es_)\n            print('\\t\\tEntity overlap:', es & es_)\n            print('\\t\\tEntity weight:', e_weight)\n            print('\\t\\tEntity score:', e_score)\n\n            print('\\tTokens:')\n            print('\\t\\t', toks)\n            print('\\t\\t', toks_)\n            print('\\t\\tToken overlap:', overlap)\n            print('\\t\\tToken weight:', t_weight)\n            print('\\t\\tToken score:', t_score)\n\n            print('\\tTotal score:', t_score + e_score)\n\n        return t_score + e_score"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a template and a regex match return a dictionary of information about the match.", "response": "def get_match_info(template, match, state):\n    \"\"\"\n    Given a template and a regex match within said template, return a\n    dictionary of information about the match to be used to help parse the\n    template.\n    \"\"\"\n    info = match.groupdict()\n\n    # Put special delimiter cases in terms of normal ones\n    if info['change']:\n        info.update({\n            'tag_type' : '=',\n            'tag_key' : info['delims'],\n        })\n    elif info['raw']:\n        info.update({\n            'tag_type' : '&',\n            'tag_key' : info['raw_key'],\n        })\n\n    # Rename the important match variables for convenience\n    tag_start = match.start()\n    tag_end = match.end()\n    tag_type = info['tag_type']\n    tag_key = info['tag_key']\n    lead_wsp = info['lead_wsp']\n    end_wsp = info['end_wsp']\n\n    begins_line = (tag_start == 0) or (template[tag_start-1] in state.eol_chars)\n    ends_line = (tag_end == len(template) or\n                 template[tag_end] in state.eol_chars)\n    interpolating = (tag_type in ('', '&'))\n    standalone = (not interpolating) and begins_line and ends_line\n\n    if end_wsp:\n        tag_end -= len(end_wsp)\n    if standalone:\n        template_length = len(template)\n        # Standalone tags strip exactly one occurence of '\\r', '\\n', or '\\r\\n'\n        # from the end of the line.\n        if tag_end < len(template) and template[tag_end] == '\\r':\n            tag_end += 1\n        if tag_end < len(template) and template[tag_end] == '\\n':\n            tag_end += 1\n    elif lead_wsp:\n        tag_start += len(lead_wsp)\n        lead_wsp = ''\n\n    info.update({\n        'tag_start' : tag_start,\n        'tag_end' : tag_end,\n        'tag_type' : tag_type,\n        'tag_key' : tag_key,\n        'lead_wsp' : lead_wsp,\n        'end_wsp' : end_wsp,\n        'begins_line' : begins_line,\n        'ends_line' : ends_line,\n        'interpolating' : interpolating,\n        'standalone' : standalone,\n    })\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tag_context(name, state):\n    new_contexts = 0\n    ctm = None\n    while True:\n        try:\n            ctx_key, name = name.split('.', 1)\n            ctm = state.context.get(ctx_key)\n        except ValueError:\n            break\n        if not ctm:\n            break\n        else:\n            state.context.push(ctm)\n            new_contexts += 1\n\n    ctm = state.context.get(name)\n\n    return new_contexts, ctm", "response": "Given a tag name return its associated value as defined in the current\n    context stack."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving the tag key of an opening section tag find the corresponding closing tag and return information about that match.", "response": "def section_end_info(template, tag_key, state, index):\n    \"\"\"\n    Given the tag key of an opening section tag, find the corresponding closing\n    tag (if it exists) and return information about that match.\n    \"\"\"\n\n    state.section.push(tag_key)\n    match = None\n    matchinfo = None\n    search_index = index\n\n    while state.section:\n        match = state.tag_re.search(template, search_index)\n        if not match:\n            raise Exception(\"Open section %s never closed\" % tag_key)\n\n        matchinfo = get_match_info(template, match, state)\n\n        # If we find a new section tag, add it to the stack and keep going\n        if matchinfo['tag_type'] in ('#', '^'):\n            state.section.push(matchinfo['tag_key'])\n        # If we find a closing tag for the current section, 'close' it by\n        # popping the stack\n        elif matchinfo['tag_type'] == '/':\n            if matchinfo['tag_key'] == state.section():\n                state.section.pop()\n            else:\n                raise Exception(\n                    'Unexpected section end: received %s, expected {{/%s}}' % (\n                        repr(match.group(0)), tag_key))\n        search_index = matchinfo['tag_end']\n\n    return matchinfo"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender a given mustache template with sane defaults.", "response": "def render(template, context, partials={}, state=None):\n    \"\"\" Renders a given mustache template, with sane defaults. \"\"\"\n    # Create a new state by default\n    state = state or State()\n\n    # Add context to the state dict\n    if isinstance(context, Context):\n        state.context = context\n    else:\n        state.context = Context(context)\n\n    # Add any partials to the state dict\n    if partials:\n        state.partials.push(partials)\n\n    # Render the rendered template\n    return __render(make_unicode(template), state)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __render(template, state, index=0):\n    # Find a Match\n    match = state.tag_re.search(template, index)\n    if not match:\n        return template[index:]\n\n    info = get_match_info(template, match, state)\n    _pre = template[index : info['tag_start']] # template before the tag\n    _tag = template[info['tag_start'] : info['tag_end']] # tag\n    _continue = info['tag_end'] # the index at which to continue\n\n    # Comment\n    if info['tag_type'] == '!':\n        # Comments are removed from output\n        repl = \"\"\n\n    # Delimiter change\n    elif info['tag_type'] == '=':\n        # Delimiters are changed; the tag is rendered as \"\"\n        delimiters = re.split(r'\\s*', info['tag_key'])\n        new_tags = state.tags(_copy=True)\n        new_tags['otag'], new_tags['ctag'] = map(re.escape, delimiters)\n        state.push_tags(new_tags)\n        repl = \"\"\n\n    # Plain tag\n    elif info['tag_type'] == '':\n        repl = __render_tag(info, state)\n\n    # Raw tag (should not be escaped)\n    elif info['tag_type'] == '&':\n        state.escape.push(False)\n        repl = __render_tag(info, state)\n        state.escape.pop()\n\n    # Partial\n    elif info['tag_type'] == '>':\n        partial_name = info['tag_key']\n        partial_template = None\n        new_dir = None\n        lead_wsp = re.compile(r'^(.)', re.M)\n        repl = ''\n        try:\n            # Cached\n            partial_template = state.partials()[partial_name]\n        except (KeyError, IndexError):\n            try:\n                # Load the partial template from a file (if it exists)\n                new_dir, filename = split(partial_name)\n                if new_dir:\n                    state.partials_dir.push(new_dir)\n\n                partial_template = load_template(filename, state.abs_partials_dir, state.extension,\n                                                 state.encoding, state.encoding_error)\n            except (IOError):\n                pass\n\n        if partial_template:\n            # Preserve indentation\n            if info['standalone']:\n                partial_template = lead_wsp.sub(info['lead_wsp']+r'\\1', partial_template)\n\n            # Update state\n            state.partials.push(state.partials()) # XXX wtf is this shit?\n            state.push_tags(state.default_tags)\n\n            # Render the partial\n            repl = __render(partial_template, state)\n\n            # Restore state\n            state.partials.pop()\n            state.pop_tags()\n            if new_dir:\n                state.partials_dir.pop()\n\n    # Section\n\n    # TODO(peter): add a stop= index to __render so that template_to_inner does\n    # not need to be constructed with [:] indexing, which is extremely\n    # expensive.\n    elif info['tag_type'] in ('#', '^'):\n        otag_info = info\n        ctag_info = section_end_info(template, info['tag_key'], state, _continue)\n\n        # Don't want to parse beyond the end of the inner section, but\n        # must include information on prior contents so that whitespace\n        # is preserved correctly and inner tags are not marked as standalone.\n        inner_start = otag_info['tag_end']\n        inner_end = ctag_info['tag_start']\n        _continue = ctag_info['tag_end']\n\n        template_with_inner = template[:inner_end]\n\n        new_contexts, ctm = get_tag_context(otag_info['tag_key'], state)\n        truthy = otag_info['tag_type'] == '#'\n\n        #if ctm is not None:\n        if ctm:\n            # If there's a match and it's callable, feed it the inner template\n            if callable(ctm):\n                template_to_inner = template[:inner_start]\n                inner = template[inner_start:inner_end]\n                template_with_inner = template_to_inner + make_unicode(ctm(inner))\n\n            # Make the context list an iterable from the ctm\n            if not hasattr(ctm, '__iter__') or isinstance(ctm, dict):\n                ctx_list = [ctm]\n            else:\n                ctx_list = ctm\n        # If there's no match, there are no new contexts\n        else:\n            ctx_list = [False]\n\n        # If there are new contexts and the section is truthy, or if\n        # there are no new contexts and the section is falsy, render\n        # the contents\n        repl_stack = []\n        for ctx in ctx_list:\n            if (truthy and ctx) or (not truthy and not ctx):\n                state.context.push(ctx)\n                repl_stack.append(\n                    __render(template_with_inner, state, inner_start))\n\n            else:\n                break\n\n        repl = ''.join(repl_stack)\n        for i in xrange(new_contexts): state.context.pop()\n    else:\n        raise Exception(\"found unpaired end of section tag!\")\n\n    return u''.join((\n        _pre, make_unicode(repl), __render(template, state, _continue)))", "response": "Given a template string a parser state and a starting index returns the rendered version of the template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender an individual tag by making the appropriate replacement within the current context.", "response": "def __render_tag(info, state):\n    \"\"\" Render an individual tag by making the appropriate replacement within\n    the current context (if any). \"\"\"\n    new_contexts, context_match = get_tag_context(info['tag_key'], state)\n    replacement = ''\n\n    if context_match or context_match == 0:\n        replacement = context_match\n    elif info['tag_key'] == '.':\n        replacement = state.context()\n    else:\n        replacement = ''\n\n    # Call all callables / methods / lambdas / functions\n    if replacement and callable(replacement):\n        replacement = make_unicode(replacement())\n\n        state.push_tags(state.default_tags)\n        replacement = __render(template=replacement, state=state)\n        state.pop_tags()\n\n    for i in xrange(new_contexts): state.context.pop()\n\n    if state.escape():\n        return html_escape(replacement)\n    return replacement"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_dict_to_str(dict, format):\n    result = ''\n    for k, v in dict.items():\n        result = result + str(k) + format + str(v) + ', '\n    return result[:-2]", "response": "Format a dictionary to the string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of numbers to a string.", "response": "def list_to_str(list, separator=','):\n    \"\"\"\n    >>> list = [0, 0, 7]\n    >>> list_to_str(list)\n    '0,0,7'\n    \"\"\"\n    list = [str(x) for x in list]\n    return separator.join(list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary with the keys a and b.", "response": "def unite_dict(a, b):\n    \"\"\"\n    >>> a = {'name': 'Sylvanas'}\n    >>> b = {'gender': 'Man'}\n    >>> unite_dict(a, b)\n    {'name': 'Sylvanas', 'gender': 'Man'}\n    \"\"\"\n    c = {}\n    c.update(a)\n    c.update(b)\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a dictionary contains all the keys", "response": "def check_validity_for_dict(keys, dict):\n    \"\"\"\n    >>> dict = {'a': 0, 'b': 1, 'c': 2}\n    >>> keys = ['a', 'd', 'e']\n    >>> check_validity_for_dict(keys, dict) == False\n    True\n    >>> keys = ['a', 'b', 'c']\n    >>> check_validity_for_dict(keys, dict) == False\n    False\n    \"\"\"\n    for key in keys:\n        if key not in dict or dict[key] is '' or dict[key] is None:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parallel(func, inputs, n_jobs, expand_args=False):\n    if expand_args:\n        return Parallel(n_jobs=n_jobs)(delayed(func)(*args) for args in inputs)\n    else:\n        return Parallel(n_jobs=n_jobs)(delayed(func)(arg) for arg in inputs)", "response": "Wrapper around joblib s parallelization.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef breeding_plugevent(request, breeding_id):\n    breeding = get_object_or_404(Breeding, pk=breeding_id)\n    if request.method == \"POST\":\n        form = BreedingPlugForm(request.POST, request.FILES)\n        if form.is_valid():\n            plug = form.save(commit=False)\n            plug.Breeding_id = breeding.id\n            plug.save()\n            form.save()\n            return HttpResponseRedirect(reverse(\"plugevents-list\"))\n    else:\n        form = BreedingPlugForm()\n        form.fields[\"PlugFemale\"].queryset = breeding.Females.all()\n        form.fields[\"PlugMale\"].queryset = breeding.Male.all()\n    return render(request, 'breeding_plugevent_form.html', {'form':form, 'breeding':breeding})", "response": "This view defines a form for adding new plug events from a breeding set. This view defines a form for adding new plug events from a breeding set and restricts the PlugFemale and PlugMale to animals that are defined in that breeding set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfixing cache issues caused by schema pre - v4.", "response": "def command(state, args):\n    \"\"\"Fix cache issues caused by schema pre-v4.\"\"\"\n    if len(args) > 1:\n        print(f'Usage: {args[0]}')\n        return\n    db = state.db\n    _refresh_incomplete_anime(db)\n    _fix_cached_completed(db)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self):\n        self.convertRSTmetaToMD()\n        self._md = Markdown(extensions=['meta', 'codehilite(linenums=True)'])\n        content = self._md.convert(self.source)\n        metadata = self._parse_metadata(self._md.Meta)\n        return content, metadata", "response": "Parse content and metadata of markdown files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestore backup to version uses current version by default.", "response": "def restore_db(release=None):\n    \"\"\"\n    Restores backup back to version, uses current version by default.\n    \"\"\"\n\n    if not release:\n        release = paths.get_current_release_name()\n\n    if not release:\n        raise Exception(\"Release %s was not found\" % release)\n\n    backup_file = \"postgresql/%s.sql.gz\" % release\n    backup_path = paths.get_backup_path(backup_file)\n\n    if not env.exists(backup_path):\n        raise Exception(\"Backup file %s not found\" % backup_path)\n\n    with context_managers.shell_env(PGPASSWORD=env.psql_password):\n        env.run(\"pg_restore --clean -h localhost -d %s -U %s '%s'\" % (\n            env.psql_db,\n            env.psql_user,\n            backup_path)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync_remote_to_local(force=\"no\"):\n\n    _check_requirements()\n\n    if force != \"yes\":\n        message = \"This will replace your local database '%s' with the \"\\\n            \"remote '%s', are you sure [y/n]\" % (env.local_psql_db, env.psql_db)\n        answer = prompt(message, \"y\")\n\n        if answer != \"y\":\n            logger.info(\"Sync stopped\")\n            return\n\n    init_tasks()  # Bootstrap fabrik\n\n    # Create database dump\n    remote_file = \"postgresql/sync_%s.sql.tar.gz\" % int(time.time()*1000)\n    remote_path = paths.get_backup_path(remote_file)\n\n    env.run(\"mkdir -p %s\" % paths.get_backup_path(\"postgresql\"))\n\n    with context_managers.shell_env(PGPASSWORD=env.psql_password):\n        env.run(\"pg_dump -h localhost -Fc -f %s -U %s %s -x -O\" % (\n            remote_path, env.psql_user, env.psql_db\n        ))\n\n    local_path = \"/tmp/%s\" % remote_file\n\n    # Download sync file\n    get(remote_path, local_path)\n\n    # Import sync file by performing the following task (drop, create, import)\n    with context_managers.shell_env(PGPASSWORD=env.local_psql_password):\n        elocal(\"pg_restore --clean -h localhost -d %s -U %s '%s'\" % (\n            env.local_psql_db,\n            env.local_psql_user,\n            local_path)\n        )\n\n    # Cleanup\n    env.run(\"rm %s\" % remote_path)\n    elocal(\"rm %s\" % local_path)\n\n    # Trigger hook\n    run_hook(\"postgres.after_sync_remote_to_local\")\n\n    logger.info(\"Sync complete\")", "response": "Syncs the remote postgres database with the local postgres database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_model():\n    if DJANGO_VERSION >= (1, 5):\n        from django.contrib.auth import get_user_model\n\n        return get_user_model()  # NOQA\n    else:\n        from django.contrib.auth.models import User  # NOQA\n        return User", "response": "Returns the user model to use at runtime."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if an app is listed in INSTALLED_APPS or in settings. INSTALLED_APPS.", "response": "def has_app(app_name):\n    \"\"\"\n    Determines whether an app is listed in INSTALLED_APPS or the app registry.\n    :param app_name: string\n    :return: bool\n    \"\"\"\n    if DJANGO_VERSION >= (1, 7):\n        from django.apps import apps\n        return apps.is_installed(app_name)\n    else:\n        from django.conf import settings\n\n        return app_name in settings.INSTALLED_APPS"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_check_config(self, config):\n        self.query = config.get(\"query\")\n        self.expected_response = config.get(\"response\")", "response": "Applies the config to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a straightforward TCP request and response.", "response": "def perform(self):\n        \"\"\"\n        Performs a straightforward TCP request and response.\n\n        Sends the TCP `query` to the proper host and port, and loops over the\n        socket, gathering response chunks until a full line is acquired.\n\n        If the response line matches the expected value, the check passes. If\n        not, the check fails.  The check will also fail if there's an error\n        during any step of the send/receive process.\n        \"\"\"\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n        sock.connect((self.host, self.port))\n\n        # if no query/response is defined, a successful connection is a pass\n        if not self.query:\n            sock.close()\n            return True\n\n        try:\n            sock.sendall(self.query)\n        except Exception:\n            logger.exception(\"Error sending TCP query message.\")\n            sock.close()\n            return False\n\n        response, extra = sockutils.get_response(sock)\n\n        logger.debug(\"response: %s (extra: %s)\", response, extra)\n\n        if response != self.expected_response:\n            logger.warn(\n                \"Response does not match expected value: %s (expected %s)\",\n                response, self.expected_response\n            )\n            sock.close()\n            return False\n\n        sock.close()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting src and dst values from a section", "response": "def _get_section_values(self, config, section):\n        \"\"\" extract src and dst values from a section\n        \"\"\"\n        src_host = self._get_hosts_from_names(config.get(section, 'src.host')) \\\n            if config.has_option(section, 'src.host') else None\n        src_file = [self._get_abs_filepath(config.get(section, 'src.file'))] \\\n            if config.has_option(section, 'src.file') else None\n        if src_host is None and src_file is None:\n            raise conferr('Section \"%s\" gets no sources' % section)\n\n        dst_host = self._get_hosts_from_names(config.get(section, 'dst.host')) \\\n            if config.has_option(section, 'dst.host') else None\n        dst_file = [self._get_abs_filepath(config.get(section, 'dst.file'))] \\\n            if config.has_option(section, 'dst.file') else None\n        if dst_host is None and dst_file is None:\n            raise conferr('Section \"%s\" gets no destinations' % section)\n\n        return (src_host, src_file, dst_host, dst_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassembling a flowtable from a tuple of descriptors.", "response": "def _assemble_flowtable(self, values):\n        \"\"\" generate a flowtable from a tuple of descriptors.\n        \"\"\"\n        values = map(lambda x: [] if x is None else x, values)\n        src = values[0] + values[1]\n        dst = values[2] + values[3]\n\n        thistable = dict()\n        for s in src:\n            thistable[s] = dst\n        return thistable"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _detect_loop(self):\n        for source, dests in self.flowtable.items():\n            if source in dests:\n                raise conferr('Loops detected: %s --> %s' % (source, source))", "response": "detect loops in flow table raise error if being present\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_hosts_from_ports(self, ports):\n        hosts = map(lambda x: 'localhost:%d' % int(x.strip()), ports.split(','))\n        return list(set(hosts))", "response": "validate hostnames from a list of ports"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating hostnames from a list of names", "response": "def _get_hosts_from_names(self, names):\n        \"\"\" validate hostnames from a list of names\n        \"\"\"\n        result = set()\n        hosts = map(lambda x: x.strip(), names.split(','))\n        for h in hosts:\n            if valid_hostname(h.split(':')[0]):\n                result.add(h if ':' in h else '%s:%d' % (h, self.PORT))\n            else:\n                raise conferr('Invalid hostname: %s' % h.split(':')[0])\n        return list(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_abs_filepath(self, ifile):\n        assert ifile is not None\n        ifile = ifile[7:] if ifile.startswith('file://') else ifile\n        if ifile[0] != '/':\n            basedir = os.path.abspath(os.path.dirname(self.config_file))\n            ifile = os.path.join(basedir, ifile)\n        return 'file://' + ifile", "response": "validate src or dst file path with self. config_file\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a flat flow table globally", "response": "def flowtable(self):\n        \"\"\" get a flat flow table globally\n        \"\"\"\n        ftable = dict()\n        for table in self.flow_table:\n            for k, v in table.items():\n                if k not in ftable:\n                    ftable[k] = set(v)\n                else:\n                    [ftable[k].add(i) for i in v]\n        # convert set to list\n        for k in ftable:\n            ftable[k] = list(ftable[k])\n        return ftable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_last_aid(func):\n    @functools.wraps(func)\n    def new_func(self, *args, **kwargs):\n        # pylint: disable=missing-docstring\n        aid = func(self, *args, **kwargs)\n        self.last_aid = aid\n        return aid\n    return new_func", "response": "Decorator for setting last_aid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an aid from the search result tables as necessary.", "response": "def parse_aid(self, text, default_key):\n        \"\"\"Parse argument text for aid.\n\n        May retrieve the aid from search result tables as necessary.  aresults\n        determines which search results to use by default; True means aresults\n        is the default.\n\n        The last aid when no aid has been parsed yet is undefined.\n\n        The accepted formats, in order:\n\n        Last AID:                .\n        Explicit AID:            aid:12345\n        Explicit result number:  key:12\n        Default result number:   12\n\n        \"\"\"\n\n        if default_key not in self:\n            raise ResultKeyError(default_key)\n\n        if text == '.':\n            return self.last_aid\n        elif text.startswith('aid:'):\n            return int(text[len('aid:'):])\n\n        if ':' in text:\n            match = self._key_pattern.search(text)\n            if not match:\n                raise InvalidSyntaxError(text)\n            key = match.group(1)\n            number = match.group(2)\n        else:\n            key = default_key\n            number = text\n        try:\n            number = int(number)\n        except ValueError:\n            raise InvalidSyntaxError(number)\n\n        try:\n            return self[key].get_aid(number)\n        except KeyError:\n            raise ResultKeyError(key)\n        except IndexError:\n            raise ResultNumberError(key, number)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _connect(dbfile: 'PathLike') -> apsw.Connection:\n    conn = apsw.Connection(os.fspath(dbfile))\n    _set_foreign_keys(conn, 1)\n    assert _get_foreign_keys(conn) == 1\n    return conn", "response": "Connect to SQLite database file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the array of multipliers for the dynamic", "response": "def to_array(self, channels=2):\n        \"\"\"Generate the array of multipliers for the dynamic\"\"\"\n        return np.linspace(self.volume, self.volume,\n            self.duration * channels).reshape(self.duration, channels)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_measurement(request, experiment_id):\n\texperiment = get_object_or_404(Experiment, pk=experiment_id)\n\tif request.method == 'POST':\n\t\tform = MeasurementForm(request.POST)\n\t\tif form.is_valid():\n\t\t\tform.save()\n\t\t\treturn HttpResponseRedirect( experiment.get_absolute_url() ) \n\telse:\n\t\tform = MeasurementForm() \n\treturn render(request, \"data_entry_form.html\", {\"form\": form, \"experiment\": experiment })", "response": "This is a view to display a form to add single measurements to an experiment. It calls the MeasurementForm which has an autocomplete field for animal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef aging_csv(request):\n    animal_list = Animal.objects.all()\n    response = HttpResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=aging.csv'\n    writer = csv.writer(response)\n    writer.writerow([\"Animal\", \"Strain\", \"Genotype\", \"Gender\", \"Age\", \"Death\", \"Alive\"])\n    for animal in animal_list.iterator():\n        writer.writerow([\n            animal.MouseID, \n            animal.Strain, \n            animal.Genotype, \n            animal.Gender,\n            animal.age(),\n            animal.Cause_of_Death,\n            animal.Alive            \n            ])\n    return response", "response": "This view generates a csv output file of all animal data for use in aging analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef litters_csv(request):\n    animal_list = Animal.objects.all()\n    response = HttpResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=litters.csv'\n    writer = csv.writer(response)\n    writer.writerow([\"Born\", \"Breeding\", \"Strain\"])\n    for animal in animal_list:\n        writer.writerow([\n            animal.Born,\n            animal.Breeding,\n            animal.Strain\n            ])\n    return response", "response": "This view generates a csv output file of all animal data for use in litter analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_queryset(self):\n        '''The queryset is filtered by measurements of animals which are part of that strain.'''\n        cohort = get_object_or_404(Cohort, slug=self.kwargs['slug'])\n        animals = cohort.animals.all()\n        return Measurement.objects.filter(animal=animals)", "response": "The queryset is filtered by measurements of animals which are part of that strain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_queryset(self):\n        '''The queryset is filtered by measurements of animals which are part of that strain.'''\n        strain = get_object_or_404(Strain, Strain_slug=self.kwargs['strain_slug'])\n        animals = Animal.objects.filter(Strain=strain)\n        return Measurement.objects.filter(animal=animals)", "response": "The queryset is filtered by measurements of animals which are part of that strain."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, request, *args, **kwargs):\n        '''The queryset is filtered by measurements of animals which are part of that strain.'''\n        strain = get_object_or_404(Strain, Strain_slug=self.kwargs['strain_slug'])\n        animals = Animal.objects.filter(Strain=strain)\n        measurements = Measurement.objects.filter(animal=animals)    \n        return data_csv(self.request, measurements)", "response": "The queryset is filtered by measurements of animals which are part of that strain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vectorize( self, docs ):\n        '''\n        Returns the feature vectors for a set of docs. If model is not already be trained, \n        then self.train() is called.\n\n        Args:\n            docs (dict or list of tuples): asset_id, body_text of documents\n            you wish to featurize.\n        '''\n\n        if type(docs) == dict:\n            docs = docs.items()\n\n        if self.model == None:\n            self.train(docs)\n\n        asset_id2vector = {}\n\n        unfound = []\n        for item in docs:\n            ## iterate through the items in docs and check if any are already in the model.\n            asset_id, _ = item\n            label = 'DOC_' + str(asset_id)\n            if label in self.model:\n                asset_id2vector.update({asset_id: self.model['DOC_' + str(asset_id)]})\n            else:\n                unfound.append(item)\n\n        if len(unfound) > 0:\n            ## for all assets not in the model, update the model and then get their sentence vectors.\n            sentences = [self._gen_sentence(item) for item in unfound]\n            self.update_model(sentences, train=self.stream_train)\n            asset_id2vector.update({item[0]: self.model['DOC_' + str(item[0])] for item in unfound})\n\n        return asset_id2vector", "response": "Returns the feature vectors for a set of docs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains Doc2Vec on a series of docs.", "response": "def train(self, docs, retrain=False):\n        '''\n        Train Doc2Vec on a series of docs. Train from scratch or update.\n\n        Args:\n            docs: list of tuples (assetid, body_text) or dictionary {assetid : body_text}\n            retrain: boolean, retrain from scratch or update model\n\n        saves model in class to self.model   \n\n        Returns: 0 if successful\n        '''\n\n        if type(docs) == dict:\n            docs = docs.items()\n\n        train_sentences = [self._gen_sentence(item) for item in docs]\n        if (self.is_trained) and (retrain == False): \n            ## online training \n            self.update_model(train_sentences, update_labels_bool=True)\n\n        else: \n            ## train from scratch\n            self.model = Doc2Vec(train_sentences, size=self.size, window=self.window, min_count=self.min_count, workers=self.workers)\n            self.is_trained = True\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a list of sentenes and updates an existing model. Vectors will be callable through self. model [ label ]", "response": "def update_model(self, sentences, update_labels_bool):\n        '''\n        takes a list of sentenes and updates an existing model. Vectors will be \n        callable through self.model[label]\n\n        update_labels_bool: boolean that says whether to train the model (self.model.train_words = True)\n        or simply to get vectors for the documents (self.model.train_words = False)\n\n            self.vectorize should not train the model further\n            self.train should if model already exists\n\n        '''\n\n        n_sentences = self._add_new_labels(sentences)\n\n        # add new rows to self.model.syn0\n        n = self.model.syn0.shape[0]\n        self.model.syn0 = np.vstack((\n            self.model.syn0,\n            np.empty((n_sentences, self.model.layer1_size), dtype=np.float32)\n        ))\n\n        for i in xrange(n, n + n_sentences):\n            np.random.seed(\n                np.uint32(self.model.hashfxn(self.model.index2word[i] + str(self.model.seed))))\n            a = (np.random.rand(self.model.layer1_size) - 0.5) / self.model.layer1_size\n            self.model.syn0[i] = a\n\n        # Set self.model.train_words to False and self.model.train_labels to True\n        self.model.train_words = update_labels_bool\n        self.model.train_lbls = True\n\n        # train\n        self.model.train(sentences)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process(self, input):\n        '''\n        Takes in html-mixed body text as a string and returns a list of strings,\n        lower case and with punctuation given spacing. \n\n        Called by self._gen_sentence()\n\n        Args:\n            inpnut (string): body text\n        '''\n\n        input = re.sub(\"<[^>]*>\", \" \", input) \n        punct = list(string.punctuation)\n        for symbol in punct:\n            input = input.replace(symbol, \" %s \" % symbol)\n        input = filter(lambda x: x != u'', input.lower().split(' '))\n        return input", "response": "Takes in html - mixed body text as a string and returns a list of strings lower case and with punctuation given spacing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _gen_sentence(self, assetid_body_tuple):\n        '''\n        Takes an assetid_body_tuple and returns a Doc2Vec LabeledSentence \n\n        Args:\n            assetid_body_tuple (tuple): (assetid, bodytext) pair \n        '''\n        asset_id, body = assetid_body_tuple\n        text = self._process(body)\n        sentence = LabeledSentence(text, labels=['DOC_%s' % str(asset_id)])\n        return sentence", "response": "Generates a Doc2Vec LabeledSentence object from a tuple of assetid_body_tuple and returns a Doc2Vec LabeledSentence object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding new labels to the internal indexing of the model.", "response": "def _add_new_labels(self, sentences):\n        '''\n        Adds new sentences to the internal indexing of the model.\n\n        Args: \n            sentences (list): LabeledSentences for each doc to be added\n\n        Returns:\n            int: number of sentences added to the model\n\n        '''\n        sentence_no = -1\n        total_words = 0\n        vocab = self.model.vocab\n        model_sentence_n = len([l for l in vocab if l.startswith(\"DOC_\")])\n        n_sentences = 0\n        for sentence_no, sentence in enumerate(sentences):\n            sentence_length = len(sentence.words)\n            for label in sentence.labels:\n                total_words += 1\n                if label in vocab:\n                    vocab[label].count += sentence_length\n                else:\n                    vocab[label] = gensim.models.word2vec.Vocab(\n                        count=sentence_length)\n\n                    vocab[label].index = len(self.model.vocab) - 1\n                    vocab[label].code = [0]\n                    vocab[label].sample_probability = 1.\n                    self.model.index2word.append(label)\n                    n_sentences += 1\n                    \n        return n_sentences"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning both pylint and flake8 commands and exits based off the evaluation of both commands.", "response": "def run(targets, config_dir='.', check_licenses=False):\n    # type: (List[str], str, bool) -> None\n    \"\"\"Runs `pylint` and `flake8` commands and exits based off the evaluation\n    of both command results.\n\n    :param targets: List[str]\n    :param config_dir: str\n    :param check_licenses: bool\n    :return:\n    \"\"\"\n    pylint_return_state = False\n    flake8_return_state = False\n\n    if check_licenses:\n        run_license_checker(config_path=get_license_checker_config_path(config_dir))\n\n    pylint_options = get_pylint_options(config_dir=config_dir)\n    flake8_options = get_flake8_options(config_dir=config_dir)\n\n    if targets:\n        pylint_return_state = _run_command(command='pylint', targets=targets,\n                                           options=pylint_options)\n        flake8_return_state = _run_command(command='flake8', targets=targets,\n                                           options=flake8_options)\n\n    if not flake8_return_state and not pylint_return_state:\n        sys.exit(0)\n    else:\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns command + targets + options in a subprocess and returns a boolean determined by the process return code.", "response": "def _run_command(command, targets, options):\n    # type: (str, List[str], List[str]) -> bool\n    \"\"\"Runs `command` + `targets` + `options` in a\n    subprocess and returns a boolean determined by the\n    process return code.\n\n    >>> result = run_command('pylint', ['foo.py', 'some_module'], ['-E'])\n    >>> result\n    True\n\n    :param command: str\n    :param targets: List[str]\n    :param options: List[str]\n    :return: bool\n    \"\"\"\n    print('{0}: targets={1} options={2}'.format(command, targets, options))\n    cmd = [command] + targets + options\n    process = Popen(cmd)\n    process.wait()\n\n    return bool(process.returncode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ynnm(n, m):\r\n    a = 1.0 / np.sqrt(4.0 * np.pi)\r\n    pm = np.abs(m)\r\n\r\n    out = 0.0\r\n\r\n    if(n < pm):\r\n        out = 0.0\r\n    elif(n == 0):\r\n        out = a\r\n    else:\r\n        out = a\r\n        for k in xrange(1, n + 1):\r\n            out *= np.sqrt((2.0 * k + 1.0) / 8.0 / k)\r\n\r\n        if(n != pm):\r\n            for k in xrange(n - 1, pm - 1, -1):\r\n                out *= np.sqrt((n + k + 1.0) / (n - k))\r\n    return out", "response": "Initial value for recursion formula"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the smallest number factorable by the small primes 2 3 4 and 7 and 7 with the smallest primes 7 and 7 with the smallest primes 2 3 5 and 7 with the smallest primes 4 and 7 with the smallest primes 5 and 7 with the smallest primes 7.", "response": "def smallest_prime_factor(Q):\r\n    \"\"\"Find the smallest number factorable by the small primes 2, 3, 4, and 7 \r\n    that is larger than the argument Q\"\"\"\r\n\r\n    A = Q;\r\n    while(A != 1):\r\n        if(np.mod(A, 2) == 0):\r\n            A = A / 2\r\n        elif(np.mod(A, 3) == 0):\r\n            A = A / 3\r\n        elif(np.mod(A, 5) == 0):\r\n            A = A / 5\r\n        elif(np.mod(A, 7) == 0):\r\n            A = A / 7;\r\n        else:\r\n            A = Q + 1;\r\n            Q = A;\r\n\r\n    return Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef s_data(nrows_fdata, Nmax, Q):\r\n\r\n    if np.mod(nrows_fdata, 2) == 1:\r\n        raise Exception(\"nrows_fdata must be even.\")\r\n    \r\n    L1 = nrows_fdata\r\n\r\n    s = np.zeros(Q, dtype=np.complex128)\r\n    MM = int(L1 / 2)\r\n\r\n    for nu in xrange(-MM, MM + Nmax + 1):\r\n        if np.mod(nu, 2) == 1:\r\n            s[nu - MM] = -1j / nu\r\n\r\n    return s", "response": "Returns a new array of the same length as the given array of n values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the HKM FFT of a given set of data.", "response": "def hkm_fc(fdata, Nmax, m, s):\r\n    \"\"\" Assume fdata has even rows\"\"\"\r\n\r\n    f = fdata[:, m]\r\n    L1 = f.size\r\n    MM = int(L1 / 2)\r\n    Q = s.size\r\n\r\n    ff = np.zeros(Q, dtype=np.complex128)\r\n    for n in xrange(MM, L1):\r\n        ff[n] = f[n - MM]\r\n\r\n    for n in xrange(0, MM):\r\n        ff[n] = f[n + MM]\r\n\r\n    # For larger problems, this speeds things up pretty good.\r\n    F = np.fft.fft(ff)\r\n    S = np.fft.fft(s)\r\n    out = 4 * np.pi * np.fft.ifft(F * S)\r\n\r\n    return out[0:Nmax + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nindex to the first n values for a give m within the spherical coefficients vector. Used by sc_to_fc", "response": "def mindx(m, nmax, mmax):\r\n    \"\"\"index to the first n value for a give m within the spherical \r\n    coefficients vector. Used by sc_to_fc\"\"\"\r\n\r\n    ind = 0\r\n    NN = nmax + 1\r\n\r\n    if np.abs(m) > mmax:\r\n        raise Exception(\"|m| cannot be larger than mmax\")\r\n\r\n    if (m != 0):\r\n        ind = NN\r\n        ii = 1\r\n        for i in xrange(1, np.abs(m)):\r\n            ind = ind + 2 * (NN - i)\r\n            ii = i + 1\r\n\r\n        if m > 0:\r\n            ind = ind + NN - ii\r\n\r\n    return ind"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a sequence of complex128s into a single complex128 array", "response": "def sc_to_fc(spvec, nmax, mmax, nrows, ncols):\r\n    \"\"\"assume Ncols is even\"\"\"\r\n\r\n    fdata = np.zeros([int(nrows), ncols], dtype=np.complex128)\r\n\r\n    for k in xrange(0, int(ncols / 2)):\r\n        if k < mmax:\r\n            kk = k\r\n            ind = mindx(kk, nmax, mmax)\r\n            vec = spvec[ind:ind + nmax - np.abs(kk) + 1]\r\n            fdata[:, kk] = fcvec_m_sc(vec, kk, nmax, nrows)\r\n\r\n            kk = -(k + 1)\r\n            ind = mindx(kk, nmax, mmax)\r\n            vec = spvec[ind:ind + nmax - np.abs(kk) + 1]\r\n            fdata[:, kk] = fcvec_m_sc(vec, kk, nmax, nrows)\r\n\r\n        if k == mmax:\r\n            kk = k\r\n            ind = mindx(kk, nmax, mmax)\r\n            vec = spvec[ind:ind + nmax - np.abs(kk) + 1]\r\n            fdata[:, kk] = fcvec_m_sc(vec, kk, nmax, nrows)\r\n\r\n    return fdata"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _use_methods(cls):\n        use_dict = JobCalculation._use_methods\n        use_dict.update({\n            \"parameters\": {\n                'valid_types': RipsDistanceMatrixParameters,\n                'additional_parameter': None,\n                'linkname': 'parameters',\n                'docstring': 'add command line parameters',\n            },\n            \"distance_matrix\": {\n                'valid_types': SinglefileData,\n                'additional_parameter': None,\n                'linkname': 'distance_matrix',\n                'docstring': \"distance matrix of point cloud\",\n            },\n            \"remote_folder\": {\n                'valid_types': RemoteData,\n                'additional_parameter': None,\n                'linkname': 'remote_folder',\n                'docstring': \"remote folder containing distance matrix\",\n            },\n        })\n        return use_dict", "response": "Add use_* methods for calculations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the inputdict for the current object.", "response": "def _validate_inputs(self, inputdict):\n        \"\"\" Validate input links.\n        \"\"\"\n        # Check inputdict\n        try:\n            parameters = inputdict.pop(self.get_linkname('parameters'))\n        except KeyError:\n            raise InputValidationError(\"No parameters specified for this \"\n                                       \"calculation\")\n        if not isinstance(parameters, RipsDistanceMatrixParameters):\n            raise InputValidationError(\"parameters not of type \"\n                                       \"RipsDistanceMatrixParameters\")\n        # Check code\n        try:\n            code = inputdict.pop(self.get_linkname('code'))\n        except KeyError:\n            raise InputValidationError(\"No code specified for this \"\n                                       \"calculation\")\n\n        # Check input files\n        try:\n            distance_matrix = inputdict.pop(\n                self.get_linkname('distance_matrix'))\n            if not isinstance(distance_matrix, SinglefileData):\n                raise InputValidationError(\n                    \"distance_matrix not of type SinglefileData\")\n            symlink = None\n\n        except KeyError:\n            distance_matrix = None\n\n            try:\n                remote_folder = inputdict.pop(\n                    self.get_linkname('remote_folder'))\n                if not isinstance(remote_folder, RemoteData):\n                    raise InputValidationError(\n                        \"remote_folder is not of type RemoteData\")\n\n                comp_uuid = remote_folder.get_computer().uuid\n                remote_path = remote_folder.get_remote_path()\n                symlink = (comp_uuid, remote_path, self._REMOTE_FOLDER_LINK)\n\n            except KeyError:\n                raise InputValidationError(\n                    \"Need to provide either distance_matrix or remote_folder\")\n\n        # Check that nothing is left unparsed\n        if inputdict:\n            raise ValidationError(\"Unrecognized inputs: {}\".format(inputdict))\n\n        return parameters, code, distance_matrix, symlink"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_for_submission(self, tempfolder, inputdict):\n        parameters, code, distance_matrix, symlink = \\\n                self._validate_inputs(inputdict)\n\n        # Prepare CalcInfo to be returned to aiida\n        calcinfo = CalcInfo()\n        calcinfo.uuid = self.uuid\n        calcinfo.remote_copy_list = []\n        calcinfo.retrieve_list = parameters.output_files\n\n        codeinfo = CodeInfo()\n        codeinfo.code_uuid = code.uuid\n\n        if distance_matrix is not None:\n            calcinfo.local_copy_list = [\n                [\n                    distance_matrix.get_file_abs_path(),\n                    distance_matrix.filename\n                ],\n            ]\n            codeinfo.cmdline_params = parameters.cmdline_params(\n                distance_matrix_file_name=distance_matrix.filename)\n        else:\n            calcinfo.remote_symlink_list = [symlink]\n            codeinfo.cmdline_params = parameters.cmdline_params(\n                remote_folder_path=self._REMOTE_FOLDER_LINK)\n\n        calcinfo.codes_info = [codeinfo]\n\n        return calcinfo", "response": "Prepare the CalcInfo object to be returned to the plugin."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cache_status(db, aid, force=False):\n    with db:\n        cur = db.cursor()\n        if not force:\n            # We don't do anything if we already have this aid in our\n            # cache.\n            cur.execute('SELECT 1 FROM cache_anime WHERE aid=?', (aid,))\n            if cur.fetchone() is not None:\n                return\n\n        # Retrieve information for determining complete.\n        cur.execute(\n            'SELECT episodecount, enddate FROM anime WHERE aid=?', (aid,))\n        row = cur.fetchone()\n        if row is None:\n            raise ValueError('aid provided does not exist')\n        episodecount, enddate = row\n\n        # Select all regular episodes in ascending order.\n        cur.execute(\"\"\"\n            SELECT number, user_watched FROM episode\n            WHERE aid=? AND type=?\n            ORDER BY number ASC\n            \"\"\", (aid, get_eptype(db, 'regular').id))\n\n        # We find the last consecutive episode that is user_watched.\n        number = 0\n        for number, watched in cur:\n            # Once we find the first unwatched episode, we set the last\n            # consecutive watched episode to the previous episode (or 0).\n            if watched == 0:\n                number -= 1\n                break\n        # We store this in the cache.\n        set_status(db, aid, enddate and episodecount <= number, number)", "response": "Calculate and cache status for given anime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator of the complete anime.", "response": "def get_complete(db) -> Iterator[int]:\n    \"\"\"Return AID of complete anime.\"\"\"\n    cur = db.cursor()\n    cur.execute(\n        \"\"\"SELECT aid FROM cache_anime\n        WHERE complete=?\"\"\", (1,))\n    for row in cur:\n        yield row[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_component(self, module_name):\n        '''Create a component out of a loaded module.\n\n        Turns a previously-loaded shared module into a component in the\n        manager. This will invalidate any objects that are children of this\n        node.\n\n        The @ref module_name argument can contain options that set various\n        properties of the new component. These must be appended to the module\n        name, prefixed by a question mark for each property, in key=value\n        format. For example, to change the instance name of the new component,\n        append '?instance_name=new_name' to the module name.\n\n        @param module_name Name of the module to turn into a component.\n        @raises FailedToCreateComponentError\n\n        '''\n        with self._mutex:\n            if not self._obj.create_component(module_name):\n                raise exceptions.FailedToCreateComponentError(module_name)\n            # The list of child components will have changed now, so it must be\n            # reparsed.\n            self._parse_component_children()", "response": "Creates a new component out of a previously loaded shared module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a component specified by instance_name from the manager.", "response": "def delete_component(self, instance_name):\n        '''Delete a component.\n\n        Deletes the component specified by @ref instance_name from the manager.\n        This will invalidate any objects that are children of this node.\n\n        @param instance_name The instance name of the component to delete.\n        @raises FailedToDeleteComponentError\n\n        '''\n        with self._mutex:\n            if self._obj.delete_component(instance_name) != RTC.RTC_OK:\n                raise exceptions.FailedToDeleteComponentError(instance_name)\n            # The list of child components will have changed now, so it must be\n            # reparsed.\n            self._parse_component_children()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a shared library.", "response": "def load_module(self, path, init_func):\n        '''Load a shared library.\n\n        Call this function to load a shared library (DLL file under Windows,\n        shared object under UNIX) into the manager.\n\n        @param path The path to the shared library.\n        @param init_func The name entry function in the library.\n        @raises FailedToLoadModuleError\n\n        '''\n        try:\n            with self._mutex:\n                if self._obj.load_module(path, init_func) != RTC.RTC_OK:\n                    raise exceptions.FailedToLoadModuleError(path)\n        except CORBA.UNKNOWN as e:\n            if e.args[0] == UNKNOWN_UserException:\n                raise exceptions.FailedToLoadModuleError(path, 'CORBA User Exception')\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunloading a loaded shared library.", "response": "def unload_module(self, path):\n        '''Unload a loaded shared library.\n\n        Call this function to remove a shared library (e.g. a component) that\n        was previously loaded.\n\n        @param path The path to the shared library.\n        @raises FailedToUnloadModuleError\n\n        '''\n        with self._mutex:\n            if self._obj.unload_module(path) != RTC.RTC_OK:\n                raise FailedToUnloadModuleError(path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef factory_profiles(self):\n        '''The factory profiles of all loaded modules.'''\n        with self._mutex:\n            if not self._factory_profiles:\n                self._factory_profiles = []\n                for fp in self._obj.get_factory_profiles():\n                    self._factory_profiles.append(utils.nvlist_to_dict(fp.properties))\n        return self._factory_profiles", "response": "The factory profiles of all loaded modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting a configuration parameter of the manager.", "response": "def set_config_parameter(self, param, value):\n        '''Set a configuration parameter of the manager.\n\n        @param The parameter to set.\n        @value The new value for the parameter.\n        @raises FailedToSetConfigurationError\n\n        '''\n        with self._mutex:\n            if self._obj.set_configuration(param, value) != RTC.RTC_OK:\n                raise exceptions.FailedToSetConfigurationError(param, value)\n            # Force a reparse of the configuration\n            self._configuration = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configuration(self):\n        '''The configuration dictionary of the manager.'''\n        with self._mutex:\n            if not self._configuration:\n                self._configuration = utils.nvlist_to_dict(self._obj.get_configuration())\n        return self._configuration", "response": "The configuration dictionary of the manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef profile(self):\n        '''The manager's profile.'''\n        with self._mutex:\n            if not self._profile:\n                profile = self._obj.get_profile()\n                self._profile = utils.nvlist_to_dict(profile.properties)\n        return self._profile", "response": "The manager s profile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loaded_modules(self):\n        '''The list of loaded module profile dictionaries.'''\n        with self._mutex:\n            if not self._loaded_modules:\n                self._loaded_modules = []\n                for mp in self._obj.get_loaded_modules():\n                    self._loaded_modules.append(utils.nvlist_to_dict(mp.properties))\n        return self._loaded_modules", "response": "The list of loaded module profile dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slaves(self):\n        '''The list of slave managers of this manager, if any.\n\n        This information can also be found by listing the children of this node\n        that are of type @ref Manager.\n\n        '''\n        with self._mutex:\n            if not self._slaves:\n                self._slaves = [c for c in self.children if c.is_manager]\n        return self._slaves", "response": "The list of slaves of this manager if any."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nauthenticate the request given the access token.", "response": "def authenticate_credentials(self, request, access_token):\n        \"\"\"\n        Authenticate the request, given the access token.\n        \"\"\"\n\n        try:\n            token = oauth2_provider.oauth2.models.AccessToken.objects.select_related('user')\n            # provider_now switches to timezone aware datetime when\n            # the oauth2_provider version supports to it.\n            token = token.get(token=access_token, expires__gt=provider_now())\n        except oauth2_provider.oauth2.models.AccessToken.DoesNotExist:\n            raise exceptions.AuthenticationFailed('Invalid token')\n\n        user = token.user\n\n        if not user.is_active:\n            msg = 'User inactive or deleted: %s' % user.username\n            raise exceptions.AuthenticationFailed(msg)\n\n        return (user, token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the cloud from the list of added clouds in mist. io service.", "response": "def delete(self):\n        \"\"\"\n        Delete the cloud from the list of added clouds in mist.io service.\n\n        :returns: A list of mist.clients' updated clouds.\n        \"\"\"\n        req = self.request(self.mist_client.uri + '/clouds/' + self.id)\n        req.delete()\n        self.mist_client.update_clouds()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable(self):\n        payload = {\n            \"new_state\": \"1\"\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id, data=data)\n        req.post()\n        self.enabled = True\n        self.mist_client.update_clouds()", "response": "Enable the Cloud.\n\n        :returns:  A list of mist.clients' updated clouds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disable(self):\n        payload = {\n            \"new_state\": \"0\"\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id, data=data)\n        req.post()\n        self.enabled = False\n        self.mist_client.update_clouds()", "response": "Disable the Cloud.\n\n        :returns:  A list of mist.clients' updated clouds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sizes(self):\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/sizes')\n        sizes = req.get().json()\n        return sizes", "response": "Returns a list of available machine sizes for this cloud."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef locations(self):\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/locations')\n        locations = req.get().json()\n        return locations", "response": "Returns a list of available locations for this machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of available networks associated to a cloud.", "response": "def networks(self):\n        \"\"\"\n        Available networks.\n\n        :returns: A list of available networks associated to a provider.\n        \"\"\"\n        if self.provider in ['openstack', 'nephoscale']:\n            req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/networks')\n            networks = req.get().json()\n            return networks\n        else:\n            print \"Network actions not supported yet for %s provider\" % self.provider"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef images(self):\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/images')\n        images = req.get().json()\n        return images", "response": "Returns a list of all available images for this machine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_image(self, search_term):\n        payload = {\n            'search_term': search_term\n        }\n        data = json.dumps(payload)\n\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/images', data=data)\n        images = req.get().json()\n        return images", "response": "Search for a specific image by providing a search term."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_machines(self):\n        try:\n            req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/machines')\n            machines = req.get().json()\n        except:\n            # Eg invalid cloud credentials\n            machines = {}\n\n        if machines:\n            for machine in machines:\n                self._machines[machine['machine_id']] = Machine(machine, self)\n        else:\n            self._machines = {}", "response": "Request a list of all added machines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new machine on the given cloud.", "response": "def create_machine(self, name, key, image_id, location_id, size_id,\n                       image_extra=\"\", disk=\"\", script=\"\", monitoring=False,\n                       ips=[], networks=[], location_name=\"\", async=False,\n                       docker_command=\"\", quantity=1, persist=False, fire_and_forget=True,\n                       timeout=6000, script_id=\"\", script_params=\"\", verbose=False,\n                       associate_floating_ip=False, provider=\"\", tags=None,\n                       cloud_init='', env_vars=''):\n        \"\"\"\n        Create a new machine on the given cloud\n\n        :param name: Name of the machine\n        :param key: Key Object to associate with the machine\n        :param image_id: Id of image to be used with the creation\n        :param location_id: Id of the cloud's location to create the machine\n        :param size_id: If of the size of the machine\n        :param image_extra: Needed only by Linode cloud\n        :param disk: Needed only by Linode cloud\n        :returns: An update list of added machines\n        \"\"\"\n        if isinstance(key, basestring):\n            key_id = key\n        else:\n            key_id = key.id\n\n        payload = {\n            'name': name,\n            'provider': provider,\n            'key': key_id,\n            'image': image_id,\n            'location': location_id,\n            'size': size_id,\n            'image_extra': image_extra,\n            'disk': disk,\n            'script': script,\n            'monitoring': monitoring,\n            'ips': ips,\n            'networks': networks,\n            'location_name': location_name,\n            'docker_command': docker_command,\n            'async': async,\n            'quantity': quantity,\n            'persist': persist,\n            'associate_floating_ip': associate_floating_ip,\n            'tags': tags,\n            'cloud_init': cloud_init,\n            'env_vars':  env_vars\n        }\n        # add as params only if they are provided\n        if script_id:\n            payload['script_id'] = script_id\n        if script_params:\n            payload['script_params'] = script_params\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+'/clouds/'+self.id+'/machines', data=data)\n        result = req.post()\n        self.update_machines()\n\n        if not async or fire_and_forget:\n            return result.json()\n        else:\n            job_id = result.json()['job_id']\n\n            started_at = time()\n            while True:\n                job = self.mist_client.get_job(job_id)\n\n                if verbose:\n                    summary = job.get('summary', {})\n\n                    probes = summary.get('probe', {})\n                    creates = summary.get('create', {})\n                    scripts = summary.get('script', {})\n                    monitoring = summary.get('monitoring', {})\n\n                    states = [\n                        'success',\n                        'error',\n                        'skipped',\n                        'pending'\n                    ]\n\n                    x = PrettyTable(['', 'SUCCESS', 'ERROR', 'SKIPPED', 'PENDING'])\n\n                    if creates:\n                        machines_created = {}\n                        for state in states:\n                            machines_created[state] = '%s/%s' % (creates.get(state, 'Undefined'), quantity)\n                        x.add_row([\"Create:\", machines_created['success'], machines_created['error'],\n                                   machines_created['skipped'], machines_created['pending']])\n\n                    if probes:\n                        probed_machines = {}\n                        for state in states:\n                            probed_machines[state] = '%s/%s' % (probes.get(state, 'Undefined'), quantity)\n                        x.add_row([\"Probe:\", probed_machines['success'], probed_machines['error'],\n                                   probed_machines['skipped'], probed_machines['pending']])\n\n                    if scripts:\n                        scripted_machines = {}\n                        for state in states:\n                            scripted_machines[state] = '%s/%s' % (scripts.get(state, 'Undefined'), quantity)\n                        x.add_row([\"Script:\", scripted_machines['success'], scripted_machines['error'],\n                                   scripted_machines['skipped'], scripted_machines['pending']])\n\n                    if monitoring:\n                        monitored_machines = {}\n                        for state in states:\n                            monitored_machines[state] = '%s/%s' % (monitoring.get(state, 'Undefined'), quantity)\n                        x.add_row([\"Monitoring:\", monitored_machines['success'], monitored_machines['error'],\n                                   monitored_machines['skipped'], monitored_machines['pending']])\n\n                    print x\n                    print\n\n                if job.get('finished_at', 0):\n                    provision_finished = True\n                else:\n                    # In case of nested logs, we have to make sure we parse the\n                    # the logs correctly in order to determine whether the\n                    # provisioned VM is running, since a story may contain logs\n                    # of multiple machines\n                    for log in job.get('logs', []):\n                        if log.get('machine_name', '') == name and \\\n                            'machine_creation_finished' in log.values():\n                            machine_id = log['machine_id']\n                            error = log.get('error')\n                            break\n                    else:\n                        sleep(5)\n                        continue\n\n                    if error:\n                        provision_finished = True\n                    else:\n                        for log in job.get('logs', []):\n                            if log.get('machine_id', '') == machine_id and \\\n                                'post_deploy_finished' in log.values():\n                                provision_finished = True\n                                break\n                        else:\n                            provision_finished = False\n\n                if provision_finished:\n                    error = job.get('error')\n                    if error:\n                        print \"Finished with errors:\"\n                        logs = job.get('logs', [])\n                        for log in logs:\n                            error = log.get('error')\n                            if error:\n                                print \" - \", error\n                        raise Exception(\"Create machine failed. Check the logs.\")\n                    elif verbose and not error:\n                        print \"Finished without errors!\"\n                    self.update_machines()\n                    return job\n\n                elif time() - started_at > timeout:\n                    print \"Timed out!\"\n                    raise Exception(\"Create machine timed out. Check the logs.\")\n                    return job\n\n                sleep(5)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactioning for the machine", "response": "def _machine_actions(self, action):\n        \"\"\"\n        Actions for the machine (e.g. stop, start etc)\n\n        :param action: Can be \"reboot\", \"start\", \"stop\", \"destroy\"\n        :returns: An updated list of the added machines\n        \"\"\"\n        payload = {\n            'action': action\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+'/clouds/'+self.cloud.id+'/machines/'+self.id, data=data)\n        req.post()\n        self.cloud.update_machines()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprobing the machine with the specified key_id.", "response": "def probe(self, key_id=None, ssh_user=None):\n        \"\"\"\n        If no parameter is provided, mist.io will try to probe the machine with\n        the default\n        :param key_id: Optional. Give if you explicitly want to probe with this\n        key_id\n        :param ssh_user: Optional. Give if you explicitly want a specific user\n        :returns: A list of data received by the probing (e.g. uptime etc)\n        \"\"\"\n        ips = [ip for ip in self.info['public_ips'] if ':' not in ip]\n\n        if not ips:\n            raise Exception(\"No public IPv4 address available to connect to\")\n        payload = {\n            'host': ips[0],\n            'key': key_id,\n            'ssh_user': ssh_user\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri + \"/clouds/\" + self.cloud.id +\n                           \"/machines/\" + self.id + \"/probe\", data=data)\n        probe_info = req.post().json()\n        self.probed = True\n        return probe_info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling or disable monitoring on a machine", "response": "def _toggle_monitoring(self, action, no_ssh=False):\n        \"\"\"\n        Enable or disable monitoring on a machine\n\n        :param action: Can be either \"enable\" or \"disable\"\n        \"\"\"\n        payload = {\n            'action': action,\n            'name': self.name,\n            'no_ssh': no_ssh,\n            'public_ips': self.info['public_ips'],\n            'dns_name': self.info['extra'].get('dns_name', 'n/a')\n        }\n\n        data = json.dumps(payload)\n\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/monitoring\",\n                           data=data)\n        req.post()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget stats of a monitored machine", "response": "def get_stats(self, start=int(time()), stop=int(time())+10, step=10):\n        \"\"\"\n        Get stats of a monitored machine\n\n        :param start: Time formatted as integer, from when to fetch stats (default now)\n        :param stop: Time formatted as integer, until when to fetch stats (default +10 seconds)\n        :param step: Step to fetch stats (default 10 seconds)\n        :returns: A dict of stats\n        \"\"\"\n        payload = {\n            'v': 2,\n            'start': start,\n            'stop': stop,\n            'step': step\n        }\n\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/stats\", data=data)\n        stats = req.get().json()\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of dicts each of which is a metric that you can add to this monitored machine", "response": "def available_metrics(self):\n        \"\"\"\n        List all available metrics that you can add to this machine\n\n        :returns: A list of dicts, each of which is a metric that you can add to a monitored machine\n        \"\"\"\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/metrics\")\n        metrics = req.get().json()\n        return metrics"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a metric to a monitored machine", "response": "def add_metric(self, metric_id):\n        \"\"\"\n        Add a metric to a monitored machine\n\n        :param metric_id: Metric_id (provided by self.available_metrics)\n        \"\"\"\n        payload = {\n            'metric_id': metric_id\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/metrics\", data=data)\n        req.put()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_metric(self, metric_id):\n        payload = {\n            'metric_id': metric_id\n        }\n\n        data = json.dumps(payload)\n\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/metrics\", data=data)\n        req.delete()", "response": "Remove a metric from a monitored machine"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a custom python plugin to the collectd instance of a monitored plugin", "response": "def add_python_plugin(self, name, python_file, value_type=\"gauge\", unit=None):\n        \"\"\"\n        Add a custom python plugin to the collectd instance of a monitored plugin\n\n        :param python_file: Path of the python file to be added as custom python plugin\n        :param name: Name of the plugin\n        :param value_type: Optional. Can be either \"gauge\" or \"derive\"\n        :param unit: Optional. If given the new plugin will be measured according to this unit\n        \"\"\"\n\n        if not os.path.isfile(python_file):\n            raise Exception(python_file, \"is not a file or could not be found in tho given path\")\n\n        with open(python_file) as f:\n            script = f.read()\n\n        payload = {\n            'plugin_type': 'python',\n            'name': name,\n            'unit': unit,\n            'value_type': value_type,\n            'read_function': script,\n            'host': self.info['public_ips'][0]\n        }\n\n        data = json.dumps(payload)\n\n        #PLugin id must be in lowercase\n        plugin_id = name.lower()\n\n        #PLugin id must contain only alphanumeric chars\n        pattern = re.compile('\\W')\n        plugin_id = re.sub(pattern, \"_\", plugin_id)\n\n        #Plugin id should not have double underscores\n        while \"__\" in plugin_id:\n            pattern = \"\\r?__\"\n            plugin_id = re.sub(pattern, \"_\", plugin_id)\n\n        #Plugin id should not have underscore as first or last char\n        if plugin_id[-1] == \"_\":\n            plugin_id = plugin_id[:-2]\n\n        if plugin_id[0] == \"_\":\n            plugin_id = plugin_id[1:]\n\n        req = self.request(self.mist_client.uri+\"/clouds/\"+self.cloud.id+\"/machines/\"+self.id+\"/plugins/\"+plugin_id,\n                           data=data)\n        req.post()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the private ssh - key as string", "response": "def private(self):\n        \"\"\"\n        Return the private ssh-key\n\n        :returns: The private ssh-key as string\n        \"\"\"\n        req = self.request(self.mist_client.uri+'/keys/'+self.id+\"/private\")\n        private = req.get().json()\n        return private"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef public(self):\n        req = self.request(self.mist_client.uri+'/keys/'+self.id+\"/public\")\n        public = req.get().json()\n        return public", "response": "Returns the public ssh - key as string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename a key in the list of keys.", "response": "def rename(self, new_name):\n        \"\"\"\n        Rename a key\n\n        :param new_name: New name for the key (will also serve as the key's id)\n        :returns: An updated list of added keys\n        \"\"\"\n        payload = {\n            'new_name': new_name\n        }\n        data = json.dumps(payload)\n        req = self.request(self.mist_client.uri+'/keys/'+self.id, data=data)\n        req.put()\n        self.id = new_name\n        self.mist_client.update_keys()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_default(self):\n        req = self.request(self.mist_client.uri+'/keys/'+self.id)\n        req.post()\n        self.is_default = True\n        self.mist_client.update_keys()", "response": "Sets this key as the default key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete this key from mist. io", "response": "def delete(self):\n        \"\"\"\n        Delete this key from mist.io\n\n        :returns: An updated list of added keys\n        \"\"\"\n        req = self.request(self.mist_client.uri+'/keys/'+self.id)\n        req.delete()\n        self.mist_client.update_keys()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the status of the bug.", "response": "def status(self, value):\n        \"\"\"\n            Property for getting or setting the bug status\n\n            >>> bug.status = \"REOPENED\"\n        \"\"\"\n        if self._bug.get('id', None):\n            if value in VALID_STATUS:\n                self._bug['status'] = value\n            else:\n                raise BugException(\"Invalid status type was used\")\n        else:\n            raise BugException(\"Can not set status unless there is a bug id.\"\n                               \" Please call Update() before setting\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n        if 'id' in self._bug:\n            result = self._bugsy.request('bug/%s' % self._bug['id'])\n            self._bug = dict(**result['bugs'][0])\n        else:\n            raise BugException(\"Unable to update bug that isn't in Bugzilla\")", "response": "Update the current object with the latest changes from Bugzilla"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_comments(self):\n        bug = str(self._bug['id'])\n        res = self._bugsy.request('bug/%s/comment' % bug)\n\n        return [Comment(bugsy=self._bugsy, **comments) for comments\n                in res['bugs'][bug]['comments']]", "response": "Returns a list of Comment instances for this bug."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_comment(self, comment):\n        # If we have a key post immediately otherwise hold onto it until\n        # put(bug) is called\n        if 'id' in self._bug:\n            self._bugsy.request('bug/{}/comment'.format(self._bug['id']),\n                                method='POST', json={\"comment\": comment}\n                                )\n        else:\n            self._bug['comment'] = comment", "response": "Adds a comment to a bug."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds tags to the comments", "response": "def add_tags(self, tags):\n        \"\"\"\n            Add tags to the comments\n        \"\"\"\n        if not isinstance(tags, list):\n            tags = [tags]\n        self._bugsy.request('bug/comment/%s/tags' % self._comment['id'],\n                            method='PUT', json={\"add\": tags})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _readuntil(f, end=_TYPE_END):\n\tbuf = bytearray()\n\tbyte = f.read(1)\n\twhile byte != end:\n\t\tif byte == b'':\n\t\t\traise ValueError('File ended unexpectedly. Expected end byte {}.'.format(end))\n\t\tbuf += byte\n\t\tbyte = f.read(1)\n\treturn buf", "response": "Helper function to read bytes until a certain end byte is hit"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes a byte string from a file - like object.", "response": "def _decode_buffer(f):\n\t\"\"\"\n\tString types are normal (byte)strings\n\tstarting with an integer followed by ':'\n\twhich designates the string\u2019s length.\n\t\n\tSince there\u2019s no way to specify the byte type\n\tin bencoded files, we have to guess\n\t\"\"\"\n\tstrlen = int(_readuntil(f, _TYPE_SEP))\n\tbuf = f.read(strlen)\n\tif not len(buf) == strlen:\n\t\traise ValueError(\n\t\t\t'string expected to be {} bytes long but the file ended after {} bytes'\n\t\t\t.format(strlen, len(buf)))\n\ttry:\n\t\treturn buf.decode()\n\texcept UnicodeDecodeError:\n\t\treturn buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bdecode(f_or_data):\n\tif isinstance(f_or_data, str):\n\t\tf_or_data = f_or_data.encode()\n\tif isinstance(f_or_data, bytes):\n\t\tf_or_data = BytesIO(f_or_data)\n\t\n\t#TODO: the following line is the only one that needs readahead.\n\t#peek returns a arbitrary amount of bytes, so we have to slice.\n\tif f_or_data.seekable():\n\t\tfirst_byte = f_or_data.read(1)\n\t\tf_or_data.seek(-1, SEEK_CUR)\n\telse:\n\t\tfirst_byte = f_or_data.peek(1)[:1]\n\tbtype = TYPES.get(first_byte)\n\tif btype is not None:\n\t\treturn btype(f_or_data)\n\telse: #Used in dicts and lists to designate an end\n\t\tassert_btype(f_or_data.read(1), _TYPE_END)\n\t\treturn None", "response": "This function decodes the data of the object in the file - like object f_or_data and returns the decoded object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _encode_buffer(string, f):\n\tif isinstance(string, str):\n\t\tstring = string.encode()\n\tf.write(str(len(string)).encode())\n\tf.write(_TYPE_SEP)\n\tf.write(string)", "response": "Writes the bencoded form of the input string or bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes the given mapping to the given file f.", "response": "def _encode_mapping(mapping, f):\n\t\"\"\"Encodes the mapping items in lexical order (spec)\"\"\"\n\tf.write(_TYPE_DICT)\n\tfor key, value in sorted(mapping.items()):\n\t\t_encode_buffer(key, f)\n\t\tbencode(value, f)\n\tf.write(_TYPE_END)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bencode(data, f=None):\n\tif f is None:\n\t\tf = BytesIO()\n\t\t_bencode_to_file(data, f)\n\t\treturn f.getvalue()\n\telse:\n\t\t_bencode_to_file(data, f)", "response": "Writes a serializable data piece to f\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode bencoded files to python syntax", "response": "def main(args=None):\n\t\"\"\"Decodes bencoded files to python syntax (like JSON, but with bytes support)\"\"\"\n\timport sys, pprint\n\tfrom argparse import ArgumentParser, FileType\n\tparser = ArgumentParser(description=main.__doc__)\n\tparser.add_argument('infile',  nargs='?', type=FileType('rb'), default=sys.stdin.buffer,\n\t\thelp='bencoded file (e.g. torrent) [Default: stdin]')\n\tparser.add_argument('outfile', nargs='?', type=FileType('w'), default=sys.stdout,\n\t\thelp='python-syntax serialization [Default: stdout]')\n\targs = parser.parse_args(args)\n\t\n\tdata = bdecode(args.infile)\n\tpprint.pprint(data, stream=args.outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch_connection(filename=':memory:'):\n\n    if no_redis:\n        raise Exception(\"redis package not found, please install redis-py via 'pip install redis'\")\n\n    RliteConnection.set_file(filename)\n\n    global orig_classes\n\n    # already patched\n    if orig_classes:\n        return\n\n    orig_classes = (redis.connection.Connection,\n                    redis.connection.ConnectionPool)\n\n    _set_classes(RliteConnection, RliteConnectionPool)", "response": "Patch the Redis Connection and RliteConnectionPool to use the rlite filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply sine in the Fourier domain.", "response": "def sin_fc(fdata):\n    \"\"\"Apply sine in the Fourier domain.\"\"\"\n\n    nrows = fdata.shape[0]\n    ncols = fdata.shape[1]\n\n    M = nrows / 2\n    fdata[int(M - 1), :] = 0\n    fdata[int(M + 1), :] = 0\n    \n    work1 = np.zeros([nrows, ncols], dtype=np.complex128)\n    work2 = np.zeros([nrows, ncols], dtype=np.complex128)\n\n    work1[0, :] = fdata[-1, :]\n    work1[1:, :] = fdata[0:-1, :]\n\n    work2[0:-1] = fdata[1:, :]\n    work2[-1, :] = fdata[0, :]\n\n    fdata[:, :] = 1.0 / (2 * 1j) * (work1 - work2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef divsin_fc(fdata):\n    \n    nrows = fdata.shape[0]\n    ncols = fdata.shape[1]\n\n    L = int(nrows / 2)  # Assuming nrows is even, which it should be.\n    L2 = L - 2  # This is the last index in the recursion for division by sine.\n    \n    g = np.zeros([nrows, ncols], dtype=np.complex128)\n    g[L2, :] = 2 * 1j * fdata[L - 1, :]\n\n    for k in xrange(L2, -L2, -1):\n        g[k - 1, :] = 2 * 1j * fdata[k, :] + g[k + 1, :]\n\n    fdata[:, :] = g", "response": "Apply divide by sine in the Fourier domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply phi derivative in the Fourier domain.", "response": "def dphi_fc(fdata):\n    \"\"\"Apply phi derivative in the Fourier domain.\"\"\"\n    \n    nrows = fdata.shape[0]\n    ncols = fdata.shape[1]\n    \n    B = int(ncols / 2)  # As always, we assume nrows and ncols are even\n    \n    a = list(range(0, int(B)))\n    ap = list(range(-int(B), 0))\n    a.extend(ap)\n    \n    dphi = np.zeros([nrows, ncols], np.complex128)\n    \n    for k in xrange(0, nrows):\n        dphi[k, :] = a\n        \n    fdata[:, :] = 1j * dphi * fdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sinLdot_fc(tfdata, pfdata):\n    \n    dphi_fc(tfdata)\n    \n    sin_fc(pfdata)\n    dtheta_fc(pfdata)\n    \n    return 1j * (tfdata - pfdata)", "response": "Apply sin of theta times the L operator to the data in the Fourier \n    domain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply L in the Fourier domain.", "response": "def L_fc(fdata):\n    \"\"\"Apply L in the Fourier domain.\"\"\"\n\n    fd = np.copy(fdata)\n\n    dphi_fc(fdata)\n    divsin_fc(fdata)\n\n    dtheta_fc(fd)\n\n    return (1j * fdata, -1j * fd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract(dump_files, extractors=ALL_EXTRACTORS):\n    # Dump processor function\n    def process_dump(dump, path):\n        for page in dump:\n            if page.namespace != 0: continue\n            else:\n                for cite in extract_cite_history(page, extractors):\n                    yield cite\n\n    # Map call\n    return mwxml.map(process_dump, dump_files)", "response": "Extracts cites from a set of MediaWiki XML dump files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_cite_history(page, extractors):\n    appearances = {} # For tracking the first appearance of an ID\n    ids = set() # For holding onto the ids in the last revision.\n    for revision in page:\n        ids = set(extract_ids(revision.text, extractors))\n\n        # For each ID, check to see if we have seen it before\n        for id in ids:\n            if id not in appearances:\n               appearances[id] = (revision.id, revision.timestamp)\n\n    for id in ids: #For the ids in the last version of the page\n        rev_id, timestamp = appearances[id]\n        yield (page.id, page.title, rev_id, timestamp, id.type, id.id)", "response": "Extracts cites from the history of a page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_ids(text, extractors):\n    for extractor in extractors:\n        for id in extractor.extract(text):\n            yield id", "response": "Extracts citation identifiers from a text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_members(self, rtcs):\n        '''Add other RT Components to this composite component as members.\n\n        This component must be a composite component.\n\n        '''\n        if not self.is_composite:\n            raise exceptions.NotCompositeError(self.name)\n        for rtc in rtcs:\n            if self.is_member(rtc):\n                raise exceptions.AlreadyInCompositionError(self.name, rtc.instance_name)\n        org = self.organisations[0].obj\n        org.add_members([x.object for x in rtcs])\n        # Force a reparse of the member information\n        self._orgs = []", "response": "Add other RT Components to this composite component as members."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves other RT Components from this composite component.", "response": "def remove_members(self, rtcs):\n        '''Remove other RT Components from this composite component.\n\n        rtcs is a list of components to remove. Each element must be either an\n        rtctree.Component object or a string containing a component's instance\n        name. rtctree.Component objects are more reliable.\n\n        This component must be a composite component.\n\n        '''\n        if not self.is_composite:\n            raise exceptions.NotCompositeError(self.name)\n        org = self.organisations[0].obj\n        members = org.get_members()\n        for rtc in rtcs:\n            if type(rtc) == str:\n                rtc_name = rtc\n            else:\n                rtc_name = rtc.instance_name\n            # Check if the RTC actually is a member\n            if not self.is_member(rtc):\n                raise exceptions.NotInCompositionError(self.name, rtc_name)\n            # Remove the RTC from the composition\n            org.remove_member(rtc_name)\n        # Force a reparse of the member information\n        self._orgs = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the given component is a member of this composition?", "response": "def is_member(self, rtc):\n        '''Is the given component a member of this composition?\n\n        rtc may be a Component object or a string containing a component's\n        instance name. Component objects are more reliable.\n\n        Returns False if the given component is not a member of this\n        composition.\n\n        Raises NotCompositeError if this component is not a composition.\n\n        '''\n        if not self.is_composite:\n            raise exceptions.NotCompositeError(self.name)\n        members = self.organisations[0].obj.get_members()\n        if type(rtc) is str:\n            for m in members:\n                if m.get_component_profile().instance_name == rtc:\n                    return True\n        else:\n            for m in members:\n                if m._is_equivalent(rtc.object):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parent_org_sdo_ids(self):\n        '''The SDO IDs of the compositions this RTC belongs to.'''\n        return [sdo.get_owner()._narrow(SDOPackage.SDO).get_sdo_id() \\\n                for sdo in self._obj.get_organizations() if sdo]", "response": "The SDO IDs of the compositions this RTC belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nactivate this component in an execution context.", "response": "def activate_in_ec(self, ec_index):\n        '''Activate this component in an execution context.\n\n        @param ec_index The index of the execution context to activate in.\n                        This index is into the total array of contexts, that\n                        is both owned and participating contexts. If the value\n                        of ec_index is greater than the length of\n                        @ref owned_ecs, that length is subtracted from\n                        ec_index and the result used as an index into\n                        @ref participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                ec = self.participating_ecs[ec_index]\n            else:\n                ec = self.owned_ecs[ec_index]\n            ec.activate_component(self._obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deactivate_in_ec(self, ec_index):\n        '''Deactivate this component in an execution context.\n\n        @param ec_index The index of the execution context to deactivate in.\n                        This index is into the total array of contexts, that\n                        is both owned and participating contexts. If the value\n                        of ec_index is greater than the length of\n                        @ref owned_ecs, that length is subtracted from\n                        ec_index and the result used as an index into\n                        @ref participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                ec = self.participating_ecs[ec_index]\n            else:\n                ec = self.owned_ecs[ec_index]\n            ec.deactivate_component(self._obj)", "response": "Deactivate this component in an execution context."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a reference to the execution context with the given handle.", "response": "def get_ec(self, ec_handle):\n        '''Get a reference to the execution context with the given handle.\n\n        @param ec_handle The handle of the execution context to look for.\n        @type ec_handle str\n        @return A reference to the ExecutionContext object corresponding to\n        the ec_handle.\n        @raises NoECWithHandleError\n\n        '''\n        with self._mutex:\n            for ec in self.owned_ecs:\n                if ec.handle == ec_handle:\n                    return ec\n            for ec in self.participating_ecs:\n                if ec.handle == ec_handle:\n                    return ec\n            raise exceptions.NoECWithHandleError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the index of the execution context with the given handle.", "response": "def get_ec_index(self, ec_handle):\n        '''Get the index of the execution context with the given handle.\n\n        @param ec_handle The handle of the execution context to look for.\n        @type ec_handle str\n        @return The index into the owned + participated arrays, suitable for\n        use in methods such as @ref activate_in_ec, or -1 if the EC was not\n        found.\n        @raises NoECWithHandleError\n\n        '''\n        with self._mutex:\n            for ii, ec in enumerate(self.owned_ecs):\n                if ec.handle == ec_handle:\n                    return ii\n            for ii, ec in enumerate(self.participating_ecs):\n                if ec.handle == ec_handle:\n                    return ii + len(self.owned_ecs)\n            raise exceptions.NoECWithHandleError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_state_string(self, add_colour=True):\n        '''Get the state of this component as an optionally-coloured string.\n\n        @param add_colour If True, ANSI colour codes will be added to the\n                          string.\n        @return A string describing the state of this component.\n\n        '''\n        with self._mutex:\n            if self.state == self.INACTIVE:\n                result = 'Inactive', ['bold', 'blue']\n            elif self.state == self.ACTIVE:\n                result = 'Active', ['bold', 'green']\n            elif self.state == self.ERROR:\n                result = 'Error', ['bold', 'white', 'bgred']\n            elif self.state == self.UNKNOWN:\n                result = 'Unknown', ['bold', 'red']\n            elif self.state == self.CREATED:\n                result = 'Created', ['reset']\n        if add_colour:\n            return utils.build_attr_string(result[1], supported=add_colour) + \\\n                    result[0] + utils.build_attr_string('reset', supported=add_colour)\n        else:\n            return result[0]", "response": "Get the state of this component as an optionally - coloured string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the state of the component in an execution context as a string.", "response": "def get_state_in_ec_string(self, ec_index, add_colour=True):\n        '''Get the state of the component in an execution context as a string.\n\n        @param ec_index The index of the execution context to check the state\n                        in. This index is into the total array of contexts,\n                        that is both owned and participating contexts. If the\n                        value of ec_index is greater than the length of @ref\n                        owned_ecs, that length is subtracted from ec_index and\n                        the result used as an index into @ref\n                        participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                state = self.participating_ec_states[ec_index]\n            else:\n                state = self.owned_ec_states[ec_index]\n        if state == self.INACTIVE:\n            result = 'Inactive', ['bold', 'blue']\n        elif state == self.ACTIVE:\n            result = 'Active', ['bold', 'green']\n        elif state == self.ERROR:\n            result = 'Error', ['bold', 'white', 'bgred']\n        elif state == self.UNKNOWN:\n            result = 'Unknown', ['bold', 'red']\n        elif state == self.CREATED:\n            result = 'Created', ['reset']\n        if add_colour:\n            return utils.build_attr_string(result[1], supported=add_colour) + \\\n                    result[0] + utils.build_attr_string('reset',\n                    supported=add_colour)\n        else:\n            return result[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets this component in an execution context.", "response": "def reset_in_ec(self, ec_index):\n        '''Reset this component in an execution context.\n\n        @param ec_index The index of the execution context to reset in. This\n                        index is into the total array of contexts, that is both\n                        owned and participating contexts. If the value of\n                        ec_index is greater than the length of @ref owned_ecs,\n                        that length is subtracted from ec_index and the result\n                        used as an index into @ref participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                ec = self.participating_ecs[ec_index]\n            else:\n                ec = self.owned_ecs[ec_index]\n            ec.reset_component(self._obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef state_in_ec(self, ec_index):\n        '''Get the state of the component in an execution context.\n\n        @param ec_index The index of the execution context to check the state\n                        in. This index is into the total array of contexts,\n                        that is both owned and participating contexts. If the\n                        value of ec_index is greater than the length of @ref\n                        owned_ecs, that length is subtracted from ec_index and\n                        the result used as an index into @ref\n                        participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                return self.participating_ec_states[ec_index]\n            else:\n                return self.owned_ec_states[ec_index]", "response": "Get the state of the component in an execution context."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the up - to - date state of the component in an execution context.", "response": "def refresh_state_in_ec(self, ec_index):\n        '''Get the up-to-date state of the component in an execution context.\n\n        This function will update the state, rather than using the cached\n        value. This may take time, if the component is executing on a remote\n        node.\n\n        @param ec_index The index of the execution context to check the state\n                        in. This index is into the total array of contexts,\n                        that is both owned and participating contexts. If the\n                        value of ec_index is greater than the length of @ref\n                        owned_ecs, that length is subtracted from ec_index and\n                        the result used as an index into @ref\n                        participating_ecs.\n\n        '''\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                state = self._get_ec_state(self.participating_ecs[ec_index])\n                self.participating_ec_states[ec_index] = state\n            else:\n                state = self._get_ec_state(self.owned_ecs[ec_index])\n                self.owned_ec_states[ec_index] = state\n            return state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam this component alive?", "response": "def alive(self):\n        '''Is this component alive?'''\n        with self._mutex:\n            if self.exec_contexts:\n                for ec in self.exec_contexts:\n                    if self._obj.is_alive(ec):\n                        return True\n        return False"}
